comment,sentiment
hi delay response indeed cause fact removed bunch fix would use well would like open add,neutral
back got ta catch everything update thank need assistance call time additionally used work well individual inference batch inference,neutral
nice ready well sun mar wrote back got ta catch everything update reply directly view id,positive
already compatible try run conversion get error suggesting need rename error loading missing key unexpected key unexpected key end missing key end create forward pas huge thanks pretty much thing script except local instead hub,positive
hey reason add one original compatible still added already compatible,positive
reason computation also sure slowdown due flag used properly believe measuring latency worse correct worse impossible launch access later discovered also precision float also something take account slowdown something try combat dive bit come float rope cal also look rope note faster taken account hard tackle take memory efficient path memory efficient take fast path use memory,negative
yes need add particular order,positive
prepare basically update appropriate value intend run model affect causal mask longer taken account rope,positive
look yes slow version need custom forward explicitly backward instead engine,negative
back got ta catch everything update,neutral
way automatically added tho since set true default usually avoid touching modeling,positive
thanks review collection documentation test like machine due size model,positive
live documentation reflected available day last update,positive
think yes need pad left generation simply model trained padding right want model continue hello world pad pad pad pad pad pad hello world batch,positive
let know like update anything,neutral
hi sorry delay bad balance quarter yes think page would still good fit link think another section would better,positive
use approach taken force float key co sin tensor example mixed precision training automatically downcast case,neutral
hi would love help one else working,positive
want load pipeline locally instead call model directly branch git clone git git add file import pipeline pipe pipeline pipe wearing print course use hope work,positive
odd issue label actually leaning considered team since clear label column label delete label issue especially module trainer receive see annotation th line may label default data collator transformer amazing convenient hope added help people process,positive
confirm param output parameter,neutral
nice catch fix try,positive
problem sorry make sense,negative
thanks let modify accordingly sun mar wrote like dev version already cohere use reply directly view id,positive
issue automatically marked stale recent activity think still need please comment thread please note follow likely,negative
yes helpful load web image know load local model pipeline load like model processor case might need add beginning path indicate relative path want load pipeline locally instead call model directly,positive
yes helpful load web image know load local model pipeline load like model processor case might need add beginning path indicate relative path,neutral
found way fix trick load image pillow library full code import import torch import image import model processor device else device image prompt wearing processor device print output like python loading warning wearing hat hope work yes helpful load web image know load local model pipeline,positive
found way fix trick load image pillow library full code import import torch import image import model processor device else device image prompt wearing processor device print output like python loading warning wearing hat hope work,positive
trainer script running directly,positive
like dev version already cohere use,neutral
thanks helpful try get soon,positive
thank made pull request happening well,neutral
need set false main reference model see comment comment try solution work like charm set false make evaluation training read doc use cache link seem like reduce inference time explain work like charm sir,negative
put together notebook one cache dev work also wrapped new test case ready pasted,positive
sequence would fine would enable pas distribution try create corresponding weekend,positive
whoop sorry saw oh fair like sort mechanism allow conversion occur maybe sort mechanism ie write,positive
wait saying llama mistral interesting gemma bug make somewhat become accurate would say maximum also fixed llama rope issue another hit however inference recently smart team made inference fully faster inference might cause non however knowledge mistral touched include update mistral suggest issue causing slowdown back envelope ie guess much mistral llama gemma,positive
hi yes hope issue since interface person file u want file please kindly guide routine thanks lot,positive
someone reopen issue think issue really fixed,positive
thanks lot advice also issue think root trained model,positive
something quite right setting training match setting seem work though behavior work module none target found please make sure pas valid,positive
hi sorry fixed yet tested yesterday commit problem still issue found custom mask pasted top corner first iteration empty cache smoothly pas next iteration set starting later layer tensor used extract attention mask causal mask since zero extracted mask different original mask ignore handling custom attention mask document cook passing custom attention mask intact somehow else ensure delivery mask intact maybe extra option also test perform one forward iteration allow testing cache know greater context part glad test though please fix need work fancy speculative show soon,positive
thanks fast review accepted made couple small,positive
already latest version even pip install work missing something,positive
hey took peek hood like setting slow super method receive parameter set passing work preliminary python import mind take edit completeness showing behavior unchanged,positive
hi found problem properly line syntax completely remainder file fixed commit look file red create new issue new line push along apart branch recent let know,positive
afternoon code side doc later,neutral
complexity could reduced bit removing setting several implementation without efficient quite slow use purpose testing watermark quality,negative
thanks feedback yes indeed higher context width better performance reluctance complexity code opening whole functionality add possibility set context width yes different implementation work form would nice right work prefer leave future see active usage feature,positive
ready go branch cut release though since lot core code want merge right release instead suggest right branch cut finalize merge blocked possibly one ready stress obviously could launch new together following release section release,positive
hello would appreciate review translation thank,neutral
hi helpful thanks super clear copyright assignment hugging face,positive
think fixed wo recommend fix anyone even thinking making patch rather fix think work check really work train gemma overfit training take look interference output apply patch would add new parameter python none also add code python apply saving none list finally spot difference trained model loaded disk base model trained warning also gone model used python model model name name sum sum sum sum sum sum print different name sum name sum zip print parameter differ return false print parameter differ return false return train training print loaded disk finally parameter differ tensor issue fixed true issue still hidden somewhere,negative
provide example see taken example script summarization would equivalent model worked module working,neutral
hi thanks opening issue want consistent form documentation help strictly enforce guarantee compatibility passing readability practicality full coverage place code think type please feel free open happy review,positive
talking following block let keep revise future necessary python raise unavailable please testing environment raise unavailable please testing environment raise unavailable please testing environment,neutral
used function change anything trained model saving still training,neutral
right also add way check done minimal script correct given setup difference generate model forward since anything generation could model output script nearly equal python import model hi model range curr token token index model next token also see much difference print generate cache might lead small precision,positive
awesome thanks tidy could try main think resolve currently failing unrelated test done,positive
read torch notice torch apply causal mask setting could another way use already defined causal mask best knowledge way faster use option,positive
hi thank opening issue correct aware issue let focus discussion,positive
need add rebase main include recent upstream fix,positive
yeah know return output whether could equip pipeline feature remove initial prompt special output functionality already request simple would useful currently need use dirty non portable piece code handle problem result result return result answer question tried chat,positive
hi great work supporting default context length would something would really advocate based fig separate concern execution speed hooked watermark convenient actually efficient approach way would probably circumvent include different implementation salt hash partition table also sure time frame feature,positive
ah understand problem prompt string touch see exact text output tried chat,positive
example result many primary color print result result following many primary color three primary color want automatically get rid request tested mistral,positive
would love feedback default chose use,positive
include output want would able provide concrete example well output want,positive
solution work one combined metric metric weighted currently way somehow define metric used use weighted otherwise use anything even trying something like following work weighted metric accuracy else none,neutral
good review brave one failing unrelated also failing main,positive
hi thanks insight like fork included change kind performance talking want know long take train model lora well model,positive
lot really single constant moving around potential create many breaking something done great care agree careful done replacement break anything still keep test unchanged everything make sure wo break anything might revert code folder convenience reason write test often copy output directly rather think shall put replace assert statement would think think whole new module necessary moment team decide confirm update like say would module consider thing need import rather let importer load entire file may start replace magic put module though high priority task often replace repeated make code maintainable explain ca duplicate thanks,positive
hi thank opening issue absolutely right oversight part fail early possible going open,negative
best u thank fixing,positive
thank amazing addition add something equivalent mamba,positive
nice file mention might want add file visibility thanks testing doc yes submit visibility fix stuff last time reading doc,positive
thank indeed error message stage feel strong make feel strong really availability also current approach precise regarding error message,positive
think explicitly like sure still express thought instead following request,positive
still good need last commit restore leave make final decision however,positive
however intermediate commit suggestion made slight difference conversion,negative
hi solve issue chose write way u save data inside test pip install thanks patience,positive
thanks review removed equivalent test function comment test behavior removed help,positive
added proxy still effect,neutral
hi please reference way testing lora fine tune performance explore lora effective base model generate message base model,negative
passing need done level actually apply like need tried configuration test could reproduce identical let know set python import evaluate import import import trainer metric accuracy negative positive return text accuracy metric return model learning rate training cosine number per batch training many done try save best model step number every iteration report metric trainer trainer train test image,positive
forgot reply time confirming issue still present closed could one reopen issue solution lot sense attempt go likely,negative
hi thanks wo time look week start next week,positive
code little bit make sanity python train sanity check model model train true true true trainer true print different print different print saving different print loaded disk different print loaded disk added sanity base untouched model comparison check model training base yes different number training check trained model merge base yes different saving model check model save base yes different number saving nothing loading model disk check saving yes different something wrong loading model saving warning loading model disk model used model trained another task another architecture model model model expect exactly identical model model model newly probably train model task able use inference check model disk base training worthless like something fishy code saving loading model disk update notice wrong check saved something,negative
stopping criterion solution preserve zero stage support,neutral
hard know without knowing technical suggest try reserve feature bug,negative
setting useless useless value able set useless want add parameter control addition bias done custom model necessary make sure different attention remain equivalent,negative
hi really good made library support new chat template model possible rebase use port make branch add support rebase realize already quite recently sorry done main,positive
also work definitely unable list working feel free open new issue think working,positive
hi really good made library support new chat template model possible rebase use port make branch add support rebase realize already quite recently sorry,positive
thank opening issue reproduction script similar issue open let focus discussion issue,neutral
similar issue open let focus discussion issue,neutral
extra would indeed handy pun intended going chat soon tomorrow gather set share publicly concrete plan factorization generate sure done parallel goal without causing big fact already motion written mind reply within week please hesitate ping want take offer,positive
following sure change would helpful please look require change correct start documentation apply since might break,positive
messy research code jam ask let know could help accelerate would happy help elaborate design plan bit also let know would helpful issue user story leave open prefer,positive
deprecate simply file removed,neutral
got say never knew feature loading file quick search found nothing extra removed get better idea check idea let know ping otherwise feel merge anyway,positive
see nan adapter work fine load via correct call saving lora model via like suppose fix training lora still,positive
good evening chance something came really would like use word thanks support,positive
regarding generate actually go beyond key idea rewrite generate way becomes model level like function let chat figure make plan open issue,neutral
suggest setting alias like lead weird behaviour per contribution suggest making virtual running,negative
hi thank model nightly failing could take look please thank advance line false true line,negative
note core especially lot serialization decided postpone serialization feature setting property false follow serialization since make compatible saving need lot way load save without serialization still good merge,positive
well trying use pip found probably old doc forum probably working,positive
tried install torch version took forever however strange follow official markdown rightly quite fast prepare,positive
still getting error working running solve recent call last cell line epoch trainer starting training model name defined,neutral
could rebase latest main branch thanks,positive
nice thank exactly looking tried install torch version took forever however,positive
pipeline weird output error output seem python import pipeline import torch device else pipe pipeline pipe grounding explain image print image bet said one would grounding explain image phrase two object laying pink blanket sleeping next one cat back laying side reproduce well feel like something around might causing python import pipeline import torch device else pipe pipeline pipe grounding explain image print grounding explain image image two lying pink blanket seemingly sleeping error import pipeline import torch device else pipeline make pastry input provided model wrong number image number image given model correct indexing batch,negative
example issue side need complete code could reproduce issue,positive
hi could try following code see look like able reproduce version question torch torch python import import torch import class self super classifier head else model print got root python parameter tensor tensor,positive
ca tell august th specifically incorrect typically path method back quite long time would say likely handled internally call made without fine removing completely feature must considered removed end user change anything since feature broken anyway feature mean load single file would good check mention logic related feature remove altogether keep dead code main source code,positive
thanks ca see rebase commit history perhaps try done git fetch upstream git merge,positive
hi yes issue thanks support,positive
tested code today assert dev python import import torch none torch try primary method get package version true except fallback method torch dev torch try package package check version dev dev true else false except package ca available false else torch attempt fallback set available false print version print version import version assert,positive
run echo alias source error,neutral
useless point parameter model attention bias weight model architecture mistral,negative
time see error like data type class class sign object class understand result somewhat chaotic transition happen tried update correctly use still possible create depending suggest general solution first try main pip install see problem note version main latest release version included version yet still fix problem try pip install still fix problem try setting environment variable still fix problem check code make sure directly like import import,positive
issue today following voice assistant course summary following system default issue tried mac got working well would nice,positive
meant add version date fake version control date,negative
yes understand example python,neutral
could open new issue help u make sure problem lost properly tracked clear resolved,positive
thanks ca see rebase commit history perhaps try,positive
hi thanks raising issue without knowing hard know happening security wo however accept file would help show code object linked issue happen incompatible old file version used create create new save reload,positive
well might lack context understanding dev use grab version import work dev could grab version unclear listed previous comment work give correct hope make concern bit clear,negative
need run make style make quality fix style,neutral
hi sure script replay import torch import time import math import import start device else assume device cache true eager end list count count sum count print per token latency model print torch print print device print print model model return try model auto except model auto model assert assert device range model map lambda,positive
thanks main could rebase finally merge sorry done,negative
hi thanks raising issue could provide running environment run terminal output information code example run specifically memory,positive
actually new model fix provide context,positive
question would reliably inference always know side padding problem pad know id pad token automatically know padding side correct value pad need know padding side building missing even simpler raised suggestion build unset information warn seem incorrect reasonable going think tackle problem general,positive
hi without context pretty hard know happening example model please make sure follow issue template provide series reproduce well information running environment question question best try reserve feature bug,positive
yep fix main disruption could try,positive
hi thanks raising could share script used generating help u investigate side,positive
working dev import fail,negative
script minimal example appear used obviously call clear model loaded model many custom outside world access ca prepare source library completely unclear bunch code removed access know particular code run access model get many pull per day order u able address timely manner need help u help would lot additional work part get working try infer environment reproduce may script minimal code necessary replicate reproducible code run box without modify understand may able share possible share equivalent public reproduce issue,positive
hi thank opening rework streaming soon look conversation think add streaming,neutral
add support way estimate additional work generate structure well test suite prevent generate currently generate monolith new optionally change return yield without massive many streaming optimization generate function enable yield yet estimate super optimistically flurry new generation model come near future,positive
totally agree say see satisfying way,positive
issue next time please comment issue directly rather opening new issue many different thing make hard track sustainable everyone,positive
hey longer associated able help thanks,positive
hi resolved still experience error install source pip install,neutral
worked push bash recent call last file line module file line return super file line file line file line load file line open return self mode directory thing,positive
comment would also quite cool could save time core diligently test main,positive
assuming run python python command library use python running make outside virtual environment running python error,neutral
one possibility change file rebase fly latest main sure great idea two code version used run version branch lead confusion pretty common development rebase even though branch review ready going end forcing people constantly rebase force push even meaningful added someone notification every commit really rather avoid test rather optional feature similar run command comment ask run final step,positive
would represent patch release reserved fixing unintended breaking minor release minor roughly month next release week two,negative
please tell solve new error batch received empty model wo able train training model ate label,positive
one possibility change file rebase fly latest main could interesting try rebase main roll back conflict run informative message added report run main might save like,positive
python also local environment,neutral
issue library python way develop work python virtual environment python run python python,neutral
thanks much run make fix failing,positive
hi everyone thanks much review final ready final review,positive
odd able run make locally error syntax run python environment version running check python,positive
thanks great work merge branch upstream main merge think done,positive
nice exciting work happy get work,positive
hi thank worried fact dev version might cause problem ecosystem example would happen would fail would give correct,negative
python file line line bool invalid syntax make error,neutral
failing test need running make pushing resolve,neutral
really amazing work thanks much,positive
hi could help review,neutral
problem lied version mismatch accelerate accelerate problem gone,neutral
hi thank flagging situation opening remove history saw fail august however ca see change day would like ping see something time period,negative
added tip static real usage,positive
strange perhaps adapter corrupted nan double check,negative
hi bias applied example used self attention browsing paper long time think indeed attention bias mention initially added bias management forgot update time feel free open fix would useful,positive
yeah see satisfying way neither one possibility change file rebase fly latest main,positive
hey added test code confirmed work well also confirmed model test working well,positive
principle however never issue past clarify bit getting issue odd indeed see make minimum reproducible example end configuration axolotl training python accelerate launch assume model big enough split across trigger error least sporadically see axolotl setup,negative
image default setting mistral setting useless,negative
hi let know need,neutral
hi meet problem resolved thanks version back,positive
wonderful hope version release soon,positive
file yet running python main branch correctly,positive
think come machine program write something,neutral
may also need rebase main test rebase wait till end resolve rebase need force push harder track main probably one pull agree preferred way like need rework check conditionally also need populate going decorator come well yes generally best practice case feel usage quite minor feel affect readability much incorporated follow style attempt consistent also top file tried put note one use file replace logic,positive
hi everyone maybe someone hugging face team could share error fix future thanks,positive
converted unfortunately old solution worked u downgrade work specific universally wonder plan update conversion script also plan make conversion script well class universal work opus let user specify vocabulary file model file file converting,positive
also branch figured main passing added appropriate watch closely main rebase missing,positive
actually better yet could maybe point issue follow keep yield generate,positive
work version get different error worth also see would help import import true jean made durable dark wash denim relaxed fit leg feature classic design straight leg silhouette jean made durable dark wash denim relaxed fit leg feature classic design straight leg silhouette made soft breathable cotton fabric crew neck short graphic print front relaxed fit nightgown made soft lightweight cotton fabric relaxed fit delicate floral print knee swimsuit made fabric flattering design adjustable bra support moderate leg cut swimsuit made fabric flattering design adjustable bra support moderate leg cut hypothesis jean provide comfortable fit room move dark wash denim jean suitable casual soft cotton fabric comfortable everyday wear nightgown shorter provide adequate coverage fabric quickly color adjustable swimsuit difficult adjust stay place data hypothesis axis data data list data data truncation true padding true true true data return data data data data error related input tensor type recent call last cell line get full stack trace call raise none finally self return self output else raise data type type accepted list exception calling layer type data type class class class class class class class class class accepted call received layer type none none none documentation would seem since class able take input made sure however error type input accepted,positive
hi probably see added mail thread pull request hub,neutral
may also need rebase main test,positive
information dev code running time hope help identify issue make llama run fast thanks lot,positive
final review watch week,neutral
looking still failing guess,neutral
hi meet problem resolved thanks,positive
thanks feature trying following could figure feature couple use known feature work model anyone tried available hub suppose assumption must assistant main model model python import assistant model assistant bob tried work also add model avoid none type assistant model vocabulary ended object type error looking candidate generation,positive
ah sorry found several discussion search thank much link well hint aware,negative
related sure different long short audio around combining,positive
hi code import o import torch import import import import import import import import import import ignore main seed print step build data split task step build model model step range batch enumerate output batch output step path prediction path print accuracy print generation path print meteor main import import torch import import import class self super llama gemma print auto revision main revision model print frozen name param false else print training model model float none model model model property device self return list self use use provided otherwise use return else return forward self encode description question label encode special range add token pad range return inference self encode description question encode special range add token pad range important return self param return running time code running time,positive
based code merge currently failing test need resolved first could try main make sure latest trigger fresh run,positive
hi thanks opening issue please provide minimal code snippet used well exact running time compare two,positive
following assume new randomly yes fine tune somehow work special case yes want need freeze case add special resize yes necessarily tied model might model model conditional vocabulary also linked function pas special added property special added documentation information think missing,positive
hi without access either hard know make work memory error try look way reduce reducing argument method used old great knowledge supposed used legacy squad question like squad show general approach recommend looking first general like best try reserve feature bug,positive
great reach model page hub like add library happy leave,positive
thanks already create quite explicitly think fine create correct none correctly infer question would reliably inference always know side padding understood correctly also possible case wrong case think raise warning overwrite user let know reasonable something might,positive
willing contribute want work,positive
right let correct comment first none mention change contrastive search different use contrastive search used stochastic contrastive search fact pas contrastive search call see warning import model quick brown print see something like terminal set false however set flag used generation set unset setting generation quick brownie great way nevertheless modify place resulting incorrect behavior describe going open,positive
yep basically load file convert format library run model call output new model,positive
provide minimal reproducer please setting could recreate issue minimal example python import evaluate import import torch import import trainer metric accuracy label negative positive load print return text print create efficient collator dynamically handle computation metric print loading metric metric accuracy metric return print model model define print training learning rate training number per batch training many done regularization penalization create trainer passing model train data method metric print trainer trainer trainer train test initiate training print training,positive
complicated could try give shot new outside contributor admittedly new stuff high level would entail conversion similar something like found model forward backwards pas,negative
testing enough default need test see long time step mark extremely slow,negative
hi thanks opening associated feature request could add description,positive
copied across could open fill model card,neutral
appear issue added ago fix bunch inconsistent behaviour used make sure appropriate set find example,positive
hi thanks opening issue check git blame find add certain normally give code logic case line added according preparation upcoming torch release necessary u explicitly pas value set true based current default behaviour comment,positive
understand like add token another token imply recognize like example control option python import text text work feel free open new issue old work python import text text thanks open issue tried another one got behavior,positive
wonder sense put comment still know call exist,neutral
yeah need either update saved testing update thanks,positive
like mismatch static real python batch removing model call issue exception raised,positive
pip install still object attribute,neutral
seem like issue expect likelihood threshold log space perhaps even nan given currently raw returned represent following application propose attribute probably rather field actually correspond raw already available another attribute concreteness relevant documentation optional returned prediction language modeling head vocabulary token generation step one element token tensor shape optional returned unprocessed prediction language modeling head vocabulary token generation step one element token tensor shape absolutely think impact content attribute feel behavior strongly encourage least clarify documentation expect find special like also consider generating like contrastive attribute wo actually represent prediction documentation,positive
hi thanks raising issue aware project could link well providing minimal code snippet issue project address deployment installation document address step step according source code,positive
awesome save would like push model back possible rest code look,positive
hey sorry reproduce issue incompatible python hi inside docker well error,negative
yet part version release recent fix need install source pip install,neutral
hi thanks raising issue quite attention mask creation recent relatively old still hold recent release main recent stable pip install main pip install,positive
order save since building top branch work,positive
thank share get time implement one,neutral
hi thanks opening feature request normally conversion model folder would think conversion script like good idea model moment back soon let reply case good reason add alongside model originally,positive
hi error trigger python module related library,neutral
wonder optimization general surely free lunch use reduced requirement gradient general everyone would might helpful understanding whether optimization whether optimization fundamentally incompatible zero,positive
hi thanks raising issue model model page working open discussion model page currently model implementation compatible library either hub best knowledge would like added open new model request suggest opening discussion model page hub might interested,positive
hi implementation read bit source code galore think galore automatically fall back classic one target get galore whereas get understanding correct yes think correct additional check without galore,positive
good point case need add warning two make sure catch new model old state loaded,positive
hi implementation read bit source code galore think galore automatically fall back classic one target get galore whereas get understanding correct,positive
yes good left one comment nit point think basically ready green final core maintainer review,positive
update step like could applied reduce memory usage training right cost speed perhaps,positive
accepted last suggestion fix naming code work least locally passing generation,negative
wanting test turning individual feature sort combination fast might need define way combining test hi make concrete class feature complete demonstrate global structure going,positive
question merge come gemma llama received static cache treatment static cache transition foresee case original support generate side propagate pattern across library,positive
hi wrote aim avoid public private aware advanced training inference rely manipulation class private library however hard add new without breaking private underlying cause one could new class handle would maintain class faced choice break undocumented library add powerful feature focus stability tend prefer former may associated growth pain believe agree beneficial everyone long run apologize friction side promise keep undocumented level minimum please open u something end fully aware law,positive
like lot failing moment lack attribute,neutral
since working test fetcher case new arise branch new test sync main respect new way prevent without much effort seeing way thing come mind ask rebase type,positive
hi thank work failing line object attribute line tensor false true single row equal take look would great thank,positive
sorry extremely response good update documentation,positive
getting error return got unexpected argument latest version,positive
update longer seem work batch work fine,positive
thanks put doc page,positive
nice work note approach may support currently please see guess may torch operator multiple optimization step,positive
thanks everyone look implementation super elegant gave inspiration add galore recent able tune single taking loss go quite smoothly bash script used python import torch import import import cosine model trainer ready review,positive
hi could open new issue u keep better track resolved original issue post feature request whereas appear possible bug regression issue make sure include minimal code snippet reproduce issue specifically information issue case running time two,positive
fact flag order save model besides however find set false continue training strong reason hard torch class value need change string although care attribute ca remove set none none none none spec key else key spec else version saved model,negative
hello thank currently security policy indeed need link end work make sure add thank,positive
besides statistic broad community,positive
run llama mistral inference run upgrade version slow please check,negative
able resolve issue following firstly added special used prompt card line code problem took could solve solution successfully problem thanks,positive
used several repository hub could let u know repository used please thank advance currently kind integration run model added,positive
hi script right support saving already need first move use either adapter save adapter,positive
think point able flag special skip problem raising like output result variable order process afterwards like pipeline working,positive
possible integrate weight galore paper image reduce memory usage significantly image original implementation somewhat complex sure weight update crucial current implementation galore may future without would require additional gram model making infeasible may refer discussion empirical,positive
similar use case want load huge efficiently following guide first empty model state empty model understand add like process support simply possible kind else would one efficiently quickly load model bit,positive
got sense thank maybe documentation accelerate would help flag difference think negligible peak memory change model added accelerate addition accelerate decided example instead already another,neutral
thanks getting closer also need squash commit history harder u track got yes mostly habit bloat however rebase failing due daily latest failing shall refrain,positive
pull request branch sure resolved never mind maybe,positive
pull request branch sure,positive
happy one question add correct would see warning loading model ask ran issue training model several day later loading would nice see warning training could rename spot think warning like would feasible fix manually rename saved rename model,positive
tried getting different error full pretty long pasted error thrown let know need full post strange busy utilization power limit training following error seem help tried throw error python file self stage assert stage loading model like python import import import loading lora directory model model model resume training path model model training python trainer trainer train everything work fine train lora scratch model model,positive
nice looking forward change crossed able yield output addition motivation chance could estimate eta yield generate change,positive
hi accepted left comment regarding question,neutral
thanks call squad training validation data call training validation data get error machine usable core operating system processor want use model train domain specific training validation data think model trained combination squad training something approach anyway get work thank,positive
hi thank taking time review run make believe previous regarding consistency please let know thank,negative
hi thank opening complex streaming moment ongoing design far added generate nutshell going add option yield stuff generate,negative
hi yes paper also tried dola got similar performance boost recently tried also worked similarly image thanks lot review submit soon fire,positive
thanks based like memory error suggest looking see whether suggest trying run one see whether successful,positive
hi theory agree issue indeed contain degeneration penalty however return selected iteration practice feasible return due compute cost regarding part issue moving main body board open accept,positive
hi interesting idea indeed nice see get performance boost strategy technique fork good looking forward fire,positive
thank opening issue linked fix,neutral
try load version way go base miss based error missing positional argument thing attempt python anyone else get effect setting believe,negative
hi thanks feature request currently indeed box also really familiar owl library would like sketch draft mind order support training also elaborate note update parameter list pruning structured,positive
hi need call whenever want lora double check,neutral
think use latest accelerate see,positive
stay tuned landing soon,neutral
work yes sure failing,positive
hey would like work still looking forward,neutral
output conversion memory error suggestion getting training validate getting squad train getting train getting squad valid getting valid converting squad train convert squad exception error sending result object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object object reason,neutral
hi port gemma modeling code let know need anything else port want try prompt used translation bot designed translate code hugging face library file library goal output equivalent code want think carefully start write top output file also add code indicate uncertainty please preface easily find follow code network method please pas attribute name name class instead inherit retain attached like forward translate even method call layer model class accept pas also start name class class module please add start name require input shape way result first argument constructor like dense usually removed argument please remove argument constructor prefer function list like tensor get import method instead replace best solution usually compute value call totally constant never forward method store method instead output class like added start name always require must use instead raw layer layer build method call usually passing attribute name name however method like make sure retain like code try translate unsure leave untranslated mark comment later remove code use want exact replacement torch function get one via import port lot cleaner main see actually library merge might copy manually still want feel like definitely somewhere prompt thorough love,positive
tried pip install quality make sure need yep specifically ran pip install quality guessing thing pip install quality let know wrong issue see tag,neutral
thanks quick reply go ahead close issue,positive
please take look let u know issue facing,neutral
fixed documentation test failing official yet,positive
make work use time tried pip install quality make sure need still fix open issue full error trace get try investigate,positive
look tried following contribution guide letter work quite right example make index range also command run automatically collected python cat impacted work either create separate issue provide thanks work,positive
hello idea load best model example python loss save like best model thanks advance,positive
thanks output running yes name main logic used script launch removed call script also put block see code caught first call code outside block version platform python version version version accelerate version accelerate found version true version false flax version version version script fill distributed parallel script fill code try attempt set start method except start method already set ignore error pas try print getting squad train path print converting squad train print getting train path print converting train print getting squad valid path print converting squad valid print getting valid path print converting valid except exception print exception print print conversion successful try,positive
hi root issue model two separate need set pipeline model inherit set opening fix,neutral
unfortunately special way know way tell inspect jinja script see used really something automatically,positive
hi like tagged special since passing assuming case confirm whether case check whether output without streaming still may able chat way decode text way filter role special token,positive
hi fix use main pip install encounter please feel free comment reopen issue,positive
also may even though available torch want make big update match torch definitely interested maybe possible find script,positive
ready simplified removing deprecation warning sure want move quickly anyway result result behaviour new functionality,positive
merge good since made tested locally,positive
thanks raising issue check next page train panel see graph metric still present although additional logged without correct prefix handling need investigation investigate update soon,positive
interesting case quite large base maximum sequence length model size due causal mask although best without memory footprint see measure time separately generate forward see additional time generate additional time forward go level profile forward main conclude following run new update start forward take significant time slow inference many subsequent kernel bottleneck negatively impact eager forward pas use main compute scratch efficient code path slowdown reason computation also sure slowdown summary script profile forward run terminal run import import torch import time import profile model auto model profile range start model end measuring latency worse correct bite bullet accept eager mode idea difference see attention mask argument clue dive find cause,positive
thanks duplicate let close issue continue discussion related could elaborate issue second said,positive
see think whisper got confused thought model used exclusively,negative
thank review review let know good,positive
hi thanks raising issue indeed desired behaviour would know require exception could fix think would hard many saved hub well custom might rely behaviour would worth warning method class let know string gamma beta encourage change least block code still yes think warning cycle best way go would put function trigger gamma beta key wo possible tell parameter old state new model warn happening behaviour removed future release update state use weight bias loaded properly would like open add way get contribution solution,positive
hi could take look well thank much,positive
hi could help review opinion technical,neutral
thanks first review cache logic week,positive
help could please provide running environment run terminal output minimal code snippet reproduce error,negative
thanks opening see model related failing ping review passing,positive
hi left single point discus,negative
thanks clarification yes also save locally load model distinguish model looking attribute model false,negative
could tell solve problem,neutral
busy try let know forked library following came problem getting error message command pip install dev error error following require different python version error could find version requirement extra dev dev none error matching distribution found extra dev,positive
see open separate issue thank,neutral
found possible cause use model convert model model former cause error latter work well latter one cause another error error illegal memory access kernel might call might incorrect consider passing compile enable,negative
thanks message considered decided smaller like saw turning flag made little difference peak memory consumption probably case activation dominated thus use case dependent good idea set true default wouldnt good user improve throughput probably user wouldnt aware turned training time could actually faster impact turned got sense thank maybe documentation accelerate would help flag difference think negligible peak memory change model added accelerate,positive
hello thank accelerate reducing memory usage forcing gradient synchronization step overall comment change default accelerate enable model way user would pas extra argument thanks message considered decided smaller like saw turning flag made little difference peak memory consumption probably case activation dominated thus use case dependent good idea set true default wouldnt good user improve throughput probably user wouldnt aware turned training time could actually faster impact turned,positive
st let know week thank know wait return,neutral
would possible resolve review suggestion get near future valuable contribution library eagerly many want use outside pure python ecosystem,positive
oh model bit saved model save function compatible,neutral
output none seem harmless however use recent call last file line module file line index file line token index file line return self file line raise id range piece id range,neutral
many different trying bit helpful without type quantization defined based oh format model need use format library instead directly example use would use model like python import import flora helpful ai assistant load model model streamer convert prompt system user prompt assistant prompt standing surface earth walk one mile south one mile west one mile north end exactly generate output,positive
possible integrate weight galore paper image reduce memory usage significantly image original implementation somewhat complex,positive
hi thanks opening recently trying push model hub much support also easier integrate tutorial sound good hi thanks already model custom model code hub one model case like integrate refined code easier loading,positive
thank working issue push save somewhere training load back interface unfortunately solution work case want adapter base model locally pipeline without needing interaction hub avoid flakiness heavy network let ask way casting model without pushing hub also loading model distinguish model guess work,negative
hi transformer post indeed excellent iconic resource would like open include,positive
good need generation update string saved need save first pas right,positive
hi wo need fix case get warning fix might interest speak training private project fast variant worked fine,positive
hi thanks opening issue without loaded able replicate confirm behaviour however adapt script equivalent import import torch import import import train text photo skin cancer device range next iter image model device processor image image processor processor device device device device device device model model similarity score similarity score print,positive
oh wow typo getting trace threw python cast floating point avoid casting check floating point cast send device device none argument position must tensor list thanks pointing,positive
good sorry responsive late yes plan good,positive
hi thanks issue something way clue start error message particularly helpful thanks issue resolved,positive
hi thanks raising issue wound converting tensor last step could clarify code example like image processor sent device line processor rather converting passing image processor moving device correct way note also typo example rather,positive
sure mean project yes whatever trying import date removed ago,positive
hi thanks opening recently trying push model hub much support also easier integrate tutorial sound good,positive
able get around implementation import prediction pipe prediction chunk start end chunk start start end end text chunk text,positive
fixed except two failing main also related current,positive
hi thanks raising issue actually know copyright going ping note based raised correct wrong yes licensed apache special certain different license example different license reflected modeling file blip text pretty sure hugging face team people currently working hugging face add wrote code copyright contribution git history sure much reserved license legally understanding header essentially reserved apart explicitly stated license perhaps clarify listed people former hugging face significantly building think sense add people added commit reflect contribution design library,positive
thanks sense resize following assume new randomly fine tune somehow work special case case add special resize would make sense inform user need apply method believe would quite useful also linked function pas sense provide developer guide easier access,positive
hi get contact potential individual would charge add mail thread give address regarding either copy initially run script provided module,neutral
thanks solution sense however keep separate consistent across use without code see interest community open push code,positive
hi preliminary fix please try let know install code fixed branch pip install,positive
delete may different version delete anyway folder go folder launch click path bar upper side explorer window type click enter reinstall command line pip install version deprecation message longer displayed,neutral
gentle ping merge additional feedback,positive
great give final review store reach magic leap ask want take control use share link model copy across update,positive
bad solution good guess close post something original issue,positive
also note brand new model lead model random adapter attached therefore likely get gibberish output think also bad intent well typical train push save somewhere training load back interface whatever think,negative
hi two first indeed throwing used work fix may also wish add invocation example include,positive
permanent buffer model casting import import torch model model print llama gemma problem since recently code cast float applied get sin co however rope like mistral yet receive treatment gladly take fix touching rope soon anyways migrate structure contrarily compatible,positive
would suggest model code hub difference discoverability usability integration system best way make code available well method encourage bar add library high model added maintenance burden find often open long time particularly contributor first model would suggest modeling code hub first see lot interest usage community integrate,positive
hi case one run mamba training within environment mamba wondering whether wait fixed install last commit main thanks,positive
descriptive name would nice need looking failing see header context internal,positive
hi thanks raising issue please make sure follow issue template share running environment run terminal output well relevant information error full clear code whether name main logic used script launch first suggestion would try run code without might two,positive
hi currently issue associated code also support lot use also associated support everything one file done currently,neutral
hi thanks opening feature request general try keep semantic backwards compatibility big theme library one aim keep mind feature true occasionally case linked believe part cache system relatively new quickly let comment sometimes knowingly break try flag release title sometimes accidental community raising u find also greater emphasis continuity lack breaking public level importable top level import library large order able fix adapt quickly guarantee level maintenance class documentation,positive
thanks feature request could provide context like feature added principle done param false care need taken make sure properly backwards compatible copy mistral attention also need make sure flash attention remain equivalent eager implementation,positive
looking also inconsistency around scope image lot code slight example use annotation augmentation extremely difficult canonically ideally want run batch mask directly disk conditional deformable guess sam use consistent interface passing probably add list update family use consistent par recent recent without modification would make sense single place subclass necessary tedious job would suggest making derived class something like image class sure sense model completely lot explicitly copied also sense get copied around kept consistent edit matcher mistake paper loss two different matching final loss computation cost matrix ca,positive
hi thanks lot opening ecosystem associated issue feature new model request linked skimming first thing note one file modeling file recently trying push model hub much support also easier integrate tutorial sound good,positive
hi previous code picture add picture better illustrate ask left right think right tag least first version without transformer link rest code file seem match image main ask code following removed gentle introduction check transformer want make sure link somewhere else hugging face although link want make sure find resource reason tell hugging face becoming open ai meant beginning resource make ai accessible humanity original attention need paper still linked see link original transformer current main transformer link attention need paper see link transformer absolute beginner think much better resource anyone wanting practice ai python think much better primer original academic paper alone,positive
hi thank much input address specifically batch thank,positive
finished review port really good one comment also note rebase main test finally fixed,positive
another reason tried install prior version model recent call last cell line model key key key module attribute project old,positive
think figured sort original making sure index going incremental add like thanks token token token sorted item item print u additional making sure incremental,positive
core maintainer review case,neutral
thanks investigate bit get time,positive
far remember falcon quantization middle process gemma remember guess dropping right start without progress bar,positive
hey case quantization progress bar,neutral
thanks quick fix tested confirmed error message longer printed,positive
hi branch fix issue try pip install upgrade let know example sent working find ping core maintainer review merge soon,neutral
thank pointing utility script added missing accordingly,negative
yes pip install eta requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied collected successfully pip show name version summary machine learning author hugging face team past future help license apache license location,positive
hi thanks raising issue aware project could link well providing minimal code snippet issue,positive
st let know week,neutral
tried recent version try recent,neutral
true well pip install upgrade verbose,positive
pip install call version fork installation raise,neutral
think best suggest linking relevant example code another great place would,positive
yes agreed oversight definitely worth fixing note class already argument need ensure correctly,positive
trying run pip install verbose,neutral
add necessary resize model method,neutral
removed ago use instead,neutral
hi thanks opening issue added page added better address german manually,positive
hey could send full command expert number supposed faster number,positive
team aware working towards fixing,positive
extend also contain optional one caveat different parametric distribution different bin also per time step,neutral
example token never still possible find someone go history conversation suggest hub token security still minimal example contain information script command make sure compatible save method rather,positive
right think useful would return output param distribution time step way one reconstruct distribution generate course internally need generation feed next time point think would easiest would return param fine,positive
hi thanks raising reason trying install almost old advise trying use recent version pip install note version want install would front version would include patch release,positive
believe anyone currently working review first ready review first rather issue closed inactivity anyone community free pick,positive
faced issue memory falcon gemma setting parameter helping since different far understand tried build optimum source small fix quantization help well common problem right could issue setup side,positive
found implementation intended close,neutral
preferable rebase onto main see make green need click look output error suggest see coming branch,negative
thanks working merge see added watch week,positive
could take look comment feedback greatly,positive
hi like good idea want similar possible feel free open tag review,positive
hi thanks raising issue three canonical identifier case advisable use instead change parameter model pas directly model call ensure correct loaded default may match way avoid unexpected loading happening default size trying load old different shape need set following work run main import,positive
would suggest searching used case also vision example though,neutral
rationale python code interpreter scratch agent chain anything else might ended security risk limited design take look thanks,positive
like specifically ago source vocabulary change went unnoticed use fix,negative
hey page hugging face link work fine work,positive
hi first problem resolved new one could open new issue u better track resolved,positive
thanks quick fix everyone,positive
python parent basically need iterate first python epoch parent,positive
hi library designed way class define base transformer without head top class define base transformer linear head top latter accept argument since compute loss classifier ground truth hence accept model initialize randomly,negative
make sure first convert model implementation saving loaded saved already format,positive
ran cluster run shell recent call last file line return access data pointer invalid python storage handling exception another exception recent call last file line module file line shard format file line file line file line meta file line return file line return file line return access data pointer invalid python storage like original error certain cause error resolved hi thanks flagging error initial glance look like problem save adapter saved would used trainer change would save saved simple script demonstrate run bash export python might happen error unrelated fix,positive
hi test script derived pipeline python import pipeline import import audio import torch train audio audio example pipeline output example audio array print output change instance support change output geld mein change output geld mein instance support change output input type float bias type change output geld mein instance support change output geld mein change output geld mein instance support change output input type float bias type change output geld mein obtain full input type float bias type recent call last file line module output example audio array file line return super file line return next file line item next file line next file line forward file line file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward return input file line return input weight bias input type float bias type instance architecture order little list thread per core core per socket socket node vendor id family model model name gold stepping min cache li cache cache cache node node de pat art aes instance architecture order little list thread per core core per socket socket node vendor id family model model name platinum stepping min cache li cache cache cache node node de pat art monitor aes la serialize,positive
got running taking python outside default outside,neutral
soft reminder help review,positive
hi met another question mamba implementation compatible train get error follow file line train file line file line wrapper return file line model file line file line loss file line backward return file line step loss file line backward file line step group group file line file line step file line file line wrapper file line return file line step group group,neutral
hi reason break previously would rely casting make work use simple single call get therefore aware internals cause sudden unexpected change behaviour trying solve problem else yet understand desire use use case case would suggest modeling code directly instead pipeline becomes widely becomes commonly feature reconsider think hi thanks clarification understand concern tested break anything test model still think acceptable follow suggestion cast input type pipeline see share tested ideally reproducible snippet suppose talk code much better could share test people discus concretely use format like test case test test,positive
hi last week reading medium page linked older version today took almost hour find following sentence removed gentle introduction check transformer remember transformer link another page hugging face computer scientist skimmed past week promising link begin diving understand page best fit entirely removed hugging face think good resource add back somewhere,positive
correct way use unused know individual unused tradition use slow always fast one since llama already indeed example work slow one python import display unused unused,negative
assumed pretty annoying bug find would worth warning method class let know string gamma beta encourage change least block code still complicated fact accelerate incompatibility two highly coupled,negative
thanks working happen rebase force push moment branch commit history lot unrelated main like without force pushing necessary effectively branch history hi thanks clarification dev branch git push force still see unrelated although seem already upstream main create separate recent thanks help,positive
thanks working happen rebase force push moment branch commit history lot unrelated main like without force pushing necessary effectively branch history hi thanks clarification dev branch git push force still see unrelated although seem already upstream main create separate recent,positive
reproducer please let know accelerate otherwise error device found least two argument argument weight method,negative
fix probably ideal current way try except start conversation working solution,positive
would like fix bug file need require,neutral
got error training step dont get error error loss completely different eager attention still,neutral
would also allow one run model large around copied sometimes context length one running model,positive
yes correct bug pointed video series due assume backwards compatibility would know require exception could fix,negative
anyone working like activity branch recently,neutral
import want import still got error import name something wrong,negative
hi indefinitely find time work task would really appreciate could help add test thanks,positive
question expect support sooner,neutral
correct way use unused know individual python import unused unused,neutral
problem try research work need forward recently may later,neutral
perspective problem someone model need utilize triton vanilla torch implementation significant memory footprint implementation several theoretically possible include optional dependency similar additionally still option use model inference without,positive
hey diving accelerate source doc came mind could found test user use sure break anything see set apex need handle differently case used apex apex like status draft happening note need read trace anyone familiar source code simply way get understanding situation various among involved trainer object method initialize attribute accelerator object accelerator class singleton singleton set retrieve method still accelerator method used know set true used know need use mixed precision method call method set true calling setting model mixed precision warning trace training model obviously data,positive
certain think always calculated float gemma llama device downcast matrix explicit like found ran code confirm would great print run confirm actually,positive
got error training step available kernel execution,positive
ah see issue throw error create model working give try,neutral
solution bug fixed consequently lot fail account expect output get instead issue might related class setting true default method neither explicitly defined return batch axis previously none set true python batch axis viable implement update continue explore issue would grateful insight,positive
hi ran script work end main double check shell model dropout linear linear linear dense linear linear linear dropout linear tried error support attention implementation yet please request support architecture believe error bug please open issue repository load model argument eager meanwhile example model eager dev,positive
team provided generation code,neutral
get similar warning please use ode instead please use ode version platform python version version version accelerate version accelerate found version true release fix noise upgrade check warning longer displayed,positive
one another solution get via accelerator model however approach every parameter resulting save real,positive
issue issue logic discovered zero stage leading storage method though actually subsequently removed removed tensor log message consequently resulting small portion removed thus avoid issue,negative
one want review go green hi could go green shall merge rebase,negative
ran cluster run bash recent call last file line return access data pointer invalid python storage handling exception another exception recent call last file line module file line shard format file line file line file line meta file line return file line return file line return access data pointer invalid python storage like original error certain cause error resolved,positive
thank explanation however still suppose layer st layer expert layer expert quite balanced solution solution formula total loss solution actually total loss therefore equivalent original loss total loss,positive
without model ca run code public could use replicate issue code example also show run class trigger error note code alone like lot library might case model generate sorry model following code saved import o import import import time import import copy import random import import import torch import import import import import import import import import seed seed seed seed seed main logger model else false prompt prompt result result model pruning assert param true sum building dependency graph input fine since computation result taken consideration imp imp imp imp else raise use pruner importance imp layer none range range pruning attention layer list range pruning layer list range pruner model start pruning range start iterative range loss model loss loss else loss model loss loss sum iter modify layer clean gradient model name module name none pruner importance imp remove layer pruner model start pruning range start iterative loss model loss loss sum iter clean gradient model name module name none modify pruner sum else raise param param ratio model prompt prompt result result model pruning memory requirement import parser llama version argument model name path save log final path would argument ratio list type argument generation temperature sequence length argument pruning wise wise wise layer previous layer block attention layer block attention layer block layer block iteration step pruning method grouping global pruning general argument device test train device test train seed save model float main,positive
think uncovered issue contrastive implementation currently raw think warped logic function currently set streaming match providing,negative
please note kind backwards like self token token id return self index index integer token return index,positive
like lot guessing sort thing since difficult identify substantive also likely cause merge concretely curious see made work single commit substantive file needle haystack,negative
hi facing error could please help fine tuning code import trainer initialize trainer trainer trainer train model save model error recent call last cell line initialize trainer trainer trainer self accelerator got unexpected argument,positive
hi great work mamba gon na soon think last one pending use mamba completely right also commit would recommend install mamba main branch instead release thank much,positive
correct lend setting typically causally masked since operation different time insist need configure default,negative
hi work class however could fork library make necessary use get hidden shape could reshape like python,negative
hi reason break previously would rely casting make work use simple single call get therefore aware internals cause sudden unexpected change behaviour trying solve problem else yet understand desire use use case case would suggest modeling code directly instead pipeline becomes widely becomes commonly feature reconsider think hi thanks clarification understand concern tested break anything test model still think acceptable follow suggestion cast input type pipeline see share tested ideally reproducible snippet,positive
yes understand transformer model perform casting assumption assume model wo handle need cast input example wo see library apart one erroneous case blip text responsibility handling user rather within model,negative
raw pipeline size possible pas ca different sized different loss one thing note known difference different batch size use batch size,negative
hi previous comment know library enable evaluate model used without merge model suggest running model significantly well consider library,positive
need another review approve last one long time many ago done final review ping review,positive
actually forgot already done core maintainer pas good merge rebase fix last unrelated test failure,positive
yeah end fixed soon point good merge think almost done though try final review today get review core maintainer good merge,positive
currently training finished share place else better share,positive
maybe batch size learning rate,neutral
yes understand dev version running pip list torch,neutral
hi thanks digging behaviour training think something want add side reason convention normally loaded change value change shape new weight would change model loading behaviour library useful however please feel free share code link example community,positive
hey like still issue unrelated,neutral
gentle ping could also share torch version listed environment,positive
ow change input size model got error input image size match model,neutral
thanks patience fixed main use pip install next release,positive
look however still fail failing,negative
without model ca run code public could use replicate issue code example also show run class trigger error note code alone like lot library might case model generate,neutral
yea also quite confused seen place operation comment loop tried initial concretely cause issue similar effect turning cache first glance first suggestion work loop memory even understand mamba correctly memory slow variant time later today weekend submit additionally suggest initial warning additionally advise cache,negative
hold still need core approval even important make sure,positive
aware issue waiting investigate lead,positive
hi good morning added test function please let know anything thank much,positive
hi sliding window logic mechanism differently depending attention layer eager vanilla attention attention mask sliding window flash attention sliding window applied forward pas attention layer main regarding rolling buffer cache mechanism see correctness model sliding window older two redundant rolling buffer cache however extremely beneficial memory working although currently llama gemma support search static cache please correct wrote correct,positive
need rebase main ready assuming ready since merge since front let u know like,positive
problem start working right away,positive
run side fail running local get main thanks working,negative
awesome let close fixed,positive
ready review essentially allow loading saved simple test ca loaded explore another,positive
hey indeed might minor typo message work,negative
expect resolve issue within week create take create,neutral
reference attribute work based rather first get priority getting stale without,negative
thanks could try reduce amount code necessary share whole project fact lot code like necessary issue need several different model context get many pull per day address timely manner need help u help ca easily replicate minimal code snippet wo simply time try infer code sorry following main part get model function load execute function run get performance class bool false officially need specific class optionally use prompt template however initial indicate template useful load self run self prompt template instruction write response appropriately instruction instruction response text else text prompt text print length return length self text float float text float float return,positive
thanks could try reduce amount code necessary share whole project fact lot code like necessary issue need several different model context get many pull per day address timely manner need help u help ca easily replicate minimal code snippet wo simply time try infer code,positive
hi take oh sorry little bit busy recently still got fixed even though plan create weekend yes take wish,negative
problem case love help first contribution appreciate help,positive
reproducer please provide code run reproduce text markdown code easily read u involve generate sorry project large give file following code main part import import list import fire import fire import import modeling import class input target self bool true prompt prompt prompt return prompt class list path list return path path split test data path raw raw data path split return data prompt range prompt return prompt evaluate model data range get prompt make sure prompt prompt prompt label prompt label print return main model print name data result evaluate model data result print result score sum score print return score fire code import import signal import time import path import optional import import import import torch import import fire import fire import import import import pipeline import import import import quant class run self prompt raise self text raise self text bool return text load self raise class engine bool false optional temperature float load self none open key engine azure run self prompt output response output try key engine else model key response role user content prompt degree randomness model output raise output except exception print output output print request return output self text return text self prompt handler signum frame raise exception handler range try time try response role user content prompt return except exception content management policy break else return class model optional optional device bool false load self none auto none run self prompt prompt return self text return text self text float float text float float return class load self none auto none run self prompt prompt type used falcon model avoid pad token warning length return length self text float float text float float return class bool false officially need specific class optionally use prompt template however initial indicate template useful load self run self prompt template instruction write response appropriately instruction instruction response text else text prompt text print length return length self text float float text float float return module type module return name module name child child name name else name return noop assert none assert none model model noop noop noop false model model model name name name model print loading model import else model model model model model print done return model class model optional optional load self false false path none none self detect beginning text test sentence length text class load self none none run self prompt response history prompt return response class model optional self path path path return path load self none model model run self prompt occurrence state none token none bob prompt special format lower performance range else token state state occurrence occurrence token token break exit token occurrence token occurrence token token occurrence else print string valid end break exit return self text return text none raise choose list return prompt write alpaca flan model print print prompt fire,negative
thanks working happen rebase force push moment branch commit history lot unrelated main like without force pushing necessary effectively branch history,positive
hi unfortunately right time implement vision would take time understand work implement ensure change behavior model would appreciate help possible,positive
hi thanks raising please provide minimal code reproducer run replicate error,positive
update would great included library,positive
reproducer please provide code run reproduce text markdown code easily read u involve generate,positive
hi provide minimal snippet reproduce error first thing suggest running version latest pip install following error code image update version new error file line fire component context name file line component file line component file line main score file line main result evaluate model data file line evaluate prompt file line run file line return file line generate object attribute,positive
yep issue binary constantly get working,neutral
think python interpreter getting confused package straight computer package virtual environment see different either one folder virtual environment,negative
hi provide minimal snippet reproduce error first thing suggest running version latest pip install,positive
like issue either environment package unable replicate either successfully run following import import therefore really anything side,positive
image getting inside actual python,neutral
thanks handling small nit done,negative
used development latest version library still working either,positive
hi thanks raising issue quite old version suggest latest stable release pip install,positive
hi thanks much patch used instead hope fine yes remove skip auto work,positive
thanks opening improving going close,positive
feel free merge whenever ca,positive
hi thanks opening auto necessary test beam search work model split across work add skip return work,positive
hi thanks reply sure understand correctly fix behavior use case actually feature attribution much le intensive full training run,positive
hi preview based cache,neutral
pity run model section written model snippet explaining use must meta,negative
thank reply indeed saw accelerate rely simply removing wo work pleasure work address issue free time weekend work though ca provide specific eta moment keep progress,positive
problem strange merge branch upstream main see main branch broken gone cool combined origin master thank hint guess ready merge please check,positive
slow would someone already kind enough run currently think reasonably current connection sure whether relevant,positive
class used hugging face library interface various natural language class specifically designed conditional text generation variant generative transformer model text generation model conditioned input instance provide prompt model generate text conditioned prompt general class used various related natural language generation classification specific conditional text generation used like text classification token classification primary difference intended use former specialized conditional text generation latter regarding usage function model directly related whisper load model load model converting input text load model load model converting text typically based like transformer similar specialized specifically speech operate differently like whisper converting audio text,positive
thanks review big say still either new forward always add new architecture properly mind file input different normal feel like whole new architecture bit change shape instead height width also still part paper extend work arbitrary image could clarify mean new forward since model single forward method defined assume mean whole new class well coming code wrote idea happening need iterate need need separate add explain happening,positive
every single input pas model indeed issue rolling likelihood added transparently,negative
hey think model tracing mark feature request thanks apply see test,positive
hello exact need get original would mind reshare notebook please file longer link much appreciate thanks,positive
already think close issue thanks raising issue fix issue thank,positive
correct understanding delay close please hesitate ping delay response thee lot sometimes slip yes right understand thank,positive
corresponding argument perfect thank question,positive
main rerun failing machine main like related test session platform python hypothesis profile collected self true self epoch result result greater equal summary unknown option unknown option key short test summary greater equal warning,positive
update kind right far explicitly saved let discus back nice need explicit explicit convinced way go could de matter simple whatever know best use definition concrete parent class abstract kind sense meant concrete run method kind special abstract class role dispatch different end end currently configuration saved file see provided previous still doubt let talk discus detail,positive
hi reason break previously would rely casting make work use simple single call get therefore aware internals cause sudden unexpected change behaviour trying solve problem else yet understand desire use use case case would suggest modeling code directly instead pipeline becomes widely becomes commonly feature reconsider think hi thanks clarification understand concern tested break anything test model still think acceptable follow suggestion cast input type pipeline see hi enable casting data type pipeline would like get feedback,positive
could rebase main make sure latest version code thanks main branch,positive
proceed merge thanks lot,positive
came across bug well causing huge headache learn lot win end zero sense way,positive
tried use list linked unknown token instead produce different,negative
issue fixed hope solution imperceptible user future,negative
like green remove copied get pas none code really direct code smaller within test class guess ca marked comment chance dig exactly work case copied case anyone analyze best practice added removed via let know copied account went ahead switched hub code point ready merge,positive
basically llama able load,positive
nice need explicit explicit convinced way go could de matter simple whatever know best,positive
alright llama unless manually reproduce every single operation,negative
every single input pas model,negative
hi failing test think related seen could try onto main see issue done yes test related,positive
pretty sure let let fix slow version tried recommend training slow forward,positive
also make sure gradient trained seen successful training would recommend set false fix,positive
ran slow double check everything green,negative
wish could help familiar,positive
let make sure green checked style done,positive
great last rebase main recently patch resolve currently failing test run slow model main might affect namely mistral llama whisper falcon,positive
worked quantization successfully thank saved,positive
trick want pas issue lied used create calibration mistral big value work,neutral
hi thanks raising taking time dig duplicate happy review fix issue coming used would normally considered place operation opposed,positive
hi thanks great contribution package main contributor could take look feature believe feature useful research community text generation please let u know thanks much help,positive
code quality currently failing run make push whoop fixed,positive
main pull fix failing test,positive
propagate would like believe sole issue however fact odd explain unfamiliar resume one exist break start way would helpful could provide output please thanks taking time reply yes odd assume strange edge case code ensure epoch present break immediately train model none training last else training new model ca hand training issue trainer state lost job description job way data train purposefully small try reproduce issue validation extremely large try reproduce issue note slightly different original example manually set batch size addition think may batch large relation number,positive
also another design issue rationale building safe python code interpreter ground want code interpreter many common like even integrate cover everything relax safety bit may envision use shorter versatile version like one,positive
see progress bar worried well progress model loading quantization loading last step visible progress bar training generation around error interestingly observing cloud platform load loaded firstly almost fully free loaded,positive
code quality currently failing run make push,neutral
hey ever issue running training setup,neutral
propagate would like believe sole issue however fact odd explain unfamiliar resume one exist break start way would helpful could provide output please,negative
already think close issue thanks raising issue fix issue,positive
give warning loading weight saved way load weight directly saved instead use converting script provided saved make sure first convert model implementation saving able weird text arent traditional speak ca help sorry finally note seamless communication known face hallucination useful dealing lot want fixed translation probably look translation translation direction hope,positive
let make sure fix failing though try main,positive
fixed believe try main pip install,positive
since done lot work,neutral
yeah model recently update better way install accelerate evaluate sure however still unable reproduce issue attached minimal notebook without real version able train successfully let know able run,positive
correct understanding delay close please hesitate ping delay response thee lot sometimes slip,neutral
actually side something amiss,neutral
might unrelated accelerate come model directly index error essentially bash block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion recent call last file line module main file line main file line train return file line model file line loss model file line model file line return file line forward file line return file line forward file line return file line forward file line return file line return file line forward return super file line forward return file line return weight input sparse error assert triggered compile enable also fail single directly accelerate related,positive
hi normally see progress follow quantization progress see might issue load model process case help issue,positive
sense either way happy approve core maintainer review,positive
processor print try example doesnt work able wrong output space least matching official get,negative
sure correct warning saying model randomly model present state used moreover according git blame added part original model,positive
think fixed sure causing error initially suspect related handle arrival code import right making update code fixed issue one test failure think fixed,positive
last point think issue weight different give warning loading weight saved way able weird text arent traditional,neutral
think owl library pruning rather owl know maybe idea,neutral
hi thanks opening like specifically assert assert need merge,positive
hi thanks looking error message likely due place operation model implementation would like open fix,positive
find list also tried also give completely wrong translation said issue dig latter particularly attentive get correspondence original implementation might something sure script got right half time python import processor model processor print print range print print greedy print another thing might related save model inference meta implementation result similar repetition really weird well mind script reproduce many thanks,positive
thanks raising think tagged wrong bad thanks,negative
thanks may know get listed list also tried also give completely wrong translation consistent also peer running different o different dont know get right half time used official notebook another thing might related save model inference meta implementation result similar repetition,positive
hi thanks feature curious look early feature see work domain data generation example data provided attached could share example script test recent call last file line module output file line return file line generate file line environment used checked head fork specific commit version python version model,positive
hey orthogonal warning fixed main sorry know mean,negative
thanks raising think tagged wrong,negative
bit nuance involved blanket rely global accelerate used example user accelerate launch use variable said configuration said done trainer level think better way looking bit morning trying come edit actually thats fact set global variable might accelerate related see hard u remove stuff without breaking said feel free open logic inside trainer,positive
thanks clarification look custom instead,positive
hi thanks digging indeed seem like desirable behaviour fact sure setting global like happy review like address,positive
able pas device map following way,positive
anything else resolved merge except failing test specific,neutral
sorry ever slow like feel free ping lot going right pull branch see figure,negative
hey see last two,neutral
think want disable effectively global state end weird behaviour previously worked triggered suggest mean warning suppressed set,negative
fix test problem fixed install branch pip install upgrade,positive
hi failing test think related seen could try onto main see issue,positive
ready unexpected bug found yesterday fixed already also merge main resolve,positive
like good one nit agree sole argument limiting seen want pas extra think could resolve issue well think issue fact input used forward method may accept specific use case sure covered anyway maybe function could validate known leaving rest awareness propagate forward pas model sure though probably could break,positive
ran task going regular card getting none limit none none metric value none limit none none metric value none limit none none metric value none limit none none metric value,neutral
anyone following thread today saw added,neutral
main goal make request available one go could added progressively like test device map without fast without need similar help also check work need make sure also test model test top used example regarding kind store used run test even running instead positional form configuration could used,positive
wait ensure failure switch next version,negative
yes provide custom need make,neutral
hey fixed main update clone error thanks,positive
hi compatible fact ca train model general however work stage detail please check following,positive
based please take another look thanks,positive
think proper support switch default immediately either,neutral
good example u ugly understand cause flakiness understand exactly write clean test,positive
work loading addition necessary sorry see mine,negative
confirm issue following guide also see following documentation notebook following install evaluate torch accelerate notebook different model think use whereas notebook pretty sure resolve thing though,positive
change default test alright keep importantly set mean make like python class self looking repository found none argument function set class variable like would problematic perspective library consistency make one specification please let know misunderstanding part,positive
like thing pipeline issue work accept arbitrary model,negative
hi amy finally mandatory pas model think good timing request review personally think check issue logic image test output order also possible difference local test test local,positive
hi thanks opening issue quickly check model first sight model seem work firstly quick note model pas model instead actually sure could change something might know proper way secondly model hallucinate current code also translation right half time thirdly switching get almost always also correct experimental principal lee three phone call city office report first seven check class three back fourthly original model greedy search batch size sampling get different time however still really weird greedy sample batch size bad thanks opening issue dig time,positive
issue feel bit like generation see obvious worry investigate,neutral
hi thanks spotting investigation think right however would need fixed llama model well used legacy fallback proper would willing make relevant model,positive
hi yes confirm issue compatibility see figure solution stay older version,positive
yes thanks probably necessarily apparent sorry included mention try make explicit moving modeling,negative
two new let quantization process stuck hour memory error docker image docker image first done next installation quantization tutorial pip install pip install pip install pip install upgrade accelerate output version dev platform python version version version accelerate version accelerate found version true version na flax version na version version second second torch manually build specifically pip install next source optimum transformer accelerate similarly first output version dev platform python version version version accelerate version accelerate found version true version na flax version na version version error memory tried allocate mib total capacity gib mib free process gib memory use memory gib mib reserved reserved memory large try setting true avoid fragmentation see documentation memory management already also facing memory smaller falcon even theoretically reach capacity ram similar everything smooth,positive
thanks ping supportive examine forgot add already path follow another commit feel free take lead work,positive
got main however something still went wrong test could help identify much appreciate,negative
hi yes exactly thinking something like link commit change,positive
could merge main fix,positive
hi right thought automatic python import torch import loss true agree float desirable output type however inconsistency two default forward manual forward bit happy close issue think,positive
hey question test code work environment work reflected everything said,neutral
get error llama type since transformer library,neutral
first begun draft accelerate fixing one old disabled torch inspection test terribly broken checked raised test somewhat need pas see test setting error much smaller want investigate closer finally yet update documentation focus please let know,negative
could rebase main make sure latest version code,positive
might need add test,neutral
hey sorry gon na super hard without knowing trained model saved probably loading best model maybe latest,positive
could add link paper well,neutral
feel free update clone call happen background effect conversion server side continue pushing forward alternative raw see bad pure thanks report,negative
hey close unrelated reproduce,neutral
hey sorry modeling code deep seek part,negative
hey think model tracing mark feature request,neutral
see also suspect related torch installation,neutral
hey think related specifically,neutral
thanks think important already already use without saving,positive
hey crash quickly revert work fix thanks issue,positive
nothing good bad mode unrelated,positive
hey token probably added print,neutral
thanks problem reworked way written bound evolve want specific react also fine try logic within direct preference regarding let see find easiest work working implementation go,positive
hey want live trainer,positive
hey sorry reproduce issue incompatible python,negative
chat function looking generate picture model,neutral
could play added support static compilation faster forced rope float,positive
hey example seem problem data,neutral
thanks response actually source tutorial guess already latest version pip install,positive
hey orthogonal warning fixed main,positive
hey doubt type hint influence casting also many play output better float afterward see llama example,positive
could share output help u well,neutral
switch llama try llama subscription may wrong,negative
could additionally share output well,neutral
reproduce model feel free open fix otherwise would recommend llama longer maintain,positive
thanks u know best luck,positive
hey could rebase add explicit,neutral
find time take look,neutral
think guarantee reliably use associated mean exclusive alone,negative
original class version tool input definition think rather list text audio type type anything float bool instance expression true advantage list ordered thus leaf bit freedom order function call parameter name help generation proper downside force u work bit tool element point discus either use type value string instance float bool probably matter best option define maybe option robust compatible type checker option flexibility serialization friendly think,positive
ah yes probably set false generation returned otherwise good issue better way fix,positive
good debate might shed light consider,positive
sorry got caught many next week week focus,neutral
design previous class used query server text generation see prefer remove part since prefer let user define generate text instance class form doc becomes unusable stay alone several replace doc text generation potential problem showing import another package doc risk circular remove example doc altogether add whole call done doc concise preferred option,positive
bug also partner code written completely independently,neutral
code add import o port proxy address,neutral
hi happen try latest version optimum library,positive
testing parameter inference work correctly small even would nice throw error warning whisper small differently without explicit model task transcribe word,positive
yes said fixed main yet mistral legacy true default train,positive
done actually warning sure test would catch,positive
problem suppose fix fix python slow thank prompt response space still appear example python import fast slow text user hi assistant hello print text print fast text print slow text text text space appear assistant print fast decode print slow decode output user hi assistant hello fast slow fast decode user hi assistant hello slow decode user hi assistant hello also found difference fast slow one use,negative
want open propagate made llama gemma,neutral
feel free open fix added case,positive
thanks question also add check device well however thinking whether would better hardware set corresponding automatically like accelerate know variable,positive
could fix red another review,neutral
yes right added test case python self test scroll bottom view summary text self self self self self self return object attribute self self self self return object attribute self self self self return object attribute self self return return super super self object language false logger logger warning self language optional union pattern none bool false logger optional logger none none logger none logger ensure system raise system setup class none false false token none revision false false none none none union optional union none bool false bool false token optional union bool none revision main derived class either string model id inside model path directory vocabulary instance saved method applicable derived class path single saved vocabulary file single vocabulary file like optional path directory vocabulary standard cache used bool optional false whether force vocabulary override exist bool optional false whether delete incompletely received attempt resume file optional dictionary proxy use protocol used request token bool optional token use bearer authorization remote true use token running login bool optional false whether rely local attempt revision optional main specific model version use branch name tag name commit id since use system revision identifier git optional case relevant inside model specify additional positional optional along method bool optional false whether allow custom defined hub modeling option set true trust read code execute code present hub local machine additional optional method used set special like see tip passing want use private model python ca directly base class let show derived class vocabulary cache vocabulary cache vocabulary directory saved single vocabulary file point directly file link special vocabulary sure vocabulary otherwise use instead assert false none none none none false none none argument removed please use token instead token none raise token please set argument token token fast none mode forcing true none raise calling path single file use model identifier path directory instead calling path single file wo possible use model identifier path directory instead list else point either directory model identifier name kept legacy kept legacy used initialize slow fast properly copy instead random try get see none open reader reader get cache disk depending case none none else ca load following cache check necessary operate none raise ca load trying load make sure local directory otherwise make sure correct path directory relevant ca load trying load make sure local directory name otherwise make sure correct path directory relevant self self self self return object attribute self self self self return object attribute setup class none false false token none revision false false none none none union optional union none bool false bool false token optional union bool none revision main derived class either string model id inside model path directory vocabulary instance saved method applicable derived class path single saved vocabulary file single vocabulary file like optional path directory vocabulary standard cache used bool optional false whether force vocabulary override exist bool optional false whether delete incompletely received attempt resume file optional dictionary proxy use protocol used request token bool optional token use bearer authorization remote true use token running login bool optional false whether rely local attempt revision optional main specific model version use branch name tag name commit id since use system revision identifier git optional case relevant inside model specify additional positional optional along method bool optional false whether allow custom defined hub modeling option set true trust read code execute code present hub local machine additional optional method used set special like see tip passing want use private model python ca directly base class let show derived class vocabulary cache vocabulary cache vocabulary directory saved single vocabulary file point directly file link special vocabulary sure vocabulary otherwise use instead assert false none none none none false none none argument removed please use token instead token none raise token please set argument token token fast none mode forcing true none raise calling path single file use model identifier path directory instead calling path single file wo possible use model identifier path directory instead list else point either directory model identifier name kept legacy kept legacy used initialize slow fast properly copy instead random try get see none open reader reader get cache disk depending case none none else ca load following cache check necessary operate none raise ca load trying load make sure local directory otherwise make sure correct path directory relevant ca load trying load make sure local directory name otherwise make sure correct path directory relevant summary unknown option unknown option key see import call implicit pep preferred see short test summary finished running,negative
yes thanks probably necessarily apparent,positive
hey happy submit idea since log matter think would good idea disable logger since eventually import,positive
problem suppose fix fix python slow,negative
interesting confirm issue gemma would error pas list proper warning fast work think test help know update,positive
expect following fail available know case testing list valid guarantee hardware available environment whenever define whether available environment slow performance lot,positive
done first overview structural review deeply thank review currently midst week aim address soon possible truly sorry delay,negative
expect model code put,neutral
hey thanks check python issue wrong python self sequence string single string since manually add prefix space remove false token enumerate make sure special model token token true else token false return fix bring implementation closer keeping python thanks reply text special want text encode decode consistent original text even code still ca achieve effect example python import fast slow text user hi assistant hello print text print fast text print slow text text text print fast decode print slow decode output user hi assistant hello fast slow fast decode user hi assistant hello slow decode user hi assistant hello decode text add space special parameter add space special,positive
nope mean configuration work training actually training data le guess bigger purpose rope training sequence length smaller separate argument could make sense,negative
decently large batch size need pas custom attention mask would curious running latest source would work situation think memory usage mask factor batch size custom attention mask still may reduce factor longer need make multiple expanded mask le confident solve issue let know try,positive
look see one case transferring loss case right fix transfer latter case well,positive
mean passing model work fine would still allocate huge mask shape reason chose cache speed way previous version also memory mask could create non square mask index make right know perfect solution yet ought make sure affect community much especially constrained environment nope mean configuration work training actually training data le guess,positive
could review first since need feature added main branch add coverage safe update pull request right sending lot,positive
mean passing model work fine would still allocate huge mask shape reason chose cache speed way previous version also memory mask could create non square mask index make right know perfect solution yet ought make sure affect community much especially constrained environment,positive
added new feature dynamic pipeline test thing finally ready review rocket,positive
waiting come back release around,neutral
hi work fine previous code like suppose least permit long even slightly comparison,negative
yes sure sorry breaking memory backward compatibility mistake side also also fixed part two bucketing know long sequence increase buffer little little different argument control shape causal mask problem want run gon na need causal mask anyways even previous code would memory,negative
tested compile alright check always check input,neutral
understand like add token another token imply recognize like example control option python import text text work feel free open new issue old work python import text text,positive
great thanks know soon,positive
worked worked code unrelated ha related data since fixed anything else need besides make sure incompatibility future,positive
chance could take look,neutral
strange reproduce issue without one test try pip install last version compatibility fix,negative
generation problem usually better use case enough data train new paper summarization task hello share training code please use current implementation import initialize note randomly model saving model load model model want given performance reduction trying train question generation task compare,positive
unfortunately appear solve issue,negative
think know happening problem indeed took compatibility confirm pip let know issue resolved temporary fix work try figure permanent solution,neutral
hi everyone issue open someone work first time would happy help,positive
good really appreciate help script require local run line creation layer import import import import import import true jean made durable dark wash denim relaxed fit leg feature classic design straight leg silhouette jean made durable dark wash denim relaxed fit leg feature classic design straight leg silhouette made soft breathable cotton fabric crew neck short graphic print front relaxed fit made soft breathable cotton fabric crew neck short graphic print front relaxed fit boxer brief made fabric comfortable pouch feature waistband seamless construction boxer brief made fabric comfortable pouch feature waistband seamless construction dress made floral print fabric flattering silhouette adjustable dress made floral print fabric flattering silhouette adjustable made stretchy fabric fit feature material seamless design made stretchy fabric fit feature material seamless design blouse made lightweight silk blend fabric front relaxed fit delicate lace collar blouse made lightweight silk blend fabric front relaxed fit delicate lace collar jean made sustainable denim fabric feature skinny fit design classic style jean made sustainable denim fabric feature skinny fit design classic style set made stretchy fabric sport bra bra supportive design adjustable fit seamless construction set made stretchy fabric sport bra bra supportive design adjustable fit seamless construction nightgown made soft lightweight cotton fabric relaxed fit delicate floral print knee nightgown made soft lightweight cotton fabric relaxed fit delicate floral print knee swimsuit made fabric flattering design adjustable bra support moderate leg cut swimsuit made fabric flattering design adjustable bra support moderate leg cut hypothesis jean provide comfortable fit room move dark wash denim jean suitable casual soft cotton fabric comfortable everyday wear graphic print touch personality fabric keep cool dry throughout day seamless construction irritation fabric drape nicely flatter body floral print feminine touch dress design comfortable secure fit stretchy fabric wide range motion blouse made rough scratchy fabric uncomfortable wear lace blouse itchy irritating skin jean made sustainable contribute environmental harm jean true size run much smaller set made breathable fabric trap heat exercise set suitable wearing public nightgown made thick warm fabric suitable sleeping hot weather nightgown shorter provide adequate coverage fabric quickly color adjustable swimsuit difficult adjust stay place data hypothesis axis train test data data train test data data data truncation true padding true true true data return shape name shape name shape name activation activation false loss metric,positive
guess also sorry annoy second script incomplete merge together one full script run without needing data extra help reproduce error quickly,positive
taking account thanks lot case clear already model actually take character input input character sequence mean many subtle simple variant sometimes remove copied due simple change need specifically tensor batch size sequence length rather batch size sequence length split text space transform token list string instance text hi hi pad pad pad pad pad maybe clear already case finish making soon,positive
suspect yes please look loop,neutral
issue trying load local model attached,neutral
think first training element shape correct thats official document run model image image image image model,positive
may related compatibility made let know need handle instead also let u know version import,neutral
hey typically reserve bug feature case would likely successful help question hugging face forum model therefore need trained base trainer class post hand whisper architecture meaning used class follow post able train base trainer class,negative
thanks looking issue understand add instead accelerate review soon,positive
process trying make green found another inconsistency shown commit decode parameter single id list single id new inconsistent behavior issue order keep low coupling pull request list single id used ensure program run issue pull know problem getting annoying,negative
hi sure please see excerpt jean made durable dark wash denim relaxed fit leg feature classic design straight leg silhouette jean made durable dark wash denim relaxed fit leg feature classic design straight leg silhouette made soft breathable cotton fabric crew neck short graphic print front relaxed fit made soft breathable cotton fabric crew neck short graphic print front relaxed fit boxer brief made fabric comfortable pouch feature waistband seamless construction boxer brief made fabric comfortable pouch feature waistband seamless construction dress made floral print fabric flattering silhouette adjustable dress made floral print fabric flattering silhouette adjustable made stretchy fabric fit feature material seamless design made stretchy fabric fit feature material seamless design blouse made lightweight silk blend fabric front relaxed fit delicate lace collar blouse made lightweight silk blend fabric front relaxed fit delicate lace collar jean made sustainable denim fabric feature skinny fit design classic style jean made sustainable denim fabric feature skinny fit design classic style set made stretchy fabric sport bra bra supportive design adjustable fit seamless construction set made stretchy fabric sport bra bra supportive design adjustable fit seamless construction nightgown made soft lightweight cotton fabric relaxed fit delicate floral print knee nightgown made soft lightweight cotton fabric relaxed fit delicate floral print knee swimsuit made fabric flattering design adjustable bra support moderate leg cut swimsuit made fabric flattering design adjustable bra support moderate leg cut hypothesis jean provide comfortable fit room move dark wash denim jean suitable casual soft cotton fabric comfortable everyday wear graphic print touch personality fabric keep cool dry throughout day seamless construction irritation fabric drape nicely flatter body floral print feminine touch dress design comfortable secure fit stretchy fabric wide range motion blouse made rough scratchy fabric uncomfortable wear lace blouse itchy irritating skin jean made sustainable contribute environmental harm jean true size run much smaller set made breathable fabric trap heat exercise set suitable wearing public nightgown made thick warm fabric suitable sleeping hot weather nightgown shorter provide adequate coverage fabric quickly color adjustable swimsuit difficult adjust stay place data hypothesis axis split train validate test train test data data clean convert format train test data data data truncation true padding true true true data return apply shape name shape name shape name form extract last hidden state dense classification activation activation freeze layer false compile loss metric,positive
indeed original always fallback short generation quite straightforward update also could open new issue discus proposal better keep track,positive
seeing odd behavior trying code solution problem getting different get getting low likely result random guess python device import batch return batch text padding true truncation true load data import import emotion true none model import model device metric import average weighted return accuracy train import trainer class property device self return else return train epoch false false error trainer trainer model model train validation print device object,negative
indeed whisper model use processor deprecate note since importable public class given currently deprecation warning need first explaining processor removed second hard error python import first stage deprecation,positive
made full feel free take look,positive
issue related one let keep discussion,neutral
llama model recently llama architecture code make compatible static particular change causal mask current version place slightly higher memory model used maximum sequence length might able reduce memory footprint edit configuration field somehow loading model familiar shorter value field save needle static tensor since first time memory regarding new causal mask might want consider making flexible somehow see also recent,positive
hi script local file data would able share short script,positive
similar issue trying add layer define new layer assuming number output class case class self backbone super backbone forward self return got unexpected argument error raised,positive
hi solution indeed make sense let start accelerate upstream note need add accelerator class instead handle logic way longer dealing accelerate instead handling new class,positive
expect following fail available know case testing,negative
never told add like structure well enough anyway brain,neutral
currently state python import hey backward compatible set false false default python import hey hey cloud review,negative
still working minimal reproducer seeing behavior seem correct render reason code copied python entry policy reference separate policy reference loss value also compare variable conclusion could wrong training returned loss device since whereas set missing subtlety update tried set device turned problem remains,negative
going merge case script reproduce would nice addition,positive
issue facing right hugging team happening cause model,positive
unable share since working yet blocking merge,negative
actually going leave generation whose trigger attribute ca store cleanly property going focus relevant feature,positive
hi increase could potentially lead memory comment comment pointed like change made another causal mask size causal mask upcast float extra assuming also rope scaling context length ie matrix trying could please take look,neutral
thank much work excited experiment,positive
way printing generation always mode cool idea,positive
test also filtering since used token finally applied update,neutral
please ignore one looking different model variant able get notebook sorry inconvenience thanks,positive
wrong current model one problem never actually added text new new number actual neural network match load model make save model convert model load model original clearly somewhere place think text,positive
different issue like change made another causal mask size causal mask upcast float extra assuming also rope scaling context length ie matrix trying like separate problem unfortunately,negative
although need pad mask theory causal mask make token focus past token fortunately pad token manner set right side future token model even without pad mask behaviour acceptable doubt need pad mask origin pretty sure comment default behavior incorrect probably update possible precisely main reason raised issue suitable implicitly create pad mask mask least incorrect also warning raised think depending way pas appropriate attention mask training trainer default appropriate method working trainer set manually even trainer accept parameter guess talking custom collator situation yes pas way want oh interesting would possible share code chance sure minium code use available extract reproduction object like file learn scratch reproduction use official import class self data data data self index return index index self return open open trainer also found something bad scene pas manually yea code looking second time think depending way pas appropriate attention mask training trainer important conversation interested giving try add arbitrary length mask method although think suitable place appropriate place collator said collator behavior,positive
like sure offering freedom want go rather restrict type taken contrary limit user freedom already restricted want work combine would think aha want combine however think ca special input list,positive
reduce get low confidence also official notebook,neutral
also tried use processor well facing issue processor height width,neutral
please find script import import o import copy import import import import import image import torch import processor height width model image badge banner processor target image size height width box model convert bounding class format font font red color color line thickness thickness image box score label zip box round box box box box round float score method label font color thickness false,negative
please provide script issue,neutral
must say estimate likely quite imprecise given made accurate statistic evidence support experience trained successfully know could many example training,positive
processor still able get,positive
wait llama gemma assuming rope use extra say extra per layer away since need matrix see extra coming gemma llama see gemma might since necessary otherwise use incorrect training long time llama used extra,negative
everything failing fix everything merge,neutral
difficult give exact number growth could depend size model instance model could,negative
bit unsure one matrix mask least causal mask even without pad mask create mask two correct think may main disagreement part would like learn believe already try create causal mask main issue think logic wrong contrary executed incorrectly usually pad mask causal mask see default causal mask without pad mask pas explicitly yea issue think design decision automatically generate attention see shape default behavior generate tensor pad causal mask also used default pretty sure comment default behavior incorrect probably update possible also happen code illustrate imagine run strongly recommend passing attention mask warning training default trainer verify yes try without related mask warning train pas call model method generate cool mask left forward pas auto generate entire procedure warning mask oh interesting would possible share code chance also found something bad scene pas manually yea code looking second time think depending way pas appropriate attention mask training trainer,positive
report running result time number repeat report running result time number repeat static mode eager result time number repeat static mode result time number repeat static mode eager result time number repeat static mode,positive
thanks review sorry took longer respond time busy past week used pretty widely due around attention mask example whisper mistral pretty much cool leaving coming back later figured,positive
mostly concerned specific function addition leave modeling much possible course,positive
thank finding would exciting could top one without least increasing least may able train due even batch size set,positive
though usage precision quality also accurate respect original,positive
yes unfortunately consideration took issue become incorrect especially longer context shorter fine tried best isolate inference fine training usage always gemma course allow faster gemma notebook,positive
interesting new might worth path well short generation,positive
hi related blip training resolved follow done get issue share reproducible handy script hi tried script local got loss nan advice thanks,positive
hey fix gemma issue llama related defined custom way converting related,neutral
also found something bad scene pas manually call wo deal arbitrary user write collator allow pas model automatic generation even right mask opinion wrong training behavior possible temporarily mock function without source code complicated forward least independent,negative
would suggest going one completely since one directly model library making class one may confuse second always good idea investigate model print model first directly one python model print model output dropout dropout layer attention self query linear key linear value linear dropout dropout output dense linear dropout dropout dropout dropout classifier linear get dropout classifier one may confirm print dropout classifier therefore one safely python param false second making class component time actually combining class let class python class self super self forward self mask label output return output case one part self go print model output dropout dropout layer attention self query linear key linear value linear dropout dropout output dense linear dropout dropout dropout dropout classifier linear note two written may cause confusion first second looking freeze therefore case use python param false therefore note one important always check first since may get confused may simply forget,positive
anyone realize fix would incur additional increase likely making code could trained,neutral
pas attention mask default one one bit unsure one matrix mask least causal mask even without pad mask create mask two correct think may main disagreement part would like learn fair default sense create corresponding default basically think logic wrong contrary executed incorrectly usually pad mask causal mask see default causal mask without pad mask pas explicitly however document two may mislead shape optional default behavior generate tensor pad causal mask also used default hope made clear willing continue also happen code illustrate imagine run strongly recommend passing attention mask warning training default trainer verify yes try without related mask warning train pas call model method generate cool mask left forward pas auto generate entire procedure warning mask,positive
hi last apart one sent waiting answer none still work case positive answer ca repository company,positive
getting bitten issue leading,neutral
hi port gemma modeling code let know need anything else port want try prompt used translation bot designed translate code hugging face library file library goal output equivalent code want think carefully start write top output file also add code indicate uncertainty please preface easily find follow code network method please pas attribute name name class instead inherit retain attached like forward translate even method call layer model class accept pas also start name class class module please add start name require input shape way result first argument constructor like dense usually removed argument please remove argument constructor prefer function list like tensor get import method instead replace best solution usually compute value call totally constant never forward method store method instead output class like added start name always require must use instead raw layer layer build method call usually passing attribute name name however method like make sure retain like code try translate unsure leave untranslated mark comment later remove code use want exact replacement torch function get one via import port lot cleaner main see actually library merge might copy manually still want,positive
hi thanks raising issue going king want able create empty model use accelerate utility python accelerate import hi chance look issue,positive
understand point pipeline must explicit used ca add side reflect source model case generic model internally would determine whether use depending whether model model actually solve problem model wondering whether special way without use inspect none,positive
either fix call consistency update depending length generate also thinking second case get rid speculative say go call consistency feasible clean interface generate still quite rigid moment handling slow ongoing generate make flexible revisit decision,positive
understand point pipeline must explicit used ca add side reflect source model,neutral
add forward method class,neutral
change batch size made whisper unhappy merge,negative
python model open file line file print added range print,neutral
done checked possibility messing calling directly ran,positive
affected also trying make speculative work briefly outline found custom mask image text used part thus require attention mask much length got initially update generate assume one token added attention mask case since attention mask textual part forward image part every call soft prompt blip two issue custom generate new attention prepared manually handling directly either fix call consistency update depending length generate also thinking second case get rid speculative decide need idea affect,positive
alright need add new sure way train new make use language specific data none old might would need retrain model train new small small corpus merge new old merge new end keep initial input incremental add new might optimal certain le alright manually add new simplicity growing exponentially potentially language huge think pretty much hi tried extend vocabulary method got odd behavior sure used correctly try demonstrate following example import text text know token tried add token import text text understand like add token another token imply recognize like example,positive
thank valid model yes way model hand model use model sorry understand question identify model model context might help working exporter export model without memory question could identify model belong could selectively use use wondering whether special identify,negative
thanks interest feel free check job apply,positive
wrote sense ideally support correct implicit attention well like often derived,positive
ah pipeline default like slow need add run,negative
fair default sense create corresponding default basically obviously either wo generate also happen code illustrate imagine run strongly recommend passing attention mask warning training default trainer verify curious hear regarding add good unless reference code create default pas attention mask default one one,positive
working stopping criterion moment depending future may work dynamically reducing batch size according stopping criterion save time,neutral
hi think overall issue valid removed functionality part think correct probably wo bother removing,neutral
thank code implement stopping criterion useful thanks thank forcing token good approach thanks address issue beam search assuming beam size two beam already met stopping criterion still generating even though beam stopping criterion generation continue approach leverage model automatic behavior pad pad model add pad beam met stopping criterion directly append token end beam way model automatically add pad content following beam generate beam meet stopping criterion pad,positive
confusion wrongly issue code know configuration calling model confusion edit issue still since push wrongly new meaning new broken pipeline also try add dummy soon,negative
comment also added documentation could please review,neutral
sorry meant add test testing suite,negative
given difference two would look like proposal,neutral
large layer tell final activation large need compare step within layer see coming,positive
happy review relevant thanks solution something merge side hopefully useful community,positive
merge keep ensure break wild,positive
also clear custom handled forward unnecessary forward retrieve correct could output model default function since interested multimodal inspect related need custom attention mask handling generation time,negative
going merge pretty sure reference,positive
adapt following need note llama attention probably working latest version mistral cache work may bit rough worst case scenario use summary script gather perplexity note although script latency whatsoever latency tracked see impact speed time usage python experiment python experiment python experiment import import import time import import path import optional import import torch import import import import model experiment text optional none overwrite bool false none path experiment overwrite raise output file already really want override use overwrite list none none text text print sequence length range model label label perplexity store data save every latency try except ex keyboard interrupt still write file stop raise ex return main parser call experiment experiment main model revision main text task split test validation test log overwrite else raise unknown model eager auto set model support one instance main summary script plot perplexity metric first run generate one script plot usage python python perplexity latency title log perplexity latency llama function input import import path import list optional import import import perplexity latency latency perplexity latency perplexity perplexity log lower better usage lower better latency time per token sec lower better plot list title optional none optional float none path fig ax input sequence length feature enumerate already plotted ax make new one ax file experiment file feature feature perplexity feature latency poly poly feature experiment feature else feature experiment feature feature feature perplexity upper right upper left center right title else log perplexity function input return fig main parser logged perplexity latency perplexity title perplexity bit unstable skip start figure plot add code like change figure print plot saved main,positive
hello state new release transformer,positive
oh yeah added check mask none ran slow work test padding soon general modeling test guess need write separate padding test right,positive
hey thank opening still working cache still render obsolete wo hope understand,neutral
thanks lot amazing work chance question fine tuning give done,positive
warning easy way turn python import o,positive
indeed question related inconsistent document code behavior fix pad mask specific question still exist like model user want train like model use default mask incorrect,neutral
problem strange merge branch upstream main,positive
hi sorry disturbing really need hand unit test issue wrote unit test pas locally however like almost idea check error detail weird code without test pas smoothly would please take look time thanks advance,negative
flax interesting speed long keep ram usage bounded get good mean feel free open work wo able provide lot help,positive
could provide reproducer wrapped around sure,positive
ah yes misunderstood like sure offering freedom want go rather restrict type taken want work combine would think,positive
though trained believe must done similar fashion train merge example import sentence print sentence sentence print sentence sentence result,neutral
thank quick response helpful,positive
hey thanks check python issue wrong python self sequence string single string since manually add prefix space remove false token enumerate make sure special model token token true else token false return fix bring implementation closer keeping python,positive
looking closer possibly overconfident idea going like code currently always additional padding pas padding according code since output together code size otherwise case sure padding default false instead default tried hand fix,positive
another model problematic llama expanded include language import sentence sentence print sentence sentence print sentence sentence perfectly result,positive
alright think fix important let try ready ping next link one well get full please review,positive
yeah think right test got get absolute difference absolute difference absolute difference absolute difference absolute difference absolute difference absolute difference absolute difference absolute difference absolute difference absolute difference absolute difference absolute difference absolute difference pooler absolute difference code import import torch import image import import import import import image image image image enumerate none pooler print absolute difference stuff test could see message difference torch,positive
would available take look moment alternatively perhaps could suggest someone else might review many thanks,positive
import pipeline import torch import import import time image generator pipeline result generator image print result hi write test third case import pipeline import torch import import import time import class self return self return else return generator pipeline result generator print list result without behavior passing list two incorrect passing list two correct passing wit incorrect passing list two correct passing list two correct passing wit correct,neutral
probably update hub well,neutral
hey thanks report reproduce note model use token one super important like would manually added fix update part make sure added open soon,positive
hello interested becoming member hugging face could please provide information apply raise submit pull request thank,positive
teammate found work around u since problem silence warning import import import logging,neutral
fixed good ask final review core maintainer,positive
input following use principle wonder add comment expand slightly clarify second comment since potentially link commit change summary image image summary plain text python none none expand instead repeat conserve memory none copy create contiguous memory edit,negative
gemma regardless one two slow ran faster though mildly concerned also checked make sure gemma got see maybe though case summary output gemma root python test session platform python collected summary unknown option unknown option key module removed future version please import function use instead function use instead function use instead given array writable support tensor result undefined behavior may want copy array protect data make writable converting tensor type warning suppressed rest program triggered internally since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade default control generation length recommend setting control maximum length generation function use instead function use instead function use instead going please use instead converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize got two llama whether run branch run main maybe environment setup issue something summary llama test summary running main branch type short test summary false true mean relative difference torch torch ca load trying load make sure local directory sam differ st id st id summary llama test summary running main branch type short test summary ca load trying load make sure local directory sam differ st id st id like bit whether case error seem super concerning maybe tol summary llama test running branch matching main branch case type time ran short test summary ca load trying load make sure local directory sam differ st id st id like trouble issue rest unsound fast gemma normal good go,positive
thank think port code better way possible share prompt script auto translation little translation wonder could keep improving prompt auto translation work also want port model bother auto model,positive
issue checked whether fixed,positive
try pip install fixed thanks lot,positive
exploring model bit found indeed bug attention multimodal linked issue install main get correct,positive
find issue trying import import import torch import sure useful eventually nice possible,positive
try pip install may fixed well,positive
sure anywhere training saving grad norm added tensor tensor naturally ca facing issue stage please fix need use different fix,positive
fix main resolve could try include trigger new run,positive
close stay open close reopen,neutral
hi unfortunately unable reproduce error clarification talking sequence classification notebook script,negative
hi gentle ping able merge,positive
spent weekend trying get work finally worked thanks,positive
incredible thank hope able one amazing point well,positive
finally hub best one,positive
address issue version library prior might work well temporary however tried,neutral
able convert model hugging face model also trying also thing would appreciate help,positive
thanks quick reply clear clarification got bit confused line description configuration yield similar configuration architecture incorrectly assumed loading configuration would load model reality,positive
hi thanks work know reproduce test latest implementation really,positive
actually effort make clear comment randomly model instead model check block linked would like open clarify randomly configuration,negative
thanks update conversion script use still need,positive
added use base except one,negative
configuration tag added calling model saving different check try loading model check calling python load model directly import model know pretty weird library working could create add custom pipeline copy use reference,negative
got one possible solution python trainer trainer model,neutral
hey please take look comment guarantee exactly hidden due numerical precision given code snippet float error might even higher float,negative
hi error fixed update main command bash pip install upgrade,positive
trying rolling back version problem,neutral
cool within next day let know,positive
test unrelated affect final review worry,neutral
fine main due many fully,positive
hello question close stay open,neutral
great last thing make sure passing decorator need import fix,positive
still place partial sure team ready fourth framework library yet port like best idea port already likely port make work properly preference starting port automatically port code,positive
hi met problem anyone could please tell currently discus issue,neutral
comment made reply basically relevant think auto class used two moment neither enable add authentication alternative able specify one class think would preferable saved processor specific image processor possible load one verify one part reason protection moment try make clear compatible image misleading unhelpful verification sure early instead quietly weirdly u ending raised,positive
need mean need update related added argument need include argument case would see need update model need update either done affected test added make sure accept different size mean need modify take reference yes reference specifically added final step run slow make sure default behaviour running mean need execute slow locally machine yes running marked slow locally integration,negative
right understanding used passing object pipeline reason rather calling pipeline directly concern going change behaviour obvious change description fixed could add make sure pipeline still intended behaviour following passing list two passing list two passing wit example use,positive
thanks feedback sure code yet far import true true true text hello name text text print flax last hidden state print last hidden state print flax print print flax print flax last hidden state last hidden state tensor flax array tensor flax array tensor,positive
example two listed difference two odd layer would cause large difference suddenly arise confirm layer,positive
sorry know implement refer appropriate,neutral
thanks small test thank reflected,negative
nice solution greatly appreciate,positive
issue previously without recently problem based information considering might issue trying revert version solve,negative
would feature request design choice like code lead implementation see distinguishing short long audio,negative
weird install latest one,neutral
update due data passing vector,negative
thanks opening digging depth particular linking relevant comment based think really much compromise order faster running test suite mind think close issue skip test linking issue reason,positive
hey think open issue maybe update version,neutral
hey everybody please look spare time looking forward merge soon used pas main getting test,positive
hi thanks awesome video however accept documentation feel free share social medium could amplify kind,positive
mark issue good second issue someone could look script recommend leverage original mae script like learning rate number learning rate would need make sure match completely would also interesting check whether original script script mainly yet run,positive
oh think misunderstood could please read comment may check test code know want achieve thanks short combine related make consistent,positive
whisper expert exploring bit found error due model failing default fix use code python result whisper pipeline generation right still pas multiple batch default case one one sequentially use whisper generation python import processor model processor result result print statement stated batch size possible please use code generation tried measure time toy sample audio speed higher batch size hope understand use generation whisper,positive
oh sorry reading code paper turn model still point apologize carelessness,negative
failing test unrelated usage test make green would able merge,positive
many single agree lot like made revert back original test input test dictionary decode function input decode consider encode method return list input list call know fact decode wo work slow understand meaning decode wo work slow think shall consider even one ca handle welly within one method consider list different according treat list,positive
mixture entirely sure code exist yet,positive
whether two sure let use one make sure pas,positive
feel free ping green try best think come provide report hard fix handshake,positive
duplicate going close feel free read great comment pad,positive
feel free ping green,positive
thanks clarification lot documentation sure find since whisper sense look course,positive
please discussion various let keep discussion single issue,negative
want help need provide reproducer still issue latest open new issue,positive
thanks issue bring shed light know padding side used padding never completely,positive
alright think fix important let try ready ping next link one well get full,positive
hi sorry late reply checked slow integration pas would mean work dev might could clarify,negative
went entire model found difference layer pooler must problem final,neutral
want add model simple fix,neutral
name test make sure test accelerate automatically thought testing auto made sense right test device long make sure test accelerate support done thanks review,positive
would feature request design choice feel free open issue feature request,positive
hi issue state contain could push contain llama something like following python import import torch import class self super meta model come loaded state model snippet would effect wo get,positive
failing test seem anything possible maybe rerun,neutral
hi thanks would like take care one well like one also written mind like thanks lot,positive
like open happy review,positive
one ben resolved thanks,positive
lot make lot better user model card available help going close issue,positive
memory leak would easier track word take lot ram sure exactly much feel free open new issue ping making sure model generate always use generate problem,positive
alright want make sure sure always get though device get differently leave merge thanks,positive
great thanks explaining mean run test session platform python collected summary style migrate style see migration guide removed see migration guide style migrate style see migration guide removed see migration guide style migrate style see migration guide removed see migration guide seed style migrate style see migration guide removed see migration guide temperature style migrate style see migration guide removed see migration guide style migrate style see migration guide removed see migration guide style migrate style see migration guide removed see migration guide truncate style migrate style see migration guide removed see migration guide style migrate style see migration guide removed see migration guide style migrate style see migration guide removed see migration guide stream unknown option unknown option key module removed future version please import directly removed future storage class matter directly access directly use instead return instance owner since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade use expanded please clone tensor operation also advanced indexing tensor index tensor triggered internally use expanded please clone tensor operation also advanced indexing tensor mask scalar triggered internally card block found setting empty card block found setting empty please pas explicitly default value false future maintain current behavior pas use refer two,positive
case code follow float else,neutral
latest version much still issue anything else next,positive
thanks quick response actually found setting false accelerate everything clear need pas guess might system mine ran ram memory might prevent unexpected thanks,positive
hi made let know issue think make fix simple possible making optional,neutral
anyone working issue would something new contributor could look think issue may calculated dumping inside loop see mismatch case smaller input calling processor param user image image calling processor param user image image padding token deliberate could issue simply padding token calculation problem complex,negative
note original python none none st unclear two separate instinct change something like python none none combined figure likely written way reason took cautious route kept pattern around case important consideration current like python none none st however fine combine edit python none none combined manually maximally order stayed continue look hopefully rather readability,positive
trying create tensor negative dimension,negative
seen similar slowdown would love know happening,positive
currently add argument also would mind check later spend time work thanks,positive
hi guess current implementation properly support yet argument missing condition check whether parameter linear component emergent case checked line import key work well previous version accelerate false also work environment hope,negative
way new looking good first contribution take opinion grain salt block implement functionality part true would make sense close issue,positive
weird suggest opening issue linking one problem coming pipeline handled,negative
description link original comment see use long raise error would great make verification general class need manually define comment,positive
suggest trying reinstall environment pip install,neutral
two link directly relevant rather new many marked resolved difficult track comment exact class done,positive
thanks opening let u know ready review,positive
hi library work either randomly initialize model configuration initialize model know may first main way initialize model actually effort make clear comment randomly model instead model could update well,negative
yes correct model run new instance yes weird issue,negative
class try put model input manually typically,negative
think get since list batch language somewhat arbitrarily address pretty major breaking change inference mixed language audio especially whisper method audio language language tracked batch tensor instead single token would fix whole issue ca figure exactly worked see none entry language token ca see none handled see might get handled like would err,positive
hi amy think pas test week similar last resolved conversation simple fixed comment however resolved conversation might need confirm do,positive
update contain confirm importable update,neutral
hi also problem even trainer saving strategy accelerator mixed precision always mixed precision change behavior save none model model found option within sure used code,positive
hi odd able replicate error hub version verify running python session python import import print print tried fact line import give error check however print give,positive
problem due bound method input correct setting wrong device setting,negative
problem due bound method input correct device setting wrong device setting,negative
similar error mistral fix previously need made mistral return input weight bias device found least two argument argument weight method,negative
comment guess issue closed,negative
seeing similar issue problematic code link defined accelerate normal false true false true true true main static true false false false could help take another look every broken side accelerator scenario thank transformer version,positive
documentation already believe find documentation checked documentation first sake clarity let throw light basic explanation architecture functionality different neural network class encapsulate model forward pas logic different whisper loading different class may differ size complexity typically leading potentially better performance computational training data trained different different performance specific data improve performance specific application difference additional specifically designed generation network audio representation corresponding text sequence guessing understanding difference first notable difference subclass base class core architecture typically audio part model question utility function automatically appropriate model class based provided model identifier source repository instance major difference load automatically appropriate model class task generation efficiently manually specify model class,positive
zero likely type hint universally correct value returned zero tensor scalar value subsequently without,neutral
hi able run sample python import pipeline import import image pipeline image question question parrot answer print image question,positive
hey sorry trouble running inference setup index either device indexed tensor import import image import import torch load model processor processor model auto prepare model see image image processor,negative
update issue model different greedy prompt depending batch size even create batch prompt even float common example making torch use deterministic help completeness python make torch deterministic import o import torch true import random import import seed everything model question order help earthquake factory rushed make batch disaster relief first workshop batch second workshop batch batch percentage question question contain different output first workshop second workshop since equal batch percentage result first workshop second workshop since equal batch result environment issue float well,positive
around today think issue might actually specific medium model reason always newly classifier head save reload random exact protocol notebook provided worked fine exactly like swap base model also struggling get sort operational saving classifier layer separately bug specific model appreciate might something fixed easily anyone would hugely thanks far,positive
sure anywhere training saving grad norm added tensor tensor naturally ca fix patch following python self save content instance format inside self open probably best approach,positive
hello read thread according sample code model card work please give issue facing thank,neutral
git push something weird closed try open new pull request,negative
quiet deeply training different scratch version found useful example first training produce submit possible change copy first greatly training tried,positive
update issue cant implement still notebook latest version python import model provide case already error recent call last cell line import model provide case already file token revision none auto balanced sequential raise passing string please choose file self module none raise support implement support model class need implement attribute else set support implement support model class need implement attribute,positive
actually gemma model hub,neutral
duplicate going close proper deprecation cycle open issue,neutral
alright pretty big change need careful see,positive
yes test eager le important flash add slow test already run new slow test flash later,negative
yes test eager le important flash add slow test already run,positive
would nice start fresh implementation new template,positive
thing missing test make sure yes test eager reason card test machine use environment test,positive
alright slow version fast version think good left test inner final follow interesting work mostly implementation custom backward save backward explicitly instead loading big big slow forward use parallel scan slow forward parallel scan algorithm use parallel scan algorithm,positive
probably yes bit critical mistral beginning suspect people used explicitly without sliding window explicitly yes would use full attention would use ram maybe accurate really went unnoticed quite,positive
still running issue thanks,positive
hello classmate trying write project proposal course would like extend rag model listed would mind u thank much,positive
hi wo go different running logic running logic like forward wo running logic like forward forward,neutral
go away branch issue closed,negative
want port model found model ca find modeling code anywhere already ported please let know please create draft like thank,neutral
hi thanks raising issue going king want able create empty model use accelerate utility python accelerate import would able help make stable diffusion model work got far python import torch import accelerate import accelerate import model model model auto none none none none model bash recent call last file line module model file line file line model file line name tensor model file line file line raise type self object attribute name object attribute,positive
issue today following voice assistant course summary following system default issue,neutral
intended tag somehow tagged wrong person,negative
found reason size input image different previous one error occur solution problem thank much,positive
thanks fixing could add test well added mind taking another look,positive
like good idea make importable module backwards compatibility like open happy review,positive
sure also post case anyone issue need change due,positive
thanks providing looking example think issue coming argument pas pipeline import pipeline import torch import import import time image generator pipeline result generator image print result default control generation length recommend setting control maximum length generation image soccer player kicking soccer ball image soccer player kicking soccer ball,positive
try different agent temporary file check work,neutral
thanks actually gon na ask know gemma approximate exact tanh image,positive
sorry extremely late response property handle model update sense logic would appreciate could take another look thanks,negative
yeah model million crazy got past failure running starting wonder python version issue version python use mac box pretty vanilla always either use virtual neither part package,negative
clarification please let know chance review code,neutral
following like following day good,positive
also tried running running forward input size took memory memory,neutral
one stand need change fine,positive
thanks lot looking forward integration,positive
actually direct conflict affect else clause whereas part clause might,positive
one thing empty maybe removed also raised one issue documentation easy fix core maintainer review,positive
trying reproduce example script provided manage reproduce,neutral
model device fixed device tracing switched different device consequently model becomes unusable,positive
tested input confirm equivalent,neutral
fix custom may see slightly different numerical due different computation turn set environment variable unable register factory register factory one already registered unable register factory register factory one already registered unable register factory register factory one already registered binary use available enable following rebuild appropriate compiler warning could find default control generation length recommend setting control maximum length generation fix custom may see slightly different numerical due different computation turn set environment variable unable register factory register factory one already registered unable register factory register factory one already registered unable register factory register factory one already registered binary use available enable following rebuild appropriate compiler warning could find default control generation length recommend setting control maximum length generation two soccer soccer field two soccer soccer field,negative
import pipeline import torch import import import time image generator pipeline result generator image print result,neutral
mean tracing device loading device impossible well use case necessarily need serialize model trace fly device,negative
pas green favorite color let know move forward,positive
rebase main push git commit crazy double check green thanks,negative
think could try play otherwise temperature fallback apply short form conceptually would compression ratio apply short audio believe audio upstream implementation yet,neutral
hearing opinion seem two firstly significant overhead switching secondly whether tracing progress additional time therefore current implementation seem appropriate reason issue necessary simply also impossible switch device device environment even four want deploy model device impossible move model due enforced device anyway thanks,negative
multiple trying predict likely target language transcription currently transcribe different single batch please make sure either force single language passing make sure input audio language error well due change behavior setting fix however get returned source language getting previously,positive
hi update would great add library hi sorry quite busy recently would like check could kindly check whether understanding correct let take example need mean need update related test added make sure accept different size mean need modify take reference added final step run slow make sure default behaviour running mean need execute slow locally machine,positive
thanks quick related question bit mean mistral related buggy last default respect,positive
almost done quick question documentation fail would recommend remove copy make custom would benefit sync see better option,positive
thought something similar root could already package provider module make sense ready review code side rocket,positive
merge stuck take kick,neutral
sure would call point worth finding end first place,positive
hello trainer stage please take look latest documentation,positive
hi kind structure would propose,positive
great thanks explaining mean run,positive
hi update would great add library,positive
hi thanks opening show could provide code snippet reproduce output terminal show working rather take screen shot readable relevant code searchable copy text able test setup tested input confirm equivalent,positive
issue error sufficient memory load model since loading simultaneously,neutral
create ami start new instance sure like copied along easily check running command line running python command run new instance,positive
sure would like open update include accelerate evaluate way get contribution,positive
hi odd able replicate error hub version verify running python session import import print print,positive
strictly breaking change code rather hub breaking fine leave title though,positive
hi reason break previously would rely casting make work use simple single call get therefore aware internals cause sudden unexpected change behaviour trying solve problem else yet understand desire use use case case would suggest modeling code directly instead pipeline becomes widely becomes commonly feature reconsider think hi thanks clarification understand concern tested break anything test model still think acceptable follow suggestion cast input type pipeline see,positive
thanks trying initially kept getting error work made separate copy notebook think would possible update tutorial current form odd run pip command install later find notebook include second pip command accelerate evaluate necessary happen version,positive
fast parallelism code probably source,positive
getting error model pip install pip install eager install optimum export task model import import pipeline sentiment model pipeline voting public health politics error recent call last cell line voting public health politics forward self none,positive
hi thanks response investigation confirm phenomenon would appear fast,positive
unfamiliar kind big add small test,positive
hey think issue related parallelism interaction underlying rather use work also add work might related way used super familiar underlying ray usage wrap around partial,positive
fix mad patch see,negative
thanks know sure wrong another force push without pull time,positive
hi title please help merge pull request thanks,positive
work weird essentially normally,negative
hi ran script work end main double check bash model dropout linear linear linear dense linear linear linear dropout linear,positive
hey bit hard print see python sample print hello assert hello python warning could find hello pretty sur reason failing python set true use worked like charm duplicate related llama,positive
hi thanks getting back tried run main python import torch import model print model confirm able load try script work end check script see work main support attention implementation yet please request support architecture believe error bug please open issue repository load model argument eager meanwhile example model eager,positive
confirming without reproducer super helpful anyways got gist fix costly better rely generation anyways always available point include patch would breaking otherwise pretty normal u rely know exist like full attention one,positive
hi exact error import pipeline recent call last file line file line module import pipeline file line module name file line module import file line module name file line module import file line module name file line module import backward compatibility file line module name file line file line return name level package level file line module import file line module name file line module import import name version version,positive
hello gemma team superb stuff thanks lovely fix,positive
confirm basic quantization example version,neutral
open get going discus,neutral
printing warning would great would like open,positive
also library load custom architecture architecture defined another one used pretty much always essential fix,positive
closed different still open,negative
ah good catch thanks try run make see get bottom,positive
made video explain important,positive
ah see happening pull step depending git probably create merge commit remote branch local create contradiction however rebase effectively history local branch git fetch upstream main git rebase git push,positive
great work first pas documentation made swift response fixed additional similar,positive
refreshing notebook session accelerate already used another library session use still use old version incompatible version start fresh notebook session install accelerate one first,positive
curious want load best model end leave want best model end trainer already value logger curious given provided,positive
import import import torch import parser device return main auto auto else print loading base model print running bit print model saved main model still big,negative
hi thanks saved model huge achievement team may script model export used work model still large original,positive
hi need move torch import block top test file,positive
facing error example implementation pip command accelerate missing accelerate error showing sure like tutorial could outdated local computer make work,negative
hey chance look thanks,positive
tried fresh install work fine got import error inside file pip install clearly code found executed ca seem reproduce bug matter put directory call,positive
alright thanks weirdly enough environment latest think might almost done need fix couple right facing weird one error test module hint make sure test valid python return name level package level module torch import module error,positive
hey issue occur copied statement header multiple whereas original class header mismatch code align advice remove copied statement affected class,positive
indeed issue implementation specifically unclear issue though mask padding like job intended weird unmasking first three,negative
yeah virtual environment failure run get warning amalgam could locate inside recent call last file line file line return file line raise find disk cache outgoing traffic disabled enable set false place also morning latest code sake consistency though virtual environment amalgam install upgrade requirement already satisfied requirement already satisfied collected found installation successfully uninstalled successfully amalgam install upgrade running command git clone none quiet resolved commit build done getting build wheel done done done requirement already satisfied building collected building wheel done wheel directory successfully built collected successfully dev remove since virtual environment code amalgam could locate inside recent call last file line file line return file line raise find disk cache outgoing traffic disabled enable set false path becomes valid output like amalgam recent call last file line module model file line raise unrecognized configuration class class kind like hit error moot got past loading model object patch change output amalgam repository custom code must executed correctly load model inspect repository content avoid prompt future passing argument wish run custom code hit issue previous go back directory relative directory fail way noted filing bug amalgam amalgam could locate inside recent call last file line file line return file line raise find disk cache outgoing traffic disabled enable set false reproducible fresh virtual environment tried fresh virtual,positive
hi exact code work replace otherwise fine think might kind environment issue try pip install upgrade pip install upgrade also log posted see none flax found wo available configuration possible issue class like failing initialize torch present,positive
maybe implementation bit since last worked need update,neutral
outdated version ruff namely causing weird hey know help error solve copy exception found following copy copy match line copy match line copy match line run make python fix running make command error exception object attribute investigation theoretical code empty print closer look variable empty string really know move,negative
code block model import model running home directory anywhere path model failure like amalgam none flax found wo available configuration used could locate inside recent call last file line file line return file line raise find disk cache outgoing traffic disabled enable set false edit remove part code work run code coincidentally correct path relative current directory code work,negative
thank kindly effort busy different project back sure related bug ago also back dutch transcribe setting forced prompt currently work dev branch solution work missing right miss back trainer wer dutch common voice inference pipeline wer exactly data problem even even first define inference pipeline use trainer immediately see also,positive
read relevant source code trivial fix issue use flax auto show availability code example python import torch import import import conversation code used detect existence flax raised error message current repository make clear distinction flax suggest repository distinguish flax reduce confusion improve,positive
ran minute edit previously said checked audio word fit ran gib ram neat way track one point support leak theory even word running running repeatedly loop eventually run,positive
made comment previous force push double check sequence running git fetch upstream git rebase git pull git push force correct double check force push seeing,negative
hi thanks issue issue exactly working main printed try main,positive
might need add tested change test see reason test work maybe change let save test think,neutral
could willing give another try,positive
think one want test accelerate support sampling probably purpose could elaborate relationship sampling device see need use case change test sampling fail,neutral
unfortunately couple ago get chance work since got closed,negative
feel free ping another review green passing test failing related empty commit rerun enough,positive
hey fixed language python transcribe provided run two calling automatic language detection inference able run code update post reflect,positive
also prevent red main,positive
case may seen message else need perfect,positive
failing torch test unrelated however check consistency related bad copied check wrapping block,negative
hey could please share reproducer,neutral
get time note generally underlying model usually work single chunk audio cut,negative
python launcher well accelerate launcher replace true true guess deprecation came little early,positive
feel free ping another review green,positive
feel free merge alright,positive
check latest release example,positive
good need generation update string saved,positive
yes wrote hey rebase merge alright reply directly view id,positive
hey part big sure save execute arbitrary code inviting read doc corresponding argument,positive
hey rebase merge alright,neutral
may ask set parameter part pipeline implementation model sometimes find really find set parameter,positive
tried code input size still see effect,neutral
done mean orthogonal want train model mixed precision without trainer way,negative
agree strange time feel quite able behaviour pure,positive
use model training cant use also strange inference mixed precision,negative
discord somewhere else report way burden,neutral
getting hello name said voice gone giver said giver life giver said giver life giver said giver life giver file line need update file line module output image code image,neutral
really way handle cache probably rename cache,positive
gradient properly handle force move torch compile profile potential memory finish documentation maybe need remove merge,neutral
thanks mistake side thank quick response great work,positive
getting hello name said voice gone giver said giver life giver said giver life giver said giver life giver,neutral
hi getting following error tried model due bug fix transcription multilingual whisper default language detection transcription instead translation might breaking change use case want instead always translate audio make sure pas multiple trying predict likely target language transcription currently transcribe different single batch please make sure either force single language passing make sure input audio language simply following used exactly code previously push model week experiment different getting error wondering could help understand issue,positive
gradient properly handle force move torch compile profile potential memory finish documentation sorry use code generate get error print unexpected exception exception falling back standard exception recent call last file string line true true true true true true true true true true true true true true false true true false true false true false true false false true true false false true true false false true false true true false false true false false false true false false false true false true true false false true true false false true handling exception another exception recent call last file line file line visit return super node file line visit return visitor node file line self node file line item file line visit return super node file line visit return visitor node file line file line file line visit return super node file line visit return visitor node file line file line file line visit return super node file line visit return visitor node file line file line visit return super node file line visit return visitor node file line file line file line visit return super node file line visit return visitor node file line file line visit return super node file line visit return visitor node file line file line visit return super node file line visit return visitor node file line return module attribute exception direct cause following exception recent call last file line file line module file line return file line generate return file line self file line return file line forward file line return file line forward file line return file line forward file line return file line forward file line grid file line run return file line run return file line run return previous line repeated time file string line file line compile compile module file line lambda lambda signature file line signature specialization file line raise node matrix matrix batch dim none none none none state none dim none dim dim dim handling exception another exception recent call last file line file line return file line return file line evalue file line else file line first file line object file line raise get source code could get source code image,positive
ready review merge make follow fix failing seen channel,positive
line python clean staging change python else clean staging,positive
gradient finish documentation scheme properly handle slow fast path single class enough torch compile,negative
thanks error spread one pointed based derived suitable training faced issue point right way select model training,positive
small update latest push training,positive
sorry still tiny issue working,negative
sorry could push model hub one trained need full training loop training please see notebook,negative
future vision let avoid,neutral
modify model trial epoch model trial epoch,neutral
understand thank another question interface support inference code inference four single machine want implement inference two four machine still use implement inference implement inference model,negative
code ready review test running locally however problem way would really appreciate could guide best file line module file line raise problem half define torch problem half define torch code exit status,positive
hi suspect getting wrapped inference context manager try,neutral
fix missing went away already model default location first make directory modify train script might wan na save different place model run train script save model saved open train file remove save change model also set model need add might better way could model also train command something like python late might might better way going training back going need sleep still think default behaviour require failing request saving training dont close need happen every save,positive
hope mind took liberty fix failing test case ca rely get correct input length thanks fix,positive
thanks understanding undoing run model slow confirm passing since float float support float float learning test talking,negative
default following work python model,neutral
hey although sure one let know think looking forward merge,positive
yes find version would great could get working also looking would love help interest,positive
thanks mistake side course first check whether accelerate available,positive
main today since main branch sure failing clear,positive
hi thanks reply line one pipe version platform python version version version accelerate version accelerate found version true version true flax version na version version script fill distributed parallel script fill context weird issue instance running model next create ami based instance start new instance based ami run model even already exist hub folder,positive
hi thanks raising issue could isolate trigger giving u minimal amount code reproduce help u figure happening share running environment run terminal output running code locally remote setting global cache run following python session get output import,neutral
actually job catch test fetched,neutral
yes right run check spit per word well character,positive
token perplexity directly comparable across different advise negative log likelihood per character sum total loss whole test set per number reference check appendix wed vincent wrote exact setup number reply directly view id,positive
thanks note helpful reader purpose demonstration different internally wed vincent wrote sorry hijack post case interested need first methid differ bit reply directly view id,positive
sorry hijack post case interested need first methid differ bit,neutral
hi run preferably report result collected selected float install run make style make well result make still check code,neutral
awesome let u know ready review,positive
suggest forward pas find code taking particularly long time identify troublesome area solution happy review amend,positive
little bit reason coupled method done part cover much ground possible pull request method stop since know want test push leave part add notebook help review,positive
slack latest set follow logic set enable explicit full control forward implicit lazy usage forward usage note ignore seeing need summary implicit compatible implicit attribute redundant slicing original path opposed custom logic bunch added feasible future forward several related ensure working,positive
latest version faced issue latest version get error,positive
hi draft support sharded let know,neutral
hi please read following concept guide particularly section design need careful put residual connection device work practice stop level layer example let take layer want modify otherwise get error also also set auto want manually allocation,positive
rotation new test make green make minor model add let u know find anything wrong supposed like ask keep someone,negative
hope mind took liberty fix failing test case ca rely get correct input length,neutral
issue case found problem main memory memory use task manager check let torch move main memory avoid still large according fix problem,positive
hi think let explain think happening bug arose case load model first initialize model random load weight file random model build call build part method method need load model automatically call build model past build dummy network explicit build difference invisible part however difference custom model implicitly depending old build behaviour build maybe without realizing also depending code layer model rather built input build method standard also follow behaviour look list missing see also loading correctly writing build method hard though use build template something like work python build self none,negative
python doubt cause issue since used,neutral
line worked sure cause issue,positive
alright failing test related co sin fixed device thing least,negative
hey module become long rather hard navigate difficult find looking making one place one nightmare would possible split multiple separating different,negative
input original suggestion generate discussion way input standardized another incoming feel better choice loading caller thanks following,positive
input original suggestion generate discussion way input standardized another incoming,positive
regarding fixed issue due timing related yeah worry,negative
next make failing perhaps skip well update header longer hit ready review longer draft tag core review since rotation amy look model,positive
may need conditional context manager,neutral
causing last set seen everything went well see green,negative
thanks might update return let make sure slow pas well missing added could review,positive
actually tiny missing thing entry still code make sure latest version running make great idea added doc string class regarding fixed issue due timing related,positive
import following error look see undefined symbol anyone faced issue,neutral
seen similar model seen graph conditional call python plan support dynamo support maximum,neutral
sorry finally back track eta tomorrow,negative
hi make sense load way used load individual defined composite model combine defined model within library like,neutral
tertiary issue citation file another set sure exactly citation file note cite repository file also message use please cite probably hurt,positive
even loading done via following block rather python last try use type return type rather python none file attribute big deal good,positive
secondary issue copyright statement top file reserved think statement apache license statement directly underneath law actually explicitly license,positive
thanks close requirement add build method somewhere use reliable reference actually look like,positive
think close issue native grad wo work need apply special treatment compiler make grad working however think able integrate grad need specify,positive
good issue thanks help,positive
fix output image fix output image,neutral
given already internally possible configure also possible,neutral
generation problem usually better use case enough data train new paper summarization task hello share training code please use current implementation import initialize note randomly model saving model load model model want given performance reduction,positive
thanks meant integration test similar would happy work yeah add test following,positive
already close enough know think agree batch dependent,neutral
hi use adapt thank,neutral
however one thing still think strange let take look,negative
hi first deliberately exposed use case internal discussion model type exposed publicly used looking two inside first line without failure get second one looking see used body even file like much properly work way go,positive
hi reason break previously would rely casting make work use simple single call get therefore aware internals cause sudden unexpected change behaviour trying solve problem else yet understand desire use use case case would suggest modeling code directly instead pipeline becomes widely becomes commonly feature reconsider think think concern reasonable could help check whether change break current across precision show break anything bring facilitate discussion,positive
comment thanks copied exactly test script attached got maximum difference quite difference change differently unless use different transformer version likely due different system also latest main branch explain difference execution mine unsure relevant use since model wo fit without quantization quantization base model isolate different error,positive
sir tried converting size reduced video long time loading model request resolve issue platform like deploy model get without,negative
hey thanks opening issue try keep could ask question forum instead sure community help otherwise could look quantization smaller model ram available frame compilation export thanks,positive
issue could someone take look,neutral
run test master also like issue related read stack log,neutral
checked everything working double check anything wrong push commit whenever thank,negative
sorry late reply due static cache good choice greedy search large buffer store past state add overhead beam search also except overhead attention module besides prompt beam search reduce first token latency especially large batch long sequence prompt following memory consumption native implementation better cache locality prompt reuse enable attention apply prompt first token get performance improvement next token get performance regression due token cache discrete kernel ca support format low efficient way firstly copy discrete state call make quick validation attention kernel extension local machine attention format aware kernel obviously next token inference code change like following image imperative following top dynamic cache bottom attention latency reduction next attention aware kernel first token prompt image merge team enable related attention team also pleasure work used store past dose dot based format attention,positive
sorry confused add model hub also model implementation library suppose need review approve marked ready review able review let know anything misunderstood new open source project,positive
hi member team review documentation soon sorry wait,negative
sorry part behavior seen python import import stage auto auto model command bash output recent call last file line module model file line return file line file line file line file line file line assert check batch related equal,negative
related could please check impact environment thanks,positive
got could problem file system let revert keep staging,neutral
additional motivation would enable locally save load model would fire,neutral
running issue kindly help review,positive
main change change add enable disable fix check move prevent performance degradation copy avoid original,positive
thanks copied exactly test script attached got maximum difference quite difference change differently unless use different transformer version likely due different system,positive
thanks might update return let make sure slow pas well thank taking time review pull request missing would like add similar,positive
sorry reproduce error image close enough used python import torch import import image import import import true processor left prompt user image content image image processor prompt image processor prompt prompt image image model print model model assert must assert maximum difference issue might create,negative
issue fast mistaken comment valid slow implementation based,negative
fixed yet fixed kind manually follow done python self replacement always legacy first return setting legacy false fast add good difficult issue similar,positive
still local result image,neutral
would love see newly added job could run successfully failing fine enforce,positive
feel free merge main run make make sure go green,positive
could run make style make make sure quality test pas,positive
hey model hub close,neutral
send reminder team help contribute thank quick response,positive
thank next time let make sure doc paper link fully merge require every model regardless release date let make exception,positive
thanks flagging fix issue,positive
generation problem usually better use case enough data train new paper summarization task hello share training code please,positive
related think default smaller value make sure take much ram also bool help see,positive
hey sure incorrect want compute loss every token use change last token one term perplexity first token end python tensor tensor tensor tensor tensor tensor single forward pas image feel free answer satisfying enough,positive
thanks meant integration test similar would happy work,positive
getting error local zero try delete file system,neutral
duplicate access could try official read linked issue,neutral
ah sorry clear meant use either,negative
could open new issue error experimented behaviour u better track new resolved,positive
issue well help go actually fixing issue work idea use python fix issue,neutral
issue since collaborator closed,negative
thanks reply per suggestion machine used transformer version however work got error,positive
hi thanks response comment possible llama padding padding script lower precision tested float issue mismatch use input float precision equivalent llama model mismatch error high use input random different batch size even float script reproduce mismatch python import torch import import image import import import true processor left prompt image content image image processor prompt image else prompt user content image prompt model need go already float model print model model assert must assert maximum difference,positive
true think whether variable would breaking change case setting right value sufficient,positive
install run make style make well,neutral
thank always dynamo reading thought,neutral
also need make run,neutral
read one understand still always open check back since,neutral
problem still node training see problem single node,negative
stopping thanks passing feature good ran slow locally well,positive
thanks context anything help static cache pretty keen implement cache whisper,positive
think need add trainer enable gradient,neutral
hi cause almost certainly building forward pas dummy building explicit build improve loading time also fix broad range dummy input build process problem though custom build method build correctly load present ca load simple solution add short build method custom model note acceptable pas none build method usually figure weight,positive
thanks fix share new couple day ago summary general flag flag whether graph via flag work even mode even without specific flag set true used mode would false,positive
hi reason break previously would rely casting make work use simple single call get therefore aware internals cause sudden unexpected change behaviour trying solve problem else yet understand desire use use case case would suggest modeling code directly instead pipeline becomes widely becomes commonly feature reconsider think,negative
try increasing see whether work,neutral
problem resolved latest version resolved,positive
hi firstly sorry taking long try reproduction struggling figure issue possibly misunderstanding problem tried following cache git local folder different relative work fine help specific reproduce issue dig going,negative
actually interesting point see arc sin sinh float doesnt list sin check done float float,positive
hi aware issue duplicate let keep discussion,positive
think fix would useful let keep open,positive
issue fixed help share reproducer make sure version included release check minimum version thanks,positive
float tensor default unless explicitly cast tensor desired thing sure get,positive
weird criterion float tensor,negative
ready review sorry got review,negative
eager seem match main even false true mean relative difference torch false true mean relative difference torch false true mean relative difference torch,negative
checked previous version work language token score associated indeed one language token regression bug new version,negative
generation thing clear start thinking small model sequence problem different infill model error missing token prompt text model error multiple infill provided prompt text model full text text sound,positive
tried add new test check work test original longer input size check feature close absolute difference index relative difference index think also problem older slow self model auto eager model model auto model mean dim model,positive
tried add new test check work test original longer input size check feature close absolute difference index relative difference index think also problem older,positive
cosine distance check solve issue way check model might best option see make better everything passing test,positive
yes think fundamental question behaviour want first question pipeline live push hub would model space case pipeline model exist space behaviour model pipeline space pipeline associated model default pipeline think still associated default model see could model default could remote model model model current behaviour coupling model pipeline together think sensible although within meant able use model option think enforce custom,positive
hi thanks raising issue hard tell information like error library rather anything,negative
thank working like growing right direction review went back reread paper addition added code general nit whenever possible try name expect use method outside class allocate nit raise instead assert would like see next review test ensure original cache change switch new cache point attention performance point code consuming memory refine according general add ut ensure output new cache,positive
ideally happen well performance force happen,positive
let fix able merge edit let add small test integration resolved conflict test think use test wrote upgrade torch version environment enable test check right convert tensor solve problem even torch think test wrote enough testing,positive
optimum perspective would really nice could enable otherwise might need override,positive
super fan complexity add new code simple want drop otherwise fa never used llama right indeed unrelated,positive
thank regarding issue possible trace model device intended used overall expensive think downside benefit change could probably flag around would maybe want call layer code path would used complexity added small benefit fair case tracing usage reverse tested sure whether something want support personally favor related,positive
remove rewrite history branch shown tab respective moment related merge reason ask force pushing current history like one force push lot main commit saying foo branch recently would try main force pushing,positive
due recent addition accelerate look,negative
would nice fix pipeline add special,positive
want open update doc,neutral
interesting share compute perplexity,positive
hi tried latest release patch release,positive
aware fix nice note failing,positive
specific gemma warning set otherwise tip warning good user something,positive
sure would address issue somehow,positive
yeah fixed locally forgot upstream added set true default want add tip model doc,positive
related context size perplexity close loss value close checked script small bug give token whole input sequence reason low perplexity give token main issue related token two main version work fine regardless whereas work unless present version support context size work fine try script python import torch import import device model device import test test test text use parallelism sum stride range stride min may different stride last loop device token model loss calculated valid model loss internally left break print perplexity token perplexity added documentation fixed somehow configuration close issue,positive
note context stride might setup,neutral
going pretty good already,positive
perplexity possibly correct observe similar discrepancy suspect issue related,neutral
think also spend time trying snippet look good perhaps move forward reason ca request review,positive
thanks sorry delay busy added compatibility trainer could please review merge,negative
python import torch import true false left model balanced hey favorite condiment definitely padding true print hey well thanks keeping busy work always happy chat lately favorite condiment definitely mayonnaise versatile condiment used variety classic ingredient many mayonnaise also used base many favorite way use mayonnaise kitchen make classic salad creamy tangy flavor perfectly crisp lettuce crunchy salty also love spread rich creamy texture flavor sandwich great base like think condiment kitchen versatile flavorful touch luxury used reproducible snippet coherent good text sequence close feel free satisfied,positive
would surprising try make sure padding little possible course limited test left generation,positive
model want train hugging face hub instruction use model name model instruct model getting,neutral
best paper available made,positive
padding never thus affect model see comment close pretty much duplicate feel free satisfied answer float issue,positive
waiting make sure thing work able merge,positive
issue fixed help share reproducer make sure version included release,positive
doc torch input positional given module forward forward defined python,neutral
weird pas locally could please help,negative
yes sure fix safety,positive
could open new issue fresh reproducer output full,positive
one want review go green,negative
think could try play otherwise temperature fallback apply short form,neutral
great work thanks much patience,positive
could review first since,positive
let wait either community good model task,positive
could also left super class would like open fix think intended behaviour,positive
hi someone like guide rest process never big know probably gotten wrong assistance would greatly aim add model easily used work affiliation feel like really good model use basis expanding get model added working taken month work get far please gentle part model portion thought used conditional generation head relevant comment token added output show good quality would good fit binary classification task think might best training task add model save done though around task base model write given think might better text generation task would something would work,positive
thank raising issue unfamiliar ray looking,neutral
let close issue related bug perplexity answer,neutral
python import torch import import device model device import test test test text use parallelism sum stride range stride min may different stride last loop device give token model loss calculated valid model loss internally left break print getting taken idea model need thanks edit,positive
thanks hello visual also related automatic transformation code following think sense automatically transform tensor type vision often need feature clip blip,positive
hello share almost similar issue transformer single error got read line permission,negative
none matter length slice none included patch well,neutral
hey running far think enough memory run regression indeed fix coming better cater enough ram use anyways would always require causal mask,positive
hi officially working script various look report back issue mainly use accelerate one exposed check also think quantization compatible though perhaps confirm,positive
yes might issue official working precisely try,positive
issue even observing increase shape exact multiple number used like index position stack trace additionally freeze part model opposed leaving everything trainable error becomes line raise currently,positive
add time keep deprecate fast longer,positive
hi thanks comment something provide function would probably better check maybe could incorporate previous comment well,positive
hey yes force push made recent comment refer force push normally supposed also remove previously shown,negative
fix please run make folder commit result fix seeing ran make well,neutral
one thing fix address cross model something considering trying would definitely cause current code consider moment library model actually anything many refer model consider directory structure define map something want support case change fix instead use join instead path could imagine flexibility super useful,positive
hi thanks raising indeed breaking change fix hi thank much fix quick turnaround,positive
fix please run make folder commit result fix seeing,neutral
believe torch team aware passing way regardless,positive
proper way handle cast input model python import torch import import image import image want process open image image load image processor model processor model prepare processor either,neutral
generate sadly monolith little flexibility moment easy working code example working static cache input preparation bug likely result conflict due volume generate side quite frequently indeed sign break generate flexible structure possibly seen writing love work soon meanwhile trying reduce scope within reasonable smaller le time open merge also lighter view testing face pain surely face core maintainer written try improve strictness judgment probably fairer testing,positive
hi unrelated still able force push,positive
add test branch instead,neutral
yeah could add already hope make clear tell test go,positive
fix like batch device,neutral
want open think fine,positive
hello could review issue thanks much,positive
thanks explaining linking relevant comment something lot confusion circle back creation,positive
believe case one normally consider added part overall count otherwise perverse someone might write necessary keep understand new feature new feature affect generate work separating generation logic tested independently otherwise development start slow ca quickly add trust work reliably,positive
update issue test tested test actually test first place question need core maintainer review merge,positive
hey thank know available left implement see many included list give example one,positive
running test script setup get tensor tensor could due different hardware context running,negative
hi thank opening large library must conservative new ensure maintain considered addition feature must one following small feature negligible code footprint maintenance feature widely community ideally linking feature feature substantially certain aspect library model usage data back claim unfortunately ca fit feature moment let know situation data support feature based criterion,positive
hi broke code think might compatibility break code call never set way change logic already set library case code none false none getting set change implementation compatible change may similar implementation similar mine,negative
change parameter shape account yes absolutely,positive
hi example linked actually full llama template simplified document full llama template added note system message actually middle first user message set set else set set false message message must alternate false set content message else set content message message message,positive
note ca common deprecation,negative
extent fix may depend following question downstream broken lack lack would fix probably would fix full story came static cache removed added time would fix latest commit content shape different related fix,positive
hi curious time review,negative
try conscious full feature future nevertheless believe approach mention different set u write le atomic touch many like generate often nasty merge,negative
thanks fixing could run slow model confirm everything passing merge ran image,negative
go feature otherwise end bunch remedy something caught first place easy test either feature properly added time something else need reworked enable testing hard,positive
condition vector correctly lazy ask add test busy worry get,negative
hear overzealous testing property ca yet trigger would build mock class sole purpose testing property worse testing property usage,negative
agreed make sure something like let people know,positive
hi show latency target model default bottom graph target size greater note use routine subsequence matching faster bash,positive
hi investigating clean give example parameter form experiment custom code use structure like check people hugging face exactly intended behaviour,positive
still follow understand current return true false new added case fact sure know correctly,positive
generate simply yet beam search heavy tensor heavy difference passing flag generate small passing generate restrict ability fully compile generate decide go path reason,negative
problem forward generate forward would see generate input tensor arbitrary length really least generate simply real behavior padding user may lead due behavior obvious one foresee input length set output length fair enough think warning could shown generate case model feature could document usage,positive
hi thanks raising indeed breaking change fix,positive
also big fan problem forward generate forward would see generate input tensor arbitrary length real behavior padding user may lead due behavior obvious one foresee input length set output length harder discover still think preferable,negative
additional test request finished yet possible time sequence base equal add stopping criterion enable u test forget include test case,negative
bad key point first time saw issue library last line file line forward try latest version issue open issue,positive
nan question remains see original thread come strategy reproduce original opt show recent free happy include change recent otherwise rather play safe disruptive,positive
hello reproduce following code import torch import import image import image want process open image image load image processor model processor model prepare processor get model model get following error forward self none else return self return type ignore else return self self return try forward self none else self return type ignore else return self self return try forward self forward self shape width grid grid self return type ignore else return self self return try forward self input forward self input tensor tensor return input class self input weight bias weight bias return input weight bias input type weight type input tensor weight dense tensor guess common problem many,positive
hi failing issue right fixed soon point able rebase documentation issue runner failing code load sharded work enable either remove point smaller model somewhere,positive
yes want trace model without input load model argument eager error raised note due following see reference,negative
problem still node training,neutral
quick update actually general method want problem prefix class prefix default ca figure make stop think use might need method always fixed token calling something stop behaviour,positive
let let go stale passing available,negative
hi happy merge conditional answer following question yes preferably backed data found significant flag added original issue author little option want add new unless result clear,positive
rope also work fine cross attention,positive
running confirm additional failing llama gemma running main,positive
trying fix merge side may see test,neutral
happy change think simply documentation current usage python model eager model eager following python import eager model also able pas constructor ultimately agree future way obey currently exposed eager work,positive
install library pip install,neutral
working solution found confident,positive
think quite right someone pas model foo also able pas constructor pas model foo model,positive
great thanks failing two need get passing failing onto main resolve quality running make pushing resolve,positive
thank providing information hear issue resolved,neutral
hello thank extremely sorry apologize got however issue fixed,negative
ghost note also way would allow use fa case custom used,neutral
thanks could extend use repository sure could point find failing without ran quality locally,positive
could help clarify removal comment image probably removed mistake still fa also add warning setting false like counterpart,negative
hi thanks issue readability feel free copy paste code issue conversation reproducible script error instance example script hash none seem pas,positive
awesome right saying faster know next question going ask correctly different splitting behaviour,positive
thank valid model yes way model hand model use model sorry understand question identify model model,negative
hi thanks issue check hub discussion exact topic default vocabulary size several also added model generate anything think snippet trying join none input let know,positive
hi thanks opening context could share snippet reproduce error correct happening model loaded,positive
hi could open new issue linking issue way better track new issue,positive
hi without context hard know going example cell first image run cell enter token error see second image see cell second image running pip python command run command cell need add prefix pip install torch sure error like able find necessary python suggest notebook fresh environment running pip command first cell notebook login second cell see work first independent code advise tool take directly rather phone make image lot clearer even better would share notebook directly would provide context necessary information able try help,positive
going close issue noted,neutral
sadly unsure training inference remember context length wo issue think however precision even inference sadly context definitely essentially last position ie whilst correct float,negative
hello able llama refer model ca fully around memory,positive
hey confirm based issue done fixed work hop help thanks,positive
hi need help test resolve first failing trying git clone related also sure fix regarding test failure skip model also unrelated port model create another create draft like also model architecture mind ported instead thank,positive
anyone run corresponding export token token secret work fine secret,negative
forgot add certain break faster inference hopefully,positive
confirmed loss normal token yes,positive
hello thank raising issue default model loaded call model sharded used initialize new lead return error rank assert rank rank true none true overcome used model zero element however head head sharded error expanded size tensor must match size dimension target size tensor size overcome need use context manager rank layer able copy head new please refer usage example finally resolve issue enable saving instead whole model via need following choose save save,negative
guess problem arose quantization mixed accelerate work properly working single found default accelerate set trying use removing default work fine guess reason,positive
confirmed loss normal token,positive
nice job ca read unfortunately ca review content know community would able review work could please help sure happy help checked translation completely reasonable,positive
nice job ca read unfortunately ca review content know community would able review work could please help,positive
typo part meant cause running prediction understand idea crash reality logic breaking change stop working previously work sorry understand working test wo work keep code work remove code give example show impact added warning remind use device support hi mind look comment,negative
hi yeah documentation available add thanks advance hey need help reply directly view id,positive
use class call complete working example summary python python version pip install pip install pip install latest import import import partial import generator import lightning import import torch import import import import import class self generator none none none lightning used call yield medium franchise produced toy company toy company primarily heroic villainous two alien robot war transform franchise animation comic video trillion billion revenue class self data data self return self index return index class self super none none self none return keep object alive model lora model self return self batch prefix batch prefix return self batch return batch self batch return batch train strategy trainer none model collate partial model train like coming multiple zero sure library need solve lightning causing lot trouble,positive
simple reproducer import import import image import image processor test processor image print test import test note image time directly indirectly cache example get see improvement,positive
warning similar previously warning try use pipeline setup following package code snippet issue import torch import import import class self self return self text return text text pipe batch enumerate pipe batch return model model pipeline configuration pipeline set true top whose cumulative probability top instead maximum number generate repetition output setup data text text text text data process pipeline,positive
work node deferent storage system use file system useless new problem non global rank may global rank rename,negative
eager address issue believe might beneficial implement code snippet similar one provided use pipeline helper import pipeline change language input language desired output pipe pipeline however unsure exact location source code make update code snippet could please provide guidance matter thank,positive
change parameter shape account,neutral
update trying use batch size first batch size epoch batch size currently official way run script time batch size number,positive
made necessary address issue could someone please review pull request convenience feedback greatly thank,positive
import code false used code install torch pip install torch,negative
sure unit failing seem unrelated change,positive
fix architecture llama yeah think,neutral
base model try talking instruct model python import torch import model auto question print print,negative
solution error class exist currently,neutral
thank case someone else upon work resize token smart way,positive
hello take one else taken already open,neutral
correct help need point right location hey green,positive
hello take one else taken,neutral
alright let add unrelated change unrelated please review test error running git clone git exit status related,neutral
quick update remove work,positive
check documentation update output,neutral
great work thanks translation read unable review content doc would happen know would interested work please take look available,positive
thanks translation would super nice could find reader review content please take look available,positive
issue fixed still facing issue even fresh release source,positive
hi sorry wait fixed note ca get information peak memory since exist yet,negative
setting model saved converted update still done full precision additionally setting speed expense precision since ultimately saved anyway effect accuracy,positive
hi thank attempt fixing issue tried latest based latest commit run following script loading model still error training script output summary python python version pip install pip install latest import import partial import lightning import import torch import import import import medium franchise produced toy company toy company primarily heroic villainous two alien robot war transform franchise animation comic video trillion billion revenue class self data data self return self index return index class self super none none self none return keep object alive model lora model self return self batch prefix batch prefix return self batch return batch self batch return batch train strategy trainer none model collate partial model train summary setting auto detect available true used true available false available false available false special added vocabulary make sure associated word trained distributed member setting auto detect special added vocabulary make sure associated word trained distributed member true stage true true true true false false false false false true loading loading model model dropout linear linear linear dense linear linear linear dropout linear recent call last file line return super name defer logic file line raise type self object attribute name object attribute handling exception another exception recent call last file line return super name defer logic file line raise type self object attribute name object attribute handling exception another exception recent call last file line return super name defer logic file line raise type self object attribute name object attribute go recursion depth,positive
hi thanks raising issue going king want able create empty model use accelerate utility python accelerate import thank valid model way model hand model use model chance look one,positive
quick update added small cache case repeatedly call generate time local testing also core call stop tested parallel big testing stop string may still end going solution make sure method obvious performance first testing also still deep graph era,positive
sure used without least prior assert like bad idea period try replace back make lot sense either,negative
happy take send elaborate reproduce bug,positive
think related let one python taking consideration dot pretty sure last problem encounter anything patch stable stress done far let handle one,positive
also bug handling likely cause fixed list fix hopefully make work small,negative
understand le sec pas model predict everything one go default mode new trying understand deal similar question generating fallback apply automatically try different beam search helping trying wrap ahead around fallback associated apply fallback,positive
short example failing even need return local variable assignment python import torch import import eager class self super forward self return batch batch batch return model model,positive
added warning user check free memory may case auto would cause saving fail otherwise without clear reason tested recent exotic everything good,positive
fixing remote dot name local custom architecture example remote custom architecture defined fall category work library local possible solution also look,negative
already latest commit work would expect require model current directory accident maybe throw exception ever model path like start rely fact fail cryptic error edge like,neutral
also tried older version library work well problem need use python work create error image,positive
thank try fix minor place code directory,negative
could try library main branch try pip install also command python import wide difference,positive
thank curiosity long patch release take,negative
hi thanks suggesting available,positive
think good merge checked docker image built correctly torch currently facing login edit image built successfully,positive
edit read believe removing signature important change look current much simpler although previous version also introduce working exception raised input tensor missing main previous commit still logic model standardize cache match static cache summary performance still degradation throughput specific modeling code intuitive agree would appropriate may used without cache precise variable help u long run move generate explicit easier maintain le error prone operation input dependent add complexity strongly disagree redundant result bug full derive like giving one set valid given first two give three different get wrong result add safety warning pad token input attention mask raising exception cache raise exception attention mask right type warning attention mask pad token missing generate plus even much work correct top like generate pipeline create correct logic anyways create mask create position push logic prevent everywhere goal iron future roll better need interface without learn prepare new input tensor use static complexity always exist difference whether outside generate get wrong one argument generate inside generate maintain logic get custom generation wrong one argument forward inside forward maintain logic experience input used incorrectly used incorrectly one strong strive keep direction think get rid would breaking change many due remember point also trying get rid performance see negligible eager forward none static forward generate resulting net performance,positive
test previous directly load arbitrary path instead current directory work think change ready consideration,positive
version accelerate transformer fix issue still error issue still use error trial task trial recent call last file line result future file line return file line wrapper return file line get raise ray file line train raise file line ray object file line raise file line file line model file line initialize engine file line model file line file line wrapper return file line raise please use model since model already set correct correct,neutral
confirm shrunk export ideally would overhead think ton,positive
issue still perplexity much worse near random different token perplexity large value sure reason problem implementation might mismatch,negative
already feature request tagged,neutral
careful investigation turn extra flag added due new method removing list since irrelevant pull request,negative
latest commit based necessary additional safe also also ensure equivalent loading added slow since lot whole thing longer run checked already many tend good reason,positive
issue program fail save trainer,negative
added demonstrate see key code version unlike version,neutral
going merge done might conflict easy resolve ping otherwise,positive
hi triggered code style fix run pip install quality make get error many make manually run make style make fix everything,positive
similar error issue distributed training environment file system across node training launcher setup cleanup command code execute due condition met slave intended clean staging always work python file line file line file directory file directory file directory file directory file directory file directory file directory file directory file directory file directory file directory file directory file directory file directory python go process rotating main process e else try except exception error rename folder folder training proceed ensure rename atomic happen based maybe delete older solely rely numerical id rotation reliable especially fuse cloud clean staging although used verification exist executed like block need well python try except exception error delete folder,positive
everything actually single node accelerate also gradient accumulation could cause problem,negative
would add list look issue collaborate,neutral
quick glance initial implementation ref actually inclusive got lost also inclusive function think behaviour make sense,positive
thanks getting back think fine,positive
wow thanks great work quick glance took care well copy mechanism quite challenge please find script suggest try large sequence length let u know go hey work problem use switch believe used least seen,positive
think one way fix would python none diagonal mask,neutral
default implementation checked make sure eager initially thought may made obviously used tracked think one error,positive
thanks indeed issue fixed tracked could thanks,positive
hey solution python model auto padding train model,neutral
honest unused vocabulary instead would use already quite huge change pretty easy directly replace model never saw would better update really want increase size tutorial one best curious otherwise,positive
sure would make sense let least breaking warning,positive
got thanks suggestion try use deprecation strategy add test,positive
ran locally gemma llama gemma llama got bunch sure related probably environment bash second set first trying whole model disk please use function instead trying whole model disk please use function instead second set first second set first trying whole model disk please use function instead trying whole model disk please use function instead second set first set also error related,positive
part issue llama see,neutral
shouldnt closed still linking wrong one,negative
feel free share solution found,positive
urgent already done save loaded,neutral
pretty important still failing,positive
hey pretty sure support sliding window yet,positive
hi one add back see build documentation failure fixed,negative
specific modeling code intuitive thing think remains strictly optional argument model work without,neutral
alright actual patch release think important,positive
sorry could push model hub one trained need full training loop,negative
think found additional add another,neutral
hi guide fix thank,neutral
seeing well without lora unused special tried replace token one unused token id avoid still high loss great idea weird still like special token instruction start token however gemma use token could instead use built chat template idea make use new work flawlessly llama mistral phi gemma yes could use chat template would,positive
seeing well without lora unused special tried replace token one unused token id avoid high loss image like special token instruction start token however gemma use token could instead use built chat template,positive
thanks confirm one last point could,positive
thanks reply working axolotl example arose issue working new virtual environment latest package meaning quantization error raised would mind check repository run bash accelerate launch,positive
work torch python import torch import model auto output model print make cake output print output mandatory since scratch space allocation need check believe requirement well,neutral
hi faced similar issue maximum value tensor must smaller input dimension added new token model layer moreover made sure correctly new token layer size actually new pretty sure model getting would great dynamically based shape,positive
hi update accelerate work pip install,neutral
quick update passing exception also quantization spot like quantization however user use need install main branch accelerate,positive
like llama insert system message first user message seen order always also llama whenever role system see example jinja template order found one case might slightly different loop separately true think exactly way people help right loop message separately set according role message ideal would get worse done currently advantage would would take care instead every individual part,positive
got import name version downgrade get working thanks,positive
hi finished able working version need work also apply feedback received time ago would like open new revision,positive
would really appreciate review translation thank,positive
thanks could review time like update latest possible,positive
thanks fixing first part source commit still running second issue code snippet file line return parent file line file line run file line step self file line assert module attribute export user code file line forward file line return file line forward file line return file line forward file line return file line forward co sin file line return file line forward co set information,positive
hey added sliding window param look thanks,positive
speak everything correct work right important language thanks,positive
something like could work several edge firstly introduce additional case might slightly different loop separately secondly like llama insert system message first user message ca safely assume match output,positive
fixed least first part hit issue co maybe commit try main,positive
thank quick patch locally full graph dynamo export sure issue still could share set,positive
true see slow test fa eager possible global test though check,positive
model well sliding window structure weird model code went wrong modification passing sliding window param attention mask fix issue idea sliding window mask worked maybe strict numerical issue short test summary false true false true false true false true false true false true false true,negative
fix issue last working right understood meant time verify remove previous test case change,positive
actually possible initialize custom code pipeline associated model yes model nice example code another use check python import pipeline pipe pipeline revision pipe mask mean model normal model pipeline custom pipeline yes done mean custom model custom pipeline try yet generate work want addition issue think sense theory practice custom seem tightly coupled custom therefore sure sense custom independently really need minor enhancement field also always dealing custom refer example library whole consistent enough working custom maybe work every issue independently one step time,positive
hi issue accelerate library need update issue handled properly pas work,neutral
hi believe linked alone need latest version finally import path need,positive
probably easiest way open issue ping need discus spend time,neutral
found issue fix issue anyone explain instead model model auto accelerate,neutral
issue propose fix found issue need fix field example ai model stuff meaning look like type image instead type image also considered issue since latter need minor extra check test pushing new registered pipeline model came know pretty specific condition ruff estimation latter solution since need fix field issue field hope,positive
actually getting collected skipping run always,neutral
try time place chat like policy want submit good understand put time thread discord regarding another got,positive
issue likely corrupted remove cache,neutral
hey issue associated fix actually familiar custom pipeline code summary going current situation right guide custom written big pipeline ago guide custom custom custom give full model relative path inside current designed custom pipeline tightly connected custom model way custom pipeline independently except save call bunch manually fix correctly point model instead assuming model always pipeline add method pipeline directly hub rather part custom model stuff understand yet right really way initialize custom pipeline independently model according way get remote code pipeline initialize associated model python import pipeline classifier pipeline therefore bit confused actually possible initialize custom code pipeline associated model addition issue think sense theory practice custom seem tightly coupled custom therefore sure sense custom independently bit call help sure clarify intention custom pipeline code possible custom separately core well even though big think touch fundamental conceptual model think lot sense allow write custom point arbitrary even however custom pipeline code right think lot code touched long time therefore think might need fundamental change custom want make work properly however might wrong still library fully understand,positive
since vacation sorry inconvenience,negative
might worth raising issue warning new setting used raise ensure override whatever unless design,positive
graph generation point prefix hello name normal hello name year old college student outgoing person love fun active easy going love make graph hello name year old college student outgoing person love fun active hard worker stupid solution would generate shorter sure good idea unstable might added graph generation test,positive
getting similar error model latest version code import model prompt prompt prompt print print generate response decode response response return response prompt hey need new one buy response prompt print response package also tried work either besides check look like release branch branch error get quantization state please call device layer first recent call last cell line prompt hey need new one buy response prompt print response cell line prompt print generate response decode response response file return file self streamer architecture used correct generation please set model added prepare used generation file self true return file self return type ignore else return file self want skip rest logic function call forward return try result none file output else output return module output file self none always none gradient else position bias position bias false file self return type ignore else return file self want skip rest logic function call forward return try result none file output else output return module output file self else none none keep relative position file self return type ignore else return file self want skip rest logic function call forward return try result none file output else output return module output file self forward self add output file self return type ignore else return file self want skip rest logic function call forward return try result none file output else output return module output file self mask return get query shape get project none else none file self return type ignore else return file self want skip rest logic function call forward return try result none file output else output return module output file self bias none none else return file bias tensor tensor tensor none assert none false,positive
issue training different issue use fixed line,positive
issue unexpectedly tensor instead float issue training already call item fix,positive
quick update one set calling get like attention mask add batch support soon automatic see figure something,positive
guess choice choice would python exist one possibility always matter one possibility use drop support attention one possibility move causal mask logic outside modeling code,neutral
met please install torch want use,neutral
almost forgot fix remote pipeline configuration remote field leaving final configuration inconsistency since related instead extra unnecessary remote maybe add else pipeline original model ruff estimation code file could like python self let know approve,negative
hey getting another error torch python configuration model model model model model recent call last cell line model raise met please install torch met please install torch,neutral
finally fixed happy dance leave part configuration highlight need fixed also consistent enough according always people push pipeline model tricky suggestion leave configuration open issue assuming model another remote better assuming one pushing messing configuration much,positive
thank issue fix sense would great open fix already,positive
would much check new pull request help,positive
hi thanks issue send small reproducer issue look,negative
awesome feel free open,positive
alright wait bit still draft,neutral
lot gather data function see done,neutral
also dummy hidden head case studied,negative
work fine notebook test based test script shall try run,positive
look automatically test eager eager fa currently done basis model whisper,neutral
hey change today tomorrow setup cache instead moving around inference support could share minimal reproducer code run code copy branch python import import auto model import torch model none none none none none,negative
thanks combine quantization coming soon,positive
thanks sorry sent wrong link something like python else issue,negative
basically got rid tensor memory allocation already slow way cope right padding padding right fully non none condition triggered update entire mask le costly anything done,positive
interesting actually script static shape custom generate one posted used work well example script use new static cache flow love try,positive
use current avoid none none none none none none none,neutral
error get main quite simple recent call last cell line return file self layer file self name name return name raise type self object attribute name object attribute believe would make sense source device outside linear layer,positive
hey inference optimization tech suggest low memory usage tried optimum support model,positive
pip install work pip install error,neutral
also want fast generation like actually fast would recommend static cache,positive
agree might optimal welcome significant slow padding course causal mask registered end casting different torch cast another one brittle profiler indicate min taking time speed would nice sure produce,positive
hey change today tomorrow setup cache instead moving around inference support could share minimal reproducer code run,negative
flash attention unfortunately please consider memory efficient training inference,negative
hi thanks think accelerate corner case automatically device map try set custom map something like pas,positive
hi flash attention support ampere,neutral
hey getting error python set text generation pipeline result primary underlying antibiotic resistance develop combat print result set false however temperature set flag used generation set unset temperature recent call last cell line result primary underlying antibiotic resistance develop combat print result causal lambda else ampere,positive
hi alright error flash attention attribute fixed issue thanks,positive
already good support wondering whether would make sense support generation directly seen user request,positive
hi model update model soon clear see since poked last time could check make sure everything right let know anything improve thanks,positive
let make sure green fixed code quality issue,positive
thanks lot best recommendation would first share model hub community interest start working addition model hub easiest way reach prepared everything share model model space model gain popularity among many people,positive
hello one fine generate speed someone like used bad speed training original mamba generate code lead nan error last token pad see code necessary test set different token sometimes lead error,positive
branch output short test summary state contain state contain state state contain state state state contain state state contain state state contain main branch output short test summary state contain state contain state state contain state state state contain state state contain state state contain,positive
please run pip install testing,neutral
hi one doubt complete bit quantization possible export model form file like bit quantization aware lora export model format run another bit device thanks,positive
problem trick python tee work worked,neutral
use another new file code work,positive
hi lora working fine mainly following notebook given bit receive following error need help attached error trace recent call last cell line file self trial else return file self trial model model loss nan simply add average previous logged file self model else loss return file self loss return none loss else file self gradient self return self self gradient file backward reason repeat comment python print first line function print last line engine run backward pas file self raise custom function implement one else return self file print print state state else size tensor must match size tensor dimension hi running code fine tuning taking place without error change information receive following error trace recent call last cell line file self trial else return file self trial model model loss nan simply add average previous logged file self model else loss return file self loss return none loss else file self gradient self return self self gradient file backward reason repeat comment python print first line function print last line engine run backward pas file self raise custom function implement one else return self file print print state state else size tensor must match size tensor dimension useful thanks thanks,positive
interact python interpreter load model stuck,neutral
let make sure green,positive
test handle pard sorry time take clear look like nice feature addition,positive
could try work think patch release,neutral
open fix right registered,positive
good catch wrong path work well thanks help,positive
related torch version python file line forward shape invalid input size wrong version shipped,negative
thanks running confirming torch version,positive
hi thanks raising issue need pas python import import import main create model model model main thanks lot,positive
python model model linear linear linear linear linear linear linear norm linear double checked eager,neutral
image sorry reproduce release branch main,negative
interesting work fine script logical make sense issue specific,positive
thanks kind reply think made mistake different fixed sorry wasting time,positive
tried main branch still running issue minimal script python import torch import model auto eager prompt upon time prompt output output enumerate print sequence torch version,positive
fixed main made release,positive
guess choice wait already issue,neutral
guessing prevent thinking listening another tool audio certain volume ignore way audio,positive
thanks catch real pity prior test catch,positive
share minimal code snippet reproduce issue,negative
thanks curiosity issue prediction,positive
yes problem prediction seem correct da yin wrote hi everyone got response sure reply better please feel free update perfectly problem file issue reply directly view id,positive
instead loading like respective save folder write location folder name thats approach worked every file except ca swear every single one leave trial error determine minimal set exercise reader,negative
thanks yeah bit since optimum store work fine got clip,positive
hi everyone got response sure reply better please feel free update file issue note bug may correct,positive
confirm running error tried following master without similar stack trace file line forward file line return file line return file line forward file line return file line return file line output file line forward file line return file line return file line output file line forward shape invalid input size,neutral
possible breaking change latest release,positive
issue cluster seeing issue notebook weird maybe thing,negative
tried rel running error dynamo module attribute export full graph dynamo export regression get dynamo export work two guess end different set error co sin,positive
correct help need point right location,positive
want act text issue change file hub see difference reproduce model hub code import torch import import import model device else model device text return else return text predict function model example example device prediction prediction prediction print prediction return prediction load test set generate output evaluation lambda model sample tab loading target median first sex first marriage birth first child among men age residence age first sex urban first marriage urban birth first child urban first sex rural first marriage rural birth first child rural first sex urban first marriage urban birth first child urban first sex rural first marriage rural birth first child rural first sex urban first marriage urban birth first child urban first sex rural first marriage rural birth first child rural first sex urban first marriage urban birth first child urban first sex rural first marriage rural birth first child rural first sex urban first marriage urban birth first child urban first sex rural first marriage rural birth first child rural first sex urban first marriage urban birth first child urban first sex rural first marriage rural birth first child rural first sex urban first marriage urban birth first child urban first sex rural first marriage rural birth first child rural first sex urban first marriage urban birth first child urban first sex rural first marriage rural birth first child rural number rural urban common across family formation among men age timing first sex timing first marriage timing birth first child timing first sex timing first marriage timing birth first child timing first sex typical timing first marriage typical timing birth first child typical timing first sex typical timing first marriage typical timing birth first child typical timing first sex later timing first marriage later timing birth first child later timing first sex later timing first marriage later timing birth first child later first diagram figure men experienced first sexual intercourse typical timing family formation among men age timing first sex typical later timing first marriage timing birth first child timing first sex timing first marriage timing birth first child timing first sex typical typical typical typical later typical timing first marriage typical typical typical typical later typical timing birth first child typical timing first sex typical timing first marriage typical timing birth first child typical timing first sex later later typical later later later timing first marriage later typical later later later timing birth first child later timing first sex later timing first marriage later timing birth first child later first diagram figure men experienced first sexual intercourse typical timing youth empowerment among age country high mali medium mali low mali high medium low high medium low high medium low high medium low high medium low high medium low high medium low high medium low high medium low mere young high empowerment mali ownership house land percent men age home alone jointly men home alone jointly land alone jointly men land alone jointly house either alone jointly land toward wife beating percent men believe husband beating wife following wife food men wife food wife men wife wife go without telling men wife go without telling wife men wife wife sex men wife sex men men agree husband beating wife least one food go without telling sex employment among working percent year occasional employed family member employed member cash cash percent distribution employed age type earnings employer according type employment agricultural total fertility rate total fertility rate data indicate declined high per woman late late dropping last half contraceptive use percentage currently married method percent substantial increase contraceptive use since late percent married percent percentage currently married whose least one wife percent urban rural central coast eastern nyanza rift valley western north eastern education primary incomplete primary complete thirteen percent currently married live polygynous one status percent unwanted later percent unwanted percent later receipt antenatal care skilled medical provider percentage live birth past data indicate rise since medical antenatal care coverage percentage age specific percent polio polio polio measles basic percent age fully time survey infant young child feeding percent total much likely fed accordance number participate percent number percent distribution currently married according number participate infant mortality rate preceding survey selected demographic per live male female true mortality exception age group infant mortality higher age older mother duration stay health facility giving birth percentage vaginal birth vaginal birth vaginal birth day vaginal birth day vaginal birth birth birth birth day birth day birth percent distribution gave birth health facility five preceding survey duration stay facility type delivery percentage age specific percentage time survey polio polio polio polio measles percentage age received various source information vaccination card mother report vaccination coverage among age percentage polio polio polio measles measles measles basic basic basic proportion received none six basic declined marginally percentage level breastfeeding status percentage exclusive breastfeeding age exclusive breastfeeding age continued breastfeeding year introduction solid semisolid soft continued breastfeeding breastfeeding predominant breastfeeding bottle feeding key breastfeeding among age living ownership access use percent percent least one percent least one every two stayed household night interview percent household population access within household percent household population slept percent least one age first sexual intercourse percent percentage sexual intercourse exact age percentage sexual intercourse exact age percentage men sexual intercourse exact age percentage men sexual intercourse exact age percentage sexual intercourse exact age percentage sexual intercourse exact age percentage men sexual intercourse exact age percentage men sexual intercourse exact age among young practically change proportion sexual intercourse age age period two current use method currently traditional method modern method female sterilization injectable pill revealed percent currently married currently contraceptive method treatment among ill diarrhea percent health provider given oral rehydration solution given antibiotic table majority ill diarrhea feeding conform,positive
ran slow everything fine stopping give true false since main goal adapt stop believe better add test,positive
like pip list want upgrade pip install,neutral
hit export time might test may run different set,neutral
problem flax use argument believe problem suggest u let know feature,neutral
general lowering precision significant impact downstream performance take person initially added upcast meta since memory reduction add flag still code feel free fork keep dont think true experimented lot mistral fused cast little impact alternative memory saving tracked like something,positive
awesome work thanks work tiny please mark resolved think suggestion applied comment comment marking main thing left update make sure model slow pas done ready merge mean move appropriate one conversion script,positive
hi thanks opening understanding deliberately exposed use internal discussion bit surprising work auto like pro know added originally,positive
hi huh funny code run bit weird error bit quantization two version running device running,negative
make sure running latest version issue need script reproduce issue,positive
hi close issue could please point implementation done really appreciate throughout use either import import custom class defined logging,positive
fixed issue test compare generation hood linked root issue though going close move discussion slack future time,positive
hi running notebook login calling trainer passing token interactive text box running,neutral
great thanks would join discord see community would interested translation also try well,positive
statement custom implementation patch open source chance mention fa finally probably help make,neutral
sure try take look one pipeline one,positive
hi able run get slack notification see job help,positive
could review one fixed update branch,positive
hey running script getting another exception latest accelerate still getting exception accelerate python pip install pip install pip install pip install pip install pip install pip install pip install optimum pip install pip install pip install accelerate import torch torch import import device else configuration model recent call last cell line model self self raise quantization accelerate pip install accelerate latest version pip install quantization accelerate pip install accelerate latest version pip install note import failing due missing package manually install either pip apt view common click open button,positive
think broken another main branch work review,negative
indeed like issue like snippet work new cache class suspect flash attention may solve issue hi thanks quick response confirm removing flash attention issue however removing long take much memory get know issue yes please help file change update file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line raise cache self access layer index access layer index,positive
hi thanks raising issue weird able run snippet without issue getting access environment running code python session notebook python session recommend logging first login make sure token available environment need pas logging session import login login notebook try import let know still issue,positive
hi definitely think notebook able run code documentation notebook sure understand question however question best try reserve feature bug regarding documentation add model add model hub,positive
hi tried version accelerate environment error message,neutral
error triggered buffer mechanism need set avoid python model model,neutral
mean come time really nice work well deserved hug,positive
solution set python model model,neutral
issue seem occur model guess solution would update package,neutral
thanks getting back try end get back,positive
able test script provided worked assuming something trainer,positive
hello already fixed ran today confirm training work,positive
pull request opening cleaner one push custom pipeline hub already registered pipeline architecture hub add since know test push pipeline yes custom pipeline code change library reason custom ai include pipeline easy way push code hub thought feature really make since keep library people create repository push custom hope reconsider feature additional method include method,positive
hi giving trying code train model python trainer trainer train validation facing error token token found need provide token logged hugging face login see thought apply token notebook like python import help please,neutral
install source upgrade upgrade torch avoid error option could enable based solution change result different loss change result different loss,neutral
thank awesome yes think need alignment point different getting bit complex different end day cherry release removed decorator reason believe bug somehow second pas generate call latency per token generate call latency per token generate call latency per token generate call latency per token generate call latency per token static would like know get copy medical primary care would like know get copy medical primary care copy medical primary care physician straightforward process important follow proper ensure receive complete accurate copy general take contact today feeling grateful opportunity explore beautiful city always visit finally even beautiful tower stunning incredible food delicious soaking every moment making time ca wait see rest trip store grateful,positive
working added cache generate might get merge,neutral
hi thanks raising issue without information error mean work behaviour wo able help snippet entirely clear code run two separate separate import,positive
leaving open would like avoid keep compatibility likely bug ca simply remove,neutral
hi thanks opening new model request open add mamba issue,positive
hi also facing issue code gave work well system however get object attribute running prediction model also running work fine error trying perform prediction local copy model python text device model loaded python also loading saved local copy model python model device hope information provided enough thanks advance,positive
set hub pas sure difference fine model may cause model tried flag instead maybe depending use case former input text later text,positive
correct option flax open issue please mention unsupported given framework raise warning raising,neutral
hi change infra team docker hub stuff repository docker hub first push ask,positive
python according failing torch let wait tad bit full graph,positive
think warning shown added support code hybrid version used default model,neutral
might rope issue might also play,neutral
hey discovered change actually training produce loss nan sure til change,positive
confirm build push work fine token issue use except token,positive
quite strange indeed able login push,positive
hi thanks getting back tried run main python import torch import model print model confirm able load try script work end check script see,positive
running server many deletion affect,positive
never program ca help trainer used require file used trainer know perhaps require file longer case defer current project case suggest open new issue issue old,positive
thanks however know add configuration variable trainer type know pas configuration method could please give,positive
hi thanks quick response minimal code see code data training cat get result process without parameter pip install pip install pip install pip install code import import o import import import import import import import import import import import loading data text clean text text empty string removing empty line line line return clean col col col return train test train test example every dialogue saving input model doc doc example use convert input easily understood model parameter maximum number defined parameter setup target variable expect much shorter text example alongside return loading model loading metric text list true true possible masked token rouge sentence label rouge score key key value add length result return round training testing,positive
yes try thank reply,neutral
hey thanks opening issue first custom local trust remote code fast feed slow indeed inconsistent would like open fix,positive
via job run link,neutral
actually ran line edit without flag memory going memory meanwhile fa current think tried variation everything work though,neutral
typo part meant cause running prediction understand idea crash reality logic breaking change stop working previously work sorry understand working test wo work keep code work remove code give example show impact added warning remind use device support,negative
think full reading try instead,positive
currently script tried used argument,neutral
ah yes sorry part share full snippet,negative
sorry think said meant say properly used git pull get latest onto transformer saved pip pip install saw commit thought right however try pip install try python line,positive
sure currently working fixing providing working run box review,positive
hi need use source pip install,neutral
hey thanks response starting training run got latest source saw commit tried pas gave currently error wish still error try training run little le hour however tried pas flag telling,positive
hi use interface install make sure load attention passing kernel use fa perhaps fix instability respect fa,positive
think come call yes would like open open later week,neutral
testing seem matter full fine tune lora fine tune fa could never get loss even consistent full fine tune right phi loss consistent consistently going single epoch tried lora maybe wan na use fa better stick lora,positive
hi thanks feature request already ongoing work add,positive
could please add notebook template would helpful hugging face team put tutorial better also custom architecture feed forward layer multiple project back dimension tested several model order magnitude performance respect number hence deploy edge issue far know use head liner layer hugging face library automatically put call certain class example since model final liner layer project multiple final linear layer,positive
would nice look well,positive
gave hope mac compatible point quantization would really speed hope working,positive
hi thanks reply feedback bit experience especially cloud see previous progress incorrect width terminal primary reason wanting implement would assumed rich mature feel free close ticket deal internally could please point implementation done really appreciate best,positive
hi thanks opening feature request previous internally rich conclusion stick moment past accelerate particular different running writing file large would mean conditional logic would still fallback want keep optional small possible rich would going moment,positive
hi one option update warning would mean would see per python session would like open make change way get contribution thanks taking time reply rather logging like option disable entirely ideally warning printed rely,positive
hi one option update warning would mean would see per python session would like open make change way get contribution,negative
right turn uncommitted certain rebase need practice bit future passing,positive
hi thanks opening issue glad hear journey mostly successful seen documentation page custom contain example code get let u know anything work,positive
yep gon na work even change different,neutral
yes totally right sorry show entire code,negative
able build image permission push image error push push access repository exist may require authorization server message authorization,positive
maybe create separate issue flax seem support option,neutral
may ask please update docker,neutral
imply aka common base class,negative
torch rather due supporting apple silicon name defined checked library source actually torch clause available guess could change library hope run later bad idea though better strategy would wait update,positive
hey way disable warning solely need torch get several warning starting get old,positive
working end import model quick brown print false still seeing issue need reproducer figure wrong set hub pas sure difference,negative
model flax model happy provide code reproduce day posted working,positive
related flax sure meant,positive
markdown link correctly issue attached reference regarding really know hugging face could please help,positive
well big rip yes agree would route code avoid calling rust fake mode,negative
ready review fixed generation work everything successfully machine,positive
fix significant manual labor update static cache sprint look,positive
rust implementation storage rust pub self name let name format file contain tensor name let name format file contain tensor name match storage let data let array python data array storage storage python let torch let torch false let torch false let intern let intern let shape let shape let start let stop let slice start stop let storage storage could find storage let storage let storage intern slice let intern let string intern let tensor torch intern intern view big let intern let torch false tensor intern intern let tensor intern tensor intern let torch false tensor intern intern tensor intern reshape shape device let device let tensor tensor intern device storage start stop shape,negative
please note took integration ask work true,positive
many current static cache generate implementation follow always keep stride decode phase generate apparently use directly variable next instead previous sliced input different stride shape loop forward generate input shape input stride shape stride loop forward generate input shape input stride shape stride loop forward generate input shape input stride shape stride loop forward generate input shape input stride shape stride loop forward generate bad stride thus recompilation triggered decode phase really necessary function forward triggered following guard failure tensor stride mismatch index actual function triggered following guard failure tensor stride mismatch index actual compile input length code believe class forward self capture every decode step recording tree key recording tree key recording tree key recording tree key recording tree key recording tree key recording tree key recording tree key recording tree key recording tree key recording tree key recording tree key recording tree key slow avoid issue graph capture first decode step avoid stateful bug main prior convince python import import torch import left model would today actually none back need set none well effect print print print finished compile call static print second call static print static image instead bug however hit believe bug added call even fix still bug properly making sure issue however change approach instead remove altogether main static cache call tensor float matrix multiplication available consider setting better performance generate call latency per token generate call latency per token generate call latency per token generate call latency per token generate call latency per token static would like know get copy medical primary care feeling grateful opportunity explore beautiful city always visit branch static cache call tensor float matrix multiplication available consider setting better performance generate call latency per token generate call latency per token generate call latency per token generate call latency per token generate call latency per token static would like know get copy medical primary care feeling grateful opportunity explore beautiful city always visit,positive
hi thanks response effect would take quite lot work measure little snippet python import torch import false model model model output tensor true main depth deterministic drop path rate used tensor false branch due stochastic depth properly,negative
working end import model quick brown print false still seeing issue need reproducer figure wrong,negative
help add configuration met following situation training found please specify without auto set correct value file pas information listed file line main file line train return file line model file line prepare result file line file line file line prefix key file line raise found please specify without auto set correct value file pas,positive
yes know course example replicate happening visualize bug actual code care padding attention code batch size one example showcase issue generally trend input sample coherent output one without padding,positive
issue sequence classification saving garbage model upon,neutral
hi please make sure tag limited set relevant people opening issue everyone busy done would able meaningfully address yet open issue track added work moment filippo working addition thanks response,positive
issue think would useful feature,positive
perfect wait release take much time please keep otherwise first merge without add afterwards,positive
hey thanks issue checked branch seeing passing fix would mind opening also since affect training script training scenario current integration training setting,positive
would certainly interested could support,positive
support python starting version away,neutral
able run python problem otherwise replace statement statement lower requirement necessary yes would best prefer keep running quantization python since actually moreover better since keep requirement low done otherwise modify use install python,positive
thanks getting back ran pip install progress made error however code line get error wrapping around recent call last module define evaluate original model type none return else none else try except raise self property load self return return self self return self self string recent call last module define evaluate original model type none return else none else try except raise self property load self return return self self return self self string hi problem kernel worked pip install pip install,positive
would also like opinion usage loss sequence classification task understanding loss designed improve training stability large language loss learning regularization trained,positive
yep fix everything wrap,neutral
might flaky well alright think torch nightly well,neutral
thanks ran server still like issue running instance let know better use,positive
error message added exact,positive
saw post tried version,neutral
hey could provide minimal reproducer would help use also note generation probably safely missing however bit problematic might tied tied properly used,negative
ran python return false true mean relative difference torch torch mean relative difference torch torch mean relative difference torch torch mean relative difference torch torch think acceptable,negative
thanks alright let make sure take everything account safe serialization unsafe bin well also use copied single idea make sure fix single place rest copy fix test copied used new test,positive
another person problem linked issue,neutral
either related base model medium basically work fine training long trained model still memory work great problem model new classification head get random tried various documentation like saving specific module whatnot success far similarly completely change random basically co final layer random,negative
hey thanks sequence classification usually try open issue feature request issue strong support community usually around example add support case first user request new class hi working classification available training done,positive
common model integration pas,negative
would say related take look medium post might helpful model perform validation set output trainer mean problem inference,negative
hi work thank contribution jump let know put prefix test commit message commit trigger full run example commit message like test check commit perfect way easy way check good test fetcher coverage speed try improve,positive
hi would useful feature plan implement indeed great feature possible approach looping calling message knowing user thereby building mask returned,positive
ah right believe force push regular push rebase fix,positive
incredibly long running context want model open open year half lot upstream particular built would need incorporated example see many necessary removed course help weird know add something model ultimately contributor responsibility finding looking moment large think able resolve find make equivalent within month suggest,positive
see see method point ca find actual code anyone point right direction defined,positive
interesting must related classification reason still making notebook demonstrate,positive
think something funny history showing coming unrelated also commit history force push necessary effectively history,positive
hi seeing error auto yet closed like add support anyone else community welcome open add,positive
sorry added wrong link description issue follow context link enable loaded new functionality basically support functionality kind another unrelated change since common test figured could good idea add support bunch well like big regression write add self model bit strange supporting bias tie yes done manually user already tied get untied loading need get loading already tied save one copy saved could sum line wrong previous behaviour need tie fail load copied used good point lot prediction copied sure marked see add back,positive
know guess would cleanliness prevent behaviour test set separate training process use training validation metric track model performance make might make sense part trainer confirm address,neutral
hi please make sure tag limited set relevant people opening issue everyone busy done would able meaningfully address yet open issue track added work moment filippo working addition,positive
support make sure merge rebase modify also return vector,positive
great main continue lot new history seeing much number ahead main rather number top certain main commit let say rebase top latest main commit say kind like shifting along history need rebase every time new main purpose make sure branch main diverge made file clear order apply well important upstream bug basically merge commit lot cleaner shown guess better see bunch force push pull request list main right yep know rewrite history important also used branch often suggest manipulation short lived guess apply mine since working couple right touch lot common want regularly particularly final getting ready merge branch least day model probably need quite often done difficult rebase rest pretty easy fact image would little like live short possible le true model still would like resolved order wherever possible anyway ready move last review process great next step get passing,positive
good thanks taking care rollback go come single everything one,positive
good thanks taking care,positive
good agree approach currently eta might take bit long also depend community ask,positive
soon confidence think support agree want avoid bloat much possible think better overall well defined behaviour easiest way avoid many version support latest version,positive
bash false true assert false false true failing match failing main well bash false true false true failing well main alright,negative
hey thanks issue check raise issue,positive
expert suggest try extreme speed knowledge mainly cache exist model mosaic based flash attention,positive
think always set example,neutral
look right padding attention manually supposed take care,positive
found reason fail flaky empty commit green also tried fix flaky test would love get review,negative
related issue know particular reason behind design choice use train rather option include test set wondering option would go sort design principle,negative
good happily take care make sure think better raising assertion used inference configuration rather raising warning even user set true believe stage first option preferred second scenario handled maybe setting model inference mode,positive
actually rebase main alright,positive
thanks opening work feature going revert add back next release happen tomorrow unfortunately test suite seem correctly necessary know run check want confident everything good stable state release combine work similar address current review,positive
leave open think still documentation clarification,neutral
would like open improve error,neutral
actually enforced call would like open,neutral
great glad hear mark issue closed,positive
agree let revert main reason trust testing suite moment another complete address failing believe explicitly testing importantly covered lack coverage failing run however release let err side caution,positive
hello tackle willing base model task cord sure masked image modeling right paradigm eventually need train model indic language done full flow dit lighter appreciate resource might point towards thanks,positive
glad fixed still bit recall seeing anything documentation either close reopen later something come,positive
hi without code sample replicate information running environment information error full much help,positive
thanks believe bit hard spot correct behaviour wondering always case inference turning maybe enforced,negative
hey error point issue would recommend upgrade without proper reproducer nothing,neutral
running inference set model,neutral
fixed problem making equal number,positive
german translation typo dismiss case review documentation,neutral
regarding error position tensor like input also adapt end,neutral
expert suggest try extreme speed knowledge mainly cache exist model,positive
model file line return file line return file line forward scale file line,neutral
hi everyone problem program added program import model define language choice may help recent call last cell line save self import spacy type ignore module attribute research,neutral
torch rather due supporting apple silicon,positive
alright need add integration test added,neutral
also favor original something urgent main many skip already heavy burden clean later another one test fetcher indeed ideal decide improve however decide job already done time spent anyway,positive
able run successfully cloud except,positive
think current since believe want use third test set create separate trainer object would need prepare data accelerate know gathering think know ahead time experienced gathering erroneous number support classification report showing different number personally taking better look time week time outside work,positive
hi yes code return loss tensor loss sum instead loss dictionary working fine thanks quick feedback,positive
able use bit quantization,positive
think come call yes would like open,neutral
let try clear say correct wrong wrong share generation provide snippet assume talking setup used many different combination attention eager attention providing small snippet help u understand mean element mode set attention mask zero mode attention mask generation used output model correct wrong talking generation greedy sampling reason attention bug support causal mask snippet reproducer help,negative
thank spotting missing paragraph translation left couple thank,negative
thanks review good marking ready let know work happy take stab,positive
hi quickly tried main python import pipeline unmasker pipeline unmasker hello mask model share snippet reproduce issue also tried python import model text replace text like text output model also try main,positive
hey thanks opening issue problem thus file used code yet use fix already,positive
thanks review according suggestion one concern new approach next need version check trainer make code slightly bloated though happy approach well think bad think,positive
seeing error please help support implement support model class need implement attribute,neutral
hi thanks much issue model maintain try without think bit work box model,positive
alright significant slow ca naive dynamic generation script probably gave unsqueezed python file line module model true file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward co sin file line return file line return file line forward none many index tensor dimension,positive
update issue anyone fixed,positive
run make sure everything alright otherwise good,positive
image case relevant listed git branch,positive
length may make training process change problem code create tensor size many input local worker therefore tensor length sum total number great device decided according,positive
holy easy think understood mechanism branch include latest ahead main fine tomorrow morning new main guess guess better see bunch force push pull request list main right often suggest manipulation short lived guess apply mine since working couple right anyway ready move last review process,positive
please share simple testing code look issue,neutral
one closed added final issue really fit official inconsistent entire library instead added classification head possible repurpose sequence classification well ended cleaning pushing code onto model hub need fine tuning easily accessible model code also available feel free play around let know think thanks everyone help support issue,positive
checked test pas thanks,positive
tested flash attention work great without flash attention flash attention get lot form operator block thread assertion index lower chunk stack trace posted file line forward file line return file line return file line forward file line file line file line apply return super type ignore file line forward return error assert triggered would great able use fa much sequence length fa like perfect accompaniment training,positive
yes fixed weight issue fix one getting difference compute,positive
meant change currently broken grateful could give look next couple day thanks added skip failing time discus change feel free whenever get chance thanks greatly appreciate input context enable loading currently unsupported also helpful getting accelerate work since need properly infer device map,positive
great thanks quick reply hard work,positive
hi thanks failing could try onto main recent compatible library resolved,positive
hi know could affecting running accelerate torch evaluate,neutral
hello came simply failing change safe help unblock work getting leave decide worth thanks,positive
two model newly happening loaded load investigate first wondering getting assertion know able answer model see similar pooler layer,positive
wait confirm understand yes let better default backwards compatibility suggest default set,positive
yep unrelated stopping criterion check,neutral
add pool option waiting review,neutral
hi thanks opening issue defined library something work suggest opening discussion model page could look randomly loading,negative
failing resolved could try main,positive
looking good let u know ready review,positive
hi merge main install source set generation able get,positive
suggest first git history branch part house tidy review iterate first glance looking good lot easier trying middle review,positive
hi thanks opening could share effect removing argument disclaimer disappear,positive
hi thanks raising issue question best try reserve feature bug,positive
hi thanks raising issue need pas import import import main create model model model main,positive
yes guide custom making available hub want use need make sure register model example modeling code hub,positive
something going try fix bit annoying effectively consequential solution put original spelling,positive
wrote add pip install upgrade top notebook work,positive
might wrong latest source still issue based model export without something,neutral
kindly paste verbose error,positive
latest version accelerate still facing issue recent call last file line import pipeline file line file line module name file line raise import following error look see import name hi work though try source pip install tried work,positive
main fixed make green except,positive
ungodly need check every map removed link open result comment open closed open yet,negative
starting separate issue sense thanks,positive
maybe mechanism somewhere loading one also correct one,neutral
yes tried incorporate change fork maybe might would solution case apart one clip,neutral
correct error post thank raising,neutral
could one please merge permission,neutral
hi thanks work opening suggest model directly hub way add try much support possible way model available use findable hub immediately model maintenance bar high review process take long time thanks reply given model already hub way model class way allow easily,positive
hi thanks work opening suggest model directly hub way add try much support possible way model available use findable hub immediately model maintenance bar high review process take long time,positive
added naming file working taking reference one doubt model know still pretty huge model use testing way without big model,positive
hey looking add support figured could good one reading regarding attention overflow think would probably affected well fa issue dependent think least issue warning attention flash attention available float float sure try hood,positive
set false however set flag used generation set unset set false however set flag used generation set unset explicit unset first intuition work fact one need call specifically remove find could allow none,negative
great work thanks one thing could sure ran unrelated test failure otherwise everything else got unexpected argument,positive
regarding skipping moment leave anyone beat otherwise wait merge,neutral
yes believe default behaviour would breaking change keep,neutral
take look test fetcher remember test fetcher design decision great balance coverage well speed situation surprising least since faced time already fix already done one related exact,positive
main flash attention discussion issue discussion,positive
class default desired behavior,neutral
eventually everyone open another use accelerate scope want mix different stuff single pro splitting pro stable accelerate main accelerate nightly run,positive
suggest moment skipping moment separate think issue test fetcher quite recently whole green main another related different code suddenly one example two would coming static cache coming fix later triggered different,positive
propose revert regarding accelerate fix issue eventually everyone open another use accelerate scope want mix different stuff single,negative
everything really appreciate work one large complex model try support get quickly possible,positive
know fix already main however fix day ago right last night run still main fix,positive
hi issue already working fix right going close issue duplicate,positive
confirm accelerate version would mean fail despite main still main,negative
know original accelerate main rather stable release,positive
still failing already main accelerate main accelerate main think change accelerate part address issue already work want use accelerate main let keep despite failing,positive
want make remove entire warning block case,neutral
failing due automatic update package let know sorted need rebase thanks,positive
could open issue ping,neutral
sure appropriate place post running told forward message wondering feature bug original issue,positive
length may make training process change problem code create tensor size many input local worker therefore tensor length sum total number,positive
wo notify version unless,neutral
wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,positive
failing unrelated known issue happening currently working fix let know rebase get green,negative
good spot could add fix,positive
hi thanks issue confirm version try latest pip install,positive
failing due automatic update package let know sorted need rebase,negative
length may make training process change problem,neutral
work force set device original error device please check type refer yes used support device,positive
hey tagged issue according documentation pip python o python import torch torch import import device else begin need access token configuration model model like import import import chain import import import import import list import field yield node score return result order score limit print class chain graph field query result property self list return property self list return self question question context context el el context result question question context context result return chain enhance specificity efficiency technology minimize effect increase potential therapeutic entering new chain function removed use invoke instead set false however temperature set flag used generation set unset temperature setting generation quantization state please call device layer first recent call last cell line enhance specificity efficiency technology minimize effect increase potential therapeutic bias tensor tensor tensor none assert none false,negative
one thing false default value argument default value true,negative
taking issue since found else working,neutral
hi thanks opening issue please make sure follow issue template provide minimal code reproducer relevant error information full error running environment run terminal output,positive
went fixed possible wondering good idea add maybe better way see lot unrelated warning guess everyone use compile generate,positive
one device validation set well get differentiate test validation trainer know gathering,neutral
work force set device original error device please check type refer,positive
hi thanks opening issue please make sure provide minimal code reproducer information bug full error issue error coming anything team,positive
hi thank suggestion support mistral added maybe insight,neutral
work force set device original error device,positive
also problem first place added statement assign device original code work new tensor device also work device assigned tensor applied fix error file line file line gather return gather tensor file line wrapper return function file line gather return tensor file line return tensor file line return data file line tensor file line wrapper return file line work tensor type associated device type,positive
hi still working issue update whenever news,neutral
hi thank notice please refer alibi attention bias every attention module,neutral
hello could update link original would recommend first start code hub issue add support model interest community thanks reply added description model plan release directly provide easy access finish runnable version converted hub think knowledgeable people review pas could please let know next,positive
thanks lot snippet think fix need return tensor instead dictionary loss try return somehow loss return instead,positive
thanks ready review note currently really need make make bit compatible therefore added test simply trainer correctly get unappropriate error compile integration seemless simply update make work,positive
please take look left padding used output model wrong found attention mask eager mode mode difference element mode set attention mask zero mode attention mask generation used output model correct test eager attention module attention module wondering,negative
hello could update link original would recommend first start code hub issue add support model interest community,positive
wrong current model one mean file folder already new number dropout float,negative
main bit broken package let wait bit,negative
ca reproduce anyway dev,neutral
hey pretty sure could raise error however,positive
yeah good reason temporary used clean test invocation run faster,positive
remove block warning well,neutral
believe calling function environment variable better current code something like python set else environment variable provided therefore explicitly set curious reason temporary used like documentation would suggest setting necessary data locally,positive
change version latest providing help,positive
hey ran without stuck super slow might lot,positive
feel like snippet provided away control user actually user keep value true may use code like snippet something similar environment variable call temporary file call,positive
understand le sec pas model predict everything one go default mode,negative
wrong current model one,negative
could elaborate bit think would better reason wrote initial way made sense allow user keep environment variable along environment also left default value empty string consistent documentation well feel like snippet provided away control user actually user keep value please let know misunderstanding missing something able fully look provided like issue module,positive
would suggest controllable argument,neutral
hey everyone tried look see actionable error anyone help figure need done pas,neutral
maintainer found investigating run code like python import import o assert train since set set empty string undesired case,negative
think new solution elegant good see found similar original implement close,positive
correct note added want fully take well thank would extremely grateful possible initially tried add model interest difficulty moreover due work progress slow sorry,negative
correct note added want fully take well,neutral
would accepted relevant library directly sort necessary,positive
hi thanks getting back need deep dive branch try fix next day,positive
thanks build docker image python maybe take inspiration sure getting error currently correct yes would great also support python thanks,positive
time actually look properly intuition perhaps design choice since across multiple may result erroneous gathering usually use validation set use one device test set,negative
thanks amazing addition great new feature ask question make sure properly code like shape dimension mask broadcast post attention shape question post like actually provide first axis batch size second post reverse since sometimes batch size testing work either way want use correctly see proper shape anywhere perhaps somewhere thanks,positive
hi thanks help per input sub class trainer custom loss function custom loss implementation running following error running code single machine program code error list attached python import torch device else print device import train category index index enumerate print keep range train considering training keep keep train train keep import import processor import model device import import transform coco category category area range category category area area list return transforming batch area image zip image image transform category area image category processor return transforming batch area image zip image image transform category area image category zip area return train train import import data range training data data calculation class object return return size size return area set bounding tensor area format area tensor area box return also return union area area none none inter union area none area inter inter union return union generalized format pairwise matrix degenerate nan early check assert assert union none none area return area union area import torch import import import class class assignment network efficiency include general case matching best thus self float float float matcher relative weight classification error matching cost relative weight error bounding box matching cost relative weight loss bounding box matching cost super assert cant forward self matching least tensor dim classification tensor dim box list target tensor dim number target class tensor dim target box list size index selected order index corresponding selected order batch element min print flatten compute cost matrix batch also target print index type print print index type print compute classification cost contrary loss use approximate target class constant change matching compute cost compute cost final cost matrix size index enumerate size return index class class loss process two compute assignment ground truth model supervise pair prediction supervise class box self matcher create criterion number object special category matcher module able compute matching key relative weight relative classification weight applied category list applied see list available super matcher self index classification loss must contain key tensor dim print assert index zip index return self index compute error ie absolute error number really loss intended logging propagate device count number last class return self index compute related bounding regression loss loss must contain key tensor dim target format image assert index zip index return self index permute following index enumerate index index return self index permute following index enumerate index index return self loss index assert loss really want compute loss loss return loss index forward self loss computation see output specification model format list applied see loss doc print output type print type print target type print retrieve matching last layer index compute average number target normalization sum iter compute loss loss index return batch item item batch item item batch device item item batch item item batch device item item batch item item batch device item batch key value item item key value device item batch batch batch batch batch print batch return batch import custom loss matcher criterion device print type print type loss criterion return loss subclass trainer import trainer class trainer self model print model print print print print custom loss calling loss print custom loss calling return loss else loss use new trainer trainer error list recent call last cell line file self trial else return file self trial model model loss nan simply add average previous logged file self model else loss return file self loss print print type type loss loss loss unsupported operand type python could find image processor class image processor model loading based pattern matching model feature extractor configuration parameter removed please specify size instead tensor tensor tensor tensor tensor tensor tensor tensor custom loss calling class class output type class target type tensor tensor tensor tensor tensor tensor tensor index class tensor index class tensor weight status tensor class reduction status mean status status custom loss calling loss value tensor tensor tensor tensor loss type class type class recent call last file line module file line train return file line model file line loss file line backward loss loss unsupported operand type,positive
meet issue accelerator version dev resolved,neutral
single run result simplified version code sequence classification task load model none model model model model trainer trainer load model evaluate,negative
reason python single statement place able run python problem otherwise replace statement statement lower requirement necessary,positive
sun bot wrote issue automatically marked stale recent activity think still need please comment thread please note follow likely reply directly view id,negative
alright think start grasp base merge first open source project knew advance would front thank taking time still continue review next,negative
try pip install torch code file import torch,neutral
possible get get original probability following example instead whisper model generation start first position default masked print float range true print tensor,positive
hi problem training may ask resolved resolved,neutral
please help translate work,neutral
indicate interest issue big gap speed gradient naive sophisticated training large bigger fit single able control strategy let u take advantage additional increase speed naive without resort typically need lot pay support edit looking closer sure code work intended except case maybe think need wrap chunk single forward pas function new implementation instead intermediate inside segment run usual forward correct logic edit given description seem author intended every many rest normally without however meaning segment defined opposite direction segment set input first layer set everything interior set mode,positive
done use eager attention attention mask correct however eager mode attention mask wrong left padding inference attention mask like eager mode mode tensor,negative
alright help test thank indeed quite understand specific behind throwing fail recent call last file line word fail recent call last file line fail recent call last file line stride greater fail recent call last file line greater issue testing sequence please update short fail recent call last file line greater equal fail recent call last file line long set none see fail recent call last file line long set none see reproduce command python latest commit,negative
update fixed issue see still problem though version noted though would like fix possible recent one keep open,positive
already basically eager attention still attend padding output never non zero exact padding instead tiny number see,positive
latest version accelerate still facing issue recent call last file line import pipeline file line file line module name file line raise import following error look see import name hi work though try source pip install,positive
found bug test whether enable flash attention used result correct know bug happen,neutral
sought tried fix dimensionality mismatch batch size could figure clue python output shape match broadcast shape test fail,negative
problem import trainer resolve,neutral
next task use llama model together sequence classification let know result tomorrow,neutral
tried head think linear classification head end saved properly random classification head personal problem related classification problem medium also know difference want write script demonstrate due time confidential data project need find time,negative
issue instruction llama model everything even inference training trainer trainer trainer load model auto,neutral
name accelerate version name version name torch version name evaluate version,neutral
issue bash pip install,neutral
solution work tried correct problem resolved get able produce file sure noted thanks,positive
correct missing something ran script could reason behind error,negative
latest version accelerate still facing issue recent call last file line module import pipeline file frozen line file line module name file line raise import following error look see import name,positive
could share testing easy solve fix thank test currently trying solve saving vocabulary vocabulary index consecutive please check vocabulary corrupted special added vocabulary make sure associated word trained truncation explicitly provided specific value please use explicitly truncate length truncation strategy encode select strategy precisely providing specific strategy truncation saving vocabulary vocabulary index consecutive please check vocabulary corrupted token index sequence length longer maximum sequence length model running sequence model result indexing aware overflowing returned setting chosen sequence truncation strategy returned list always empty even removed aware overflowing returned setting chosen sequence truncation strategy returned list always empty even removed aware overflowing returned setting chosen sequence truncation strategy returned list always empty even removed aware overflowing returned setting chosen sequence truncation strategy returned list always empty even removed index sequence length longer maximum sequence length model running sequence model result indexing truncation explicitly provided specific value please use explicitly truncate length truncation strategy encode select strategy precisely providing specific strategy truncation argument removed future version use pad sequence batch use pad length case give specific length leave none pad maximal input size model truncate maximum length provided model maximum length default truncation saving vocabulary vocabulary index consecutive please check vocabulary corrupted vocabulary vocabulary index consecutive please check vocabulary corrupted special added vocabulary make sure associated word trained saving vocabulary vocabulary index consecutive please check vocabulary corrupted set yet set yet set yet set yet set yet set yet set yet set yet error recent call last file line word file line token file line return token file line return object attribute error recent call last file line range file line range file line return file line return index object attribute fail recent call last file line output differ first element second list additional first extra element fail recent call last file line greater equal fail recent call last file line differ first element fail recent call last file line differ first element,positive
could share testing easy solve fix,positive
hi model fully working fix tested many different wrap fix fix failing running test need help figure best way move forward look share code review please let know missing anything would like wrap,positive
optimum pinned much older version accelerate sadly need put fix like ideal though solution put accelerate,positive
thanks help let know anything else thanks,positive
manually tested example following,neutral
lot neuron resulting related may due environment trying tag think trying master would resolve,negative
hi thanks lot confirm loading model misleading gone continue weekend,positive
please confirm whether issue open rest affected simple way test would python import print fix good thank,positive
thank begun task work recognize new model exceptionally eager give try,positive
see docker image last two day since installation library python least change quantization edit tried install python work unable locate package found tutorial sure best way install,positive
bit confused warning saying based need pas case guess said prepare shifting appropriate beginning many thanks,positive
default instead current solution,neutral
hey need specify training need prepare pas forward use start token model find user see code snippet input,neutral
hi thanks open feedback,positive
hi saw message community help review translation would happy review,positive
would love see well thanks working,positive
hi way specify training well like model loss batch may require different training batch specific input language output language sometimes output language time per batch seem good approach specifically lot inconsistency accelerator,positive
fixed layer get output additionally get following warning run test model newly wondering getting assertion,positive
hi answer last quite one like saved however present probably made older version code correctly set therefore error getting spurious want go away suggest making like python import model push hub account loaded model saved correctly included model everything resolved,positive
fixed run locally model feedback think good go,positive
meant change currently broken grateful could give look next couple day thanks,negative
thanks opening issue fix,positive
want add test slow whisper already failing independently,negative
anyone help finish assign someone knowledgeable whole process little complicated idea next thanks,negative
hey happy help understand checked difference input get processor output get code,positive
hey sorry long delay unfortunately mac try could open discus also might better experience,negative
main concern inspect test suite instead actual validation function way inspect compare static new model remain optional nothing run recommendation validation could warning instead raise exception good chunk hub would break,positive
finishing draft removing validation functionality,neutral
probably work knowing versed enough implementation guarantee always work flawlessly additionally whether work probably backed library yeah mostly suggesting convenience method functionality difficult emulate,positive
working fixing default locally,neutral
update well thanks still waiting go pas otherwise code le complete,positive
hi thanks script unsure causing issue unable replicate successfully run script run last main could share accelerate torch evaluate,positive
hello error testing sure resolve error recent call last file line file line return file line file line argument object converted,positive
everything passing approve let merge thanks,positive
confirmed bad two training stepping ran verify closed,negative
hi thanks opening request model defined suggest opening request,positive
hi thanks issue therefore available instance load usual argument also load model argument require accelerate integration case let u know advanced flash attention basis specific question see support fa open separate issue feature request work feature awesome model simply amazing suggest added bit default running bit,positive
thank quick response update however resolve issue additionally accelerate latest problem still attached code use training python import import trainer model model model name module epoch epoch linear trainer trainer auto auto type auto auto auto auto type auto auto auto auto stage true true auto auto auto auto auto false launch code,positive
got thanks fix even leave code block still really nice usability improvement,positive
hello saved calling given line model saved saved properly allow usual see following resume pas ago,negative
hi thanks raising issue related,positive
gentle ping feature addition,positive
would good add test alongside change,positive
let know draft want final review,neutral
thanks great contribution patience failing,positive
please confirm whether issue open rest affected simple way test would import print,neutral
well often something unexpected rebase time even seeing also already main may suggest close update main fork create new branch fork past open new additional work,positive
used python import open index token token prompt hey prompt print print prompt print prompt print prompt print prompt,neutral
thank interest issue close issue,neutral
thank glad issue resolved well,positive
got release ready review merge,positive
hi thanks much issue u understand better issue share full command training might unrelated safe zone could try main include,positive
hey believe due library due explicit definition fixed install main get issue running code example commit import print running code example commit import print better solution however would update configuration hub work without explicit override take care link thanks,positive
hi thanks issue try also try run end compile understood really think case throw error trainer fix good end,positive
way rerun issue recent commit passing absolutely necessary regardless whether code related feel like simple rerun may solve python short stack type self object request get stream true verify true none try resp except branch raise raise read timed read request id code exit status,positive
hi plan want export dit detection layout analysis model please let know new comment,positive
given file code executed git clone pip install pip install python context question,neutral
provide reproduce way know could share small snippet calling model,negative
hit issue find use like append used paper use change work well source,neutral
looking test merge fix new test need alternatively smaller model like used seem access locally write access even know touched image,positive
thank try individual would nice establish minimal guarantee performance compatibility provided feel free close issue,positive
done thanks help best open source,positive
solve problem experienced bug today discover one raised way back,positive
also tracked support issue still note fix side also like,neutral
good thanks feedback made another minor change default value empty string rather none,positive
current unrelated two current failing resolved could try one final rebase get green,negative
try main resolve currently failing,positive
something think worth setting massive scale conversion hub especially many compatible torch issue lot attention community reconsider threshold number welcome open affected hub explaining conversion way decide something want could also open issue list known affected get community help effort opening,positive
hi thanks raising issue question best try reserve feature bug general setting add token end sequence control length looking code base flag used llama code llama generating ca set word limit set limit number passing read generate,positive
thanks review added explanation research great get feedback someone knowledgeable,positive
sure go ahead merge see time write policy idea think make complicated end allow flexibility people want formal way load depending environment tag get,neutral
indeed quite weird failing seem related however failing think happening collected test fetched reference image feature extraction pipeline added run going look let know,negative
test failure unrelated chance kick also,negative
thanks catching sorry know run across time oh need need manually something wrong end run automatically mind taking quick look get affect people thanks looking,positive
start move tutorial section,neutral
thank let wait share proposal,neutral
thanks catching sorry know run across time also worked fill failing mind taking quick look get affect people thanks,positive
thanks opening let u know want review,positive
merge typo reference thanks fixing,positive
idea running typo ready,positive
reminder code taken following code work notebook python script via,neutral
thanks flag confirm work change trainer accelerate main via pip install,positive
code code import import map import value sequence import import import import import import import import partial import import import import o import import import import torch import accelerate import accelerator import import import import import chain transformation import import import optional import iterable import torch import cyclic import import try except return class self data data start data start data return data date return date batch batch start date date batch start return batch transformation bit like return chain step remove step convert data potentially else else expect extra dim case else step handle nan filling target zero return mask true false mask loss unobserved see inside model step add temporal based month year case serve positional step add another temporal feature single number model life value time series sort running counter step vertically stack temporal key else step rename match mode optional none optional none transformation assert mode train validation test train validation test mode return data optional none bool true iterable transformation data initialize training instance train instance splitter sample window context length prediction length possible time series randomly within target time series return stream cyclic stream return data transformation data create validation instance splitter sample last context window seen training validation apply train mode return data transformation data create test instance splitter sample last context window provided test apply test mode return,negative
import import import partial import import import accelerate import accelerator import evaluate import load import import import context length coming helper given add time month year age see single static categorical feature namely time series id possible model learn size possible transformer model return model main set data train validation assert target target train test make sure data correct form partial partial model get data accelerator accelerator accelerator device device print model model epoch range batch enumerate model device else none device else none device device device device device device loss loss print loss inference batch device else none device else none device device device device load load enumerate target target print evaluation print print plot fig ax index major every half year minor every month index target actual index median index main,positive
request verify logic rather public private good logic quite complex like properly covered certain method right thing whole modify safely,positive
think take another look one add much rebase one,positive
indeed number might issue conversion something handle backward compatibility would problem unfortunately case previous comment private hub access model hub would break thank publicly accessible would reasonable update add alongside outdated,negative
hi thanks raising issue could provide minimal code example reproduce error specifically script,positive
sure page example model quite big project want tackle something smaller first issue get used good first issue great place start,positive
hi cool idea worry create policy object although might simplify internal infrastructure would complicate people often want make one single call anyway think merge consider removal entire code block future want open issue support pressure mostly code cleanup rather essential feature could also open issue suggesting policy class want might anyways since core maintainer approval already,positive
thanks first issue would possible please share helpful similar understand code structure,positive
picked patch release unless critical guess make,neutral
hi thanks raising issue going king want able create empty model use accelerate utility python accelerate import thanks look specific whereas work model otherwise generality feel important feature support project might insight,positive
fix bug least one set cause problem due fact one constructor also method therefore like evaluation got multiple argument solution problematic method want make sure present state temporarily removing method original state,positive
cool fixed missing also added skip special many failing due use ran following made sure pas also think need run command common perhaps go page follow think sense,positive
well anything else want add merge,neutral
case think ready final review leaving cache another,positive
one question people maybe return format right pas chat like python role system content system message role user content test get response chat continued python role system content system message role user content test role assistant content reply think right thing behaviour pipeline prompt start string let know different opinion though,positive
hey going see got ago recent release ago include,neutral
better understand purpose behind work right following work end import copy import,positive
well also due beam search le example various use greedy generation already greedy generation,negative
seem unrelated idea might causing,neutral
thanks based issue linked cache turned model lead change understanding disabled default forward loaded model still noticeable difference difference smaller tensor tensor loaded model still noticeable difference difference smaller tensor tensor ran forward prop rather difference tiny tensor tensor right padding padding right padding error even smaller still tensor tensor,positive
make next release hope,neutral
purely internal maybe mark private leading instead,neutral
fixed could try please,positive
make go green need rebase latest main run make terminal within folder force push,positive
looking test merge fix new test need alternatively smaller model like used,positive
indeed number might issue conversion something handle backward compatibility would problem unfortunately case previous comment private hub access model hub would break,negative
certainly feel free open ready ping u review avoid becoming stale first open first comment,positive
hi regarding first question normal force push necessary effectively history branch second question way still merge manually interface like mean interface rebase might necessary resolve trying move first commit branch head main however branch originally branched older commit might branch make also main might also happen also main branch interim branch information upstream main therefore rewrite history branch tip main obvious easiest way manage rebase main branch often avoid merge branch nice found useful first learning,positive
understood happy hear able add remote code model let u know ready review,positive
guidance add make would welcome,positive
thanks add git issue optimum ago hopefully someone acknowledge problem exist,positive
like commit outside broke something python recent call last cell line import auto auto type return token revision none model self model along return model self model self model false return model self name value name value else super name value self name ca set attribute,negative
well also due beam search le example various use greedy generation,negative
believe comment relevant issue,positive
added documentation model page inference seeing without ie change unrelated think falcon failure likely edge case problem reason difference little higher one case whereas failure likely due incorrect implementation,negative
facing similar issue object attribute training llama solution,neutral
hi thanks snippet see relatively small difference fa quite small acceptable note even though fa numerically identical practice due different always going small difference,negative
hi thanks issue therefore available instance load usual argument also load model argument require accelerate integration case let u know advanced flash attention basis specific question see support fa open separate issue feature request work feature,positive
know used compute attention score update grow exactly compile size key following knowing current size none sin sin co co specific rope print always print print output first forward following see cache size decreasing dimension also confirming dram usage decreasing iteration,positive
hi thanks great work wondering could update installation cell notebook install source instead fork way could catch potential future release,positive
know used compute attention score update grow exactly compile size key,positive
example breaking behaviour working fa careful new,positive
hey normal behavior auto regressive key value shape actual cache size know first forward big new forward add sequence length dimension never seen leading decrease shape actual size variable decreasing sequence length dimension every new forward call variable grow size every forward call right cache,positive
hey normal behavior auto regressive key value shape actual cache size know first forward big new forward add sequence length dimension never seen leading decrease shape actual,positive
mistral sliding window attention case llama thought maybe better test think good leave well,positive
hey fine tuning optical music recognition trying read sheet music model single staff single line sheet music aka pair getting word level accuracy know improve performance wer word error rate used,positive
hi due recent main branch instead making merge directly interface advised main branch branch onto main resolve implementation ended first question normal second question way still merge manually interface like image end feel like manually interface would faster missing something also pas,positive
awesome thanks interest assigned happy would ping review content since active translation hi sure happy help,positive
training new mess since use specific,negative
hi thanks opening could share tracing model main able trace clip without issue import torch import image import import model processor image processor photo cat photo dog model load model back model model,positive
thanks opening anyone coming future could share torch version environment running pip list torch review king version package handling like get opinion merge day would next week get,positive
still working another failing obvious marking draft,neutral
hi thanks raising issue indeed despite profit think something handling within assuming one version fair one able parse torch version something throughout library better failure hit version matching raising error function could found instead trying magic inspection caught problem expect match import torch though think easy fix reason happening reading correct torch question know correct version without,positive
hi indeed number might issue conversion something handle backward compatibility would problem new file different name could say example would match would without possibility content file conversion fly possible bu purpose hub,positive
like still need added ex realm trying run full range find aside like already quite bit test even running main head even like wo really uncovered unless someone specific model case encounter unrelated change perhaps worth continuously,positive
awesome thanks interest assigned happy would ping review content since active translation,positive
hey would love translate developer,positive
green help landing appreciate,negative
hi thanks prompt reply already custom model however also want submit live natively library thus help guidance would much,positive
order able import necessary add auto essentially place find need add equivalent also need add class said object exposed top level think another alternative would list skip script,positive
necessary input range already set false,negative
hi thanks contribution community wondering true necessary let know miss anything also encounter problem thanks best,positive
nice let review first confirm current logic skimming main comment need criterion particular,positive
exact code look like,positive
hey approach also work transcription audio understand beyond sticking legacy code method second audio,neutral
ran given code multiple environment type distribution environment distributed environment process index local process index device converted notebook script calling via python anyone help issue accelerate,neutral
getting best lucky rebase worked,positive
thanks general let wait add set bias merge head get rid go would good add performance description model page similar done flash attention look run confirm compatibility change one tricky locally method already failing without change chance try run test see seeing machine well setting attention class get away attention removed,positive
like request follow lot abstracted smaller modular agreed however believe ball mostly generate side made flexible like becomes task exact pattern,positive
unconventional cache yes may underlying cause bug seeing however code error optimum interface optimum also usually handled optimum side suggest following issue happy sort side arise along way,positive
git remote add upstream git fetch upstream git rebase handle merge git push origin possibly something went wrong merge stage,negative
yeah short script sorry dedicate short reproducer issue without significant external code,negative
something gone wrong long indeed git fetch upstream git rebase suppose meant main git revert push undo last two git rebase main,negative
something gone wrong long,negative
going change cache structure bit widespread nutshell given hard static cache obvious interface similar new static cache original cache implementation progress done expand usage interface,positive
actually fixed added recommend following rebase main run make folder commit resulting persist dive,positive
nope speech recognition example torch,neutral
failing unrelated going see open separate fix,neutral
ready review amy core code totally incomprehensible tensor stress ca follow wrote one afternoon also forget look away kind trusting main problem clean way get vocabulary handling two common seem universal method get actual string token yield probably work though stop contain anyway,positive
hi thanks raising issue unfortunately simply possible u convert compatible currently listed hub well many private backwards compatibility important library although currently version would likely break many many one option would open model hub converted explanation would owner whether would like update care would need taken make sure correct avoid suggest instead conversion fly need note default serialization use library open case got wrong anything else add,positive
hope mind moving minor around make happy thank taking time,positive
hope mind moving minor around make happy,positive
hi thanks raising issue going king want able create empty model use accelerate utility accelerate import,positive
quick glance generate method previously like review,positive
made variable public one yet importable see,neutral
hi thanks raising issue please make sure share full error issue information could also make sure code properly example run fully reproduce error,positive
hi thanks raising issue could please provide full error please note compatible guarantee possible loaded run able load hugging face transformer could clarify comment unable load note likely happening loading loading suggest load model instead,positive
look comment believe comment apply reproducer issue,neutral
mistral llama close architecture support find necessary think different enough warrant added,neutral
great failing two look like might related merge forward got unexpected argument forward got unexpected argument build documentation recent fix main include resolve know test case failing make provided add change test case,positive
hello come add pull request reason behind intend expose token also know test push due lack information able assist wish add please let team handle thing provide code example pull request,negative
hi sprint add support generate tracker halting addition substantially modify method complete particular beam search likely need come different shape keep,positive
failing also failing main due static cache unfortunately run,positive
sure cause would first suggest main make sure recent trigger,positive
memory increase sequence length used materialization multiplication may grow quickly flash attention wrote key grow linearly eager maximum possible memory usage time roughly peak memory usage given input implementation,positive
hello issue could anyone recommend solution please known work greater garbage returned model generate method many thanks edit sorry found correct place,positive
hi thanks opening easiest way make model available add modeling code directly hub much support let u know anything working working model found used immediately without go process find lot bar code library high due maintenance cost every new model take quite,positive
great failing two look like might related merge forward got unexpected argument forward got unexpected argument build documentation recent fix main include resolve,positive
doc recent commit main hopefully resolve,positive
quite urgent failing main,positive
actually one thing need add test like quality running make pushing resolve make need make additional modeling properly reflect,neutral
thanks testing invite continue branch code hub feature modify modeling file need make sure quant logic let know need help,positive
getting attribute error recent call last cell line result primary underlying antibiotic resistance develop combat print result self name name return name raise type self object attribute name self name value union tensor none object attribute link reference,positive
converted appropriate format read sorry forgot step push used thanks,positive
thank answer read given made short script checked error version platform python version version version accelerate version accelerate found version true version na flax version na version version script distributed parallel script code import list import torch import class object self list list sequence sequence none none get self list return sequence list sequence sequence sequence sequence sequence list none output list output output list return output return else return else return self return self value return value model list none none list none task none return model lambda lambda task model task list none none list none zip sent sent sent sent status sent status status sent status sent sent else raise return sent status sum sent status return status return else return sent sent return else return sent sent sent sent else return none return else none sent enumerate sent return sent sent mention sent none none mention else raise return sent return return class sample self list none marginalize bool false list else return encode self sentence return sentence class pas class genre model return model script sentence somatic gene thought initiate tumor formation familial adenomatous polyposis syndrome sporadic carcinogenesis respectively model model sentence tumor familial adenomatous polyposis syndrome output sentence print output output somatic gene thought initiate tumor tumor formation familial adenomatous polyposis syndrome familial adenomatous polyposis syndrome sporadic carcinogenesis respectively script output somatic familial adenomatous polyposis syndrome gene thought initiate tumor,negative
thanks everything section left untranslated automatically special syntax know section automatically generate case documentation still,positive
test seem related tried well,neutral
approach fairly robust straightforward exception sharded memory allocation current block allocation suffice saving hence extra added avoid device may something,positive
update torch error training loss still loss random never consistently going,negative
converted appropriate format read sorry forgot step push used,neutral
repository double string format code running style see contributor guideline,neutral
yes problem check made ensure none go however training provided initiate one possible correction could done believe maybe current implementation came original implementation however accurate implementation could one default think,positive
problem could completely since unlearn model likely since think text model issue try load model getting error loading size mismatch param shape shape current model current model,positive
general note immediately next accelerate release make class seen wo annoying big logic difference shove rather accelerator based accelerate version,negative
reading correct wrong whole work equal number least understanding ca thus raise user ensure number set,negative
thanks love see collaboration fire hey fix main branch get test pas would mind main resolve error,positive
thank saw ram usage loading model drop like working,neutral
provide context issue simple script reproduce issue associated output note random tensor consistency saved associated another training script loaded pickle object script import import import torch import import import import import version import import import model eager model model model model model model dummy tensor tensor tensor import pickle open model pas model loss loss model loss loss model output print else print print print print print print print print loss print loss print loss print loss loss loss print loss else print loss output script use flash attention without torch might lead unexpected behaviour use flash attention model make sure move model flash attention provided run training inference automatic via decorator flash attention provided run training inference automatic via decorator loading loading tensor tensor tensor loss tensor loss tensor loss,positive
actually much harder thought sadly wo time finish leave form facing following problematic actually kind tricky run whole batch thought better run function every batch index would also make tricky easier redundant need split cross attention input output length essentially output length defined individual segment input length start end done function anybody community willing give try feel free use code well,positive
hi thanks linking relevant issue still outstanding question occur provide context behaviour,positive
main could install source retry,positive
yes look main see build failing lot unrelated moment happy merge passing,positive
hi could please get need get accelerate test succeed thanks,positive
change trained used older format longer able properly load unless set contain key longer wonder way land change would backwards older model well see different result change lot loading model sometimes difficult scale,positive
feedback code good go think,positive
assume build documentation check failing unrelated merge,neutral
hi like still loss issue update ignore comment apparently new installation loss tried rerun training training file line return file line return file line return file line forward file line return file line return file line forward return input mat mat must got float,positive
see yeah summarization understanding situation think removing path lead warning instead failing fast appropriate error message would awesome another thing thinking add kind object used aggregate loading model class al instead propagate future proof additional policy object wo change way stack still expose allow deep code access stuff original calling function one also visible explicitly policy loader used across different code concretely something like class bool bool string open fill policy,positive
posting understanding issue load figure class key determine model class key present initialize correctly call therefore unwanted confirmation prompt load custom code set defined model also custom code solution add argument argument nothing function call also pas value investigating think good change introduce security conflict library willing approve however one thing worth actually use class string loading code purpose code block issue check class name name raise warning loading different class since people load could consider removing block instead think need big code block raise mostly unused warning,positive
whole block throw warning load wrong class,negative
hi delay internal trying figure right approach problem since fairly complex issue core code want fix also avoid create future need anything else give u little bit time investigate,negative
hi final would remove final de file change time la de first paragraph de remember keep tag tip original since part syntax,positive
anything else need verify get,neutral
need fix upstream like git remote add upstream git fetch upstream git merge,negative
following python import prompt hey prompt print print prompt print prompt print prompt hey print prompt hey following python import prompt hey prompt print print prompt print prompt print prompt hey print prompt hey tried ca get would like ask code provided original file,positive
sure point file add,positive
version added device fix running setup added corresponding similar,neutral
oh see added new fix checked empty tensor break anything,positive
issue segmentation model test could figure problem,neutral
way window attention mistral original code base attention mask custom attention mask attend check looking method point tried convey passing attention mask think implicitly get sliding window attention passing correct attention mask let know sense thank quick reply solution pas custom mask think way passing param function image,negative
kernel python import o import optional import torch import load load maybe incorrect installment really sure test maybe could try path somehow environmental variable try reproduce well,positive
hi thanks raising issue question best try reserve feature bug value come input sequence length number input,positive
work single triggered error pas multiple visible use auto work another machine set make work due underlying machine,negative
hi code import import import torch print llama model prompt hello world prompt print model print error python loading tensor tensor operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index recent call last line co co error assert triggered,neutral
need change based information found,neutral
hi look issue soon short reproducible script otherwise next impossible reproduce bug consequently find wrong,negative
issue exactly issue line,positive
hey still getting right python model argument removed please use token instead recent call last cell line model register raise model class passing attribute consistent class model fix model class passing attribute consistent class model class class fix one match,positive
hi thanks review good appreciate feedback thanks well,positive
beam search track specific would change indeed pas previous beam index could outside scope want support least suggestion would fork library change generation loop need,negative
hey thanks pointing added fix behavior install main get correct generation,positive
think goal simple default alright otherwise never stop lot model agree issue still image believe need le,neutral
hey ah yes indeed training loop evaluation loop inside sadly let user pas generation key word language however fix easily following cell notebook python import model hi define language choice training work thank getting back quickly give try,negative
hi firstly taking long get one slipped past first time like clean sure accept list special specific code short think would make sense added token support since however user clean way separate user assistant another user getting method return optional mask array similar could use mask hi thanks feedback indeed better keep special short besides suggest weight mask key input list provide flexible unify single turn tuning reason production environment bad case hot fixing modify specific turn become without rest turn user choose train model learn specific turn believe high quality ignore chat role system content friendly always style pirate weight role user content hello weight role assistant content great help today weight role user content cool weight role assistant content weight role user content bigger virus bacterium weight role assistant content bacterium weight chat set turn weight already pipeline local training yet make use think also help,positive
difference see instance mine version update loop back,neutral
inference script work great however python latest torch constantly get bash recent call last file line module output file line return file line generate return file line error device kernel image invalid compile enable idea might wrong,positive
exactly purpose pipeline necessarily part need task saying let rely handle pipeline see thanks push soon,positive
know correct behaviour get overflowing returned data structure need consistent prefer fast behaviour none inconsistent reference help try fix late march would appreciate decision direction go since expert,positive
also seeing similar issue loss quite unstable learn slowly running full latest phi model another run see issue still fa post loss curve next incorrect loss curve like change chart like new transformer installation include essentially plot tried training failing file line return file line return file line return file line forward file line return file line return file line forward return input mat mat must got float,positive
exactly purpose pipeline necessarily part need task saying let rely handle pipeline,positive
hey thanks opening issue would like dive open fix might known bug overflowing slow fast probably right behaviour,positive
hey thanks opening issue try keep example use case suppose would recommend ask question forum track warning source check happening question probably missing ne thanks,positive
sure indeed bug would like open make sure python import,positive
local agent use limited give default model hub think goal simple default alright otherwise never stop lot model add feature request setting think keep way,negative
also problem want use mask filling task output always strange modify input format model card code import model thing happen mask model print output telephone change input always output strange think wrong confused whether model suitable task modify get proper thank much,negative
let rebase think could merge,neutral
like already getting stale post message,negative
added file find similar speaking test mean,negative
hi yes happy help general good would comment following de instead de image natural way el de instead la de line translate part guide line remember add new documentation file read guide information,positive
thanks issue issue related custom code hub recommend opening issue make sure trust remote code model correctly layer,positive
hi yes please great,positive
way window attention mistral original code base attention mask custom attention mask attend check looking method point tried convey passing attention mask think implicitly get sliding window attention passing correct attention mask let know sense,negative
hello quick since script loading run small hack since chance look issue yet new branch hack similar branch also first time run new please two comment new python please let know run,positive
something also tried problem,neutral
corrected description link issue testing like shift possibly zero leading zero division error,neutral
thanks following torch version torch torch version fixed setting best,positive
fully functional working solution static shaped whisper extensively tested get accuracy original model,positive
handle like trainer directly would rather see code accelerate bring trainer automatically since preparation especially would possible please elaborate bit move model logic logic suggest move,positive
hi thank suggestion actually similar idea work well one catch must greedy soon add beam partial function share state way distinguish beam operating currently think sort new parameter generate along pas class beam used generation,positive
hi easy way get language pipeline according add parameter pipe get language text would kind show example code tried following code abut return language type device else else model device processor pipe pipeline result pipe sample word task transcribe case need put return language declaration pipe based code like python model device processor pipe pipeline result pipe sample word task transcribe declare obtain de language hope help,positive
like forgot git add folder fixed also issue model big added configuration smaller one,positive
hi thanks opening could give context issue ideally reproducible snippet looking think ever case,positive
hey thanks bug note bug already main could try install pip install run code snippet hi thanks quick reply indeed fixed working dev version,positive
feature request bug say happy look,positive
understand correctly matrix different size stay cache cause surge memory usage,neutral
might help keeping track everything going start working issue hope fix soon,neutral
hey thanks bug note bug already main could try install pip install run code snippet,positive
hey could maybe open new issue ping sadly able reproduce error running code linked,positive
done also chat template new pipeline,positive
hey ah yes indeed training loop evaluation loop inside sadly let user pas generation key word language however fix easily following cell notebook import model hi define language choice training work,negative
hi thanks raising issue unable replicate issue locally could try pip recent version pip install hi problem many thanks,positive
hi thanks raising issue unable replicate issue locally could try pip recent version pip install,negative
thanks review leverage however really possible model considered unlike like modify text model include attention one would need work make work directly inject vision expert attention inside language model rather defined new vision attention sure model refer figure,positive
would interested helping review translation,positive
try instead theory fix python,neutral
hi similar issue normal take look could share torch especially torch version switch thanks advance,positive
also seeing module becomes naive replacement get import name sufficient,negative
thanks indeed something great thanks lot,positive
pretty sure failing nothing,positive
quick fix ensure new model leaving issue open perform bigger option align flax stateless design,positive
hi need enable request arbitrary function mutable input use mutable input state example create function state import partial function state input assumed dictionary foo state none state raise state must provided dictionary else state state else state return foo state partial u create new function fixed value state state input mutable new function keep track state partial print print print,negative
please help progress epoch found progress epoch progress epoch commit sure fix one thanks,positive
issue instead meta use accelerate instead work fine basically causing python import accelerate import model,positive
hi could open new issue error full minimal reproducer hi short example,positive
understand memory look guide especially section layer,neutral
hi understand triggered remote code issue surfaced said reproducer helpful let see figure cause fix,negative
thanks tried model confirm pointed actually tried see memory jump still confused inference work within language understanding fixed memory usage network fixed regardless big always capped context size memory load model memory utilization model equal utilization fixed inference time,positive
hi popular project like get many support feature want maximize much help community community help u stay productive end please share short script issue clearly reproducible computer thank first issue u check guide,positive
hi wave wrote memory consumption grow throughout generation pipeline call script longer memory contrarily see fixed memory footprint model loaded also independent model text generation work confirm memory leak try simple test call pipeline repeatedly input see memory repeat,positive
identical code inside try running following code python import model work model work iteratively python import pipeline pipe pipeline work pipe pipeline work,neutral
thanks see sliding window attention difference eager attention implementation attention mask passing arbitrary attention without problem problem support window attention,positive
custom library problem due configuration file wrongly model class try recreate another dot name custom architecture experiment ready bit,negative
hi bit confused issue saw bug however many put name fact naming convention like extremely common surprising never seen issue make investigate determine exactly model class trigger bug issue may specific custom code rather general issue,positive
hi yes still going work soon open,neutral
hi could open new issue error full minimal reproducer,positive
could provide code snippet reproduce example provided correctly input,neutral
already tried set float image scale float optional none optional union none,neutral
yes would like get float order use inference input type float instead float,neutral
hi thanks raising issue could clarify behaviour want returned float provided example dummy image float pas image processor get following warning like trying already input set avoid dummy image set call directly image processor cast desired type import import image range image output batch torch image cast desired type try avoid passing wherever possible brittle solution,positive
got side tracked done meditating stateful stateless approach want take support torch compile without extra complexity similarly advised mamba cache work stateless manner,neutral
slow pas ready merge always extra important use new behaviour,positive
hi would like ask update appreciate help thanks,positive
seeing error python import import torch meta model device device trying figure work maybe given lot big model inference update also work use rotary class one llama,neutral
thanks work word pipeline line private method error object attribute,positive
hi firstly taking long get one slipped past first time like clean sure accept list special specific code short think would make sense added token support since however user clean way separate user assistant another user getting method return optional mask array similar could use mask,positive
hi would useful feature plan implement,positive
getting similar error training help,neutral
hi thanks opening working previous moment see model image processor next step would,positive
let try make green need resolve quality running make pushing failing need try main,negative
need final review maintainer,neutral
sure fix tried token set getting memory profile also anything want hit every time memory sure length expect theoretical provide assume particular length produced,positive
hi like try hi still working splinter enate hi working,neutral
review confirm fix line trainer behaviour final maintainer review,neutral
hi ca reopen something upstream since either branch force push open new link one reference,positive
hi thanks opening feature request third party rather team anyone else community would like open add happy review,positive
hey feature useful one think couple since sure intended behavior initially,positive
hi similar past hard say code went wrong given generation would advise check set verify correctly get given sentence also ensure special start end separately without adjacent,negative
want use data collator specifically case create put user conversation parameter use create data collator,neutral
try one review thank try,neutral
import local torch version match torch version built would need rebuild,neutral
like precision problem normal range version,positive
thanks helping indeed issue,positive
added file find similar,neutral
example seem work get name defined,neutral
hey custom code probably wo best recommendation put see happening,positive
would like open failure,negative
could rebase main make sure green help ca finish,positive
given thus first change data,positive
hey used used sure explicit call work feel free reach work,positive
hi think blocked missing issue think continue resolved want share quickly issue thank,positive
nice could share full stack trace,positive
great see tried latest running full attention event device type match blocking stream device type import undefined symbol,positive
hi tried replacement sample normal attention way better swa even produce fluent text sure implementation actually correct missing hand guanaco swa generating text still gibberish replacement still also way normal attention normal attention faster swa measured theory otherwise long text long text summarization vanilla attention swa swa mostly gibberish reason swa significantly higher vanilla model code snippet used generating output output set true false see different effect usually sampling function prefix summarize following bill focus summary important bill summarize everything particularly focus related appropriation effect bill however need go complex acceptable provide use active describe bill like use ambivalent like prefix doc doc summary return summarization article incase feel free let know,positive
future likely remove channel support unless see explicit demand case please put monitor otherwise channel usually date thanks raising issue would prefer pointing official channel way install,negative
answer difference test case come content explicitly defined entry need check see class defined via patch understand behavior looking given get code already calling class class allow override work test case,neutral
custom model class self super range forward self loss none none criterion loss criterion return,positive
thanks help really worked following processor model true need redefine define parameter class train save everything worked designed import pipeline pipe pipeline import processor classifier,positive
hi could see solution create branch scratch everything since beginning original related error make fact environment outdated merge huge fixed said everything course may need discussion tried make sure everything point included also term widely used literature anyway implement model contribute told would take time method take mess made part learning path let know think next believe almost steven,positive
ran bug sure much two code base point report issue code batch batch typically pad consistent length happening file line input array except concatenation axis must match exactly along dimension array index size array index size fix rather simple feel like probably handled batch calculate number padding pad batch add list batch else batch thanks,positive
change loss following well,neutral
hi deploy mobile advice mobile platform model format use,neutral
another work around since much manual python load use pipeline helper import pipeline pipe pipeline layer override pipeline pipe method,positive
library load custom architecture unless pointing architecture fix try find custom,neutral
since architecture model need add new architecture following need defined library model,positive
another question basically want classifier head class provide clear description load appropriate different size example want reuse layer,positive
thanks response exactly get following error reading model remote repository import processor model appear file available,positive
anyone reading future found work around rename remove dot name follow technically fix following go around issue pull request find create another dot name example put code custom model push dot name pull request make sure without dot name example type image float dev,positive
llama fast issue tried slow version,negative
hi think could use match text length python import import torch import image import import image return image processor model text woman dog sitting beach woman dog beach processor text model print image text probability model print image feature text feature cosine similarity get image text compare text python import import torch import image import import device else image return image processor device device text woman dog sitting beach processor processor print print,neutral
lose load image image,neutral
would mind taking look thanks,positive
hey nice work also interested blip retrieval specifically finding relevant text query regular user understand final want give feedback help testing try pas multiple model get error valid use case class intended single image multiple usage code python import import image import import image return image processor model text woman dog sitting beach processor model error recent call last file line module model file line return file line return file line forward size must match except dimension size got size tensor number list,positive
line probably gonne removed ready use torch nightly torch,positive
hi tried device make causal mask used mask standard causal attention slided window attention mask range start end min mask start end mask mask mask return mask none none function mask according window size assuming length input mask like standard causal mask attention like wondering change enough plain vanilla swa implementation interfering original original one window please share really appreciate,positive
problem find reason thanks,positive
cool thanks affect improvement think lot continuous integration testing tend skip seem unaffected change done go wrong time time one seeing said many tend separate process continuously full gamut fast slow integration build imagine something like well might working right reason,positive
checked change black ruff made sure right version ruff make style make quality seem make reason,positive
good catch yes oversight curious needing edit see failure passing,positive
hi bug thanks catching confirm interested caught pull request commit added also confirm current state undo improvement loading time add,positive
want investigate attempt find root problem comparison new model code previous one update fixing fa amazing model,positive
happy help begin multimodal,positive
hi would like continue working received reopen create new thanks,positive
got answer got response argue user project exact project basically said trying instead sheep answer code line assign child module import array open file array row pipeline array simple resize correct overwrite content better error,positive
still getting enough testing environment get,neutral
branch master let see pas,neutral
thank much contribution pretty new since marked mean use pipe pipeline given automatically use sequential method something else reason ask code first post successfully data module however try read file processor result result get error result file line generate file line file line unsupported operand type suspected something read file printed output array float,positive
plan close near future take time,positive
way use torch graph wrapper appear working due line none equivalent model,negative
make ready yet generate check past key value class signature take give something like work making,negative
command original make sense code work exactly like section,positive
recipe added testing environment,neutral
point add test case case also update documentation,neutral
worked might also consider thank finding,neutral
working general specific hardware tried fresh source today tried test script forward correctly multiple,positive
code well attached unit test exercise bug sure key difference test look get chance,positive
made appropriate minor ready another review,positive
sure new argument forward call model strictly backwards compatible example motivate following work python import torch import model eager random input id hey run code get following error file line forward size tensor must match size tensor dimension summary full file line module file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward size tensor must match size tensor dimension forward call none reshape attention layer instead reshape none instead slicing insert extra dimension size mismatch add attention mask user need specify argument forward call order work overall think avoid extra require code user especially already design like flax keep track internally abstraction user,positive
hi still bit confused like tried loading like see prompt issue even though custom code merge like know actually trigger bug example occur local rather hub specific value issue code issue,negative
yes remove old sooner rather later given already work due conversational removed hub,negative
thinking bit indeed maybe make variable potentially could revert original behaviour would happy adjust accordingly need add new variable blip class set,positive
really breaking even opposite subtle respect training see comment,negative
ran stated execution pip install optimum export model image,positive
hi nice work getting across line hub suggest reaching lead paper author would happy host model hub profile model obvious go experience everything need done,positive
figure need look locally currently failing pas quality need fun make make,positive
comment working date fork sorry confusion,negative
change could expand bit mean ago comment saying still getting reflect run verify,negative
indeed thanks flagging would like open fix way get contribution,positive
thanks lot indeed issue fixed getting exact metric batch inference flash attention looking forward next version,positive
someone please merge write access,neutral
way get transcription know give would useful would great feature addition,positive
probably also make sure install accelerate notebook require accelerate training say also pin currently set main,positive
sorry bit late yes error recently default behavior language detection language train model shown notebook make sure pas hi model try detect language transcribe,negative
hey thank much quick response best environment version platform python version version version accelerate version accelerate found version true version na flax version na version version script yes distributed parallel script code defined runner class trainer metric want reproduce issue ignore self model compute metric function self print compute accuracy correct label label zip accuracy sum correct correct metric accuracy return metric code issue get model right none auto else none get data main train train test initialize trainer classifier answer collator trainer train evaluate float convert back float param come class instead writing cause past issue script execution interrupted evaluation metric calculated logged error thrown image hope gladly answer additional,positive
thanks update translation accordingly german translation hugging face hub also formal hugging face course outlier,positive
hi please take look whenever time thanks,positive
pipeline function work give good example ground truth pipeline ba following tutorial,positive
hello support zero show run non trainer example accelerate normal training loop official example experience lightning could look provide minimal example run code missing lot run,negative
thank code like input south girl stock severe piebaldism profound congenital deafness output south girl stock severe piebaldism profound congenital deafness code real output south girl stock severe piebaldism piebaldism profound congenital deafness constrained done second constrained also like model function like code model task list none none list none assert none none none model zip key value key value sent sent none range none none range sent sent status sent status status sent status sent else raise return sent status sum sent status return status return else return sent sent return else return sent sent sent sent sent sent sent else return none return else none sent sent sent sent else return none return else none sent sent sent sent else sent return else return return else return else return else return sent none enumerate sent none import return sent sent mention sent task none none mention else raise return sent return return thanks,positive
hi thanks raising issue best help could please provide information running environment run terminal output provide minimal code snippet reproduce error moment important information missing example particular instance self information issue behaviour without completely stop evaluation loop could share logging output run,positive
hello thank code calling method distributed process group whether distributed training run place know process rank work trainer please create instance calling instance distributed process group information,negative
starting think become history,neutral
try upgrade library stable version pip install set following variable empty string python import o none work try,negative
hi saw got way use generation wrapping entire model torch graph wrapper right getting graph breaking thanks get outlook android sent mention subject torch generate issue need final review week reply directly view id,positive
inside docker vent set yes,neutral
hi thank much additional sorry assumed already previously added made necessary code furthermore test suite successfully please let know anything thank much,positive
remember discussion form sie last time hugging face team decided formulary sie rest,neutral
write access actual merge,neutral
problem resolved latest version,positive
thank fix dependency issue believe need add torch otherwise list added dependency enough install dependency thank applied still breaking import unfortunately try think weekend,negative
library inside docker container set python version,neutral
case need add custom architecture hub first problem related pipeline model kindly fix might also consider reading custom architecture working,positive
understand error code work define issue section,neutral
sorry always really hope wonderful day far go everything pretty much main branch change added method far method go pretty much working already used multiple time push custom hub check commit made version documentation section documentation misleading little bit personally really think fixed different pull request since feature really important let know think tag really need pull request since need test stability issue,positive
hope feedback community welcome nice work,positive
tried weight directly seem different original loading saved model,positive
hi ca find root cause tried running like could extract much output think relevant part could take much different stack trace self self model model file generation file generate saved model model le equal also tried running test model weight loading saved model match except prefix suppose image mean,positive
anyone still could figured big file tracked git maybe file extension list simply ask git track file example git track,neutral
know anyone particular afraid maybe help since german grin,negative
passing also ensure model output eager please take another look get chance thanks,positive
thanks iterate fast possible get let know soon done,positive
ran issue test reason failure source unrelated change issue bias term tied test ended failing bias well added commit get test pas let know whisper model bias fix affect gain change,negative
unfortunately time contribute moment spread thin look unfortunately simple fix probably hack could entire design assuming certain pattern exist looking time work figure support fast source code saw implementation based factory pattern extended bridge facade pattern normally issue multiple pipeline coerce user remotely access vocabulary dug bit learned conversion seem well working personal moment higher priority happy bounce one another sure able dedicate time implementation though made throughout process,positive
hi thanks reply accelerate along see file post accelerate running script command accelerate launch attached script run see ram go mistral model twice much space take given billion twice loading model plus space taken environment would roughly think accelerate loading mistral model ram according fix import import import import import trainer import define training false epoch default default default training run number cosine default linear define import model model import data data right mask object result text return result text collator none train model model trainer trainer save model,positive
run similar example making similar see usage engaged though python import torch import print model,neutral
hey dictionary defined whisper model since guarani whisper model avoid dictionary note still whisper guarani without change detailed guide refer tutorial hi model trying inference part get following error unsupported language guarani language one creole trying get python processor guarani work let know fix try pipeline function see work,positive
hi thanks raising issue delay issue weight,positive
review signal approve ready final review,positive
maybe easier merge work top version,positive
would micro release reserved patch resolve newly breaking fixing last release next minor release release roughly monthly schedule would around two want use feature immediately install source,negative
none worked case could downgrade python version without causing issue code note issue worked fine,positive
totally sense close comment case anyone else come searching future link topic forum,neutral
integration test missing model used model,negative
still several failing related resolved ping final review,neutral
hi thanks raising issue similar issue posted resolved could try source confirm issue,positive
next minor release come included,negative
matter text put accept flaky master,neutral
hi following extra branch main,positive
matter text put accept flaky,neutral
thanks input leaning towards convention case actually copy logic one think cleaner reuse logic inside self code help avoid code think already bit issue way think need call otherwise missing dropout,neutral
deal going hack look current directory restore current directory test run think root cause open new confirm basically via pas subsequent call partly take either eventually call take none,positive
fact bug indexing thanks,positive
issue today used notebook transformer package guess issue version notebook preinstall package worked hope help,neutral
think order consistent keep name since attention class,positive
merge failing test unrelated model tag flaky flakiness internal slack,neutral
simple merge think main question whether want use approach,positive
hi thanks raising issue order able use custom class register corresponding auto class following would load model import model find,positive
original designed individual stopping criterion class support class list pointed today assuming use,positive
head marked ready review dig couple get pas let want chat thanks,positive
yes hypothesis somehow code gather statement single training epoch done going next epoch need test hypothesis,negative
see issue training perhaps need gather setup finally test,neutral
actually error like following python device line return input line forward return line return line return line forward finished reduction prior iteration starting new one error module used loss enable unused parameter detection passing argument making sure forward function participate calculating loss already done distributed data parallel module able locate output return value module forward function please include loss function structure return value forward module issue list iterable parameter index receive grad rank addition set environment variable either detail print information particular receive gradient rank part error,negative
system driver version bug unless set work without set,neutral
used basis data sure portion code addition able run code single already local notebook run cluster work hypothesis somehow differently perhaps notebook configuration,positive
hi thanks opening issue reproduction could provide minimal code snippet error get many per day address timely manner need help u help,positive
thanks opening provide context necessary passing around behaviour try avoid possible,positive
alternative rope break faster vanilla issue save tensor loading wrong sure python class self dim super dim base device build make work self device different paper different permutation order obtain calculation forward self co sin return co sin image,negative
hello working might merge change making propagate bit different kept line class level class line use corresponding necessary go instead rationale behind code code figured better name let know think discus approach better adopt would mind looking let know think like implementation doesnt create complicated simple merge think,positive
think major public way exploit code would get run manually general think fine test suite conversion like ca modify insert malicious code get silently executed,positive
ran model loading missing,negative
generation problem usually better use case enough data train new paper summarization task,positive
would expect use model master would extracted save saving method valuable precision saving happen problem size simply ensure model loaded starting training,neutral
thanks code quality need run make push behaviour reset see logic already happen fixed,positive
thank issue well suggestion fix sense would great want open fix,positive
hello use feature would need use accelerate along accelerate launcher use please refer,neutral
hey everyone quick question shut warning token index sequence length longer maximum sequence length model running sequence model result indexing figured deal large want switch error,positive
problem mistaken generator pipeline argument take precedence anyway function issue,neutral
hello model saved selected training else logic convert increase size hence current logic limitation use case,neutral
try write something appropriate original notebook posted could illustrate problem use problem instead binary classification problem might good publicly available test reload model random linear layer end instead one giving random people properly save final classification layer,positive
try running training single see issue since data somewhat sane perhaps also set,negative
ready merge list input opposed tensor serialization,positive
hello could please share minimal example run deep dive,negative
hi thanks opening issue could share minimal code snippet reproduce error,positive
still issue classification tried latest accelerate notice notebook binary classification could account discrepancy,positive
got busy get last worked think pretty close clip still update specific long enough code would require major work,positive
two failing sure error understand small made llama could impact,positive
bit attentively original paper section set single pas set significantly typical number image could find analysis impact number see lowering much hurt model still would expect rather bad performance outright nan,negative
think drop much would kind like way know try since use,positive
would like open bit low currently,neutral
got bug side think need update converter,neutral
hey think feature actually useful replace standard attention swa replacement without wo performance drop,positive
however experienced negative input text,positive
going give helping hand trace function check got try help tomorrow thought post public let know start,neutral
thanks wed wrote hey thanks opening issue try keep could ask question forum instead sure community help example would recommend properly define class generate something like reply directly view id,positive
low think sense lot,neutral
hey easily done custom register,positive
hey thanks opening issue try keep could ask question forum instead sure community help example would recommend properly define class python generate something like,positive
hi thanks quick review like point recent commit comment yet support added field make check let know suggest,positive
sure probably faster community help review translation since pretty busy let see organize something awesome community help review hi thanks support really appreciate attentive feedback,positive
thanks like close issue,positive
sure probably faster community help review translation since pretty busy let see organize something awesome community help review,positive
also found smaller learning rate fixed nan issue,positive
hello working might merge change making propagate bit different kept line class level class line use corresponding necessary go instead rationale behind code code figured better name let know think discus approach better adopt would mind looking let know think,positive
hello working might merge change mostly go get rid downstream let know happy discus chat best way forward necessary,positive
sorry wrong ca really help thanks suggesting,negative
please take look guide,neutral
hi thanks answer since trying calculate perplexity answer guess code incorrect since probability selected check confirm close,positive
hi likely case want model sampling look feature get,neutral
speculative yet compatible training unless manipulate code,neutral
typo part meant cause running prediction understand idea crash reality logic breaking change stop working previously work,negative
still failing run recent version main resolve,positive
error self please use one following class instead raise self current model class compatible language model head,neutral
try latest version main,positive
also error could provide environment information run snippet run reproduce error,neutral
oh nice guess could get fa free eventually upgrade thanks link similar work think could cause merge message try resolve go,positive
ticket since issue ticket,neutral
issue please let know solution done,neutral
thanks think yielding around previous,positive
yes similar built support flash attention depending environment flash attention yet possibility road built already need many end,positive
hi model run still see,neutral
hi usually like issue tensor naming one correctly saved loaded reproduce error locally try running test log model loaded see see name missing weight,negative
curious similar also highly related,negative
per node total network note training long enough successfully save disk trying write second training later,positive
took body script added test see latest,positive
hi know might cause got locally testing also last time remember seeing related error safe,positive
wrong several people want decision review include feature indeed sampling argument active default substantial part signal given comment issue might useful,negative
hi looking stack trace like error open issue,neutral
sure want remove conditional logic inside like able fully configure backbone behaviour use case image want create new model train instead default architecture want use different backbone want backbone return different feature default moment possible behaviour hard ca load different easily ca pas specific configure backbone ca configure backbone passing set feature completely agree would better one argument context first added four backbone whether load backbone behaviour model backbone model used load backbone backbone backbone used load backbone backbone backbone mutually exclusive type configure ideal think solution alternative new backbone everything old used would able immediately deprecate case code old want happy code something decide best two context example final step removing modeling see need single call backbone,positive
give u information setup please many single node computer,positive
understand try python custom pipeline python custom pipeline try difference configuration,neutral
hello need different auto wrap policy see,neutral
yes thought cache number fix unfortunately prevent get desired time look depth took quite time last two day let unblock quick way check later,negative
also facing issue setting training continue longer still eventually hit error like file directory,neutral
hey idea failing test think failing branch locally,neutral
pretty sure solution add pipeline case fixed,positive
hey thanks opening try leave custom loss computation user providing trick,positive
redundant type hint first torch library second torch data type within library actual conversion within second torch unnecessary,negative
problem well trying change parameter sometimes even unable train try test everything ran whole tried overfit single image giving image run could worked like charm starting without single image,negative
thanks reply try open feature request,positive
hey still documentation wo pas yet related,neutral
personally think bug rather correct behavior error triggered param set user either opt beam search beam sample two sampling keep candidate searching really return beam sample sample multiple time user get multiple random sampling param removed,negative
please update version fixed next patch week ago,positive
hey want help going need reproducer,neutral
gon na great could take look time,positive
feel free open issue track feature request,positive
demand sure seen yet,positive
could make sure green,positive
test tried take close look current unit th said need test least one confirm get correct loss case fount inside class test forward method suffice yes suffice test look merge alright,negative
gently might still worth,positive
think new solution elegant,positive
like good addition let review final pas,positive
suggestion logic model would complicate lot pipeline code require lot code included directly pipeline code since much related keeping separate support token box support like easier maintain remove redundancy thinking something like model model corresponding support box infilling none place infill token python none mid extract prefix suffix mode else else directly since infill token box need token please let know opinion,positive
look main pain point overhead generation token overhead tensor mode,positive
hi thanks detailed reply understand current situation thanks,positive
fa interface wrap model forward generate following context manager,neutral
hi due fact returned attention might correct unfortunately false consider enable side returned output correct,negative
sufficient trying compute attribution respect input first sequence pas get error,positive
forgot pull first think last three similar sorry,negative
main branch broken really lost would love get feedback sorry pushing multiple time really fix literally identical,positive
hey like new related could take look,positive
hi going familiar respect,positive
hi thanks issue think class meant used class automatically list none agree could make one way could initialize none range preserve keep consistency across,positive
ca review content since native speaker know anyone community would interested work hi thanks work,positive
currently transformer accelerate training one machine seeing mistral model loaded onto ram processor understand since fix transformer accelerate load model onto independent number insight anyone super file false true false false true true main static true false false false,negative
resolve merge main branch,positive
got error tried pas output,neutral
thank continue digging mismatch,neutral
hi think latest version main fix one also final review fully green think left,positive
done missing error gone though unfortunately still fixed let know get stuck,negative
hi looking code like saved buffer actually easy rewrite code replace constant push something,positive
however failing documentation test understand issue idea reason,neutral
alright believe ready thanks patience help,positive
copied specific meaning actually verify two identical code like something like skip copied,neutral
latest version got running,positive
hi think comment left unaddressed one seem,neutral
yes unfortunately lot compatible past current culprit unrelated,negative
thanks review failing think related feel free merge think fine let know thanks advance,positive
hi facing issue self none done solve problem thanks,positive
hi thank quick patch still see missing gated cross attention layer model used model thanks help,positive
posting keep going stale,negative
hi give code reproduce issue picked recent release custom get prompt ran sure working test could get test suite run properly test script rename dependency quick create try put temp directory could seem get param work either likely broken drop meat test work test,positive
thanks handling version compatibility except,positive
visibility going merge keep green,negative
hey build suspect missing problem built correctly hopefully also correspond bit closely let know still happening fix dig,negative
anyone best practice far inference like chat completion set padding token whether set new pad token resize layer set default token commonly used set padding side left default reasonable try see better result,positive
facing issue pip install pip install still error,neutral
mostly corrected review also could take quick look custom generation mostly,positive
failing disappear unpin torch,neutral
good merge torch job,positive
hi give code reproduce issue picked recent release custom get prompt ran,neutral
failing already main idea torch failing already ago let move check torch equivalence later also failing known waiting take look,positive
failing related let know fix failing,neutral
sorry end lost track try see come,negative
yep think might accidentally original problem fixing prompt let take look fix,positive
like good idea clearly throw specific,positive
thank sorry get chance get,negative
thanks warning quite urgent catch,positive
pull request add highlight need fixing,neutral
could fix since way around leave final decision pipeline pull request add method people push custom pipeline easily hub refer notebook,positive
wow thanks quick response fix,positive
hey thanks really relevant could give another tour core maintainer opinion know yet efficiently initialize part opinion,positive
thanks added extending regarding question subtle difference used first token generation token used sentence provide calling generate filled start generation general format generate automatically provide,positive
specific error message red herring real issue rank getting rank getting rank blocked loading rank start training behavior anyways pretty clear going close thank helping figure,negative
resolved conflict hope right way else let know,positive
use device directly let know insist use torch full,positive
great thank behavior though optional model different token id token may want change something like union optional model different token id token optionally use shape specify prompt passing generate tried passing tensor instead match,positive
agree bug code happy close issue see improve user experience raise error output provide function generate random string across,positive
issue part conventionally need provide output directory trainer,negative
ah sense part code temp last back worked fine guess might change launcher point process create different temp directory name maybe point process charge loading process broadcast process load shard different folder user code standpoint script executed multiple time sure could define temporary folder name used script temp folder input ugly sure anyone probably different error maybe calling else none,positive
could luck trying secure malicious,positive
oh yes notice already ran code worked fine import logging import o import import import import torch import import output output message copy remote cloud storage local use two local example output lambda range model trainer output temp directory related issue change code import logging import o import import import import torch import import output output print message copy remote cloud storage local use two local example output lambda range model trainer output see different process instead directory setting auto detect,positive
sure basically get error snippet replicate problem rather long tutorial object detection import trainer import import index index enumerate category area range category category area area list return area image zip image image area image category zip area return batch item item batch item item batch batch batch batch batch return batch train train trainer trainer train,positive
hi thank looking see code added could statement added block already another,neutral
hi team resolved suggestion code review master branch rather though know whether appropriate yet hopefully would single commit right test tried take close look current unit th said need test least one confirm get correct loss case fount inside class test forward method suffice figured deal covered forward could leave thanks,positive
hello thank minimal reproducer replicate unrelated error also notice complete recent call last file line module file line train return file line step enumerate file line file line file line file line wrapper return file line type ignore trying create tensor negative dimension killing killing error return code also directly output directory without logic temp output everything work,negative
thank thought may due different reason strange behavior previously tried batch inference one image sample try find another example later see still,negative
training inference change posted description like decent across board think would add lot think could good addition,positive
minimal notebook example wherein task linear load model inference via everything working please let u know recent fixed issue,neutral
great able want open add help community,positive
hello latest accelerate call adapter get saved inference would need use please refer saving loading adapter want save base model adapter please following model load usual,negative
thought let see reproduce public report back,neutral
thank comment latest commit,positive
figure trying use work however following work able finish path,positive
sorry big change rather take care small batch decode decode ca handle well impossible distinguish difference list power without logic extra yes becomes big change since already may take care based need sorry direction library bit different explain think code style wo affect direction anything may improve removing everywhere indeed way flax cool glad also think better consider consider import statement frequently used definitely bad idea coz function look every time even python one true import one library heavy operation may also need consider necessary true import,negative
flax different scheme alright cross flax,neutral
yes definitely would like open,neutral
thanks pointed wondering change error message something informative,positive
sorry big change rather take care small batch decode direction library bit different removing everywhere indeed way flax,negative
hey thanks error people use native implementation enable feature well,positive
image flaky marked flaky automatically,positive
alright need use copied everything thanks explaining,positive
sure feel free open update doc,positive
might worth raising issue warning new setting used,positive
hi thanks explanation answer discrepancy place layer output notice right end matrix actually output sure nope none also still getting warning symbolic weight tried fixing commit still see warning please take look get chance ran latest code gibberish output keep digging linear output output,positive
would helpful kindly share,positive
someone clarify ratio training per epoch total training latter know total number total number training model going train,neutral
confirming issue marked stale still,negative
acknowledge issue fast attention training loss hardly fa turned work pretty well turned,positive
hi want specify different element passing tensor shape case line generate solve issue shape,neutral
hey sorry late response issue model slow yet find time resolve getting back soon possible week,negative
think case try different per prompt chat let say first prompt one image second prompt attention mask look like bash image prompt image prompt image prompt think reason prompt python prompt image image difference two prompt image describe prompt image describe image image processor prompt prompt prompt image image image image getting attention mask presence two image prompt try following prompt image image difference two prompt image image difference two prompt image describe prompt image describe image image processor prompt prompt prompt image image image image way attention mask become standard believe think,positive
latest worked thanks song,positive
loading model setting adapter separately work thanks lot,positive
also found similar issue tried implement batch inference know attention mask theoretically use standard mask,neutral
think related model attention mask script,neutral
hi trying load adapter different branch main need pas revision argument python import model revision,positive
branch merge verify conversion,neutral
useful need done get anything would helpful take look,positive
need cleaning leave another pull request since affect image,neutral
feature resume training exact start new training want change understand certain would difficult change within library like possible could raised feature request,positive
continued handling compatibility final fix main could try,positive
hi thanks raising issue issue torch implementation support like suggest opening feature request,positive
thanks review together green one unrelated test failing see also regarding would use keep deformable,neutral
hello please see reason possible raise error,neutral
legacy load particular file load perfectly fixed upstream doubt trigger warning,positive
hi thanks raising issue could provide information behaviour specifically seeing error processor could provide minimal code snippet run reproduce error sample data model public,positive
hi would need access audio file able reproduce issue reproducer,positive
hello gone implementation specifically compute store related metric store private attribute specific used log method log hand use need pas function trainer instance compute metric interested taking input parent trainer evaluation loop thereby metric unavailable leading key error resolve need pas internally prediction step adapt issue trainer handled calling,positive
thanks review slow pipeline pas make subsequent correct fact long transcription script beside whisper never,negative
great new feature way get transcription know give would useful,positive
sure yes trained exactly purpose one input always constant close issue feel free discus,positive
solution dig digging trainer actually wrench turn format following example python replace path,neutral
relevant u use directly could adopt avoid warning,positive
looking consequence used torch load state error thrown type particular warning message removed future storage class matter directly access directly use instead cause issue use unsure meant directly,positive
answer everything clear understanding generative model probability every piece case progress training learn relation first text sequence however guess entirely true generative aspect true particular provide initial context first prompt otherwise generative conditional generative generative entire right understand saying correctly truly generative setting special token probability always first token every text sequence something like token trained token generative think,positive
hi thanks raising issue problem coming commit used install image added suggest recent version,positive
way say always since input given user certain,positive
ca compute first token given user since constant apply rule,positive
thanks running notebook version pip install ran today worked quite sure issue bug,positive
thanks sense intuitively flamenco style single whether image text mistaken easier plug play see pretty significant advantage good inference useful ca use well production need read guess drawback entirely trained scratch wonder whether would better take approach making,positive
thank answer case longer first token probability correct want know compute probability first token token token,positive
run forward pas always distribution next token quite simply model let say compute distribution,neutral
simple fix let send,neutral
understand change necessary error normally raised attention provided solve issue please either load model argument eager pas input tracing model explicitly solution,positive
know exact still used fused already believe course,positive
gentle ping sure fixed,positive
high efficiency support fallback enable,positive
get following error since batch size therefore dimensionality matching match broadcast shape also used know handle line,neutral
could provide make sure latest version main,positive
sorry afraid nothing help situation least one tip future use git merge use git rebase keep linear avoid undesired situation,negative
anything unwrap data loader naturally split data already run unwrap model see device better try,positive
sorry see simple unwrap generate work across multiple batch split everything manually,negative
trying solve let know ready need assistance,positive
older version case error still used case correct answer decided add error make sure nothing get silently,positive
hey reproduce main python import work make sure use latest version,positive
thanks sure open question forum get additional help thanks suggesting another small query regarding said error generate doesnt accept given code working older version without error like sense would mean model accept older version older version case error still used case,positive
update title tried copied llama top class facing entire class code bloom make bloom param forward function llama seem handle anywhere bloom forward llama add commit reason error greater equal figure flaky besides exotic model test,positive
general model trained special special mind use padding typically use token fallback generally work sense think sense coming trainer model team side,positive
thanks feedback think could support past mistaken,negative
think use default fused layer norm maybe whole difference make sure also run greedy generation avoid potential bottleneck implementation,positive
whether original token choose content token used use something like make sure exist solve something like yield want,positive
code provided manually set token course force behaviour way need define string token useful community whole training,positive
hey thanks opening issue try keep could ask question forum instead sure community help mean time absolutely use new model old simply missing code issue easily generate accept used model model signature either add remove whole place step step,positive
could explain unable reproducer hub,negative
hey sorry late reply thanks opening issue try keep could ask question forum instead sure community help thanks,positive
probably padding side issue skip special longer see get rid function anyway keep encode decode simplicity recommend use encode,positive
hey sorry think forgot merge thanks,negative
think either time tackle marking good difficult issue,positive
hey thanks opening issue try keep could ask question forum instead sure community help thanks,positive
week waiting green light merge clear,positive
good catch would like open fix,positive
need final review week,neutral
never use bad industry practice lot represent string solution overcome size limit correctly padding model architecture thus token addition without performance loss,negative
hey pretty sure look first first activity anyone take update contribution really enough ca track progress stuck even sorry,positive
thanks let try ping many people please,positive
probably running local host latest version make sure update pip install import print,positive
following python import prompt hey prompt print print prompt print prompt print prompt hey print prompt hey thank advice main problem original implementation pad token set implementation hugging face situation unsure set pad token setting token would meet therefore like special hack case would like ask acceptable definition repository merge implementation model custom implementation could corresponding repository hugging face example,positive
following python import prompt hey prompt print print prompt print prompt print prompt hey print prompt hey,neutral
either provide additional explain issue let know really glad assistance,positive
hi alright ca wrap head around git problem feel like ended branch tried think erase completely branch create fresh one main branch could put back work done past,positive
worked bit probably related model thank,neutral
believe need unfortunately ca get system right think true true resample size maybe give try system file found model see whether work adjust system work click contribute model page submit fix,positive
also issue well thing working perfectly fine may right supporting mean model therefore testing right also,positive
hi issue duplicate share full error fix issue hub gut feeling model compatible model code need make slight change make work also post issue model full issue,positive
hi working hi still working take want issue change code several would happy try work might need guidance think also previously might willing help,positive
relative path issue size mismatch python loading recent call last file line module model file line return file line file line self file line file line raise loading error loading size mismatch param shape shape current model size mismatch param shape shape current model suspect might training training command mistral model use used llama model size mismatch issue adaptor merge used llama model none print use mistral model use used llama model pasted,neutral
quick update tested main yet currently longer returned transcription however still returned transcription dev,positive
thank got much validation end range good enough hi may ask solve problem got stuck know,positive
thanks alright make sure heck evaluate agent well code run also thanks check today,positive
thanks posting good hint fix underlying bug,positive
thank fixing found another case learning use increasing value scenario used also problem,neutral
hi exactly looking close try,positive
hi everyone one question fix supposed work part unwrapping model memory access unwrap one shard model saved,neutral
hi issue fixed thanks eta get blocked issue need resolve export issue,positive
working flax implementation phi based flax llama code attention rest pretty straightforward expect ready couple,positive
case already radar let know cause error import like import error import following error look see module attribute guessing due change generally error come import like might fix,negative
issue latest resolve issue,positive
great looking forward seeing eta happen sent mention subject torch generate issue still much active reply directly view id,positive
determined cause usage indeed axolotl default false gradient going go ahead close issue,negative
hey see ended resolution clear padding token something else thanks,positive
follow need done another issue modify based information,neutral
hey took quick look pretty ready quick cleanup get repository consistency pas everything else green besides one yet,positive
question make new one new vocabulary example completely different vocabulary make new vocabulary extend vocabulary new vocabulary part really regarding issue many included add completely new ca find hugging face better train find best combination tutorial hugging face,positive
question make new one new vocabulary example completely different vocabulary make new vocabulary extend vocabulary new vocabulary part really,positive
hi thanks response would like avoid duplication work future selection process first comment first thought first comment outlined contribution documentation handled,positive
hi false loading theory share similar latency,negative
hello long doc page open feedback thanks,positive
also linked discovered logic flow fitting abstract separately deal actual main try stick fix necessary satisfy,positive
yeah training verify applied correctly,neutral
hello circling back issue used pinpoint transcription print text turn hallucinate much python sentence ref police superintendent said accused court covered struggle independence mau movement peaceful gathering town killing paramount chief swore new provisional electoral council cep nine yesterday continent relatively small many independent normal multiple would mean go visa passport control multiple time thermal behavior steady large earth often maintain fairly constant temperature consistent deep ground said glen united geological survey team northern university flagstaff sleep interruption process purposefully awakening normal sleep period falling asleep short time later complex rebuilt order give better idea originally angel continuum approach method used help reach higher level performance due underwater topology return flow concentrated fast current deep water may form towards end middle western develop style one biggest time result people use button fasten clothing system would allow aircraft fly shorter save million fuel year cut carbon full percent water planet come likely notation added simply label deputy prime minister wong kan trade terrorism prime minister lee disease carried like calcium potassium considered course also like silver gold field large part classroom quite often teacher would love take bus trip option people write computer never come close sharpener one common used illustrate importance socialization draw upon unfortunate neglect misfortune abuse growing added however take go beyond development stage responsibility damp clothes help dry many iron board available loan even one present room capital town island population curry dish based together either meat four sitting group finish total giant rank race debate sparked controversy spending relief reconstruction wake hurricane fiscal humorously new deal two free southern turning towards great pyramid one seven still standing today two react one another form may block kidney function university said technological determinism share two general development technology path largely beyond cultural political influence technology turn inherent rather socially conditioned hyp police superintendent said accused court covered struggle independence organized mao movement peaceful gathering town killing paramount chief swore new provisional electoral council nine yesterday swore new provisional electoral council nine yesterday continent relatively small many independent normal multiple would mean go visa passport normal multiple would mean go visa passport control multiple time thermal behaviour steady large earth often maintain fairly constant temperature consistent deep ground said glen united geological survey consistent deep ground said united geological survey northern university flagstaff process deliberately waking normal sleep period falling asleep short time later best way use continuum approach help achieve higher performance next gen system would allow aircraft fly shorter save million fuel year cut carbon system allow aircraft fly shorter save million fuel year cut carbon disease carried common used illustrate importance socialization one common used illustrate importance socialization draw unfortunate neglect misfortune willful abuse adult growing city capital city home population people debate sparked controversy spending relief reconstruction wake hurricane fiscal humorously bush new deal southern turning towards great pyramid one seven still standing today two react one another form may block kidney function university said technological determinism share two general development technology path largely beyond cultural political influence technological determinism share two general development technology path largely beyond cultural political influence technology turn effect society inherent rather socially conditioned number transcription work quite well lot sequence indicate work better low tried already get much better wer python sentence ref police superintendent said accused court covered struggle independence mau movement peaceful gathering town killing paramount chief swore new provisional electoral council cep nine yesterday continent relatively small many independent normal multiple would mean go visa passport control multiple time thermal behavior steady large earth often maintain fairly constant temperature consistent deep ground said glen united geological survey team northern university flagstaff sleep interruption process purposefully awakening normal sleep period falling asleep short time later complex rebuilt order give better idea originally angel continuum approach method used help reach higher level performance due underwater topology return flow concentrated fast current deep water may form towards end middle western develop style one biggest time result people use button fasten clothing system would allow aircraft fly shorter save million fuel year cut carbon full percent water planet come likely notation added simply label deputy prime minister wong kan trade terrorism prime minister lee disease carried like calcium potassium considered course also like silver gold field large part classroom quite often teacher would love take bus trip option people write computer never come close sharpener one common used illustrate importance socialization draw upon unfortunate neglect misfortune abuse growing added however take go beyond development stage responsibility damp clothes help dry many iron board available loan even one present room capital town island population curry dish based together either meat four sitting group finish total giant rank race debate sparked controversy spending relief reconstruction wake hurricane fiscal humorously new deal two free southern turning towards great pyramid one seven still standing today two react one another form may block kidney function university said technological determinism share two general development technology path largely beyond cultural political influence technology turn inherent rather socially conditioned hyp police superintendent said accused court covered struggle independence organized mao movement peaceful gathering town killing paramount chief swore new provisional electoral council nine yesterday continent relatively small many independent normal multiple would mean go visa passport control multiple time thermal behaviour steady large earth study earth crust often done relatively stable temperature consistent deep ground said united geological survey northern university flagstaff process deliberately waking normal sleep period falling asleep short time later due underwater topology return flow concentrated fast current deep water may form towards end middle western develop style one biggest time result people use button fasten clothes next gen system system allow aircraft fly shorter save million fuel year cut carbon discus trade terrorism prime minister lee long disease carried like calcium potassium considered course also like silver gold field large part classroom quite often teacher would love take bus one common used illustrate importance socialization draw unfortunate neglect misfortune willful abuse adult growing island home population people debate sparked controversy spending relief reconstruction wake hurricane fiscal humorously bush new deal southern turning towards great pyramid one seven still standing today two react one another form may block kidney function university said technological determinism share two general development technology path largely beyond cultural political influence cultural political influence technology turn effect society inherent rather socially conditioned number also quality transcription longer audio despite indeed impression reach end could maybe bit transcription algorithm work moment chunk common sequence already sequence composed concatenation previous chunk add chunk match way improve could maybe linked stride fact stitch different,positive
use entire class one failing flaky former given explanation hope seen latter pray flaky,neutral
great integration step would great think unlikely see would integrate different closed project simple possibly building good way go key feature see valuable continuous really spin end point production much stretch try add continuous guessing yes considered generally getting modal,positive
thanks everyone nice evening hope,positive
hey indeed forum best place ask linking may help reference question get going answer whisper author thread hub,positive
guess still super easy achieve subset run process log example might get test wo work concatenate many test paste long line terminal run test utility grab name seen process realistic urgent task could work later,positive
context anyone coming cold motivation enable run process help recently issue flaky affecting example global logger state resolved like happen sporadically always process easily narrow culprit subset size,negative
hi yeah issue key torch layer lot like bias superclass code ported layer special stuff like result think true suspect either cause error cause bias uniformly set value cast integer however kind irrelevant forced code never actually use result think implementation fine check big discrepancy layer discrepancy equal either layer give output,positive
working cluster took forever resolve issue however finally worked,neutral
hi thanks idea even think make contiguous sense problem model training well found odd error occur since built upon,positive
feel free ping additional help glad help,positive
thanks ratio wrong correct,negative
sorry time come back think push hub,negative
raw audio audio instance audio reduction factor instance remember actual ratio usually mel spectrogram also factor remember top head name factor mel basically raw audio happen,negative
yes possible load though many load generic yield error,positive
sorry get notified previously first please use main method use text proper format instead ca really say error message like modeling class definition file number actual could custom modeling file follow error resolve step step way go,positive
like outstanding package open fix final rebase,positive
sure happening honest commit history good pretty sure independent going see elsewhere,positive
sure let unblock red first fine probably need green need,positive
thing actually even remotely competitive something like llama even obviously making,negative
mean might want change daily well,negative
might nightly sorry mean mean daily time check recently quite lot,negative
great model lot community feel free open ping review,positive
look detail global attend token one token attend global information forum better place question,positive
two failing error know fail considering minor last commit,negative
le ready would love get feedback need bit custom work compile step already help,positive
behavior duplicate try pas relative path instead run script final folder submit fix,neutral
thank quick bit sensitive merge,positive
gently look stale quite time,negative
yeah waiting one sure already another another think let know,positive
issue related evaluate think,neutral
marking good first issue doc,positive
feel free open ping,positive
failing test unrelated merge,neutral
stale still working hard put model working small model pas requirement lower priority unfortunately finish finish within mid,negative
hey thanks fused default per documentation,positive
hey would recommend try,neutral
think seen something similar recently,neutral
throwing o error even model,neutral
usually assign rather let code talk open pinned someone working something check progress,negative
waiting answer last comment,neutral
mean think fine keep hub usually go integration really community lot activity lot activity issue thought really great want contribute still want add would recommend make close possible like llama persimmon otherwise good dev,positive
good idea job let see,positive
would git reset git add git commit force push git push,neutral
real way yet think check first,positive
feel free open start working need ready,positive
want merge probably end next week,neutral
file line file line gather return gather tensor file line wrapper return function file line gather return tensor file line return tensor file line return data file line tensor file line wrapper return file line work tensor type associated device type transformer error still could help look thanks also problem first place added statement assign device original code work new tensor device also work device assigned tensor,positive
immediate fix would set pad token generation generation generate end add better model generation management error today range prompt prompt device,positive
hi exception raise dynamo python dynamo python thanks,positive
hey could give reproducible snippet sorry company project ca provide relevant code recently import model place causing error addition shape error normally give zero auto hysteresis auto auto auto auto stage true true auto auto auto true,positive
yes fresh environment latest thanks,positive
hey think bad rather check list longer warn every might right module necessary,negative
hey think bad rather check list longer warn every might missing know model image,negative
hi little question since calculated window right use patch first patch sequence downstream doubt whether patch,positive
hi looking much better minor made commit soon first full forward pas super close getting fully working output input equivalent output input see issue output layer general think issue similar issue fix,positive
hey think additional feature use data would useful,positive
hi face issue update pip install,neutral
hi tried script main new token model commit hash able make sure latest pip install,positive
hey great see mamba wondering eta thanks much,positive
hey running issue snippet already getting raw prob related quest well another forward pas get python import torch import import right pad policy query temperature forward model return model generate way affect padding output already handled generation return policy query response print print output forward policy temperature print,positive
hi thanks much original intent torch working system bit prohibitive u use model inevitably require model compile used figured would eliminate think develop warm model separate thread launch thankfully panoptic segmentation first thing done run application blocker u nice currently happy try first attempt would benefit well community appreciate look either way,positive
thanks reply context local fix solution put unblock exporter short term waiting,positive
believe branch appropriately still something missing,positive
thank ping thank linking relevant well solution tracing without think possible due reason issue tracing dynamo solution error think would easy implement would need magic model solution probably doable would need look pad currently try much possible pas since able dispatch attention flash attention path case already avoid setting none case tracing hi solution easy implement something like python else,positive
currently facing error loading model loader current version environment error text generation web loading loading false true true true false none none recent call last file line module file line output loader file line return file line model file line file line file line file line raise key error execute run python listen model loader see error bin main process result,positive
thanks encounter error running python recent call last file line module file line raise problem half define base import fix,negative
thanks run code get error recent call last file line module model missing positional argument look size see latest total latest total latest total latest total latest total latest total latest total latest total total also take llama base model script work fine put model path hence case mistral tried running script model path get error loading loading recent call last file line module main file line main model file line file line self file line file line raise loading error loading size mismatch param shape shape current model size mismatch param shape shape current model issue ticket hence close issue want keep duplicate open trying understand wrong fine tuned mistral base model,positive
added test familiar testing happy take,positive
file line file line gather return gather tensor file line wrapper return function file line gather return tensor file line return tensor file line return data file line tensor file line wrapper return file line work tensor type associated device type transformer error still could help look thanks,positive
great open soon could assign issue,positive
hi thanks raising issue detailed description really u address model compatible yet unfortunately think correct type model several logic incorrect eager execution trying compile moment convert modeling make compatible happy review anyone community would like tackle issue,positive
recent new package compatibility circle pipeline main pushing resolve currently failing,positive
confirm get environment variable work great parameter thanks help,positive
hi recent call last file line file line raise client error unauthorized exception direct cause following exception recent call last file line file line return file line raise file line file line return file line file line raise message response client error request id exception direct cause following exception access gated model gated must access recent call last file line load file line return file line model file line file line file line file line raise trying access gated make sure request access pas token permission either logging login passing,positive
problem already resolved added library yes anyone give explaining actually problem call unwrap generate,neutral
error properly recent call last file line module file line file line raise exception exception following public exactly like,positive
could try update latest version release yesterday fixed,positive
hello usually recommend start model hub allow quick distribution model already hub custom modeling code suggestion simply rename remove custom implementation sorry,negative
thanks another great contribution,positive
example failure case import import torch auto auto cast explicitly set assert assert sequence length smaller length problem assert sequence length length problem length different type impact assert extremely easy find bug used instead original sequence length main due order anything time like run problem immediately even breaking sequence length perplexity script also find problem method cast model moving forward ensure stay longer present,positive
problem already resolved added library yes anyone give explaining,neutral
understand correctly suggest rebase force push forced push last commit think erased commit history,negative
hi excited hear progress also multiple added could sense overlap segmented word like current implementation go left right look added token first instead left right way control added token take precedence segmented first reminder thinking language space hence seemingly weird example,positive
tried work python version following request name version summary python author license apache location,neutral
issue fixed instead use also training added default true thanks problem,positive
none yet link ready,positive
merge let rebase main fix,positive
like safe assumption though obviously like confirm happy open,positive
issue still ca review huge rebase force push erase history,positive
think might help effort support compile bit related functional,neutral
ah unfortunate tried calling model separate script instead directly code worked without issue bit long work huge issue thank help,negative
hey thanks usually recommend start model hub allow quick distribution see tutorial,positive
hey could give reproducible snippet,neutral
sorry unrelated rather related scorer library,negative
thank fault wrong everything work fine,negative
think good way se worth shot code check different,positive
hey thanks could provide,positive
hi annoying tried unfortunately ca actually get access reproduce issue hugging face academic institution ca get free maybe could run save load another python process handle realize convenient sure else try kind stuck come problem,positive
hi thank fast reply tried separately one import first result segmentation fault tried well issue strange issue even tried see would solve issue,positive
also diagnosis last post correct probably new token expanding size model layer sync model tried look token available matrix try add special token,positive
thats great hear let know like help happy contribute,positive
hi good timing actually something yesterday generation guru situation right generation end model token included generate output however sequence like may get different way broken multiple way handle control like special model special always single token possible set however realize several possible example model trained ca add new without model therefore something like list termination halt generation keep,positive
two failing error know fail considering minor previous commit,negative
confirm bug appear something guess,neutral
hi thanks raising issue based error message though file corrupted possibly empty outside script able run following import model look size see note script wo run return anything also need pas safe serialization saving,positive
added initial draft attention guess bit different llama mistral please point similar model close rather let know additional file also handled correctly clue handle additionally corresponding test case also necessary,neutral
matter rebase going put top head main branch yes force pushing resolve,positive
let know want explore option block last commit message form,neutral
yes necessary unfortunately force push next commit think solve issue,negative
update also met problem export model model generate support,neutral
hi moment many unrelated commit history like might force necessary git push essentially history,positive
facing exact issue script consolidate model single file way save model try also linked torch directly load sharded state unfortunately currently start training create finish training save model still run trying continue training currently useless statement mean model trained saved middle training must iteration,negative
copied full test suite,positive
hey input padding removed input instead numerically masked attention layer minuscule impact output exactly case without padding slightly different may unpadded increase used nevertheless infrequent happen difference retain similar meaning notice frequently presence padding let u know may ask padding would result different principle,positive
hi still review anything else would want add example also add one,neutral
around default far think main thing routing call pipeline specific rather task responsible model worry backward compatibility think best course action promote python pipeline instead python pipeline default model within would non breaking still date new,positive
yep already feedback would like open fix pull request,neutral
might slow tad bit new model class course fine anything else,positive
remark attribute via script usually modify manually fine next time need run script add back anyway despite corresponding test skip thanks flagging anything else need make sure automatically added running script action side,positive
title relevant trying another ping,positive
worry unrelated merge without,neutral
hi used fine tune mistral base model used accelerate launch train model model run code get loading recent call last file line module model file line file line file line raise id must form use argument,negative
also try python import import import torch model model,neutral
hi thanks lot comment base model used mistral preference data ran following code merge import import import torch model however getting following error loading recent call last file line module model file line file line self file line file line raise loading error loading size mismatch param shape shape current model size mismatch param shape shape current model size mismatch param shape shape current model size mismatch param shape shape current model base model match model could please suggest possible way,negative
result though found official solution accelerate ultimate solution close issue,neutral
sure wrong try train model without together,neutral
hi thanks issue used box trainer need either subclass new class overwrite method loss design custom training loop look notebook start,positive
hi thanks issue order run trained adapter docker need first merge adapter base model push save somewhere either hub locally adapter make sure convert trained model model becomes compatible please see understand merge model run python import model model point model model,positive
issue fixed instead use also training added default true,positive
hi could reconsider opening issue think worth opening training flash attention still viable performance gain almost essential though appreciate thank,positive
thank quick response possible configure stop token without model fine tuning lora already expensive used vast expanse data slightly fine tuning script initially added new token padding led unhelpful figure issue initial issue padding layer recognize new token id would end case well correct,positive
hey thanks detailed issue think solution monkey patch pipeline best practice define single stop token rely brittle previous,positive
hey thinking could good addition change look hairy far lot might much look difficult implement figured whatever little improvement might still helpful aside flash attention training still decent let know worth add also fix thanks,positive
tutorial also push code hub try load ca find code modeling one code structure saw tutorial pushing previous question seem work code modeling file weird case see directly kernel understand mean want put,negative
thanks related recently recent package incompatible fix main branch include resolve,positive
sorry urgent attend forgot code work although sure may cause failing error unrelated code import following error look see import could due install please pip correct torch build,negative
great glad hear good stage regarding class always added issue new library compatibility fix main include resolve make sure read printed telling class need added public module happening class import like torch available look model like one need modify fully add model,positive
understood correctly patent way least paper le month,negative
remark attribute via script usually modify manually fine next time need run script add back anyway despite corresponding test skip thanks flagging anything else need make sure automatically added running script,positive
test passing locally still use local hub yet think ready review implement done yet bit work could follow similar design,positive
sure test failing error object attribute idea fix go ahead merge,positive
hi need provide memory particular case sudden spike go many see increase note case generation making model model repeatedly increasing input length selected would expect memory increase even model loaded suggest pipeline modeling code directly give control enable monitor better causing memory import import torch class self auto temperature temperature raise temperature strictly positive float otherwise next token invalid looking greedy set self example message role user content explain mixture le try role user content prompt prompt prompt return except exception print model explain mixture le model print generation wizard know getting run well could related structure eight expert observe memory jump one expert loaded run experiment model see,positive
test failure unrelated import could due install please pip correct torch build error import following error look see error import following error look see need pas another merge upstream trigger another test run,negative
also sure understand made sure library recent library partly compatibility code partly compatibility require see related resolved main include recent resolve,positive
regarding big ca really explain suspect come merge made really know revert thing way undo file main branch upstream commit file main branch git git add,positive
principle think might good idea people want get good serving best model default probably improving user experience however change silently backwards compatibility calling default pipeline see change possibly compatibility environment hardware suddenly stop working bad user experience reason different see older basic like simplicity size might reason default model pipeline call anyone run better one need large machine anyone responsible new model become default,positive
hi still working would great contribution next would recursive logic,positive
lost track bumping back list deal soon,neutral
also failing check parameter name model properly found parameter model class properly sure exactly also copy test follow done original model implementation library weight different default behaviour need override test like blip,positive
two still failing could trace module model yet sure testing suite force test run setting false like,positive
perhaps message came across incorrectly trust wrote meant concrete failure test prevent document made modeling especially increase since yet come across particular issue aware type numerical issue systemic drift infrequent failure extra speed reproducible script would even better request precise target search infrequent large going dig clear example open add corresponding rope fix,positive
thanks wo default set generate sure manage find exact place limitation come look tool level separate tool model limitation refer documentation default value one possible solution see set validation error wo appear,positive
hi thanks opening handle unrelated let split doc device placement,positive
hi added proper function match behaviour torch numerical error call hopefully lot closer let know,neutral
hi quite unusual bug see figure test try also model import anything unusual behave like model library help figure issue try separately see issue issue try another language model class like let know issue,positive
thanks ran locally also main another run,positive
possible forgot mention version platform python version version version accelerate version accelerate found version true version true flax version na version version script yes distributed parallel script o default,positive
since custom word differently string word piece problematic indeed could please add somehow related step also previous comment without new added vocabulary taken account example token inside,negative
yes get set run nothing change test run anyway wo reach line change,neutral
wrote custom trainer fixed issue locally suggestion pull request doubt work lot repository since see custom trainer deep learning final project however pretty sure missing something work,positive
hi answer would say transpose back convolution run reason usually mixed like dense copied code assume layout operating dim say rule single convolution transpose convolution convolution multiple row transpose first convolution last convolution ah actually issue code quite annoying one wo work create layer call project rather attention original code need similar function let work ping ready,positive
thank time great learning experience,positive
sorry got confused alright thanks providing file ca reproduce issue work box,negative
alright even hub would something similar done custom,neutral
hey thanks raising issue hardware related issue see example nothing much side believe,positive
doc helpful enough function signature yes unused handled properly agree,neutral
hey pretty old model safely ignore otherwise removed feel free open issue repository discussion tab,positive
special case change bit add next subtext one,positive
slow fast give behaviour add front print add look implementation part add since custom word differently string word piece problematic indeed would like open potentially better support added,positive
mean issue initialize python import model batch work thanks know doesnt make sense inform user pas error clear case,negative
hi take look notebook sentence process converting model hub able adapt convert,positive
thanks hard work merge,negative
sure notebook basically code wrote initial comment,positive
hey thanks like already alright find another good model implement closed,positive
hey thanks like already,positive
yes model work well however loss made size input shape optional masked language modeling loss index either see index set masked loss doc length sequence length,neutral
hi latest version still odd behaviour initial comment see master already solve thanks manage make work thanks also right reading another issue solution would change unused vocabulary made little experiment unused work correctly,positive
think close issue thanks,positive
great yes quite yesterday new release broke everything fix main resolve version could rebase update trigger another run,positive
hi done repeat main,positive
also make sure latest version version,positive
fixed please update use false thank,negative
practice allow increase beam size much sequence size long key value cache loading part cache really help hey approach really cool would like share link implementation feature could benefit,positive
first make sure model fully defined file rely relative long directory support feature yet model triton problem,positive
rebase main still failing wrong git fetch upstream git rebase git push origin make stupid forgive,negative
right python import clip import torch import import import model model image image print temp print temp print none print temp print temp bash sorry false alarm raised issue,negative
hi sure original clip repository one match scenic original implementation clip might would first check whether scenic implementation clip repository,positive
let make sure green feel free rebase main,positive
let hub thanks sent author subject external added model bit easier first share hub popular community add support reply directly view message external sender message came outside organization report suspicious bit easier first share hub popular community add support reply directly view id,positive
bit easier first share hub popular community add support,positive
actually contact author paper could probably provide also take look via hub problem let know think would hub still better proceed,positive
still getting widely different implementation scenic clip one python import import torch import image import import import model clip size size size return image size image size format return return come image processor model image height output model temp print temp print image temp print temp quite different exact input would clue,positive
thanks review added also made sure green waiting review get,positive
alright capitalization copy statement test thank pointing,neutral
yep already feedback would like open fix course make fix,neutral
thanks response set speculative throw error saving model,positive
hi everyone sure yes please give u time review properly make sure fix going landed,positive
issue anyone solve problem,neutral
alright sure publicly available checked original seem like would actually great addition problem easiest way distribute model add hub tutorial wo face issue make work lot faster,positive
usually first one try merge case,positive
hey thanks opening like bit try get,positive
sure anyone working already small doubt lot impact,positive
mean issue initialize python import model batch work,negative
think fa need merge add support,neutral
flash attention bias part develop sure status neither flash support,positive
yep documentation test merge without also help left last nit key key let merge rocket,neutral
use token part token,neutral
fixed could rerun got load extension,positive
also make sure latest version,positive
ping whenever another review,neutral
yep already feedback would like open fix,neutral
hi remember said forget transpose back would right place transpose back thinking end forward pas back output without transpose back rest code correct please confirm fixed one issue mismatch epsilon perceiver code like mismatch call attention layer wondering since attention output call different little still running run see far actual output text output right trying load killing pro box one huge drawback lot ram free high ram use quickly test,positive
oh yea like added support last month seem yet think someone else working fa take crack like relative position fallback original attention relative position please let know know someone else already working look something else thanks,positive
thanks prevent people cry,positive
thanks quick detailed reply,positive
want set pad need specify call separate issue link issue comment need open new one following ca seem make work simple,positive
need change install everything work full transition reason model code written work code need replace break compatibility ca cleanly update place two wait time year drop support older begin transition add fourth framework alongside start supporting deprecate either way need big community push port right cautious probably choose option accelerate lot feel like lot want,positive
thank fix understand also support want move information added next release also let know additional setting keep,neutral
really strange marker description string change failing unrelated stuff maybe runner right hardware mixed precision training apex half precision evaluation used certain mixed precision training apex half precision evaluation used certain mixed precision training apex half precision evaluation used certain mixed precision training apex half precision evaluation used certain mixed precision training apex half precision evaluation used certain mixed precision training apex half precision evaluation used certain mixed precision training apex half precision evaluation used certain mixed precision training apex half precision evaluation used certain,positive
hi run short test summary believe due running without reason failing due error object attribute error object attribute know might,negative
hacking making something like discover function super helpful,positive
definitely bug fixed know version,positive
getting import following error look see object attribute sure also regarding scarce right also got error recommend test structure also layer special triton implementation fallback case triton create address,positive
splitting sentence testing testing possible,neutral
reproduction case start thank fast code reproduce error import image import import model processor prompt image content image image answer image stop sign corner road processor answer dont count loss loss model print loss loss,positive
run get desired version see show,neutral
internal still want able load backbone fact still inside u pas model directly load model based backbone,positive
hi fixed main thank,positive
sure understand follow logic see doc split final given text role model split learned also responsible corresponding vocabulary model thus split known,positive
added test case failing one model though test model test also useful show importance ugly pattern,negative
hi still getting following error trying model forward call index dimension size code work fine change model another like thank kind,positive
still normalizer python import,neutral
thanks detailed review also setting black made unwanted file also file original branch version,positive
think ago think one kept let decide way forward considering duplicate,neutral
addition would extremely familiar enough internals particular cache blocker complete comprehensive necessary file able maintain code afterwards accept becomes stable time,positive
ping mechanism yes code image,neutral
yeah problem tied input matrix therefore need hello mean tied input trying convert flax model format folder get message flax model newly probably train model task able use,positive
thank added test current main due correctly,positive
fixed coming rebase main also incorporated please look,positive
yes sorry got tried merge pay attention fix make style push,negative
thank reproduce issue python import torch query key value query key value false issue used temporary alternatively load model eager argument avoid model eager think bug automatically fall back math path case used device supporting,negative
slow work script use fast,negative
object attribute since mistral model llama architecture based think way solve,neutral
make style automatically also could history bit merge force push,neutral
indeed import auto live add new would firstly live inside see everything quite stable subject lot breaking change port back core sound also work well,positive
hi thanks raising issue program would indicate look memory utilization watch utilization top able see topping loading model,positive
yes feel free upgrade version,positive
think rope float partly fix,neutral
testing would recommend follow usually done code base check loss properly simply passing dummy dummy make sure green well rebase main,negative
let try find container issue side note think may due package,negative
code quality check saying need sure part problematic,positive
good look test know look,positive
mistral already covered like attention bias might,neutral
want set pad need specify call separate issue,neutral
see feel free ping,positive
also easily push hub use class open want,positive
like version maybe need try library version image,neutral
also failing check parameter name model properly found parameter model class properly sure exactly also copy test,positive
would recommend set add see happening usually layer extra token,negative
separate better stated best way share first use custom would recommend make sure fill model card make sure people discover get,positive
possible create externally subclass support independent class could outside,neutral
import right sentence testing sentence print without print import right sentence print print,positive
hi reply yes try balanced reproduce problem easily code import model balanced,positive
two still failing could trace module model yet sure,positive
thank rest team involved getting merge watching work past month,negative
hi chance take look thanks,positive
reason took long trying fix issue constant size pinning use object tracing compatible time usage script get constant size pinned model tracing found way reliably resolve issue ask counsel please see test run see test latest version work fine thank much trace model without run inference apparent device pinning constant size pinning perhaps worth ask review merge master,positive
incorporated rebase main brought back fix please look thanks,positive
thanks late review code perfect style loss may request implement test least one confirm get correct loss case otherwise good go hi team busy recent believe great chance learn design python unit testing could please provide kind test checked fount whisper tester audio classification whereas thank,positive
hi easy way get language pipeline according add parameter pipe get language text would kind show example code tried following code abut return language type device else else model device processor pipe pipeline result pipe sample word task transcribe,positive
hi thanks providing information something think merge support removing cause running prediction see model passing pipeline alternative pipeline intended cover rather give simple entry point suggest modeling code directly control hi thanks reply st issue know support think let crash forward know use data cast model issue ca understand ran code device removing code wo impact,positive
well sometimes need train need make sure model pad pad make yes set padding token attention mask still token padding like somehow padding regardless value set,positive
master make small accordingly,negative
hi thanks try mention facing similar running mistral local code snippet device else begin need token model print model loaded device full text pas model min number generate output without output list table text enumerate print result summarize following table detail dont abbreviate expand keep information precise possible original text text print result print table issue sure filling memory ideally release memory inference wrong somewhere help would,positive
someone please explain generation warning setting generation difference define separately generation used inference padding generation phase unlike need set padding side length need,neutral
also perplexity average score overly familiar typical test data assume probably pushing corner well formed looking forward pas different float float significant output model could also quite significant looking worst case average mean pretty close worst well outside range consider reasonable worst case cause blow,negative
thank opening cool written easy solve thank detailed taking time explain hopefully fixed new tried flax fix maybe something,positive
personal test float increase test buffer saved ask load going float co sin float query type float overflow known rope mostly slow float sin co float cast back everything mixed precision sin co float think perplexity something ever use kind rather proper bench perplexity similar affect tad bit make sense distribution little little even small affected,negative
merge directly properly credit great work thanks review,positive
thanks lot extra work let keep lot missing,neutral
great work finding elegant solution,positive
hi indeed fix issue need either call pas false regarding second point tried balanced instead balance although think auto evenly split model across available,negative
definitely see issue think review,neutral
interested taking look mistral model still otherwise please let know still need work thanks,positive
hey everyone sorry get latest added hence response time worked mostly implementation fix documentation related end regarding big ca really explain suspect come merge made really know revert thing also sure understand made sure thinking,positive
suggest resolution length context past sequence overall sequence length let provide clarification matter issue line lag parameter set index assigned value one data point lagged discrepancy example size lag rendering unsuitable index lag shift lag index raise go history length found lag index history length modify code shown rectify index negative issue regarding scenario lag previous implementation index lag shift lag index lag shift lag else lag index raise go history length found lag index history length check analysis evident length context batch provided context length confirming training status apparent training smoothly additional print added verify index generating observe made without best,positive
issue could anyone please help,neutral
seeing similar error execute line import model batch error got unexpected argument,positive
hi thanks raising issue could provide public model issue happen llama model observe issue running flask work import torch import import time import logging import o print model auto data fill data conversation data prompt conversation prompt print request took print based error message willing bet indexing issue rope generate,positive
click see error code quality detail resolve issue run make push another model model attention class great fa two need apply equivalent model,positive
handling layer shape internally hugging face prevent undefined behavior sorry understand mean layer shape able call handling generation logic handled,negative
practice allow increase beam size much sequence size long key value cache loading part cache really help oh thanks pointing think observation sense indeed efficient,positive
last issue stuff resolved,neutral
make sure run slow,positive
update two always preferred default multilingual whisper language detection transcription,neutral
handling layer shape internally hugging face prevent undefined behavior,neutral
would propose update default depth estimation model,neutral
hi thanks reply tried help memory increasing also tried running model via supposed manage process efficiently still memory although memory error use memory usage around stay could related structure eight expert observe memory jump one expert loaded also thanks tip,positive
failing due made copied standard procedure made order accommodate new feature,positive
understood latency case though namely calculate float rule cast usage store target device would memory overhead new length switched embed would significantly closer intended,positive
absolutely space test large measure however since latency penalty change ability recompute buffer yet another flag like reproducible example failure case least fully understand going added note fully open run long many sadly hard find time proper deep dive find failure mode,negative
sure perplexity test representative wide enough range use particular case testing input significantly different float calculated lower precision bad similarly fully calculated different device calculated low precision well beyond range comfortable might something want consider user make via,positive
awesome final review get approval merge one,positive
please make sure look review catch obvious opening tab first thing one need filled oh rebase main undone let fix review,positive
thanks nice overview read paper disagree following statement emphasis mine principle update arbitrary depth without need synchronization end split one layer across split across need split second layer across need gather section paper,positive
please make sure look review catch obvious opening tab first thing one need filled,positive
hi thanks raising issue actively large try streaming instead whole thing,positive
awesome kind stuff usually small issue one two think getting quite close,positive
hi like shape behavior also gone likely due one added weight inside build focus mismatch start diverge first forward pas going trace,positive
hi someone team please review,neutral
yeah able continue fine tuning worked well use branch waiting,positive
regarding going open fix,neutral
first run rope problem done quick perplexity main rope without commit comment see tiny upgrade almost negligible come expense equally small increase memory latency side see going beyond original context length much expensive new sin co must expensive rope worth happy suspect logic may flawed day ago could difference pronounced experience,positive
fast address first comment sure see one need running make run change,positive
fast address first comment sure see one need running make,positive
understanding forward method object model return yep,neutral
understanding forward method object model return,neutral
hello thanks review fixed every remark good go,positive
hi guide cover everything open tag link issue thank important fix great willing handle,positive
turn kernel busy still saved,positive
hi thanks raising issue get output import check version python session import print,positive
thanks everyone looking hopefully behavior never see future,positive
would beneficial current project would love,positive
nice assuming work really excited get issue quick one like local may different line length default since bunch relevant want revert hate hold expect problem since style quality would make look smaller keep file consistent,positive
thank hard work issue,negative
certain like default implementation apply override test model testing module like,positive
incorporated question please look,neutral
thanks reminder change mind thread exactly,positive
still need make sure whisper default language detection behavior stay see,positive
question test python self model signature order deterministic else else equal know case set pure vision model,positive
fixed also change log may annoying,negative
look run manually apply work linter run locally good question opinion always people continuously project yes simple linter command easiest people occasionally joining currently focus spending extra effort setup another environment locally possibly conflict lot log,positive
way run anything locally make whatever change directly log look run manually apply work linter run locally,positive
best way help make model visible making sure model easy find hub easy use detailed model card code snippet showing get space showing model many become popular hub example falcon hub,positive
something causing feature add moment becomes something many people measure comment common pain point revisit,positive
hi thanks raising issue poetry issue successfully pip run pip wheel error message note flash attention model however use attention listed requirement,positive
also assign image predict depth image removed part,neutral
thanks answer would suggest trying enforce library take discussion python directly,positive
thank elegant solution work,positive
thanks ran please let know need true false true false false true fraction ca da de en e fraction run mistral model run model accelerate launch false beta true content output folder latest merge lora python content import import import torch import o import parser device auto return main else print loading base model device device print loading model print model print running model print model saved main error get running code loading base model loading loading recent call last file line module main file line main model file line file line file line file line error header,negative
feedback welcome waiting new version use instead fork version give feedback many thanks,positive
hi dude pas running issue turn due update pas otherwise wo stop generating unless triggered,negative
hi left regarding sure yet could take look,positive
failing running make pushing resolve issue trigger new run would great actually lint check print git way run anything locally make whatever change directly log also usually setup linter based observation everyone extra step,positive
practice allow increase beam size much sequence size long key value cache loading part cache really help,positive
yep issue permission issue arose check handle race,neutral
failing running make pushing resolve issue trigger new run,positive
convention consideration regarding deprecation cycle typically deprecate release commit part,negative
schedule around month latest release last week probably around,positive
pull latest commit install work fixed,positive
hi also another problem loaded auto node would use although balance python import model load model tried use solve problem would cause trainer example could save model ram although set large per still used ram,positive
hey came across language model repository interested learning process would willing share code used model understanding would really helpful new time,positive
thank attention one code easier hope would splendid work,positive
test failing poorly newly added exception like ask try fix test ping get stuck hi made fixed test well test failing unrelated bash le equal difference flax,positive
guess implement linear attention,neutral
thank understood issue let verify,neutral
hi thank much valuable input guidance apologize response comment regarding copy mechanism branch successfully make test additionally speed test however significant noted opt test conserve memory model used revision set float attached graph review could also perform test comparison thank time look forward hearing,positive
hi sure anything else contribution guide aware making,positive
already built need way elegant way anything wrong request least ago low priority add ignore close leave un done forever,negative
thanks work possible add note page use model would useful people,positive
great thank eta next release,positive
problem following example like also issue,neutral
another two fa without fa blue line testing model without fa blue line model fa basically stopped fa model come much worse ca pinpoint identical exception image,negative
try use code load model whenever add new pad dimension multiple done training trained adapter example sorry forget everything wrote please work necessary model layer big enough additional see add need resize code python none,negative
want decode soft prompt sequence output head output shape,positive
hi let try explain better description bit short perhaps unclear indeed raw relevant rag setup large example document project question location project model llama default meaning warping turned usual based generate best possible next however need want confidence score answer one review model provide course got creative came possible solution find setup significantly accurate rag provide one prompt model generate confidence score work parameter smaller following answer model given context provided answer correct yes interested relative probability interestingly perhaps surprisingly practice ratio turn pretty reliable confidence score ie closer zero inaccurate closer one accurate work need unprocessed warping query normally one token remains yes probability confidence score always help u model somewhat unconventional way one believe useful sense much functionality otherwise missing far seen hope clear may different way afraid aware hence unprocessed reason think would useful addition,positive
perfect knew something missing guess train early thanks,positive
hi thanks opening feature request development branch use install source pip install,positive
yeah open new happy merge make shortly,positive
prevent duplication let focus discussion,neutral
hi currently sibling model model give separate,neutral
test failing poorly newly added exception like ask try fix test ping get stuck,positive
productive way engaging maintainer library get within library complete feature within certain feel free build code available expand,positive
hi help u figure wrong get u short reproducible script issue size see two pasted code one another,negative
like whisper large set generation exclusive model mostly audio alternative custom would create class model like model however since infrequent imposing new additional file per model would wasteful also enable sort weird expense like,positive
yeah open new happy merge,positive
hi thank opening seen request feature recollection personally also convinced useful end day model select next token since would increase complexity convinced wo however may poor perception feature usefulness going standard bargain comment looking feature case add feature whoever th reaction please tag,negative
since acceptance criterion different speculative generation think would great able run speculative generation sampling sure good idea see greedy model probability likely token everywhere else turn thus candidate would accepted speculative would simply assistant model apply would sampling would greedy previous implementation generation update value consecutive generate current implementation generation end generation therefore next call generate start initial value intentional bug open fix good point revert previous behaviour test would,positive
copy think pretty similar,positive
hi latest main branch,positive
think clearly broken either think broken clearly broken case former root issue poor model configuration token issue case going open run issue meanwhile feel free set call,negative
thanks reply make get following error assume need device also way correct error thank time file line device found least two code posted import pipeline device else else model device processor pipe pipeline,negative
getting back work one,neutral
try transform token used pip install installation token always auto finally worked,neutral
reason took long trying fix issue constant size pinning use object tracing compatible time usage script get constant size pinned model tracing found way reliably resolve issue ask counsel please see test run see test,negative
thanks update along maybe,positive
thank detailed description said exist however like mine use inference took make available well another story think location implementation location different added option think,positive
modify output sequence avoid showing transcription actual generation device line python apart token part implementation effect misuse,neutral
hi bad idea possible wo catch load model incompatible model confirm issue viable pretty upsetting new model old thread conclude unused sure,positive
advice fix error could remove reinstall pip install,neutral
everyone worth give shot,positive
convention consideration regarding deprecation cycle,neutral
show full code please getting error module,positive
probably kind bug silence left padding warning change anything,positive
could one please merge accelerate,neutral
race condition error trying call directory file work see,neutral
hi thanks quick response guidance far see hub make easier add slight work someone use still acceptable however trained far modest scale see paper actively seeking support train believe model library rather hub could increase visibility within community turn might help find support easier thanks,positive
like nearly one comment could make current method consistent library perhaps also name return set binary notebook showcase inference might break inference example instead use example,positive
sorry comment closed issue still fa loss different fa without loss different even two fa used happen without fa loss always exactly chart,negative
hi thanks raising issue believe accumulation happening due multiple forward model pipeline context help prompt side note like wo effect generation,positive
hi thanks providing information something think merge support removing cause running prediction see model passing pipeline alternative pipeline intended cover rather give simple entry point suggest modeling code directly control,positive
note self create auxiliary,neutral
ran issue due custom weight tying scheme output layer transpose vocabulary former contiguous got around error turning safe serialization noted,positive
law think used either partly rigorously review normally something like would minimal impact would easy remedy user side removing would however case issue big impact lot activity issue decided add back think easy way deprecate type unfortunately least know able find one,positive
suggestion thanks making understand difference,positive
thanks fix mind much pretty clean simple let get opinion whether break weight loading library possible,positive
hey thanks lot opening new model request ecosystem recently trying push model hub much support also easier integrate tutorial sound good,positive
awesome see much interest model given ast super popular hub de audio classification model model permissive license original implementation somewhat difficult run think would valuable new model addition feel free open start contribution start related model either mae ast gradually update code bring alignment full guide model process well,positive
sure ever seen argument,positive
also extended code personal fork basically sequential section think would good thing add,positive
original target use case refrain solve use case may still open refine think necessary could consider flexibility specify example would easy think would great personal fork currently look use size case see separate argument would better attached solution,positive
related possible concern confirm problem test bench also issue brought think fully improvement one due device tensor creation done device library like calculation rope onto wrong due rather nasty floating point differ enough significant impact div pow outer product convert low precision float llama model close forced float explicitly low precision even double precision enough avoid problematic approach viable always done extra prevent forcing onto last step used cast computation approach related likely breaking use like tracing edit also think forcing rope applied float instead default computation think original llama,negative
flax implementation phi added yet anyone else community would like contribute happy review,positive
issue marked neither find implementation code tree load model via please point implementation,positive
first thanks still able reproduce however work even upgrade,positive
chance error remember generate inside virtual environment like,neutral
also pasting last script initialize trainer model train save,neutral
thanks indeed open fix,positive
hi thanks raising issue believe torch hardware issue rather something library people torch loading see issue comment said resolve issue related suggest trying load model instead reference,positive
model loaded package understanding used load model prior running docker content null bias none false true true null null null null lora revision null,positive
thanks opening fa model let u know ready thanks believe ready,positive
hi thank code saved day think line need modify bit else add line without run lot,neutral
could give reproduce please would really helpful sure run latest name bit model import import torch name auto name,positive
docker image fine failure end related everything successfully,positive
yes llama one think issue need update feel free open fix,positive
fixed ca push everything hub llama fix soon duplicate,positive
good still need check docker image,positive
hi believe last need note still wo full support least continue working,positive
could give reproduce please would really helpful,positive
hi thank least trigger run let see go decide go,negative
think anyone would auxiliary deprecate type variable idea throw warning user type,neutral
hi also meet used load fix,neutral
hi thanks raising certainly think fallback support however suspect previous difficulty raised hard manage incompatibility library interface completely agree pinning old version brittle best update possible,positive
thank review believe finally definitely pro given currently something specifically however pipeline class accept accept even used possibility explicitly setting passing rest follow,neutral
issue fix worked reiterate issue loading fine tuned llama bit ago wo work latest release,positive
documentation failing consistently following error error line inconsistent leading fib however example pipeline part code output python import pipeline prompt fib return fib fib generator pipeline generator prompt fib fib fib ignore,neutral
think current error model currently think two possible model loaded package model saved script model adapter expect adapter look something like could share content,neutral
hi thanks raising issue fix could try source see issue pip install yes work fine thanks,positive
fix tried work let know issue thanks issue,positive
hi thanks raising issue fix could try source see issue pip install,positive
ran problem scenario custom data set llama model used following function process piece data example prompt example prompt action example action example prompt action prompt action prompt action want train model fill data batch training get error trainer trainer else site line raise unable create tensor probably activate truncation padding length perhaps case excessive type list type,negative
hi thanks raising issue composite use load respectively per load model import model,positive
hi still problem could open new issue since original issue old many modeling code ca sure thing,positive
awesome per understanding good let know whenever progress support dive add support directly,positive
thanks opening fa model let u know ready,positive
next release support tested edit answer question plan next release release include support marlin support support hopefully support expect could done,neutral
apply quality run make push,neutral
thanks think way forward would extend new corresponding quantization object feel different quantization second point might bit much edge case always create new,positive
sorry indeed bad fix properly open,negative
hi thanks providing environment information could also provide minimal code snippet reproduce issue flash attention added mistral first suggest latest version pip install code,positive
hi combining originally mind know think somehow making fork check,positive
hi thanks improving could combine,positive
hi thanks raising issue could share running environment run terminal output error also successfully install version dev platform python version version version accelerate version accelerate found version true version na flax version na version version script fill distributed parallel script fill could know fix,positive
hi thanks raising indeed happening looking,positive
see possibility people matching different quantization even different use le compression attention wild mix get extra perplexity way calling right module custom subclass fit,positive
hi need fast model port architecture easiest would look code follow similar design already available model ported add model load however please feel like port architecture longer want time completely fine people community likely interested helping,positive
indication probably drawing right around set agreed,positive
thanks reminder complete task hi use model,positive
hi original issue error following,positive
hi saw normal hugging face inference context size however context length,positive
set proxy set environ import o issue,neutral
know failing think broke compatibility guess clue,neutral
fixed overall additional modification pas try real code failure function backbone,negative
share code snippet showing usage full error without fix hi run following script python import pipeline import import audio import torch train audio audio example pipeline output example audio array print output get following error recent call last file line module output example audio array file line return super file line return next file line item next file line next file line forward file line file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward return input file line return input weight bias input type weight type input tensor weight dense tensor output correct answer applied geld mein,positive
use pipeline model work generate caller know reminder pipeline meant make easy use performance however potential really worth especially sending directly model work need know limitation anything related also work instance thanks clarification,positive
except failing job tried running make quality fix fix seem valid option know could fix,neutral
find thread enlightening hopefully address question,positive
anything wrong way loading model docker,negative
work fine least situation problem usually model trained used thanks kind reply range lose something small decimal point converting inference converting may lead overflow significantly change,positive
mean loading model docker docker run volume model case command ran training accelerate launch false beta true,negative
could share loading model expect pattern import model,neutral
hi thanks comment see run folder latest,positive
hi thanks working another open gradient suggest removing relevant copied modeling file way include respective without,positive
hi thanks raising issue could list saved,positive
hi thanks raising issue conversion script originally written original llama suspect discrepancy confirm simple case condition script happy review anyone would like fix,positive
sorry let explain meant model output issue main main far tell issue arise model directly issue compiler really understanding kind object compile one inside model call,positive
recent call last cell line import import name selection,neutral
thanks super super context bad take,positive
maybe different error message,neutral
unfortunately ca reopen reopen another happy quick review sure thanks reply,positive
work fine least situation problem usually model trained used,negative
yes need thank pointing,neutral
need model test functionality randomly model nonsense fine small model used might help build model test found,negative
unfortunately ca reopen reopen another happy quick review,positive
hi really understand logic one call current output wrong ignore special like either warning hey use know removal capability going completely faithful however respect decision commitment faithful thanks,positive
give time large reproduce error reproduce error start working,positive
hi thanks raising issue digging coming due line option selected example import parser foo,positive
mistakenly could reopen order finish,neutral
think ready go minor want submit yes would like submit,positive
hi understand wo break code still prefer stick principle faithfully original model unfortunately ca accept one personal level though definitely interested see people add auxiliary information like like help advice making custom code model happy offer advice support,positive
quick update tested couple well behaviour run threw error pip install backwards compatibility package slow,positive
think ready go minor want submit,positive
provide full error please,positive
understand module agree everyone move optimum broken fix literally five case going release next week remove whole module worth think feel free close,positive
found setting work see,neutral
thanks clarification work discussion add flash attention model hub thanks,positive
thanks consider warn user disabled found quick fix mitigate,positive
update definitely issue latest version annoying bug latest still issue case,positive
hi thanks raising issue model loading code hub whereas code library support fa divergence happening model originally hub ported library good news fa already model pretty easy add hub model suggest opening discussion addition model know intended hub code usage way map two want use fa directly falcon import model directly instead auto model import torch import hugging face model id load model model auto,positive
update definitely issue latest version annoying bug,negative
always happy help next one rocket,positive
hi keep related consider one de encode method process single impossible distinguish difference list power without logic extra consider thing check type think add batch parameter de encode de way wo solve issue co decode wo able know method input consider wo add extra encode method however acceptable well scenario happen misuse use decode method decode without thinking backward change logic function like method list type input treat rather single string way solve issue lose ability process either list encode method wonder ask decided remove leading batch axis encode method may provide convince prefer use rather use magic everywhere project inner fine provide user convenience refracting period found lot legacy code used magic even condition try use define base class nit wo use suitable class like class project consider already python import import everywhere rather top source code via consider monkey logger class wan na rename following class wo coz lot name batch suitable encode method result always added new python sequence value move,positive
hi yes would like open,neutral
hi unfortunately work urgent come back also take,negative
holy moly one later still need thread source code otherwise really want please make world fast,positive
well tried disable connection reproduce issue open fix thanks,positive
hi like modify condition indeed thank however make sure would really like able reproduce issue far following situation code snippet work without error could describe detail reproduce please cache process running provided code example connection python import processor,positive
diet default interpolation original implementation see let know,positive
added test function corresponding,neutral
hi thanks raising issue could give exactly bug error full minimal code snippet reproduce issue device print load model done processor model cache process docker recent call last file line module processor file line return file line file line file line raise could connect load file could find like path directory file connection see run library mode,positive
hi thanks raising issue could give exactly bug error full minimal code snippet reproduce issue,positive
write access repository merge,neutral
hi could please elaborate solve error thanks,positive
hi thank indeed add guard torch fix thank,neutral
hi thinking bit complexify write new test similar one decorator make sure training fine resume adapter thank try,positive
would enforce filled via would get default pretty much user lazy fill model card also lazy fill right thinking enforce field object way enforce understand mean could elaborate would bad model trained wo work box require may succeed running suppose thinking inference wo work first model mixed precision used inference wo case hi thanks nice analysis mean model mixed precision computation loaded inference would change model hurt inference performance,positive
root test session platform python collected,neutral
root test session platform python collected summary unknown option unknown option key module removed future version please import directly see import type ignore call implicit pep preferred see since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade removed future storage class matter directly access directly use instead return instance owner attribute removed version please use reconstruction attribute retrieve final output instead please pas explicitly default value false future maintain current behavior pas use refer two card block found setting empty card block found setting empty please use please use please use please use,positive
root test session platform python collected summary unknown option unknown option key module removed future version please import directly see import type ignore call implicit pep preferred see removed future storage class matter directly access directly use instead return instance owner since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade please pas explicitly default value false future maintain current behavior pas use refer two use expanded please clone tensor operation also advanced indexing tensor index tensor triggered internally use expanded please clone tensor operation also advanced indexing tensor mask scalar triggered internally,positive
import name pip install image,neutral
found commit interface added even still right,positive
got weight must issue zero library also library add two lora model one reference one trained model solution removed layer target module lora worked sure since stack trace file line return weight input sparse tried removing,positive
model code python import import logging import import load model model true quantize save model logger name message parser quantize model model path locally name output base folder trust remote code quantize bit quantize group size version,negative
hey tested still issue reproduce example,neutral
thanks reply issue resolved,positive
clear issue note true able resume training would nice version model model distributed equivalent best model without training information either way appear currently bug,positive
note made model copied taken deformable want change model could instead remove revert file,neutral
oh man exactly thanks much pointing close issue,positive
believe problem seen issue also seeing may dupe,neutral
look original llama code issue python theta dim dim dim end type ignore type ignore complex also problematic see sin co sin co sin co,positive
code either definitely concern likely depending involved,neutral
give u full clean reproducer please best way help,positive
hello anyone solve issue locally getting error error wrapper found quantization import torch print else print device found tensor anyone get solution please share,neutral
slightly concerning warning still understanding release correct saving loading natively used library implement next research paper know whether actually use given warning model loading let chat see get bottom,negative
facing issue well solution yet,neutral
thanks ton making look,positive
yep fairly quickly add something like python print saving saving print saved resolved,positive
thanks confirming work improving,positive
hi pas torch issue resolved well type hint stuff several still test suspect focus throw shape first hopefully resolve lot numerical also show hint stuff see trace shape let know get stuck dig,positive
yes checked confidential information commit go ahead merge,neutral
actually give quick go everything fine awesome stuff ill close ticket thanks much,positive
thanks much amy give test point soon feed back really appreciate help,positive
hi patch resolve type able revert back original file install latest run code,positive
taking inspiration mistral well llama add similar let know,neutral
already said included extension file attach button check want try attach file comment ca simpler way attach get point confusion,neutral
hi thanks opening resolve quality need run make difficulty even though size part attention class copied many throughout library might protecting hidden behaviour check added sure spirit fence let ask added,positive
hi thanks raising issue able load processor without issue error might due connection issue could try,positive
thanks fix let decide best way handle flag trainer side,positive
think figured problem properly would silently trainer run evaluation doesnt compute metric neither loss custom metric providing function making sure key setting fix problem one faced problem think would better print warning provided thus metric misleading see progress bar metric arent arent found input,positive
think reproducer need change call main process saved properly import torch accelerate import accelerator import import import none none false stage true accelerator accelerator model opt model opt model opt model model model remove,positive
issue might related setting didnt change,neutral
previous implementation generation update value consecutive generate current implementation generation end generation therefore next call generate start initial value intentional bug open fix,negative
yes correct see also original implementation thanks spotting hence feel free open update along image processor conversion script ideally assert original implementation like done,positive
hi reason probably seeing local instance torch environment torch environment unless specifically marked stuff investigate,positive
share code snippet showing usage full error without fix,positive
thanks problem way set none loss false true error come line model trial epoch line model trial line metric,positive
would remove label smoothing code actually usually add regular loss label smoothing subtle change line making suffice work,negative
bump wed bot wrote issue automatically marked stale recent activity think still need please comment thread please note follow likely reply directly view id,negative
hi tried fixing commit sure see import issue running locally failure locally tell run view please pipeline able also still port locally know properly fix yet share help resolve score match commit made today fixed issue still test running see behavior see model giving different fixed inside build fix able load state model still work end end done match port start working,positive
understand correctly need match interpolation example per correct take let know,neutral
hi noted previous comment help without knowing code running important get many per day order u able respond timely manner necessary help u please provide information running environment run terminal output minimal code snippet run reproduce error relevant error full error,positive
hello version accelerate version platform python version version version true available false available false system ram type accelerate default found platform python version version version accelerate version accelerate found version true version true flax version na version version script fill distributed parallel script fill extension report note compatibility system meet install name compatible warning dev object found warning please install package apt warning already perhaps source try setting environment found warning please specify cutlas directory environment variable warning torch version warning untested triton version known compatible transformer general environment torch install path torch version install path unknown unknown torch version torch hip version none version wheel torch memory size code import torch accelerate import accelerator import import import none none false stage true accelerator accelerator model opt model opt model opt model model model remove command output null null false stage true false false true removed tensor saving check receive warning stage many sharded across device basically flat taken individual also limited example torch compile also shown reproduce also able reproduce command accelerate launch false true false false true true main static true false false false output removed tensor saving check receive warning possible disable via,positive
look snippet work latest accelerate,positive
hi confirmed work example case without gated indeed far wo work private originally concerned miss precise information file could found hub however already add extra concern anything improve could done separate u,positive
hi thinking bit complexify write new test similar one decorator make sure training fine resume adapter,positive
yes please merge branch upstream main test fixed able merge,positive
could try update latest version release yesterday wow working properly thanks support,positive
finally pas losing mind,neutral
well sometimes need train need make sure model pad pad make,positive
sure follow file type,positive
thanks review failing due unrelated,positive
solve partial related issue closed create another done,negative
keep discussion related issue,neutral
hello directory saved model file content model model seem,neutral
would recommend check try solution might saving work concurrency code recently,neutral
fixed move last position reduce tol,positive
hello thanks raising issue custom code would recommend open issue discussion tab think remote code issue issue token,positive
test would go ideally let know need help designing test hi thanks test anything similar already exist save time could change one,positive
hi please look sure failing,positive
hi model type yet used hope support soon,neutral
provide minimal reproducer latest everything mistral,positive
run code following got issue,neutral
thanks much much better made one change comment help u merge many thanks,positive
still understand setting ex optimal generate attention mask ignore padding,neutral
may next step plan deprecate let decode single batch along code style place post ask may reach team public discord server thanks,positive
hi issue instead let encode directly support multiple main fix error behaviour single batch let call first step plan,positive
import following import import logger removed raise although already modeling file following found environment run pip install,neutral
hi think correct solution enable necessary pipeline rather removing cast sorry make clear see function name ensure tensor correct device instead tensor course enable necessary pipeline function,negative
finally got instance ram wondering make faster understand library model save missing make faster take,negative
code worked two added one driver yesterday code error code error assert triggered line file disabled,negative
believe current test conversion script make sure use conversion script create small random test model like might already one sure use conversion script convert full fix code necessary test code reference implementation make sure passing test modeling code reference implementation make sure passing test write integration combining code modeling code make sure pas full finish writing documentation code finish documentation note believe official explicitly specify imu data sure extra depth thermal data imu data also research well issue depth thermal data omnivore believe previous work obvious either right thing might make sense confirm reasonable guess another possible path would implement portion model opinion le ideal,positive
temporary solution set work,neutral
thanks script class self model model generate self tic print peak memory consumption print clearing print input print output print time generation tic return plot like gradient linear sloping bit still double expect also like clearing cache desired effect memory consumption generation still beginning bit assume memory prior generation guess since expect generate least would explain flat line running container notebook thought might worth given flask issue,positive
hi much time investigate issue argument issue affect found anything else worth due probably wo time investigate issue also believe issue current skill level solve close issue necessary fixed,positive
right certainly translate get python equivalent one area difficulty would making sure custom validation fa library still give minimal set compatible hardware,positive
actually helpful python would import torch check major minor check architecture ampere major minor major minor return first second,positive
mistake read quickly think easy way,positive
flash attention private attribute set true like bark think see flash attention imp,positive
flash attention private attribute set true like bark,positive
hi thanks raising issue best help make sure follow issue template include important information running environment run terminal output example descriptive could mean dev branch minimal code snippet run reproduce error full error full,positive
run slow model branch,negative
awesome thanks review working model trained make sure add,positive
oh bad documentation fix assertion immediately,negative
way check python flash attention want code use parameter machine flash attention vice,neutral
future reference anyone visiting fix wait come back vacation confirm reason extra generation,neutral
could try running latest release,positive
could figure issue run code provided example data also early stopping find early stopping disabled run following neither loss score also issue code recapitulate false true function going source code trainer try figure detail function tried introduce bug function error happen function error come end evaluation loop find key puzzled would great fix thanks,positive
hi think correct solution enable necessary pipeline rather removing cast,neutral
hi looking quite good one request get chance quickly fix causing fail import torch usually stuff like torch file like instead clean lot easier see left done without needing branch run locally think getting quite close,positive
hi thanks raising issue first suggest would update version many recent weight loading quantization pip install best help please make sure follow issue template provide running environment run terminal output minimal code snippet run reproduce error ca run current code example many undefined relevant error full error,positive
sure feel free open take account,positive
could try update latest version release yesterday,positive
tried every latest version reproducer happy help whenever reproducer,positive
sure already working create later today,positive
yet nice want ti,positive
yes reproduce following file allow change file open print simple pull applied open print,neutral
hi please explain please expose part trainer could root issue error fully completely well full complete code,positive
much cleaner approach dummy thank,positive
author pull request publicly available yet,positive
removed directory need update file well make new generic already listed file get file remains unchanged thank bearing know way around git helping lot,positive
fully agree encode take decode single one support batch way plan deprecate simplify favor encode decode support single would like work fix,negative
hi could someone explain chose use interpolation bilinear official find bilinear good motivation fill,positive
thanks reply sorry issue fine,positive
yes also help since cache output,neutral
git complement git add way step staging still need applied commit push remote check looking tab see still would recommend file see make review,negative
file reason got git push fresh commit,positive
version code correct print,neutral
sure could make sure green,positive
feel free check issue duplicate,positive
think different issue namely sharded tensor serialization therefore state definitely aiming frictionless code better able help thanks minimal reproducer thread pasting import torch accelerate import accelerator import import import none none false stage accelerator accelerator model opt model opt model opt model model model take look like quite thanks lot,positive
could set return token type false default wo break code return token token already option base currently wrong use,negative
problem solve rust different error python bit win pip install requirement already satisfied line requirement already satisfied line done requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line build done getting build wheel done done requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied six line click building collected building wheel error error building wheel run successfully exit code output running running build running build running running channel latest update rust version fa component component component component component component component component component component component component cargo release warning unused manifest key index rand ghost inventory rayon either clap console quote paste quote running bin link allow running bin link allow running link allow running bin link allow running link allow running bin link allow running bin link allow running link allow running bin link allow running bin link allow running bin link allow running link allow running bin link allow running bin link allow running bin link allow running bin link allow running link allow running bin link allow running link allow running link allow running bin link extern allow running bin link extern allow running link allow running running running running running running running running running running running running running running running running running link extern allow running link allow running link allow running running link allow running link extern allow running link extern allow running link allow rayon running bin link extern allow running bin link extern allow running link extern allow running link allow running link allow running link allow running link allow running link allow running link allow inventory running bin link allow running link allow running link extern allow running link allow running quote link extern allow running link allow running link allow running link extern extern extern extern allow running link extern extern allow running link extern extern extern allow running link extern allow running link extern allow running link extern extern allow running running link extern extern extern allow running running running running link extern extern extern extern allow running link extern allow running link extern extern allow running link extern allow running link allow error feature removed feature nightly feature feature removed note split feature information error try explain error could compile due previous error process exit successfully link extern allow exit code warning build waiting finish error cargo release code end output note error likely problem pip error building wheel build error could build install python version exactly question running mac tried lower version work,positive
let move optimum anyways,positive
test would go ideally let know need help designing test,positive
fixed unsure user situation deal really clean solution python,positive
sam problem feedback transformer zero install command pip install,neutral
probably unrelated run timed set,neutral
completely true many time past please look circle assertion process resolve past,positive
hi appreciate work sure accept trained accept default even get return model code also general hugging face aim reproduce precisely extra input original support really fit philosophy completely agree valid protein though think right place custom code model rather class guide use code base make want add token type train share model custom code approach used nucleotide transformer,positive
worked update look replace run pip install anyone currently issue version still fine setting work patch,positive
due remove completely removed think best thing add back patch deprecation warning,positive
hi thanks raising issue would like open fix link way get contribution,positive
hi add default python code model serving signature unfortunately exclude input entirely general prefer leave important available default signature however pas dummy alternatively save model custom signature something like python signature signature want signature,positive
hi find thanks lot,positive
worked update look replace run pip install,neutral
exactly mean performance running time probably library way one likely faster inference yes expect close would small however tricky influence would expect greater emphasis replication training likely match original model case,positive
hi let answer reference intended add also another open looking add pipeline although specific training regime may provide context pas model,neutral
curious left list would helpful whoever,negative
hi thanks raising issue without knowing error triggered full error message much best help please make sure follow issue template provide running environment run terminal output minimal code snippet run reproduce error relevant error full error,positive
reproduction code python import trainer import import accelerate import import version raise error accelerate please lower accelerate version error later main none model trainer trainer main run script version torch accelerate description thank interest first issue discovered trying pretrain llama model computer running low memory tried save middle training would get training would end however saving training cause save model training finished however saved model latest version model saved properly file problem code added understand added thought least included warning,positive
yes second said ca use expect support sooner,neutral
add official pretraining support,neutral
hi interesting sure wrong give designing quantization running multiple generate used get ram fix simply empty cache test maybe cache somehow behaviour try call generate call also add import call reference check thread,positive
yes get soon possible merge fix main,positive
hi sure wrong test one latest release check regression example try pip install also alternatively try accelerate first run accelerate select run accelerate launch,positive
really new feature rather definitely broken moment,negative
agree since related optimum make sense add new point,positive
expert looking alternative would use atomic since atomic except work really good solution think many fairly uncommon expect user issue hopefully know solution try replace particular file would exist folder let know comfortable else include hack,positive
hi could take look,neutral
let see would recommend move optimum,positive
failing due yet available organization,positive
thanks included branch latest nice day,positive
yes sense migrate optimum close however would great quantization would work even module version,positive
hi sorry delay graduate student rebase onto current main new test intended test case already test ensure model unchanged loading saving even test new behavior added output never tied commit broken two first test second fix check commit test added run something effect python see test current main branch second commit test pas,negative
great work may know merge looking forward model thanks,positive
use case best subclass trainer override evaluate method example given need original bit like use case since attention returned trainer order safe hello ask find proper example example link error,positive
actually solve problem prediction trainer used hugging face trainer theoretically bug native trainer already left project unable provide reproducer sorry,negative
thanks lot fixing pleasure guess,positive
thanks reply suggestion another change deal flagging general issue happening generation model found inheritance generation model model process also method defined class logging procedure guess maybe wrong,negative
error found thread searching,neutral
thank explanation time looking closed since really happen,positive
although use task use added training full unfreeze need detailed script delete private content day,positive
export export last line work case work,neutral
thank much quick use unfortunately helpful note case looking original code snippet still error invalid number provided running provided default model handle default however make work essentially copy pasting code setting default original call definition import set default work sure team might want consider handling default complicated implementation work thank help also thank pointing override edit link pointing incorrect,positive
may line philosophy let know one prefer,neutral
exciting let u know ready review believe mamba implementation might want,positive
though notebook fine get warning running multiple via shortly afterwards due assume something even setting warning parallelism avoid pause work fine path done,positive
hi install pip install basically latest version source hey snippet suffix echo put cat auto hysteresis stage true true true false device true true type auto auto auto auto type auto auto auto auto auto auto false run script way bash,positive
hi able fix issue could provide minimal reproducer help lot issue see trainer issue also happen trainer thanks,positive
hi thanks saved model huge achievement team,positive
hi able fix problem,positive
done upon latest main passing,positive
alternatively loop could tried try try try prefer additional flag already many generate thanks suggestion think would good improvement reduce work user side open another proceed,positive
hi could someone explain chose use interpolation bilinear official find bilinear,neutral
thanks raising issue could also share standard deviation sure mean,positive
perform model indeed run however expect,neutral
thanks raising issue could also share standard deviation,positive
continue work run test model wo ca reproduce issue people,neutral
hey thanks probably right would like open change make friendly,positive
sure maybe potential silent overhead,positive
thank helping continue along review,neutral
still much draft mode script found support everything yet would recommend wait tad bit better state end week,positive
install version pip install restart kernel,neutral
ah yes course thanks git main tree llama llama main,positive
yes pretty sure rather use tree use tree directory hidden folder modern o,positive
problem setting false know,negative
due nature potential included believe going static static believe method flexible able accommodate extra whistle kind exogenous thinking,positive
confirm import import work cache directory git main tree llama llama main,positive
problem close pull request add hub,neutral
hi stated reproducible snippet would much also run script python accelerate launch python,positive
hi thanks raising issue comment,positive
wait maybe let check version,neutral
thanks run run method empty directory real token checked llama still root root root root root root subsequent call work,positive
need funky case whilst still support logger issue resolved,neutral
yes folder snapshot folder llama see help investigation,neutral
regarding getting multiplying also function close device use might give different different thank edit clarity hi congruency thank actually need get already positive actually value around see value previous assert oh fix code suggestion,positive
currently whole directory part need remove git,positive
hi looking one minor stuff get clearly run method empty directory checked llama total root root root root root root root root model root root model root root root root root root root root root root root root root root root root root root root root root root running real token list shown ca see running following indeed work could help understand aware,positive
hi thanks raising issue could share reproducible snippet show loading model version could share commit running,positive
contributor welcome open demonstrate new feature something community define feature like add clear feature description able help,positive
hi thanks raising issue question best try reserve feature bug suggest searching first see related,positive
recent fix push main failing currently pushing trigger new run resolve,positive
hi indeed issue fix issue optimum,positive
tried yesterday day threw error get configuration error install fixed time use case trying find tune already model model already fine tuned find tune memory issue brought think merge two already via mon wrote tried able reproduce error could provide system information running command well code snippet try reproduce otherwise afraid wo able help reply directly view id,positive
thank ping thank linking relevant well solution tracing without think possible due reason issue tracing dynamo solution error think would easy implement would need magic model solution probably doable would need look pad currently try much possible pas since able dispatch attention flash attention path case already avoid setting none case tracing,positive
hi thanks raising issue ray rather team case id logged step training anyone else community would like open update happy review,positive
thanks contribution suggest model code hub way anyone else community use see many consider custom modeling code without logic align philosophy,positive
thank would recommend use optimum,positive
quick note although lot core idea simple import possible fall back import raise error version avoid modeling import probably bad let know want copy import block around instead,negative
tried able reproduce error could provide system information running command well code snippet try reproduce otherwise afraid wo able help,positive
think fix go optimum side sure,positive
hi thanks issue think disable default might break many ability load instance among many possible see case fix warn save model make block optional argument either set false warn case general encourage use would say option might best solution would happy open one think,positive
hi late reply general interested new quantization currently waiting merge order make support new quantization easier anyone future internal discussion inference support currently feel library quite fast moving added making quite maintain overall debatable feel free let u know think consider,positive
hi issue let know need clarification,neutral
hi thanks pointing would happy address issue pull request introduce,positive
hi theory safe however advise double check running anything please refer documentation page,positive
want continue instead switching get signature main pip install upgrade,positive
quick update correct signature already still fix issue able switch fix issue immediately get performance,positive
hi issue actually input signature missing affect standard model mean export signature incorrect open fix want get code working though background use internally first model export base conversion verify source code model saved specify saved essentially version model flexibility original specify going receive default use input model however overrule passing see source code want convert model custom signature save model custom signature conversion saved model control attribute bit complex though hold tight try soon,negative
use device python import torch import import hello streamer print load successful true input user print recent call last file line module file line return file line generate return file line sample self file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward raise attention mask size,positive
hi think correct would willing open fix issue,positive
hi thanks raising issue fixed part recent release could try verify issue pip install,positive
yes quick going moment,positive
think critical training first maybe add condition python,positive
remove leading batch axis return tensor none mean consider annotation text parameter method wo need batch axis remove return tensor,negative
hi think interested looking prompt old man sitting bench public park alone reading part token id end image find token id take along sequence dimension work need leading place next token given token previous image,positive
anyone stumbling upon save model also need load inside model actually supposed work,neutral
hi tested difference find could please help review,neutral
hi error empty folder working directory removing issue literally angel,negative
thanks reminder complete task happy use arch,positive
like work image indeed issue differently expert dev python good solution sorry,positive
hey thank added question post however even looking code quite understand pas prompt fine tune model think yet guidance happy implement,positive
back driver fixed issue,positive
likewise ability include extra llama token word space front word would huge,positive
hi delay load need load entirely model try snippet latest accelerate pip install accelerate python import o import accelerate import import thread model infer prompt prompt print generate streamer thread thread print streamer print thread id new text print prompt prompt state united thread prompt thread prompt,positive
used default anyway set none set true,positive
regarding getting multiplying also function close device use might give different different thank edit clarity,neutral
lot sense thank taking time point think ultimately confused thought number resulting case apply one label understand number different possible label took thank,negative
since acceptance criterion different speculative generation think would great able run speculative generation sampling,positive
hi like generate strive strict causing avoid unexpected case generating possible generation involved suggest increasing severity associated warning exception instead hi made change new,positive
know test local test random unused unused import random module test file mean assert,negative
seem fail example test module hint make sure test valid python return name level package level module class raise library found environment know specify environment install first would appreciate guidance,positive
know test local test random unused unused import random module test file,negative
sure understand network attached storage node might actually complete operation next process come check path complete sometimes core issue suggestion use something like else synchronize everyone afterwards would use main process false otherwise local main think intended behavior part sure file used later downstream could introduce race condition would nice could ensure rename actually nice add found saving model sometimes probable see watchdog caught collective operation,positive
thanks reminder complete task,positive
say would hugely helpful u,positive
hi working use case code worked fine reason way change linear actual getting standard script form drop dropout dropout dropout act dropout dropout,positive
hi could please review work compatible family,neutral
also found missing part team put also model passing,negative
hi please help would love give try push forward would actually first contribution quite familiar blip model,positive
id label label id,neutral
hi tried whisper find code gone current main branch multiple merge someone check going,positive
main branch dev think ready,positive
like assertion remain go,neutral
know test local test local test ran picked test test,neutral
restart even work either step pip install torch run installation step pip install torch comment step restart session,neutral
ever used differentiate two used next sentence prediction training task best understanding let explore pair let first define two short python text text use text pair include additionally clearly mark distinction two python import text text unlike produce output python import interestingly given function approach unusual similar code model well hand matrix correct,positive
would able check solution,positive
thank fixed running warning model used model model model newly list non long like almost everything weight passing attribute name name argument anything else missing edit like lot missing weight,negative
produced minimal version definitely issue main latest difference working think clearly broken like speculative could across batch certain payoff could enable python import import torch model auto hi wow work work print error python recent call last file line module file line module file line return file line generate return file line object attribute,positive
believe question related let know redirect elsewhere trying convert model model running due incorrect understand correctly may fixed issue maybe future ago question way override incorrect model presumably trained fix went already tried due property reproducible code note missing call definition import import hello world word box zip word box add bounding return model print note work model convert model converter model interpreter invalid number provided running provided incorrect believe,negative
thanks update add good test make sure hi added load balancing loss,positive
like change support change made,neutral
use default said advantageous version,neutral
thanks update add good test make sure sure add test make sure implementation correct,positive
quick note failing test likely somewhere since implicit cast test whether run beforehand case mixed precision policy accidentally carrying u,positive
part error always training step thinking due deadlock evaluation saving even set evaluation saving step first evaluation saving step error error still likely cause also tried problem,positive
set custom configuration somehow automatically program,neutral
interesting thanks rather odd comparison behave think display somewhere doc model card instance,positive
made another commit problem nice way,positive
see root issue since generation parameter accept custom use case cost readability hub suggestion would store handle side accordingly,neutral
thanks reply sorry code public something like python import torch import model auto provide variable self bool true function error,positive
thanks really sure incentive philosophy use hi new large model trained latest version gated method release model training finished fully tested downstream current implementation exactly,positive
good would say know hub gated linear yes currently training new model privately release soon finishing training testing,positive
hi exception message got point attempt sort turn used key file intended may ask short reproducer problem desirable visually inspect want sure problem exactly fixing,positive
could also remove two method already also remove perform conversion hub,neutral
alternatively loop could tried try try try prefer additional flag already many generate,positive
original target use case refrain solve use case may still open refine think necessary could consider flexibility specify example would easy,positive
meet install main branch work problem thanks,positive
open model change generation would great could check merge,positive
hi could help merge many thanks,positive
original target use case refrain solve use case may still open refine,positive
continue generating input prompt ca end prompt start generation trained handle convert input prompt vector fed case input trained handle padding right padding left,positive
test lot compute fit,positive
tiny forward pas different token time row time result slightly different written detail source numerical look similar thank running experiment intention integrate code separate,neutral
speculative already try calling,neutral
hi like generate strive strict causing avoid unexpected case generating possible generation involved suggest increasing severity associated warning exception instead,positive
forgot mention line similar model believe problem sure since tested model well goal model reasoning one per time,positive
recent call last cell line warning going fine train self trial else return self trial model model self model loss model self model else none model save past state need fixed made cleaner later self return call used forward self move correct device enable loss self return call used forward self input target forward self input tensor target tensor tensor return input target input target weight reduce reduction none reduce none reduction reduce return input target weight reduction input match target,positive
second approval nothing big side tiny slow pas yes confirmed,positive
thank much answer usually strategy port think worthy performance use model equivalent transformer model supposed get,positive
also would nice add model architecture tiny model update next week succeed create tiny model though,positive
tiny forward pas different token time row time result slightly different written detail source numerical look similar thank running experiment,neutral
hi see comment saw also issue,neutral
another thing note need use otherwise output wildly see,positive
hi thanks issue past full issue,positive
thanks much stated confirm running cell issue,positive
hi thanks much message think two wrong issue,negative
thank much precision update soon merge,positive
hi valid question thanks library providing access image recent like library script reproduce original great extent library currently image classification library hand machine learning would like put production rather cutting edge research also support various downstream besides image classification usually strategy port think worthy also benefit like optimum library among another reason usually also add various library depth estimation framework used backbone go instance transformer,positive
hi run following script main working import import import o import torch print model model auto left hi variable huge slowdown without line model need retouching print wo work technique well technique manipulation going open improve exception message,positive
general paper performance almost double good reproduction able reproduce paper also know feature experimental let really appreciate work know people use sake think important feature even behaviour ca knowledge like key right,positive
hi unusual latest version recognize architecture correctly version import python least,positive
yeah bit concern u suspect quite niche though hopefully improvement standardization small number people depending old behaviour,negative
think custom need trigger problem,neutral
fixed import torch import python import o import patch import import list work around return return patch model model auto text attention function query set output query output output text result print result,positive
got error converting model device device model print trace saved recent call last file line module convert model file line convert model file line trace return file line output tracer might cause trace incorrect valid container structure change based module consider constant container instead list use instead use instead absolutely need know side effect pas trace allow behavior,positive
wrote code check python import torch import sorted index return sorted index print print print print print return model print standard sampling print print short get token input sample time prob get token always look top following see close equal course apply produce insignificant result tested normalize apply output look similar however still equal final output way go extend test next well output bash standard sampling tensor tensor tensor tensor tensor tensor tensor tensor speculative tensor tensor tensor tensor tensor tensor tensor tensor tensor false false false false false,negative
however know method rerun time error,neutral
interesting also thanks new,positive
update issue think issue,neutral
issue fig simple solution bypass check model,neutral
got error package hugging face try removing local model cache worked,neutral
still got problem last version,neutral
see issue issue also late fine couple change code tremendous,positive
sense thank also run styling make hi made styling still something missing please,negative
note install install older version instead current pip version lot thanks pip install within environment get,positive
got see issue quite odd attention mask used,negative
thanks review added fix review please,positive
said want integrate loading give suggestion idea,neutral
see passing address merge,neutral
problem good also disk storage used appear change storage usage head branch sure need disk storage check added current test however,positive
hi would love understand issue fixed issue,positive
hi thanks looking may right related run trainer get newly added message must le look confirm update order determine issue due think need know training large training sample,positive
hi trained without said differently text padding hence explicitly set hub model internally attend still provide possibility create want padding although pretty bad trained regarding argument indeed yet add part,negative
thanks looking agree force problematic sense make behavior consistent rest ecosystem although bit nervous silently breaking backward compatibility special without flag,positive
yes pretty much might also possible write complex kernel convert format model quantization method marlin format loading model reasonable time could useful single storage format however sure many current hub already use marlin,positive
thanks reason processor hello return attention mask perhaps make sure sure understand python hello processor hello recent call last file line module got unexpected argument like call processor,positive
model still come significantly worse fa anyone model recommend use without fa currently running code fa without fa identical seed second woke checked training fa went meanwhile fa went full fine gon na train fa check point additional epoch see loss fa see edit loss terrible start went low high epoch training check point trained exact already significantly better soft scaling still something loss acting quite random comparison fa loss consistently went second update training learning rate order loss consistent hyper parameter training fa fa perfect better third update tried base model fa loss going anywhere almost marginally consistently random every logging step,positive
hi make sure additional task side complete,positive
tiny llama notebook announce tomorrow support float directly saving bottom work calling investigate well update latest release,positive
according leviathan paper speculative higher greedy however current implementation work,positive
done least let know review,negative
yeah mess something sorry new git push also add,negative
nope use explain bit case,neutral
claim paper correct suggestion work regardless distance target token end prompt see distribution st token token given st token suggestion would generate given prompt time super small confirm,positive
test suggestion would great addition,positive
generation around trainer argument whose content corresponding model attribute prediction step generate,neutral
concern work starting latest version torch thus would need keep old solution older think people older torch would current fix contrast would get correct device set anyway thanks much review approval,positive
perfect let know think ready post social medium,positive
working marlin also support quite easily,positive
oh check run notebook investigate,neutral
hi thanks lot issue correctly running training think properly care lora base model call also print model error well,negative
thank meant argument set accordingly work well,neutral
thanks confirm looking code like essentially need replace linear sure interface simple definitely add support think passing also backlog waiting finalize new quantization scheme,positive
hello thank observe decrease memory usage following wherein false custom auto wrap policy account different indeed save ram device memory usage device memory usage size device memory usage device memory usage size believe issue possibly fixed latest according discussion,positive
thank aware framework agnostic constraint think please review,positive
plan loading handled across also account local versus hub see reason class together making scope class problem would,neutral
fix main flaky logging,positive
could please provide minimal snippet reproduce want investigate attempt find root problem comparison new model code previous one,negative
hi thank interest serialization method apparently model likely model used draft naming scheme quantization later case try model latest something side try whatever worked may try extract diagnose precisely none,positive
thanks taking look might library issue function,positive
hi first uninstalled install source include proper version sh pip install load model previously saved fork work perfectly fork together python model else none following error python file module device value module meta device meta meta value none raise meta device need value put device none name defined restart environment install latest source sh pip install loading model fresh python file module device value raise version compatible serialization make sure latest version pip install upgrade else device file data device self device file device strip assert set offset float device clue,positive
thanks work see documentation though would nice take look stuff also would interesting seen discrepancy thought might interesting done core specific potentially,positive
hi thanks raising issue related open oh good job,positive
good point added make sure people use use guess would better explicitly pas explicit choice,positive
hi thanks raising issue related open,positive
thanks pretty much known bug fixed llama,positive
run multiple time quite confident would fix current situation,positive
concern work starting latest version torch thus would need keep old solution older hi old solution longer fall perfectly line conclusively code used used accelerate breaking related core would require change side would definitely raise significant address change perfectly fine stack thanks,positive
rewrite code rebuilt class accomplish local hugging hub,neutral
would happy address fix,positive
yeah think also logic exception commit would also like merge three private one parameter consider also quite familiar decided even change fine feedback welcome,positive
hi usually mean one script wrong case line none failing none acceptable shape layer fix like find place layer file see input dim layer copy build method example change line keep stop seeing get stuck ca figure let know help fix,negative
folder indeed time library file exist information load optional avoid making head call file time user folder time actually folder used far know case missing cache fully model loaded correctly understand correctly comment specific gated also well long,negative
problem closed issue thanks,positive
seen happen network mounted network flaky read might fail even though rest went fine error transient though could hit issue reason probably issue except network fluctuation thanks,positive
first graph comparison flash attention loss change much fa yellowish curve,positive
issue though may due instead hardware however seeing bigger difference inference local model false,negative
apparently issue due commit cache well cache instead new implementation pas list cache python optional none optional cache none later used currently list cache note kind cache attention cache guess version llama attention correctly change code cache none none,positive
yes either compare wonder though guarantee next token subsequent next token test feed standard one time compare one time one sent mention subject fix implementation want test probability sample token target model probability sample speculative maybe following test work given prompt target model calculate distribution next token simply running model prompt generate next token speculative speculative correctly large enough get distribution would match calculated distribution make sense reply directly view id,positive
thanks helpful contribution everyone persistence getting across finishing line think significant step good direction wed wrote hi typically new every month release bit later new year period likely end week next thank reply directly view id,positive
model still come significantly worse fa anyone model recommend use without fa currently running code fa without fa identical seed,negative
yes added implementation pipeline yet need approval time use repository try pip install installation yes work whisper model,neutral
hello thank observe decrease memory usage following wherein false custom auto wrap policy account different,negative
image image text model text model text like get image text model respectively,neutral
lot problem somehow repeated way sure root cause assume security really thank reply,positive
several turned forgot drop unused problem still remove loading model enable training got following error flash attention might lead unexpected behaviour,positive
want test probability sample token target model probability sample speculative maybe following test work given prompt target model calculate distribution next token simply running model prompt generate next token speculative speculative correctly large enough get distribution would match calculated distribution make sense,positive
hi also internally solution still,neutral
sure kind test expect see wrote following test directly dummy input data kind testing looking python import import import torch class test self assume size input length input float,positive
new feature broken python sh file line file line return file line generate return file line object attribute reproduction python import import import o import torch try print already loaded except print model model auto left hi variable huge slowdown without print,positive
first draft wrapped easier find testing integration main guide flow better example installation memory zero stage think important address first user memory chose wrong zero stage also condensed made concise appropriate without losing context,positive
hi thanks opening issue note making easier enough reason add something model however equivalent added llama reasonable could open review,positive
thing three way load order keep behavior three attention implementation call three twice case initialize model call turn need already,neutral
thanks lot conflicting local folder working,positive
first need double check correct move,positive
hi thanks raising issue like issue model wrapped side model natively method,positive
regarding failing internal thread,neutral
hi thanks raising issue best help please make sure follow issue template provide running environment run terminal output minimal code snippet run reproduce error relevant error full error,positive
hi thank much proceed farther would like give context trying trying compile whisper model specific compilation process complete compilation order lower model lowering way specific need input model format process look like saved model import convert specific kernel generation executable inference order tried save model save function see use save function load still call perform inference model way run inference saved model function call purpose find example yes please point direction find worked documentation use call function specifically looking set following tried calling call function could understand following error message positional total none none none none none none none none none none none none none none false match one following option option positional total none none none none none none none none none none none none none none none none none none none none true option positional total none none none none none none none none none none none none none none none none none none none none false thank much wonderful work community,positive
hi thanks raising issue suggest opening discussion model hub page like might related model able address,positive
awesome feel free ping review ready,positive
bit think key need pip install restart session may something else magically worked good luck pip install evaluate pip install pip install need restart session previous cell ensure,positive
example would like generate let say different single input sequence however hardware multiple hardware ca handle case say batch size single input want beam search maximum number handle parallel method divide beam sequence run sequentially currently argument specify set identical something could discus want add new argument generation function trying run code like could get something expect happen set beam search get regardless option true false wrong could provide reproducible example,negative
awesome work believe oversight feel free add part current quite large recently main commit history like rebase without forcing make sure branch development branch working branch git fetch upstream main git rebase git push,positive
hi quickly ran getting error sure code general due build added thanks recent call last file line module file line model file line return file line return file line return file line make sure model built file line raise none file line return self file line call file line build none file line build none file line build none exception calling layer type object call received layer type,positive
nice catch would able open fix model reference yes happy new model reference would happy make,positive
happy take care want take look,positive
case recommend looking different generating try force sensible information found,neutral
nice catch would able open fix model reference,positive
thank pick back weekend,neutral
hello successfully example actually outlier provide want would also like point think problem isolated large base,positive
indeed model differently depending whether activate prediction task figure whisper paper different model depending whether true false see prediction task text token prediction task practice input different giving different transcription configuration general found degrade rate performance evaluation audio improve performance however experiment distribution data find optimal configuration,negative
hi thanks raising issue sure happening however think issue code clearly model generating token however generation equivalent difference hidden size following paper trained predict difference size vision difference hidden size model thing slightly unexpected layer randomly would affect large model always behaviour occasionally ever observe correctly generating token,positive
meet problem snippet save model,neutral
awesome check also work,positive
approve recent also equivalent script intended part,neutral
failing test flaky idea way check main mean time,negative
hi seared case insensitive like happy take question raised possible share bit related mean logic inside right check path exist perform different,positive
thank work however case load several time python process python import import torch model print load model second call log anything thing three way load order keep behavior three attention implementation call three twice case initialize model call turn example script without fix may get log flash attention current run training inference automatic via decorator load model argument example model use flash attention model make sure move model flash attention current run training inference automatic via decorator load model argument example model flash attention current run training inference automatic via decorator load model argument example model second point original post want avoid duplicate duplicate currently fixed suggestion let know prefer way despite issue,positive
hey would happy work,positive
thanks fixing comment make understandable epsilon could also add least one test main fix fine integration test trying verify correctness speculative implementation add test token get indeed similar close token standard sample based understand correctly paper far found token sample input similar identical perhaps due hardware since low probability affect sampling outcome think test verify top token close enough chose like think better way generation get similar even though theoretically faster due relaxed acceptance criterion consequence think important verify correctness implementation like bash target assistant data random setup method token latency acceptance rate sample true false true true true,positive
hi thanks update based error suggest making sure latest environment need convert official though many already available hub access provided filled access form llama,positive
hi done raised please let know anything else example use python import pipeline import device else else model device processor pipe pipeline clean validation sample audio print pipe audio whisper without print pipe audio whisper,positive
check version additional implementation meet,neutral
hi typically new every month release bit later new year period likely end week next thank,positive
hi thanks time say otherwise confirmed gradient change functional impact model correct minimal implementation sample notebook issue background original code article educational simple lora implementation like python code worked fine tried gradient article aware specific code two example may worth pointing middle road freezing forward function module point adapter implementation forward pas work fine without gradient maybe problematic gradient code example linked easier consumption reproduce method python model class self super forward self return freeze except classifier false adapt linear transformer excerpt output full output linked notebook check without gradient gradient tried observe behavior hope narrow,positive
thank advise version platform python version version version accelerate version accelerate found version false version na flax version na version version script yes working environment current compute node hard disk node node connection load model via hard disk compute node work distributed parallel script two sorry le sure respond official meta llama folder git clone transformer source code try run conversion code error get python default legacy behaviour class simply legacy previous behavior used nothing want use new behaviour set set understand thoroughly read reason added recent call last file line module main file line main file line file line super file line file line false file line file line load return file line return self found file directory error large share file around advise would grateful,negative
local directory confirm indeed logged really seem problem,positive
hi thanks raising issue best help could make sure follow issue template provide running environment run terminal output minimal code reproducer full error,positive
hi able run provided code data without error main could try version latest release,positive
hi typically new every month release bit later new year period likely end week next,positive
thank much would case also,positive
thanks use pattern instead,positive
fact folder file exist information default another file case use populate missing would really make sense agree bug rather think able load model already even folder therefore transferring issue thanks providing reproducible example problem would time take look loading already,positive
tried login login token provided personal token company try login already connected,neutral
yes else alright worry make sure someone use new feature older version would get error saying need update version use,positive
hey make sure logged hub work python import model model,positive
sorry saw bit late,negative
thanks much deep dive fix,positive
try ask actually non contiguous odd need create non contiguous training non contiguous abuse storage system force several locality think optimize network transport therefore easy fix condition easy rework behalf since non contiguity really important model,positive
hi thanks raising issue best help please make sure follow issue template running environment run terminal output full error minimal code reproducer access could share reproduce error,positive
edit actually loading custom model class scratch function like use instead specify old code model new code model,positive
sorry bad let check different,negative
solution whole process much,positive
could explain mean detail mean,negative
thanks sense left single comment,positive
thank much wed wrote hi use image grounding add special grounding token prompt paper also use phrase token get specific format phrase import import image processor model grounding image image phrase image image processor generate completion convert token back default text extracted print print image warming campfire campfire reply directly view id,positive
hello think due compatible would target add lora along saved calling support new sorry late reply actually already put beginning code none model class self super self add one end token attention false false true,negative
hi thanks lot work latest change last line assert model loading script block end python model print however put logic script feeling something strange let know able reproduce bash output like bash root python tensor root python tensor,positive
related issue causal instruction tuning completion use library problem perhaps solution would provide necessary,neutral
multiple think necessary new class,positive
actually apply fix properly method limitation specific specific hardware think fine may want,positive
problem parameter link definition true wo able resume training current implementation able resume training even setting pointed trainer along even setting fix issue auxiliary issue pointed need capability persist load short trainer part object help implementation analysis reasonable,positive
related affecting internal thread,neutral
see failing test related whisper generate method also able reproduce locally false true used found ensure warning shown start strongly recommend passing found ensure warning shown end strongly recommend passing found ensure warning shown ensure different warning shown equal may ignore warning found,positive
great thanks working complex generation logic difference observing handle original whisper package always offset correspond actual audio timing python import torch import whisper import transcribe import model clean validation sample audio array sample sample transcribe model writer start end text print start end text print output quilter apostle middle class glad welcome gospel quilter manner le interesting matter u festive season year roast beef looming u drawn eating occur readily mind grave whether sir work really discover little rocky sort atom mason exquisite national jingo poem foster smile one much way used flash teeth collier sitter cheerful slap back like shampooer bath next man whereas currently reset back zero python import import clean validation sample audio array model processor processor sample char char print char else char char print output quilter apostle middle class glad welcome quilter manner le interesting u festive season year roast beef looming u drawn eating occur readily grave whether sir work really discover little rocky sort atom mason exquisite national jingo foster smile one much way used flash collier sitter cheerful slap rectified keeping track time offset element batch nice observation couple code whisper getting quilter apostle middle class glad welcome quilter manner le interesting u festive season year roast beef looming u drawn eating occur readily grave whether sir work really discover little rocky sort atom mason exquisite national jingo foster smile one much way used flash collier sitter cheerful slap,positive
hi thanks opening feature request want specific file hub use auto class necessary example import model,positive
transparent model run inference pipeline still one one meaning modify according actual hardware without actually needing unpack manually also running next batch even finished manually leading better usage,positive
hi use image grounding add special grounding token prompt paper also use phrase token get specific format phrase import import import image processor model grounding image image phrase image image processor generate completion convert token back default text extracted print print image warming campfire campfire,positive
another training framework little complicated easily reproduce problem following code import torch import import image import image want process open image image load image processor model processor model prepare processor get model model error forward self got return self return type ignore else return self self return try forward self input forward self input tensor tensor return input class self input weight bias weight bias return input weight bias input type float bias type,negative
since various seen addition see one could add well,neutral
hey version python working hardware getting issue python,neutral
comment reference pull request made one thing notice slow part internally hence sure thing done please clarify thanks,positive
hit issue notebook official said currently ampere hopper support coming soon please use ampere hopper head head dim backward use environment especially,positive
message someone might benefit environment working access get error case turned need use absolute path right problem whole day try use absolute path work thank,positive
keeping word stack trace ticket open one,neutral
type permission rerun aborted reset peer,neutral
get find way import thank worked note remove option additionally seem tell note removed use import name,neutral
trying run code like could get,neutral
example would like generate let say different single input sequence however hardware multiple hardware ca handle,negative
way batch size could made separate,neutral
think interesting idea want consider binary matching generate probability vector corresponding different text example different would pas list image airplane image automobile image truck furthermore would entry code snippet output correspond image two sleeping couch much general image two dog sleeping couch hi could please confirmed also like said like fix ca moment completely new would least like advice,positive
predict function trainer predict function trainer class return super pas send predict function case defined within class parent correct check function call trainer class function well wonder whether generate function model getting appropriate try use trainer running whisper see whisper language one specify example consider example official see,positive
status issue progress thanks advance,positive
hi thank reply feedback far tell main issue since added check make sure training fast however case happening way might good keep warning log per original think,positive
could please reopen issue issue despite automatically closed,negative
hi latest version release contain recent one want use bleeding edge stable version install source alternatively wait include feature release new version roughly every month due release one end week hi thanks issue curious would please let know thanks work,positive
also probably replace import transition period prefer import make sure getting,positive
thanks detailed review way make work torch sorry wo work torch also user program file later load file another python process thanks catching made minor change issue longer occur please let know think,positive
code incorporate let know,neutral
alignment handbook train like llama mistral multiple human preference direct preference optimization,positive
thank anything specific help,neutral
hi thanks raising issue fix way address would like open add script way get contribution,positive
error another issue ugly fix look patience use separate,negative
hi thanks raising issue best help please make sure follow issue template provide full error minimal code snippet use reproduce error case ca help without knowing running environment run terminal output,positive
sure also trouble accuracy trained zero zero,positive
hi added may still encounter later triggered inside build method complain layer getting shape sign issue automatically usually script wrong shape let know start work usually case input dim relevant layer build call layer,negative
realize issue probably arise strategy epoch epsilon would go around problem hacky way evaluate save model first step desired epoch multiple would recommendation edit digging bit proper way fixing problem would add trainer would enforce saving end training default behaviour still wrong believe would warrant least clear disclaimer doc,negative
used inference see comment,neutral
hi make sure understood correctly issue run generation llama enough llama default right ca sure neither reading issue tip section,positive
thanks raising given fairly easy fix,positive
hello everyone could please test latest revision report might found issue,positive
definitely skull say feel strongly happy go think best,positive
hi expand comment use save saved pure model model want export model inference lose access like generate part want save load model use recommend saving pure want export model inference something like,positive
hi resolve try let know issue install branch run command pip install upgrade,neutral
hi sorry late reply hectic period time unfortunately help issue,negative
think ready core maintainer review find let know dig,positive
hi would able provide example,positive
fix sure general solution though model,positive
bot stale hold static cache worked likely static cache important,positive
think would sensible deprecation time frame agree become sensible exception raising exception breaking training model artifact warning perhaps minor instead standard would want leave would little incentive something,negative
finally decided pas projection directly would like inform update time,positive
case speed training solve speed sorry may ask problem meet one thanks could resolve problem,negative
yes error exactly sample,positive
thanks back error exactly could update script use public provide sample run code reproduce end,positive
thanks lot answer close record actually tried pas one hot batch batch label label label batch label return batch default collate function line first label vector label first label first label else first label create new custom collate function run hoped make le painful update idea use one hot straightforward way would great,positive
hi thanks raising issue way save load reason instead,positive
hi thanks raising issue yes fix added part next release,positive
hi thanks opening word caution like importable part public documentation subject change never behaviour fact think make change semantically function check equivalent function would implement easily moreover know might already function exception raised however agree better single function return bool user decide output raise want add use hi thanks comment agree far better one feel writing tracer choose different tracer based input model would helpful elegant,positive
worked appreciate prompt help,neutral
thank review test use small model,negative
hello thank feedback think first option better close keep grammar constrained generation yet complete support continue improve easier maintain update thank offering amplify work social medium let know complete version grammar constrained generation working thorough testing done yet know may unexpected behavior tho may even notice,positive
token argument latest version try update pip install,positive
hi thanks quick response error even pas trainer original issue,positive
result fixing bug found really help much example include bug reinclude stride parameter model forward method meaning use custom stitching given high hallucination rate really help actually worse entire file particular instance,positive
judge leak code entire new occupy memory leak exactly leak would occur memory would freed discard,positive
thanks investigating result fixing bug found model known much model principal usage translation likely produce big moreover usage really good short,positive
hi thanks raising issue provided snippet appear passing trainer class,positive
reference totally supposed support every option use example code want control generation since normal behavior,positive
thanks much investigation fix,positive
first big warning experimental around bit found bug pipeline although wo fix since really model lot cut version without output superintendent said accused court covered th independence struggle organized movement killing supreme leader chunk rerun missing overlap superintendent said accused court covered independence struggle organized movement killing supreme leader newly president united martin king sworn age second sentence like pretty bad hallucination actual audio model known much,positive
hi thanks raising issue error calling,positive
use library still model use blip pipeline deal task test blip pipeline,neutral
hi change along example provided one major question another minor one minor one way make work torch major issue variable used work well script saving loading done python process buffer however despite much experience topic think common case user program file later load file another python process give see full log end question idea address common use case saving python import torch import io import import model model loading python import torch import io import import model assert model error log bash recent call last file line module buffer file line load return load file line load artifact file line file line file line file line return file line else none file line file line return protocol file line context context file line,negative
overall ready missing rebase core review,neutral
hi thanks lot valuable feedback speaker fixed seed additionally run relevant slow end everything work ready merge appreciate support guidance throughout process best,positive
true pretty much issue also fix python make sure correctly set custom,positive
think need version check feature thanks suggestion let explain background dependency parser model become widely used text analysis recent wondering certain model may hope keep need upgrade use together reason wrote else clause,positive
first case conversion slow use never set correct custom,negative
good suggestion candidate model indeed set resulting candidate sequence length length main model always generate one additional token,positive
well done specific case,neutral
alright function function set different set another one consistent idea done way,positive
hi status think call predict function trainer even appropriate set generation critical many,positive
either found solution facing problem bad issue closed think relevant issue single model much since actually want convert entire pipeline model like hey solve issue also need convert model single file hi solution meet problem hey unfortunately worked fix side decided past company task nowadays use still package could help ago manage make work use case package mature maybe give try,negative
reason hello sorry bit last week fixing small pointed push immediately,negative
hi solution meet problem,neutral
let add small test pas make sure usable added test pas word,positive
seen implementation fork somewhere source author fork removed triton found training working tho maybe get touch author,neutral
flash attention yes know issue come fixed main week,positive
feel free rebase main well unrelated ping whenever final review,positive
know higher loss use done,positive
hi want use whisper model language identification language want recognize one custom data source help look forward hearing thank,neutral
thanks need specific version torch accelerate work would suspect patch yes making work people need follow link set installation support team also working meta make device stock time use stock,positive
tried latest main today include commit unfortunately still file line backward loss object attribute accelerate think root cause still,positive
case speed training solve speed sorry may ask problem meet one thanks,negative
actually problem even removing,neutral
agree general solution similar sensible,positive
thanks passing ready review also left model file test regarding original implementation regarding conversion thank effort,positive
thinking inside method wo generate python min help avoid several big also cause problem prevent unnecessary compute think,negative
maybe one tweak would folder,neutral
like got stuff working know stuff,neutral
hi thanks digging little issue particular case found old issue first load model install yep already tried upgrade many,positive
hi yes provide stack trace close issue open new issue mon wrote hey related could provide open issue reply directly view id,positive
add class since make fix,neutral
actually yes would great thank also like need add build,positive
otherwise feel like silently equal length one length honestly probably better,positive
script used build let know want run,neutral
oh yeah accidentally normally fix,positive
one comment think issue test built correctly reason full build add build well script used hopefully resolve issue,positive
sure main parser text data language modeling next word prediction task train train dev true false false try model false else true except exception print model main nothing special except related epoch batch batch seed seed seed tee,positive
force push push remote necessary force effectively history current commit history contain everyone else,positive
arose cheeky made pas related,neutral
need rebase fetching upstream git fetch upstream main git rebase make,positive
thank lot sense give try upstream work next,neutral
yeah sure ran git fetch upstream ran make make style make quality still sync,positive
curl sh source pip install upgrade please tell exactly enter,positive
hi idea special mostly come language general special two main special true special never split especially relevant general people generating apply secondly never text always one character one token unlike like commonly used natural language think sensible solution update like default even though might affect backward compatibility slightly think,positive
found mistake call model python import pipeline classifier pipeline python use pipeline helper import pipeline pipe pipeline close issue,neutral
hey thanks opening issue seem able reproduce bug following code python import pipeline import torch processor pipeline processor hey gen true hey gen audio let know help send full script reproduce issue many thanks,positive
could update title reflect change fix applied,neutral
although interesting find trying pipeline work custom model hub feature documentation work new pipeline add bloat library long run,positive
hi think suspicion going let explain made old approach building used dummy ran inference step build new approach build proper build several dummy poor performance suspect issue related think solution twofold rebase recent version get new build add build since new added open realize bit awkward let know want handle one,negative
interesting lot seem done think processor really able used training token text prompt even though removed original prompt size,positive
hi due breaking change library see related thread issue fixed need install optimum source recommend latest version building time faster switching notebook since want blocked,positive
thanks agree calling parent method elegant solution resolved,positive
feel free open otherwise mark good second issue hi raised issue help review close old follow contributor guideline,positive
added able rebase get argument thanks try later today,positive
added able rebase get argument,positive
testing used trust remote code gradient resulting model terrible knew loss going preform testing worse however trained model old phi code got great model loss full fine flash attention critical without training memory help testing done training mem script used keep mind script reflect however time trained model compute set,positive
model completely issue happening many trying read file time,positive
thank kind deep general reply first trying repack code make easier quantization see today effectively class numerous remain found combining would nice may require massive understanding code history feeling bit depth specific help moving forward another reason keep code familiar context would possible postpone separate although would ideal set class interface properly raised class like removed still need way keep track chosen maybe possible use class need additional parser original code selected possible make class method instead already quite heavy currently parser factory potential outside community use envisage possibility people custom quantize like also easier add quantization like recently perhaps time raise similar place quantization meet stopped quantization method type parser simpler class single call,positive
hi thanks raising might interested define use hub working could open new issue following issue template running environment,positive
thanks raising issue detailed investigation proposal yes think context manager like good idea,positive
raised test local environment test environment removed following section top description side note test turning would also like update outdated test console short test summary differ differ differ differ differ ne ne dictionary found see installation test test test slow test test slow,negative
looking code model return attention model output function appropriately handle pad right understanding,positive
hi deliberate see add argument one sec,negative
pulse also nothing could use either regular think point stop trying support everything might exist giving good error message might better,positive
merge merge good go,positive
hi without knowing running full error message wo able help find list specific file try forum,positive
error basically trainer save however need use model convert create model show create model used purpose could please try python model best th,positive
amazing let know ready another review,positive
indeed accept method equivalent know deliberate oversight part two class,neutral
great quality make sure toe run make resolve locally push make green example see recent there never used script,positive
custom class removed ca fix script use instead class part public importable top level guarantee never still cause unexpected however open add dummy class go proper deprecation cycle first,positive
still highly interested model,positive
else may ask difference two replace line line definition like would equivalent second one little clearer also allow extra check removed,negative
would like work issue go linked file today ask,neutral
cool fix included next release going close issue feel free reopen encounter loading custom code,positive
thanks confirm main branch issue,positive
sure latest main version branch latest version main see latest force push last week day ago many since specifically one,positive
hi made last week specifically issue install main pip install upgrade try running without needing edit work properly,positive
see sanity check share,neutral
padding side anything loss,neutral
use left loss go without fa green line fa image,negative
remove explicit following link loss still high model,positive
similar issue would great someone take look builder error use following code snippet python validation found solve issue building time become much longer,positive
thanks getting back think get error patch try build source bash pip install hopefully solve let know face,positive
hello think due compatible would target add lora along saved calling support new,positive
hi confirm model exception instead seem implement,neutral
hi full fine tune trainer padding model loaded try loading get error flash attention might lead unexpected behaviour python model,positive
hi thanks contribution could share small code snippet show training model description error u anyone coming back future,negative
hi thanks raising issue could provide minimal reproducer error specifically full command used launch training job,positive
thanks opening reason moment intended class attribute instance attribute consistent object instance instance motivation like,positive
yes think conversion script good idea even suggest separate conversion script old new format le hook loading state something magic said general,positive
question appear eighth line think version exclude module import o import import import torch import import import import import import import import device else device module input output ignore assume attention mask input empty version use output since output output module input output input input main model else false model else raise return device device model model else model print complexity print print memory requirement parser llama version model name help pretrain main,negative
strange issue also find similar many like deep speed available might idea quick fix,positive
thanks need specific version torch accelerate work would suspect patch yes version still currently,positive
agree trivial add technical point view catch possible none good absence good solution far preferable continue handling case outside generate stop operating principle generate quickly become bloated even harder u iterate completeness considered open new crop outside generate original snippet issue manual task automatically crop length issue condition might hit content regarding past round conversation rendering generate unusable flag issue discover flag bloat generate underlying issue moment allow hold redundant information mutually exclusive information u making throwing informative,positive
fact format model much super like help u lot,positive
thanks ping review think way dealing,positive
hey true used model natively thanks reply yeah explicitly passing work however prompt user terminal ask whether want trust remote code explicitly pas understand behave similarly,positive
sure would love open,positive
hey thanks review let know work,positive
worse add loading state hook make sure,positive
let open conversion script support scheme would,neutral
people either convert use specific revision best solution yes right conversion script wo work conversion script single matrix whereas,positive
feel free open otherwise mark good second issue,positive
still think simplify way handle alright otherwise,neutral
fix main currently failing could rebase include trigger new run,positive
hi per need make sure load model train also share train model load model use,positive
resolved everything work fine latest package together main,positive
let know want look one,neutral
going close related setup reproduce main local probably corrupted,positive
hi still loading model main issue right call none like hack ugly feel correct seem invoke inference code loading model see issue missing none stuff saying sense chance bug elsewhere misunderstanding way like killing process looking need understand handle issue call thanks advance,negative
hi thanks raising issue could share minimal code reproducer error full error,positive
thanks need specific version torch accelerate work would suspect patch,positive
hey related could provide open issue,neutral
like deep speed available might idea quick fix,positive
nice fire feel free ping review,positive
otherwise yes prompt user look,neutral
hey true used model natively,positive
open issue reproducible snippet,neutral
else may ask difference two replace line line,neutral
correct taking look fixing next release thanks report,positive
functionality llama model greedy beam search help review function good start point understand,positive
environment version platform python version version false version false flax version na version version script distributed parallel script help information following along course end chapter information concerning use perform certain pad truncate convert problem official example code flawlessly point ever following number smaller number sequence self example simply simple example show error sure handling ragged pas value smaller sequence default behaviour leave sequence alone truncation result different together similar also occur truncation passing working entire course consumption reproduce reproduce behavior open example run code code snippet code work python waiting course whole life code python waiting course whole life two main python ca convert python sequence tensor python unable create tensor probably activate truncation padding length example handle situation python waiting course whole life behavior sure desired functionality case however believe throwing error probably correct action also error message make sense would think warning argument truncation true would acceptable option interest create pull request feature thanks advance think error still padding long small length maybe cut token sequence small length use padding function,positive
yet past key hope fix well,negative
usually let people merge could rebase main,negative
added new test find,positive
thanks lot waiting run first pas review well,positive
hey see already working let duplicate work unless week feel free take starting review,positive
change work correctly please get blocker partial work already create another quickly,positive
hi close use final solution thanks,positive
today decided move forward solution give well thanks lot,positive
problem go away change could incompatible,neutral
false accelerate true error gone away thank,negative
hi could share code,neutral
experienced thing set code flash attention loss went old code without flash attention strange,positive
got working training loss wack went used old flash attention also set model different flash attention saw someone else experience thing twitter,positive
able get trainer working right standard training loop accelerate handle stuff,positive
think got fully ready review time love handling left padding python output processor position output index first tensor passing left padding output output tensor filled output position output position return output let know else get,positive
hi thank see last comment regarding argument,neutral
thank much reply reason torch type instead padding work,positive
still getting issue specially mistral upgrade possible make import following error look see lambda positional given hope provide working underling,positive
like nearly one comment could make current method consistent library perhaps also name return set binary notebook showcase inference,positive
closed one new one available,positive
simple merge like leave core maintainer sure whether,positive
description could make sure resolve include latest yes work progress aware available yet sort thank,positive
thanks effort use lot appreciate close issue,positive
hi thank much reply error version running able run example snippet without error recent version running version latest fine thanks,positive
hello main branch two first documentation issue ca load model trying load make sure local directory name otherwise make sure correct path directory file problem fixed model alternatively add example second issue know would appreciate could tell cause solution problem main present run make fix ran make following error make python recent call last file line module file line overwrite buffer file line file line object attribute,positive
yeah according run ran make latest version main well,positive
hi keep getting similar recent version loaded model python model raise following error even set flash attention might lead unexpected behaviour got error trying whisper model input type weight type error got open issue also converted input data python format call model via python model got different error message file line train return file line model file line loss model file line model file line return file line return file line forward file line return file line file line reraise raise exception caught replica device original recent call last file line output module input file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward return file line return weight input sparse tensor argument one following scalar long got instead interesting use fa language work fine luck whisper yet suggestion proceed,positive
thank fixed error new converted format onto new error due script think removed future storage class matter directly access directly use instead return instance owner use flash attention model make sure move model recent call last file line module train file line train model file line return file line model file line file line range file line range file line file line super file line file line return super key object attribute edit fixed latest,positive
hello streaming although training run common training process stuck training error batch size per device sequence may different long short found easy confront problem contrast pad length becomes rare confront problem could know reason way address problem,positive
brutally expensive kind test could prevent happening,positive
really feel like extremely straightforward case like text generate able straightforward way duplicate memory work without like ditch last token would big deal,positive
lab mila multiple people issue,neutral
agree silent generally bad would suggest making citizen case million time common could another argument come forward call,negative
understand correctly displayed attempt implement non llama model case despite slight loss performance context size perplexity model affected case sorry misunderstanding,negative
went ahead added memory via test idea function memory disk given time saving process accounting margin error somewhat perhaps easier check large model think test small model still valid increase shard size substantially use default shard size model size memory check,negative
avoid wasting following run forward pas equivalent step outside generate add token selection step forward pas append token would somewhat close running one generation step manually automatic slicing would go generate design philosophy avoid except common use experience automatic input handling future,negative
let amplify feature make,neutral
hi look comment believe question,neutral
issue closed like rag,negative
hi current pipeline model public also tested rather may check however might need install unwrap store pure since wrapped python class also think understand whole pipeline currently working training pipeline also minority think perfect version might take time feel free discus team happy get mandatory,positive
hi thanks amazing draft model trained scratch regarding design great already however would need include preparation inside image processor pas image processor go would need make conform meaning method would need would also allow model compatible pipeline making sure work inference hub discus team regarding addition model,positive
yes work want also work parameter seem search posterity following snippet use python import else none thanks helping,positive
stuck probably server company blocked request whatever could proxy address setting company connection need set proxy variable correctly need outbound request check terminal export export export export export export export export export export making transformer go,neutral
ago probably part yet hence one part next release,neutral
hi set code hub used case defined rather defined natively library hence convert native one work flash attention one leverage conversion script host converted part organization cause currently one get lot following import torch import model due model single matrix code hub separate matrix,negative
sorry late reply sure update version version python main type help copyright license information import test meet total persistent create worker total number worker current system smaller going create please aware excessive worker creation might get running slow even freeze lower worker number avoid potential necessary recent call last file line module train file line train file line train number training file line skip past already trained training file line helper wrapper group together context file line model file line return file line return file line file line forward loss file line return file line result file line forward return file line return file line result file line forward return file line forward encode file line return file line result file line forward return file line return file line hook self file line file line module file line file line file line return file line assert true none true text issue fill two last version platform python version version version accelerate version accelerate found version true version na flax version na version version script fill distributed parallel script fill,positive
source dev still getting incompatible error even trust remote code set true cosine false warning name please use instead false false special added vocabulary make sure associated word trained pad token token token load recent call last file line module train file line train model file line return file line file line file line raise support flash attention yet please request add support model model hub page script import copy import random import field import optional sequence import torch import import trainer import instruction return ai assistant coder model company answer related computer science politically sensitive security privacy science refuse answer instruction response class optional field class field help path training data class optional field field field help maximum sequence length right possibly truncated trainer state dump disk key key value sequence list text text return sequence sequence data zip label zip label return class object collate self sequence instance key instance key return instruction instruction output output return train parser print print right none pad print pad token print token print token print load model print load model train running print training index range print sample index training set index index print sample index training set list index trainer trainer train,positive
think lot current image processor believe big unless told trained make really work working correctly going close like best way fix would use different implementation,positive
case guessing want add either nonstandard amino like represent leucine isoleucine correct correct goal add new like also seen lot folk di fold seek protein language far understand constant similar amino special constant inseparable difference normal special,positive
alongside issue hugging face thread audio course unit unable train also resolve similar problem outlined error related solution,negative
also would cool use something like python use pipeline helper import pipeline pipe pipeline custom,positive
hi like may need update version work main package version version platform python version version version accelerate version accelerate found version true version na flax version na version version script fill distributed parallel script fill model work model image,positive
basically key value last token come generation drop key value associated last token input would fun generate basically detect input number key value number come forward generate,positive
hi thanks raising issue yes moment compatible pipeline specific different many specifically two create model ongoing work unify hopefully like quickly happy review anyone community would like enable see also,positive
get work hidden python import auto left yer lizard harry yer lobster harry important line text print text,positive
related issue issue speaker could potentially address,neutral
consistency look run update instructed make sure recent main branch,positive
hello made text speech class focus improving handle speaker two key matching batch size one per sample single used batch key include replication logic replicate single speaker across multiple necessary error handling alert dimension speaker testing added comprehensive test ensure robust functionality across additionally handling speaker outside main model class approach used original implementation decision speech forward method consistency model structure please let know account feedback greatly thank guidance,positive
think close think seem worth,positive
problem current solution patch issue pipeline child pipeline share new logic added pipeline set new still fail also make sense class accept image processor solution address divergence would making certain class default none way new pipeline similar already without handle set better solution update super point either setting necessary additional logic,positive
ca guarantee backwards compatibility architecture loaded officially order able maintain move rename delete way may cause suggest loading model recent compatible version model use torch activation implementation model resolve issue allow load model recent,positive
ran make style make quality make independently still fail,negative
yes set fixed issue,positive
tried comment rest thread linked,neutral
sometimes get around try make style make quality separately instead make,neutral
related discussion slack day ago yes pad token character boundary one character since text split converted phonemic representation later segregate character phonemic representation phonemic representation respective based duration prediction later attention mask set attend think reason choosing case make able across language trained day ago oh wow interesting joy thanks day ago also checked indeed also set pad token one vocabulary day ago yes bit messy essentially picked one act padding token case mask forward pas attention mask integration used behaviour original use padding token probably would elegant new padding token pad layer account new padding token randomly mask come padding token forward pas done already would break though unfortunately done would better option retrospect day ago solution pad token amy day ago related discussion,positive
thanks much sense token id special pad token well standard token something probably model case need encode subsequently decode input text happy take look fix otherwise add disclaimer saying due behaviour,positive
ensure could correctly set pipeline fix functionality pipeline,neutral
fixed feel free install main get update otherwise next release,positive
agree memory inspection profile best next step,positive
typo already applied previous last week,negative
would helpful create later fully licensed,neutral
unfortunately super sure suggest slow version known slow fast version automatically used bumping issue guy someone like account stability team actual implementation used create proprietary issue experienced imagine used training one used fine tuning essentially saying matter long behavior exactly consistent training found evidence file actual implementation even almost certainly buggy match behavior training,positive
model indeed performant see space permissive license one gnu licensed without used license paper like great candidate model addition wonder boat community interest,positive
indeed understand synchronized main appear see fork,positive
hi thanks opening moment large unrelated description could make sure resolve include latest main rebase merge branch,positive
thanks regarding issue could open separate issue discus avoid thanks advance,positive
running import model work could share produce issue,neutral
hi thanks opening seem,positive
hi issue indeed specific code new added suspect reason felt token list constant since list amino therefore new token outside normal vocabulary case guessing want add either nonstandard amino like represent leucine isoleucine correct valid think update code support issue backward compatibility though see two possible backward compatible update default manually specify like update default like need manually specify add special probably better solution may break see grab member team comment,positive
checker affecting file actually fine added class list checker green,positive
good core maintainer review,positive
quick request going update error message,positive
could epoch based training added route feasible accelerate iterable,neutral
guess everything thanks much feel free close one still find training loss periodicity puzzling idea also different base model image,neutral
float something like use,neutral
thanks advice going tweak two,positive
use check encode python prompt label see single token token,negative
know section defined last week ca share small reproducer,negative
import help regression throw warning error look deprecation cycle,neutral
make sure latest version code hub added,positive
thanks let make sure green,positive
alright sorry running bit late day,negative
mind taking look feel free ping,positive
fixed thanks much issue,positive
regarding shuffling bit hidden set see used fine,positive
space yeah sense hidden since,negative
informer example setup input vector output side emission head independent diagonal post notebook review could kindly look,positive
current notebook think real explanatory notebook would quickly increase uptake hugging,positive
time series currently target care series even new entirely unusable idea get data went helper helpful example would extremely example go trainer trainer,positive
fine thinking would great post page change,positive
people either convert use specific revision best solution,positive
probably git pull upstream main test might hard issue alright,negative
similar call favor context manager internal python want modify none import model immediately model across avoid overhead time memory first model else model try catch also fine since put two,positive
hello could please review issue contribution,neutral
might want update would like open doc,neutral
hey thanks raising idea fix python trying convert object know happening,positive
observe issue visit hugging face tutorial training encounter concatenation error due incorrect,negative
whisper state longer version min wer however exactly small used parameter used parameter true wer maybe role since turn seemless minimal reproducer text audio file wer language audio file import pipeline import police superintendent said accused court covered struggle independence mau movement peaceful gathering town killing paramount chief swore new provisional electoral council cep nine yesterday continent relatively small many independent normal multiple would mean go visa passport control multiple time thermal behavior steady large earth often maintain fairly constant temperature consistent deep ground said glen united geological survey team northern university flagstaff sleep interruption process purposefully awakening normal sleep period falling asleep short time later complex rebuilt order give better idea originally angel continuum approach method used help reach higher level performance due underwater topology return flow concentrated fast current deep water may form towards end middle western develop style one biggest time result people use button fasten clothing system would allow aircraft fly shorter save million fuel year cut carbon full percent water planet come likely notation added simply label deputy prime minister wong kan trade terrorism prime minister lee disease carried like calcium potassium considered course also like silver gold field large part classroom quite often teacher would love take bus trip option people write computer never come close sharpener one common used illustrate importance socialization draw upon unfortunate neglect misfortune abuse growing added however take go beyond development stage responsibility damp clothes help dry many iron board available loan even one present room capital town island population curry dish based together either meat four sitting group finish total giant rank race debate sparked controversy spending relief reconstruction wake hurricane fiscal humorously new deal two free southern turning towards great pyramid one seven still standing today two react one another form may block kidney function university said technological determinism share two general development technology path largely beyond cultural political influence technology turn inherent rather socially conditioned pipeline never use parameter score long audio temperature task transcribe true err round text print err,positive
need set false main reference model see comment,negative
thanks added general test tiny model merge address potential follow,positive
hello everyone fixed dev version please make sure loading model work,positive
might probably hanging trying find whereas saved regarding loading import model working regardless version,neutral
would actually suggest parameter pas,neutral
let know change version root test session platform python collected summary unknown option unknown option key module removed future version please import directly see import type ignore call implicit pep preferred see argument removed argument removed please use instead argument removed please use size instead argument removed please use instead please use instead copy construct tensor use true rather please use instead please use instead please use instead please use instead since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade removed future storage class matter directly access directly use instead return instance owner converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize width converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize height converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize height width converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize,positive
hey would mind latest version test also share output,positive
use static cache might fix,positive
indeed related also related fact even saved used encode pas thus better error save wo checked look,positive
hey thanks opening issue sure prevent usage transfer sure anything however auto always allow load model without going ram usage peak come torch fact model might loaded float,positive
thanks still need keep rather confusingly flag set two within within model modeling file used rather ah thought variable let revert fix,negative
case auxiliary head exist first place test established,positive
loss added yet pas,neutral
found issue shard size set pushing hub docker container size set container size shard size push everything work bigger bigger hence thanks help appreciate worked,positive
thanks still need keep rather confusingly flag set two within within model modeling file used rather,negative
definitely finish theory nothing keep patient,neutral
well generate generating train split,neutral
thank way may ask batch size per training setting per set checked trainer handle different batch size different automatically,neutral
llama care much custom built especially removed fallback definitely try look resort find obviously best choice training without fallback issue think particular exhibit,positive
happy take look either even dummy need look reproducer idea non contiguous kind non contiguous,positive
interact anyway supposed something else play,neutral
please issue still much working though little week,positive
get value valid use model id valid use model id make sure command line absolute path local model directory,positive
confirmed issue reproducible thanks lot fix,positive
example build pas trainer,neutral
thank lot check example,neutral
dug around option exactly want test weekend report back one thing set parameter also manually add like default otherwise,positive
thanks green please take look get chance thanks,positive
thanks everyone might issue think let look get back issue well,positive
thanks let try blend current framework let use additional relevant like revision example use suppose different depending version simple best really know remove try think something else may offer revision work,positive
hey might bit harsh review need help finish sorry delay busy work besides little confused whether start new meet latest branch,negative
thanks deep investigation solution think context manager like valid solution might cover edge would curious hear think think right sense error reason original never get set back leading corrupted context manager proper method correctly set back original exception thrown,positive
quite unusual interact right sure version across use version,positive
fix issue training transformer file directory check main branch might issue,positive
thanks review left open let know,positive
see help overcome issue worked,neutral
fix issue training transformer file directory,neutral
thanks address another stated reply,positive
another issue like data shuffle shuffling important model image,positive
thanks everyone might issue think let look get back,positive
note generalized importer also able take account python import import,positive
thanks finding work special try except share case expose issue python import o false import looking test properly fix bug thinking involve use ast shown,positive
error basically trainer save however need use model convert create model show create model used purpose,neutral
hi thanks response problem special token even set would still output length image check last token prompt empty string rather space prompt image guess default behavior,positive
pro chip python able bash pip install quality testing torch enough run without still unable install latest link manually build need,positive
import train wrote issue right reply directly view id,positive
might give solution well thanks review meeting alignment get back soon,positive
reference thus switched block trying import package working,neutral
thank comment check weekend,neutral
hey green went made well filling documentation much comprehensive marked obviously fixed resolved left open let know think,positive
full command end training file hub alongside everything else easily replicate running model directory,positive
yes recent sorry aware commit,negative
currently failing flaky address,neutral
used model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model model wrong saying name unmerged presume,negative
hi think misunderstanding different order present right order used library model compare two different find within acceptable range please let know explanation,positive
added see torch torch secure still buggy take moment compare,positive
always experience issue first place thanks pointing u,positive
always change source code need either marc two large number yes,positive
thanks opening issue indeed could define one well already great state let know need help,positive
would great communicate main many people old fused,positive
thanks attention entirely clear whether issue resolved current expect work distributed training trainer without trainer following one two,positive
hi possible merge facing issue,neutral
something deeply wrong checker new definitely correct failing investigate,negative
hi like may need update version work main,positive
still get change necessary without change serialize type aka would like able save load another time personally would appreciate lot example added example description hood use save use load also linked issue failing due serialization support,positive
note rebase help diagnose issue include make sure pull latest working branch locally,positive
spent far long forgotten custom processor class,negative
quite similar set library side also limited looking solution similar initial suggestion example format without quantization think following could work dirty way want though check later python import add list quantization model manually cast back,negative
thank minimal example lot plate unable ended scrapping use functionality altogether would really appreciate fix,negative
fixed problem might still trouble important protein research way confuse people try make model learn new different usual instruction extending layer llama could cause clearer documentation fix might help,positive
legacy folder see longer code run,neutral
legacy folder see longer,neutral
patch accelerate give clear error going similar trainer shortly give direct,positive
might give solution well,neutral
think draft maybe working might better,positive
hey unable reproduce problem locally though digging around led fix different issue print u going remove figure,negative
length different length number rather nice fix sure change,positive
alright pretty much duplicate made mistake advertising test bit get ready feel free share,positive
hello yes please read following doc know,neutral
sorry right something wrong check,negative
mind also prevent gather single device compute layer device transfer really community sure paper mistral implementation way simpler feel free open want computation would nice,positive
would still like review time,neutral
going merge feel free drop detailed comment something need fixed,positive
something like git pull upstream main remote upstream exotic fixed main merge without,positive
hi well tried data big fast case network,positive
thanks per previous issue causing issue sync find proper solution thanks,positive
yep tried follow instead paper code thanks anyways,positive
waiting fell asleep sorry thanks awesome join contribution rocket,positive
could please merge anxious fix,negative
hello also facing issue code reproduce issue python import o import import import range yield let set model none trainer trainer assert assert remove resume training trainer time trainer nothing produced assert assert output missing model loaded single sample stopping training step set higher number available trainer run inconsistent think correct behavior current documentation trainer executed even finite smaller python,positive
sorry inconvenience error raising fix pad might nice indeed,positive
ready review regarding comment see response call hub image currently critical think better deal follow fact know true one call hub processor considering processor,positive
tried could figure way test think previous test could either test would need check input ca really check check error raised time catch problem error raised without ever calling input test error,positive
duplicate favor reference open support,neutral
seem unrelated building collected building wheel error error python run successfully exit code output building number running running build running build running recent call last file line version file line return run file line run process file line executable file line raise file directory handling exception another exception recent call last file string line module file line module file line module setup file line setup return file line setup return file line file line file line super command file line file line run build file line command file line super command file line file line run file line command file line super command file line file line run self file line run file line file line file line raise find executable find executable end output note error likely problem pip error building wheel running clean build error could build install know something,positive
thanks got example try create one weekend,positive
yes working fine smaller suggesting indeed memory somehow revert next time run model trying push false,positive
suggest test effectively thing example description small model happy put somewhere else think better,positive
hey issue related check help problem,neutral
could share actual file output actual output fully complete reproducer really narrow kind work like following chunk every audio length default run model stitch finding maximum output together many could play would easier get good actual issue seamless might trained long audio something making harder run extra affect trained length tho could issue stitching code whisper without issue fact arbitrarily audio model meaning stitching include hallucination since maximally string good solution one either try precut audio along exist find way model handle whisper happen although degradation whisper fraction percent wer also sometimes whisper maybe st sensitive,positive
hi thanks advice get point problem fixed instead related model,positive
thanks note chunk parameter highly experimental pointed could good opinion since added algorithm could give explanation work improve many thanks,positive
sure initialize pipeline pipeline temperature task transcribe true transcribe given language model remember longer already see issue long min probably work better even unsure setting affect right,positive
removed idea able save track usage make seamless pas cache instance able generation argument rather attention mask mostly left control flow non trivial tried take inspiration still working update attention mask cache see main attention challenge different attention class need different kind goal pas attention mask attention use,positive
new slow use individually alright parachute thanks,positive
image build due still old image therefore torch check error solve process sh python pip install complete successfully exit code error error solve process sh python pip install complete successfully exit code,positive
need make make test work,neutral
could provide full command line used fail push hub,negative
thanks improving truncation mostly used used encode basically lot model use let run slow make sure everything alright,positive
hey share exact sure thing recent call last cell line train self trial else return self trial model model self model loss model self model else none model save past state need fixed made cleaner later self return type ignore else return self self return try forward forward return act like decorator self self return self return decorator script mode type ignore forward self none convert self return type ignore else return self self return try forward self mask length calculated via length past many unpack hi actually writing talk work trying get accuracy rate able better hello yes previously worked honest achieve high accuracy believe accuracy high target tested tesseract tesseract perform best model specific font would also suggest trying paddle paddle,positive
issue even set padding side left error still training,neutral
temporary implementation python import math import float return float float return progress float float coefficient float progress return coefficient implementation upon cosine provided two new parameter maximum number learning rate decay parameter proportion constant learning rate comparison reaching practical impact following figure alternatively adopt following implementation slightly different curve python float return float float progress float float return float progress,positive
good issue similar float initial learning rate ramp float target learning rate decay minimum learning rate return float progress return float float,positive
glad appreciate use feature law play good get dev branch thanks pointer,positive
hi thanks advice fixed please take look,positive
hi done raising error device available,positive
also tried greedy removing output still end die ist number one e ist,neutral
hi inference greedy default whereas beam search providing generate method one would need use generation used inference compare thanks understand difference output different strategy fact beam search generating output sequence end like another problem bug beam search implementation,positive
hi everyone issue directly linked set true passing process data main process broadcast reliable le compute since one process however indeed issue data size trying might size temporary solution either pas default behavior trainer accelerate integration use instead pas split full batch batch size round multiple number example set process related code python one batch main split batch next else main split add one one remainder available range next issue since necessarily size batch concatenate,positive
completely new see added size despite fact new set issue following line python,positive
probably try following code different reproduce import torch import import model model model hi print dev work new version wo work able go back old dev version,positive
good artefact running machine fail running main running mac everything good one test value th decimal place change outstanding thing address comment added include loading backbone anything else like add cover,positive
fun review fire really cool feature lot work done section paper end review ca find part corresponding initial constraint see dynamically remove given batch computation becomes redundant data need fallback something considering main generate method notice significant speed increase suggestion future move whisper file importantly add documentation whisper doc page way whisper functionality becomes instead mixed generalist thanks extensive review diving complex yes already make sure actually incorrectly set generation correct value see update model yes nice observation ran really see huge soon row removed probably also quite good parallelization think see bigger speed give downside great torch compile think bit outside could added future suggestion future move whisper file importantly add documentation whisper doc page way whisper functionality becomes instead mixed generalist agree happy move actually directly whisper model directory cleaner,positive
made small post based big thanks everyone,negative
thank note indeed correct local path base model hub error remove line error,negative
could reproduce small training script small script geometric accelerator worked fine unfortunately issue contain training code believe issue temporarily issue wrapping call code parquet row row row row row return row recent call last file line module main file line main train file line train batch file line iterable file line file line batch concatenate file line concatenate return type data concatenate data data file line return type data concatenate data data file line concatenate raise concatenate got type data concatenate got class wrapper access data typical way batch believe issue accelerate,negative
hi open person taking already multiple contribution latter take one,neutral
right thinking framework like tab someone easily see related either thanks,positive
since possible class compatible got new cache format open issue,positive
though agree dynamo likely able properly trace,positive
confirm problem still load model local folder instead easy reproduce get module attribute error occasionally,positive
could possibly circle back get chance,neutral
whisper receptive field transcription audio need enable transcribe stitch resulting together see run python quite simple passing one extra line pipeline leave advise integrate,neutral
maybe missing something two statistic loss expert assignment average router probability per expert concatenate beginning layer input function right two statistic without regard layer came basically care individual unbalanced expert long across whole model balanced average might matter interpretation whether loss look like balance expert per layer script two loss showing difference made unbalanced layer cancel combine,negative
almost finished would like make last get one let u know happy help,positive
hi usual bargain seen demand feature current decision negative however comment within time spawn revisit decision community interested feature whoever th reaction please ping,negative
hey script use reproduce error identify memory increase maybe worth trying flag full well avoid inference,positive
really cool use read token healing past never thought added would think could include inside generate passing simple flag think,positive
indeed feel free post since like model regression,positive
little bit mistral different specific process model current implementation,negative
correct need core maintainer,neutral
got thank let try accidentally another sorry confusion,negative
also like issue model maybe familiar though maybe load,positive
yes recent make compatible library code look error message fixed use phi,positive
one work fine still get assume intended looking recent could cause,positive
update end finishing gave error shell recent call last file line file line raise server error bad gateway set logging level going discussion shell starting new connection head get get get get get get,negative
hi question still none switched suggest change current version still behavior like small thing sure understand none way,positive
work phi model never tried model time load issue accordingly specify happening phi,neutral
merge thanks lot bearing added thanks help,positive
hi team expand moment,neutral
hey still seeing error set notebook already set left padding still perform generation may lead unexpected behaviour flash attention version mistral make sure call sure something wrong within transformer,positive
taken sequence length cache sure understand feel free share unexpected help,positive
thanks failing test unrelated let rebase main rebase main branch sure right please tell else need,positive
loading likely issue could also resolved specify training,neutral
sure would good place,positive
full context use make sure local directory corresponding name command line launch training case,positive
need final output want confirm every operation output within model,neutral
output always since need cast output index motivation behind getting output,negative
confirm final output model cast point model,neutral
handled python import import torch check layer loaded correctly print output model newly probably train model task able use inference true see loaded correctly head newly,positive
pas layer output floating point load model inspect,neutral
output model also way model,neutral
reply pip install upgrade error still version upgrade version platform python version version version accelerate version accelerate found version true version na flax version na version version script fill distributed parallel script fill,positive
could convert class without every downstream model actually parameter could convert actually wo work right also one point discussion model name convention bit edge case since original question whether want consistency want adhere let decide one,positive
could convert class without also one point discussion model name convention bit edge case since original question whether want consistency want adhere,positive
added sigmoid function model image classification matching text classification pipeline per suggestion agree would good idea partially enough would like activation function explicitly defined,positive
hi passing need stay way run inference following model auto main tell interesting fact tell interesting fact output,positive
hey thanks review iterate soon regarding task right implementation remove corresponding class however question convert done pretraining class opinion whatever main class one clear head trained model card,positive
hi easily control batch size extra calling looping batch smaller input function much simpler solution additional complexity already complex function,positive
hey error llama see information update,neutral
alright keep work happy accept someone willing work,positive
hi think change done model still may dispatch fully masked see recommend trace could clarify motivation working intended sorry make clear yes trace model use trace model generation contain inference st token used tricky way input length see way could avoid input none model none input cause model error satisfy model forward conclusion fail make change good avoid please ask anything make clear,negative
left running model specific rather bug,neutral
hey sorry delay big review later week,negative
solve azure side struggling issue would appreciate hint,neutral
could give first pas,positive
hi added let know anything need added,neutral
root test session platform python collected summary unknown option unknown option key module removed future version please import directly see import type ignore call implicit pep preferred see removed future storage class matter directly access directly use instead return get instance owner since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade card block found setting empty card block found setting empty root test session platform python collected summary unknown option unknown option key module removed future version please import directly see import type ignore call implicit pep preferred see removed future storage class matter directly access directly use instead return get instance owner since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade use expanded please clone tensor operation also advanced indexing tensor index tensor triggered internally use expanded please clone tensor operation also advanced indexing tensor mask scalar triggered internally card block found setting empty card block found setting empty root test session platform python collected summary unknown option unknown option key module removed future version please import directly see import type ignore call implicit pep preferred see removed future storage class matter directly access directly use instead return get instance owner since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade use expanded please clone tensor operation also advanced indexing tensor index tensor triggered internally use expanded please clone tensor operation also advanced indexing tensor mask scalar triggered internally table transformer root test session platform python collected unknown option key module removed future version please import directly summary unknown option unknown option key module removed future version please import directly see import type ignore call summary unknown option unknown option key module removed future version please import directly see import type ignore call implicit pep preferred see removed future storage class matter directly access directly use instead return get instance owner since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade card block found setting empty card block found setting empty currently unavailable test,positive
actually need action end like ask rebase branch main otherwise wo go green,positive
could provide reproducer issue full error information running environment able look,positive
yes please edit see fit please let know need anything,positive
would nice bit outside scope would nice working example recommend register hook fly run different might le optimal would also single head always faster,positive
corrected error wait fix,neutral
make tip warning mean time,negative
hey model true allow want use converted use share fixed,positive
code latest main pretty much ready next round marc resolved left open form marc revisit one open item consider removing used return want left consider please comment delete try pursue actively corresponding code accelerate done replace done used main code may sliced smaller one per method long pas true false found sure yet maybe specific setup update find,positive
sure point support torch added see might better either backward support added,positive
object servant transport distance en de gaz one male wrong talking ai,negative
hey could refrain copy pasting problem unrelated either create new issue failure might side try check wheel available custom hardware setup,positive
hi think change done model still may dispatch fully masked see recommend trace could clarify motivation working intended,neutral
hey thanks opening issue without isolated reproducible snippet issue sure help,positive
behavior attribute something like false addition token also false de well,negative
torch quite old way able upgrade recent torch,positive
hey still want work close,neutral
alright pretty sure math equivalent compute individual sum compute let merge,positive
pretty sure previous answer interpretation switch layer auxiliary loss added total model loss training sum loss layer manner understand concept first second layer unbalanced expert mistral implementation routing switch expert capacity example previous comment way go,positive
type list little bit import random import import callable list optional union import import class data collator dynamically pad received used data padding bool optional true select strategy pad returned according model padding side padding index among true default pad sequence batch padding single sequence provided pad maximum length argument maximum acceptable input length model argument provided false padding output batch different optional maximum length returned list optionally padding length see optional set pad sequence multiple provided value especially useful enable use tensor hardware compute capability optional type tensor return allowable padding union bool true optional none optional none self list key key key row batch batch row row row key value key continue key batch batch key batch key value return batch,positive
information could help merge release week need need help finish new implementation correct concise made new commit maybe deep discus whether concatenate gate,positive
delighted add show significant however add research phase made whole open everyone freely experiment locally forked repository,positive
hope mind fixed minor syntax error make happy,positive
thanks much merge must admit security primary domain experience curiosity potential security different use library believe information could beneficial community especially varied therefore like share summary knowledge additional research discussion understanding potential security within library issue generally critical library primary use become significant specific production particularly library input web application risk arbitrary code execution insight importance context security need careful consideration security best especially diverse deployment looking forward community matter good going ensure security robustness library sincere,positive
token already part size change,neutral
method good pad useful,positive
hi inference greedy default whereas beam search providing generate method one would need use generation used inference compare,neutral
please help review doc,neutral
sorry issue somehow got second said confirm would great moreover issue error calling handle sometimes something index please script give new exact position try run script without quantization thanks,positive
hi unfortunately want use need use original author library,negative
used code initial error file missing found indeed file file put model folder run following error input provided model wrong number image number image given model correct indexing batch generation image,negative
hi thanks work building main use distributed found longer worked might something code simple import model recent call last file line return file line load return file line invalid argument handling exception another exception recent call last file line version file line decode result data final ca decode position invalid start handling exception another exception recent call last file line module model file line return file line file line raise unable load file tried load model please set related incompatible wonder maybe better fallback original implementation case error emit warning really know come might related pickle sure python also tried cache still also checked error think problem everything worked fine fixing shall eliminate thank time need help feel free ask,positive
try change somehow work true,positive
hello thank patience clear minimal reproducer ran see performance without gradient code use deterministic import import o import torch import import import import evaluate import import import gradient run add command model model model model false run without gradient mem usage without gradient output please note fast method faster method encode text call pad method get epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch gradient output please note fast method faster method encode text call pad method get epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch epoch performance gap gradient without gradient,negative
hi working related well would like collaborate join,neutral
python unable downgrade version help install,negative
alignment notebook script error regardless version already force pip remove flash attention work bit code adjustment able fine tune one flash attention strange part,positive
would appreciate could make sure preserve commit message git log,positive
also getting error trying use data tensor ever find solution,neutral
either need merge perhaps core maintainer,neutral
cool enough merge access time stuff whenever well,positive
good go feel free merge,positive
tried also tried file running code issue still phi,neutral
hi tried get error argument understood tried looking documentation source code function see anything regarding parameter,neutral
fixed rebase make make error make following error make error edit fixed make error know make edit fixed rebase understand make following error make argument list long make error,positive
try use code load model assume error try merge adapter base model problem multiple see whenever add new pad dimension multiple done training trained adapter example python pad pad phi default see notebook complete example phi,negative
research paper important idea insertion,positive
sorry posted inference also work work time writing quant see,negative
hi understanding constrained beam search used community code related moment limited empower community,negative
issue error token model resolved passing token directly agent like agent new error agent image mind like strict limitation,positive
hi equal difference seeing,neutral
release future reference thank suggestion,neutral
hi reproduce issue smaller available model like got often would weird get one given model let rule first,positive
please review image accept segmentation segmentation like,neutral
ready review time reliant made accelerate able rent machine development testing attached test file may help create actual test case system see test output log size full model test information summary test pull accelerate via train resume train additional epoch load best test final trained adapter summary test python text input output input output input output input output input output input output summary output log training test loading training training loading training load best adapter size output output output output output output output,positive
hi latest version release contain recent one want use bleeding edge stable version install source alternatively wait include feature release new version roughly every month due release one end week,positive
hi thank wonderful work understand correctly work right however run directly import error calling converted model currently raised sure whether difference bit saving work well need load bit model another fashion removed already latest thanks lot,positive
model card always load model prevent find load model following work python import model,neutral
thanks concern good closed,positive
would like work issue,neutral
hey thanks opening issue look first could send snippet show use pipeline,positive
sorry think related wrapper around natively,negative
hey thanks upgrade version,positive
new secret token added update accordingly push,negative
thank fast reply open issue repository,positive
closed merge support auto batch size finder ah see linked month ago thank,negative
closed merge support auto batch size finder,negative
bound matrix mind trying explain issue instead pointing notebook pad return text text train problem come run return return import torch import trainer model device else epoch trainer trainer error model newly probably train model task able use inference model newly probably train model task able use inference please note fast method faster method encode text call pad method get recent call last cell line input weight sparse remove script weight input return weight input sparse index range self help resolve,positive
think also add default image default label example consistent image general enough,positive
definitely want see issue marked stale,negative
sure let look also similar issue llama look thanks fix last commit two one able solve,positive
yes please use proper order,neutral
could also flax module class perform necessary post per rather currently mix model used flax,neutral
alternatively use version without mistaken,neutral
hi thanks issue model therefore bug end would mind opening similar issue original repository hub,positive
think much flexible le thanks simple description trainer compatible together case first combining automatically successfully python import import import model train trainer text example already overwrite add new one already pushing model tag hub see repository new tag well,positive
sure let look also similar issue llama look,positive
bound matrix mind trying explain issue instead pointing notebook,neutral
also favor also given much use flax wonder whether bigger breaking change default correct design flax,positive
kind looking thank review old method section code new method old method section code new method old method section code new method old method section code new method measure million level took long evident increase speed loop removal becomes extremely slow massive difference speed two,positive
use pipeline model work generate caller know reminder pipeline meant make easy use decent performance however potential really worth especially sending directly model work need know limitation anything related also work instance,positive
hi wrap directly pipeline also note bitten hard inference pretty fast data movement experience almost always better add proper manually get good performance,positive
related issue improve user experience,neutral
write test see work ecosystem,neutral
yes mean main get le equal idea might difference test force pas still get result,negative
working recent main force pushing fetch latest main git fetch upstream main note replace upstream whatever remote make sure branch git rebase branch git rebase force push git push,positive
hi still looking completely correct diagnosis difficulty fixing tracing model moment looking improve improve current logic tracing fixed torch,positive
personally know right token token token token since think infra team fine need talk infra team make sure right one read access use push token better read token talk infra,positive
failing test happen running,neutral
problem change exactly well difficult already access secret could use,negative
ah fast see comment short require token work,positive
fix whisper slow test,negative
made modification discussion model found also description,neutral
thank hi code main branch,positive
thanks insight edit right tested load best end work resume work,positive
could rebase main make sure latest incorporated,positive
yes basically write problem work data collator issue fairly well solution following modification import random import import callable list optional union import import class data collator dynamically pad received used data padding bool optional true select strategy pad returned according model padding side padding index among true default pad sequence batch padding single sequence provided pad maximum length argument maximum acceptable input length model argument provided false padding output batch different optional maximum length returned list optionally padding length see optional set pad sequence multiple provided value especially useful enable use tensor hardware compute capability optional type tensor return allowable padding union bool true optional none optional none self list key key key batch batch key key batch key return batch,positive
use image stuff theory try use bad idea torch test also job,negative
following discussion load balancing loss loosely since custom training script trying add proper support load balancing loss still wrong since per layer end paper switch layer auxiliary loss added total model loss training imply loss separately per layer end found implementation like auxiliary load balancing loss scalar must combined point read code closely yet intuitively also sense would want unbalanced expert one layer cancel another layer happen likely happen concatenate everything across beginning done know compute either sum sum first still need fi per layer concatenate gate beginning somewhere average get expert assignment losing breakdown think need explicitly keep everything layer end custom training script load balancing loss like work well python none float given token determine given expert think collection return sure correct please check closely feel free use reference right note minimum loss implementation sure desired divide somewhere minimum loss always also whether concatenate gate one tensor consider two first layer first two token second layer last two test calculated individually layer loss average sum made concatenation final loss layer tensor would make unbalanced load balanced unreasonable however test based premise independent know token expert different expert routing layer tensor actually separately however final loss inaccurate although difference small hidden state dimension large,positive
know pull request waiting anything,neutral
know successful without even giving prompt merge present bug visible locally present used given thread,positive
might try reference thanks,positive
sorry reply late new solution good deal top top implementation final loss divided python given token determine given expert shape top top tensor tensor tensor return tensor close top implementation top participate calculating auxiliary loss python mask compute ce ce,positive
issue inference also interestingly never issue inference smaller model,negative
glad use contact slack iterate quickly,positive
think important note feature request bug attribute source code fail result familiar version sure case previous python recent call last file input line module file line converted file line object attribute,positive
hi think add pipeline since may know function model,neutral
hi think could enable full pipeline,positive
hi would please help review,neutral
main get le equal idea might difference,positive
yeah make link also nothing else add topic let merge first,positive
main problem length change new token therefore code extend layer problem special token new size change could help understand correctly increase size model make sense define manually python model,positive
let include please leave handling issue thanks done thank,positive
would good way making sure fine branch currently none branch even added assert branch still except one irrelevant error see anyway added guard meta device,positive
great glad everything working include slow model hi used command documentation suppose mean,positive
interesting thanks pointing curious happen difference training unfortunately training code difference trained longer bigger model saw far clue could specific combination overflow still clear idea,positive
interesting thanks pointing curious happen difference training,positive
hi flash attention already phi hub version please use library properly enable flash attention correct order used library model please use first update latest version pip install run python import model print text print text let know work,positive
time small addition bug solve code reproduce main branch import torch import model auto hello dog cute model print always nan due attention overflow class funny thing happen model reference extensible model,positive
sure start looking today appreciate suggestion consider integrate without dependency necessary looking forward feedback,positive
also fully understand use docker image pipeline use image maybe change image pipeline,neutral
hi maintainer project happy assist anything,positive
following discussion load balancing loss loosely since custom training script trying add proper support load balancing loss still wrong since per layer end paper switch layer auxiliary loss added total model loss training imply loss separately per layer end found implementation like auxiliary load balancing loss scalar must combined point read code closely yet intuitively also sense would want unbalanced expert one layer cancel another layer happen likely happen concatenate everything across beginning done know compute either sum sum first still need per layer concatenate gate beginning somewhere average get expert assignment losing breakdown think need explicitly keep everything layer end custom training script load balancing loss like work well python none float given token determine given expert think collection return sure correct please check closely feel free use reference right note minimum loss implementation sure desired divide somewhere minimum loss always,positive
hi could please provide snippet show following snippet python import accelerate print print used get check one right version accelerate inside,positive
change bit warning message would like take another look,neutral
hey good question nutshell thing allow prompt whisper specific prefix like please use moment use working improving usability whisper moment,positive
try find time delve,neutral
thanks time working following discussion,positive
hi best try reserve feature bug,positive
hi thanks raising issue example provided error none image correspond thing defined model check image correspond model although behaviour highlight general difficulty model issue raised past able load alternative local load address,positive
support still follow progress,neutral
hi paste relevant part reply unfortunately unavoidable consequence trained trained system part training data model trained system prompt use represent system prompt trying insert prompt confuse model probably significantly reduce output quality model trained without system prompt model chat template raise error system message included input indeed correct intended behaviour really way fix without solution suggest usually different system prompt example instead use instead use dolphin trained system understand correctly apply chat template possible write chat template system first user message like chose although like clean solution practice think significant risk blending system message user message would create message would reduce model performance also please note chat set individual model code result hugging face actually power overrule model force accept system chosen template support see post chat information,positive
yesterday guess poster version see existence check happening per node could race output directory used sometime later code case issue,neutral
hi unfortunately unavoidable consequence trained trained system part training data model trained system prompt use represent system prompt trying insert prompt confuse model probably significantly reduce output quality model trained without system prompt model chat template raise error system message included input indeed correct intended behaviour really way fix without solution suggest usually different system prompt example instead use instead use dolphin trained system understand correctly apply chat template,positive
hi many thanks detailed answer yes right try new way via file hope work another question whisper pretty huge thus loading one model switching different inference often memory see example know perhaps easy way free memory used model loading new one found following really big effect import torch kind memory tried allocate mib total gib mib free process gib memory use memory process gib memory use memory gib mib reserved reserved memory large try setting avoid fragmentation see documentation memory management,positive
hi think correct realize would quite significant job update across want contribute one happy accept,positive
hi thanks investigation really good give context reason change latest main version working behind well biggest one use proper build instead building dummy lot related name see think plan code going completely remove direct use import version version although still need support understand version going starting primary goal ensure break backward compatibility code even fully support backward compatibility secure fully support probably require community push make full use partial hold number backward compatibility need resolved first,positive
hi mistral use llama prompt format include special token start assistant instead enforce must alternate assistant response always written immediately user message finished token result effect correct intended behaviour going close issue please let know,positive
cross attention change always use state change thus change across generate function,neutral
statement custom implementation patch open source chance,neutral
hi sorry delay huge getting model working though probably quite close getting whole thing working firstly suggest probably need convert reason already loaded framework code working able load additional secondly dummy much le important used since moving explicit build something add well let leave end help painful finally notice lot test test runner trying import torch reason torch file try instead still think getting close able reply much quickly please let know encounter,positive
still extra work model training thanks,positive
alright need add new sure way train new make use language specific data none old might would need retrain model train new small small corpus merge new old merge new end might optimal certain le alright manually add new simplicity growing exponentially potentially language huge think pretty much,positive
another issue following guide letter local path hub value valid use model id,neutral
suggest making prevent holding one happy go whatever prefer,positive
would take look well quite sure best reviewer case many thanks,positive
upon thread implementation whisper based pipeline found bug related thread rather parameter able reproduce however went stop longer accordingly duplicate tested sec min min min used language long story short scoring faster whisper model able reproduce original every model degraded insanely fast whisper whisper wer original completely broke wer faster whisper model parameter think would beneficial give option use especially new model even performance already language know reason without semantic background falling apart implementation decision,positive
hi issue code switched class class code class therefore reason compile method incorrectly actually loading model calling compile method something different unfortunately yet want use model need either use another model change code back use want stick switch notebook instead want,negative
hi would like work issue,neutral
hi yep noted think going remove replace equivalent also probably replace import transition period avoid version significantly version,positive
pipeline female noun otherwise alright according petit male waiting feedback gender,neutral
final merge able merge thanks latest merge everything good,positive
yes saving want make sure work want avoid later hub always almost impossible add common yes probably different version,negative
hi struggling reproduce familiar poetry dependency manager tried package python pip model loaded fine issue specific environment figure list package install pip issue reproduce work fixing,positive
good know critical thanks lot collective feedback approval,positive
saving also add common,negative
anything help acknowledge ticket hearing nothing let,neutral
hi sorry delay correct though issue almost certainly trying load model big test runner smaller use could also try,negative
hey according documentation model optional model train evaluate use provided must passing string thanks add added return got error recent call last cell line forward self assert none handle batch size padding token defined none handle batch size padding token defined,positive
hey according documentation model optional model train evaluate use provided must passing string thanks add,positive
problem however make another modify,neutral
hey according documentation model optional model train evaluate use provided must passing string,neutral
combination work python legacy false met today met today,negative
forget install follow install,neutral
update running slow model integration output similar exactly digging detective,negative
final merge able merge,positive
actually thought issue python import import torch import torch available torch used later name defined properly function code clearly issue come think anything fix,positive
hey reproduce python current process got forked parallelism already used parallelism avoid disable warning either avoid fork possible explicitly set environment variable true false pad mask pad mask python pad mask pad mask python pad mask,negative
alright test masked language modeling loss none expert loss gon na fixed,positive
yes please hub ca update right certainly downgrade warning cycle however give opportunity correctly update necessary giving necessary information future,positive
interesting behavior met today met today met today decode extra space added token however extra space added token tried thinking would better match behavior regular like success far able find combination preserve spacing check fix next,positive
thanks update name fix unmask unattended hi thanks,positive
great glad everything working include slow model would time help look issue model,positive
thankfully lot seem unrelated see lot branch main could rebase sync include recent hopefully resolve current,positive
hey thanks feedback add new model instead one could review note one failing test torch unrelated change probably flaky one trained model none available yet training one moment use ready moment otherwise related,positive
agree better remove shell access command line think however critical since already utility leverage directly,positive
sure tagged glad found solution,positive
thanks lot information many actually file repository,positive
keep something use great agree legacy worth code really used wild sure case currently used image,positive
like bug use library relevant people,positive
valid sure break lot community code keep something use great legacy worth code really used wild sure case keep warning also ideal lot discussion keep move eventually see indeed used lot community put back legacy,positive
sure doable expect update would upset lot hub ca update right suggestion would gracefully handle internally method without,positive
hi version ca reproduce issue,neutral
found return none trainer use get python else none else none,neutral
thanks update thanks effort,positive
thanks notification handled someone else currently hold strong opinion understanding breaking change likely pin version instead however previous regarding stability still unavoidably break lot community code without clear benefit strong benefit please state,positive
yes local directory name checked would fail directory necessary,negative
hi ca directly copied different believe separate one right,positive
hi thanks review propose since model return support related model right wrap model script thanks review feel free close,positive
came back end week maybe later,neutral
also either use slow false follow fix known issue normalizer,negative
feel free check better explanation,positive
hey thanks think worth fixing hack fix think actual fix someone already familiar could give guidance best approach fix put together,positive
thanks sound fair want change warning include version number involved release since current one,positive
hey think code hub open issue directly model discussion page,positive
hello seem direct issue would recommend ask forum,positive
yes happy submit fix,positive
information could help merge release week need need help finish,neutral
alright release week well everything included well visibility,neutral
hi although get idea overall still like previously would love see detailed still get change necessary without change would get issue say graph old torch version could used torch version case could elaborate situation since code content added something public lot use internal personally would appreciate lot example assume public description demonstrate situation instead fine use testing would help lot put example description looking forward,positive
hey thanks opening issue try keep since custom code unless isolate trainer way process data would recommend ask question forum instead sure community help thanks,positive
agree general pipeline excessive amount code support use like really want wrap model wrap model return instead python class wrapper forward self return also instead though new way kind seeing,positive
would like open fix,neutral
could tell may know feature,neutral
let put another close one,neutral
hi thanks lot let wait bit response,positive
hello plan remove see ready side merge,positive
time extra space token import met today met today individual,neutral
sorry making sure fixed next release,positive
hey idea access full custom code would recommend use latest version share full snippet otherwise ca help,positive
thanks opening conversion always rely use example however true supporting kind direct conversion could would help would like open help converter class converting fast,positive
hey sure understand issue ampere flash attention implementation install support yet either old new would rather look flash attention support specific hardware,positive
think time look marking good difficult issue,positive
let answer familiar try latest pip install,positive
would good way making sure fine branch,positive
feel free open fix want,positive
come play default token normalizer prefix space token space removed add token false,negative
hi thanks getting back think would great could first resolve merge complete final marc final review run good,positive
confirm latest commit branch issue,positive
hi ready finalize together team see following open response rebase resolve hope trivial time optional another round,neutral
hi everyone please let u know whenever share small reproducible snippet ca anything without fix bug,negative
hi error disappear pas false,negative
thanks lot easier look definitely reproduce quick fix get rid nan definitely regression confirm passing come failing either load different test,positive
think would work loading preferable load thanks,positive
hi sorry getting back late issue unfortunately currently plan model forward comply pipeline stated think still make happy review merge sense thinking way effectively currently,neutral
think would work loading preferable load,neutral
hi sure yes please refer comment understand perform call directly,positive
cool definitely interested inference support cool thing indeed need order quantize explore bit side let know go,positive
hi happy new year interesting clear idea either could causing looking commit history modeling code could attention dropout support rope scaling potentially gradient well long ran try see one might responsible regression,positive
also found padding worked better likely model vast amount data working strongly probably might one whisper slow inference speed really,positive
hi thanks great work let review much familiar,positive
hi thanks working saw look,positive
hi happy new year thanks much would happy helping u otherwise happy take well want,positive
hi thanks review default work well said however sometimes would like enable optimization like model model get significant acceleration ca recognize set return think considering different model would better since support return,positive
personally loss much lower new implementation sure instead pretty sure error original side far still waiting training result new balance loss finish also implementation might able reference,positive
running example listed get correct output token present pad cool eat many love among u love dessert one got amazing ca go straight dinner eating love made pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad,positive
fixed weight error reason failing,positive
reversed work pip install accelerate pip install pip install optimum pip install pip install optimum pip install accelerate,positive
hi sorry forgot update doc file include phi example guess add example show many people unable use library raised appropriate order use instead official load successfully agree would create quick add example,positive
error project example code today progress issue,neutral
error misleading actually available library work see note token twice rotate,positive
second issue ran mistral well,neutral
impact issue training fix conceivability improve quality training likely previous good could like important issue working waiting merge approval,positive
hi day ago yet gotten around testing would like finish issue rerun locally without everything side,neutral
hi back test mainly added function check two major set true whether output auxiliary loss none check output equivalent due last test also check segmentation also following result root test session platform python collected summary unknown option unknown option key module removed future version please import directly see import type ignore call implicit pep preferred see removed future storage class matter directly access directly use instead return instance owner since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade card block found setting empty card block found setting empty root,positive
mostly merge without community pretty eager,positive
version number tried gave exact error like thanks update quit install use pip install issue gone upgrade package,positive
may relevant found recent change axolotl made significant difference usage previously could squeeze lora model batch size length immediately undid change certainly possible since commit initial train run drop batch size unfortunately training instance right test next time try train,positive
may relevant found recent change axolotl made significant difference usage previously could squeeze lora model batch size length immediately undid change,positive
yesterday guess poster version see existence check happening per node,neutral
code work without set true load model fa hand maybe need added class level difference generation speed eager attention fa,positive
run find recent call last file line file line raise client error found,neutral
temporal solution warp original function skip execute iterable self print inject hacker prepare none raise trainer training training else training sampler return return work well sure potential impact efficiency skipping,positive
big problem reject system prompt chat template wildly inconsistent seem way tell model system prompt end getting error pas system prompt format chat template function conversation must alternate really seen anybody raise missing something tell model system prompt,positive
hi hub last commit think see please let know anything thanks,positive
hi tried distilled model,neutral
could point implementation one yes,neutral
gold resource used training session memory used memory used check resource,neutral
could point implementation one,neutral
thanks last query would make sure work case meta mean add test case meta,positive
also type hint considered image,neutral
thank reply new guy type hint sure situation previous version python version python one element type image example provided document python checker indeed raise warning,positive
one thing note reuse block model folder true location include unfortunately flush related staging directory still behavior network attached storage,negative
tried version dev issue dev give,neutral
possible disable without export running python program terminal,neutral
running inside container arm useful feature,positive
issue llama model tried add eager got following new attribute file line return file line forward return file line return file line return file line forward return file line file line reraise raise exception caught replica device original recent call last file line output module input file line return file line return file line forward file line return file line return file line forward else object attribute,positive
hello facing issue stage object attribute file line return super name defer logic file line raise type self object attribute name object attribute eventually maximum recursion depth calling python object lightning python trainer trainer strategy class module self none model model model,positive
still face issue mistral model seen similar work,neutral
hi would like u still interested working pushing code broken would recommend pushing unless want save history later,negative
great catch many thanks left review,positive
yet seamless streaming since architecture somewhat seamless license unfortunately also barrier usage anyone community interested model sure happy lend hand integration open feature request community,positive
also found padding worked better likely model vast amount data working strongly,positive
could race condition could done certainly logic sense would like open chance get start sure best methodology running rename operation give shot,positive
pretty sure naming standardized immutable sure venture want go unless mistaken trainer support custom model due everything else right,positive
could please final review good repository reflect implementation well soon everything thanks attention,positive
need within default might need,neutral
tried new token error,positive
reference best practice power pow work better think matter exponent multiplication instead constraint faster impact,positive
hi thanks raising issue indeed issue happening deformable box account new image height width open update,positive
tested also experienced without simple setup also rely full implementation folder might good explicitly call fail hard fake,positive
memory issue ram dram depending data model running need delete issue yes ran memory,neutral
hi tried one model course working error like trying load model pipeline model compatible task provided example demonstration want use whichever model trying get probably better idea use new file directly inference yes guess use like pipeline function call right switch somewhere else necessary default behaviour library last try ago got error maybe still older transformer version really could tell without know code running however suspect get error error raised method responsible different pipeline passing pipeline ambiguous argument supposed raising error pipeline know use argument general want start pipeline might easiest work code directly example,positive
agreed looking right solution,positive
thanks linking issue solution accelerate case problem saved trainer,positive
could race condition could done certainly logic sense would like open chance,positive
good afternoon happy new year best luck new year also rocking weekend today continued working translation much time join development unfortunately work translate volunteer thanks taking implementation feature couple time forum official version,positive
thanks detailed explanation principle pro recursive check however audio rely logic might also affected change make sure well tested let add iron might flag vision ask audio team think,positive
hi thanks issue indeed get error try run example script like coming hub likely definition enough get working think work without dependency suggest opening discussion model page hub flagging issue,positive
much progress yet due may another week revisit want build try get rest way feel free,positive
honestly idea happening run without issue machine mac best guess version run python environment one pip version seen warning message part library since check version run python command posted running environment need make sure pip running import print python environment confirm happening,positive
accelerate please try common issue,negative
thanks thought problem memory may seek help,positive
see perhaps could help use nightly build use latest version,positive
thanks fix push hi think good please reach,positive
finally thanks fast review,positive
come across many similar add new vocabulary reference couple link useful made roughly however concerned first identify make sense add vocabulary also possibly whether sense consider removing vocabulary context situation situation believe quite typical reasonably large text available end goal produce language model used various downstream text classification sentence similarity additional current understanding first obtain language model basically two train scratch use train scratch modify vocabulary adjust also matrix work new size text something like struggling first option far know training language model scratch quite expensive although budget order starting explore second option confused properly modify vocabulary way sense model concerned side effect could cause summarize really like know low cost option training scratch option option sense properly modify vocabulary find good new remove unused adapt model overcome potential negative side effect messing thanks help sorry long question thought context may since might wrong question first place someone,positive
yep everything thanks running doc test runner merge thanks work,positive
one mistake sorry see last commit slow test root test session platform python collected summary unknown option unknown option key module removed future version please import directly see import type ignore unknown option unknown option key module removed future version please import directly see import type ignore call implicit pep preferred see removed future storage class matter directly access directly use instead return instance owner since removed future release please use instead find detailed upgrade since removed future release please use instead find detailed upgrade use expanded please clone tensor operation also advanced indexing tensor index tensor triggered internally use expanded please clone tensor operation also advanced indexing tensor mask scalar triggered internally think problem answer question function training code accelerate launch accelerate launch work fine,positive
thanks kind comment issue check hard disk memory left guess got point since library made individual official thought unknown incompatibility like thing made new environment run code led error memory line file indicate something mean issue forum instead delete forum waiting answer thank much,positive
thanks suggestion everything fixed recommendation,positive
open patch linking issue,neutral
hey sure would like open fix,positive
alright remote model would recommend open issue unless also,negative
hi could regenerate token save issue happen token reason,neutral
hey thanks opening issue check whether machine yes try use float training rather full precision also check memory usage training related custom code could ask question forum instead sure community help try keep thanks,positive
hey think used mind ellipsis type expert,neutral
right sorry confusion sure play maybe integration,positive
default generate function use instead everything probably looking past key still take account previous context efficient manner see answer,negative
moreover use correct class convenience always get output jitter training,neutral
feel free mention class use example,positive
yep let keep way,neutral
hey help could provide actual reproducer one current model class compatible language model head please use one following class instead,neutral
tried best clean original inferior notebook able reproduce error find code first section onto hugging face hub though need run second code section reproduce error might find difference working inferior within function edit pinpoint culprit function data collator issue fact package inferior notebook pip install install pip install torch install accelerate second pip install import error thrown without trainer accelerate please run pip install torch pip install accelerate environment setup somehow break trainer produce batch size issue confirmed pip install torch error persist behavior also working notebook second method thanks lot problem,positive
feel free ping whenever,positive
would need approve reference think anyone successfully would great get also model yet due coco,positive
really access currently feel free test still draft mode ready review,positive
hey thanks report potential would suspect fa fix gradient come back would great isolate commit lead,positive
yes many thanks feature looking,positive
hi suspect might something version try official version also try train mixed precision need load model pas hello may ask necessary load model,neutral
sorry late response could please give instruction merge lora base model push save final float model model auto save lora,negative
posted please check thanks,positive
test error change model change next take test error,neutral
mixed precision issue last post actually fixed seeing somewhat outdated version system instead another issue mixed precision affected testing fixed pathway may affect saving also broken patch everything found except loss,negative
hey also apply please image hey already made,neutral
latest still face issue,positive
would propose point understand might would beneficial close pull request open new one,positive
hey comment hey thanks mind think pretty easy anyone build top add custom logic community lot interest let merge usually wait feature request interest clarify interested merge something would appreciate utility library happy everyone,positive
hello use case grammar dependent input text wondering current implementation passing batch along batch input constrain output based different hey interesting use case working keep,positive
torch bin favor harm improving coul add test well sure please let know test case correct,positive
also wish new feature found may able modify trainer create train probably need update every epoch train loss would get suddenly increase every new epoch please let know solution,positive
much memory note need billion ram run model,positive
hi working first contribution made want add alone local hybrid attention thrown,positive
model need key use even,neutral
hey also apply please,neutral
question building conversational recursive use case important previous conversation previous hidden new context used default behavior would best way reuse previous hidden thanks,positive
think correctly note also,neutral
sure understand network attached storage node might actually complete operation next process come check path complete sometimes core issue suggestion use something like else synchronize everyone afterwards would use main process false otherwise local main think intended behavior part sure file used later downstream could introduce race condition would nice could ensure rename actually,positive
case prudent one process wait rename operation complete however used rename limited multiple making repeated,positive
yea sure go ahead add variant simple enough might useful start another,positive
thank timely response llama,neutral
sure exactly causing function added however seen import error validate slow able run locally main python session run import could try main make sure included branch,positive
work indeed thanks happy open sure kind fix contain since issue simple fix work,positive
completely sense example support certain make sense,positive
hey removed whenever tried run test getting following error image,neutral
hi tried one model course working file line file line transcribe file line pipeline file line pipeline model file line file line raise key probably better idea use new file directly inference guess use like pipeline function call right switch somewhere else last try ago got error got unexpected argument maybe still older transformer version kind,positive
think issue across lot believe file atomic possible file system case might reflected causing observe wrong state found good way ensure rename catching exception would handle though ideal way deal race condition,positive
suggestion sense although also need define data none slightly suggestion regarding extending logic image think root issue function class already account data list first converted however case data list recursive type check would fix issue sure would break maybe open separate python value value list value return value return value value value value return value,positive
problem setup set correct branch default correct branch default,neutral
thank kindness understand thank assistance,neutral
dummy test case proper let know would like something involved test wise,positive
tried best clean original inferior notebook able reproduce error pinpoint culprit function data collator issue fact package pip install install pip install torch install accelerate second pip install import error thrown without trainer accelerate please run pip install torch pip install accelerate environment setup somehow break trainer produce batch size issue confirmed pip install torch error persist,positive
thanks across different working working see python environment method install pip,positive
hey thanks quick review relevant feedback logic bit also pending comment testing wait comment,positive
update making sure clear class defined notebook readable format save load model directly need import previous comment notebook class work directly instead trace pending pull request hope,positive
think issue fixed current main branch,positive
many thanks need help run model static shape,positive
may ask whether get solution main branch also issue block u node training thanks,positive
use generate function otherwise idea model model necessarily predict token give input property python prompt please provide code sample import model prompt string print string provide code sample interested use,positive
use token need also pas token agent like done notebook think,neutral
thanks yes main focus ship end week,positive
processor processor fix think issue conversion work well otherwise want open fix,neutral
like add alias deprecate param name create unexpected people assume,positive
hey could try torch version bit old,positive
forgot leave comment solve issue closed help understanding issue short notebook reproduce error original mess following comment knew problem speaker batch understand thus two code speaker extraction function use data collator made function though likely faulty two made instead directly speaker audio used load audio first converted returned processor within batch honestly think orthogonal issue though blindly think issue make another comment reproduce error,positive
need call used full,positive
yes copied somewhere let review,neutral
error natively need use better transformer usage looking,positive
checked cluster file system independent file default fine make sure serialize execution node still work think might still issue,positive
would work setting without file system,neutral
end simply setting worked everything python setting trainer method local set false consequently following explanation provided work correctly false first process rank node rank environment likely want use main process first node python self work context manager torch distributed environment need something main process blocking finished one use map feature efficient run main process upon completion version automatically loaded local bool optional true true first process rank node false first process rank node rank environment likely want use main process first node however main process node need default behavior optional work work description used,negative
without tested like right direction,positive
rank pop first tight set python model folder true location,negative
however global fact rank printed first,negative
checked code snippet number per node total python import logging import import import torch logger check node rank address family protocol root check node rank root check node rank root check node rank root check node rank root check node rank root check node rank root check node rank root check node rank root check node rank root check node rank root check node rank root check node rank node appear correctly node rank going node node rank node node rank node however global context proceed without waiting result global rank could cause operation python model folder true location,negative
work around trainer replace method added try exception class trainer self model trial always reference model want save except assert model internal model reference save model none trial none destination directory already saving proceed saved may invalid else save save state determine new best metric best model metric none none metric operator else none none operator save trainer state place final location saving finished first wait everyone finish writing go process starting process try model folder true location maybe delete older except exception print error rotating skipping pas,positive
think patch ready please review thanks,positive
hi believe pull request ready review like highlight especially regarding entirely confident ensure branch make check removed done involved somehow complex would really appreciate guidance standard preferable way handle intricate please let know make necessary thank time,positive
tied running notebook main version model set token image error getting image,positive
understand correctly close gap inference inference usage would become much,positive
curious reason must process main process e tested code yet compute currently filled experiment set finish couple day get potential solution rename main process avoid race condition especially distributed else,positive
another finding version image also issue guess dev work differently depending platform,neutral
hi tested case local machine got following warning think case failure patch know run case correctly thanks lot upstream test session platform python collected summary unknown option unknown option key warning,negative
approach issue working well however root cause handling properly,neutral
could locate best model running distributed training multiple activate use whisper notebook,positive
original also scalar however actually matter,positive
would also mind installation,neutral
thanks link look correct gradually replace later thanks,positive
dear problem language ca handle error try run forward self size must match except dimension size got size tensor number list help much thank,positive
keep talking apparently correctly multiple loss number ca quite work bigger issue however mixed precision broken file line return data file line zip file line file line apply finite file line cond return cond file line raise none file line exception calling new graph often happen function synchronization point function control flow statement synchronization point body yet instead please avoid control flow may potentially cross synchronization boundary example wrap entire inside move control flow please avoid going tackle one today,negative
basically silly mistake end advice fixing test error,negative
latest branch longer according,positive
sorry quite understand mean saying remove originally want wrap wrap train however encounter error process trying minimal configuration still encounter,negative
think idea remove call error mention would like open,neutral
use parameter tried passing identical unlike tested without send snippet error passing instead device else processor slightly typical people quite common among especially faced unknown however must press forward nonetheless bark configuration model device model configuration device processor device output error argument must passing directly generate think setting low enough dropping little static keeping clip shortening probably working need play around,negative
oh sorry eager default used idea training expert idea training relevant,negative
hey thanks flagging issue manage solve could send snippet reproduce issue hub,positive
long ride thanks great work patience,positive
hi enate currently already appropriate code seen,positive
appreciate review probably link comment issue future likely remove channel support unless see explicit demand case please put monitor otherwise channel usually date mention channel good go well,positive
use parameter tried passing identical unlike tested without send snippet,neutral
hi use instead python import modify necessarily pas generation simply pas still seeing tried gen fig also starting get sort import import import torch import import import import import use split device else processor slightly typical people quite common among especially faced unknown however must press forward nonetheless bark configuration model configuration device processor device output error argument must parameter tried passing identical unlike tested without,negative
thanks fix issue also,positive
thanks quick reply ran following python auto fail test namely equality test reference output also catastrophically test closed model removed equality check model reference openly available model,positive
hi use instead python import modify necessarily pas generation simply pas,neutral
ran eager seem make difference also number really seem alarming yeah guess tricky look different significant regression reward model training maybe hidden index somehow,positive
dear amy many thanks speedy response quite helpful tried proposal principle following directory however bin far model single file smaller used directly inference following six necessary inference one single many thanks help,positive
also number really seem alarming,positive
could try eager instead wherever model one biggest see,neutral
hey feel free ping review ready,positive
hey trying outside scope token done function fast version right expect absolutely reason model fuse thus slow version wrong specifically really allow way,positive
luck device else processor slightly typical people quite common among especially faced unknown however must press forward nonetheless bark configuration model configuration device argument must,negative
well like doc might generation properly maybe fix,neutral
add whole specifically normalizer thing need plan yet,positive
found bug feel free,positive
intuition correct slow version implementation library fast implementation based,negative
sorry confusion simply ran locally well glad show might docker issue issue,neutral
please forward issue wed wrote hey thanks opening issue try keep could ask question community tab instead thanks reply directly view id,positive
hey thanks opening issue try keep could ask question community tab instead thanks,positive
thanks reply following since become different also fine image python epoch update loss update loss update loss update loss update loss update loss update loss update loss update loss update loss update loss update loss epoch update loss update loss update loss update loss update loss update loss update loss update loss update loss update loss epoch update loss update loss update loss update loss update loss update loss update loss update loss update loss update loss update loss epoch update loss update loss update loss update loss update loss update loss update loss update loss update loss update loss update loss epoch update loss update loss update loss update loss update loss update loss update loss update loss update loss update loss update loss,positive
working improving model performance experienced issue bring speed confirm issue looking forward,positive
pip environment message yes problem original code,positive
thanks review got back vacation proceed exactly still need due slight double check everything back,positive
hello wish provide information first stack trace landed specific line throwing error forward self size must match except dimension size got size tensor number list error thrown within batch tested batch returned data collator training proceeds per usual far concerned however component inference shape within batch returned data collator size still post help much problem,positive
could take look made segmentation,neutral
additionally way retrieve edit merge slow fast respectively,negative
hi happy new year yes work older added argument part however first experimental feature bug free strongly advise recent many different saving serialization since simply load model saved save import model,positive
sure raise today thanks,positive
also get missing model loaded last rather however next restart suggesting important really,positive
mean change logic aspect ratio input different one would expect performance difference even though model image previous might bring better performance true aspect hence image speculation,positive
architecture implementation correct need fix cause,neutral
comment yes separate modeling file think sense,neutral
system version dev platform python version version version version true version false flax version version version script distributed parallel script help forum post put like bug accept local without trying information official example officially task folder task give reproduction import behavior processor error repository found make sure correct trying access private gated make sure hey try modify function like python open return,positive
hey thanks swift response still rather new actually confused slow fast based really understand one based based supposed trained way different hence think position open fix sorry,negative
let run python virtual docker machine see issue go away ran docker container slightly different thanks quick reply thought something since saw warning recently post back later today update also description top issue present sample docker container higher happen though docker container issue present running within python docker mac even higher sure would interested knowing ran reproducible example docker container environment think environment thing feel free close issue,positive
reason generate inference training custom sampling logic thank quick reply however would like use sampling generate function valid training strategy would convenient access code purpose,positive
hi thanks like issue segmentation,positive
happy new year many thanks information lot issue whisper maybe know also work older simply cause error possibility tool converter code extract original file many thanks hint,positive
hey could try main think compatibility taken care,positive
reason generate inference training custom sampling logic,neutral
sense would like open fix,neutral
hey think conversion support implementation model could rather open issue library full reproducer trying full going ask relevant,positive
hey thanks raising issue want open fix,positive
hey think conversion support implementation model could rather open issue library full reproducer trying full,positive
private guaranty properly part public removed recommend go back previous rather use,negative
hi yes enough support phi conversion script verify test phi phi close,neutral
sorry source regression might pretty much anything model come bug might ca would mind closer might help isolating otherwise scope way broad modeling code might mechanism torch might fix,positive
like fault ran code warning whatsoever might need built,neutral
oh checked thought lot enough let rename one phi add support phi close one agree support phi,neutral
let share result problem docker container dependency issue dev even though printed version system,neutral
issue persist llama well next token two still give error string string,neutral
thanks really worked yet initial draft try taking soon get time would fine,positive
hey could share training command used reproducer training well full,positive
overall would recommend model rather llama,neutral
know could try python import import true first,positive
want keep white space used indeed kept would need also add new line via want behaviour regarding might frequency token model learning related last issue probably,positive
progress look good ping whenever another review,positive
make sure use main branch,positive
hey like scheme need need skip normal function,positive
hey layer part public documentation self thus luck probably,neutral
case might need tiny rework,neutral
hey issue rather added almost certain commit issue would present since method wrong since example sentence special want open fix feel free might affect exact,positive
alright ca load pipeline without configuration like would recommend ask outside scope,neutral
hey think best way solve issue still feel like,positive
hello update find highly useful save memory used along file however use without work save memory wonder anything properly gradient help would,positive
thanks information image logic reasonable mean accuracy drop,positive
code test passing run test externally small confirm model amount memory one time checked via memory trace ie via added test considered desirable,negative
thanks fixing future could make sure link relevant code description make easier review,positive
find probably need version library use pip install everything go back normal,positive
hi open new way one lot,positive
thank feedback please expand issue patch install branch patch branch return error hint issue manifested code passing without effect original correct might special use alternative device device use variable instead microphone special issue,positive
delete hub cache command delete local directory could resolve issue reasonably somehow model saved locally could cause issue example model received exact error message,positive
thanks following reflect channel latest version went ahead use channel,positive
thanks confirming reason change logic image classification script reflect model image processor previously size could directly size many model edge image size edge keep image aspect ratio however feature default behaviour size resize image size size case script behaviour image processor even,positive
thanks response regarding none issue default behavior actual issue replicate original example example data point store full command python caption error bash recent call last file line module main file line main file line train return file line model file line loss model file line model file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward size tensor must match size tensor dimension,positive
could test small model check memory utilization still occur call,negative
hi would like take,neutral
hey feel free open increase specific doc case yo feel like,positive
hey thanks wanting contribute would recommend put model hub following tutorial make easily available,positive
hi thanks taking care great logic quite difficult handling nicely two general add test bark modeling code well best stay instead question hope thanks question believe logic coarse generation quite bit make work key following generation loop see thing rest quite static opinion keep track length merge corresponding generation loop first sample taking account sample use pad right length thanks lot reply yes took bit time understand model shape going regarding bit puzzled regarding question body setting try something evening,positive
could rebase make sure green,positive
hey sure first make sure green,positive
follow update dynamic cache issue,neutral
alright feel free share community,positive
feel free ping good,positive
hey thanks follow could ask question forum instead sure community help ca custom code,positive
add detail python python input answer,neutral
change custom remote code understood correctly,negative
fix merge fixed iterate together want opening marking good difficult issue,positive
hi thanks opening issue different address caption none null na prepared example different one general comment meant provide simple script show train model specific task written cover use may need adapt code use case parameter specify hub repository push model hub wish change use control also place track experiment like many different available trainer actual issue without copy data possible u help could share example sample publicly available error,positive
could share issue version sure understand recommendation would recommand job plan bug fix language properly handled could elaborate,positive
hey thanks alright overall let add phi since hi phi modeling code library phi model guess also verify sorry missing something,negative
great good place work common sometimes see large happening intermediate important final small difference huge however still certain loaded found model first see still significant,positive
give basically new cache attention properly gon na priority week anyway,positive
class variable forgot update follow help cache length different number seen get attention layer,neutral
thanks opening internal slack channel rather transition least two update unlike clip loading feature vision necessary change ensure future compatibility,negative
hey thanks raising issue pretty much fixed already work around following tutorial,positive
hey key could try latest version share exact,positive
thanks looking issue ran problem today voice assistant project audio course tried latest branch reason work however name work array registered smart sound technology registered account special symbol registered,positive
hey thanks still small bug implementation review yes given random model uniform test closer however wrong fact routing layer layer paper following page switch layer auxiliary loss added total model loss training long sum auxiliary good go total auxiliary loss across given total loss let factorize expression use addition total loss actual question question basically summed deal top top top top either balance distribution top top routing meaning top top overall give uniform repartition average fraction expert across balanced top top separately balance distribution top distribution top meaning top uniformly route top also uniformly route fraction expert balanced independently mistral really share support meaning let user decide want optimize end day important python tensor top top let write explicitly either want top top python close want top top python close used implementation python none float compute average probability routing return,positive
hello happy new year give hint issue going,positive
hi conversion script added phi test,neutral
hi feature added yet thank prompt response following would appreciate input way extract feature image hugging face feature extraction notebook work way want extract convert image victor use another model recommendation best way clip model give good result,positive
added overwrite could look also yet right bit weird behaviour default match original implementation since original implementation wo ever keep special,positive
thank work may seem like little would huge step bring closer parity like training,positive
hi accuracy drop commit git bisect issue stably running following command even latest issue still python false,positive
hi thanks raising issue year subsequent image logic could confirm commit performance get running main different,positive
thanks confirming could provide minimal reproducer replicate issue,positive
opening discussion discussion relevance totally new modeling code instead one main reason behind new model totally different feature extractor two main input name instead totally rid first block also architecture could justify choice personal position subject leaving new model thing choose keep add possible input different,positive
hi thanks providing information able information version relevant running could also provide minimal reproducer link provided issue description go directly rather specific script get many get timely manner need help u help case need amount code run reproduce error,positive
thank make code much cleaner dare drastic initial commit quite happy way,positive
could confirm environment pip list pip list python environment python import import print print,neutral
please help solve issue,neutral
hi feature added yet,neutral
anyone else getting import name added need,neutral
thanks made work end regarding best try reserve feature bug able find community tried general mean appropriate certainly combine different another subclass however aware loaded train model weight update alongside model reload model model unless freeze likely encounter performance class choose parent class model use loaded use end llama logic two different combine one model need look combining processor class,positive
hi thanks put making work wondering regarding issue particularly forwarding long thanks advance,positive
hi sorry late response vacation python comment pip pip install upgrade result full recent call last file line module import pipeline file line module import file line module file line return requirement hint file line requirement hint file line raise normal module found try pip install pip install dev working git main refer test code import torch import pipeline device else else model device processor pipe pipeline,negative
certainly feel free open ping ready review let u know,positive
happy new year thanks update let u know,positive
hi thanks issue impression model rather code force last initial id certain token let call remove equal believe case whisper work better maybe trained removing corresponding token model keep mind different initial lead different case lead note tried code another sample time contrarily example hope,positive
yes ideally add class refer original implementation hi want know done trying use getting error object attribute use use come,positive
used generate code keeping often lead code method know,neutral
hi happy new year code previous dependency batch padding case want run test notebook link please let know think thank much time,positive
ran issue commit need,neutral
hi meet question tell solve,neutral
thank think still even various think quick fix would disable normalizer use instead python import import true first fix example add two return old new add example class self self return old new self input self fix separate final output instead instead represent double indentation code also character character probably missing replace normalizer something like python import import true first make sure use fix example add two return old new add example class self self return old new self input self fix single character however importance code different indentation character character added said final dot issue however inner punctuation like house final vocabulary think need add python first fix example add two return old new example class self self return old new might better one think aggressive splitting punctuation like character character ideally like similar example add two return old new example class self self return old new,positive
also facing issue directory resolved issue,neutral
hi solve load old without new loaded zero normally,positive
hi thank much attention believe error two preliminary implementation eliminate redundant key query execution testing configuration address original cache concatenation additionally actual eight similar llama phi currently code make work next step,positive
also met problem make image,neutral
similar issue happen mistral well training data loss fast loss,positive
happy new year update,positive
help reopen refine based latest,positive
hello going merge patch thing something add feel free point patch audio like whisper without use fix accept special special read correctly assigned thank kind major,positive
hi error message something like used could solve error least case opt run make make green,negative
could please review added fixed sure test failing,positive
note install install older version instead current pip version,positive
hi happy new year feedback two last commit facilitate training new use feature extractor consequence input name instead think leave let know need,positive
hi current main think ready review,positive
sure totally fine agree readable theory check available user also set torch device external case code exactly equivalent much readable future support hopefully temporary fix immediately spotted removed tested short long audio please check,positive
hey thanks progress far happy new year could actually update code easier review test keep track many thanks,positive
hey whisper expert yet understand documentation processor see code input text worry special need inserted tell model text context start transcription code place use want pas context whisper much flexible could reproduce use advanced use whisper hope want correct give advanced explanation,positive
could actually use method available check available instead error catching mechanism,positive
see feature loading following would solve problem python,neutral
close issue since open keep issue open,neutral
problem model work fine image,positive
sorry last minute small code change running load test audio file another instruction line fix initial commit second fix got could complete full transcription also initial could find apparent incompatibility current status please quick check,positive
hey thanks opening indeed something slipped net offering fix support probably go thanks work,positive
hi update final conclusion,neutral
need done may break operation version time properly disable getting easy way need investigate fix problem well far able train least one epoch loss seem model loss go time,positive
moreover wondering implement accordingly implementation done associated flash attention found decorated,neutral
current progress running dive fix error,neutral
indeed line recommend would best would like open,positive
thanks issue take look issue soon back leave week thanks lot,positive
hi still working take want issue change code several,neutral
still worked project label bot close,neutral
add simple test case also test without patch patch pas without patch main branch fail,negative
python import import torch pipeline pipe simple resize overwrite content better work unable select since path directory pipeline object dont even cant stress enough know one load generate trained dont object clip model want add inside file provided neither valid local path valid give directory get error file found directory,neutral
getting error running locally mac accelerate version kernel work work fine though,positive
properly compute loss uniform distribution side effect issue actually degrade training,neutral
another option could instead define function would allow user determine want write file compute metric,neutral
python import logging import o import import import import torch import import output output message copy remote cloud storage local use two local example output lambda range model trainer output new folder save run first run initialize run successfully save second run try load crash multiple running python work fine additionally reason copy output somehow important remove use output everywhere bug also happen additionally use instead crash forever loading,positive
confirmed unexpected behaviour would willing research behaviour setting submit pull request issue,positive
python unable downgrade version help,negative
rebase took care one failing documentation test anything else need done,neutral
hello tell use train scratch want use vicuna clip vision transformer training like original author,positive
hi issue accelerate single resolved,negative
sat bot wrote issue automatically marked stale recent activity think still need please comment thread please note follow likely reply directly view id,negative
hi also difference implementation wondering something objective also differentiable line page,neutral
care training finish faster approach definitely work whether wasted time please see diagram better understanding instant flagging future right solution many instead need figure wrong network setup enjoy fast faster training time agree thank,positive
try without lora get different maybe useful error message file line step enumerate file line file line file line file line wrapper return file line type ignore trying create tensor negative dimension,neutral
hi version issue pip still face question standard loading model get warning without passing forward call used please make sure provide class eventually access layer index dive code image check also nothing wrong print possible reason cause since issue closed shall open new one thanks help advance,positive
care training finish faster approach definitely work whether wasted time please see diagram better understanding instant flagging future right solution many instead need figure wrong network setup enjoy fast faster training time,positive
code flask import flask request import torch import import time import logging import o print version used print flask get directory file model auto print request made starting timer response print request took return response data conversation data prompt conversation prompt return able run one wont work try run error model configuration control generation strategy control generation removed soon future version please use modify model generation configuration see operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index operator block thread assertion index index size index running arch driver version version compilation release build fine single try run multiple like tried none make difference always tried many many size doesnt work doesnt work either option disable bios test came back good think latency test device device access peer access peer note case device access one back normal procedure see lesser unstable latency u connectivity matrix unidirectional matrix unidirectional matrix bidirectional matrix bidirectional matrix latency matrix u latency matrix u let know else need help get fixed,positive
may work cost variable peer peer transport direct access lose performance greatly zero stage training single host performance case training task,positive
thank see following cause backward compatibility import fixed locally use due issue report import similar sure issue import import dense import import dense dropout layer import image import import,positive
think opt solution need one work without file system,neutral
ran well think still bug one question default installation guide currently even clean environment error due think reason separate need rather source code heavily since otherwise cause separate could cause bunch linking,positive
one library version old one create package leading error library warning documentation direct fork package awesome original package plus adapter implementation share ideally environment library environment instead solve problem pip install accelerate evaluate tyro,positive
hi think may great modify way multiple necessary example tab consist written particular framework,positive
hi since think close also,neutral
may work cost variable peer peer transport direct access lose performance greatly,positive
like server training work turning communication export,neutral
tried run following code stuck work python import import import torch device device run command shell python,neutral
think bug incorrect usage saving adapter disk loading leading adapter loaded twice leading multiple would cause error going close think issue would occur used correct way hi bug incorrect usage however struggle find correct usage would really appreciate could share knowledge thanks would appreciate help code loading saving pushing hub,positive
removing tensor parallel converted cache hi also faced issue may ask actually removed tensor parallel also chat code,neutral
hi like inquire may proceed work actually already draft progress moreover wondering implement accordingly implementation done also confirm access compatible fa,neutral
also encounter head setup like obvious race condition indeterminately sometimes save sometimes th save,neutral
hi thrown batch generating output work fine know whether correctness output see warning python saying model might lead significant drop,positive
hey would like work issue,neutral
finally bios ref driver version help test helpful,neutral
finally bios ref test helpful,neutral
hi behavior casting float still present whisper phi create fix,neutral
hi getting error file tried possible solution get solution anyone please help recent call last file line return file line file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file line module import file frozen line file frozen line file frozen line file frozen line file frozen line file line module import file frozen line file frozen line file frozen line file frozen line file frozen line file line module data import file frozen line file frozen line file frozen line file frozen line file frozen line file line module import file frozen line file frozen line file frozen line file frozen line file frozen line file line module import import name exception direct cause following exception recent call last file line module file frozen line file frozen line file frozen line file frozen line file frozen line file line module import file frozen line file frozen line file frozen line file frozen line file frozen line file line module import file frozen line file frozen line file frozen line file frozen line file frozen line file line module import pipeline file frozen line file line module name file line raise import following error look see import name got brother,positive
issue went away library many thanks discovered issue run problem tried get value stepping resulting,positive
thanks review gone made address couple await review let know chance go,positive
hi hope wonderful break following feedback function enhance key include capture truncate true currently function pad function detailed redundancy code review function notebook original spectrogram tested next step plan implement simple efficient batch padding method current reliance implement batch version add function annotation please let know thank much,positive
main reason work right,positive
exact issue version would get exact code wo,positive
hi happy le test ca wait get model working integration test start fully testing edit update th figured issue due bad reshape able run model run next still follow coming weirdness understand integration test slightly understand test fail help flush model processor processor calling go forward pas invalid size go back last time said input missing test like problem code actually right observing correct size call tensor first forward pas inside might tried converting error need get implementation working convert still like,positive
dont think understood meant want use dynamic padding function found another solution thanks,positive
set found command modify output padding value fixed,positive
yes working also work trainer thanks lot,positive
error single accelerate false inductor main static true false false false calling save model get removed tensor saving check receive warning reload model get error loading missing key calling work solve underlying problem calling throw error removing two sure best solution automatically handled,positive
hi think may broken run trainer turned found one process waiting mode tried save log process waiting main process perform model folder true location,positive
hello open feedback continuation work sort like addition version doubt translation link correct since found documentation thanks,positive
hi problem scenario also need add training also set may ask problem end looking forward reply idea yet whole guess could weight problem roger thank reply,positive
hi problem scenario also need add training also set may ask problem end looking forward reply idea yet whole guess could weight problem,positive
hi thanks review handle gradient accumulation model,positive
may arise hardware thanks time attention hi issue may ask cause problem later,positive
sorry method may related environment may always effective research better address issue,positive
still facing error fine tuned mistral model trying inference still giving could complete request status code error library found environment page follow match environment please note may need restart done pip install fine tuning,positive
fix problem met issue change use implementation know fix loss always constant change,neutral
fix problem met issue,neutral
thanks tried install driver still ca work quite weird another server basically could work working server driver python python main type help copyright license information import torch print built version math kernel library version build architecture git hash usually provided capability usage architecture built built magma build working server driver import torch print built version math kernel library version build architecture git hash usually provided capability usage architecture magma build working server also tried create another environment python python main type help copyright license information import torch print built version math kernel library version build architecture git hash usually provided capability usage architecture magma build still raise error inference wondering need install separately library could find difference install,negative
working reason cant access certain network,positive
even facing issue way resolve,neutral
model whose structure different hence use method,neutral
hi thanks raising issue sure save bias loaded make dummy model following pattern example layer loaded randomly running python import torch import class foo self super foo foo set set zip assert else different note class inherit however tried work even randomly initially set weight bias sorry late reply thank patient first question incorrect saved magically without code difference observe method default save instead file still approach appropriate way combine model together use trainer save strange combined model training since two totally different solution think initialize class save delete folder change class code load keep llama combined llama llama seem like normal form model trying jointly training retriever like model together trainer sure community tried rag really popular,positive
tested driver driver working got operator block thread assertion index index size index someone please give help thanks maybe try driver,positive
still searching solution nearly convert condition tensor,positive
sorry specific case pas image,negative
hi could clarify still need prepare image model even set false,negative
future likely remove channel support unless see explicit demand case please put monitor otherwise channel usually date thanks raising issue,negative
time yet make probably time next couple day otherwise next week work,neutral
thanks lot posting someone team,positive
specify static cache static cache also work every forward call always shape think good idea class variable,positive
know issue old far know way achieve unless solution posted worked find documentation important people logging solution would cool clean way keep logging loss every since still work user disable printing terminal,positive
answer posted forum short tied input output saving one two used one transformer,neutral
hello wish provide information first stack trace landed specific line throwing error forward self size must match except dimension size got size tensor number list error thrown within batch tested batch returned data collator training proceeds per usual far concerned however component inference shape within batch returned data collator size still post help much,positive
hello ran experiment mistral task epoch code version dev platform python version version version accelerate version dev accelerate found version true version na flax version na version version script fill distributed parallel script fill dev plot similarly plot zephyr training,positive
issue coder like issue tried model recent call last cell line file self private token revision save return file self variant token shard point need deal better used distributed joyfulness enough shard format else shard file union optional none dictionary raw format error o code kind message space left device recall issue many size,positive
similar error trainer error exact moment another process choice try block get around python model folder true location try except exception could rename directory reason dev,positive
class pooler self super self return self forward self shape optional sequence output last layer used model shape optional mask avoid attention padding token index input mask used model mask selected masked masked shape optional language modeling loss next word prediction index see index set masked loss length shape key value hidden attention used speed used user optionally input last past key value given model shape instead shape bool optional set true key value returned used speed see example import import torch model hello dog cute model none else none false return none none prediction shift prediction input one output return output none else output return self model used model attention mask fly none cut past used past none return past none none true self past past return loading weight appear model used model trained another task another architecture model model model expect exactly identical model model model newly deal,positive
either try fix addition think force push open new clean hard read thanks open new clean,positive
would appreciate could check time thanks,positive
thanks still slightly confused say used provide initial context still side actual text different,negative
thank much work well,positive
got issue even anyone resolved issue version worked well without got issue worked day got issue training library cause model solve issue make sure calling training job solve training single training please make sure correctly library reference model reward model initialize model passing trainer class contrary reference incorrect may see error like must true none true true none true downgrade also personally think good solution depend new future,positive
hi problem scenario also need add training also set may ask problem end looking forward reply,neutral
thanks working code inference model must simply load bin model end training python model code architecture,positive
yeah true since channel running quite behind,negative
regarding potential weird interaction flash attention gradient cause inefficient redundant computation flash attention equivalent redundant fa forward reason flash attention backward kernel recompute similar recomputation backward process original gradient interested efficient flash attention friendly gradient mitigate issue need pip install import monkey patch accelerate training saving one flash attention forward every layer gradient thank answer sense speed issue however make sense memory saved,positive
mean try load binary file directly,negative
latter latest version available channel mistral bug likely unavoidable channel,positive
problem mistral bug able install channel latest version still,positive
related note latest version bug anyone channel official run bug get channel,positive
supposed fix show snippet code work already supposed read environment variable see snippet code saved python import print modify add output python try load cache already add print add break point print bash python run,neutral
supposed fix show snippet code work already supposed read environment variable see like use method function load model python import see user start project bash command python user defined still origin default value issue modify add output python try load cache already add print print test project use method bash git clone depth git create activate pip install python load new model think fix newly commit issue gone,positive
yeah mean native function one specific maybe general,negative
initial add model multimodal model paper original model actually added code yet scaffold,positive
initial multimodal model actually added code yet scaffold,neutral
hey version python working hardware thanks lot problem wish good day,positive
thanks actually model like python import import import image import torch import processor model model set beam search true inference image processor print way get list inference maybe manner loading model exactly code load file python import model,positive
hi understanding constrained beam search used limited mostly experimental also expensive maintain want avoid expanding unless significant demand going decline offer usual bargain comment looking similar case happy revisit decision like hit magic number time feature take flight,positive
regarding potential weird interaction flash attention gradient cause inefficient redundant computation flash attention equivalent redundant fa forward reason flash attention backward kernel recompute similar recomputation backward process original gradient interested efficient flash attention friendly gradient mitigate issue need pip install import monkey patch accelerate training saving one flash attention forward every layer gradient,positive
faced problem think forgot change special loading,positive
image processor static model get training would recommend fitting model single example see issue make sure prepare image text model way training inference,positive
hi bit issue found problematic line,neutral
question image processor get training think,neutral
first training loading inference set special used set beam search true second thing load model,positive
hi following similar approach load model inference training model work well even good epoch made extra inference one image end good one maybe good code save reload model share save load code model training fashion code python processor model code load transform model set special used set beam search true save processor final model load model inference two python processor model making like python processor model output absurd first get something like one second one get series line one one one,positive
yeah sorry missing silly point come author provided work fine splitting however author trained algorithm always use model link,negative
solve let know issue,neutral
tested driver driver working got operator block thread assertion index index size index someone please give help thanks want inference maybe try tensor parallel problem thanks help actually working training found problem also shown inference,positive
tested driver driver working got operator block thread assertion index index size index someone please give help thanks want inference maybe try tensor parallel problem,positive
problem batch inference harder reproduce find specific image,negative
issue happen case one custom past key value case fixed,negative
hi thanks trying model valuable feedback reason mean processor former image processor corrected indeed another thing could add list see included fast testing converted one perhaps problem regarding fast part yet simply converting going result equivalent due use function punctuation use need added separate testing hypothesis template cause significant output especially pipeline correct say photo one use prompt clip paper see notebook simply pas apple san instance model use clip prompt template testing hypothesis template cause significant output especially pipeline correct say photo one pipeline support added working used whereas sigmoid perhaps warning length ca tell many spent sure pipeline usage account good point maybe opinion add default processor,positive
tested driver driver working got operator block thread assertion index index size index someone please give help thanks,positive
hey basically input token provided model generating text initial context model begin generating text hand mainly used part example transformer structure provided part model help guide generation subsequent sequence come generating text via whisper used provide context guide model text generation also prefix feature whisper mainly either combination provide context model implementation difference usually provide initial context generation process mostly guru,positive
require use generate parallel mode ram prompt parallel source model folder treat thingy parallel work fine test python print hello name parallel setting generation copy construct tensor use true rather cache scale hello name year old female outgoing person recurrent python print hello name recurrent setting generation copy construct tensor use true rather cache scale hello name world first thing size room even say use recurrent forward mode prompt pas unless use,positive
hi suspect might something version try official version also try train mixed precision need load model pas,neutral
hi thanks much interest hopefully quite soon need time make sure match merge,positive
python regarding issue possible solve issue yes involve yes must help highly,positive
totally support proposal code available,positive
facing similar issue training fine one stuck multiple setting raise error block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion block thread assertion recent call last file line module file line train return file line model file line loss model file line model file line return file line forward file line return file line file line reraise raise exception caught replica device original recent call last file line output module input file line return file line forward file line return file line forward file line return file line forward file line return file line forward error assert triggered kernel might call might incorrect consider passing compile enable,positive
got problem run inference conversion model find since conversion fix,neutral
convert opus model never mind got,neutral
result get correct necessary generate whole sentence want convert model format translate suggest use optimum generation work example python import pipeline import model pipeline result hi tried code working edit indeed work forgot print result,positive
thanks lot token token following error self optional token token special part add end order addition following token token token,positive
really edge case output model actually generate sort output like becomes much common model related think make sure hence closed issue,positive
removing tensor parallel converted cache,neutral
thanks would love example training script try domain specific image,positive
hi far tell error message code trying find file commit getting error short time fix quick fix try convert according commit long time fix since phi added would suggest upgrade latest convert previous according library model weight structure would require rearrange way wo care new model phi library let know need,positive
hello issue load model recent update error message received seem contain file please check available someone relatively new anyone help figure tweak code load model properly model help,positive
yes totally right hugging face thankfully opt use return instead setting model loading method python import torch import print tensor print tensor,positive
hi take task model,neutral
save support default type used hi checked way issue related indeed exist bug create separate issue,neutral
processor train path processor processor test path processor processor model model set special used make sure size set correctly set beam search true,positive
print sh format properly example whole think may something wrong since list argument properly use use environment variable optional solution hack fix add python import rank rank,negative
hi response life threw back track thanks patience regarding request output bash version platform python version version version accelerate version accelerate found version true version true flax version version version script yes distributed parallel script let know anything else help,positive
hi amy right similar solution save portion however afraid issue could arise load unless parameter set false pull request resume true false magically issue previously moreover like object scratch cycle fact resume save support default type used currently scenario saved loaded bit handle scenario minimal commit,negative
anyone new version easy dirty fix python text translate text text manual fix bug decode return text return hello print notice wrong wont break working since version would recommand job plan,negative
got issue language set default wanting change like straight bug code wrong result overwrite stuff manually spec,negative
hi sorry understand fa flash attention motivation bug reproduce following script python import pipeline import import audio import torch train audio audio example pipeline output example audio array pipeline support think could enable fix problem load pipeline python pipeline run script input float data type must limitation,negative
got small layer layer scratch would also recommend starting positional layer scratch small hi share code accomplish also trying replace,negative
hi seeing test failure related document building testing run example might due killing worker like previous example configuration think change example smaller one,negative
tested work great busy reason mean processor former see included fast testing converted one perhaps problem testing hypothesis template cause significant output especially pipeline correct say photo one also see model significantly different maximum length correct behaviour example code image cat image cat always pad length mention one otherwise see made note make sure pas model model trained perhaps warning length ca tell many spent sure pipeline usage account,positive
hey share exact sure thing recent call last cell line train self trial else return self trial model model self model loss model self model else none model save past state need fixed made cleaner later self return type ignore else return self self return try forward forward return act like decorator self self return self return decorator script mode type ignore forward self none convert self return type ignore else return self self return try forward self mask length calculated via length past many unpack hi actually writing talk work trying get accuracy rate able better,positive
hi stab necessarily area happy learn contribute,positive
hi thanks fix still face issue think relevant figured post trying use whisper model batch size example word level get error method file line yield file line forward file line file line generate file line file line many unpack matrix object shape indeed shape idea overcome thanks update perhaps fix might line none list none list saw array update fixed open review,positive
hi could please let know anything add happy thanks team great work,positive
hi thanks feature request model code hub request add fa support done discussion page model update message end route correct place recent call last file line module main file line main file line file line model file line model file line return file line file line file line raise support flash attention yet please open issue request support architecture,positive
still issue forward pas instance segmentation,neutral
written tentative idea import list optional union import torch torch import import import import class self super none none else initialize apply final self return self value value forward self optional none optional none optional none optional list none optional none optional none optional bool none optional bool none optional bool none optional bool none union none else loss none none loss output return loss output loss none else output return work well,positive
solve problem reconstruct within function like sample understand necessary make sense,neutral
thanks file separate issue tag also provide reproducer would great got issue hopefully cover related index,positive
think experiment could load model train model converging nicely,positive
include fix patch release,neutral
tried still error think core issue python may induce index really quite understand piece code fix think look like know correct python need attend python need attend might better,positive
pip install code cluster node host point saving watchdog cause collective operation cluster training interrupted positional use instead refer positional use instead refer positional use instead refer positional use instead refer positional use instead refer positional use instead refer positional use instead refer positional use instead refer torch saving torch saved torch saving torch saved zero saved torch ready rank watchdog caught collective operation ran timing rank watchdog caught collective operation ran timing rank watchdog caught collective operation ran timing rank watchdog caught collective operation ran timing service thread connection closed service thread connection closed service thread connection closed rank abort complete timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing service thread connection closed rank watchdog caught collective operation ran timing rank watchdog caught collective operation ran timing service thread connection closed service thread connection closed rank abort complete timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing rank abort complete timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing rank abort complete timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing rank watchdog caught collective operation ran timing rank abort complete timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing rank watchdog caught collective operation ran timing service thread connection closed service thread connection closed rank abort complete timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing rank abort complete timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing rank abort complete timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing warning sending process signal warning sending process signal warning sending process signal warning sending process signal warning sending process signal error binary recent call last file line module main file line wrapper return file line main run file line run file line return list file line raise avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing warning sending process signal warning sending process signal warning sending process signal warning sending process signal warning sending process signal error binary recent call last file line module main file line wrapper return file line main run file line run file line return list file line raise time host rank signal received time host rank signal received root cause first failure time host rank signal received,negative
also issue fixed meta meta per node per node,positive
see error version well file system across node,neutral
issue resolved problem trainer set otherwise work,neutral
sure minimal example python import pipeline import torch pipeline patch loop best sec per loop patch loop best sec per loop note run much faster patch marginal improvement like randomly something float really expensive sanity check nothing terribly bad result patch also public domain audio test downstream tool work intended resultant post patch identical audio edit although failing test documentation check mistral touched,positive
fixed code getting following,positive
address hi could please give context mean mean difference model library model hub,negative
thanks lot review merge soon release information,positive
anyone use fix next release bash pip install,neutral
thanks help wish happy,positive
hi think issue try main,positive
thanks try take look though probably,positive
thanks lot review passing version docker image use rocket thanks insightful discussion fix,positive
warning following accelerate launch used instead set value set value set value avoid warning pas problematic run accelerate recent call last file line module main file line main file line file line file line return name level package level file frozen line file frozen line file frozen line module need define part command try following thanks patience shell accelerate launch main sentence label,positive
need define part command try following thanks patience bash accelerate launch main sentence label,positive
error warning following accelerate launch used instead set value set value set value avoid warning pas problematic run accelerate unable register factory register factory one already registered unable register factory register factory one already registered unable register factory register factory one already registered recent call last file line module main file line main file line file line attribute name must string recommend accelerate launch python done work make sure spawn still work fine try running shell accelerate launch sentence label,negative
accelerate used always accelerate heart trainer,neutral
hello whole afternoon found bug tiny bug line somehow wrote range instead range former sense guess typo hidden well happy work failure irrelevant,positive
accelerate use version script misunderstood,neutral
note try fix regression perform separate meanwhile always use model eager revert previous behaviour,negative
sure ca use notebook,positive
recommend accelerate launch python done work make sure spawn still work fine try running bash accelerate launch sentence label,positive
correct believe point gotten red warning message accelerate version later saying already environment restart case please provide u fully working notebook full execution order may able find way guard warning better thanks,positive
thanks yes second said put high merge,positive
pretty sure fixed make work need load model train model make sure use main,positive
thanks issue resolved close,positive
hi indeed correct despite model explicitly trained perform well nice visibility suspect something able deal attention need time properly investigate fix meanwhile two use eager attention implementation import image import import model model eager processor prompt image image difference two prompt image describe prompt image describe image image processor prompt prompt prompt image image image image key key key print key key generate print process instead generation well,positive
issue executed command pip install accelerate went restart session problem resolved running pip afterwards,neutral
yes doc automatically run security merge thanks contribution,positive
thanks improving code failing unrelated fix merge main recently could rebase push branch resolve trigger another run need extra approval finish merge,positive
confirm disable output identical regardless origin,neutral
since merge near future written small extension library use straightforward,positive
hi fixed phi work could please run let know showing,positive
model trained hugging face architecture use code load model import equivalent format model model thank beautiful lady answer given confidence scientific research path hope keep touch,positive
understand correctly loss two categorical loss one hand balancing loss expert print two final one however first one second one tensor without final loss instead balancing expert loss change training process maybe see due categorical loss open new issue,positive
hi thanks opening library let u know ready final review could provide context used ideally example snippet assuming enable fa already possible pas model specific auto might interplay tested make sure unexpected behaviour,positive
hi looking first error log exception direct cause following exception like error early stage relevant rather inside modeling part bash file line module main file line main sure relevant even accelerate let wait back people issue example,positive
spent time investigating reason mismatch notice pas without failure shown latest commit numerical issue like something batch try figure implementation bug input,positive
green blocking feature passing day,negative
thank much look forward compatibility future,positive
hi thanks ongoing address issue reference,positive
model trained hugging face architecture use code load model import equivalent format model model,neutral
hi thanks opening nice big fire could share tested speed,positive
thanks opening issue fine open community need add image clear working done well ideally one done making sure run slow,positive
weight file downstream task file want load framework solve import equivalent format model model weight file downstream task file want load framework solve,neutral
yes main rebase main merge main branch include fix push branch trigger,positive
create check merge commit,neutral
hi specific file convert torch conversion call import equivalent format model model,neutral
hi thanks raising issue could run following command line check version run python environment python import import print print,positive
thanks code assume extend include correct another note first code snippet trying bunch different get different cosine ended high,positive
ca work yet static cache fix,positive
could push hub way access,neutral
hey wo solve issue sure might solve well could open solution also give pointer projection weight many thanks,positive
would remove label smoothing code actually usually add regular loss label smoothing subtle probably useful noisy data,negative
thank much see already implementation kudos help way,positive
hi compatible training yet future though,neutral
see accelerated tried session accelerate package know obvious completely foolproof,positive
thanks reply wait merge actual learning process projection used last dimension convolution dimension conformer block different example dimension projection weight error converted format inference model random weight projection used result ruined thanks,negative
thank patience operation track work fine ago root git clone remote done remote counting done remote done remote total delta delta mib done done note switching head state look around make experimental commit discard make state without switching back branch want create new branch retain create may later switch command example git switch undo operation git switch turn advice setting variable false root python python default type help copyright license information import import recent call last file line module file line return file line file line super file line token token token file line token token token type,positive
try running smaller model successful hit issue see running script,positive
behavior considering relationship image specifically image patch image patch issue come across mention related image paper approach used official implementation would beneficial example would open pull request example,neutral
another weird thing behaviour saving import text text text,negative
able test think error memory disk probably initially definitely close total model size trying push loaded model may much assume issue find different later reopen,positive
issue trainer following recently pip install upgrade epoch edit one thing note per node looking code around error feeling may used going try model folder true location edit edit like still working even false full command job bash accelerate launch false,negative
could give working example show problem would happy investigate,positive
currently dev python prompt image image difference two prompt image image describe two prompt image describe image image processor prompt prompt image image image image output text user difference two assistant two primary difference presence flower dog mouth first image dog holding flower mouth second image dog holding flower subtle change scene dog interaction flower may evoke different depending viewer perspective user describe two assistant two show cute brown white dog standing grassy hill one image dog holding green leaf mouth holding yellow flower capture dog playful curious nature surroundings implementation correctly upon final effectively multiple,positive
either try fix addition think force push open new clean hard read thanks,positive
hi thanks add performance section would like check new alignment,positive
ah sadly chance look back luckily ill next week gon na add list complete reach help need,negative
hi sure able finish soon open someone else help finish also try work much,positive
get thanks reply suggestion,positive
hi version first input prompt image image difference two two note query well trained according,positive
hi thanks opening issue default file longer saved default format save file instead explicitly use argument calling,positive
ran following example code got gibberish issue still python import pipeline pipeline cat yea think ended making new anaconda environment set old main python could access,positive
please try pip install alternative try agree believe issue file system,neutral
hi version please feel free realign also create performance section like,positive
try either pip install reinstall git line version fix,neutral
return true uncommented removed also kernel cache notebook good,positive
hey assign issue feel free open link issue,positive
thank investigating everything properly installation stable version,neutral
hi thanks investigating tell following snippet python import print print strange working one following available maybe issue check,positive
translation whose section documentation got impression file open,neutral
think running issue file line file line train return file line file line file line file line file line file line raise loading error loading missing key necessary load resume tested top main,positive
interest reason lower version accelerate quite simple pinned issue bumping transformer package pinning package usual scenario production also torch lead accelerate poetry like bit unclear decision fix two side minimum version displayed message going forward python raise trainer accelerate please run pip install torch pip install accelerate would guarantee dependency resolver line code min version would redundant,negative
hi want work issue could please assign,neutral
hi thanks opening writing detailed description indeed bug correctly evaluate true however version bug longer wo resolve issue interest reason lower version accelerate,positive
available read thanks interest feature work standard check done bit meanwhile need update documentation,positive
answer meaningful thanks quick reply,positive
hey thanks opening issue clear line model seamless communication supposed integration making sure two may well outdated specific certain model regarding could provide model testing script replicate fact different regarding issue make sure case none actually never need add unnecessary complexity regarding issue parameter simply skip layer norm however name might somewhere else case enough might generalize thanks,positive
hi error version run however looking day ago able resolve running dev version source pip install,positive
add maintain integration raised issue relevant,positive
thanks many good could take one last look also could review normally wait approval opinion approaching rather speed process,positive
great work available read,positive
hi thanks raising issue reason officially way load method,positive
mine also work probably something wrong,negative
install issue unluckily still work image error message still image mine work fine idea leave notebook,positive
hi admire thanks raising issue question best try reserve feature bug,positive
hi thanks raising issue indeed logging line reflect default safe serialization behaviour would like open update logger message way get contribution spotting sure fix,positive
install issue unluckily still work image error message still image,neutral
got small layer layer scratch would also recommend starting positional layer scratch small,negative
sure understand small python import import model sentence properly model print changer model print tensor tensor could open new issue elaborate,positive
would able merge resolve also make sure use source wo included next release pip install upgrade,positive
thanks failing pad found pad however removal seem resolved,positive
ran following example code got gibberish issue still python import pipeline pipeline cat,neutral
hi thanks question would probably best reach question implementation match author hazard guess would think mistake implementation indexing entry point let u know say perhaps update implementation suggest close issue feel free reopen hear back,positive
show scan dependency show impact,neutral
also meet loss tensor loss constant grad,neutral
train new model trained scratch learning new token literally away one trained,positive
need iterate audio processor model sec long inviting read related get,negative
inference time hardware version well use float inference,neutral
following code fixed python question rewrite return,positive
thanks work issue update status day,positive
ah similar issue make help think otherwise,neutral
think would best way modify state avoid individual currently list expert since effectively class expert actually forward method,positive
resolved scope throwing error notebook ran fine afterwards,positive
running notebook time rather fresh environment therefore new installation python get exact error loading model optimum pip install optimum library pip install could way presence checked,positive
use configuration error create without file may something version accelerate version accelerate version please,neutral
hi particular use case somewhat related quest context instead directly vector output model also learnable context conditional model model perspective think train list form note output model also involved one naive approach feed vector directly context vector assume match work could test soon better solution,positive
think issue first add end data model parameter good might parameter matter set true false final output keep good parameter issue think due way handling data method add end text manually add end text people reference meet problem,positive
understand wo typically use decode method still quite odd behaviour,negative
triggered update downstream concerned extremely limited protect pickle loading really unsure critical report good done nonetheless thanks,positive
running similar problem make difference number across pas list test setting almost looking implementation set sentence last label stop accumulator batch full sure mean effect,positive
ready go finally build,positive
ah yes please could open another separate skip retain grad,neutral
see going failure explicit skip let know add skip branch prefer different,negative
hi working internally happy try one basically want attempt suggest model seeing replicate text class like quite easy take look file copied new model want add probably several use standard file could copied barely enable happy support want try looking around making,positive
code issue python extra dimension probably come way data collator,neutral
see thanks explaining sorry meant flash attention wonder weird interaction flash attention oh see take look without flash attention,negative
see thanks explaining sorry meant flash attention wonder weird interaction flash attention,negative
hi thanks issue think training faster observe without fa hi say training become faster thought save memory cost training said found save memory slow training weird fa,negative
hi model size image dictionary model defined however image output size always fixed output height width calculated based input example resize image edge image edge match input aspect ratio error version running able run example snippet without error recent version,positive
odd test even running explicitly local branch see skip condition,negative
fix problem work case,neutral
yeah merge code correct revision convert open hub time modeling code,neutral
work understood work great add new else forward ca keep model per philosophy alright,positive
hi thanks raising issue indeed result two saving image pipeline reason removing super call,positive
finally chose following solution python pad model resize note version may make size divisible add special return load right none pad resize input token matrix model reference reference work well side best th,positive
take array set false special pipeline object already model loaded,negative
someone know blocked bias support flash attention yes need attention want exactly made fa custom implementation patch implementation quite trivial,positive
latest commit main think still,positive
yep flaky test patch skip testing suite recently main prevent affecting unrelated like one could rebase include recent trigger new run,positive
thanks unified triggered unrelated failure someone take look test indeed unrelated object attribute,negative
thanks exactly either fit extending scope leverage potential use regarding conversion either way work example merge merge new conversion script merge conversion script directly convert current code new structure,positive
passing quality going add slow test corresponding bad output previous commit regress,negative
thanks reply think could take indeed already issue give try,positive
hey still getting split thing,neutral
python import import torch pipeline pipe simple resize overwrite content better work give array loop set since going add special pipeline object line pointer physical file object memory save disk,positive
supposed fix show snippet code work already supposed read environment variable see,neutral
idea u model phi phi single conversion great mind believe yes phi modeling file guess add phi regarding current conversion script create separate address included,positive
well though think better add processor common testing avoid situation wo able fix future new processor file saved change draft,positive
please upgrade problem training multiple multiple perhaps solve problem either already check presence yes also problem even though,neutral
ever introduce think exactly auto class instead abstraction one level explicit class auto allow load work explicit class allow load specific model type work clip processor well allow give used like use back question guessing wo issue use model specific construct ever allow use behave like hood still try find actual type class use type file construct object type see issue regarding change,positive
see exposed first thought talking exposed also clear make exposed would suggest comment say used making clear exposed case want table,positive
class clip family like change merge unless,neutral
yes said text component used vision component used idea expose need model could add check make sure done sub,positive
think could given flexibility cache attention layer well side ready review label good difficult issue,positive
feel free ping review whenever ready applicable model class made please check logic class selection big first message,positive
added support attention mask inside converter alright yeah check related,neutral
default generate method also beam search think big difference convenient use generate rather compute mask position mask yes think right use version difference two although framework task blank generation ability,positive
hi thanks lot suggestion tried approach pad beginning model forward call implementation ended bloating modeling code therefore decided move forward approach maybe revisit,positive
important used class use argument method use function mask generation handful model class meet criterion class multimodal may require use pure language class fit use mask,positive
please upgrade problem training multiple multiple perhaps solve problem either already check presence,neutral
think found issue check see version date problem trainer model save saving trainer ca train whole model problem fixed check version use update pip install,positive
hi thanks raising issue sure save bias loaded make dummy model following pattern example layer loaded randomly running import torch import class foo self super foo foo set set zip assert else summary summary different note class inherit however tried work even randomly initially set weight bias,positive
hi thanks raising issue recent version compatible could try pip pip install upgrade error message could share full,positive
new test self added,positive
hi yes work code added test thanks suggestion,positive
mean know need add different exposed,negative
added said final dot issue however inner punctuation like house final vocabulary think need add python first,positive
model auto model model none model model model task medical data,neutral
ca guess model use trust remote code,negative
thank feel indexing error get target length index index however index index different need attend,neutral
hi well thank much think issue closed,positive
think great idea given visibility model found,positive
hi meant disk storage yes perhaps issue insufficient space push although previously run lora inference,negative
thanks quick response file see name type null split none valid server maybe error transient different null fix would remove line null method model card know part library fixing problem recently found fully lora expert likely hood since method create commit hub every method always end calling hood real happening witness difference problem usually rather difference method hope clearer record issue underlying library fail early case invalid model card avoid confusion future,positive
upon passing good go think,positive
finally got around doc string tup,neutral
seem generating time generate loop forward loop probably wrap finally multinomial sampling greedy used default generate,neutral
hi one please note file model lora interestingly recently found fully lora,neutral
hi thanks issue think training faster observe without fa,positive
hi could share file trying error happening bad request commit must string server commit model card correct weird transient error given server always model card matter file sometimes,negative
hello thank reply even train batch size equal got error command bash version dev platform python version version version accelerate version accelerate found version true bash display description compatible controller corporation fabricant corporation information bus version configuration description compatible controller corporation fabricant corporation information bus version configuration configuration,positive
added specifically support text component used anywhere exposed,neutral
could added tip nit done,neutral
hi thanks investigating let know facing issue german split batch size log extract python file line run batch file line output trainer file line output file line return file line return file line file line generate file line return file line generate return file line self file line return file line return file line forward error assert triggered compile enable fix issue though,positive
hey thanks opening issue related actual class rather way work merge individual based learned case part merge example python,positive
oh found mistake argument documentation fixed thanks,positive
feel free ping review whenever ready,positive
hey thanks opening issue try keep related way model trained could ask question forum instead sure community help thanks really valuable would probably make sure remove might appear lot count make sure properly add input fed model true,positive
draft already think would help u follow progress potentially help,neutral
yep sure already make sure rebase main,positive
got issue even anyone resolved issue version worked well without,neutral
thank however issue author,neutral
true seem consistent space left device model size expect let say currently used memory sure additional increase memory model mean graphic memory mean computer memory disk storage,positive
extended hidden hence first layer stay first past cache remain thanks think example batch size one pad reproduce error padding token input manually though import import image import torch import prompt user image image model processor device processor prompt device add padding token manually output since image padding token first position fail first time create try index happen without padding likelihood higher longer likely run experiment see float much frequently float import torch import alt import import function run experiment range running original example return running visualization length float float plotting chart zero array length chart image practical think maybe elegant way identify logic attention mask still issue though case real padding batch,positive
thanks working might able add backwards compatibility compatibility rest otherwise good hi also modification resolved several obvious however still conversation misalignment think one comment still fixed distribute part,positive
test passing incorporate work reason manually reflect latest still since change already main,positive
hi thanks reply transient provided snippet working smoothly day ago first time see error although work cluster,positive
time finish thesis welcome try figure tue wrote hello status fix look still get warning message reply directly view id,positive
hello status fix look still get warning message,neutral
thank attention attention applied false attention mask flash attention compatible compatibility rather code original mask mask coming function description avoid confusion necessarily agree original mask may also documentation make hopefully responsible verify,positive
check file got say know work like text think place put entry,neutral
hi thanks raising issue indeed logging line reflect default safe serialization behaviour would like open update logger message way get contribution spotting,positive
would possible create small dummy model two linear one set explicit resize test try handle complex layer naming state yes could done similar test new one added model class independent probably move new test class rather inside let update suggestion leave moving test follow,negative
support handled optimum library model found,positive
yes run script many like llama case issue,positive
hi update script readiness review already good let u know need help library,positive
know ever modify log verbosity would enough set everything ratio quite low believe,neutral
like brittle fine situation intention user ambiguous deliberately set still would unexpected behaviour warning would show much agreed since perfect solution issue original design core accept change let least handling block easy figure thing someone get trapped issue,positive
hi update progress model think able finish soon model like library soon possible something time would open someone help finish making sure course still get contribution already done large part,positive
thanks issue delete issue one close original marked done well thanks,positive
error torch model model output printed comment torch output,neutral
could try small model fit many time memory see whether still see large explosion memory footprint,positive
thanks pointing could give first review,positive
section lot information used make,neutral
let know anything help,neutral
hi thanks raising issue say tried batch size tested able run batch size python running small model python size used machine,positive
yes yes actually going post entire long hence thought would better post reference entire recent call last self item try return item except raise handling exception another exception recent call last module print image print print image function call forward return input call used forward converted shape batch function call forward return input call used forward forward self optional function call forward return input call used forward forward self height width raise make sure channel dimension match try return item except raise self return data,positive
hi thanks raising issue issue transient seen multiple time hub master crown,positive
nope completely right forgot class processor part future plan potentially modality specific make explicit handled guessing wo issue like auto class use model specific construct,positive
still get issue saving latest version even dev used three one two tried saved storage storage still got error file directory file line train return file line model trial epoch file line model trial file line file directory although already,positive
error happening ca call tensor graph mode need use code look like two different raised key error attribute error next time please make sure text instead,positive
function description avoid confusion necessarily,neutral
would possible create small dummy model two linear one set explicit resize test try handle complex layer naming state,negative
applied false attention mask flash attention compatible,negative
fixed error think new one file import import import import image image image print image print print image error getting,positive
provide fix give u headache,neutral
probably missing replace normalizer something like python import import true first make sure use,positive
hi share fully reproducible snippet capacity generate time issue running code base forked chat step sorry provide information,negative
ah sorry blame yeah previous commit well,negative
thanks like link work safari like link somehow giving check team see wrong,negative
present even see line come form tested preceding commit,neutral
hi modeling file hub conversion script working properly try fix get running tomorrow,neutral
note following due import torch import import class model forward self return model model model model model,negative
error running stop server run start server fixed,positive
work vocabulary correctly however punctuation bash test test,neutral
verge weight bit delay schedule last bit weight link writing integration working hard,negative
tried model model could loaded mean something abnormal model tried override script could load model nonsense besides tried downgrade everything fine model loaded normal,positive
hi everyone fix issue,neutral
hello state loaded loaded rank avoid excessive memory usage loading model meta materialization code example ca access private model import import import stage device device false true false false true false true model model launch command time result warning warning warning setting environment variable process default avoid system please tune variable optimal performance application warning setting auto detect setting auto detect removed use instead removed use instead module removed future version please import directly module removed future version please import directly finished model loading loading real user loaded sharded across model behaviour,negative
research found except rank tried load model weight meta device like behavior extremely slow,negative
enate trying understand status estimation code change see contribute,neutral
good point lot sense thank waiting believe let double check talking process call method particular text important however saving file processor argument processor miss anything message,positive
weight matrix need randomly good point think model always identical could seen inside exposed way testing would say identical still implement bit worried also edge skip change actual weight loading intended loaded loaded way problem must something wrong,positive
bit late might help someone despite static contextual still gave reasonable model device word model encode word get token word convert token tensor move model device forward pas model model retrieve hidden model output first element hidden sequence length return model model model model print print king print man print woman print queen import similarity print cosine similarity gave king tensor man tensor woman tensor queen tensor cosine similarity regarding question import import torch import import iterate entire vocabulary word model word return already dictionary return word encode word get token word convert token tensor move model device forward pas model model retrieve hidden model output first element hidden store new dictionary word return word vocabulary word model keep track top min remove current similarity min word sort top similarity sorted zip print gave reasonable result nearest vocabulary,positive
hi amy right similar solution save portion however afraid issue could arise load unless parameter set false pull request resume true false magically,negative
best knowledge many almost hub attention bias suite family polyglot public yet training undergoing want release model hub trained architecture without attention bias check configure model hub see note public gated user access approve automatically available related closed,positive
great huge improvement threshold part already code maybe bounding box might disappear visualization check part,positive
thanks work wondering hold saving processor otherwise going introduce maintain backward compatibility would blocking anything else,positive
hi tried branch balloon notebook initially look good set loading model image thing still weird bounding shown confidence even though need look already awesome training lot thanks,positive
able find related forum alternatively ask discord,positive
hi thanks raising issue question best try reserve feature bug difference see learning rate step different due learning rate difference number different likely due randomness would suggest running seed see observe loss step running see different loss across,positive
also ran transcription flax whisper demonstrate wer equivalence torch implementation huge get overall transcription time,positive
given increasing usage whisper model speech recognition model library propose improve time whisper library since involved take time,neutral
tried source resolve issue,neutral
hey tortoise pipeline composed two one diffusion one designed work mix whereas designed work single model therefore pipeline better fit add note pipeline still easy work similar,positive
hello yes remove want open go ahead thanks lot,positive
thanks want close submit one hi discus team come back thanks,positive
hi strange lead try also hardware sure train padding use library make sure pack input,positive
error related provided fix think image image,neutral
hello please give minimal reproducer issue first meant resolve sorry bug related company project since code proprietary share despite build minimal example find simple way achieve error running loss computation line without provided fix line report error zero without fix,negative
issue framework generate response face error also problem long time train reinforcement learning code,negative
think quick fix would disable normalizer use instead python import import true first,positive
would data whether garbage cleaner explicitly otherwise sure,positive
default generate method also beam search think big difference convenient use generate rather compute mask position mask,neutral
get chance review code thanks advance,positive
yes user may aware calling task pipeline could user task user want use acceleration task see boost,positive
use validation rather test,neutral
current status saw added progress happy contribute though need help,positive
think interesting idea want consider binary matching generate probability vector corresponding different text example different would pas list image airplane image automobile image truck furthermore would entry code snippet output correspond image two sleeping couch much general image two dog sleeping couch,positive
main idea may know pipeline integrate pipeline decide function need extra pas help apply torch compile avoid useless easily example pipeline use otherwise think wo check pipeline code detail best way define model function need pipeline important clarify model wo work model generation still use original forward function inference,positive
hello thank think better solution would following check yes solution better think avoid training,positive
necessarily code automatically enable properly one important aspect training heavily tested could make sure set printing sure checked found printing also showing training progress bar incompatible gradient setting unfortunately memory saving,positive
confirm able successfully run script description recent python yes well exact script,positive
thanks want close submit one,positive
hi normally assign people saying working open hub directly linking comment related work case welcome tackle,positive
someone know blocked bias support flash attention,neutral
thanks anyone add fa someone know blocked bias support flash attention,positive
yeah found originally tried total disk space plausible space saved another copy model space enough yet still got error adapter model would thought portion longer relevant point,positive
hi try find example error thrown yes would really great thanks,positive
thanks example would also agree something length output certain image sure anything input length three tried different prompt length much answer use one image per prompt use long somehow reproduce image find yes used one image per prompt three different used different prompt biggest one try find example error thrown,positive
ah nice know checked like problem still zero related care zero able work around issue thanks pointing right direction python import yield else yield note work accelerate launch use accelerate launch handling little bit different would change would nice something like library,positive
hi thanks raising issue could provide minimal code snippet use reproduce error,positive
saw issue however personally think would great unified way run transformer pipeline support right harder use also would make easier switch without significant code,positive
investigation main hypothesis memory two part keep track attention something bypass easily something case memory usage higher mostly computation introduce memory leakage sure would best way deal issue tried cause memory still,positive
thanks reproducer try run end also appear first place extended hidden hence first layer stay first past cache remain,positive
hi comment longer really relevant,positive
hi thank much understood correctly think might trick consider snippet python import image import import model processor image text image two sleeping couch processor model print tensor label match tensor size first label case image text second label image image gave score prompt image two sleeping couch score prompt image two dog sleeping couch,positive
reason work slow input ignore equal want fix logic feel free think really lot let keep issue open otherwise,positive
hey high priority issue python update logic data processor work well point fed input,positive
hey saw safe use,positive
hi thanks update would still love see code snippet run easily action address following could show u example see name without change,positive
thanks update update see,positive
hey would like open,neutral
update script work box,neutral
hi thank issue used probably wo merge fix sorry,negative
chat whether keep two sampling context agreed leave perhaps abstract different way accept block conservative new candidate end unused squash method except token selection like sample mode method file several private exclusively used one generation method,positive
also got difference used encode difference use method series import import range row row print difference sum difference row row print difference sum difference method difference method difference,neutral
true seem consistent space left device model size expect let say currently used memory sure additional increase memory model,positive
hi thanks raising issue recent main resolve training setting soon part patch release could try source see resolved issue,positive
hi thanks opening let u know ready review,positive
yes however would stop importable top level would breaking change instead make exception doc effectively,positive
really affect inference default used feel free use token common practice,positive
hey facing similar issue appear long enough hence different behaviour different one way replicate import import image import torch import describe image prompt user image assistant model processor processor prompt print output print print output running current main python unable register factory register factory one already registered unable register factory register factory one already registered unable register factory register factory one already registered binary use available enable following rebuild appropriate compiler loading special added vocabulary make sure associated word trained applied issue install triggered internally return input weight bias added extended attention mask attention mask target batch index tensor non tensor operator block thread assertion index index size index recent call last file line module output file line return file line generate return file line self file line return file line forward error assert triggered compile enable understand correctly resulting instead also appear first place,positive
set token id sorry mean setting actually id token llama would affect inference time result,negative
quick one small like get upsetting,positive
hi thanks feature request model code hub request add fa support done discussion page model update message end route correct place,positive
hello way use torch compile trainer please let u know issue python inductor refer information,neutral
hi one recommend taking look optimum library export optimization like probably reduce size model quantization,positive
stale please quite close ready let know need help last bit,negative
hello try let u know issue already context manager zero main fill check model fit model model auto raise,positive
hi please follow issue template provide information running environment run terminal output error stack trace minimal code snippet reproduce issue,negative
error message space left device enough space device make sur read lot,neutral
thanks lot fork largely inspired original mamba mostly code hesitate start upstream linter couple dead code upstream remain fork keep eye,positive
hi move model load auto splitting model across available either model model think received warning log,positive
hey share exact sure thing recent call last cell line train self trial else return self trial model model self model loss model self model else none model save past state need fixed made cleaner later self return type ignore else return self self return try forward forward return act like decorator self self return self return decorator script mode type ignore forward self none convert self return type ignore else return self self return try forward self mask length calculated via length past many unpack,positive
unless strongly oppose like keep two sampling written think beneficial long run otherwise whole new generation method written,positive
happy take look issue,positive
also note template usually use prefix space sequence even,negative
hey make sure open new issue proper reproducer want help,positive
introduce generation abstracted candidate generation part generation load candidate way load generation call candidate generator get candidate may may contain associated depending method technique thus added new candidate generator candidate may added way experiment concept needing speculative limiting constraint rather keep two sampling,positive
confirm able successfully run script description recent python,positive
got error even last transformer version pip install command got following error got unexpected argument resolve,positive
thanks opening issue given sensitivity model team take look fork add,positive
hi ca fix issue since unable reproduce error willing give minimal reproducer help many thanks kind warm code log python import model text replace text like text output model print output recent call last file line module output model file line return input file line output file line forward file line return input file line output file line forward file line return weight input sparse device found least two argument argument index method,positive
hey post library instead,neutral
two sampling new necessarily speculative need ca applied method think put speculative method,positive
hi great hear like use blip model get text output example gave list text image cat image dog gave image cat would expect high image cat lower image dog pretty much gave class spin please forgive sound like could understand could use output variable per achieve could please advice,positive
hi ca fix issue since unable reproduce error willing give minimal reproducer help,negative
yep clue either always done recurring issue,neutral
want merge need run something like git pull upstream main push,positive
also note template usually use prefix space sequence,negative
thanks post forum similar content already exist let create new post,positive
indeed different issue also come piece code see quick fix thanks,positive
help share reproducible snippet way part code work trigger error past key thus probably issue,negative
confirmed fused module working properly thank much kind attention,positive
running script may reveal error,neutral
course create tensor different size pad context,neutral
thanks intended default behaviour think sense disable cleaning model get result completely figure want cleanup thinking,positive
hi thanks much issue problem huge confusion around class possible make class compatible hub involve many breaking whereas also python exist class unfortunately designed designed first place confused near future remove class deprecate intent retrieve text vision blip believe still use without problem,positive
hey attribute python looking,neutral
make sure running latest version ca reproduce python import,positive
problem seem completely resolved try error still file line module file line return file line file line super file line token token token file line token token token type,positive
must fail see might behaviour also,negative
also get like tried though also get result instead last step,neutral
would want publish post attention use propose include memory efficient beam search example look ahead,neutral
get issue change model size behind,negative
thanks reply already added decorated problem sorry cause thought also thanks anyway,negative
issue release following information,neutral
main code working checked generation try add would really helpful could guide need done spend time tomorrow day yet able figure better way going static plan spend time soon proper might needlessly delay please let know please feel free implement current version prompt really anything better since day first posted,positive
even following step previous comment still incorrect output certain import print test broken import true first print test fixed printed slow legacy set false import slow print test also tested saving loading see problem latest worth fix like hey old incorrect new correct,positive
completion happy help integrate feature,positive
two sampling new necessarily speculative need ca applied,positive
another test case text main fast,positive
right stated previous response special kind added vocabulary want reproduce behaviour add extra space basically added space special however getting opposite incorrect import text hello world legacy text print correct print hello world correct text print correct print hello world incorrect also different related since,positive
thanks feedback check update doc example merge forward hi fix constant loss problem,positive
issue framework generate response face error,neutral
hey thanks looking stated following dynamic padding tutorial code taken added reason want pad want pad function later get dynamic padding would prefer use instead error come added example function supposed work,positive
necessarily code automatically enable properly one important aspect training heavily tested could make sure set printing,positive
could please guidance fix pull request,neutral
please change issue specifically wo closed auto every time,negative
thanks also able test successfully work well,positive
tested code found work nicely repository due robust implementation previously would half time throw error seemingly valid grammar able generate structured output like given prompt many practical processor easy setup find extremely valuable,positive
used official trainer model none none none print training avoid behavior change add train scratch trainer trainer none none none print loaded train metric metric metric mean still need add even use default official trainer,negative
unfortunately super sure suggest contact someone share actual implementation used train clip never happen super satisfactory answer agree writing grind compare likely authoritative implementation know authoritative implementation probably maybe forked make adjustment clip based reading order file clear done hugging face possible convert file proto based order text still iffy maybe get closer implementation used training add library discover either way hugging face implementation versus different differ slightly go wrong end observe clip may familiar text hugging face method method clip trained hence actually well trained bet around time however trying lora text image generation training choose rare far disproportionately appear actually probably observe context lora training time perhaps lora training text going poorly mismatch implementation used community implementation used train large think already know ca tell directly issue real know expert observing buggy behavior nonetheless ticket something else,positive
added argument based device see added support model please let know correct need make amends,neutral
anyone solve next release,neutral
sorry test duplicate sure got failing added everywhere gone still,neutral
regarding super efficient try see taken also library around time unfortunately super sure suggest slow version known slow fast version automatically used,positive
thanks link think python code one used train clip could code used train clip repository used inference code speculate code used train clip ordinary python code extremely slow slow intractably slow use trained people real use accelerated implementation work disk format usually code people use people maintain essentially unmaintained could go implement paper maybe publish maintain maybe write use real data however opposite happening free people deal large whose matter matter run code get output none material none materially measured put uncomfortable position supposing default stance issue real issue delicately tell someone know talking might unsure reasonable person invent clip talk guy anyone maybe find real implementation really material one correctly something material,negative
feel free ping another review,positive
yep thanks able reproduce bit edge case could check want open,positive
thanks ping first review,positive
time tackle yet sorry good difficult issue tag,negative
yes separate deal llama converter,neutral
need pad data need add padding token need set following worked python example return example sentence example sentence padding true need pad well truncation sequence well output format,positive
hi thanks much patience sorry wrong actually fused try bash pip install release able switch,negative
sorry gon na give function use official trainer training script maybe,negative
hey hope following help around feedback past highly doubt real issue,positive
perfect let u know go,positive
thank fix ran got another error since model guess error recent call last cell line else model streamer file type return raise unrecognized configuration class kind model type one file token revision raise pas loading model use inference quantize algorithm please refer quantization none pas loading model use inference quantize algorithm please refer quantization,positive
also struggling error use inside python code import o get away problem work,neutral
thanks feedback check update doc example merge forward,positive
use attention attention mask,neutral
hi script used convert would perhaps great add clip folder general utility script,positive
thanks clarification ping review done,positive
yeah sure idea open everything done making sure model type ask author merge issue regarding modify weight modify split,positive
function really always output constant grad norm,positive
thanks amy pointed show potential alternative help visualize alternative would look like keep draft mode close along time come thanks,positive
issue resolved would like work,neutral
hi would like work issue could please assign,neutral
function really always output constant,positive
thank much helping working fine thanks lot code,positive
try use let know,neutral
tried pas forward new model ca output try could use weekend also someone mistral work well,positive
follow model card model used following code worked bash pip install optimum pip install use import pipeline use different branch change revision example model auto main prompt tell ai prompt helpful respectful honest assistant always answer helpfully possible safe include harmful unethical racist toxic dangerous illegal content please ensure socially unbiased positive nature question make sense factually coherent explain instead something correct know answer question please share false information prompt print generate output print output print pipeline pipe pipeline print pipe,positive
following issue would like ask review pull request,neutral
fix confirm although override base class normalizer made wo fix problem,negative
thanks working long time exciting feature enable many left nothing blocking hi thank left rest review,positive
hi error exception following public would fixed removing following,positive
error used model warning know wo used even though used class default think warning good told exactly look potentially behaviour believe correct resolution would add right yes get value key warning wo given key update file use trouble understanding exactly need change could maybe open,positive
love get early review failing fixed copied statement however job request could painful propagate key review header,neutral
feel free review thanks,positive
related proper place default true model,positive
see proper fix go back old behavior always null training ever want create unless generation certainly explicitly set code huge pitfall old behavior,positive
thanks familiar concern model enough space device good point code first move move back,positive
hi thanks opening think feature want add moment already pas pipeline use control wo many optimization one might make loading model whether quantize keep pipeline simple possible leave additional configuration model outside pipeline let opinion weigh heavily whether added pipeline mine,positive
thanks reply reason failing always none training leading block never whereas always training since model fallback create thanks also explaining sense,positive
mostly sense quite understand issue previous though never compute past training regardless gradient even worked good past generating significant extra memory model forward activation save backward pas recompute rest,positive
use lora work enable nice meant zero clear,positive
hi look consider snippet python import torch import import import import hello world print model auto eager model model range model loss print loss case fix applied happen step step model work perfectly fine case set true model therefore pas logic model create none false late already since set none value pas line well therefore layer note point shape past key script call somehow module forward attention since previous state line leading set raised since match master going hood one torch fix issue making sure dummy case gradient training regime hence false line issue always cache fix worked universal training except patch modeling class class case apply patch well let know anything unclear,negative
think use lora work enable thanks providing niche issue based available niche issue feel like people would rather make use avoid ease wrong problem occur llama,positive
hi amy sorry delay time write test minimal script try soon want investigate script outside main script model custom code need local folder load model main script main model else model run script whatever distributed launcher like accelerate likely bunch time hit issue sure whether issue hardware specific,positive
three triggered calling provided used initialize value provided used initialize value provided used initialize value,neutral
thanks great spend time understand used fail main provide update,positive
hey unfortunately think fix work use different training framework handle activation great understand fix root cause fully usable raw thanks always quick,positive
good revisit hard make sure truly necessary anyone else work around via type ignore return,positive
guess add case change draft,neutral
like documentation wrong could clarify merge correct shape input,negative
thanks another edge case,positive
hello wondering would recommend make everything compatible removing kind present place let try example see latest version,positive
hello understand issue error zero sense prediction stage would get sharded stage would leading memory case would better use would inference,positive
start looking closely accelerate configuration part let configure pas anything want rather blowing many different wait first something workable,positive
full perhaps small reproduction script think still race condition directory existence atomic rename attempt possible already attempt rename catch potential exception also simpler true main process rename rename succeed main process perform training setting file system node rank process try rename race condition,negative
hello lot single issue agreed sorry think interesting would like separate discussion work fine zero static batch size even loss almost theoretically also sense sharded trainable seen community axolotl also use successfully zero indeed work anyway topic issue reason brought zero cause uneven consumption disagree batch size everything stuck,positive
since know trying keep eye,neutral
regression fix fix issue,neutral
regression fix like bug,neutral
hello lot single issue trying bit think problem attribute line whenever would fine model already wrapped previous try leaving none thanks providing niche issue based available,positive
full perhaps small reproduction script think still race condition directory existence atomic rename attempt possible already attempt rename catch potential exception also simpler true main process rename rename succeed main process perform,positive
want use unfortunately broke quite deeply latest broken last minor bear u fix time task support wait next version unless could add backward compatibility,negative
would probably good disable double quantization maybe raise error quantize worse model end yes already handled meant following model merge lora base model push save final float quantize model,negative
running slow two failing likely related current output deep dive let wait,negative
would probably good disable double quantization maybe raise error quantize worse model end,positive
alright flakiness investigating flaky failure speech none probability,negative
function parameter bool true logger file name constant used situation bool false ambiguousness,negative
believe taking care issue,neutral
think would make sense patch release include currently perform fused either need compile source use,neutral
hello limited even normal training say model sharded wo hub present,positive
much better simply run script need provide thanks instant reply love see run code efficient way following find resolve issue please keep posted,positive
thanks guess issue feel free close,positive
hi thanks lot issue fact script slows inference price memory efficient since linear twice disable double quant faster sure faster depend problem setup batch size want fast model deployment would advise quantize model algorithm use fused generation faster native also consider model support fused yet read quantization scheme use let know face issue,positive
hi thanks clear license part see discussion problem regarding license modify transfer profile addition later finish transferring code update official one,positive
also related sure inside seem break unable create tensor probably activate truncation padding length perhaps label case excessive type list type,negative
hi thank interest list outdated clip might likely longer require find anything need update go ahead,negative
hi thanks raising issue yes possible convert clip equivalent script without access possible reproduce issue side error like problem serialization original error raised pickle library trying load state need make sure load original ensure script used,positive
apologize closed issue anybody need feature want briefly mention code train training method,negative
taking look thanks taking time describe issue code working though,positive
work clip model one working,neutral
great thanks explaining providing relevant link good merge,positive
per understanding would applied went recent cache impacted since llama mistral hood part recent would say keep mind future copy llama mistral hood,neutral
sense would happy address let go ahead adjust type tell pas multiple tip evaluate please would require slightly evaluate method,positive
hey thanks script fix,positive
hey theory training translate text audio since classic classic objective improve training model randomly skip speech block thus none attention break training,negative
hi thanks raising issue could please provide information running environment run terminal output script run please note research actively may longer compatible recent able reproduce issue loading possible transient issue hub could try running code able run import train,positive
hi first thank review feedback believe raised resolved currently let comment added reconversion back lot copied added think change feel confused needing freeze work expand also made character change remove deprecation warning let know,negative
hi could sense yes know proven decreasing number lead performance degradation also wonder make implementation go philosophy,neutral
much better simply run script need provide thanks instant reply love see run code efficient way following,positive
much better simply run script need provide,positive
hi thanks issue think try tried getting error usage error following exception use see full exit use warn exit use,positive
hi thanks issue think try,positive
think faster designed reduce memory computation time,neutral
regression yes think patch release also,neutral
removal modeling say yes never used whether think ever added future let check mistral team added also think removed,neutral
package clip found original implementation,positive
would sense patch release include regression issue able train usual attention,positive
thanks fixing training happen model fail none could open issue track training either exception made possible probably,negative
sure added phi feel free take,positive
thank fix yet way access,neutral
hi please help way far tell architecture phi library slightly bigger need convert phi script transfer also maybe add integration test make sure let know could help,positive
thanks far know new cache fixed,positive
yes thanks attention help,positive
hi issue fixed please see comment try main pip install,positive
hi thanks indeed made,positive
stale done elsewhere since,negative
eta probably end next week got ta finish add mamba phi quite fast,positive
hi thanks much believe one issue maybe would try main,positive
great hear work already approximate expect get added,positive
problem system want falcon library set loading model getting error help information official example officially task folder task give reproduction device found least two argument argument weight method behavior want falcon library set loading model getting error,negative
believe correct resolution would add right yes get value key warning wo given key update file use open hub used clip avoid warning,positive
audio file audio noisy conversion know thing thing found search also shown original good idea done,positive
thanks well worked cache let keep mind,positive
phi well yes author interested given community interest good way go anyways,positive
thanks lot fix well,positive
loss made used model merge batch sequence length,neutral
could help review thanks,positive
trying mistral get following error attention mask size tried true still get error version version version code float model auto true true cosine reduction memory decrease speed true none trainer text,positive
help without proper reproducer script hopefully work,neutral
sorry delay context little added information summary added test please let know need,negative
hi thanks issue interest feature currently broken fixed issue side release yet fix issue either downgrade pip install install main branch,positive
also tried setting seem respect,neutral
also even use flash attention without torch might lead unexpected behaviour feel like missing something fundamental work,negative
also could someone help land could resolve issue,neutral
also thought even used maybe though,neutral
hello remember seeing warning likely right bit heuristic ca better wo look depth either bug report simple good enough general moving suitable thanks answer wo fix,positive
hey based inspection seem architecturally similar slightly bigger phi see assume need update good go something could potentially help,positive
ah able try well reproducer still broken side,positive
worked fine llama mistral weirdly release memory one rank moment trainer stuck completely mib mib like one rank went decided adjust batch lower rank use help proper fix setup like accelerate post trying bit edit reading code enable auto batch size finder wonder sync agree batch size edit seem like batch size correctly set sure get correctly,negative
address format least one coming error used model warning know wo used even though used class default think warning good told exactly look potentially behaviour believe correct resolution would add right would note fix sufficient information like see,positive
checked yet architecture need change update file good go,positive
thanks lot set model somewhere branch ca find correct fixed slow pas review,neutral
thanks prompt response sense keep lookout necessary hub like work nicely thanks awesome work support phi curiosity add support well,positive
could share minimal reproducer main parser print print print train train false false try model false else true except print incomplete retry model main training script training command line epoch batch batch seed seed seed tee,negative
hi already hub transferring necessary need wait someone merge use without passing,neutral
hey curious choose please point source work see documentation made best guess,positive
whole word single word work know hugging face respective buggy file trained implementation big picture implementation actually correct would used original work way way u change try reproduce original work way clip one used training set clip trained know original work probably one issue actually training clip stable diffusion strong text stability perform worse used enthusiast training match used stability,positive
hi still code hub feature want use version need use converted think transfer suffix phi integration also open hub original,positive
hi thanks raising issue three issue raised error message argument used generation second transcription behaviour best answer way treat long audio final point configure model behaviour inference defer,positive
hi would able help review thanks,positive
thanks fix quick question used see error index either device indexed tensor indexed tensor must index tried fix instead model work find model problem thanks also assumption input wrong input come clearly input moving device also ran script first fine start something training done,positive
sure current loss function convenience none float auxiliary load balancing loss switch transformer see switch transformer function loss function paper routing unbalanced union gate shape optional number auxiliary none return cat along cast expert index otherwise fail given token determine given expert cast float otherwise mean fail return example range shape error recent call last file cell line range file cell line return size tensor must match size tensor dimension,negative
thanks fix quick question used see error index either device indexed tensor indexed tensor must index tried fix instead model work find model problem,positive
hi thanks raising feature request input different size making may happen depending processor configuration map alongside great way process image admittedly unfortunately slow principle purpose make easy possible go image fed model user quickly test get prediction part reason engineering image library many great already exist notice training vision use reason,positive
one overall mistral flax implementation,neutral
hard say would depend exactly loading initialize model via explicitly call build method get slightly different weight would every case initialize implicitly call build via calling model difference think notice issue would need load model explicitly build build script use load name model think pretty unlikely anyone even fix instead,negative
thank quick response know cache tried specify cache python step command worked well one python use pipeline helper import pipeline pipe pipeline later without problem problem anyway would good idea able verbose output see problem package,positive
input necessary homogenous unlike tensor batch dimension along format definitely use batch feature task see,neutral
somewhat related conversational pipeline seem correct brought issue signature list single conversation list list batch although list conversation according list also conversation finally compatibility pipeline call input generator also,negative
hi check produced trainer training make sure contain thanks fast reply,positive
hi check produced trainer training make sure contain,positive
make sure behavior model training thanks,positive
sure understand model generate negative token output model determined always positive possibly,positive
seem wrong whole word single word work original work way way u change try reproduce issue actually training clip stable diffusion strong,positive
getting overflow error similar error original issue thread indexing error like block thread assertion block thread assertion block thread assertion,positive
line error within none error generation fine tried making model problem still internally generation first comment generation,positive
getting range integral type conversion generate inspection model set fixed solution,positive
sorry pad token id vicuna specific would recommend however really understand issue obtain,negative
one thing certain seem properly part trainer logic handle moment,positive
help without proper reproducer,neutral
hey ca reproduce yet local loading script python load local python train range yield could find script,neutral
hi share fully reproducible snippet,neutral
attribute model print object attribute explicitly set said fix problem show difference token model like print print print print would recommend try think solve problem pointed,neutral
good catch even fix along provided work thanks,positive
set want pad token used overall negative indexing never good solution would recommend set,negative
llama mistral use mask provided otherwise nan arise mask llama always related attention mask overflow sure code looking,positive
bit concerned effectively patch inside add backwards compatibility already handled main question whether fa need check still possible pas clear handled model final review recent might missing something also clear passing would change whether pas,positive
got solution think better another gon na merge one see nightly work next phase compatibility,positive
thank quick reaction problem gone hope green side looking forward merge,positive
least know immediate regarding,negative
thanks think know culprit indeed triggered looking detective,positive
sure approach correct good took care attention,positive
thanks indeed possible therefore regression made resolve problem,positive
would better posted rather believe,positive
hi thanks raising issue unable reproduce side like partially cache setting cache path model found could check see first try running pipeline small model quickly see work import pipeline pipe pipeline work could also try loading model outside pipeline import model,negative
official site ampere hopper support coming soon please use nothing ca unless run different machine,neutral
hi could open new issue problem running environment linking issue related,positive
thanks wish modify model loading setting wonder could pas,positive
hi thanks raising issue taking time write really u indeed seem like desired behaviour linked issue known issue blip model convert update progress hub compatible used instead code,positive
thanks suggestion merge green,neutral
hi thanks lot issue think pas elaborate bit would like pas well canonical way load fa model python import torch import model also enable fa directly however enable fa passing,positive
indeed address well done,neutral
rerun slow confirm passing ready review,negative
hi thank much push forward thanks also providing detailed description new problem fix go accelerate suggest wait get feedback accelerate see approach case merge together accelerate tell install accelerate source face similar issue also meanwhile confirm problem,positive
thank much confirm training work,positive
problem vicuna model specifically specific indexing inference tried bit diagnose even making work line none issue revert think generation correct sample tensor indexing return weight input sparse help looking get root cause,neutral
llama unaffected hand instead mistral falcon probably many alternative solution waiting fixed suggest following line python hi since code sure exactly put raise attention mask size get work still got nan sure right place,positive
case merge edit note reason failing confirmed independent,positive
observing exact behavior th st layer llama update issue still use update investigation found nan come said variance variance next line variance variance turn multiply produce nan lastly nan spread attention result lot nan last hidden add clamp forward let know better solution hi like ask add clamp added clamp still get nan,positive
exist try load model model could print model well,neutral
hi take look issue ask necessary anyone taken already,neutral
ca merge due failing test also failing main would able merge,positive
issue saving model error loading missing key module bias module weight module bias module weight unexpected key bias weight bias weight case model module draft quick wish fix found unwrap model work know module model model potential used distributed training model model since could multiple wrapping unwrap model module return else return model model model fix,positive
saw issue except onwards bigger index could present filtering made sense avoid hack see either root problem,neutral
hi thanks raising issue could share running environment run terminal output,positive
hi agree quite hacky let take time investigate provide proper fix,neutral
see available going hugging face search name filter name,positive
could share minimal reproducer,negative
read issue see simply could read issue,neutral
sorry could either show issue detail problem computation different output shape also different routing also different,negative
glad see issue fixed,positive
confirm experience since work,neutral
distributed error single device following script edit fail use sliding window import torch import foo bar model auto role user content summarize following print print,negative
failing test also failing daily unrelated depend generation ca reproduce end,neutral
thanks giving opinion ready review test specific new feature far common processor test file yet current set detect however,positive
indeed like issue like snippet work new cache class suspect flash attention may solve issue hi thanks quick response confirm removing flash attention issue however removing long take much memory get,positive
hi thanks issue contrast official implementation decided language model vocabulary size introduce new image token instead fact sufficient decided go performance document let know sense,positive
hi thanks clean reproducer confirm bug,positive
generation rebase one ping one become exclusively speculative,neutral
indeed like issue like snippet work new cache class suspect flash attention may solve issue,positive
facing problem official trainer specifically get exact error training context window sliding window context window le get different error training work,positive
found weird bug accelerate apparent pure string well moment thought done prepared see merge celebrate last minute failing odd error weight meta device need value put especially odd since almost code worked flawlessly similar found initially loaded model missing therefore saved available subsequent loading later stripped dive accelerate found magic line code took wrong branch apparently device device device device come based equal string result torch call note device wrong branch code line happily parameter bit none error much also found issue class name relevant problem apparent switched commit deal pick number compare device accelerate line convert device device along way instance accelerate simplify overall handling additional accelerate move logic make constructor robust would test data phase fail case like ask make device equal device sense time may cause testing consider testing device rest code see device device problem may arise code work slow would pas empty device map hesitate recommend resolved please guidance make accelerate,negative
hi sorry clear version without issue yesterday test getting error mistral execution recent call last file line result await type ignore file line return await scope receive send file line await super scope receive send file line await scope receive send file line raise file line await scope receive file line await scope receive send file line raise file line await scope receive sender file line raise file line await scope receive send file line await scope receive send file line handle await scope receive send file line response await request file line await file line return await file line response time file line output file line return file line file line forward file line file line return file line generate return file line sample self file line return file line return file line output file line forward file line return file line return file line forward file line return file line return file line output file line forward file line return file line return file line output file line forward file line raise cache self access layer index access layer index thanks help,positive
also saw notebook custom code similar notebook distributed training one important fact matching also used backbone default training gave final balloon also remind setting true also important also share final,positive
awesome take look detail next coming day also working notebook custom mainly notebook replace however time look successfully would awesome,positive
also issue training directly resolved image training immediately problem would thank,positive
undecorated work true however based implementation undecorated module import model specification environment python,positive
see help overcome issue,neutral
thanks current solution check condition multiple one found object multiple class self state control pas handled trainer else,positive
class text image thus include,neutral
hi recent release listed beam score calculation section breaking also add section would clear people check relevant breaking change thanks,positive
hi issue please submit issue,neutral
quick replacement implementation running expert forward pas overhead since want deal state overall training faster python class self super self range self size size sort row index column index get column index note sort operation width index input avoid overflow large activation matrix cast constant number every row sparse matrix offset block ordered transposition divide get column index zero zero return topology self assert assert sparse matrix number nonzero dimensionality single expert index sparse matrix index intermediate matrix dynamic depending unused remove need use meta save device memory data meta shape shape data shape return shape data self sort expert produce index permutation index histogram expert identify number expert round token block size used matrix starting position bin calculate bin sorted return index self index permute pad prepare expert computation padding route computation index create sparse matrix topology topo expert expert expert perform expert computation type ignore topo topo permute back remove padding type ignore index return forward self return,positive
maybe check directory first issue also exist empty directory assume mainly seek different directory directory relative empty might return none try check directory addition may use delete empty one one empty directory issue also issue thanks,positive
quick update build proper instead old method working solution,positive
done inside something see need make copy inside,neutral
oh awesome close issue,positive
great thanks let know action side want address failure documentation build,positive
understand unable produce issue code provided think ca input,negative
work thanks quick fix,positive
believe already active however might need little work polished solution yet least think,negative
detailed documentation audio course setting definitely add model next whisper generation,positive
load model flax python import model save flax model load flax model python import model,neutral
would able take look protecting edge case,positive
hi thanks raising issue providing error snippet could also provide information running environment run terminal output,positive
favor change still handle,neutral
would great hesitant much one since main issue base trainer follow possibly part patch look axolotl ensure break would good,positive
give example happening scenario axolotl framework figured process skipping model saving unconditional save state main process directory look contribute failing test would motivate change,positive
thanks providing part recent release mean error message loading different model issue example unable reproduce ask anyone posting issue full many review day help get issue resolved soon possible need help u help partial information ca know trying solve right issue might able reply solution seeing stack trace,positive
seen recently process entering function skip past save would create directory arrive point another process chance create directory give example logic code see need directory instead best guess model put instead option would good u able write test,positive
whether current process write disk save yes find used sparingly saving internal check process,neutral
somewhat aside guarantee previous writer directory point seen recently process entering function skip past save would create directory arrive point another process chance create directory also ever true single process main process misnomer whether current process write disk save scenario multiple participate disk writing directory sorry bug test original change,negative
hi please pip install rerun code fix issue thank much patience flagging,positive
different fix coming work issue rename staging folder happening main process,positive
hi error token length greater sliding window size error version upgrade get error thanks version platform python version version version version true version na flax version na version version script yes distributed parallel script yes recent call last file line result await type ignore file line return await scope receive send file line await super scope receive send file line await scope receive send file line raise file line await scope receive file line await scope receive send file line raise file line await scope receive sender file line raise file line await scope receive send file line await scope receive send file line handle await scope receive send file line response await request file line await file line return await file line response time file line output file line return file line file line forward file line file line return file line generate return file line sample self file line return file line return file line output file line forward file line return file line return file line forward file line return file line return file line output file line forward file line return file line return file line output file line forward file line raise cache self access layer index access layer index,positive
sorry let rebase merge,negative
hey modify manually field saved field saved really support manually way reserved already part overwrite well make sure removed would hard content added trying modify,positive
class script get research example work modify code use epoch instead call resolve,neutral
hi good work curious deal wrong cache token rolling back draft model mask,positive
help need code used reproduce error full error well,positive
incompatibility need find compatible get run related discussion forum people suggest python pip install upgrade,neutral
thanks split two separate one generation speculative,positive
yep getting exception calling layer type argument position must tensor call received layer type,neutral
hi thanks raising issue could share error full well,positive
one tricky coupling data image processor size control logic independent model behaviour image processor wo automatically update necessary model model input image size extracted however model interpolation input image different resolution inference able pas different sized run fine want train model suggest processor model align two need something like python import create new model randomly model height width thank much complete reply looking source code found parameter must width optional know pas rectangular specific model moreover case accept said input size size none size size size false height size width size size size height size width else raise size must contain either try height width true get following error file self image size resample size size size raise size parameter must contain key got image strange since corresponding doc error different raise size must contain either know wrong sorry little bit lost,negative
understand regarding agree consistency pattern important happy revert change ensure contribution code base,positive
understand knowing model saved really handling issue user side one resolved looking model nevertheless extended reason auto class prediction task,positive
thank philosophy code already kind deviate philosophy cache attention mask one indeed argue could attention,positive
great glad hear working thanks following issue confirm,positive
thank much figured reduced model removed unnecessary training working thanks,neutral
thank taking time however something going merge go pattern model code base potentially still expect able use generally harder read yet hard use wish enable setter output add following pattern,negative
tried got issue rank watchdog caught collective operation ran timing timed due asynchronous nature subsequent might run data avoid data inconsistency taking entire process rank watchdog thread exception rank watchdog caught collective operation ran timing terminate throwing instance rank watchdog thread exception rank watchdog caught collective operation ran timing check existence folder like python place final location saving finished work smoothly,negative
hello made code review comment making accidental easier understand use code especially method output way modify important parameter ensure consistency avoid potential enhanced make code robust le prone separating access modification functionality code becomes easier maintain understand setter cleaner concise code logic handling update code readability additional improve performance unnecessary memory setter output future logging,positive
hi thanks raising issue loading model correct way go model error happening though could share minimal code snippet could run error training could also share error message full,positive
thank quick fix load model python import import model eager model like class true error see reference,positive
ever get beam search working monkey patch notebook would interested able get working already especially depth first cache hi thank interest made working implementation beam search mask got high llama far limit complete share gist beam search part code hope find useful,positive
hi following format following link displayed throwing exception detailed issue template close one,positive
hi thanks opening make,positive
issue trying get target via following call target error message one gradient computation operation however trick like model set evaluation mode,neutral
hi thanks raising help tiny random testing,negative
problem problem case problem,neutral
add encode call easily behavior fix also longer add random full sequence,positive
awesome chance really looking forward,positive
think also affect sure positive negative hope positive viewpoint inference pipeline original implementation assigner code written even though may recheck confirm see line addition pipeline default however original code use classification threshold also extra modification make work,positive
hi thanks much could try make green running make locally fixing would time would great look evaluation open object detection high paper,positive
minimum code import torch import class forward self none none none none none none none none none none none none none none else none else none else none else none else none extra input merge text none memory efficient save hidden stated default full else raise unexpected select feature strategy none else case none case generation cache none none retrieve first layer inspect mask hidden set add get target length need attend print output recent call last file line module file line return file line forward error assert triggered kernel might call might incorrect consider passing compile enable,positive
thanks patience ghost fixed,positive
hi thanks support set like code snippet found run model still generate cache even setting python promote table package package already map,positive
thanks tried work check implementation function memory setting none returned similar manual operation solve met problem,positive
also due change major think need rewrite test function,negative
quantize method script easy model model might also want converting inference model find complete usage example hi able use class convert linear layer thereby quantize problem face last layer layer none shall done case issue,positive
hi limited mask code proceeding far monkey patch based negligible difference old new way dent believe rounding error somewhere would support basis put new test hi try monkey patch notebook see work implement idea ever get beam search working monkey patch notebook would interested able get working already especially depth first cache,positive
final change well mail open separate issue model doc listing yet,neutral
may arise hardware thanks time attention,positive
hi intent make bit easier work collection may know exactly model class form example import work known advance one thought might enable three time series support prediction task,positive
could confirm whether computation indeed accurate instance training freshly parameter llama model whereas quite substantial difference scaling update discrepancy currently calculation take account following al recent work notably chinchilla take account layer flop computation,positive
yeah passing directly work thank,positive
need set import torch like import o path path path path path import import torch,neutral
hi thanks opening intended specific task two new moment suggest waiting know time series case,positive
best answer believe safe way correctly track loaded correct applied token id possible define load know behaviour original,positive
appreciate issue update temporal solution fix issue,neutral
hi thanks raising issue research actively error anywhere else library,positive
need fixed insanely fast whisper eta,positive
need use create python environment check hidden directly,negative
hi thanks raising issue version running environment,positive
one tricky coupling data image processor size control logic independent model behaviour image processor wo automatically update necessary model model input image size extracted however model interpolation input image different resolution inference able pas different sized run fine want train model suggest processor model align two need something like import create new model randomly model height width,positive
unable reproduce issue fresh copy fresh environment dependency related,positive
hey could confirm latest version library pip install upgrade,positive
still issue easy reproduce python import import processor model processor range range model recent call last file line module model file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line file line none else size tensor must match size tensor dimension,positive
really nice certain part need look,positive
set speculative method assuming familiar feel free delegate someone else familiar method,positive
glad know cause many extra,positive
hi thanks raising issue question best try reserve feature bug learn modify check documentation example add vocabulary method thanks reply know function want change vocabulary size introduce process like resize layer wondering whether method replace,positive
none moment far know one attention figure time used make particular prediction,positive
hi thanks raising issue likely version environment contain recent release please make sure follow issue template running environment version error unfortunately super clear merge commit make error clearer,positive
hi thanks raising issue best help please make sure follow issue template provide information running environment full error note widespread support added recent release would first make sure install latest version see issue third party providing full error information enable u figure library error coming,positive
hi confirm severe bug probably solve issue llama classification context rope scaling training model get macro micro evaluation however tried reproduce inference pipeline evaluation random tried training without rope scaling could reproduce evaluation metric also confirmed head saved setting score nothing worked,negative
hi model output defined code hub suggest opening add decorator opening discussion decorator added,neutral
really appreciate hate bother another problem wonder interpret time series feature importance like shap time series forecasting,negative
thank post far one sure bug code maybe understand correctly also post,positive
sorry missing run locally get back solution indeed work yes follow would great,positive
confirmed work system yes detailed description starting sentence setup run test also agree happy make,positive
hi thank much quick reply case meaning parameter model file let say process height width model file keep default parameter would behaviour training thank lot,positive
hi specify size image resize call crop size height width note know example order,neutral
hi thanks raising issue best help could provide issue tried well recent stable release could provide example good versus bad output difference getting good bad version could share code snippet run reproduce running inference see behaviour well model provide model trained training script reproduce lora configuration,positive
thanks support fixed issue,positive
hi thanks raising issue question best try reserve feature bug learn modify check documentation example add vocabulary method,positive
yes splitter still need applied training else dynamic split target check,neutral
figured bad update post mean time use python data transformation data create test instance splitter sample last context window seen training validation apply train mode return,negative
hey might bit harsh review need help finish,negative
stale strong need community take,negative
issue python language token language id,neutral
actual code id sure error message,positive
target save additional trainable part parent preserve parent model theoretically model reproduce still happen curious,negative
thanks python lora make head input layer trainable hence strange behaviour facing sure support merge unload try without thanks wrapper save trained trainer also must saved exist therefore block necessary task see finding thank,positive
already anything wrong image,negative
thanks lora make head input layer trainable hence strange behaviour facing sure support merge unload try without,positive
awesome work thanks everyone ready final review,positive
directory name id contain necessary fail looking inside local directory instead remote model repository,negative
please check comment comment yes make directory,neutral
python import processor appear file available,positive
think behaviour still strange usually language error,negative
fix issue install new backward compatible enough last release backward compatible could confirm work older dependency library standard library user fulfill least standard linked library run error,positive
thanks opening mark good second issue,positive
note available python import import clean validation audio array processor model processor without prompt print grave whether sir leighton work really discover little rocky prompt let change spelling leighton passing prompt print grave whether sir work really discover little rocky looking pretty sure answer looking,positive
train model like export python fix include,neutral
hi found problem model transformer version reproduce code found import torch import model pneumothorax pleural effusion pleural effusion pneumothorax seen extent pleural effusion constant error subclass must use decorator could add missing decorator model also thank much,neutral
facing issue training file line parent directory added annoying front,negative
issue temporarily fixed different staging directory python destination directory already saving proceed saved may invalid else insert,positive
getting error running saving model saving state file saved special file saved rank torch saved rank saving model torch saving torch saved torch saving torch saved zero saved torch ready recent call last file line train return file line model trial epoch file line model trial file line file line open file directory,negative
hi thanks raising issue link work confirmed work thank,positive
feature request add argument determine shifting class argument added training model must check none true none shift predict default false except causal language related trainer motivation current state code shifting training use assume unintended specifically training default code loss inside properly corresponding however train loss inside function trainer part already proceed shift believe whether shift explicitly determined argument another argument like case team training totally different given reason due misalignment turning contribution willing make confirmation want know prediction text like case model learn transformation always predict last input token repeatedly curious,negative
issue temporarily fixed different staging directory python destination directory already saving proceed saved may invalid else,positive
ca replace new line believe equivalent,positive
would possible provide complete code reference kindly,positive
hi added made review let know think also thanks sorry forgot mention last comment,negative
hello lot technical open feedback,neutral
yeah trying print hidden check output error test,negative
well like class trainer self hack fix none return super trial entirely sure correct would even right decision zero least zero work auto batch size finder,positive
cleaner implementation working model like image inside model rather processor class,neutral
problem attribute whenever would fine model already wrapped previous try leaving none get around,positive
thanks one thing left add buildable always going quick replacement become actual extra none impact either way,positive
correct understanding closed issue resolved,negative
without knowing behaviour work error able help note want actually calculate something going spotted printing matrix,positive
hi delay side away past two race condition exactly happening process trying location ultimately want try easiest fix making sure first running script load cache process could share minimal code snippet reproduce error even sporadic well variable set used launch script,positive
actually another parameter source truth clear behaviour user unexpected,positive
breaking change current state since currently last stage index user specify think added matching logic originally last layer added know motivation better backbone logic standard model essentially making leaky need know get hidden loading backbone moreover going break stuff still set model easy rectify would know user last hidden state default forward code harder understand tying logic still stable moment alternative approach would different argument loading backbone,positive
awesome work left couple confirm bit bit slow pas something right accelerate get weight meta device need value put give time watch case need side hope,positive
give hint name defined error apparently decorator import missing ca figure import seem place,negative
thanks answer maybe help initial prompt without pipeline,positive
hi modify script loop call get different generation time import torch import model auto hello name range print removed clarify hello name looking long term partner hello name drew student college looking job want find job hello name following code create game player move object around screen hello name ai language model assist information need help hello name looking job help develop progress career still see deterministic run code example recent version release,negative
import recent call last cell line import file none none file get associated base file file try load local folder cache model hub cache except raise environment error raise helpful error message original exception file token revision raise appear file available else return none appear file available,positive
hi issue script passing attention mask generate throughout best effort infer attention mask token equal pad token attention mask otherwise particular example setting pad token token token signal beginning sequence attention mask different two leading different output always recommend passing attention mask working example passing attention mask import model left print assert zero beam beam assert one beam beam assert zero one,positive
also check local directory look path instead hub,neutral
could try import error please also share full error,positive
running machine likely connection issue afraid check maybe try line code another model id say local see issue also,negative
unable run line also,negative
work notebook could share work notebook except memory issue end enough ram loaded without issue notebook notebook morning notebook worked without issue,neutral
work notebook could share work notebook except memory issue end enough ram loaded without issue,neutral
recent call last cell line import file none none ensure pas none auto file none none file get associated base file file try load local folder cache model hub cache except raise environment error raise helpful error message original exception file token revision raise appear file available else return none appear file available,positive
great glad back active back feel free ping,positive
first pip install accelerate import import torch model,positive
ca reproduce following work import please follow template provide necessary information opening issue like system code snippet,neutral
hi intended run input always receive test mostly designed work text might need override also critical test skip instead also thing transpose normal work mode careful transpose back convolutional though else rest code wrong,positive
thanks get everything green,neutral
issue still relevant able look,positive
something broken implementation different whereas implementation yielding every time run given supposed work update experimentation see implementation able attend token system prompt beginning system prompt idea see truncated,positive
hi separate problem could open new issue,positive
yes shape data time confirm case,neutral
hi thanks raising issue link work,positive
exactly complete error information recent call last cell line batch next iter print file self self yield file self self yield file self self iterable bool try yield file self data self data batch data yield batch file self data self data yield batcher data file return list file self self iterable bool try yield file self self yield file self self iterable bool result file self self yield file self self iterable bool try yield file self yield except exception raise file self try yield except exception raise file self data self data bool return data file self data transform self data data data none output else data output file tup casting list return input array except concatenation axis must match exactly along dimension array index size array index size,positive
paste bigger error trace transformation,neutral
thank much attention already tried came across though,positive
hi cause something different could open new issue issue include version used error triggered inside,positive
could let quickly try,positive
still different import left model print assert zero beam beam print zero assert one beam beam print one assert zero one explanation suggestion help problem inside involve,neutral
hey progress looking good thanks work quickly make aware due failing revert change postfix sure sure final review recent change account thanks notice tried post got,positive
hi also similar issue accelerate import following error look see import name binary use available enable following rebuild appropriate compiler warning could find recent call last file line return file line return name level package level file frozen line file frozen line file frozen line file frozen line file frozen line file frozen line file line module import file line module import normalize file line module import file line module import file line module import file line module import file line module import base file line module import base file line module import file line module import functional file line module import training file line module import file line module import file line module import file line file line file line raise module attribute module attribute exception direct cause following exception recent call last file line module import main file line module import file line module import file frozen line file line value module name file line module name file line raise import following error look see module attribute latest version,positive
hi thanks lot wondering whether could remove since discussion favor since easier maintain could open regarding,positive
hi although officially think would work pas single would still inside template could access template might little cleaner let know try,negative
quick update issue first big prepare likely work done one major blocker,positive
hi error would still present running fix yet part release source hour ago fix issue please let u know comment,neutral
merge unfortunately included recent release hope would,negative
auxiliary loss true automatically like switch modeling code custom able use returned model,positive
fixed import relevant module,positive
sorry response due sustainable progress work sandstorm working may start working issue thank,negative
hi getting error recent call last file line module import file line module import file frozen line file line value module name file line module name file line raise import following error look see module attribute output version platform python version version version accelerate version accelerate found version post false version na flax version na version version script yes distributed parallel script trying install particular version found trying reproduce also got different sequence differently,negative
follow may related trying convert version got error version work fine report error python recent call last file line module file line return file line model file line value name file line file line raise type self object attribute name object attribute,positive
hey progress looking good thanks work quickly make aware due failing revert change postfix sure sure final review recent change account,positive
hi pad value end successfully padding sentence value padding index dictionary end token tell model ignore documentation hi pad value end successfully padding sentence value padding index dictionary end token tell model ignore documentation like highlight problem position padding set instead value may either remain unchanged undergo future,positive
hi sure mean provide sentence like hello world pad certain length import text hello world text print look like hello world pad pad pad pad pad pad internally model function seen let take closer look position import print tensor see position padding set also position layer optional contribute gradient therefore vector training remains fixed pad newly vector default another value used padding vector hence padding position vector learned like highlight problem position padding set instead value may either remain unchanged undergo future,positive
thanks lot work left couple regarding minimal version please look thanks thank pointing left one comment fixed rest check fly,positive
hey thanks taking care let u know ready,positive
hi thanks issue recall formulation lora figure fundamental difference case adapter whereas second model model lora attached technically inference lora simply simple matrix multiplication regarding issue trained well think properly take care sure confirm version thanks issue correctly training like trained trainer none version pip freeze thanks,positive
hi thanks issue recall formulation lora figure fundamental difference case adapter whereas second model model lora attached technically inference lora simply simple matrix multiplication regarding issue trained well think properly take care sure confirm version,positive
might storage issue well make sure enough disk space device,positive
import pipeline import torch model pipeline mac false audio language print audio error log special added vocabulary make sure associated word trained implementation support padding training fused support attention beware passing data training may result unexpected please refer recent call last file line value file line yield file line forward file line file line generate,positive
hi intentional modify reproducer set padding left,neutral
explicit overwrite faster non always failing make sure,positive
missing name parameter comment,negative
hi code suggest use model outside pipeline transcribe first portion audio transcribe first audio audio almost long wrong thank python import import import torch device else load whisper processor model processor model device load audio file adjust sample rate process audio generate output device print without prompt print prompt generate token running model forward sequentially token text transcription print transcription,negative
hey part latest release need install source wait tomorrow,positive
use model outside pipeline also use transformer sorry beginner,negative
thank much provide working following python import train train city col col col col test test city col col col col class self data data start data start data data data return data import map list map train list map test like list length list length corresponding list length list length also tried number context length found always length turn error really confused bug long time would really appreciate help thanks,positive
like good please merge,positive
interesting think print first thing considering adapter last thing considering structure backbone considered random state inference sometimes input result different inference reason structure last one exactly structure backbone model lora adapter one already numerical difference difference object carried different one one however every configuration except object,positive
hi thank much taking time review really appreciate insightful definitely follow original function currently working towards deadline week making necessary thank valuable input support,positive
good afternoon think might interested know thanks feature able whisper small wer,positive
running make style change,neutral
hi thanks suggestion still got data converting related error approach full trace error message input hidden silently float might related fact layer norm float cast back input recent call last file line module file line train return file line decorator return function file line model file line loss model file line model file line return file line return file line forward file line return file line file line reraise raise exception caught replica device original recent call last file line output module input file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line return file line return file line forward file line file line return file line apply return super type ignore file line forward file line support data type,positive
interesting think print first thing considering adapter last thing considering structure backbone considered random state inference sometimes input result different inference,positive
yeah still main branch even could perhaps run slow branch pretty sure affect integration,positive
thanks suggestion close issue,positive
yep value affect return given model need well betting issue rather anything shape apparently super familiar model share well would help,positive
suggestion measure ratio per second ratio per second without share prompt target model device want show rule possibility slowdown rule possibility must measure varied better understand configuration unique prompt target model distribution calculating harmonic mean help,positive
hub anyway use automatically cache accordingly,neutral
confused evaluate metric report directly like need evaluate many time training decide trial good bad,positive
hello sir met error loading th sixth file still error try,neutral
believe may also issue content batch even size remains static change resulting text,positive
actually identical except set,neutral
thanks quick reply detailed related bug python import transformer model data transformation data create test instance splitter sample last context window seen training test apply test mode return,positive
one added made separate class test full model loading added support following pas new pas please review hopefully ready merge,positive
hey could make sure provide reproducer isolating bug ca really code stead,positive
hey think possible handled within,neutral
release path clear big thanks code latest main relevant long quantization section pas please review,positive
work get warning time able quantize air torch nightly accelerate train torch setting device like everything work except fantastic quantize could share code please save simple train leave default one remove,positive
tried import trainer error load procedure could found import following error look see load procedure could found resolve error environment anaconda python torch,neutral
test documentation last time,neutral
hard reproduce since error try create similar input python processor output supposed resemble something like model example box something similar think example closed issue tho think good,positive
root make style ruff check fix ruff format left unchanged make make entering directory running make leaving directory make make entering directory python python python make leaving directory running make style change anything last irrelevant would would would would left unchanged code exit status something,negative
work get warning time able quantize air torch nightly accelerate train torch setting device like everything work except,positive
suffer issue install version acceleration apple silicon install install accelerate check torch device print return true set device type working sure working check code see work tried,positive
also hit like backwards incompatible change appreciate error message link page clear say replace would need done deep trainer class u trainer looking code see set work lose whatever supposed give u compatibility sure somebody nobody know,positive
usually working mono audio file loaded duration one hour two long,negative
thank another test failure could use help input without code responsible kind input right build dummy attached full error code still little forward pas first thing change last format get kind input thank,positive
hey could try use latest release otherwise sure help private,positive
tell use prompt feature whisper pipeline,neutral
hi would like support please let know support thanks offering help however working example pipeline need help moment,positive
hi would like support please let know support,neutral
hey curious choose please point source work,negative
access specify package fetch local python work thanks actually case able access hugging face stably proxy want enable proxy time used firstly model proxy running code without proxy afterwards without code setting trying local network error false reasonable situation,positive
hey think already see need,neutral
think worked work main work fix,positive
last need run make style,neutral
let know sorry got caught review early next week,negative
hey think model like model available,positive
hey added new token need resize token layer model,positive
hey version python working hardware,neutral
release one small test fix,negative
look bit nasty indeed,negative
hello want help going need full reproducer contribution point,positive
use model outside pipeline,neutral
visibility share full reproducer would helpful,positive
test also failing main,positive
might better supporting relevant depend task text generation example would need padding,positive
hey padding decided skip true try set padding left generation padding token right always impact model,positive
hey could share full reproducer output,positive
python import import torch pipeline pipe simple resize overwrite content better work,positive
hey duplicate lot issue token set content true work python import false,negative
failing copy input preserve need check none apply old logic case familiar code help would,positive
could please expand mean like look good lower bound rather good average,positive
thanks response right something like text guess example simple usually prompt would need several example write article location perspective occupation year year ideally pas dictionary like location occupation farmer year currently template would need look like following bit unintuitive write article perspective year input location occupation farmer year,positive
like loading model trying perform code snippet running inference model pipeline python import pipeline pipe pipeline pipe replace input path audio much noise text,positive
llama unaffected hand instead mistral falcon probably many alternative solution waiting fixed suggest following line python,positive
relatively new python facing challenge converting right shape output need help transforming,positive
understood new model word level new simple code natively help get would call word level simply another character special meaning sense yes concept could used non architecture nothing stopping anyone older architecture let know clarify anything else end help merge,positive
ready summary relevant new false true mean relative difference torch already failing main differ st id st type null already failing main assert cooing dove gone gone good squeeze room assert watch show know spent lot time right patiently astutely boxwood mahogany assert sleeping instead conquering lovely rose princess become fiddle without bow poor main false true assert false assert false main false true new false true mean relative difference torch main false true pas pas pas already failing main id must form id must form use error run secret setup error run secret setup error run secret setup error run secret setup error run secret setup error run secret setup error run secret setup error run secret setup flash new false true mean relative difference torch already main unrecognized configuration class class kind false true unrecognized configuration class class kind false true false true false true false true false true assert false assert false,negative
work great image red run length completely,positive
got reproducer change script look,neutral
script header endlessly without find reproducer,negative
get attention size upon running script get error like generation script,neutral
per suggestion reworked avoid post slicing new function get usable cache length function used obtain rework qualitative result upgrade see test case suspect inadvertently fixed bug would able rerun,positive
thank making example robust different,neutral
hey fixed main brought,positive
hey could take look code quality failing unrelated documentation know going failing example even,neutral
yes high variance better consider minimal rate could ensure long tail otherwise might suggest slowdown,positive
thanks could also add maybe example log run since training log almost entirely difference transformation instead add log concerned training part,positive
already pas also hand utilization around believe pas verify every model correspondence model,neutral
additional image thanks change commit bee near main experiment calling individual exclusively memory usage linear low cost perplexity additionally test calling case multiple took index book kept index fed model subsequent index running cache also included loss converted perplexity note ca compare previous graph try observe whether model eventually perplexity image script main test perplexity stay constant good edit continued image change regarding slicing cache right main without special perplexity quite heavily non ideal perhaps indicative perhaps bit hard tell beyond left right cache behave identically unless made measuring mistake bit odd confidence fix afraid,positive
yep reproduce indeed good catch idea happening yet opening,positive
hi really interesting question designing spec realize people would eventually want use chat besides chat result prompt format quite flexible fact believe able pas raw string write template support loop like message list however even though input always think would still work string case could probably write template like summarize following article article solution article key message would also work might feel free experiment let know encounter think first person know trying use case definitely interested hearing experience,positive
hi thank default temperature probably said black standard deviation min save exact data ca share le probably high variance example phenomenon courtesy image sorry delay working try work weekend,negative
hey time take look dedicate time,neutral
hey let take look back mind think add something similar true could great add time,positive
hey pointed fun add feel free open help,positive
thanks raising issue take look last question try reproduce though audio file use particularly interested duration,positive
failing setup differ ock dock pier extending pier dock extending body differ ant visiting serene location one ant water calm image could possibly also case main check,positive
hi thanks help please let know anything else,positive
error usually standard deviation measurement centered symmetric moment denote measurement range reading distribution huge moderate,positive
thank follow tried new branch gave another error python recent call last file line module train file line train file line train return file line file line file line file line file line inner return recursive file line return file line raise loaded state parameter group loaded state parameter group match size group,positive
record code issue python import torch dim range pow range pow range dim print dim print,positive
actually sure ship fast enough need additional testing,positive
set proxy set environ import o,neutral
hi thanks issue fixed close feel free take look good first,positive
already working last week see comment time finish yet want work together idea available,positive
hi team got solve specific image thread,neutral
facing mistral accelerate pip install accelerate happen execute program appear run program got solve getting different version mention trying run locally apple silicon anyone fix,neutral
news force first waiting new release,positive
hi review test case modification fully understand testing function think understand,neutral
could shed light detect tensor specifically check,positive
getting following error pip import following error look see import name worked pip pip install problem altogether,neutral
thanks many fine tune meant instead meant provide single response prompt example may batch inference summarize lot chat use case notice implementation list chat history template ca look like summarize following article article could make article text single element array template like summarize following article basically single generation use case would considered subset chat use case recommendation another option could generic making input flexible list conversation,positive
anyone end start continue keep going,neutral
hi first time open still issue try take crack,positive
testing onto main seem testing implementation share example intentionally cut audio actually provoke implementation inconsistent different sometimes significant audio example tested produced lower quality,positive
turn image hi thanks great work two quick temperature use sampling sampling interpret go minimal per second rate,positive
actually discovered well problem end still felt like situation like bug initialize empty set additional special end special token already set remains empty effect special token set guess intended,positive
wrote test self import image image image image print print print work,neutral
running issue user interface,neutral
add clip model could merge main branch,positive
oh god pas think ever going happen,neutral
going source code link like function param true excerpt param true list additional special list provided otherwise extended former case removed full vocabulary added token added empty list like wan na explicitly set param false also execute print verify token added prior saving see code sample clearly special token loading,positive
thinking something like pipeline add thinking making function prototype big,neutral
commit suggestion redundant computation somehow suggestion thanks,neutral
hi yes could take look thanks,positive
cache abstraction today start working top happy provide,positive
wrote working expect see soon,neutral
found shape used compute loss different way rather cross entropy copy issue make case checked identical since former marked copied latter reason issue previously time certainly sure executed git pull rebase yet included result push forcibly sincerely apologize lack practical git experience,positive
removing confirm python import import import torch model model model assert assert assert work fine fix failing main,positive
hi able help would need reproducible script rule issue,positive
commit suggestion redundant computation,negative
sadly easy advice could numerical issue due different like subtle bug see two forward able pin source mismatch safely confirm something like numerical issue run resulting metric similar low memory mode bug,positive
hi thank opening really familiar topic ensure name consistent change often could show u example see name without change could share documentation guess doc providing type name practice name finally example demonstrate change job thank advance,positive
going trigger main order include fix,positive
hello following data getting error resolve thanks,positive
yes think synergize new weight building well able get good shape soon,positive
useful long want translate whole thing wrote thanks response need set generate method necessary set parameter reply directly view id,positive
hello please let u know issue,neutral
sure weird push run,neutral
hi added different copied,neutral
understand trying could explain give exact code snippet want add model case clip thats inside variety file clip,positive
strange see issue know,negative
indeed unfortunately go one need eager torch use successfully support good shape left falcon need use default make work attention attention mask provided tracing work default supporting loaded torch see loaded eager use tracing may future record due fact want much possible able dispatch flash attention passing impossible still going see think keep flexibility minimum torch version use class may want bump torch future,positive
done style unrelated fixed image,positive
solution like good compromise finally made interactive replicate probably update model redirect quick test without installation,positive
reload base model apply adapter ideal best wound full model try dont run bit work easy havent quality,positive
reload base model apply adapter ideal best,positive
issue force added slow fast course ca token index slow fast update force default default,negative
yep merge main soon one regression model,positive
ah sorry pas input thus mask shape,negative
hey version ruff main usually make sure correct format seem failing ruff,positive
still get type error try,neutral
set return everything maybe mean make,negative
ah ruff could try main,positive
understand trying could explain give exact code snippet,positive
latest version behaviour pip freeze,positive
breaking change current state since currently last stage index user specify think added add backwards compatibility would update default case,neutral
hey version ruff main usually make sure correct format seem failing,positive
yep like non breaking overall,neutral
thanks response need set generate method necessary set parameter,positive
hello thanks issue time look think due supporting real hence falling heuristic suggest change model use compatible need absolutely get entity text offset like word text entity start entity end fused think something immediately type good day,positive
hey pipeline use yeah super intuitive,positive
also found audio classification except audio spectrogram transformer similar therefore well patch make despite fact support regression make seem solve problem removing fix main branch upstream made merge main branch one included reset merge branch forcibly sorry mess,negative
hey good actually passing would great general better full one issue really go well hierarchy pas custom argument custom could specific,positive
exact issue load bit like fine tuning never applied figure fix,positive
think issue fixed main would mind trying latest version,positive
yep make style rebase trick,neutral
base mima python false hey could try,negative
big deal think main might help well otherwise push styling also pip black could help,neutral
think yet feel free open want,positive
slow support option set false default common use case need special token fast going support near,negative
llama multimodal one actual model clip two split different clip clip,neutral
please refrain copy pasting text like gon na help help need full reproducer access local folder see like see following,positive
hey thanks would like open fix,positive
oh nice memory leak maybe look,positive
thanks bearing u long review merge rocket,positive
mean pas list list list string need special token like example sure understand last comment take input usually support input need work,positive
able solve word whisper also page,positive
think force seeing positive side though modeling like fixed mind opening clean merge,positive
done made lot idea correct,neutral
update right came across issue running trainer use streaming different sequence length,positive
let get one release,neutral
maybe disable testing think super important,positive
sure would copy entire flax script try make minimum update flax code already heavily format speed integration significantly feel free open ping review happy answer,positive
hi thanks issue able reproduce issue version source model call snippet used python import import import accelerate import import train none model trainer model text try script latest accelerate pip install accelerate,positive
ah try main push force,positive
like loading model trying perform code snippet running inference model pipeline python import pipeline pipe pipeline pipe replace input path audio,neutral
super glad logic sense sorry entirely right attention mask change attention mask prompt got carried away example see code feel free copy update data collator script,positive
hey dictionary defined whisper model since guarani whisper model avoid dictionary note still whisper guarani without change detailed guide refer tutorial,positive
know ahead time kind jargon could first try whisper prompt python processor model processor without prompt leighton print grave whether sir work really discover little rocky prompt leighton print leighton grave whether sir leighton work really discover little rocky next best method would original much data possible regime freezing call line see issue custom vocabulary note require data standard completely sure standard original work trying also note completely reset rather append new vocabulary,positive
hey super cool hear team release hub integration working well might time efficient leave want something similar inference could create simple model allow pas client space button bottom use via model space even model inference,positive
unhappy implementation making fail tricky one,negative
hi issue fixed bug nightly,positive
agree better context looking would say appropriate term feature however propose put feature parenthesis standard term may used,positive
got confused saw added output sequence name grouped led think sequence truncated case,negative
behavior truncation set false input string long would throw error truncation model bit actually truncated vector model input string never truncated need check visually prove work try setting false seeing fail definitely try fixing far tell think probably working wed wrote sure hi thank response might hardware difference use long sequence correct one truncated adjust main concern pas truncation true seem effect test take time could allocate time following let know reply directly view id,negative
failing current variant already failing failing test current failing,neutral
whoop attached wrong one attached correct one thank edit ca seem error look weekend,negative
also encounter issue complete code reproduce issue actual code use removing line code run without otherwise evaluation file line return file line return file line forward output shape match broadcast shape python python import import import evaluate metric precision recall model negative positive negative positive text label return self list map list map return macro key train test key key key key label trainer train test print system pip freeze python version python wed driver version version name volatile fan temp compute mig mib mib default type process name memory id id usage running found,negative
totally agree touched ruff version reversed ran pip black pip install ran make style made see test also tried run suggestion make got error fix,negative
hi sorry delay firstly generally handle like python self self none supply argument supply argument default value set also took look attached file ca see error content,negative
hi got feedback people also particularly successful happy sense regarding integration think still would amazing use inference according possible current setup would like ask something could side facilitate otherwise hard fit happy leave close issue,positive
let know would like test,neutral
added back ready merge,positive
yes work well thanks,positive
sure hi thank response might hardware difference use long sequence correct one truncated adjust main concern pas truncation true seem effect test take time could allocate time following let know,positive
put check class class still one think,neutral
sure whether help tree bloom data rank run script generate name true profile true stage false include sentence label true profile true stage false name true profile true stage false include user custom data configuration loading generating data data took min computation took min data data generating train split generating train split generating train split generating validation split generating validation split generating validation split unable verify size prepared subsequent reuse data loading configuration file cache model false true true true bloom null true false false dev true loading file cache loading file cache none loading file cache loading file cache loading file cache model model used model task similar task model trained already use without training running running running running running builder script builder script auto half precision following training set corresponding argument user sentence user sentence safely ignore message recent call last file line module main file line main file line train return file line self file line model file line raise please correct following mismatch easiest method set setting auto detect warning unable find proceed training local sentence label setting auto detect world class setting setting auto detect warning process rank device distributed training true training true false false load local file train load local file validation custom data configuration loading generating took min computation took min generating train split generating validation split unable verify size prepared subsequent reuse data setting problem type single label classification finished model warning key model equal key run ignore sample training set sample training set sample training set accuracy classification score use overwrite killing error return code,negative
error reproduce error running cell,neutral
also usage version following length entire sequence prompt consistent,positive
thanks prompt response close issue would like,positive
following length entire sequence prompt consistent,positive
update latest version solve,positive
full latest run update install run pip install upgrade pip user create user run gid gid optional add support omit need install update install echo anything else want like clean go optional set default user omit want keep default root user,positive
good catch thanks fixing run test may need update value expectation value test also incorporated ready go,positive
hi green would please help review merge,negative
try running make pip black pip install fix failing test sure,positive
also need run make style ruff package pip install make style unrelated permitted submit,neutral
understand respect position even though different thanks considering input,positive
ran perplexity attention persimmon llama mistral look good performance latency plan phi tomorrow assume good also try use generate,positive
example could show model generate think valid use case,neutral
running twice give exact deterministic response issue could validate um hi thanks issue able main branch import torch import model auto hello name print tried without got different perhaps try upgrade pip install reply directly view,positive
might simply terminal output showing word mask though oddity though fact instead failing long input text true point unit test sure since tested locally change unit test check rather particular set would work everyone,positive
strange expect see name mask dolor sit would output name mask dolor sit instead see name dolor sit know need get time night dig,negative
hi tested issue pipeline instance without pipeline case load pipeline iterate audio loop first loop work afterwards subsequent pipeline include previous prompt previous included also subsequent pipeline produce le le coherent edit related issue fixed,positive
hey thanks think already partly fixed let merge part want hi current version master branch fix guess fixed code shown following class self super following llama code suggest move update accept solution also need pas past let check latter class model true true,positive
want merge black longer used rebase receive ping green tick,negative
initial show impact whisper amount added try optimize near,positive
let resolve good go,positive
also need run make style ruff package pip install,neutral
loss computation done outside model recommend custom usage missing dealt saving general class save load see python import,negative
hey related backwards compatibility favor directly,positive
recent call last cell line import file none raise class exist currently return otherwise creative model class used default file token revision else loading file cache return file token try except raise unable load vocabulary please check provided vocabulary accessible corrupted file self legacy false super file self dependency return open file self return return file self self return self string might issue local version taking working,positive
hey sorry ca reproduce snippet missing end full,negative
instance attribute instead class attribute agree put attribute concrete class rather say case value also usage far python check none none instance yet,positive
sorry think disagree matter alright,negative
hi related accelerate open issue hi unsure whether listed accelerate relation know whether independent factor would impact splitting data accelerate reopen accelerate thank,neutral
syntax error sorry false alarm example able fix,negative
ca really see better solution currently either work,positive
model python false work standard model also,negative
kind want support static compile faster inference might already well,positive
pip install quality make style make quality work branch exit appear step setup problem actual failure,negative
build documentation step click see image,neutral
able fix trainer tried passing false unrecognized argument error currently,positive
also try assume got fixed somewhere else,positive
quite see token issue pip install quality make style make quality,neutral
hi related accelerate open issue,neutral
elaborate issue face ideally small reproducer,positive
hey usually assign open link issue,negative
alright passing still code quality failing find token seem like something fix need rebase,neutral
hi thanks issue able main branch python import torch import model auto hello name print tried without got different perhaps try upgrade pip install without got bash hello name insert name insert profession help got bash hello name following little bit love home,positive
issue way fix best post thanks,positive
even know soap merge,neutral
sure week feel free ping need help,positive
hi thanks lot answer time aware word alignment extra work implementation whisper experience increase memory usage difference due fact pipeline perhaps possible free space computation batch notice memory usage increase number batch progress maybe could input future thanks lot work,positive
hey pretty much extra computation order compute word alignment cross attention really seem lot documentation part pretty much part doc,positive
look like cut run sentence label setting auto detect warning unable find proceed training local directory directory start running starting model profile run wrote wrote resource connect host port assign address exit code done cleaning following done running resource error model runnable error end running optimal configuration found,negative
unfortunately trace leaf whole line blank,negative
actually test reveal behavior change trainer run reuse directory fail sure common practice one test training thus trainer step support adjust change make compatible destination log warning alternative everything destination directory destructive could break people,negative
trace chunk talking cut bash error model runnable error end running optimal configuration found,neutral
sorry sure mean already whole log output,negative
provide bit trace see model runnable error,neutral
thank contribution please make sure green run make style fix,positive
test fail since new test block added get shown end could check truncation true job thank advance name dolor sit dolor sit dolor sit ing dolor sit dolor sit dolor sit dolor sit lo dolor sit dolor sit dolor sit dolor sit sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit name dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit dolor sit,negative
alright feel free open time,positive
see like may cleaning temp right testing also maybe remove possible done needing,positive
sure got point entirely flattening whole batch passing model getting output mean concatenate together token represent sentence concatenate across axis,positive
tue bot wrote issue automatically marked stale recent activity think still need please comment thread please note follow likely reply directly view id,negative
dealt added model also correct import index file,neutral
hello status update thanks,positive
hi need run make sure image could build build step work contain bug could trigger run could really care many failing,positive
thanks sick let push new image manually locally verify everything working image pushing,negative
temperature seen calibration model model use temperature also see flattening probability distribution overlap top difference top temperature log top end possible temperature impact relative distribution optimal temperature across llama around instance model used around suggestion assumed use like llama mistral,positive
hi let know question need help regarding,neutral
try make pip black pip install,negative
merge full slow touched pas,positive
like open another update notice leave open,neutral
failing either flaky unrelated considering priority merge,neutral
facing mistral accelerate pip install accelerate happen execute program appear run program got solve,neutral
thanks input think work case aside entirely different name use case intended loss calculation also completely different variable wo able control configuration manually set depending would default value anyways viable solution need,positive
thanks revisit find time,positive
case helpful similar error found forgot change model size special resize similar,positive
good go saving large fit memory although still cleaning code test approach map shard shard key saving shard safe serialization wrinkle meta default sure good way around without trying preserve memory open however,positive
thanks edit revealed issue fail fast see test related,negative
understood new model word level new simple code natively help get hello could take another look recent added support batch inference feel getting close,positive
approve grateful could review,neutral
problem thanks bloom well last step run make style code test,positive
main code quality check failure related,negative
quick make style make quality fix set good thanks patience,positive
yes also second link however fast another typo corrected second commit found option delete first commit hope experienced yet pull request,positive
merge apply change recently,neutral
personal summary issue model working many mean issue side said pip solve problem note version pip freeze pip install ended create run script model need import import o validation model time although model cache may sure work note full model curling also error thing,positive
hey new comment since pretty well buried ready go removing two let know agree make last commit merge,positive
instead thanks new release coming week,positive
latest accelerate automatically disable card support marking unless someone else,positive
sorry delay require class method static believe unless seeing something,neutral
still similar need recently need experiment bit organize,neutral
thanks suggestion please feel free open,positive
thanks busy feel free take time need,positive
duplicate let discus thread keep track issue note loaded correctly warning need,neutral
think need stripped compute common algorithm otherwise try match token right prompt input left inevitably going,negative
let cancel show usually experimentation tomorrow,negative
well probably affect tomorrow,neutral
trigger many usually removed unrelated although bit time consuming mind keep running way time,positive
solution work parameter exposed,neutral
ready new round much better new generate compatibility cache subclass among flag cache instance empty cache model stick format deprecate future point pas generate use cache instance therefore format generate ready receive static see generation test example accept dictionary update responsibility subclass process dictionary u plenty room flexibility grow without signature cache method also start thinking cache removing added added support partial rotation extra set would warning bug beam search legacy cache format suspect subtle bug chase edit bug sorted,positive
hi please assign thanks,positive
sorry late reply fixed version accelerate work end thanks also great reproducer issue properly copied hence still old forward instead forward,positive
propose enable default torch written apart exposed eager may used disable may need agree name eager,neutral
ah good catch fix tomorrow,positive
hi believe order pas update flax also mistral also belive revision removed pas,neutral
hi could please review let know feedback quite similar think wait add fix,neutral
bug torch since public import import dev set storage tensor device storage different device longer must match fix work moving everything desirable could shed light detect tensor specifically would implement tensor device although could easily brought bug minimal choose,positive
hi thanks think might introduce lot verbosity would relevant case people use,positive
agree whisper special enough also agree seen exception,positive
good shape many though guess way designed,positive
simply support use might fail particular work,negative
see case think like seen might running issue right without explanation cause would nice least warning loading large,positive
running passing pas well mi two failing seem unrelated leave one issue image nicely dependency resolution outdated warning missing incompatible torch well example use instead issue open,negative
gradient dear well received thanks best th,positive
use efficiently currently ca generate method context manager overwrite method make sure context manager huge amount memory memory error would easily triggered thank much nice day best th,positive
noted think need separate though since composite model fix though merge urgently release work something fix like rag,neutral
sorry tiny issue condition false fix merge thank review,negative
thanks agree good idea sense,positive
model used internal building block explicitly hidden might acceptable agree still somehow recent ca even find exist good,positive
one hesitate need help conflict,neutral
hi made add translation open feedback,neutral
hi example refer meant image classification meant image classification image label clip script want improve image classification certain domain,positive
failing test already ping,neutral
test appear unrelated none none raise run secret setup run secret setup error run secret setup error run secret setup also loading code could use sophisticated validation automatic fallback validation failure issue,negative
nothing get approval core ping one,neutral
thanks great want get clarification comment otherwise good merge,positive
hello finally back settled continue working dealt minor added opinion like deal bigger later week let know thanks keeping alive,positive
hi resolved final please review help merge good change respect segment one comment pending add segment following syntax getting auto removed make resolved,positive
would like help possible,neutral
thanks helpful trying paper also another recent progress speculative look ahead fig also useful,positive
still great missing merge,positive
also implementation instead end user pas three input go ahead one token inspired done prefix suffix loaded user input contain token text split get prefix suffix text also add option pipeline return filled text total text prompt,positive
currently pipeline code copied pipeline discussion implementation since compatible like list example good starting point python,positive
use efficiently currently ca generate method context manager overwrite method make sure context manager,positive
thanks close since obviously top already noise,positive
time properly finish right blocker set none even make sure forward pas correctly except skipping test sure proceed,positive
try merge branch upstream main perhaps fix current failing,positive
test suggest test test daily,neutral
hi stated default shard size support inference easily previous unfortunately one need manually shard model push hub new model id,negative
hi thanks lot issue perform pure fully leverage quantization need train top example library check example documentation section train,positive
several much lightweight also plus latest know permit commercial use good found good,positive
thank response case llama confirmed billion famous identically fast slow even version image,positive
linked let try put everything single,negative
hey please refrain question every single issue unrelated forum best place discus,positive
look week thanks continued work mon wrote hello update issue way save bit model hi ready need one last hopefully fix please reply directly view id,positive
release week part install source pip install,neutral
hey could share warning make sure backward compatible backward compatible reflection used check existence new method,positive
three new version version version three ready review,positive
issue recently fixed think,positive
hey class seem exist motivation want create new class benefit backbone novel field contrastive learning bottleneck,positive
test main feel free rebase,positive
probably pack data also efficient think separate,neutral
understood new model word level new simple code natively help get,positive
thank quick response try main branch,positive
hi already fixed main,positive
attention recently see method,neutral
sure ask forum instead thanks,positive
want used think probably use sequential,neutral
hey waiting review time work working accidentally wrong fixing opening new one today sorry inconvenience,negative
rebase main help get fix,positive
well pipeline already pretty bloated whisper stuff problem either already edit support think,positive
hey waiting review time work,neutral
issue passing quantization python model auto error serialization saying python file line object type pas,neutral
inconsistency due various always difficult use latest version could please let know version issue,negative
hello update issue way save bit model hi ready need one last hopefully fix please,positive
shell pip install thanks work,positive
setting correct parameter looking work well also confused difference,negative
hi need make sure input model set true duplicate try add python model else module input output true somewhere training script call use efficiently,positive
hi might explain getting call via true control memory usage,positive
unable see library yet test,negative
able make pas due import please guide,positive
turned decorator generate method looking body method solve problem run problem need solution thanks solve problem line file added true inside generate still sure work image image please note two,positive
thanks follow test understand made commit pas testing via integration share get run integration test make sense git git index class self return class else none slow self class float model auto model processor processor pas adopted code image code code sure pas attached error log another failure get coming assert object none full error file attached understand look related thanks help,positive
mixed another close reopen another relevant,positive
hey thanks opening issue try keep could ask question forum instead sure community help thanks help code running,positive
personal experience whisper well given short without padding maybe something wrong might start notice get,negative
hi thanks working suggest mine first thank best school computer science engineering technological university sat shin wrote hi support building upon work proceeding new like discus submit fork detail review please let know preferred approach reply directly view id,positive
yes problem would also recommend change work properly padding python reduce shape otherwise need use audio length may impact training speed average audio length short,negative
thanks great work use utilize compare based like based give fairer comparison,positive
could advise intimately familiar test operate,positive
tried implement flash attention best could get error must shape module try tomorrow,positive
facing issue anybody find solution thanks,positive
perhaps suggest lowering temperature bit use default nowadays suggest either lowering temperature top imply truncation sampling intended use standard temperature sense default provided also first place temperature also representative original arbitrary transformation sense least compare value unless,positive
thanks could explain motivation behind fast whether slow example mostly need new folder new model use implementation also add file like flan example thanks helping push team help explain part main motivation world improve support within especially character based without benefit,positive
hello open feedback thanks,positive
proxy may help import o number thanks save china every thing went well setting proxy manually even though globally,positive
got added quickly support seamless streaming well,positive
course size matrix new warning general add new special part thus seen dear thank much reply also one question training model practice kind length set train model use length length use pad length example maximum length batch pad length effective practice thank much nice day best,positive
also blocked approval maintainer,neutral
see test run none raise run secret setup run secret setup,negative
hey think former case per model need dive deep problem understand interested happening run get failing like differ differ differ two different also get failure though frequent differ differ differ,positive
sure would love pick,positive
different issue definitely want version otherwise accelerator unrelated allocation threshold simply install correct version official detailed possibly outdated,negative
hey could try since priority python set probability zero generation pas directly generate precedence generation far easier set see set transcribe speech transcription example python import import audio load model processor processor model load streaming read first audio sample test audio audio next iter audio audio sample spectrogram processor array generate token transcribe decode token text transcription print transcription thing forced hood setting token whisper,positive
need train since model need learn affect output translate speech translation transcribe speech transcription mask task token model way training time perform randomly inference time language need train language token model differentiate train model training time model translate speech translation force inference time control behaviour want however one task resp one language mask task resp language token accordingly done training original model example removed entirely,negative
good assuming sensible good like perhaps suggest lowering temperature bit use default nowadays,positive
pipeline designed wrapper go audio text want something granular best use model processor python import import import torch model processor clean validation sample audio processor sample array print print print print output quilter apostle middle class glad welcome gospel tensor,positive
hi doubt documentation written serve purpose missing something would happy corrected approach forum doubt image,positive
would indeed cool fa support since attention mechanism copy would like go integration quite fun get quite big much additional effort,positive
thanks complete per recommendation,positive
thank discussion language induction indeed something investigating research rocket,neutral
note available python import import clean validation audio array processor model processor without prompt print grave whether sir leighton work really discover little rocky prompt let change spelling leighton passing prompt print grave whether sir work really discover little rocky looking,positive
worked code busy life stuff feel free poke around follow may pick one day,positive
hey sorry radio silence would like proceed happy current remote code integration hub look like already got nice integration,positive
hi also check following repository team recently added support might support near future,positive
super interesting language detection wonder whether general inference would benefit accurate language detection stage passing language audio whisper transcription could use interesting see near wer degradation smaller context window suggest keep pinned prevent creeping aware model fixed context length thanks much interesting discussion,positive
want go explicitly writing feature extractor make signature method precise astutely,positive
genius possible would great clear whisper thing seen long time sure everyone else,positive
make training really slow think really affect way fix reinstall library,negative
hey pipeline helper function compatible audio library meant fully comprehensive rather easy audio common set audio text hand argument example concept speech like hence currently pipeline given prominence whisper current popular model would making exception valid pipeline,positive
suggest start simple going top alone le work straight point realize gathering enough expand multiple multiple better overview help roadblock along way much interest crossing min successful see doable top min top min top min top min temperature,positive
hey ignore warning mean training anything wrong script change threshold want hide per,negative
hey could possibly share audio sample run example currently local audio file reproduce prompt phenomenon happy advise fix,positive
running see test correct,neutral
could try removing model setting speed none wer cast correct let trainer handle,neutral
thanks lot pas however still failing slack report weight name start internal error unless something really evil please open weight name start internal error unless something really evil please open issue weight name start internal error unless something really evil plea weight name start internal error unless something really evil please open weight name start internal error unless something really evil please open,negative
hi test version dev still facing log attached,neutral
problem regarding fix version relevant briefly think fix try submit another fixing later week sorry delay next severe flu last week start working,negative
final user use enable,neutral
hi add much value let know would like close still keep deprecation,positive
preliminary seem point obvious let wait see regarding sampling greedy greedy default legacy sampling far popular chat like summarization translation automatic speech recognition tend use greedy beam search though finally regarding default default none detect whether user use default legacy set model level prevent u however suggesting,positive
case interchangeable generate converted equivalent,neutral
yep also gon na fixed need package maybe specific like candle custom maybe support rare torch,positive
ow nice catch nice reproducer try look otherwise,positive
good spot method tested slow failing passing added,positive
hi comment update update en folder little busy past think back work best,positive
hey sorry really time dig must say know generate pipeline could fill hole algorithm used would mean explore technique otherwise similar,negative
hello could please let u know issue yes would solve first glance report back running whole process,positive
thanks quick response trying argument got error recent call last file line module model file line return file line raise accelerate pip install accelerate latest version pip install pip install note already latest version accelerate,positive
hello could please let u know issue,neutral
turn need update message clear enough,positive
everything use good add cutter add script verify indeed script used add everywhere import o import path return div flex path print open doc div div flex div print else print open,positive
suggest start simple going top alone le work straight point realize gathering enough expand multiple multiple better overview help roadblock along way much interest crossing min successful,positive
fine name think might adjust documentation bit current one whether allow custom defined hub modeling option set true trust read code execute code present hub local machine like user run custom defined hub safe update,positive
let look bit way use distributed launcher irrespective whether working without,neutral
knew would work distributed launcher however would work without launcher could actually sure current way launcher intended design stop gap measure,positive
understanding little bit hacky first create model card completely overwrite trainer content like proper solution would trainer update model card already understand would work hacky approach yes trainer ideally update already everything would open making cleaner,positive
hello per documentation need use accelerate launcher pas launcher launcher accelerate launcher,neutral
hey work issue would love work contribute,positive
thanks implementation environment variable remote code also past given proximity think would make sense variable name two good indeed taking risk code hub pickle running approach might also locally still require flag set strongly happy hear proposal think would nice unify two argument used elsewhere would,positive
script missing downcast tensor tensor hypothesis internal correct would make equal case import time import torch import range print print print print time bit print time bit end get time bit time bit difference small think worth change,negative
looking good able reproduce fast also actual token addition,positive
merge let check hub say running,neutral
let try maybe merge rest,neutral
setting correct parameter looking,neutral
yeah super clear maybe hypothesis template doc good start,positive
hey thanks never issue running native code would recommend run native o,positive
yes snippet automatically account every right see doc much,positive
fee free ping ready review,positive
important note behavior might various quality audio input specific model used accurate label anyone stupid rightly pointed way address concern firstly whisper model translate content rather automatically language language address try language transcription via generate method example might modify code image code snippet definitely add doc somewhere open issue code documentation understanding,positive
would feature would maybe never idea,neutral
happy give another name come better one though,positive
model description want test although ran model context length could test context length sufficient open source status model implementation available model available provide useful link implementation met issue anyone else solve,positive
really remote code code pickle file could local file link file code,negative
please refrain question many,positive
need model like true automatically convert save model,positive
hey feel free check key adapt script accordingly,positive
feel free open fix,positive
hello issue lie torch support,neutral
thanks explaining like good addition feel free ping need help model custom model fire,positive
solution want actual certificate use bash script made certificate file cisco convert update folder script curl new certificate cisco still need set environment variable function package used file inside package instead set directory certificate import o import model model without security hope,positive
please refer experienced exactly issue resolve one line bit turned,positive
likely artifact running logging loss try make sure plotting real batch raw loss want see noisy true loss basically probably logging issue,positive
wondering original sure tried situation,positive
someone actually subset taken test set model without special text normalization idea explicit set en generate function length wer length wer length wer explicitly setting generate function length wer length wer length wer,positive
thanks response link close,positive
output might cause unexpected later line,positive
hi intended behaviour treat first word bit differently check answer full explanation better way see setting false really want override set true import print dog print dog dog see token used dog one corresponding dog prefix space,positive
alternative method significant amount time,positive
hey following issue like wondering current status progress could share latest code link really help thanks,positive
thanks yes awesome none stage random brain spark type moment,positive
thanks yes access check stuff like,positive
permanent solution getting error,neutral
could optimize one one naive assuming figure apply equally would possible one used greedy one used sampling even handled hood even presumably exposed would simpler know exact tune,negative
image significant difference greedy sampling still gain proper analysis phenomenon would effort probably try run similar thing code well think something could try let know one question popular method greedy sampling would assume greedy since default know sampling better quality could optimize one one,positive
context also interested feature understand trivial implement general one current example axolotl harness efficient sample correct block diagonal attention series monkey underlying model popular like llama mistral though code detail believe fact flash attention implement scheme relevant efficient reason incorporated axolotl general wisdom inside large corp suggest type block diagonal better large scale training code relevant like focus may beam use case slightly general use case also relevant forum post,positive
hey sorry late reply quite busy work recently like please feel free pick instead think wo work best,positive
hi limited mask code proceeding far monkey patch based negligible difference old new way dent believe rounding error somewhere would support basis put new test hi try monkey patch notebook see work implement idea,positive
enough people willing test evaluate rule margin error though definitely enough people run proper study agree build interface study happy promote also power allocate space order run study main thing looking minimize included improving truncation usually low probability begin going hard test without sufficient data sample normally unless change sampler pick least likely token way measure truncation consistency directly agree biggest difference however output may effect bad probability hard observe noticeable human preference sure difference also test turn success gain much power distribution technique human preference especially certain metric perplexity dubiously unreliable quality space agreed understood never made space new territory though look sure since empirical data would helpful would fair comparison value top would prefer something might aggressive though next problem think finding scale scale min obvious understood epsilon difficult determine,positive
hi translation example import pipeline pipe pipeline pipe let go see tower every translation model think inclusive add require,neutral
enough people willing test evaluate rule margin error though definitely enough people run proper study agree build interface study happy promote also power allocate space order run study main thing looking minimize included improving truncation usually low probability begin going hard test without sufficient data sample normally unless change sampler pick least likely token way measure truncation consistency directly agree biggest difference however output may effect bad probability hard observe noticeable human preference sure difference also test turn success gain much power distribution technique human preference especially certain metric perplexity dubiously unreliable quality space agreed,positive
depending answer comment move differently wait answer,neutral
question currently tutorial sure best translation feature literal translation would point de respectively however wonder spot even translate standard would appreciate another opinion,positive
good test would compare technique blind human preference nothing better human preference happy participate evaluation enough people willing test evaluate rule margin error though main thing looking minimize included improving truncation usually low probability begin going test without sufficient data sample normally unless change sampler pick least likely token way measure truncation consistency directly done exactly top min saw min obvious improvement would like reproduce experiment typical sampling inference engine choice broken implementation typical sampling moment fix use adopted anywhere else world learn use test like necessary future also aware appeal popularity hard evidence think marker case would otherwise given context certain metric perplexity dubiously unreliable quality space,positive
precisely past added ended much use like eta sampling additional validation instance eta sampling blind human preference test shown preferred top relatively low sample size however upside marketing large enough community decided stick simpler established like top technique make inherently good science let collect data yet see data beyond note nothing creation actually agree principle large library conscious add good test would compare technique blind human preference nothing better human preference happy participate evaluation,positive
third step think need define enough detect content assistant prompt know unmask inside example assume prompt help unmask help mask,neutral
agreed would even go beyond case constrained generation part text generation set possible logical error idea text generation,positive
hi thank opening issue addition temperature top top apply three typical epsilon sampling eta sampling similar thing mention apply top transformation probability distribution since already similar backed reluctant add technique without maintenance heavy burden want contain min comparison top objectively consistent beyond min also highly interpretable comparison locally typical sampling subjective information theory question whether typical sampling le intuitive use end user addition typical sampling epsilon sampling eta sampling seen extremely limited real world adoption open source large continued use top top wake two seen mild popularity would argue latter two epsilon sampling eta sampling perhaps proven subjective quality conclusion min interpretable end listed le risk unintended behavior goal top top typical sampling le proven world proven scale consistently comparison nucleus sampling practice consistently seen positive reception adoption open source language model community large point inference adopted image image also note common issue open source language lack truly objective metric testing beyond manual human analysis apparently testing metric given considered absolute final compare sampler specific metric would like see specific try provide support case beyond subjective widespread adoption technique figured would stand would beneficial assuming trust assumption hesitant make without sufficient fundamental evidence use beyond used,positive
look paper see figure important note found potential discrepancy getting poor fine tuning phi sure would reason,positive
vocabulary extended special white white though upon inspection word dog giving word also sentence moose dog good dog output output dog token first word sequence sentence space used word first word sequence also true well,positive
great let conservative approach lower enthusiasm enjoying,positive
also past review approval,negative
found vocabulary extended special white paper,positive
hi great see project working example love add point capacity maintain new text generation project moment probably see response time moment quite limited since used production ca add capacity maintain suggestion let add code team maintenance move main folder soon capacity end sound good research project able make want pretty much side great thank couple test ready would happy write like one introduce feature,positive
hi great see project working example love add point capacity maintain new text generation project moment probably see response time moment quite limited since used production ca add capacity maintain suggestion let add code team maintenance move main folder soon capacity end sound good research project able make want pretty much side,positive
must made mistake code original notebook loaded data work,positive
space character account different token dog case use dog dog get output output respectively sure intended look,positive
good second look time,positive
dont see version number,neutral
ended force distributed training used prepare accelerator prepare step model else model add force model else old behavior model else handle wherein pas model force model else old behavior model loss loss hack force accelerator backward loss setting loss else loss think problem accelerator case code distributed launcher like accelerate,positive
hey general good direction fully aware problem would dig code might able next week,positive
convert format model format format,neutral
said could tell convert format model format format thanks lot,positive
convert format model format format thanks lot,positive
considered signature generate account future however every generate call slow highly undesirable like leave like consider robust method future still,negative
faced exact issue pip install upgrade trick,positive
wonder time variable potentially,neutral
location tutorial based discussion community post context comment comment target repository got,neutral
mistaken native automatic mixed precision entirely accelerate see,neutral
location tutorial based discussion community post context comment,neutral
hey impossible convert without rounding model lose performance switch switching might depending model long give model chance recover rounding however switching inference source distribution drift almost surely negatively impact downstream performance said note indeed better due dynamic precision range excel inference time due better accumulation precision easy answer finally like lora see library get away downcast,positive
please leave start side find optimal update maybe best tuned default value update schedule user really change default value go provide new get back start trying standard summarization maybe look code sort,positive
yes open add even advice would following love certain method work well little moment see two run find whether get away one set update heuristic best input hand method find way make work technique would become thus higher chance correctly used u also great settle final implementation validate different given simplicity technique suspect mostly hardware agnostic,positive
one fix error please,neutral
import torch import import assuming already initiate policy action difference policy procedure average long take complete policy action policy submit draft upo talk question content policy find previously policy legal sufficiency review substantive change know policy last posting policy departmental web page initiate policy action contact university policy office policy policy governing principle outline necessary implement policy rest make sure length assert mismatch number assuming correct combine list zip combined create return create modify represent whether question answer simplicity let assume answer answer else update training loop train model device else device criterion assuming binary classification answer epoch range batch batch device device device model loss criterion print epoch epoch loss model train model train save trained model load trained model prediction function question model question model answer return answer example usage initiate policy action answer print answer answer code getting bellow error model newly probably train model task able use inference recent call last module train model train save trained model train model model loss criterion object attribute,positive
testing via fix issue consumption quite significantly min audio file without word able complete process batch size default consumption gib word batch size best able batch size took around still consumption batch size going around gib,positive
thanks lot offering help keep u page code already runnable directly use pip install new library challenge library choose directly develop model library avoid writing except like way already release hub directly load develop custom model first pretty happy proceed though think necessary,positive
thank prompt response since file make reference disable want generate personal use case share model thousand thanks,positive
copied resolved branch ready review passing,positive
got thanks aware problem indeed feel little bit strange need pad said may lead worse going close issue however evidence degraded performance unpadded audio,negative
made addition would nice use list tutorial otherwise help model hub rather library,positive
close believe behavior likely necessary batch,neutral
also suggest code help access,neutral
correctness test assuming done correctly import torch import import range tensor tensor tensor range print mismatch found default got print tensor else print tensor print current mismatch count based output every value tensor small portion output mismatch found default got mismatch found default got mismatch found default got mismatch found default got mismatch found default got mismatch found default got mismatch found default got mismatch found default got mismatch found default got mismatch found default got also note default tensor float forced float tensor float anything anything misunderstanding issue,negative
correct wrong random input would also require device without loop think still manually casting tensor float call loop would first parameter,negative
thank much detailed answer,positive
issue fixed running following code terminal install,positive
tried crack struggling allot complicated since looking file tried spending different ai try follow code got dont know actually get due format save done back fully functional file someone want help load model model path file load model file update model loaded path text file new one token per line read text file file open add token token save file vocabulary,negative
took look user issue actually something example limitation integration like metric trainer good merge though,positive
hey ray tune integration trainer along remote function case metric along trainer via parameter fix task else metric load metric inside method instead implicitly return,negative
made working implementation based start,neutral
fix ton pesky test,neutral
excellent question first parallelism superb set various ca compare base well solve one specific problem main parallelism modeling code far trivial easy introduce unfortunately seen invisible late fix main zero modeling code code separate user need get modeling code right successful training write code train single zero scale number needing anything specifically claim get throughput long fast internode connection personally seen yet yet given chance run cluster one high speed used far slow given zero sharded data overhead compute previous stage speed fast enough mostly hidden additional overhead math another similar general additionally recently hybrid version zero hybrid fit sharded model single node use speed reduction multiple used except faster version since need gradient shard traffic lead much higher certain size node fit model mixed training wo help link becomes speed even speed slow one le zero important consideration make whether team finish training sooner later parallelism certainly take longer time zero equivalent need train something fully close dev overhead need introduce might spend dev time depending complexity possible zero training still deliver faster outcome le hair lost process also know well awesome give u incredibly useful work combining best,positive
thanks pointer code similar code directly custom model image would like suggest solve,positive
hey thanks agree would like summarize double check align example gave widely usage generate object reliably force generate structured like need constrain generation process let stop necessary object complete two way handle return set instead let generation stop return empty set discussion everyone agree option better option considered exception agree,positive
hi right used class recommend training like like really exist wrote curious mostly relevant anyway result integration quite different integration recommend another model instead rather trying exactly copy may also drop entirely full use quantization result may able run,positive
yep see removed entirely work,neutral
yep raising exception good well le silent side,positive
new default model saving security performance recommend via,positive
hey thanks opening issue would recommend look mistral model support fairly context length linear memory consumption also make sure activate gradient computation,positive
one place need change think currently set logic none sample else raise set generate currently generate greedy search sample suggest change support prompt,neutral
hey urgent ping let wait next week,neutral
like ca would mind main trigger,positive
hey totally feel pain sorry whole documentation open pull request make visible snippet python import import torch model simple resize overwrite content better hope thank much suggestion best th,positive
hey totally feel pain sorry whole documentation open pull request make visible snippet python import import torch model simple resize overwrite content better hope,positive
hello update issue way save bit model,neutral
model parallel data parallel,neutral
see anything failing ran slow locally good,positive
whisper model trained context fixed size spectrogram default spectrogram length give gibberish go beyond since randomly go get since model trained thus follow original always fix spectrogram length expect,negative
hello sorry late reply duplicate,negative
problem ca generate file anyone kind explain thanks advance great work,positive
hi thanks feature possible load one corresponding image classification,positive
thanks lot reminder causal model yet left,positive
hi run integration example failing due see defined trainer class think used anywhere suggest resolve thanks,positive
hey happy open experiment though think right way fix might look either following small constant denominator making instead probably research,positive
thank raising issue already fix would great could open,positive
hi understanding constrained beam search used limited mostly experimental also expensive maintain want avoid expanding unless significant demand going decline offer usual bargain comment looking similar case happy revisit decision,positive
hi thank opening issue part separately issue end lack informative exception would catch fix issue immediately open catch,neutral
input sentence different number need pas append trailing padding wed wrote thanks response one query need length input tensor batch input reply directly view id,positive
hey curious claim get throughput long fast internode connection everyone still throughput thanks,positive
hey regarding feel free ignore skipping pasted end comment regarding flakiness pas time time per model test latter per model failure rate low simply add test decorator comment pointing comment linked former per model failure rate super high may something else must uncover,positive
hi echo comment related issue issue generate rather issue attempt fix problem user might result unexpected behavior instead raise informative exception user set unfeasible set,positive
issue quite old still looking answer load model head instead tried work since model still without head token hence ca simply use decode output model must compatible model generate method output list token,positive
hi thank opening issue addition temperature top top apply three typical epsilon sampling eta sampling similar thing mention apply top transformation probability distribution since already similar backed reluctant add technique without maintenance heavy burden want contain,positive
hi thank opening seen measure time assuming limited number would benefit moreover list generate streaming work instead generate would become would become trivial listed merge limited time existence plus maintenance burden feel strongly please react comment happy reconsider reach,positive
hi read documentation see use method reply outdated link redirection link send error,negative
thanks response one query need length input tensor batch input,positive
hi original specific problem thanks avoid suggest open new issue discus bounding box method observation possible appear backed single let work best way solve problem trying address,positive
yes course right first time seeing erroneously assumed tensor shape constant given might true size certainly true size script well take care soon possible thank attention,positive
hello curious reason like inquire whether continue translation work document,negative
thanks great update confirm tracing work without device pinning local used slightly test script import torch import io import model model tracing work without device pinning model list print print tracing saving done print work fine get following model list registered trace safely ignore warning use function create constant would every time call function case might cause trace incorrect scale copy construct tensor use true rather scale converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize min registered trace safely ignore warning use function create constant would every time call function case might cause trace incorrect copy construct tensor use true rather converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize converting tensor python might cause trace incorrect ca record data flow python value constant future trace might generalize registered trace safely ignore warning use function create constant would every time call function case might cause trace incorrect output,positive
mind explaining attention pad sampling never read real anyway understand would make difference,positive
hello let take step back understand loss function go modeling file check observe thinking task regression task instead classification regression label float prediction change shown default binary classification case training integer model,neutral
hello reduce shape better change sorry solution anyone need type return trainer trainer,neutral
meet problem minimal reproducer mine python import pipeline case audio process audio pipe pipeline return pipe audio dependency version accelerate,negative
hello thank notice look,neutral
thanks look code think able get something running tonight ist,positive
got block issue could please check thanks,positive
thank feedback understand prefer chose use instance two conciseness concise code clutter especially multiple activation global accessibility globally accessible throughout need import every time,positive
sure unfortunately suggest removing decorator test whether built checker hi tested like still get want push,neutral
hi thanks taking look actually work well resulting see given metal assumed think issue may specific model,positive
done extensive research testing subject tracing process forward pinned tensor present code since review yet decided solve luckily intertwine code solution move tensor separate callable exactly sadly provide test case since never sure available someone try done snippet code model import torch import io import model model working yet model print tracing saving done print would love hear feedback code extension task device problem feel like model become quite extensive,positive
hey nothing much different code leave user define new bunch give good well cool barely know python fancy stuff written used like magic use generate train fit add get model clip tried ask similar question specifically filter post explanation wonder dont find one question huge influx new people ai past year feel stuff need readily available write thousand python code code hint possibly either program give text file like would convenient even rather behind learning python write might giving search someone made program add new cant person would want,positive
throw error inference python model stage rest bit tricky resolve reason happening backbone added therefore contain loading model first case method applied one time still consistent part reason wanting deprecate handling annoying see something reasonable convoluted resolve,negative
thank careful review much cleaner,positive
sure unfortunately suggest removing decorator test whether built checker,neutral
want better understand mechanism wrapping know automatically assigned default understanding latter post actually device map later wo work model residual input block added output block given layer device indicate accelerate passing list module split argument understand module split forward pas however sure see connection way indicate class wrapped based limited understanding connection two simply way quickly find name transformer since convention always consistent,positive
found error following work prompt provide part stop use provide directly generate method unsure set used default recognize,positive
gladly thank work always part o project,positive
either resolved please review,neutral
moreover seeing epoch variable reset would possible bug,neutral
thanks explanation wondering would see switching model trained example version llama model case would optimal continue moreover would see model loss degradation switching back tuning thanks,positive
new structure still subject review concrete idea mind technique top straightforward,positive
whole pip source see forward signature thanks,positive
build also thinking issue according documentation list command different see list currently system check like cat work mac mac command would true future mac,positive
nice feature indeed work together least since final model loaded,positive
literally change avoid padding time,neutral
hey always pleasure contribute,neutral
thanks going extra mile fixing fire,positive
hello would recommend try check feeding model also truncation padding done truncation padding method also set directly,positive
need account possible token check model stop set custom processor linked,neutral
admit ca figure without dive offhand get around day two need wait anyway,neutral
maybe check directory first issue also exist empty directory assume mainly seek different directory directory relative empty might return none try check directory addition may use delete empty one one empty directory issue,positive
hi got error import import following error look see import name kindly show problem thank best,positive
yeah change alright mostly pas,positive
also version get install source,neutral
hi executed notebook order able run anyways data well could problem arise instead think problem converted right,positive
hello please provide minimal reproducible example along related launch command,negative
thanks quick reply dev,positive
proper documentation added added,neutral
bit confused response would like chat something interesting human yes model following like chat something interesting human yes please print print tried add also tried use following stopping criterion import class self self bool return prompt text prompt key key value prompt work model stopping output,positive
sorry delay looking week,negative
hey yes recently could print python import print,neutral
trainer accelerate hood surprising go ahead open like look handling accelerate side,positive
fact solution issue accelerate context memory saturated without fix accelerate launch fix accelerate launch python import trainer torch import import import class self super forward self loss return loss loss main parser model trainer trainer main,positive
hey expert believe quantization allow batch size help speed loading model may help run batch size tried import torch import model auto text love pizza e love pizza love pizza text time range print eu pizza la pizza la pizza time user total wall time used peak,positive
hi thanks raising issue error torch ca decode notebook immediately obvious happening see error reading run order statefulness therefore difficult error message could update notebook cell executed order,negative
one last final update failing could add model doc page page sure done,positive
sure like contribute model feel free open ping u ready review,positive
solution would great however think solution eventually accelerate side since handle creation hood,positive
goal still fast without accelerate,positive
hi need string case following line fix issue mask token behave like normal word include space mask change slow token send patch otherwise rely strong case skip fast test,positive
forked need recommend make branch forked transformer forked transformer branch,neutral
wrong perhaps strong word suboptimal would precise llama use see model practice model saved load model format want operate precision superior,positive
hi try run shell pip install like missing file nice thank,positive
generate wo able train suggest review material training replace case following reserve repository feature like invite use forum discord,positive
big deal still issue need quickly submit approval,positive
sorry mean chose rather u le readable,negative
course size matrix new warning general add new special part thus seen,positive
sorry want vision model want text model use rather add,negative
think understand point thanks surely look report find solution think one way fix check code cause model pinning try get rid try open issue,positive
yep sure try find time week,positive
hi thanks raising issue solution good consistent model open happy review,positive
better use slow instead get actual good otherwise token,positive
yeah sorry big super time sensitive pushing back try week,negative
need string otherwise part anyway saying test work skip need good reason like edgy case whatever otherwise loading slow fast hub produce correct,positive
hi look new update item main hope want,positive
hello please also see comment,neutral
main branch observe following resolve issue coupled true default internally accelerate meta device leading directly meta context manager linked issue,positive
additional testing tag thanks responsiveness,positive
thanks response would happy expand solution however quite sure get suggestion right fix issue model think missing something also believe device pinned effect usage place testing code problem since run single test setup actually saved anywhere unless mean complete model use lead pinned device actually check tomorrow might misunderstood something since learning rope sorry advance yes solution correctly open issue absolutely correct device pinning separate issue previously experienced issue device pinning decorator trick model export instead torch script get around comment inspired seeing quick submit solution issue would imagine one way ensure device pinning model instead,positive
hey introduce thus convinced fix simple would mind also reproducer issue might related specific hard soft hi issue thought fixed like could set accelerator think example trainer one reproducer,negative
would great also believe use take account one best metric looking scale delete look bit complex end,positive
issue closed prematurely see comment trainer use instead could please issue disagree simply issue even purely bug obvious clear see corresponding issue simply include class causing bug trainer class least official,negative
thanks response would happy expand solution however quite sure get suggestion right fix issue model think missing something also believe device pinned effect usage place testing code problem since run single test setup actually saved anywhere unless mean complete model use lead pinned device actually check tomorrow might misunderstood something since learning rope sorry advance,positive
think truncation pipeline instead text,neutral
one question special warning special added vocabulary make sure associated word add special resize input token matrix model python resize input token matrix model one problem could know whether newly added thank much best th,positive
model confirmed think pad equivalent pad edit might wrong pad would also note string neither set training model wo stop generating inference since trained output token per since working saw even though model specify pad token string still row token matrix pad token run print model see token matrix index reserved pad token case print model model also see pad token although also discussion seem matter actual pad token work well use following python pad right auto right best th,positive
right randomly would assign new directly change weight example python import torch import model print print new vector added end matrix print randomly matrix print vector shape way received warning special added vocabulary make sure associated word write specific word thank much best th,positive
great work think solution still device model one solution create model instead perhaps might better solution,positive
close completion test pas conversion script also think wise abandon,positive
hey also implementation add library find work believe current implementation bit library could take look make sure claim right want collaborate,positive
custom usage would recommend build top really standard gon na missing unexpected yes though adopt independent clip lie within flexibility,positive
point even ported code really see anything similar current processor good would like see original like,positive
issue potentially line work instance segmentation additionally,neutral
hi think command anything take running command,neutral
thanks final review preview still building think side table content fixed starting wrong section level quantization,negative
realizing example working solution fully written class however point issue class provided namely class expectation work trainer especially incompatibility perhaps incorrect latter count,neutral
take look merge thank,neutral
issue work someone find solution workable please reopen issue closed real solution problem logic training run reproducibility feel perhaps fundamental happening transparently obvious work identical one run got reaching second epoch therefore first epoch one run continued past second epoch clear incontrovertible evidence bug since different training logic happening depending whether true false let put another way agree number training desired number number batch size number desired training number desired number batch size agree number desired training number desired number batch size going true many yet precisely condition upon error least according error message simple mathematical fact problem reason inequality conundrum sense,positive
work setting epoch without train function class self state control,neutral
hello iterable reiterate reaching end number number iterable sorry response make sense issue marked closed number trainer maximum number entire training run however run stop training number le number within single epoch clarify technically correct iterable reiterate reaching end however trainer class handle already unclear fail handle training logic expect run current epoch yet start new epoch reach,negative
hello saw brought conversation keen feature potential enable function calling open source could discus feasibility,neutral
hi tried use without without setting model loaded wrong even whilst warning provided solution fix issue model used whilst found inconsistency trying determine tied without following code list entry three contrast set function return list empty list warning displayed far tell regarding symptom actual cause untied code supposed move missing back meta device move back get left meta device model however contain necessary look get properly next day,negative
trying base version inference speed much original version way lower latency possible run inference batch size memory model following error case error calling handle may know run model high cost like run hence reduce inference latency response would really appreciable thanks,positive
better also white space training data mean feeling good human also feeling good instead feeling good human also feeling use word word code first post use,positive
automatic speech recognition pipeline passing path file see snippet available,positive
saw today think table content displayed correctly would great fix since doc quite big thanks feel free merge,positive
hi thanks raising looking detective,positive
hi thanks opening solution highly specific fix environment variable control logic single issue something going merge right great thing open source modify original wish fork include code,positive
hi answer first question already open model hopefully get merge repository consistency test tried running make still working let know dig,positive
one last final update failing could add model doc page page,neutral
code fairly new correction please let know import torch import time import device else list iteration range record start time without without range result time default float range result time forced time,negative
bug merely step convert label line rid error replace ca decode label result return result score,negative
spirit common processor starting make sure multimodal follow least extent possible similar done testing forward signature,negative
thanks understand set loss masked prompt need modify like,positive
thank mon wrote yep done set doc pas exist reply directly view id,positive
hey setting flag could make sure setting false load cache,positive
yep done set doc pas exist,neutral
shoot aware already feature yes make work ticket official appreciate pointer thank quick response,positive
want train model generate inference time transcription audio train prompt model learn generate intended behaviour thus train text mask prompt also train model see page whisper paper,neutral
list best according assuming last best available output work documentation mistaken stated like latest also resume training easily wrong missing something like separate issue please open another one minimal reproducible example currently given enough u reproduce done issue work someone find solution workable please reopen,positive
sorry blindness thanks thanks work,negative
keep backwards compatibility code could try open hub update let also hear core say course,neutral
yes meant given model one month old could still update alternatively added backwards compatibility let know think work best,positive
convert file array suitable model,positive
defined corresponding counterpart properly set true,positive
fixing issue string call converted instance based file therefore slow token false despite line added instance string,negative
default size make sure,positive
also cover breaking previously whisper happy merge know whisper compatible yes speculative list,positive
hey already see work problem,neutral
yes defined globally used throughout entire improving code clarity reducing redundancy,neutral
based like llama always add prefix space input trying get actually getting stop,neutral
good try read implement agree well maybe could even user provided decision qualified make,positive
hey think possible box custom usage feel free share solution ask forum,positive
hi aiming review today,neutral
hey pretty sure fixed setting python mask token behave like normal word include space else slow try,positive
thanks run local folder valid model identifier listed private repository make sure pas token permission either logging login passing could course check see point reproducer reproduce would recommend make sure shape feeding processor correct doc sequence batch sequence array list float list list list float must mono channel audio stereo single float per example working snippet,positive
hi first thank clever strategy openly simple elegant really great thinking point view core functionality simple core also seeing speed generation candidate proposal open today content generalist technique arbitrary function generate would variant function technique parallel work add technique top generalist technique define technique class none none technique would get triggered generate pattern code nearly ready state different share social medium sound good like,positive
hey would mind link paper code,neutral
comparison bit image bit text image bit model image bit model image model model almost,neutral
thanks since general use case also blue many like well think fix least warning thanks,positive
hi gradient already phi python import model currently transferred print false enable gradient print true want freeze internal alongside please add python false,negative
flag would suggest default numerical code path breaking change whoever actually wo make sense hidden expensive bug,negative
training argument flag provide flexibility still supposed keep code path use case,neutral
error file line else height width height width height width disabled new error file line object attribute line prepare basically try bit understand error loading without transformer wow would slow assume also example code everything every image really would immensely slow made work still guess way work low keep loading would super slow thank,negative
sorry given incorrect code working one fixed,negative
hey anyone else issue running unit test module also configure ignore documentation use case fixed,positive
need point exact line code stuff unable see python auto totally possible given thank reply properly everything error inference exactly breaking line like file line generate real error file line return file line else object attribute set true full code auto pipe auto today today return function return function seed bool seed return seed generate function include batch count generate prompt style bool false seed width height schedule float float bool false bool true print range seed seed generator seed schedule schedule else else raise unknown schedule schedule none type ignore prompt style prompt pipe return seed error code shown create public link set launch recent call last file line output await file line output await file line result await file line prediction await file line return await file line return await future file line run result file line wrapper response file line wrapper response file line generate pipe file line return file line else object attribute recent call last file line output await file line output await file line result await file line prediction await file line return await file line return await future file line run result file line wrapper response file line wrapper response file line generate pipe file line return file line else object attribute exception direct cause following exception recent call last file line response await batch file line raise exception error else none error exception none,positive
fix straightforward set false rely part name like know true currently hello gone issue fix sense training argument flag provide flexibility encourage raise interested thank,positive
need point exact line code stuff unable see python auto totally possible given,negative
dear thank response separate turn conversation check single token exactly currently separating turn feeling good human also feeling white space white space good also add one white space,positive
list best according assuming last best available output work documentation mistaken stated like latest also resume training easily wrong missing something like separate issue please open another one minimal reproducible example currently given enough u reproduce,positive
understand correctly yes processor general fixed signature could elaborate bit processor output part really concern processor output fact clear acceptable since property already model string ideal book ideal name ideal something something aim absolute set pragmatic always beat ideal code ideal code never actually,positive
hello iterable reiterate reaching end number number iterable best example library main code snippet given setting number greater number iterable try next buffer except iter end reset start else false break notice logic exception handling reassign corresponding warning end reset start hope,positive
hello think better idea,positive
version command detect input non existent unlikely issue probably lot version absence,negative
everything need simple tutorial many could achieve want various recommend running diverse set running special might useful test suite verify match,positive
hey thanks would loading casting torch think smaller usually case feel free use,positive
end let update double check,neutral
documentation failing pointed exist deliberate,neutral
leave closed keep every week know ping back reopen,negative
hey thanks raising issue kind hard architecture specific something work well anywhere else specifically given sense rather really favor,positive
hi could give set size height width set example mean file,negative
hey might community found needing feature overall design ca give estimate time might take leaving issue open,neutral
would also possible set size height width image made general making sure square size,positive
recently ruff instead black make quality pas need black pip black update necessary pip install quality make push made,negative
well add currently probably go without problem still kind breaking,positive
hey thanks anyone want open fix meaning probably error decode function feel free low priority list,positive
hey thanks opening issue try keep could ask question forum instead regarding quantization train fully model library way unstable gradient computation probably explain better come back know training script would recommend check library thanks,positive
first come first please feel free open link issue happy review,positive
hey access missing form reproducer similar probably looking need check thoroughly token feed processor check indeed make sure encode raw without prefix space added add option set false soon use word word,negative
hey think version work attribute removed private method breaking change,neutral
hi current code support gradient could support,neutral
hi thanks suggestion dig found line slow mask false however give instance fast token true constructor true explicitly issue fixed test setting false change file outside however comment line reason true mask token behave like normal word include space let know send accordingly,positive
python import import processor model duration audio duration processor output model output,neutral
possible add use class apply work well,neutral
progress eta main branch thanks,positive
issue template wrong requirement,negative
record measure running time minimum time seem correct,neutral
also convenient use copied difference best solution let rather adapt make compatible,positive
failing file get error run would fix hi thanks comment place read repository consistency test match object signature could give going around two failing fixed thank,positive
hi resolved review latest update slow pas seed parameter removed thank,positive
hey working whisper algorithm provide support contribution like help,neutral
failing would great revert change remove attribute since legacy behaviour size used integer dictionary,positive
ready review added two almost code structure respectively transformation repository make work function training work yet train proper model compare performance let know need make script,positive
llama add prefix token sentence input main different training different algorithm different stripping mechanism single space,positive
hey like issue rather,neutral
hey like officially snippet external library call make sure help would mind full snippet,positive
could review sorry wait,negative
thanks open successfully impossible build view locally able ago sure longer possible view locally,positive
see fixed see also notebook need provide image rather original one visualization,positive
hi setting getting torch version default time default param median mean standard deviation minimum maximum forced time float param median mean standard deviation minimum maximum seeing much difference though seeing due precision change data due random generator,negative
transformer incorrectly transformer fixed please ignore comment immediately sorry,negative
ah got mistake actually empty string token added middle added beginning always saw empty string token present naively assumed meant would add extra turn case main confusion stemmed fact across llama like llama implicitly print people print print string people none unlike people remove token id get print people like difference causing behaviour please correct wrong,negative
also getting error several day tried pod new different different always getting error,positive
yes revision fix working issue time,neutral
hey issue silent would like start look,neutral
run set new issue running setup fixed right,positive
clear following error know right way rue rue long set none see,positive
respect image processor present removed clip image processor later original code take parameter scale short side factor scale long side factor like done image addition also align width height final image factor ref hence introduce let know missing something else,positive
thank much worked another question regarding use example example please select correct answer given multiple based given context context oceanography study word oceanology might accurate since ology study graph write map making oceanography surface covered water almost water ocean remote go cant yet much ocean remains unexplored people call ocean last frontier big impact fish marine specie polluting global warming melting thick ice warming water warmer water along water melting ice sea rise many oceanography physical oceanography study water movement like ocean figure marine geology ocean chemical oceanography natural ocean water marine biology marine life question chemical oceanography study human pollution ocean water naturally ocean water rising ocean water ocean floor answer matter come code work add answer need rewrite example different way,positive
working kernel hopefully see better soon,positive
could add warning yes want take stab ran issue would like create warning left call,neutral
hi thoroughly code performance local machine strongly recommend package version ensure compatibility latest dev import import answer collator example please select correct answer given multiple based given context context abrasion another type mechanical weathering abrasion one rock another rock gravity abrasion rock slope moving water abrasion bump one another figure strong cause abrasion blasting sand rock finally ice cause abrasion rock ice bottom glacier scrape rock ever collected beach glass stream work abrasion question gravity erosion following except moving air flowing water mass movement answer pad example print collator output please note fast method faster method encode text call pad method get tensor tensor tensor,positive
left like contribute maybe,neutral
documentation test time intermittent known issue something wrong code,negative
make style still getting test failure,negative
already accept height width format see nothing wrong method see notebook illustration reason pointed internally image hence need shown image rather original image see also notebook illustration hence fix one need update code snippet set based image rather original one address,positive
failing far tell failing due fact currently length returned returned generate search know taking input make towards output generate messy fix would edit return possibly logic multiple search alternatively maybe generate simply graduate student facing final figure good time look tell right track could also use advice last also checked still example original issue topic thanks advance,positive
hi thanks opening delay sure want apply fix fixed sequence length feature think would make sense change setting,positive
case copy instead thanks wrote exist profile believe would saying exist someone tried still would rather copy reply directly view id,positive
regarding document best place would add example model doc page officially could add small code snippet model doc page show pas use merge notebook linked officially hugging face certainly still open add like,positive
hey glad hear able monkey patch fix since need maintain equivalence need set processor numerically match hopefully comment pointer facing lite,positive
would exist profile believe would saying exist someone tried still would rather copy,neutral
thanks opening ideally also allow following size size slack image accept height width change much pro accept something think behaviour,positive
recently added model fix update processor accept target size rather documentation standard format image,neutral
hey thanks would require use check output sent token mask make sense specify list empty would like open fix meaning something like self mask enumerate sent enumerate mask sent sent mask return mask,positive
thank working please make sure green find information,positive
feature would massive advantage,neutral
fix sense wonder open token along pad token might need terminate generation nothing,neutral
thanks would indeed great addition variety hardware community leave,positive
good tend think mask transformation might better rather model class else implementation basically probably last blocker regard modeling yes great point related private understanding first exactly moving logic class mechanism avoid layer however understanding mechanism elegant forward level agree super super ideal issue enough need actually check mask,positive
hi provide minimal reproducer thanks,positive
review last ready need agree regarding last discussion,positive
following discussion last meeting tried line certain level indent inner function class single block however error prone like python class true setup self super python class ignore copied true true setup self super line setup well defined name reliable way map source target simple case might say arrange bit would work guess work special case suggest treat block inner single block sill theoretically reliable work writing code strange format,positive
starting look good tried model took sent initial request first minute spent waiting previous wrap possibility u speed even,positive
hi change already done take,neutral
hello look code include reproducer really help,positive
hey thanks opening would update need make sure support torch latest would like open sure work previous well,positive
hey saying issue without reproducer help anyone feel free open new issue reproducer use external package case issue load something issue open issue,positive
without handled separate see full context,positive
hey indeed opening fix thanks,positive
update team raised make prohibitively expensive practically impossible make use certain alternatively advice working large without due issue mistakenly closed due stale without actually resolved regular essentially forced use perhaps aware,negative
merge whenever time check please ping trigger thanks,positive
time could also look fix bug important feature work properly thanks,positive
thank feedback agree greedy search random feature already solid enough indeed popular use case feature add support bit later working bit write feature example really want test every model generate start atomic like,positive
thanks pointing two yes know important work technique,positive
still waiting solution still willing contribute,positive
hey done deep dive issue link related comment summarize rightly u figure mismatch seeing like say wrong precision use llama,negative
response lade fa flash attention weight quantization,neutral
confirm seeing issue get mistral,neutral
like old post got warning missing model loaded trying restart training training run working base passing trainer think recently attempt migrate maybe warning crept also rather strangely progress training bar remember trainer used forward data loader point left stopped training nothing really happening indeed update reading pretty sure training training rather quickly betting farm batch update expect skip trainer state would saved right given code mistakenly metric calculating metric best model presume mystery skipping question remains training,positive
hey think better move assuming people go mine wrote hi delay getting back also learning model would prefer move original directly duplicate still exist profile reply directly view id,positive
hey training however stuck unable figure convert fast could please link could,negative
ah thanks clarification reading understand fine link share helpful,positive
need maintainer approve run order merge,neutral
suggest break feature multiple starting basic one motivation people collaborate greedy search good start,positive
found running calling pipe method language parameter issue,neutral
patch external active always found issue found connect wireless headset foreign name read special might fixed correct though code tested import import pipeline import import torch device else else pipe pipeline often text often microphone checked new audio print start talking item pipe print item text item partial print,positive
hey thanks opening think implementation follow comment posted see also add support,positive
hi spent time particular version could pasting example collapsible summary lade test script import import import time import torch import o import lade assert device model model validation iter warm end warm range chat role system content helpful model given article role user content next article chat device start end end start print memory print print generation speed post increasing get additional throughput good model small device hard achieve modest consumer fiddling lade able get model model without fa running default actually slows model despite high compression ratio bottleneck work correctly fa output significantly different work manage get setup work case without quantization top post know modeling code model require lot work add maintain limited greedy meaning support common use case technique much like speculative generation may one accepted token per forward pas idea look promising would amazing able speed model without external however current limited oversized task hand addition heavy especially original code also despite limited llama solution positive found technique happy reconsider decision let keep issue open discussion,positive
people happen ladder cross,neutral
hi could please check let know anything need,neutral
still draft hit one failure trying good next week hopefully,positive
hi yep like everything done properly checked like small repository consistency test match object signature following match signature run make fix following contain need fix search failing file get error run would fix output loss shape expect either return loss per sample shape single loss batch shape may get error return scalar loss null shape try classification model,negative
already found done looking model forward signature,neutral
thanks try argument still something understand set still unused column,positive
hi code example use think something simply following example good bash python caption,positive
case check choose one work also assume working splinter enate hi already hence please refrain duplication,neutral
hi circle run example see python true latest main branch accelerate latest version pip install eager nothing extra check,positive
hello excited share feature great shape eager hear implementation feature quite complex aim make compatible beam search sampling relatively straightforward integrate greedy search greedy sampling first question break feature multiple starting basic one would better aim comprehensive solution single merge perspective thoroughly tested greedy greedy sampling might beneficial merge first already cater wide range use additionally facing feature currently setup similar outlined create simple verify accuracy generation however systematic testing approach tricky example want test grammar compatibility running model actual becomes necessary without model might generate nonsensical syntactically correct help effective testing actual lead valid generation significantly slows process appreciate navigate testing continue refining feature,positive
integration pending would like take thanks,positive
added conversion script convert known initially part,neutral
hi thanks lot advice flash attention passing,positive
hi worked added logic add according comment please let know change,neutral
able spot line missing line en ligne en ligne line line utile thanks pointing corrected checked believe,positive
yes correct llama overwrite test see real would great test next make sure use,positive
error bash got unexpected argument model code hub feature used old gradient logic support thanks please refer comment issue issue unrelated issue also fixed switch main bash pip install,positive
still face issue latest,positive
hi also currently another,neutral
thanks wonder fix manner,positive
let know need address anything else,neutral
hey looking current documentation sam think relevant place add probably tutorial create issue done think could considered ready,positive
thanks set prevent happening check script need,positive
think quality coming recent library recently ruff instead black align need black pip black install pip install quality make,negative
hi unfortunately able find time work recently able work near future think hit yet,positive
thread save sanity digging day thought something wrong end trying run zero axolotl library exact issue would extremely useful run bit zero understanding zero way get model parallelism would allow u multiple smaller moment possible run naive model parallelism would active given time although spread across multiple please let know update issue manage solve issue,negative
hi issue cause problem coming padding image padding model within square thus always padding used target size noted please add note documentation example even sample image bounding due,negative
hi basic script initial scribbled idea achieve trying infilling task would helpful,neutral
hi delay getting back also learning model would prefer move original directly duplicate still exist profile,positive
oh sweat draft status somehow,neutral
thanks review draft mode draft,positive
sorry knowledge library lot able open fix probably removing call,neutral
hey seem improve readability wo accept sorry fair enough thanks close pull request since improve readability,positive
hey people let minimize,neutral
like make sense support support common use case ram big enough hold model ca save anyway could come clearly sense support instead ram even big ram also faster,negative
strange ran last month today issue make sure correct version provide reproducible snippet link,positive
hey got ta know function like first time internally try infer computation graph process known tracing might notice generation time fast successive function infer computation graph given function follow shape computation graph initially built problem fixed input must pay attention working variable input shape text see documentation,positive
wondering change logic line python python another possible logical estimate may relevant change section starting,positive
hey thanks opening issue try keep could ask question forum instead think answer use case thanks,positive
ah know input batch got sure want create new file fine loading state need put well guess,positive
thought solution approach implement generation,neutral
would super valuable many require word level,positive
hi know hugging face team help solve issue people issue work author indeed used framework also contact git author another problem lower accuracy validation sincerely hope team help fix error thank much,positive
getting error also able reproduce error example provided also dropping fix issue well,positive
hi slight continuation assumed inclusion language code bug seeing intentionally included question possible language code included output inference practice already know output language code sent model need unnecessary cleaning specific code tried removing generation translation completely possible switch behaviour output want inference translation input text thanks,negative
able spot line missing line en ligne en ligne line line utile,positive
generate like harder challenge individual sequence expanding thus need reorder mask step believe implement need write custom possibly logic happy test drive side intend write efficient beam search,positive
still actively working excitement,negative
learning rate scientific notation decimalization unknown reason,negative
tested time first time library second time kernel repeated test third time sure problem result three image code used python class log self float none print super trainer train test trainer train test naturally executed,positive
thanks much review make sure understand correctly compatibility model saved fit memory possible method state dictionary saving certainly agree would best able save fit memory rather memory currently case general question guide added accelerate sent could implement compatibility feature either,positive
hey thank issue resolved,neutral
thanks already based comment everything alright git fetch git rebase running make style seem change anything git make quality ruff check ruff format check left unchanged python python python,positive
hi ran test time every time inference test fail generation test,negative
try run test multiple time sometimes flaky apart look great end,positive
yes checked passing flash attention found one integration test related opt,neutral
fa pas except odd given test passing test,negative
tried branch work fairly well attention mask llama might due missing logic,positive
think ought try llama derivative mistral zephyr indeed really like use awesome feature,positive
hey sorry late reply currently decided implement scan flax see brief reason happy conversation feel strongly could done generally currently tend view scan specific feature built top library advanced require,positive
interesting would feature also enable sure would separate function somewhere beyond scope besides one able pack multiple series batch way,positive
hey future find time continue want jump back would happy assist final integration done nice job setting case backwards compatibility,positive
sadly wo time contribute flax model would happy assist anyone give integration go could largely leverage flax code could quite fast addition let open one community call flax would like go quick model addition,positive
super sorry delay still list case look otherwise anyone community would like try hand feel free go ahead already review,positive
interesting would feature also enable,positive
hey working whisper algorithm provide support,neutral
hi king issue related used device attention yielding nan fixed,positive
thanks lot great raised review happy hear opinion well,positive
sorry bother need worry gradient training call fail found inside see one inside guide observation correct need add thank,negative
hey model forward already passing attention mask format attention implementation would mind currently might related yeah might make clear current matrix batch explicitly token attend relevant,positive
waiting cache think might worth considering exactly manage configuration especially since associated window size size suppose would better introduce,positive
hey feel free open fix seen time think important fast also slow though sure lot,positive
pip install torch work machine indeed work,neutral
still need store away somewhere assumption given fresh run want run iteration loop find right batch size found prior call still need saved somewhere outside file system,positive
correct part torch applicable torch never always install accelerate base,negative
thank detailed explanation help moving,positive
hey want help provide reproducer error output,neutral
make sure way added token fast slow right absent slow token line getting tackle test mask token differently constructor appropriate rue rue long set none see,positive
sorry late response apparently assumption getting generic load video see memory full pretty sure problem wrong stopped working drive copy machine training instead working path drive happen,negative
hi latest version sorted enumerate see thank much,positive
applied agree full abstraction better long run two minor warn otherwise user would get repeated beam check cache perform check calling,positive
hi latest version sorted enumerate see,positive
well noted issue follow next time also great thanks kindness used lot,positive
hi let see usually add argument like take like take look example main thing guard torch behind something like make sure code crash people one think problem multiple include normalization layer python self module module result iterate exist layer model additional suggest change help value mainly used trainer used training use instead get functionality,positive
hi get error try running make style main command make however like fork little behind main try following run make make style sync upstream fork browser interface local change main pull latest version change back rebase onto main force push push branch pip black pip install quality make sure getting latest style,positive
agreed bit would better create new file instead something like instead made something like,positive
sorry thought bit different setting would better possible enable flexible later future way around would backward compatible backwards compatible new might raised would start flagging unexpected behaviour multiple something user handle side going update add relevant instead,positive
hello thanks feedback tried running indeed pas time time think indeed due effect thread bravo detailed explanation solution addition flaky failure case failure mostly due model specific dive bloom special implementation past key value tensor shape old model different cache format wo fix due implementation past key easy fix reformer old model different cache format wo fix would like know general principle specific one hand could try add code handle specificity also code quality bit energy let know think must saw different skipping cache complete bloom reformer,positive
current reference implementation directly top plan release custom kernel speed method wait kernel opinion wait plus skeptical whether kernel would compatible flash attention kernel see,neutral
match new time stamp processor new time stamp processor wer applied failing test related,positive
thank fast reply generate function decorator unable obtain corresponding output token via function solution use forward call multiple time obtain gradient output token respect input efficient way obtain,negative
include error message running train apparently torch recent call last dae cell line self raise trainer accelerate please run pip install torch pip install accelerate trainer accelerate please run pip install torch pip install accelerate,positive
last nit would mind main make sure correct styling,positive
hey yeah problem let use current path merge slow respond,negative
yeah like would like open,neutral
bounding box problem good question doubt ported code see special code handle original assumed added vocabulary short snippet reproduce,positive
keep track might good cache,positive
case check choose one work also assume working splinter,neutral
hey need help integration,neutral
thank much extensive review left open let know think,neutral
attention look like start first item masked prompt two end mask extra padding token could please explain need mask prompt attention mask mask loss prompt token setting label prompt,positive
hey want know actual python text import text two different different thus produce training distribution example hello hello uncased model thus frequent thus split different,positive
hey model forward already passing attention mask format attention implementation would mind currently might related,neutral
anyone provide train script thanks,positive
training inference would useful used instance,positive
hey think better help,positive
allow use revision finish,neutral
hello would added feature,neutral
hello possible data parallel paradigm meaning get least one sample far know fundamental requirement zero style shading one batch per data parallel definition least one sample per could state claim,negative
maybe know someone team could help solve rate field width issue output,neutral
related issue specifically last example assume related original bounding box conversion function since think worked part note found feel free open description code reproduction separately would helpful,positive
work passing case would passing,neutral
bug fixed evaluation done training,positive
close finishing fa implementation persimmon see also separate package loading dynamically would allow fused fa fused,neutral
hi quite flaky time sometimes,neutral
hey want know possible release training part possible,neutral
hi training training llama loss significantly higher yes,positive
error advice resolved resolved simply kernel make sure put first cell let know go,positive
would first contribution however familiar project help,positive
case could also work fa yet open list,neutral
could find opening community,neutral
thank positive feedback appreciate willingness help opening new would much cleaner way mind approach open providing additional information might need,positive
done please let know made,neutral
know failing anyone let know work resolve,neutral
thanks yeah draft work priority thanks detailed report thanks,positive
thanks work splinter well,positive
thanks removing research future stumble previously closed older officially support present onwards already,positive
feel free take splinter model unfortunately work,negative
hey would recommend install make sure quality check pas long green thanks definitely try,positive
already done according document,neutral
thanks right misunderstood bot reply problem min ago load hugging face model class sorry thanks,positive
hi thanks already done upgrade work issue posted like problem,positive
fit card tuning faced memory issue running single card,positive
thanks everyone discussion need spend sometime figure implement test case,positive
hi suggest handle right hacked view stuff unblock getting weird error something wrong sure error object attribute full attached also code added commit look right thanks advance,positive
might worth fair amount related stuff ended monkey patch various model processor list done model processor sure understanding latest one feed processor value one instance make think something,positive
sorry delay sure tied true set true default however good chance set false model similar solve following model,positive
hi way compute gradient produced generate function respect input,neutral
ensure two interchangeable review order added test confirm two interchangeable converted back forth way however hood use new cache internally would best add long slow test key llama mistral correct confirm attention working,positive
think measuring distance provided generation model would desirable indeed open try help want,neutral
system air anaconda python got similar bug bug import name used pip install working best,positive
suggesting something think solution mind open directly ask review another relevant person working hugging face,positive
could something wrong way pipeline single file output sec note chunk size defined example snippet top work intended model generate tho rocket overall usage remark developer clarify whether transcription strategy used conditional allow choose supporting via pipeline important edit clarify one biggest people use pipeline throw audio file whichever format get nice catch typo added test pipeline well,positive
good way check think better provide let user skip check one want,positive
indeed set pipeline one input argument hood however model processor need set twice time user coupling model processor ca propagate forward automatically,neutral
throw error inference thanks flagging look,positive
good see error documentation build unrelated last week fix main could rebase include trigger another run green good merge,positive
particular solution mind feel free open ping review,positive
hey convention assume user latest version install specify additional run evaluate case need include user installation hope sense,positive
recently library use ruff resolve need rebase main black pip black install necessary pip install quality format code make push applied need force push hi getting error run command make make argument list long make error,negative
failure since limit running multiple large used training advised run ahead time single machine flag command remove flag launch done loaded straight cache wo second time otherwise want waiting use original command flag set number set something large like slightly wasteful since need use,positive
hey could elaborate motivation behind order choice rather bug fix sure choice current bug choice whether allow passing different bug whether reflected moment pas wo reflected returned another option input verification raise error user could implement instead might bit defensive,positive
hi another fix merge later file think subtile distributed evaluation repeated would mind providing useful subtitle update en file file,positive
previous issue import name use command install fix,negative
ignore comment ruff check fix undefined name undefined name undefined name undefined name undefined name undefined name undefined name undefined name undefined name undefined name undefined name found check permission check,neutral
hi however indeed anormal generating generating blank image problem looking weird end sequence token could error something related embed defined related,negative
thanks hard work merge,negative
thanks yeah draft work priority thanks detailed report,positive
cool like great option start,positive
may want update well explaining output,neutral
need pas model equivalence part library would fail running cross also tried passing directly getting desired output hidden maybe something wrong help bit might able help would need information example code run behaviour,negative
throw error inference python model stage rest,neutral
recently library use ruff resolve need rebase main black pip black install necessary pip install quality format code make push applied need force push,negative
hi great work super excited architecture happy help finishing taking new add architecture make sure add main author contribution work let know sound,positive
machine version machine yet may able check sometime week,positive
yes used anywhere code,neutral
affect functionality fine done separately,positive
thanks encounter padding well however also ran exact length padding still got significant variance natural language generation metric output shuffling test set batch size leading different batch,positive
hi would really appreciate anyone provide simple reproducer bug way open fix quickly resolve issue thanks python save adapter retrieve base model model please note way adapter returned without base model loading adapter model model auto change model model error gone,negative
anyway would great library would need make output information,positive
hi could get fix,neutral
think bug incorrect usage saving adapter disk loading leading adapter loaded twice leading multiple would cause error going close think issue would occur used correct way python way adapter saved without base model,negative
already try convert float scientific notation python work di support,neutral
perfect thank working possible would nice delete unused confirmed end relevant passing speculative mean delete replace new,positive
hi think fixed would please review,positive
hey think way log nice need log scientific format,positive
right case prompt different batch related attention always deterministic yes would expect padding input probably right awesome explanation happening available,positive
thanks explaining follow could add explicitly way clearer anyone code subject silently tested handling,positive
let try put sample script,neutral
yes model generate saved problem model parallel,neutral
good catch indeed checked going,positive
time take contribution need properly done able tackle month,positive
yep let expend scope use another feel free open another afterwards,positive
hey would use something like python model filter lambda sum return note try keep kind question forum instead thanks,positive
hey thanks add feature request error getting following python framework torch export favor favor recent call last file line return code none file line code file line module main file line main file line model file line file line raise yet want support please propose open issue,positive
let make sure testing,positive
store model first evaluation round generate yes issue need short script reproduce issue may able provide depending error short script reproducibility would help,positive
hi thanks solution tried shift shown following much modification causal training pipeline training good loss none none shift predict loss look forward fix,positive
see missing one variable many model seen far cache information without sure properly slice without setting assumption one token time would disable use mistral generation newly added ability pas across,positive
case see call prepare regardless looking next week holiday sorry delay,negative
think last keep open track conversion script update,neutral
hi sure try log past key type past key type type output image,neutral
open support anyway another contributor also stuck,neutral
likely something outside need open issue yes tell full log,positive
thanks still bit confused clear explicitly cache set anywhere,negative
hey thanks kind could share full sure integration unfamiliar attribute,positive
hey update version hey like error calling would recommend upgrade recent version also isolate post issue,neutral
custom usage would recommend build top really standard gon na missing unexpected,positive
support added list first post,positive
sure need train new recommend completely losing thus model useless add rather train new one scratch want leverage,positive
hi thank review added mistake meant added hence confusion apology,neutral
hey think lot cool new multilingual require scheme meant person need mostly use multilingual train llama need help question forum sure community help thanks,positive
hello also issue import true test test true get stack trace recent call last cell line import true file token revision loading configuration file cache hash hash detect whether instance return type recent version,positive
hey definitely like good suggestion got first step relatively easy could check first element input figure single conversation list second although might consider backward compatibility third tricky though definitely understand important given arbitrary sure automatically template,positive
two different redirection work work old new fail old name shown wo become green merge given complexity said generally try avoid suggest remove rename keep sorry noise,negative
hi thanks reply output given batch maybe clearer way illustrate unexpected given ruby import load llama model bit model prompt hey conscious talk prompt give recipe porridge prompt weather today prompt prompt prompt prompt generate print print loading different response prompt prompt batch change happen load model,positive
also problem class agent self prompt stop result role user content prompt return result message content however since access unable test code someone job,negative
could help issue would appreciate help,neutral
would also need support chat,neutral
hey lingy aware however since share script reproduce issue best ask u,positive
hey following reserve repository feature like invite use forum discord still believe bug code check guide since first issue u going share get best trained embed whole sentence used standard generative see sentence similarity task page check want use say average hidden simply last token code,positive
hi thanks quick response false even following looking get,positive
hi think small test different case missing anything,negative
interesting see faster without two version torch confirmed whether two without input,positive
hi everyone resolved main bear mind need remove method avoid future support old removed,positive
thanks yes would awesome edit indeed already added see message,positive
yes deal issue feel free think case,positive
hi type list default llama hence true,positive
hi lingy see exception object confirm type exception none whether model true,positive
ping stuck ready review,positive
according response need merge turn red green maybe make call case,negative
regarding example situation main currently unless redirect,positive
train see sequence classification inference yes generative use inference time see model variable type major effect model natural consistent across afraid rather modeling following reserve repository feature like invite use forum discord,negative
trying run model model getting model anyone please help predict model giving output hand normal taking one another one tried multiple still getting correct output anyone please suggest correct need give model thanks advance,positive
hub would better however merge could always create internal copy point test,positive
logic start mi intention add new default first open new add new logic leave message able continue,positive
hi astern draft get finish really working,positive
good way check think better provide let user skip check,positive
perhaps explain found problem fix necessary situation let say model model model ordinary backward step training phase time another momentum model whose set false momentum model weighted sum model transferring model two still vocabulary table may change word silently false true able find problem error module used loss,negative
update review sorry forget change two line best,positive
way might want review synchromesh use improve generation code,neutral
way guidance although seem like easily define text file grammar programmatically,positive
link issue faced issue trying load llama model issue similar type goal behind key key key make sure replace version broken tested quite bit would make sure version patch intent need key key key since want use exist unless actually want retrieve based instead latest,positive
along accelerate package also issue pretraining llama scratch quickly failing loss issue issue resolved know exact reason behind though incorporated many help much case,positive
trying get bottom issue fresh pip install dev name version summary flash attention fast exact attention author tri dao license location torch installer pip language python license license operating system ran following showing whisper assert false false true assert false assert false mistral raised raised bark support data type unrecognized configuration class class kind support data type unrecognized configuration class class kind false true llama pas falcon get error might due fact running memory might sufficient output pip freeze post post,positive
hi would possible merge made lot progress would helpful test pipeline,neutral
hi issue still open would like take compatible,neutral
still open contribution would love help,positive
could also use model generate text try next time thanks,positive
always looking way improve code please hesitate share,neutral
thanks would mind small snippet testing suite sure thing however notice set accept see line change allow test instead write test another already set accept expand scope useful allow passing however different behavior far sorry bothersome question,positive
yes true handling python messy particularly considering provide context one reason provide clear error within library many additional optionally certain functionality pillow image explicitly user need install package,positive
use example suppose one beam search starting prefix need check present code would pack batch shape run mask passing mask expanded internally way would batch shaped mask would shape look like tensor subsequent beam search mask reflect past new attend mask need pas intact memory cache thus beam search similar inference like longer limited another use case kindly,positive
would enable word level whisper inference right would love see another possible,positive
ever get work thing model gibberish end able get whisper correctly learn new could,positive
hello shall post request saw second post training custom however fix used switch back regular train longer seem like would much effect issue page decided post well also similar issue help get model train information code correct maybe leave comment author issue seeing got work anyways thanks ill post,negative
add reference output loaded layer linear linear linear linear dropout dropout linear linear wo linear dropout dropout dropout dropout,neutral
thank reply agree would weird repeat language especially word chunk reason language language shown top level similar text shown best,positive
sure still want work feel free close,positive
big deal go back version rename simpler,neutral
generally tend avoid experience safely perhaps could advise,positive
fault rename file consistency problem locally also issue please let know better way deal kind situation also saw ko translation add,positive
incorrect behavior true well talking core would like open long like ca likely say guess quite least,neutral
need green merge like check failing run locally fix,negative
oh right spot bad,negative
feature request bug probably related fact language small chunk full sec chunk audio language seem make sense also start end language would bloat code think potential solution worked ago,negative
breaking backward compatibility landed file bug major problem lack provision backward compatibility model loading,positive
review tomorrow meeting explain discus together might easier review,neutral
sure said considered finite iterable finite iterable find iterate twice twice precisely true iterable batch technically create two iterate would question library would disagree question role trainer argument according doc set positive number total number training perform case finite iterable training may stop reaching set number data exhausted may wrong range finite iterable since calling next enough time two either documentation unclear least bug training stop example instead,negative
ah thanks confirming fix user take look,positive
hi fix issue however note respect new gradient use code hub define method done support automatically thanks attribute fix would remove method done currently pas,positive
rip sure way fix without bug,positive
hey data collator meant custom usage,neutral
person added change test last commit current test look unrelated,neutral
sure said considered finite iterable expectation finite take minimal example python import torch import import data range data range batch print batch find iterate twice twice would question library,positive
hi thanks detailed issue example intended simple follow adapt instead trying cover every use case training model also little complicated setting example multiple also important correctly sample per language especially number per language balanced different way use case would better fork training script modify according need working example would awesome could share thanks also hi since issue originally raised especially rise interest multilingual think support multilingual training think would important allow multilingual effectively stay multilingual even unfortunately still relatively new sufficient experience contribute pull request,positive
quality recent update use ruff code could update make quality pas pip black pip install quality make,negative
doc still looking although put redirection,neutral
hi team try transformer version get error accelerate pip install accelerate latest version pip install pip install downgrade get key error mistral anyone help exactly issue whether use,positive
could also use model generate text,neutral
correcting ping addition call working moment yet indeed,neutral
hey could share behind independent feature extractor clip text model generating query though similarly model instance process around transformer need avail clip separately,negative
believe help bit fixing,neutral
hey might seeing effect run test multiple time single model list failing always get failure test stable flaky see whether test flaky install pip install run since randomly random well different test flaky mismatch probably due effect threat beginning comment quick test,negative
future want random people yes let go one scalar,negative
thank contribution please make sure quality pas guide like need run make style hi reminder,positive
sure look need add doc open support revision think,positive
hey thanks lot opening issue try keep could ask question forum instead sure community help otherwise follow tutorial train whisper model see thanks,positive
tried running notebook however found different error local ray instance recent call last self value try value except serialize self value else return value self value self value raise finally self value value protocol file return dump self try return self except pickle object exception direct cause following exception recent call last cell line maximize self direction none else self direction none run self trainer direction run self trainer direction return trainer direction self trial trainer direction analysis none run name metric mode stop verbose restore resume none ray wrapper return ray return return wrapper address storage hook hook flush self else return return wrapper return ray return return wrapper put value try value except self value raise must first place construct python entry local could serialize put value object object fail serialization pickle object function fail serialization pickle object global cola module metric name glue value value usage compute glue evaluation metric associated glue list score translation list list translation reference list depending glue subset one several accuracy accuracy score correlation spearman correlation correlation print print print round round print fail serialization pickle object bound method metric name glue value value usage compute glue evaluation metric associated glue list score translation list list translation reference list depending glue subset one several accuracy accuracy score correlation spearman correlation correlation print print print round round print fail serialization pickle object bound method object fail serialization pickle object function warning find object bound method object may oversight variable bound method metric name glue value value usage compute glue evaluation metric associated glue list score translation list list translation reference list depending glue subset one several accuracy accuracy score correlation spearman correlation correlation print print print round round print name glue value value usage compute glue evaluation metric associated glue list score translation list list translation reference list depending glue subset one several accuracy accuracy score correlation spearman correlation correlation print print print round round print found may multiple undetected consider either removing moving scope check information improve error message please reach ray deprecation error fixed,negative
new feature request would good different honestly open accelerate side probably since use provide want modify lot need keep way general clarity specifically wanting would great,positive
hey sorry really sure help code snippet wise fix pip install upgrade,positive
know provided link notebook minimal reproducer would still might related latest want look,positive
would love see community,positive
pas sure intended test learning rate list rather scalar fail several warning like list scalar value extracted like value tested later everywhere else like scalar extracted,neutral
hey thanks anyway share getting goal behind key key key make sure replace version broken tested quite bit would make sure version patch core issue quick,positive
hey entirely sure code run evaluation provide full reproducer model use properly set could share full reproducer usable make sure properly also really sure true distillation,positive
hey think best path add create instance class something like python import pushing hub converted loaded yes appropriate name used plus constant ca model saved see,positive
hey thanks new token initialize average layer best would mind trying,positive
hey given tensor based input type mostly given use would suggest disable quick fix otherwise check type casting come really time dig code minimal reproducer would also,positive
good state week slow fast document already something like python import true first,positive
unrelated test fixed main,positive
first rebase main run make automatically ported similar yes copied well testing affected,positive
hey read documentation set flag legacy false wo get fix slow fast already fixed python import true replacement first llama set none proper fix coming,positive
thank contribution please make sure quality pas guide like need run make style,positive
hi thanks providing issue resolved,positive
seeing failure accelerate since run got added test data type class,negative
hey would mind latest version provide reproducer model,positive
hey idea feel free take lot integrate attention layer similar way recommend check,positive
phi recently fixed main close providing reproducer ca really help feel free open new issue reproducer,positive
thanks use code see use instead see import would keen keep ideal still think next release make official case used next quite soon finally regarding removing favor unified let separate really related variable harmonization could last message approve think good merge current version thanks advance,positive
stress rush whole community really appreciate work one,positive
still getting test try upstream main branch forked development machine pull latest main branch change branch rebase finally force push resolve everything,positive
know took long similar fixed something close slows lot fixed rope copy llama use dynamic scaling,positive
another addition disable probably read discussion see considered relation already,neutral
hi thing unable help giving code integrate,negative
good point case let merge hub open support revision,positive
hello might fix issue could please try let u know,neutral
thank explanation one nit question table like wondering still usage,neutral
hi still able reproduce run snippet confirm end python import torch import model print model main difference setup main try update package pip install,positive
hi thank raising issue generate full input prompt even case input prompt full conversation script replace input generate something along result expression please introduce hello ai language model help might may assist today fact need full conversation may obvious taking issue opportunity write documentation feature clear,positive
hey attribute whisper hub try access get default length correct length update note current method generation related generation find length indeed set correctly python import model print print output going forward generation generation update purely backwards compatibility note train whisper sequence longer whisper maximum positional length filter longer training script,positive
hey would mind performance bigger worst model worse algorithm remember observing little loss large sure run well sure though see strong gain smaller add nevertheless,positive
would passing whole audio input result usage audio fly loop,positive
please provide reproducible code snippet reproduce bug,neutral
generally problem pas attention explain use case little bit want pas attention,negative
check precedent direct integration also compare directly fine setting new pipeline,positive
hi still face issue latest version pip install,positive
hi thanks review clear fixed would please review,positive
issue regarding attention mask fixed longer affect forward training done instead think pretty much overflow significantly slow training might best use float,positive
hi case dont issue without sorry late response,negative
understood dive deep done code rather slow affect fast one could come configuration made directly hub like length argument padding token id argument also set certain point happen two bit hard guess wrong community might able help leaving open,negative
hey thanks opening issue try keep could ask question forum instead sure community help thanks go forum ask,positive
dear review together discus translation discord channel,neutral
hey thanks opening translation fire pretty big change text generation documentation match one would mind sorry look item quickly material need detailed comparison thank know let keep touch,positive
hey would mind performance bigger worst model worse algorithm remember observing little loss large,negative
hey feel free ping ready review green need help,positive
hi thanks review would please help check merge,positive
hey nothing much different code leave user define new bunch give good well,positive
thanks worked would expect would work well index want use pipeline still get transcription case worked transcribe curious going behind scene thanks,negative
yep model probably went wrong point part code long due quick fix recent better,positive
hey find open model feel free inactive quite time ping author make sure alright taking still want contribute unless inactive think alright work,positive
hey open check doc would recommend still open draft mode ping doc render make sure rebase main help hard without looking code,positive
label good second issue anyone tackle,positive
last suggestion done three day ago let merge good go,positive
little test compare performance function without parameter declared float float tensor setup iteration two loop run time first loop random tensor size function second loop parameter time loop execute time every iteration machine default time default param median mean standard deviation minimum maximum forced time float param median mean standard deviation minimum maximum someone else run machine apparently better mine default time default param median mean standard deviation minimum maximum forced time float param median mean standard deviation minimum maximum run time typical forced float appear take slightly longer execute mean median time float forced consistently higher though significantly time without parameter curious see corroborate,negative
hi know nucleus ai unrelated maintenance going tight,negative
believe part code would include argument prompt argument remove prompt well else block strip prompt response,neutral
hi review latest made based deadline side would appreciate help make faster,positive
hi training training llama loss significantly higher,positive
hi wondering could please review seen hope good time believe many occur would like already know failing since doc test allow load flag load since long time execute,positive
hi sorry could open week try complete,negative
right randomly would assign new directly change weight example python import torch import model print print new vector added end matrix print randomly matrix print vector shape anything past would one custom self trained model add help training split know multiple turn damage already generate garbage,negative
tried upgrade version think training loop stuck evaluate triggered,neutral
running code snippet method find state whereas state never used think cause issue might well still need investigate least randomly even start model,negative
dormant take one let know one,neutral
problem connection alternative like version,neutral
problem falcon model sequence classification one device auto option added line worked perhaps somebody,neutral
main branch open separate pull request code parser image,positive
thanks opening could separate code parser image separate sure,positive
hi mismatch related classification layer namely whose randomly loading model assume since model hence layer depend,negative
hi translation work folder translation keep many title origin format class double check best,positive
able still resolve issue,positive
issue automatically marked stale recent activity think still need please comment thread please note follow likely still waiting review hope get process,negative
hi know nucleus ai,neutral
add python file take inspiration,neutral
believe ready review made quality pas also added integration used deformable use,positive
sound reasonable thanks sense reset grammar state user continue generation one minor correction rule generating string grammar string instead string,positive
hi still open contribution thanks,positive
regarding state grammar processor consideration currently state think may useful reset state every generation could allow user continue generation see code example user start new generation new instance indeed also add reset method make would expensive parse grammar afresh every single call get point though user goal start another generation scratch grammar afresh think way avoid user goal continue generation example solve problem would need start scratch simply continue old state sound reasonable regarding design choice put state inside sure best way would like opinion python import import import import logging streamer model open file grammar root valid string valid string shopping cart prefix grammar generation grammar constraint output output print valid string shopping cart name continue generation constraint need use output previous generation input next generation reuse parser state output output output print valid string shopping cart name price description want generate another valid string create new empty parser state use prompt input grammar output output print valid string shopping cart name price description,positive
thanks removing problem simple prompt prompt sample employee record still see warn log line warning empty stack grammar processor reset state one call calling grammar processor expectation would expensive parse grammar afresh every single call fix issue unless bug include state transition machine,negative
hi thanks know reckon already share another link talk,positive
current forgot handle edge case length fixable refusing contrastive search broke work expand passing think output size hidden part compare entire broken temporarily resolved work doc reading soon,negative
one problem adjust data collator supply example,neutral
thank generate also work work getting snippet code tried gave shape invalid input size import torch import true model llama print model,positive
problem seeing assertion error run run occur since also tried passing directly getting desired output hidden maybe something wrong help bit,negative
able reproduce strange behavior actually bug rather behavior grammar object string value string value last basically model generate arbitrary white space new line object white space break syntax may desired behavior removed grammar work correctly surprise model pick finishing object maybe added token list due bug correctly grammar implementation,positive
thank read source code think need real script line found remove content try except block try block also let think without python raise error originally raise error think superfluous hand python dynamic language ca check import statically text many flexible conditional branch grammar like try except else see source code consider grammar else,positive
figured leverage override policy,neutral
hi try reproduce try use transformer version flax model unfortunately got unexpected argument sure thanks,positive
great job thanks tackling big dense doc,positive
hi glad hear arch done see still outstanding also need alongside currently model like library soon mention busy propose continue one someone else help finish complete already one suggest focus let u know need help,positive
would passing whole audio input result usage,positive
thank filling model beautifully,positive
case going hand case last time,neutral
hey sure follow need fix end test case added python import import torch model auto print print true share short reproducible script failure actually need set least trigger error code reproduce issue python import import torch model auto print,positive
hi thanks raising issue right understanding example line code expect error script equal original script without knowing code running full error message script relative modeling guess issue could provide code snippet run relative model could link original script seeing whether module relative absolute error raised configuration,positive
thanks testing back open resolve issue side note need follow installation original llama use model install use model directly latest,positive
hi suspect issue come fact llama grouped query attention dummy model llama except number ran python import torch import model print script work main help create small reproducible snippet issue,negative
prompt prompt sample employee record get response model output token limit id name age salary department id name empty stack,negative
thank testing look issue way feature run python import import model open file grammar root valid string valid string shopping cart prefix prefix output decode output output print valid string title theory text theory type text text theory type valid string shopping cart name price price price price time could try call via confirm problem remains work may related specific implementation try fix,neutral
closed ca could open new review,positive
rebase main last week resolve,positive
thanks work following piece code python streamer model open file grammar root grammar prompt difference nuclear fusion fission response prompt output get response nuclear output till token limit please note specify custom get pretty valid output difference nuclear fusion fission response nuclear fusion fission two different occur nucleus atom nuclear fusion process two atomic nucleus combine form nucleus process tremendous amount energy used source power like nuclear fusion common example nuclear fusion reaction inside sun nuclear fission process heavy nucleus two lighter nucleus significant amount energy process used generate electricity nuclear power however also potential catastrophic properly summary nuclear fusion fission involve nucleus atom differ number nucleus involved type reaction place,positive
hi tried left le inconsistent right still way retrain model left,positive
theory yes practice interested complexity generate unless widely community ability maintain limited seen request standard bargain comment actively looking case happy include person th reaction please ping,positive
hey sure follow need fix end test case added import import torch model auto print print true share short reproducible script failure,positive
hi thank use model modeling page please kindly review,positive
feel free good shape,positive
hi got working end without change added whisper test must revert set going detail entire remove replace following added bottom file bool model mask defined length else return mask mask mask return model defined length none return return replace code update assistant next round replace code prepare model replace code prepare assistant model special case assistant like else make compatible use even complex,negative
thanks would mind check finish merge think update file learn best,positive
model processor anyway forcing specifically transcribe translate model never choosing maybe ignore calculating loss model prediction training loss calculation model potentially focus learning core like content accurately instead task already example current scenario whisper model next step given list possible audio example actor remove list previous add item remove item list previous masked model still attention think task also affect model accuracy certain task,positive
language token model prediction source audio language mask model trained differentiate translate transcribe tell language audio inference time task transcribe task token desired value kind generation perform model forced perform task specify,positive
holding review quite would make removing applied reflected running make code,neutral
hi thanks much update usage across appreciate team prompt action matter keep fantastic work,positive
hello probably missing new line end grammar try root object object string value string value value object array string number true false null array value value string number instead root object object string value string value value object array string number true false null array value value string number let know work,negative
happy see also working research project related also working research project related would mind u zoom chat time may spark new,positive
thanks explanation task like loss calculation model language silence detection loss task clear training impact loss calculation since model loss le informative example token token training step give loss token loss calculated based probability token mostly trained transcribe task loss slightly higher actually task token incorrectly,positive
hi thanks response explanation see issue common problem one thing would like point trained sequence classification right default value left even inference right default value also tested model bit original precision float inconsistency something retrain model minimize inconsistency,positive
shift input right get shift input attention mask right get mask mask set ignore loss mask prompt set ignore loss prompt included input model condition prompt masked loss model trained predict prompt sense intuitively want train model prompt target transcription indeed refer page whisper paper see,positive
oh wait sorry got mean supply together right previous prompt,negative
think data collator may suitable since dealing loss understood achieve data collator need replace previous prompt simply removed training process useless simply replace initial prompt model able access information contain loss loss calculation step due whisper implementation way mask loss loss calculation step certain token,positive
think way actually data collator per,neutral
hello would like failure terminal differ size must match except dimension size got size tensor number list differ differ differ differ differ differ differ differ differ differ differ differ reveal subset generate different sequential beam search beam search though difference big recent phi llama mistral identical tried locally confirm beam search deterministic across sequential beam search also deterministic across indeed differ may difference interesting question give running different batch size like thank,positive
hey tried running get following error bash recent call last file line module state file line state file line state name false file line string index range,negative
thanks providing snippet resolve issue,positive
see already draft exactly addition starting research project working text generation explore text generation potentially area community post might interesting,positive
think great idea compatible,positive
hey coming back request finally usage everything close issue thanks,positive
hi thanks lot loading model bit bit work box simple instance make sure select type made quick example made sure work,positive
seed solution global setting make model training reproducible global different conflict right,positive
idea extra work internally,neutral
hi already function doc call complete reproducibility import code load model generate code would solve use case want avoid complexity generate unless strictly,positive
great addition simple way get probability language well quite easy original model found way make work implementation,positive
hi long story short way avoid effect matter numerical precision order read comment twitter thread comment mostly whenever modify shape input add padding,positive
python import try import import except import import content content print content import import see thing try block,neutral
sure try please send,positive
thanks opening could separate code parser image separate,positive
running dev version accelerate ca guarantee compatible run find,neutral
would desirable compatibility repository work direction custom library original see implementation compatible,positive
unfortunately package dependency week fix main see get working thank patience looking issue final build documentation run finished merge,negative
hi thanks raising issue recent logic use ruff instead black could try ruff run pip install ruff,positive
cool like passing resolve error merge sure done,positive
thanks following file python accelerate dev source container image code torch import import import import torch import import import import import import import import accelerate import auto print loaded device else print printing device print device print loading model true model load model directly print loaded model work,positive
officially python find search docker hub hugging face one compatible library found running pip install find install compatible warn possible based different need mistral,neutral
also suggest container image docker hub use running,neutral
hope often say thanks work far change,positive
able irrelevant docker image ca built one issue testing time even build image manually,neutral
code assistant model soon learn thanks approve,positive
could provide even fake tiny replace public hub,negative
model interesting would love collaborate,positive
torch version need use mistral,neutral
python version cause problem also suggest container image python test,neutral
hi thanks raising issue could make sure follow issue template provide minimal code snippet reproduce error running environment respective still correct linked useful however able really need know precisely error detailed description bug unexpected behaviour error well tried far exactly respective still correct,positive
system python version tried error inference test code import o import import thread model infer prompt prompt print generate streamer thread thread print streamer print thread id new text print prompt prompt state united thread prompt thread prompt error recent call last file line file line run file line return file line generate return file line self file line return file line output file line forward file line return file line forward file line return file line forward layer file line return file line output file line forward file line return file line output file line forward output file line return file line output file line forward file line return bias state file line apply return super type ignore file line forward output mat mat,positive
agree got issue ran instance even still issue basically code module one folder overall model another folder hypothesize saving saving duplicate full thing try remove save disk space guess case removing except tail layer luckily setting false fixed hope figure fix yes use issue also gon na fix version issue full,positive
version platform python version version version accelerate version accelerate found version true version na flax version na version version script yes distributed parallel script yes function precise script like following format import model true model model range llama according create use pip install build replace torch torch suit version python session print torch torch import torch print print import print import print print import ignore import error,positive
train led python summarize prompt completion epoch epoch,neutral
hi also code base would please help merge,negative
many thanks kindly review help check unsuccessful lost image detail log image even ran make quality main head oh collision collision would would left unchanged run command style,positive
hey get script working trying well would appreciate example script,neutral
need functionality pipeline would like take go,neutral
initial post intention contribute model personally working model lot recently however definitely open collaborate maybe could work together,neutral
bu way case zero stage trainer use gradient stuff pas even though stage,neutral
agree got issue ran instance even still issue basically code module one folder overall model another folder hypothesize saving saving duplicate full thing try remove save disk space guess case removing except tail layer luckily setting false fixed hope figure fix,positive
hi addition also file added,neutral
think transfer merge maybe work later sure think wrote pull request let know ready last step reply directly view id,positive
thanks explaining thought mutually exclusive,positive
