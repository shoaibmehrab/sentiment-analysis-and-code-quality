comment,sentiment
could someone take look pull request maybe,neutral
chance fixed support python thanks pull request address,positive
chance fixed support python thanks,positive
hi sorry slow response big question single right answer unfortunately hate kind response true text identification version problem classification could run gamut simple linear logistic regression naive deep neural recent transformer classification particular canonical certainly performant example naive classifier image analysis huge swath although meaningful day convolutional neural layer backbone classification would generally form loss unfortunately documentation like think pretty good correspondence popular deep learning might check well get familiar,positive
thanks raising comprehensive notebook appreciate right like version beta transpose rather beta dimension mismatch prediction try push fix shortly thanks,positive
hey thanks response implementation comparison naive classifier well library already code naive model per issue brief file comparison performance model comparison already model say comparison part significant added functionality feel free include like entire repository look forward much wrote hi thanks sorry response also issue suspect may confusion explain code implementation gone detail given already naive model leave unless significant added functionality thread reply directly view,positive
hi thanks sorry response also issue suspect may confusion explain code implementation gone detail given already naive model leave unless significant added functionality,negative
also please let know improvement pull request happy work repository learn best feasible way,positive
issue please refer pull request,neutral
hi thanks correcting implementation learnt lot pretty satisfied model round lastly thank lot,positive
sorry kind busy get back latest another time,positive
closed via think would great generalize multiple new issue,positive
awesome thanks try take look soon,positive
thanks nice feel free merge grateful great support,positive
awesome thanks took liberty fit update bit make bit cleaner store inverse data covariance matrix fit efficiently update later update method also expanded little bit verify correctness instead model new verify model match gold standard implementation look let know think everything good think ready merge,positive
hi thank much go get back weekend,positive
awesome welcome addition think general strategy sense going go closely soon thanks,positive
thanks first glance great going reserve time go thoroughly shortly,positive
hey thanks thorough look brief summary module rather keeping single hanging model think might original log likelihood believe correct version please go make sure agree expanded unit test included model good start thanks think actually masked implementation particular testing multiple random revealed accuracy current implementation actual class rather revealed bug log posterior calculation expanded documentation provide better overview model please feel free make ask agree implementation happy model performance happy merge,positive
plan testing look import import import import import import seed seed loading ground truth model update model zip print score must similar implementation,neutral
final version look import import import class self none none update self ym first run algorithm allow batch data matrix form input print else allow single row column input error ym print error testing seed seed loading,positive
seen better way touch impart exist logic class file mind distinguish batch mode would update beta field new method update call new data beginning call new data call update modify beta field update model manner self none none update self ym first run algorithm ym else error ym error,positive
think would descriptive relation source code currently working,neutral
code well build project hard python system lot want mess raise soon snapshot expect,negative
thanks yes clustering model would great addition decide implement hard soft propose within model object choose version use via also reminder please include standard implementation algorithm help verify correctness,positive
would great welcome addition difficult would add object rather create new one could choose fit algorithm introduce new method self used method,positive
amazing first glance great pretty work right going look weekend,positive
good thanks much catching fixing,positive
also added commit close,neutral
oh interesting thanks tip need look like could much cleaner pretty busy day impatient would like submit happy consider,positive
yet would like contribute model would happy consider,positive
oh see saying right square norm want proper penalty gamma beta gradient gamma beta make fix thank much pointing,positive
sorry quite understand penalty case need square penalty square case saw term penalty like image derivative,negative
general rule like reserve making feature able provide detailed explanation logic may find following page helpful,positive
whoop get hasty regularization penalty gamma beta beta gradient gamma beta case recommend explicitly writing norm trying derive gradient beta quickly become clear term,positive
read article understand implement detail,neutral
thank reply image term gamma beta term gamma beta beta think previous comment gradient penalty beta gamma beta gradient penalty beta gamma beta proportional gamma beta actually thought term gamma beta beta gradient term sometimes thought norm sometimes brain norm term likely mess figure clear left problem multiply beta case since gradient penalty gamma beta confused,negative
linear regression term gamma beta beta gradient penalty beta simply gamma beta keep mind gradient penalty term penalty use special ide unfortunately display sphinx see documentation build source directory may also ide try render aware,positive
sorry totally forgot implement today starting implement lasso,negative
package finally available minimal installation documentation available current expanded coming day,positive
work like touched stage would need master outstanding set,positive
think great suggestion going try get shape pip package shortly bump complete update install,positive
would great would feel comfortable package plan put,positive
line code think think right code please,positive
wow thanks much chance take look yet time weekend feel like happy review otherwise try address shortly,positive
line code import maybe delete function,neutral
line code import right code line import line,positive
maybe fork create pull request,neutral
line code backward self default true function code missing argument resulting error line,positive
line code backward self default true code missing default value resulting error line,positive
moment would something love include point,positive
thanks complement yes try create model test,positive
absolutely would great able test implementation see example let know hit,positive
hi delay absolutely right bug thanks ton catching think proper thing would pas depth argument method rather rely attribute push fix momentarily,positive
bug alright yet another reminder never code thanks pointing,positive
hey thanks lot report sorry slow response course absolutely right happy accept fix feel like writing one otherwise try fix shortly thank,positive
code talking work see,neutral
hi took look see code copied directly unacceptable submit reflect earnest model absolutely clear fine reference code fine blindly copy without attribution plagiarism appreciate eager contribute trust contribute key word pull request wrapper around someone else code contribution still want work suggest first trying understand mathematics behind model work towards implementation,positive
hi thank working clear random data generation would result since receive input data perhaps missing something anyway feel free submit try work code together identify going difficult know right certain passing since know model code like finally help track cause recommend directly ensure individual data way two help better identify failing thanks,positive
hi took time finish test pas probability model model predict accuracy know sometimes good sometimes good think result related distribution randomly data think code think test code python import import import random load model import import import import import true generate ignore split error data class continue generate param kernel linear tol fit predict judge error assert print print else print error test code run result shell error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error error,positive
preliminary version model unfortunately suspect going use sizeable performance first let know decide try,negative
hi finally look right update code momentarily,positive
sorry please check code fix bug thanks,negative
hi see closed decide bug happy take look still think error,positive
look great think pretty much ready go see sigmoid optimization,positive
good analysis method best balance conceptual clarity memory usage think possible mitigate conceptual confusion grad function clearly second derivative logistic sigmoid way everything clear,positive
looking code strong idea could implement library vector store back forth array,positive
like ca use training oh well,neutral
like exactly looking familiar lot computer science guessing looking edit reading paper magnitude return almost instantly memory efficient lazy loading directly disk instead load entire model memory wow thanks recommendation,positive
want able keep explicit nature code like one library apart guess hard strike balance efficiency clarity example sigmoid activation python grad self return code lot clarity easy digest versus method python grad self return longer clear hidden inside equation could something like method python grad self return clearer read already code would suggest,positive
thinking library unused disk trained trained like good idea end definitely consider think could quite useful number different model sparse evolutionary training layer currently dense matrix might look magnitude package used potentially relevant,positive
thanks looking forward reason interested great sparse training use case training eats lot memory loaded memory time necessary since small fraction trained update step thinking library unused disk trained trained,positive
thanks sorry chance take look shortly,negative
finally one last caveat interested training word model highly recommend library like since make use model component code meant clear straightforward often come expense efficiency,positive
sorry question almost usage documentation right first bad news moment require little bit extra end particular need implement either negative contrastive estimation loss hierarchical loss latter could use module good news actively working writing loss object hope push also probably include convenience layer make bit faster update thread ultimately two place able write relatively straightforward model see model object might look like look either,positive
thanks definitely right inefficient memory intensive neural network module general feeling fine long obscure actual math behind calculation clearly heavily side inefficient explicit opposed fast obscure though think many probably false dichotomy example included think optimization totally like work say use discretion atomic review basis,positive
sure take time let know,positive
decide try algorithm think need time job finish soon possible nice meet,positive
need apologize implementation would list crux algorithm properly suspect decide would worry much efficient focus making everything possible rather clever also end writing code please make sure cite important code submit work finally thanks let know go along,positive
thanks ur review rewrite method individual code immediately individual sincere effort original work declared inspire determination thank give sincere apology previous bad work time try best finish correctly original possible also soon possible,positive
hi sorry delay review pas left specific addition two general want emphasize please double check make sure code submit least one bug code would easily caught tried running example important code represent sincere effort original work particular directly copied based implementation function documentation recognize simple difficult implement cleanly way resemble expect able write unique code want however code represent rather someone else,positive
general comment like right documentation copied directly need merge update documentation next thanks,positive
hi thanks first glance great take closer look soon,positive
general comment like right documentation copied directly need merge,positive
ping finished take look thank think need help grad co loss test grad function,neutral
ping finished take look,neutral
hi thanks interest actually model part module though explicitly specifically object argument classifier regression feel free propose interested working though,positive
order avoid package way need first discus implement regularizer general commit everything,positive
cool except simple want update new major feature need discus please let know,positive
sorry let sit need think best way include regularization basis need loss able access regularization layer add formulation also need adjust appropriate layer stage particularly bad need make sure proper place time later week work also general comment prefer avoid directly torch code documentation possible realize simple like really single way write obviously use discretion fine fact compare think focus trying work come behavior big goal project supplement like providing explicit transparent discussion,positive
great perhaps cosine loss depending enthusiasm sophisticated like triplet loss temporal classification loss would awesome though reason put,positive
think correct line gradient regularization penalty penalty penalty calculated note regularization penalty beta gradient vector partial regularization penalty dimension beta exactly gamma beta said comment make realize sloppy lining gradient expression according line assumed regularization penalty squared norm penalty modify momentarily,negative
correct change tested got following screen shot regularization seem break anything however loss went slightly however may mean nothing since also tested code regularization similar current regularization implementation put implement change,negative
see closed thinking loss function,negative
thanks le keep activation since think already covered majority used modern deep learning compelling reason add paper architecture good let know otherwise think probably best keeping,positive
maybe refer alpha dropout create unit test version,neutral
python shorthand matrix multiplication see,neutral
hi thanks submission general notice code library order consistent rest modify model expect instead order demonstrate code sound please include unit implementation model module like might relevant please include brief documentation style public model style much possible try follow pep style method function variable ensure readability code black see general contribution let know,positive
suggestion correct must forgot change testing model case anyway pretty unreasonable consider sort test file instability never seeded different different thus different fixed seed parameter class within ensure reproducibility across think assertion saw due mixture component caught properly added small code guard think address though please check let know finally went code rather parameter fit method code address,positive
update test test stable ask discussion,neutral
ran test test stable,neutral
get another error recent call last file line module plot file line plot ret file line fit file line file line file line raise equal tolerance mismatch array array,positive
could run another time encounter error change could den den,neutral
start change line model pas test could please take look copy,neutral
could please create unit test maybe refer,neutral
please check latest update employed weak version bilinear like dilated convolution bilinear convolution best performance approximate overall also feature original equivalent coarse bilinear interpolation method feature dilated convolution bilinear interpolation maximum sampling rate see theorem show simple use convolution work,positive
hi nice work stage implement like system computational graph functionality looking something like see amazing love support syntax similar functional future yet one item list,positive
great made documentation remove expanded thanks,positive
fix part linear hard sigmoid exponential test test plot test follow plot could look issue,negative
beautiful fixed small unit ready go,positive
agree problem suspect numerical precision issue personal computer ca check right try go detail tonight thanks import sure accidentally ton weirdness circular writing individual unit take look detail tonight,positive
great taken quick glance basically perfect go bit detail tonight able merge shortly,positive
create unit test base example thanks finished going could include unit see hacky reference would help ton also refer pull request example unit test,negative
leave linear exponential hard sigmoid test linear exponential need evidence contain hard sigmoid,negative
hi update test test part activation test part new file test add test version found running test first running test problem file error failure import name recent call last file line raise file line file line return file line file line return name file line return spec file frozen line file frozen line file frozen line file frozen line file line module import file line module import import name remove line problem run function problem error recent call last file line file line file line file line import file line module import file line module import file line module import file line module import import name think problem right import method comment part testing target testing sigmoid activation testing activation testing tanh activation testing activation testing activation ran test found test pas set decimal function default setting think still work,positive
awesome think set last thing ensure function appropriate unit test forward pas gradient step merge,positive
yeah le major difference code wo backward method implement model,positive
basically kind way building model right,positive
hard sigmoid way function fixed,negative
general want implement model probably want following python self initialize network store within reliably iterate forward backward forward self perform forward pas specific model architecture come play since need define early flow subsequent backward self perform backward pas route take network specific particular model architecture,positive
unfortunately really good documentation point list likely take time lot document particular case two might go building full network section general code going quite slow comparison code readability speed efficiency said think great idea simple show code,positive
already fix hard sigmoid function think better beautiful way write grad function decided create new pull request talk pull request removed one,positive
include one file fact artifact neural network module much smaller would better migrate relevant file appropriate delete entirely though time yet anticipation went ahead file since contain real really sure even ended name first place,positive
thanks finished going could include unit see hacky reference would help ton start new pull request unit test include one file,positive
linear affine perhaps make linear activation instance affine fixed slope intercept mean still create linear class use affine cool linear affine perhaps make linear activation instance affine fixed slope intercept mean still create linear class use affine cool could something like python class linear affine self super self return linear,positive
thanks finished going could include unit see hacky reference would help ton,positive
linear affine perhaps make linear activation instance affine fixed slope intercept mean still create linear class use affine cool,positive
cool fixed commit review code style python update,positive
awesome two quick linear special case affine need implement separate function actually already layer rather activation function decided much efficient calculate gradient activation projection together rather sequence otherwise good look spare moment,positive
add documentation based documentation,neutral
great think also worth looking,positive
great thanks lot added unit separate agree feel free commit merge master thank commit,positive
great thanks lot added unit separate agree feel free commit merge master,positive
whoop good catch thanks,positive
