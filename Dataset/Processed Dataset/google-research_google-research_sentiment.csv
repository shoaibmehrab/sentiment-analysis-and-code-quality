id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
2002573529,"In [kws_streaming/layers](https://github.com/google-research/google-research/tree/master/kws_streaming/layers) including [svdf](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/svdf.py) I avoid using any custom TFLite operations including fused [TFLite SVDF](https://www.tensorflow.org/lite/guide/ops_compatibility#experimental_operations). 
There are several reasons for that:
1 Benchmarks of [svdf](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/svdf.py) was not that bad in comparison to fused SVDF.
2 Fused SVDF works only with TFLite, but we need to run SVDF on TPU, GPU, etc. [svdf](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/svdf.py) does not use any special op and can be executed/compiled for any hardware.
",avoid custom fused several bad comparison fused fused work need run use special hardware,issue,negative,negative,negative,negative,negative,negative
1982789995,I've contacted the author. He said only CPU is supported right now. GPU support may be released in the future.,author said right support may future,issue,negative,positive,positive,positive,positive,positive
1979992452,"@rybakov The code is still not working, removing references to internal variables made the code crash at later points in the pipeline. I don't think the code works properly with the current version of tf-nightly, is it possible to update the code so that it is compatible?",code still working removing internal made code crash later pipeline think code work properly current version possible update code compatible,issue,negative,neutral,neutral,neutral,neutral,neutral
1979991124,"@rybakov I'm also finding that the kws library doesn't work on the available versions of tf-nightly, regardless of whatever replacements I can find for deprecated code, the code crashes (I thought it didn't before because it was training successfully, but it crashed after the training part). Can you help update the code so that it works with the current version of tf-nightly?",also finding library work available regardless whatever find code code thought training successfully training part help update code work current version,issue,positive,positive,positive,positive,positive,positive
1974750790,"> so anyone knows how to calculate `RealNVP Loss`? @amughrabi @SJoJoK

sorry, idk.",anyone calculate loss sorry,issue,negative,negative,negative,negative,negative,negative
1972979482,"> @VictorDominguite I have a similar question as well. Do you have any findings or progress to share?

Hi @kismeter, unfortunately, still no progress.",similar question well progress share hi unfortunately still progress,issue,positive,negative,negative,negative,negative,negative
1972374199,@VictorDominguite I have a similar question as well. Do you have any findings or progress to share?,similar question well progress share,issue,positive,neutral,neutral,neutral,neutral,neutral
1968147685,I also have this problem. currently no environment to run KWS sample,also problem currently environment run sample,issue,negative,neutral,neutral,neutral,neutral,neutral
1964464963,"I would like to follow up on this. Will a checkpoint be available. If not, when we train our own model, do we train on just the synthetic data? Whats the best way of setting this up? ",would like follow available train model train synthetic data whats best way setting,issue,positive,positive,positive,positive,positive,positive
1963992694,"Hi @gyd-a what was the Root cause and solution to the issue? Facing a similar issue
",hi root cause solution issue facing similar issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1963204641,"@rybakov kms lib not working at currently latest nightly version, would you help solve it",working currently latest nightly version would help solve,issue,positive,positive,positive,positive,positive,positive
1961742770,The issue was with the model. I think its was not clear from the documentation on the use of model. The paper makes it clear on prompt design when using CoC with chat interfaces.,issue model think clear documentation use model paper clear prompt design chat,issue,positive,positive,positive,positive,positive,positive
1953980712,"How do you do object detection based on LVIS text embeddings? I cant seem to follow through in the demo, as it makes use of custom text embeddings, which I dont want to use.
",object detection based text cant seem follow use custom text dont want use,issue,negative,neutral,neutral,neutral,neutral,neutral
1953711614,"Hi is there any fix for this? Able to run on CPU, but its very slow!!",hi fix able run slow,issue,negative,positive,neutral,neutral,positive,positive
1953679989,"Hi @zhszysrh , I am also looking to do test my custom images and want to computer the text embeddings for my own images. Are you able to finish it ?If yes, Can you pls guide me the steps you followed?
@mcahny ,Do I need to train my own data with few samples of images and corresponding labels in the right format to get the embeddings for custom dataset? I'm new to the Zero Shot/Few shot models. Your guidance will help a lot. 
Thanks in Advance.",hi also looking test custom want computer text able finish yes guide need train data corresponding right format get custom new zero shot guidance help lot thanks advance,issue,positive,positive,positive,positive,positive,positive
1951345812,@rybakov would you please help confirm which tf version works?,would please help confirm version work,issue,positive,neutral,neutral,neutral,neutral,neutral
1951295069,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/google-research/google-research/pull/1942/checks?check_run_id=21699652174) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.",thanks pull request like may first contribution open source project look pull request need sign contributor license agreement view invocation check information date status view section bottom pull request,issue,positive,positive,positive,positive,positive,positive
1950043844,"Hello @rybakov  

it seems that tf_nightly-2.3.0.dev20200515 cannot be downloaded anymore. I used the last google research package release with TF 2.14 and I got the same issue as mentioned here : https://github.com/google-research/google-research/issues/1753.   

Could you please either tell us where to find the tf_nightly release or which TF release has been correctly been tested successfully with the last kws_streaming ? 

thanks a lot !",hello dev used last research package release got issue could please either tell u find release release correctly tested successfully last thanks lot,issue,positive,positive,neutral,neutral,positive,positive
1929343182,"Hello, I have the same issue as well. Could you please provide support for python 3.11?",hello issue well could please provide support python,issue,positive,neutral,neutral,neutral,neutral,neutral
1926328468,"@rybakov thanks for your advice. However, the previous logs are the result of creating a fresh virtualenv with these nightly version. I'm not sure if there's any other dependency that I need to handle.",thanks advice however previous result fresh nightly version sure dependency need handle,issue,positive,positive,positive,positive,positive,positive
1922943462,kws lib works at HEAD of tensorflow; tensorflow_addons and tensorflow_model_optimization. Could you create a fresh virtualenv and install all of these with nightly versions?,work head could create fresh install nightly,issue,positive,positive,positive,positive,positive,positive
1916736696,"Hi @sadeepj ,
Thanks for the quick answer.

Correct me if i'm wrong but when considering the scaling, the bias term is not negligible
For 30K samples, it's a bias of $1000 * \frac{2}{30,000} = \frac{1}{15}=0.06$.

This is amplified for a more data-efficient version of 5K, resulting in a bias of $\frac{2}{5}=0.4$! 
This is not negligible when you consider the gap between the 5K and the 30K metric in Figure 8 where the gap between the 2 data points is visually 0.03 at most. 

I am missing something here?
Thanks for the help!",hi thanks quick answer correct wrong considering scaling bias term negligible bias version resulting bias negligible consider gap metric figure gap data visually missing something thanks help,issue,negative,positive,neutral,neutral,positive,positive
1915649091,"Hi @nicolas-dufour, thanks for reporting this. This code implements the minimum-variance version of MMD estimator as explained in the function docstring: https://github.com/google-research/google-research/blob/583d3178157a3dc1eaec04935387ec797004f09b/cmmd/distance.py#L35. We did this in our codebase since most MMD implementation out there implement this version and we did not want to introduce a confusion.

Note however, that the unbiased version and the minimum-variance version are almost identical as explained in the docstring (for COCO 30K benchmark m = n = 30,000). We will explain this in the paper.",hi thanks code version estimator function since implementation implement version want introduce confusion note however unbiased version version almost identical coco explain paper,issue,negative,positive,positive,positive,positive,positive
1907048614,"Hi @rybakov, thanks for the suggestion. I updated tensorflow_model_optimization, which solved the above issue, but was still running into problems where the code was referencing tf._keras_internal, which doesn't seem to exist. Maybe I'm using a different version of tf_nightly, or something else is out of date, but I've gotten the code to work by either removing or replacing occurrences of tf_keras_internal. Let me know if there is an alternative solution.",hi thanks suggestion issue still running code seem exist maybe different version something else date gotten code work either removing let know alternative solution,issue,positive,positive,neutral,neutral,positive,positive
1902627454,"https://huggingface.co/google/t5_11b_trueteacher_and_anli 
 The TrueTeacher dataset contains model-generated summaries of articles from the train split of the CNN/DailyMail dataset [(Hermann et al., 2015)](https://proceedings.neurips.cc/paper_files/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf) which are annotated for factual consistency using FLAN-PaLM 540B [(Chung et al.,2022)](https://arxiv.org/pdf/2210.11416.pdf). Summaries were generated using summarization models with different capacities, which were created by fine-tuning T5 [(Raffel et al., 2020)](https://jmlr.org/papers/volume21/20-074/20-074.pdf) on the XSum dataset [(Narayan et al., 2018)](https://aclanthology.org/D18-1206.pdf). We used the following 5 capacities: T5-11B, T5-3B, T5-large, T5-base and T5-small.

The Dataset is available in 2 locations: Hugging Face datasets and a public Google Cloud Storage Bucket.



> Information from Documentation",train split factual consistency summarization different used following available hugging face public cloud storage bucket information documentation,issue,negative,positive,neutral,neutral,positive,positive
1902075843,the [group_repeated_entities_into_nested_entities](https://github.com/google-research/google-research/blob/8274ab08df1aca9eae1c09efd36d23716d40d040/vrdu/benchmark_utils.py#L338) is completely wrong. this so disappointing that researchers manage to write such a messed up code. this function accept a list of entity names and their values and group the line_items by their occurrence. it works when all the keys are present if a line_item contains only two items in the middle of the list this function just put them in the wrong group and all the groups afterwards are wrong   ,completely wrong disappointing manage write code function accept list entity group occurrence work present two middle list function put wrong group afterwards wrong,issue,negative,negative,negative,negative,negative,negative
1893875246,"> hi,can you give the unprocessing_srgb_loss code,I run the process.py to process bayer RGGB into sRGB imag on DND dataset.But the color of the restored image is not right.

Can you share me DND dataset, thank you!",hi give code run process color image right share thank,issue,positive,positive,positive,positive,positive,positive
1890759916,"Today, I tried to run this notebook on google colab, and it fails at pip install, is there working versions of modules in requirements.txt as notebook with the all working versions of modules to share with me please?",today tried run notebook pip install working notebook working share please,issue,positive,neutral,neutral,neutral,neutral,neutral
1887400032,"Hello does the answer work for you. I am having the same problem. I have tried model.save_weights() but even with that, I am having another issue when loading the model",hello answer work problem tried even another issue loading model,issue,negative,neutral,neutral,neutral,neutral,neutral
1887022483,@waan1 The gguf files are not part of the official release by Google. They only work with [candle](https://github.com/huggingface/candle). You can find the instructions [here](https://huggingface.co/google/madlad400-3b-mt#running-the-model-with-candle). There's also this [thread](https://huggingface.co/jbochi/madlad400-3b-mt/discussions/4#654d0fc6e8c5f79f5d0d9f6e).,part official release work candle find also thread,issue,negative,neutral,neutral,neutral,neutral,neutral
1886947266,"could you provide example how to use qualtized gguf version?
for some reason it does not work with oobabooga text generation",could provide example use version reason work text generation,issue,negative,neutral,neutral,neutral,neutral,neutral
1886747017,"same issue on m1. please provide support for mac. 
`ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann`",issue please provide support mac error could find version requirement none error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
1882973534,I tried recenly tf-nightly version(**2.16.0.dev20240108**). But still same error.,tried version dev still error,issue,negative,neutral,neutral,neutral,neutral,neutral
1880522035,"i found the emb[:83] is different from each other, but should be 1 background and 80 classes?",found different background class,issue,negative,neutral,neutral,neutral,neutral,neutral
1880306626,"I can not reproduce either, the unseen bbox AP50 still has a large margin with the paper result.",reproduce either unseen still large margin paper result,issue,negative,positive,positive,positive,positive,positive
1880115309,"There is another issue with the evaluation of the line_items. Each line_item contains multiple keys, such as ('channel', 'program_start_date', 'program_end_date', 'program_desc', 'sub_amount'), but the order of these keys in the dataset is inconsistent. for instance in the sample ""08b27cc0-dcaf-2f19-0c24-975a6a9c6e45.pdf"" we can see two order of the keys in the same document. 
the function [get_matching_result_per_doc](https://github.com/google-research/google-research/blob/8274ab08df1aca9eae1c09efd36d23716d40d040/vrdu/benchmark_utils.py#L396)  disregards the match between the Ground Truth and the extraction if the order of the keys does not match. For example:
gt_entity_item:
(('sub_amount', 'channel', 'program_start_date', 'program_end_date', 'program_desc'), [(...)])
(('$450.00\n', (...), [...]), ('WWJ ', (...), [...]), ('05/27/20 ', (...), [...]), ('06/02/20 ', (...), [...]), ('The Late Show\n', (...), [...]))

ex_entity_item:
('channel', 'program_start_date', 'program_end_date', 'program_desc', 'sub_amount')
(['WWJ'], ['05/27/20'], ['06/02/20'], ['The Late Show'], ['$450.00'])

which is just a weak method of assessment.
This can be avoided by sorting the keys in both the Ground Truth and extraction before matching.
I hack my way around in this branch:
[https://github.com/minouei-kl/google-research/tree/minouei-kl-patch-2](url)",another issue evaluation multiple order inconsistent instance sample see two order document function match ground truth extraction order match example late late show weak method assessment ground truth extraction matching hack way around branch,issue,negative,negative,negative,negative,negative,negative
1878341548,I Liza hereby accept the CLA agreement on behalf of myself,hereby accept agreement behalf,issue,positive,neutral,neutral,neutral,neutral,neutral
1877345500,"> > > Hi @heyuwen123 , Apologies for the inconvenience. Could you please share the project name for which you need the dataset? It will help me check and address the problem.
> > 
> > 
> > Sorry, I didn’t explain the problem clearly. It’s the data set link in the flare_removal (How to Train Neural Networks for Flare Removal) project: https://research.google/tools/datasets/lens-flare/ Open to display Page not found Sorry , this page could not be found.
> 
> Hi @heyuwen123 ,
> 
> No problem at all! Regarding the dataset for the flare_removal project, it appears that the link provided is currently unavailable. However, you can find the required dataset through alternative sources:
> 
> 1. Google Dataset Search: [Lens Flare Dataset](https://datasetsearch.research.google.com/search?src=0&query=lens-flare&docid=L2cvMTF2MHgybnJfbQ%3D%3D)
> 2. Papers with Code: [Lens Flare Dataset](https://paperswithcode.com/dataset/lens-flare-dataset)
> 
> Alternatively, you can try searching for the dataset directly on the Google Dataset Search platform by entering the name in the search bar: [Google Dataset Search](https://datasetsearch.research.google.com/)
> 
> I hope this helps!

Thank you for your help！",hi inconvenience could please share project name need help check address problem sorry explain problem clearly data set link train neural flare removal project open display page found sorry page could found hi problem regarding project link provided currently unavailable however find alternative search lens flare code lens flare alternatively try searching directly search platform entering name search bar search hope thank,issue,negative,negative,negative,negative,negative,negative
1876563193,"> > Hi @heyuwen123 , Apologies for the inconvenience. Could you please share the project name for which you need the dataset? It will help me check and address the problem.
> 
> Sorry, I didn’t explain the problem clearly. It’s the data set link in the flare_removal (How to Train Neural Networks for Flare Removal) project: https://research.google/tools/datasets/lens-flare/ Open to display Page not found Sorry , this page could not be found.

Hi @heyuwen123 ,

No problem at all! Regarding the dataset for the flare_removal project, it appears that the link provided is currently unavailable. However, you can find the required dataset through alternative sources:

1. Google Dataset Search: [Lens Flare Dataset](https://datasetsearch.research.google.com/search?src=0&query=lens-flare&docid=L2cvMTF2MHgybnJfbQ%3D%3D)

2. Papers with Code: [Lens Flare Dataset](https://paperswithcode.com/dataset/lens-flare-dataset)

Alternatively, you can try searching for the dataset directly on the Google Dataset Search platform by entering the name in the search bar: [Google Dataset Search](https://datasetsearch.research.google.com/)

I hope this helps! ",hi inconvenience could please share project name need help check address problem sorry explain problem clearly data set link train neural flare removal project open display page found sorry page could found hi problem regarding project link provided currently unavailable however find alternative search lens flare code lens flare alternatively try searching directly search platform entering name search bar search hope,issue,negative,negative,negative,negative,negative,negative
1876380837,"> Hi @heyuwen123 , Apologies for the inconvenience. Could you please share the project name for which you need the dataset? It will help me check and address the problem.

Sorry, I didn’t explain the problem clearly. It’s the data set link in the flare_removal (How to Train Neural Networks for Flare Removal) project: https://research.google/tools/datasets/lens-flare/ Open to display Page not found Sorry , this page could not be found.",hi inconvenience could please share project name need help check address problem sorry explain problem clearly data set link train neural flare removal project open display page found sorry page could found,issue,negative,negative,negative,negative,negative,negative
1875968556,Thanks for the feedback. Now the demo response jsonl is fixed.,thanks feedback response fixed,issue,negative,positive,positive,positive,positive,positive
1875510373,Hi@Arm0222 What could this issues be about there is no description which can explain what the issue is about kindly give details thank you.,hi arm could description explain issue kindly give thank,issue,positive,positive,positive,positive,positive,positive
1874995603,"> _No description provided._

Hi @Arm0222 ,
I noticed the task with issue number #1876 on the project board, and I'm keen to assist. Could you please provide more details about the specific issue you're facing? Understanding the context and requirements will help me investigate and propose an effective solution.",description hi arm task issue number project board keen assist could please provide specific issue facing understanding context help investigate propose effective solution,issue,positive,positive,positive,positive,positive,positive
1874989747,"Hi @heyuwen123 ,
Apologies for the inconvenience. Could you please share the project name for which you need the dataset? It will help me check and address the problem.
",hi inconvenience could please share project name need help check address problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1872630791,"@AustinCStone, I just found the implementation differences. First, to match the `resampler_flat_gather` function with PyTorch's `grid_sample` it is required to set `align_corners=True`. Second, Tensorflow's `meshgrid` function performs differently to PyTorch's `meshgrid` function. To match the behavior of Tensorflow's `meshgrid` in PyTorch it is required to flip the last dimension.

```python
delta = torch.stack(torch.meshgrid(dy, dx), dim=-1).flip(dims=(-1,))
```

With these changes to the RAFT TorchVision implementation, I am able to match the flow prediction of the Tensorflow implementation (see below).
![Screenshot 2023-12-31 at 01 04 24](https://github.com/google-research/google-research/assets/34400551/5c77ed1c-e59d-48d0-bb23-6781003fbee7)",found implementation first match function set second function differently function match behavior flip last dimension python delta raft implementation able match flow prediction implementation see,issue,negative,positive,positive,positive,positive,positive
1872254289,Small update: I traced down that the disparity between implementations occurs within the sampling of the correlation volume. More specifically the function `resampler_flat_gather` leads to different results than PyTorch's `grid_sample`...,small update disparity within sampling correlation volume specifically function different,issue,negative,negative,negative,negative,negative,negative
1871587363,"I ported over the weights to PyTorch using this script:

```python
from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file, py_checkpoint_reader
import torch

tf_to_pt = {
    # Context encoder
    ""feature_model/cnet/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.convnormrelu.0.weight"",
    ""feature_model/cnet/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.convnormrelu.0.bias"",
    ""feature_model/cnet/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.conv.weight"",
    ""feature_model/cnet/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.conv.bias"",
    ""feature_model/cnet/layer1/layer_with_weights-0/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer1.0.convnormrelu1.0.weight"",
    ""feature_model/cnet/layer1/layer_with_weights-0/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer1.0.convnormrelu1.0.bias"",
    ""feature_model/cnet/layer1/layer_with_weights-0/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer1.0.convnormrelu2.0.weight"",
    ""feature_model/cnet/layer1/layer_with_weights-0/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer1.0.convnormrelu2.0.bias"",
    ""feature_model/cnet/layer1/layer_with_weights-1/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer1.1.convnormrelu1.0.weight"",
    ""feature_model/cnet/layer1/layer_with_weights-1/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer1.1.convnormrelu1.0.bias"",
    ""feature_model/cnet/layer1/layer_with_weights-1/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer1.1.convnormrelu2.0.weight"",
    ""feature_model/cnet/layer1/layer_with_weights-1/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer1.1.convnormrelu2.0.bias"",
    ""feature_model/cnet/layer2/layer_with_weights-0/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer2.0.convnormrelu1.0.weight"",
    ""feature_model/cnet/layer2/layer_with_weights-0/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer2.0.convnormrelu1.0.bias"",
    ""feature_model/cnet/layer2/layer_with_weights-0/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer2.0.convnormrelu2.0.weight"",
    ""feature_model/cnet/layer2/layer_with_weights-0/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer2.0.convnormrelu2.0.bias"",
    ""feature_model/cnet/layer2/layer_with_weights-0/downsample/layer_with_weights-0/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer2.0.downsample.0.weight"",
    ""feature_model/cnet/layer2/layer_with_weights-0/downsample/layer_with_weights-0/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer2.0.downsample.0.bias"",
    ""feature_model/cnet/layer2/layer_with_weights-1/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer2.1.convnormrelu1.0.weight"",
    ""feature_model/cnet/layer2/layer_with_weights-1/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer2.1.convnormrelu1.0.bias"",
    ""feature_model/cnet/layer2/layer_with_weights-1/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer2.1.convnormrelu2.0.weight"",
    ""feature_model/cnet/layer2/layer_with_weights-1/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer2.1.convnormrelu2.0.bias"",
    ""feature_model/cnet/layer3/layer_with_weights-0/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer3.0.convnormrelu1.0.weight"",
    ""feature_model/cnet/layer3/layer_with_weights-0/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer3.0.convnormrelu1.0.bias"",
    ""feature_model/cnet/layer3/layer_with_weights-0/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer3.0.convnormrelu2.0.weight"",
    ""feature_model/cnet/layer3/layer_with_weights-0/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer3.0.convnormrelu2.0.bias"",
    ""feature_model/cnet/layer3/layer_with_weights-0/downsample/layer_with_weights-0/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer3.0.downsample.0.weight"",
    ""feature_model/cnet/layer3/layer_with_weights-0/downsample/layer_with_weights-0/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer3.0.downsample.0.bias"",
    ""feature_model/cnet/layer3/layer_with_weights-1/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer3.1.convnormrelu1.0.weight"",
    ""feature_model/cnet/layer3/layer_with_weights-1/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer3.1.convnormrelu1.0.bias"",
    ""feature_model/cnet/layer3/layer_with_weights-1/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer3.1.convnormrelu2.0.weight"",
    ""feature_model/cnet/layer3/layer_with_weights-1/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.context_encoder.layer3.1.convnormrelu2.0.bias"",
    # Feature encoder
    ""feature_model/fnet/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.convnormrelu.0.weight"",
    ""feature_model/fnet/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.convnormrelu.0.bias"",
    ""feature_model/fnet/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.conv.weight"",
    ""feature_model/fnet/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.conv.bias"",
    ""feature_model/fnet/layer1/layer_with_weights-0/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer1.0.convnormrelu1.0.weight"",
    ""feature_model/fnet/layer1/layer_with_weights-0/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer1.0.convnormrelu1.0.bias"",
    ""feature_model/fnet/layer1/layer_with_weights-0/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer1.0.convnormrelu2.0.weight"",
    ""feature_model/fnet/layer1/layer_with_weights-0/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer1.0.convnormrelu2.0.bias"",
    ""feature_model/fnet/layer1/layer_with_weights-1/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer1.1.convnormrelu1.0.weight"",
    ""feature_model/fnet/layer1/layer_with_weights-1/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer1.1.convnormrelu1.0.bias"",
    ""feature_model/fnet/layer1/layer_with_weights-1/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer1.1.convnormrelu2.0.weight"",
    ""feature_model/fnet/layer1/layer_with_weights-1/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer1.1.convnormrelu2.0.bias"",
    ""feature_model/fnet/layer2/layer_with_weights-0/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer2.0.convnormrelu1.0.weight"",
    ""feature_model/fnet/layer2/layer_with_weights-0/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer2.0.convnormrelu1.0.bias"",
    ""feature_model/fnet/layer2/layer_with_weights-0/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer2.0.convnormrelu2.0.weight"",
    ""feature_model/fnet/layer2/layer_with_weights-0/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer2.0.convnormrelu2.0.bias"",
    ""feature_model/fnet/layer2/layer_with_weights-0/downsample/layer_with_weights-0/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer2.0.downsample.0.weight"",
    ""feature_model/fnet/layer2/layer_with_weights-0/downsample/layer_with_weights-0/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer2.0.downsample.0.bias"",
    ""feature_model/fnet/layer2/layer_with_weights-1/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer2.1.convnormrelu1.0.weight"",
    ""feature_model/fnet/layer2/layer_with_weights-1/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer2.1.convnormrelu1.0.bias"",
    ""feature_model/fnet/layer2/layer_with_weights-1/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer2.1.convnormrelu2.0.weight"",
    ""feature_model/fnet/layer2/layer_with_weights-1/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer2.1.convnormrelu2.0.bias"",
    ""feature_model/fnet/layer3/layer_with_weights-0/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer3.0.convnormrelu1.0.weight"",
    ""feature_model/fnet/layer3/layer_with_weights-0/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer3.0.convnormrelu1.0.bias"",
    ""feature_model/fnet/layer3/layer_with_weights-0/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer3.0.convnormrelu2.0.weight"",
    ""feature_model/fnet/layer3/layer_with_weights-0/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer3.0.convnormrelu2.0.bias"",
    ""feature_model/fnet/layer3/layer_with_weights-0/downsample/layer_with_weights-0/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer3.0.downsample.0.weight"",
    ""feature_model/fnet/layer3/layer_with_weights-0/downsample/layer_with_weights-0/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer3.0.downsample.0.bias"",
    ""feature_model/fnet/layer3/layer_with_weights-1/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer3.1.convnormrelu1.0.weight"",
    ""feature_model/fnet/layer3/layer_with_weights-1/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer3.1.convnormrelu1.0.bias"",
    ""feature_model/fnet/layer3/layer_with_weights-1/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer3.1.convnormrelu2.0.weight"",
    ""feature_model/fnet/layer3/layer_with_weights-1/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.feature_encoder.layer3.1.convnormrelu2.0.bias"",
    # Update block
    ""flow_model/update_block/encoder/conv/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.motion_encoder.conv.0.weight"",
    ""flow_model/update_block/encoder/conv/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.motion_encoder.conv.0.bias"",
    ""flow_model/update_block/encoder/convc1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.motion_encoder.convcorr1.0.weight"",
    ""flow_model/update_block/encoder/convc1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.motion_encoder.convcorr1.0.bias"",
    ""flow_model/update_block/encoder/convc2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.motion_encoder.convcorr2.0.weight"",
    ""flow_model/update_block/encoder/convc2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.motion_encoder.convcorr2.0.bias"",
    ""flow_model/update_block/encoder/convf1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.motion_encoder.convflow1.0.weight"",
    ""flow_model/update_block/encoder/convf1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.motion_encoder.convflow1.0.bias"",
    ""flow_model/update_block/encoder/convf2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.motion_encoder.convflow2.0.weight"",
    ""flow_model/update_block/encoder/convf2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.motion_encoder.convflow2.0.bias"",
    # Recurrent block
    ""flow_model/update_block/gru/convq1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru1.convq.weight"",
    ""flow_model/update_block/gru/convq1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru1.convq.bias"",
    ""flow_model/update_block/gru/convq2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru2.convq.weight"",
    ""flow_model/update_block/gru/convq2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru2.convq.bias"",
    ""flow_model/update_block/gru/convr1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru1.convr.weight"",
    ""flow_model/update_block/gru/convr1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru1.convr.bias"",
    ""flow_model/update_block/gru/convr2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru2.convr.weight"",
    ""flow_model/update_block/gru/convr2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru2.convr.bias"",
    ""flow_model/update_block/gru/convz1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru1.convz.weight"",
    ""flow_model/update_block/gru/convz1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru1.convz.bias"",
    ""flow_model/update_block/gru/convz2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru2.convz.weight"",
    ""flow_model/update_block/gru/convz2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.recurrent_block.convgru2.convz.bias"",
    # Head
    ""flow_model/update_block/flow_head/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.flow_head.conv1.weight"",
    ""flow_model/update_block/flow_head/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.flow_head.conv1.bias"",
    ""flow_model/update_block/flow_head/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.flow_head.conv2.weight"",
    ""flow_model/update_block/flow_head/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.update_block.flow_head.conv2.bias"",
    # Mask predictor
    ""flow_model/update_block/mask/layer_with_weights-0/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.mask_predictor.convrelu.0.weight"",
    ""flow_model/update_block/mask/layer_with_weights-0/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.mask_predictor.convrelu.0.bias"",
    ""flow_model/update_block/mask/layer_with_weights-1/kernel/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.mask_predictor.conv.weight"",
    ""flow_model/update_block/mask/layer_with_weights-1/bias/.ATTRIBUTES/VARIABLE_VALUE"": ""raft.mask_predictor.conv.bias"",
}

def main() -> None:
    reader = py_checkpoint_reader.NewCheckpointReader(""../ups/optical_flow/checkpoints/smurf-kitti-smurf-ckpt-31"")
    var_to_shape_map = reader.get_variable_to_shape_map()
    state_dict = {}
    for key, value in sorted(var_to_shape_map.items()):
        if key in tf_to_pt.keys():
            if ""kernel"" in key:
                state_dict[tf_to_pt[key]] = torch.from_numpy(reader.get_tensor(key)).permute(3, 2, 0, 1)
            else:
                state_dict[tf_to_pt[key]] = torch.from_numpy(reader.get_tensor(key))
    torch.save(state_dict, ""../test_scripts/raft_smurf.pt"")



if __name__ == ""__main__"":
    main()
```

However, when loading the weight to the TorchVision RAFT model I get some unplausible flow results.

```python
from typing import Dict

import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch import Tensor
from torchvision.models.optical_flow.raft import ResidualBlock, _raft
from torchvision.utils import flow_to_image

from ups.data.utils import load_image


@torch.no_grad()
def main() -> None:
    # Load first and second image
    image_1: Tensor = load_image(""../toy_data/cityscaps/darmstadt_000047_000018_leftImg8bit.png"")[None, :, ::2, ::2]
    image_2: Tensor = load_image(""../toy_data/cityscaps/darmstadt_000047_000019_leftImg8bit.png"")[None, :, ::2, ::2]
    # Init RAFT
    raft: nn.Module = _raft(
        weights=None,
        progress=False,
        # Feature encoder
        feature_encoder_layers=(64, 64, 96, 128, 256),
        feature_encoder_block=ResidualBlock,
        feature_encoder_norm_layer=nn.InstanceNorm2d,
        # Context encoder
        context_encoder_layers=(64, 64, 96, 128, 256),
        context_encoder_block=ResidualBlock,
        context_encoder_norm_layer=nn.InstanceNorm2d,
        # Correlation block
        corr_block_num_levels=4,
        corr_block_radius=4,
        # Motion encoder
        motion_encoder_corr_layers=(256, 192),
        motion_encoder_flow_layers=(128, 64),
        motion_encoder_out_channels=128,
        # Recurrent block
        recurrent_block_hidden_state_size=128,
        recurrent_block_kernel_size=((1, 5), (5, 1)),
        recurrent_block_padding=((0, 2), (2, 0)),
        # Flow head
        flow_head_hidden_size=256,
        # Mask predictor
        use_mask_predictor=True,
    )
    # Load checkpoint and remove raft. from key
    checkpoint: Dict[str, Tensor] = torch.load(""raft_smurf.pt"")
    checkpoint = {key.replace(""raft."", """"): value for key, value in checkpoint.items()}
    # Load checkpoint
    raft.load_state_dict(checkpoint, strict=True)
    # Make forward pass
    optical_flow: Tensor = raft(image_1, image_2)[-1]
    # Optical flow to rgb
    optical_flow_rgb: Tensor = flow_to_image(optical_flow)[0]
    # Plot optical flow
    plt.imshow(optical_flow_rgb.permute(1, 2, 0))
    plt.show()
    plt.imshow(image_1[0].permute(1, 2, 0))
    plt.show()
    plt.imshow(image_2[0].permute(1, 2, 0))
    plt.show()


if __name__ == ""__main__"":
    main()
```
<img width=""291"" alt=""Screenshot 2023-12-28 at 23 30 11"" src=""https://github.com/google-research/google-research/assets/34400551/fe2aa256-2b1d-413d-81d7-54cbc8000270"">
<img width=""290"" alt=""Screenshot 2023-12-28 at 23 30 33"" src=""https://github.com/google-research/google-research/assets/34400551/8844f2b5-dc22-4075-bfda-70b42a56c0b2"">

Image from the Cityscapes dataset.

For reference here is the optical flow predicted by supervised RAFT (large).
<img width=""291"" alt=""Screenshot 2023-12-28 at 23 31 53"" src=""https://github.com/google-research/google-research/assets/34400551/9088035b-71a6-4719-831e-407e34b9b1cb"">

I believe some minor disparity between your RAFT implementation and the TorchVision RAFT implementation causes this. I'm currently searching for differences between implementation. Any help would be greatly appreciated. Thanks :)

",ported script python import import torch context weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias feature weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias weight bias update block weight bias weight bias weight bias weight bias weight bias recurrent block head mask predictor weight bias main none reader key value sorted key kernel key key key else key key main however loading weight raft model get unplausible flow python import import import torch import torch import tensor import import import main none load first second image tensor none tensor none raft raft feature context correlation block motion recurrent block flow head mask predictor load remove raft key tensor raft value key value load make forward pas tensor raft optical flow tensor plot optical flow main image reference optical flow raft large believe minor disparity raft implementation raft implementation currently searching implementation help would greatly thanks,issue,negative,positive,neutral,neutral,positive,positive
1864429483,"Hello
I am a beginner.
Can you please explain how to make the tfrecord file that I have produced or the file related to the KINETICS400 or HOWTO100M dataset to the factory file?

I always face this problem:
```
‍‍‍  ‍‍‍File ""/home/zahra/Desktop/google-research/vatt/data/dataloaders.py"", line 133, in __init__
     factory_class = ds_fctr.get_ds_factory(
   File ""/home/zahra/Desktop/google-research/vatt/data/datasets/factory.py"", line 39, in get_ds_factory
     ds = DS_TO_FACTORY[dataset_name]
KeyError: 'HOWTO100M'


   File ""/home/zahra/Desktop/google-research/vatt/data/dataloaders.py"", line 560, in __init__
     factory_class = ds_fctr.get_ds_factory(
   File ""/home/zahra/Desktop/google-research/vatt/data/datasets/factory.py"", line 39, in get_ds_factory
     ds = DS_TO_FACTORY[dataset_name]
KeyError: 'KINETICS400'
```
I don't even know what is meant by checkpoint data and index?

Please explain to me with an example.
Thank you",hello beginner please explain make file produced file related kinetics factory file always face problem line file line file line file line even know meant data index please explain example thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1864231876," Hello,
Can you please tell me what you did to solve this problem?
I am very beginner.",hello please tell solve problem beginner,issue,negative,neutral,neutral,neutral,neutral,neutral
1860446767,I would appreciate your help @yangli169 . Thank you ;D,would appreciate help thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1855411917,"@rybakov  Sorry, and thank you so much! I pulled the latest version of kws_streaming and bc_resnet was converted to streaming inference well. 

But I have a last question, model's prediction is difference between streaming inference and non streaming inference. I want to know bc_resnet's input data_shape at streaming inference. I used data_shape = (160,).

Here's my output.

```
*** Here is streaming inference ***
[silence]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[silence]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[silence]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[silence]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[silence]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
{1: '_unknown_', 2: 'yes', 4: 'up', 10: 'stop', 3: 'no', 11: 'go', 5: 'down', 9: 'off', 8: 'on', 6: 'left', 7: 'right', 0: '_silence_'}
../datasets/data2/yes/1b88bf70_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/yes/05b2db80_nohash_1.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 0, 0, 0, 0]
../datasets/data2/yes/b66f4f93_nohash_5.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
../datasets/data2/yes/750e3e75_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 11, 11, 11, 11, 11, 11, 11, 11, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/yes/e49428d9_nohash_3.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/yes/af7a8296_nohash_3.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]
../datasets/data2/yes/778a4a01_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/yes/b00dff7e_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
../datasets/data2/yes/e77d88fc_nohash_1.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[6, 6, 6, 6, 6, 6, 6, 6, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
../datasets/data2/yes/0cb74144_nohash_2.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/yes/3d794813_nohash_4.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/yes/11321027_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
../datasets/data2/yes/06f6c194_nohash_4.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 11, 11, 11, 11, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
../datasets/data2/yes/b97c9f77_nohash_1.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/yes/7213ed54_nohash_4.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/yes/c50f55b8_nohash_5.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 11, 2, 2, 2, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/yes/09bcdc9d_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
../datasets/data2/yes/cae62f38_nohash_1.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
../datasets/data2/yes/1942abd7_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 11, 11, 11, 11, 11, 11, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
../datasets/data2/yes/321aba74_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 11, 11, 11, 11, 11, 11, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 2, 2, 2, 2, 2, 2, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/no/b66f4f93_nohash_5.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
../datasets/data2/no/66cbe2b3_nohash_2.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 4, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/no/750e3e75_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 4, 4, 4, 4, 4, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
../datasets/data2/no/3852fca2_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 11, 11, 11, 11, 11, 11, 11, 11, 6, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/no/e49428d9_nohash_3.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 4, 4, 4, 4, 4, 4, 4, 4, 11, 11, 11, 11, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
../datasets/data2/no/778a4a01_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/no/b00dff7e_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 6, 6, 4, 4, 4, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
../datasets/data2/no/61e50f62_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
../datasets/data2/no/e77d88fc_nohash_1.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
../datasets/data2/no/0cb74144_nohash_2.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
../datasets/data2/no/17c94b23_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
../datasets/data2/no/3d794813_nohash_4.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]
../datasets/data2/no/e55a2b20_nohash_1.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]
../datasets/data2/no/06f6c194_nohash_4.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]
../datasets/data2/no/b97c9f77_nohash_1.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
../datasets/data2/no/7213ed54_nohash_4.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
../datasets/data2/no/c50f55b8_nohash_5.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
../datasets/data2/no/09bcdc9d_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 9, 9, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
../datasets/data2/no/1942abd7_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
../datasets/data2/no/321aba74_nohash_0.wav
[input_data.shape] (1, 16000) [out_tflite.shape] (100, 12)
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
*** Here is non streaming inference ***
yes
../datasets/data2/yes/05b2db80_nohash_1.wav
yes
../datasets/data2/yes/b66f4f93_nohash_5.wav
yes
../datasets/data2/yes/750e3e75_nohash_0.wav
yes
../datasets/data2/yes/e49428d9_nohash_3.wav
yes
../datasets/data2/yes/af7a8296_nohash_3.wav
yes
../datasets/data2/yes/778a4a01_nohash_0.wav
yes
../datasets/data2/yes/b00dff7e_nohash_0.wav
yes
../datasets/data2/yes/e77d88fc_nohash_1.wav
yes
../datasets/data2/yes/0cb74144_nohash_2.wav
yes
../datasets/data2/yes/3d794813_nohash_4.wav
yes
../datasets/data2/yes/11321027_nohash_0.wav
yes
../datasets/data2/yes/06f6c194_nohash_4.wav
yes
../datasets/data2/yes/b97c9f77_nohash_1.wav
yes
../datasets/data2/yes/7213ed54_nohash_4.wav
yes
../datasets/data2/yes/c50f55b8_nohash_5.wav
yes
../datasets/data2/yes/09bcdc9d_nohash_0.wav
yes
../datasets/data2/yes/cae62f38_nohash_1.wav
yes
../datasets/data2/yes/1942abd7_nohash_0.wav
yes
../datasets/data2/yes/321aba74_nohash_0.wav
yes
../datasets/data2/no/b66f4f93_nohash_5.wav
no
../datasets/data2/no/66cbe2b3_nohash_2.wav
no
../datasets/data2/no/750e3e75_nohash_0.wav
no
../datasets/data2/no/3852fca2_nohash_0.wav
no
../datasets/data2/no/e49428d9_nohash_3.wav
no
../datasets/data2/no/778a4a01_nohash_0.wav
no
../datasets/data2/no/b00dff7e_nohash_0.wav
no
../datasets/data2/no/61e50f62_nohash_0.wav
no
../datasets/data2/no/e77d88fc_nohash_1.wav
no
../datasets/data2/no/0cb74144_nohash_2.wav
no
../datasets/data2/no/17c94b23_nohash_0.wav
no
../datasets/data2/no/3d794813_nohash_4.wav
no
../datasets/data2/no/e55a2b20_nohash_1.wav
no
../datasets/data2/no/06f6c194_nohash_4.wav
no
../datasets/data2/no/b97c9f77_nohash_1.wav
no
../datasets/data2/no/7213ed54_nohash_4.wav
no
../datasets/data2/no/c50f55b8_nohash_5.wav
no
../datasets/data2/no/09bcdc9d_nohash_0.wav
no
../datasets/data2/no/1942abd7_nohash_0.wav
no
../datasets/data2/no/321aba74_nohash_0.wav
no
```",sorry thank much latest version converted streaming inference well last question model prediction difference streaming inference non streaming inference want know input streaming inference used output streaming inference silence silence silence silence silence non streaming inference yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes,issue,positive,neutral,neutral,neutral,neutral,neutral
1854764858,Please confirm that you pulled the latest version of [kws_streaming](https://github.com/google-research/google-research/tree/master/kws_streaming),please confirm latest version,issue,negative,positive,positive,positive,positive,positive
1854763494,"My guess is that you are using outdated version of tensorflow_model_optimization which is mismatched with tf.
So I would suggest to update [tensorflow_model_optimization](https://www.tensorflow.org/model_optimization/guide/install)
",guess outdated version would suggest update,issue,negative,negative,negative,negative,negative,negative
1854405364,"I'll update with a fix that worked for me and points to the c++14 issue being the cause. I still don't know how to update it to work with the code as is from the repo, but I found that if I changed the line
`std::make_unique<Task<F>>(task_index...)` to `std::unique_ptr<Task<F>>(new Task<F>(task_index...))` that it now works.",update fix worked issue cause still know update work code found line task task new task work,issue,negative,positive,positive,positive,positive,positive
1852112992,"Hi everyone, thanks for google research team for to make available the project and paper, like also my colleagues I also have interested on ExtendedTSMixer, will be awesome if you folks make this public. 

Besides that, I have some confuses on block construction, like time features, history features and static feature are combined, what exactly is conditional feature mixing, and how this is implemented. Anyone have some references for this specific approach?",hi everyone thanks research team make available project paper like also also interested awesome make public besides block construction like time history static feature combined exactly conditional feature anyone specific approach,issue,positive,positive,positive,positive,positive,positive
1851235892,"I used `Modes.STREAM_EXTERNAL_STATE_INFERENCE` and I found that my error is raised in first Conv2D layer after expand_dim. frequency_pad in Stream layer occur frequency dimension mismatch. In training, this frequency dimension was applied mean layer, so this is not important, but in STREAM_EXTERNAL_STATE_INFERENCE this mismatch occur error. This issue may be my fault, please review this issue.

[+] I changed inference mode to STREAM_INTERNAL_STATE_INFERENCE and conversion is works. But I could not believe my model works well because streaming model's output and non streaming model's output is different...",used found error raised first layer stream layer occur frequency dimension mismatch training frequency dimension applied mean layer important mismatch occur error issue may fault please review issue inference mode conversion work could believe model work well streaming model output non streaming model output different,issue,negative,positive,neutral,neutral,positive,positive
1850938152,"Hi,

I'm currently attempting to fine-tune VATT for the MELD dataset following the instructions provided by @Adam-fei and @adaniefei. Unfortunately, I'm encountering the same issue with the error message:
`2023-12-11 22:26:34.523213: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.`

I've tried adjusting the batch size and ensuring the input CSV is consistent, but so far, no solution has worked.

Does anyone have an idea on how to solve this problem with VATT?
Or maybe @hassanhub can help?

",hi currently meld following provided unfortunately issue error message slice index dimension tried batch size input consistent far solution worked anyone idea solve problem maybe help,issue,negative,negative,neutral,neutral,negative,negative
1849427753,"Thank you for answering my issue, @rybakov.

I'm trying to apply causal padding and Delay layer for residual and identity connection.
But I got an error when I convert my model to tflite_streaming_model.

I'll try to find out where I'm going wrong, but I don't know how to fix it at the moment. Could you please review my errors?

```
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 2
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 2
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 4
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 4
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 4
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 8
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 8
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 8
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 8
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 8
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 16
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 16
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 16
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 16
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 16
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(1, 16000)]              0         
                                                                 
 speech_features (SpeechFeat  (1, 98, 40)              0         
 ures)                                                           
                                                                 
 tf_op_layer_ExpandDims (Ten  [(1, 98, 40, 1)]         0         
 sorFlowOpLayer)                                                 
                                                                 
 stream (Stream)             (1, 98, 20, 16)           416       
                                                                 
 transition_block (Transitio  (1, 98, 20, 8)           464       
 nBlock)                                                         
                                                                 
 normal_block (NormalBlock)  (1, 98, 20, 8)            304       
                                                                 
 normal_block_1 (NormalBlock  (1, 98, 20, 8)           304       
 )                                                               
                                                                 
 transition_block_1 (Transit  (1, 98, 10, 12)          648       
 ionBlock)                                                       
                                                                 
 normal_block_2 (NormalBlock  (1, 98, 10, 12)          504       
 )                                                               
                                                                 
 normal_block_3 (NormalBlock  (1, 98, 10, 12)          504       
 )                                                               
                                                                 
 transition_block_2 (Transit  (1, 98, 5, 16)           992       
 ionBlock)                                                       
                                                                 
 normal_block_4 (NormalBlock  (1, 98, 5, 16)           736       
 )                                                               
                                                                 
 normal_block_5 (NormalBlock  (1, 98, 5, 16)           736       
 )                                                               
                                                                 
 normal_block_6 (NormalBlock  (1, 98, 5, 16)           736       
 )                                                               
                                                                 
 normal_block_7 (NormalBlock  (1, 98, 5, 16)           736       
 )                                                               
                                                                 
 transition_block_3 (Transit  (1, 98, 5, 20)           1400      
 ionBlock)                                                       
                                                                 
 normal_block_8 (NormalBlock  (1, 98, 5, 20)           1000      
 )                                                               
                                                                 
 normal_block_9 (NormalBlock  (1, 98, 5, 20)           1000      
 )                                                               
                                                                 
 normal_block_10 (NormalBloc  (1, 98, 5, 20)           1000      
 k)                                                              
                                                                 
 normal_block_11 (NormalBloc  (1, 98, 5, 20)           1000      
 k)                                                              
                                                                 
 stream_33 (Stream)          (1, 98, 5, 20)            520       
                                                                 
 tf_op_layer_Mean (TensorFlo  [(1, 98, 1, 20)]         0         
 wOpLayer)                                                       
                                                                 
 conv2d_21 (Conv2D)          (1, 98, 1, 32)            640       
                                                                 
 stream_34 (Stream)          (1, 1, 1, 32)             0         
                                                                 
 conv2d_22 (Conv2D)          (1, 1, 1, 12)             384       
                                                                 
 tf_op_layer_Squeeze (Tensor  [(1, 12)]                0         
 FlowOpLayer)                                                    
                                                                 
=================================================================
Total params: 14,024
Trainable params: 11,032
Non-trainable params: 2,992
_________________________________________________________________

WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 5
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 2
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 2
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 4
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 4
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 8
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 8
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 8
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 8
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 16
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 16
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 16
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 16
WARNING:absl:ring_buffer_size_in_time_dim overwritten by the passed-in value: 5
Traceback (most recent call last):
  File ""decode.py"", line 196, in <module>
    tflite_streaming_model = utils.model_to_tflite(sess, model_non_stream_batch, flags, Modes.STREAM_EXTERNAL_STATE_INFERENCE)
  File ""/tf/kws/koowater/builder/kws_streaming/models/utils.py"", line 386, in model_to_tflite
    model_stream = to_streaming_inference(model_non_stream, flags, mode)
  File ""/tf/kws/koowater/builder/kws_streaming/models/utils.py"", line 318, in to_streaming_inference
    model_inference = convert_to_inference_model(model_non_stream,
  File ""/tf/kws/koowater/builder/kws_streaming/models/utils.py"", line 249, in convert_to_inference_model
    new_model = _clone_model(model, input_tensors)
  File ""/tf/kws/koowater/builder/kws_streaming/models/utils.py"", line 109, in _clone_model
    functional.reconstruct_from_config(
  File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 1495, in reconstruct_from_config
    if process_node(layer, node_data):
  File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 1435, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer_v1.py"", line 838, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 692, in wrapper
    raise e.ag_error_metadata.to_exception(e)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 689, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 439, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/__autograph_generated_fileizg6by4c.py"", line 105, in tf__call
    ag__.if_stmt((ag__.ld(self).mode == ag__.ld(modes).Modes.STREAM_INTERNAL_STATE_INFERENCE), if_body_4, else_body_4, get_state_4, set_state_4, ('do_return', 'retval_', 'self.output_state'), 3)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1266, in if_stmt
    _py_if_stmt(cond, body, orelse)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1319, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/__autograph_generated_fileizg6by4c.py"", line 103, in else_body_4
    ag__.if_stmt((ag__.ld(self).mode == ag__.ld(modes).Modes.STREAM_EXTERNAL_STATE_INFERENCE), if_body_3, else_body_3, get_state_3, set_state_3, ('do_return', 'retval_', 'self.output_state'), 3)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1266, in if_stmt
    _py_if_stmt(cond, body, orelse)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1319, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/__autograph_generated_fileizg6by4c.py"", line 71, in if_body_3
    ag__.if_stmt(ag__.ld(self).ring_buffer_size_in_time_dim, if_body_1, else_body_1, get_state_1, set_state_1, ('output', 'self.output_state'), 2)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1266, in if_stmt
    _py_if_stmt(cond, body, orelse)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1319, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/__autograph_generated_fileizg6by4c.py"", line 65, in if_body_1
    (output, ag__.ld(self).output_state) = ag__.converted_call(ag__.ld(self)._streaming_external_state, (ag__.ld(inputs), ag__.ld(self).input_state), None, fscope)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 441, in converted_call
    result = converted_f(*effective_args)
  File ""/tmp/__autograph_generated_file_lmun91g.py"", line 166, in tf___streaming_external_state
    ag__.if_stmt(ag__.converted_call(ag__.ld(isinstance), (ag__.converted_call(ag__.ld(self).get_core_layer, (), None, fscope), ag__.ld(tf).keras.layers.Conv2DTranspose), None, fscope), if_body_6, else_body_6, get_state_6, set_state_6, ('do_return', 'retval_'), 2)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1266, in if_stmt
    _py_if_stmt(cond, body, orelse)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1319, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/__autograph_generated_file_lmun91g.py"", line 157, in else_body_6
    ag__.if_stmt(ag__.ld(self).use_one_step, if_body_5, else_body_5, get_state_5, set_state_5, ('do_return', 'retval_'), 2)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1266, in if_stmt
    _py_if_stmt(cond, body, orelse)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1319, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/__autograph_generated_file_lmun91g.py"", line 134, in if_body_5
    memory = ag__.converted_call(ag__.ld(tf).keras.backend.concatenate, ([ag__.ld(memory), ag__.ld(inputs)], 1), None, fscope)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 331, in converted_call
    return _call_unconverted(f, args, kwargs, options, False)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 459, in _call_unconverted
    return f(*args)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/keras/backend.py"", line 3581, in concatenate
    return tf.concat([to_dense(x) for x in tensors], axis)
ValueError: in user code:

    File ""/tf/kws/koowater/builder/kws_streaming/layers/stream.py"", line 411, in call  *
        output, self.output_state = self._streaming_external_state(
    File ""/tf/kws/koowater/builder/kws_streaming/layers/stream.py"", line 563, in _streaming_external_state  *
        memory = tf.keras.backend.concatenate([memory, inputs], 1)
    File ""/usr/local/lib/python3.8/dist-packages/keras/backend.py"", line 3581, in concatenate
        return tf.concat([to_dense(x) for x in tensors], axis)

    ValueError: Dimension 0 in both shapes must be equal, but are 40 and 43. Shapes are [40,1] and [43,1]. for '{{node streaming/stream/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](streaming/stream/strided_slice, streaming/stream/Pad, streaming/stream/concat/axis)' with input shapes: [1,4,40,1], [1,1,43,1], [] and with computed input tensors: input[2] = <1>.
```",thank issue trying apply causal padding delay layer residual identity connection got error convert model try find going wrong know fix moment could please review warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value layer type output shape param ten stream stream transit transit transit stream stream tensor total trainable warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value warning value recent call last file line module sess file line mode file line file line model file line file line layer file line layer file line file line wrapper raise file line wrapper return file line result file line self file line cond body file line return body cond else file line self file line cond body file line return body cond else file line self file line cond body file line return body cond else file line output self self self none file line result file line self none none file line cond body file line return body cond else file line self file line cond body file line return body cond else file line memory memory none file line return false file line return file line raise none file line concatenate return axis user code file line call output file line memory memory file line concatenate return axis dimension must equal node input input input,issue,negative,negative,negative,negative,negative,negative
1848860567,"I also encountered the same error in an environment with WSL2, Ubuntu 22.04, and CUDA 12.0.

Edit:
I'm a beginner with TensorFlow, so I might be saying something off the mark, but I was able to run demo.py using the following method.
In my environment, it seems that the part `output = text_model({'text': ids})` cannot be processed when using CUDA. Therefore, by specifying to process on the CPU with `with tf.device('/CPU:0'):` for the two lines `text_model = tf.saved_model.load(text_model_dir)` in dito/demo.py and `output = text_model({'text': ids})` in utils/clip_utils.py, text processing should now be slower, but it was possible to use it. 
If there are still some GPU-related errors, it might be good to add `with tf.device('/CPU:0'):` before `app.run(main)` in dito/demo.py to process everything on the CPU.
",also error environment edit beginner might saying something mark able run following method environment part output therefore process two output text possible use still might good add main process everything,issue,negative,positive,positive,positive,positive,positive
1848850178,"Hi @rybakov,

I'm currently using tf-nightly (tf-nightly==2.16.0.dev20231209), is this an acceptable version of tf-nightly?

Here's the full error log: 
[3](vscode-notebook-cell:/home/vdhanraj/Desktop/kws_streaming/colab/00_check_data.ipynb#W5sZmlsZQ%3D%3D?line=2) import logging
      [4](vscode-notebook-cell:/home/vdhanraj/Desktop/kws_streaming/colab/00_check_data.ipynb#W5sZmlsZQ%3D%3D?line=3) from kws_streaming.models import model_flags
----> [5](vscode-notebook-cell:/home/vdhanraj/Desktop/kws_streaming/colab/00_check_data.ipynb#W5sZmlsZQ%3D%3D?line=4) from kws_streaming.models import models
      [6](vscode-notebook-cell:/home/vdhanraj/Desktop/kws_streaming/colab/00_check_data.ipynb#W5sZmlsZQ%3D%3D?line=5) from kws_streaming.layers.modes import Modes
      [7](vscode-notebook-cell:/home/vdhanraj/Desktop/kws_streaming/colab/00_check_data.ipynb#W5sZmlsZQ%3D%3D?line=6) from kws_streaming.train import test

File [~/Desktop/kws_streaming/colab/./google-research/kws_streaming/models/models.py:17](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/models.py:17)
      [1](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/models.py:1) # coding=utf-8
      [2](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/models.py:2) # Copyright 2023 The Google Research Authors.
      [3](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/models.py:3) #
   (...)
     [13](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/models.py:13) # See the License for the specific language governing permissions and
     [14](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/models.py:14) # limitations under the License.
     [16](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/models.py:16) """"""Supported models.""""""
---> [17](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/models.py:17) import kws_streaming.models.att_mh_rnn as att_mh_rnn
     [18](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/models.py:18) import kws_streaming.models.att_rnn as att_rnn
     [19](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/models.py:19) import kws_streaming.models.bc_resnet as bc_resnet

File [~/Desktop/kws_streaming/colab/./google-research/kws_streaming/models/att_mh_rnn.py:18](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/att_mh_rnn.py:18)
     [16](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/att_mh_rnn.py:16) """"""BiRNN model with multihead attention.""""""
     [17](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/att_mh_rnn.py:17) from kws_streaming.layers import modes
---> [18](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/att_mh_rnn.py:18) from kws_streaming.layers import speech_features
     [19](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/att_mh_rnn.py:19) from kws_streaming.layers.compat import tf
     [20](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/models/att_mh_rnn.py:20) import kws_streaming.models.model_utils as utils

File [~/Desktop/kws_streaming/colab/./google-research/kws_streaming/layers/speech_features.py:26](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/speech_features.py:26)
     [24](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/speech_features.py:24) from kws_streaming.layers import random_shift
     [25](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/speech_features.py:25) from kws_streaming.layers import random_stretch_squeeze
---> [26](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/speech_features.py:26) from kws_streaming.layers import spectrogram_augment
     [27](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/speech_features.py:27) from kws_streaming.layers import spectrogram_cutout
     [28](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/speech_features.py:28) from kws_streaming.layers import windowing

File [~/Desktop/kws_streaming/colab/./google-research/kws_streaming/layers/spectrogram_augment.py:20](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/spectrogram_augment.py:20)
     [17](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/spectrogram_augment.py:17) from typing import Any, Dict
     [19](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/spectrogram_augment.py:19) import tensorflow as tf
---> [20](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/spectrogram_augment.py:20) import tensorflow_model_optimization as tfmot
     [22](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/spectrogram_augment.py:22) from kws_streaming.layers.compat import tf
     [23](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/Desktop/kws_streaming/colab/google-research/kws_streaming/layers/spectrogram_augment.py:23) from tensorflow.python.ops import array_ops  # pylint: disable=g-direct-tensorflow-import

File [~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/__init__.py:86](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/__init__.py:86)
     [82](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/__init__.py:82) # To ensure users only access the expected public API, the API structure is
     [83](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/__init__.py:83) # created in the `api` directory. Import all api modules.
     [84](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/__init__.py:84) from tensorflow_model_optimization.python.core import version
---> [86](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/__init__.py:86) from tensorflow_model_optimization.python.core.api import clustering
     [87](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/__init__.py:87) from tensorflow_model_optimization.python.core.api import experimental
     [88](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/__init__.py:88) from tensorflow_model_optimization.python.core.api import quantization

File [~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:16](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:16)
      [1](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:1) # Copyright 2021 The TensorFlow Authors. All Rights Reserved.
      [2](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:2) #
      [3](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:3) # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     [13](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:13) # limitations under the License.
     [14](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:14) # ==============================================================================
     [15](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:15) """"""Import API modules for Tensorflow Model Optimization.""""""
---> [16](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:16) from tensorflow_model_optimization.python.core.api import clustering
     [17](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:17) from tensorflow_model_optimization.python.core.api import experimental
     [18](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:18) from tensorflow_model_optimization.python.core.api import quantization

File [~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:16](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:16)
      [1](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:1) # Copyright 2020 The TensorFlow Authors. All Rights Reserved.
      [2](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:2) #
      [3](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:3) # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     [13](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:13) # limitations under the License.
     [14](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:14) # ==============================================================================
     [15](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:15) """"""Module containing code for clustering.""""""
---> [16](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:16) from tensorflow_model_optimization.python.core.api.clustering import keras

File [~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py:19](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py:19)
     [16](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py:16) # pylint: disable=g-bad-import-order
     [17](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py:17) from tensorflow_model_optimization.python.core.clustering.keras import experimental
---> [19](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py:19) from tensorflow_model_optimization.python.core.clustering.keras.cluster import cluster_scope
     [20](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py:20) from tensorflow_model_optimization.python.core.clustering.keras.cluster import cluster_weights
     [21](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py:21) from tensorflow_model_optimization.python.core.clustering.keras.cluster import strip_clustering

File [~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py:22](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py:22)
     [19](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py:19) import tensorflow as tf
     [21](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py:21) from tensorflow_model_optimization.python.core.clustering.keras import cluster_config
---> [22](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py:22) from tensorflow_model_optimization.python.core.clustering.keras import cluster_wrapper
     [23](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py:23) from tensorflow_model_optimization.python.core.clustering.keras import clustering_centroids
     [25](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py:25) k = tf.keras.backend

File [~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py:24](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py:24)
     [22](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py:22) from tensorflow_model_optimization.python.core.clustering.keras import clusterable_layer
     [23](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py:23) from tensorflow_model_optimization.python.core.clustering.keras import clustering_centroids
---> [24](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py:24) from tensorflow_model_optimization.python.core.clustering.keras import clustering_registry
     [26](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py:26) attrgetter = operator.attrgetter  # pylint: disable=invalid-name
     [27](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py:27) keras = tf.keras

File [~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:57](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:57)
     [53](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:53)         return ans
     [54](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:54)     return ClusteringAlgorithm
---> [57](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:57) class ClusteringRegistry(object):
     [58](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:58)   """"""Registry responsible for built-in keras layers.""""""
     [60](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:60)   # The keys represent built-in keras layers and the values represent the
     [61](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:61)   # the variables within the layers which hold the kernel weights. This
     [62](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:62)   # allows the wrapper to access and modify the weights.

File [~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:76](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:76), in ClusteringRegistry()
     [58](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:58) """"""Registry responsible for built-in keras layers.""""""
     [60](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:60) # The keys represent built-in keras layers and the values represent the
     [61](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:61) # the variables within the layers which hold the kernel weights. This
     [62](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:62) # allows the wrapper to access and modify the weights.
     [63](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:63) _LAYERS_WEIGHTS_MAP = {
     [64](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:64)     layers.Conv1D: ['kernel'],
     [65](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:65)     layers.Conv1DTranspose: ['kernel'],
     [66](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:66)     layers.Conv2D: ['kernel'],
     [67](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:67)     layers.Conv2DTranspose: ['kernel'],
     [68](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:68)     layers.Conv3D: ['kernel'],
     [69](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:69)     layers.Conv3DTranspose: ['kernel'],
     [70](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:70)     # non-clusterable due to big unrecoverable accuracy loss
     [71](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:71)     layers.DepthwiseConv2D: [],
     [72](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:72)     layers.SeparableConv1D: ['pointwise_kernel'],
     [73](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:73)     layers.SeparableConv2D: ['pointwise_kernel'],
     [74](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:74)     layers.Dense: ['kernel'],
     [75](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:75)     layers.Embedding: ['embeddings'],
---> [76](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:76)     layers.LocallyConnected1D: ['kernel'],
     [77](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:77)     layers.LocallyConnected2D: ['kernel'],
     [78](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:78)     layers.BatchNormalization: [],
     [79](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:79)     layers.LayerNormalization: [],
     [80](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:80) }
     [82](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:82) _SUPPORTED_RNN_CELLS = frozenset({
     [83](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:83)     # Sometimes v2 RNN will wrap some v1 RNN cells and we need
     [84](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:84)     # to consider this
   (...)
     [92](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:92)     tf.compat.v2.keras.layers.StackedRNNCells,
     [93](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:93) })
     [95](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:95) _SUPPORTED_RNN_LAYERS = frozenset([
     [96](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:96)     layers.GRU,
     [97](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:97)     layers.LSTM,
   (...)
    [100](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:100)     layers.Bidirectional,
    [101](https://file+.vscode-resource.vscode-cdn.net/home/vdhanraj/Desktop/kws_streaming/colab/~/.local/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_registry.py:101) ])

AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'LocallyConnected1D'",hi currently dev acceptable version full error log import logging import import import import test file copyright research see license specific language governing license import import import file model multihead attention import import import import file import import import import import file import import import import import file ensure access public structure directory import import version import clustering import experimental import quantization file copyright reserved licensed apache license version license license import model optimization import clustering import experimental import quantization file copyright reserved licensed apache license version license license module code clustering import file import experimental import import import file import import import import file import import import file return return class object registry responsible represent represent within hold kernel wrapper access modify file registry responsible represent represent within hold kernel wrapper access modify due big unrecoverable accuracy loss sometimes wrap need consider module attribute,issue,positive,positive,neutral,neutral,positive,positive
1848839867,"kws lib does not use or import LocallyConnected1D explicitly (full log can be helpful to see where the error happened).
kws lib is at sync with HEAD of TF, so please try tf_nightly.",use import explicitly full log helpful see error sync head please try,issue,negative,positive,positive,positive,positive,positive
1847883491,"I fixed bc_resnet with 'causal' padding and added a test with 'bc_resnet_causal'. 

You could apply Delay layer to bc_resnet the same way it is done in [delay_test.py](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/delay_test.py) has several examples combining conv with delay layers.

It can be easier to re-design it using sub class api as shown in [example](https://github.com/google-research/google-research/blob/master/kws_streaming/models_sub/conv_model.py)",fixed padding added test could apply delay layer way done several combining delay easier sub class shown example,issue,negative,positive,neutral,neutral,positive,positive
1846595833,For deployment you need to convert kws keras model to TFLite (it is already part of the lib) and then run TFLite on [Android](https://www.tensorflow.org/lite/android/quickstart) or [iOS](https://www.tensorflow.org/lite/guide/ios),deployment need convert model already part run android,issue,negative,neutral,neutral,neutral,neutral,neutral
1845206596,"Hi, the demo response jsonl `data/input_response_data_text_bison.jsonl` seems to be stale, too! 
The demo command cannnot be executed.
```
I1207 11:08:47.873337 140161660089536 evaluation_main.py:256] Generating eval_results_strict...
Traceback (most recent call last):
  File ""/root/IFEval/ifeval/evaluation_main.py"", line 277, in <module>
    app.run(main)
  File ""/root/miniconda3/envs/alpaca_eval/lib/python3.10/site-packages/absl/app.py"", line 308, in run
    _run_main(main, args)
  File ""/root/miniconda3/envs/alpaca_eval/lib/python3.10/site-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""/root/IFEval/ifeval/evaluation_main.py"", line 259, in main
    outputs.append(func(inp, prompt_to_response))
  File ""/root/IFEval/ifeval/evaluation_main.py"", line 101, in test_instruction_following_strict
    response = prompt_to_response[inp.prompt]
KeyError: 'Write a 300+ word summary of the wikipedia page ""https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli"". Do not use any commas and highlight at least 3 sections that has titles in markdown format, for example *highlighted section part 1*, *highlighted section part 2*, *highlighted section part 3*.'
```",hi response stale command executed generating recent call last file line module main file line run main file line main file line main file line response word summary page use highlight least markdown format example section part section part section part,issue,negative,negative,neutral,neutral,negative,negative
1842384305,"@shakedzy I am fixing similar issue, were you able to solve this?",fixing similar issue able solve,issue,negative,positive,positive,positive,positive,positive
1839449965,"Hi @dibbla, sorry I completely missed this notification. I believe we used a [frame stack](https://github.com/google-research/google-research/blob/master/xirl/sac/wrappers.py#L38) value of 3 (17 x 3 = 51), [see here](https://github.com/google-research/google-research/blob/master/xirl/base_configs/rl.py#L50).",hi sorry completely notification believe used frame stack value see,issue,negative,negative,negative,negative,negative,negative
1839363117,"Hi, the demo with custom text embeddings is now available at [here](https://github.com/google-research/google-research/blob/eae296c7e3668db1fceab3c0e371b1714f87a286/fvlm/dito/demo.py#L98) ",hi custom text available,issue,negative,positive,positive,positive,positive,positive
1827178209,Sorry. There's a stupid bug. The prompt set is wrong. Fix coming in within 12 hours.,sorry stupid bug prompt set wrong fix coming within,issue,negative,negative,negative,negative,negative,negative
1820154431,"@creiser 
Thanks for your reply! 
And could you please provide the per-scene fps or rendering time results? Since it seems that the webviewer allows 60 fps rendering at most, its hard for us to reproduce the fps numbers.",thanks reply could please provide rendering time since rendering hard u reproduce,issue,positive,negative,neutral,neutral,negative,negative
1817450880,"I encountered the same problem, hope it gets fixed soon, I can't wait to try out this fantastic model. ",problem hope fixed soon ca wait try fantastic model,issue,positive,positive,positive,positive,positive,positive
1814846613,"just noticed the same. it seems that the `prompt_to_repeat` is in the first line of `prompt`. 
I fixed it with 
```python
if isinstance(instruction, instructions.RepeatPromptThenAnswer):
  instruction.build_description(prompt_to_repeat=inp.prompt.split(""\n"")[0],**inp.kwargs[index])
else:
  instruction.build_description(**inp.kwargs[index])
```
.. in `test_instruction_following_loose()` and `test_instruction_following_strict()`",first line prompt fixed python instruction index else index,issue,negative,positive,positive,positive,positive,positive
1814239194,"And by the way: These two methods((`removeprefix` and `removesuffix`)) were introduced in Python 3.9. 
[Code](https://github.com/google-research/google-research/blob/master/instruction_following_eval/instructions.py#L460) will not work in older version. 
Maybe you should declare it in the readme? ",way two python code work older version maybe declare,issue,negative,positive,positive,positive,positive,positive
1808812523,"Thanks for your kind words @Yiming-M. These are great suggestions/ideas. Due to our bandwidth, we do not have a concrete timeline/plan to prioritize these at the moment, but we'll take these into account for our future projects. ",thanks kind great due concrete moment take account future,issue,positive,positive,positive,positive,positive,positive
1803841951,"Hi,

Not sure if you are referring to something else. If you are talking about using AutoProcessor and Pix2Struct processor.

I checked AutoProcessor and Pix2StructImageProcessor. Turns out they output the same values. Understandable because Matcha uses Pix2Struct architecture.
 
![image](https://github.com/google-research/google-research/assets/106730702/914ebcaf-7d09-4f8a-b36f-bdf77efaf700)",hi sure something else talking processor checked turn output understandable architecture image,issue,negative,positive,positive,positive,positive,positive
1795947858,"PS: You can inspect the tokenizer to see what the available language code ares

```python
for i in range(500):
  print(i, tokenizer.decode(i))
```

> 4 <2ace>
> 5 <2ace_Arab>
> 6 <2af>
> 7 <2am>
> 8 <2an>
> 9 <2ar>
> 10 <2ary>
> 11 <2arz>
> 12 <2as>
> 13 <2az>
> 14 <2ba>
> 15 <2ban>
> 16 <2bar>
> 17 <2be>
> 18 <2bg>
> 19 <2bho>
> 20 <2bjn>
> ...",inspect see available language code python range print ace ar ary ba ban bar,issue,negative,positive,positive,positive,positive,positive
1795922339,"No problem! I'm glad it worked.

To choose the target language, just prepend the input text with `""<2xx>""`, where `xx` is the BCP-47 language tag.

For example:

```
from transformers import T5ForConditionalGeneration, T5Tokenizer, GenerationConfig

model = T5ForConditionalGeneration.from_pretrained(
    'jbochi/madlad400-3b-mt', device_map=""auto"")
tokenizer = T5Tokenizer.from_pretrained('jbochi/madlad400-3b-mt')

text = [""<2pt> I love pizza"", ""<2es> I love pizza"", ""<2it> I love pizza""]
input_ids = tokenizer(text, return_tensors=""pt"").input_ids

outputs = model.generate(
    input_ids=input_ids.to('cuda')
)

for i in range(len(outputs)):
    print(tokenizer.decode(outputs[i], skip_special_tokens=True))
```

This prints:
> Eu amo pizza
> Me encanta la pizza
> Amo la pizza",problem glad worked choose target language input text language tag example import model auto text love pizza e love pizza love pizza text range print eu pizza la pizza la pizza,issue,positive,positive,positive,positive,positive,positive
1795890680,"> @atillabasaran I can't reproduce the error. What version of `transformers`, `tokenizers`, and `sentencepiece` do you have?
> 
> I tested with these:
> 
> ```
> transformers==4.35.0
> sentencepiece==0.1.99
> tokenizers==0.14.1
> ```

I was working on 4.33 now it's fixed, thanks. Now I want to ask how can I choose language? ",ca reproduce error version tested working fixed thanks want ask choose language,issue,negative,positive,positive,positive,positive,positive
1795857182,"@atillabasaran I can't reproduce the error. What version of `transformers`, `tokenizers`, and `sentencepiece` do you have?

I tested with these:

```
transformers==4.35.0
sentencepiece==0.1.99
tokenizers==0.14.1
```",ca reproduce error version tested,issue,negative,neutral,neutral,neutral,neutral,neutral
1795680208,"> Hello, I've converted all the machine learning translation models to huggingface's transformers / safetensors.
> 
> You can find them in this collection: https://huggingface.co/collections/jbochi/madlad-400-65491e6a78726cac9a4b84b7
> 
> ```python
> from transformers import T5ForConditionalGeneration, T5Tokenizer, GenerationConfig
> 
> model = T5ForConditionalGeneration.from_pretrained('jbochi/madlad400-3b-mt')
> tokenizer = T5Tokenizer.from_pretrained('jbochi/madlad400-3b-mt')
> 
> text = ""<2pt> I love pizza!""
> input_ids = tokenizer(text, return_tensors=""pt"").input_ids
> outputs = model.generate(input_ids=input_ids)
> 
> tokenizer.decode(outputs[0], skip_special_tokens=True)
> # Eu adoro pizza!
> ```
> 
> IMO, inference with these models is simpler than with T5X, but it doesn't fully answer how to reproduce the evaluation from the paper.

I got this errror while loading tokenizer.
ValueError: Non-consecutive added token '<extra_id_99>' found. Should have index 256100 but has index 256000 in saved vocabulary.
",hello converted machine learning translation find collection python import model text love pizza text eu pizza inference simpler fully answer reproduce evaluation paper got loading added token found index index saved vocabulary,issue,positive,positive,positive,positive,positive,positive
1795573110,"Hello, I've converted all the machine learning translation models to huggingface's transformers / safetensors.

You can find them in this collection: https://huggingface.co/collections/jbochi/madlad-400-65491e6a78726cac9a4b84b7

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer, GenerationConfig

model = T5ForConditionalGeneration.from_pretrained('jbochi/madlad400-3b-mt')
tokenizer = T5Tokenizer.from_pretrained('jbochi/madlad400-3b-mt')

text = ""<2pt> I love pizza!""
input_ids = tokenizer(text, return_tensors=""pt"").input_ids
outputs = model.generate(input_ids=input_ids)

tokenizer.decode(outputs[0], skip_special_tokens=True)
# Eu adoro pizza!
```

IMO, inference with these models is simpler than with T5X, but it doesn't fully answer how to reproduce the evaluation from the paper.",hello converted machine learning translation find collection python import model text love pizza text eu pizza inference simpler fully answer reproduce evaluation paper,issue,positive,positive,positive,positive,positive,positive
1793716860,"> > Thank you for the good work of SMURF. I am trying to replicate the reported performance on the training set of the Sintel dataset, namely 1.71 EPE on Clean and 2.58 EPE on Final.
> > I used apply_smurf.py on the sintel sequences but cannot get the same performance. Could you please provide some instructions on what commands to run?
> > Thank you.
> 
> It turns out that the provided Sintel model is Sintel-Test (which is useful to me as well). In case this information is useful @remimar. In addition, adjusting max_rec_iters can make a difference, say 16.

Can you reproduce the results of the paper from scratch? There is a big gap between my reproduced results and those reported in the paper.
",thank good work trying replicate performance training set namely clean final used get performance could please provide run thank turn provided model useful well case information useful addition make difference say reproduce paper scratch big gap paper,issue,positive,positive,positive,positive,positive,positive
1793532633,"The dataset seems to be available in https://huggingface.co/datasets/allenai/MADLAD-400

I've converted the 3b param model to transformer's T5ForConditionalGeneration format for easier experimentation.

It's available here:
- [huggingface model](https://huggingface.co/jbochi/madlad400-3b-mt)
- [huggingface space](https://huggingface.co/spaces/jbochi/madlad400-3b-mt)

<img width=""786"" alt=""image"" src=""https://github.com/google-research/google-research/assets/292712/e9292e4a-ba8b-4cad-acb6-d0f93c98fc51"">

Disclaimer: I'm not currently affiliated with Google and I was not involved in this research.",available converted param model transformer format easier experimentation available model space image disclaimer currently involved research,issue,negative,positive,positive,positive,positive,positive
1791845862,"@ianbstewart any progress on it? The model checkpoint is missing some key files.
OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /content/checkpoint_1140000.

I wanted to make use of hugging face as server.",progress model missing key error file found directory make use hugging face server,issue,negative,negative,neutral,neutral,negative,negative
1782721743,"> I've implemented a 3D version of the non max suppression algorithm in C following the tensorflow tutorial to create custom operation. I've compiled, run the basic tests (similar to those in 2D that can be seen found in tensoflow sources) and used it with success within a deep learning algorithm. I'm going to publish this 3D NMS in the tensorflow repositories in the next few days: I'll then add a link here.

Hi! If you had the chance to finish this, can you please post the link? Thank you!",version non suppression algorithm following tutorial create custom operation run basic similar seen found used success within deep learning algorithm going publish next day add link hi chance finish please post link thank,issue,positive,positive,neutral,neutral,positive,positive
1773957638,"Hi, don't you know how to convert trillsson to ONNX then get the embedding?",hi know convert get,issue,negative,neutral,neutral,neutral,neutral,neutral
1771714684,"Update on my question: I was able to train and save ScaNN using the TensorFlow implementation available at: https://www.tensorflow.org/recommenders/examples/efficient_serving

I'd like to highlight two important notes:
1) If you are using ScaNN for document indexing, remember to normalize the embeddings before feeding them to the layer.

`normalized_dataset = embeddins_array / np.linalg.norm(embeddins_array , axis=1)[:, np.newaxis]
record_count = normalized_dataset.shape[0]
embeddings = tf.convert_to_tensor(normalized_dataset, dtype=tf.float32)        
embedding_dataset = tf.data.Dataset.from_tensor_slices(embeddings)
scann_model = tfrs.layers.factorized_top_k.ScaNN(num_reordering_candidates=record_count,
                                                   num_leaves_to_search=record_count,
                                                   distance_measure='dot_product')
scann_model.index_from_dataset(embedding_dataset.batch(32))`

2) Before saving the trained model, be sure to activate the model by making at least one inference. Otherwise, it won't save.
`scann_model(query_embedding,top_k)`

I hope this information is helpful to others! Cheers!",update question able train save implementation available like highlight two important document indexing remember normalize feeding layer saving trained model sure activate model making least one inference otherwise wo save hope information helpful,issue,positive,positive,positive,positive,positive,positive
1767258700,"it looks like an old version of tf, please try nightly version.",like old version please try nightly version,issue,positive,positive,neutral,neutral,positive,positive
1758447477,"@DavidWiesner and @pseudonymmm , I am using  ScaNN version 1.2.5. However, I am not able to save the serialized version of the searcher. I have used both numpy and tf approaches. For tf approach, I have even this error that the searcher does not have serialize_to_module().
```
record_count = normalized_dataset.shape[0]
print(""start_reasercher"")
start = time.time()
searcher = (
    scann.scann_ops_pybind.ScannSearche.builder(normalized_dataset, 10, ""dot_product"")
    .tree(
        num_leaves=record_count,
        num_leaves_to_search=record_count,
        training_sample_size=record_count,
    )
    .score_ah(2, anisotropic_quantization_threshold=0.2)
    .reorder(100)
    .build())

os.makedirs('.scann_artefacts/scann_index', exist_ok=True)
searcher.serialize(f'.scann_artefacts/scann_index')    
    
```
Within the specified directory, I have a file named 'scann_assets.pbtxt,' but I'm encountering the following error:
``` RuntimeError: Failed reading scann_config.pb: Failed to open file .scann_artefacts/scann_index/scann_config.pb.``` 
I have changed the version to another 1.2.6 and now I even have an empty directory as the code gets terminated in the middle of serialization. 
I believe my major problem is that the size of ScaNN is big and I am out of memory since my searcher is built based on a (89k, 5500) document embedding. 
I would greatly appreciate your assistance in resolving this issue. If you have any insights or suggestions, please let me know. Additionally, if you can provide guidance on how to serialize the ScaNN searcher properly, it would be of immense help.

Thank you in advance for your time and support.",version however able save version searcher used approach even error searcher print start searcher within directory file following error reading open file version another even empty directory code middle serialization believe major problem size big memory since searcher built based document would greatly appreciate assistance issue please let know additionally provide guidance serialize searcher properly would immense help thank advance time support,issue,positive,positive,positive,positive,positive,positive
1758257016,"gotcha, thanks! i will work on adapting the code to make it work on video stream
",thanks work code make work video stream,issue,negative,positive,positive,positive,positive,positive
1758208662,"The colab has support to record a webcam video and and run RepNet in the ""Get Video from Webcam"" section. It is possible to adapt the code to make it work on a stream of video but it is not supported in the current colab. ",support record video run get video section possible adapt code make work stream video current,issue,negative,neutral,neutral,neutral,neutral,neutral
1757575983,"Thanks so much Debidatta, it works now! really appreciate the help. 

Another question: Is there anyway to use this model on real-time webcam streams? ",thanks much work really appreciate help another question anyway use model,issue,positive,positive,positive,positive,positive,positive
1756393246,"Hi @vantony1 , I updated the colab to adjust for some API changes [here](https://github.com/google-research/google-research/commit/c53d8fbc574ff4b1037e22b392505584b1af6fc4) .  Let me know if it doesn't work.",hi adjust let know work,issue,negative,neutral,neutral,neutral,neutral,neutral
1754206855,"afaik they're having copyright issues (understandably) and AI2 is handling the release, so just give them time. 

@sneha-rk just floating an idea, would it be possible to instead generate a few (un?)conditional samples from the 8B param LM trained on madlad-400, and post those in lieu of the original dataset for people interested in getting a sense of its quality / coverage? not sure if that would be more feasible in terms of legal complexity or whether the team has any time for it? 

I was imagining starting it off with sentences from each language in the UDHR (https://github.com/thammegowda/014-udhr-dataset/blob/main/data/romanized/udhr_007.tsv), which could work both as a means of assessing memorisation, and for eliciting a diverse selection of languages since I don't think the training procedure used language-id control codes...

would love to see how coherent it is for hakha chin in particular. and also, no pressure – this is incredible work and I'm excited to see whatever comes next!!
",copyright understandably ai handling release give time floating idea would possible instead generate un conditional param trained post lieu original people interested getting sense quality coverage sure would feasible legal complexity whether team time starting language could work diverse selection since think training procedure used control would love see coherent chin particular also pressure incredible work excited see whatever come next,issue,positive,positive,positive,positive,positive,positive
1751964253,"@amughrabi Hi!
Did you know how to solve this problem?
Here is the problem I met:
```
user@juneyoung:~/MixNeRF$ /usr/local/python3.6.15/bin/python3 -m pip install -r requirements.txt 
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: gin-config==0.5.0 in /home/user/.local/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.5.0)
Requirement already satisfied: absl-py==0.15.0 in /home/user/.local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.15.0)
Requirement already satisfied: flax==0.3.5 in /home/user/.local/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.3.5)
Collecting tensorflow==2.6.2
  Using cached tensorflow-2.6.2-cp36-cp36m-manylinux2010_x86_64.whl (458.3 MB)
Requirement already satisfied: opencv-python==4.5.5.62 in /home/user/.local/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (4.5.5.62)
Collecting oryx==0.2.1
  Using cached oryx-0.2.1-py3-none-any.whl (192 kB)
Collecting jax==0.2.16
  Using cached jax-0.2.16-py3-none-any.whl
Requirement already satisfied: scikit-image==0.17.2 in /home/user/.local/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (0.17.2)
Requirement already satisfied: dm-pix==0.3.0 in /home/user/.local/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (0.3.0)
Requirement already satisfied: six in /home/user/.local/lib/python3.6/site-packages (from absl-py==0.15.0->-r requirements.txt (line 2)) (1.15.0)
Requirement already satisfied: dataclasses in /home/user/.local/lib/python3.6/site-packages (from flax==0.3.5->-r requirements.txt (line 3)) (0.8)
Requirement already satisfied: optax in /home/user/.local/lib/python3.6/site-packages (from flax==0.3.5->-r requirements.txt (line 3)) (0.0.9)
Requirement already satisfied: msgpack in /home/user/.local/lib/python3.6/site-packages (from flax==0.3.5->-r requirements.txt (line 3)) (1.0.5)
Requirement already satisfied: matplotlib in /home/user/.local/lib/python3.6/site-packages (from flax==0.3.5->-r requirements.txt (line 3)) (3.3.4)
Requirement already satisfied: numpy>=1.12 in /home/user/.local/lib/python3.6/site-packages (from flax==0.3.5->-r requirements.txt (line 3)) (1.19.5)
Collecting gast==0.4.0
  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)
Requirement already satisfied: typing-extensions~=3.7.4 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (3.7.4.3)
Requirement already satisfied: protobuf>=3.9.2 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (3.19.6)
Requirement already satisfied: astunparse~=1.6.3 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (1.6.3)
Requirement already satisfied: clang~=5.0 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (5.0)
Collecting tensorboard<2.7,>=2.6.0
  Using cached tensorboard-2.6.0-py3-none-any.whl (5.6 MB)
Requirement already satisfied: google-pasta~=0.2 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (0.2.0)
Requirement already satisfied: opt-einsum~=3.3.0 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (3.3.0)
Requirement already satisfied: termcolor~=1.1.0 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (1.1.0)
Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (1.1.2)
Collecting tensorflow-estimator<2.7,>=2.6.0
  Using cached tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)
Requirement already satisfied: keras<2.7,>=2.6.0 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (2.6.0)
Requirement already satisfied: grpcio<2.0,>=1.37.0 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (1.48.2)
Collecting h5py~=3.1.0
  Using cached h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)
Requirement already satisfied: wheel~=0.35 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (0.37.1)
Requirement already satisfied: wrapt~=1.12.1 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (1.12.1)
Requirement already satisfied: flatbuffers~=1.12.0 in /home/user/.local/lib/python3.6/site-packages (from tensorflow==2.6.2->-r requirements.txt (line 4)) (1.12)
INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.
Collecting opencv-python==4.5.5.62
  Using cached opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)
INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of flax to determine which version is compatible with other requirements. This could take a while.
Collecting flax==0.3.5
  Using cached flax-0.3.5-py3-none-any.whl (193 kB)
INFO: pip is looking at multiple versions of absl-py to determine which version is compatible with other requirements. This could take a while.
Collecting absl-py==0.15.0
  Using cached absl_py-0.15.0-py3-none-any.whl (132 kB)
INFO: pip is looking at multiple versions of gin-config to determine which version is compatible with other requirements. This could take a while.
Collecting gin-config==0.5.0
  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)
ERROR: Could not find a version that satisfies the requirement jaxlib==0.1.68 (from oryx) (from versions: 0.1.63)
ERROR: No matching distribution found for jaxlib==0.1.68
```",hi know solve problem problem met user pip install user installation normal writeable requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied six line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied line pip looking multiple determine version compatible could take pip looking multiple determine version compatible could take pip looking multiple flax determine version compatible could take pip looking multiple determine version compatible could take pip looking multiple determine version compatible could take error could find version requirement error matching distribution found,issue,positive,positive,positive,positive,positive,positive
1750120751,"I had the same Scann install issue for inference testing (my TFRS+Scann model was not trained on M1), it can be solved by using docker. Start colima with the following options: `colima start --arch x86_64 --cpu 4 --cpu-type ""max"" --memory 8`. Then run the image with your model (see image creation example [here](https://www.tensorflow.org/tfx/serving/docker#creating_your_own_serving_image), replacing the TFServing image by the [TF+Scann image](https://hub.docker.com/r/google/tf-serving-scann)): `docker run --platform linux/amd64 -p 8501:8501 -t yourimage:yourtag`.",install issue inference testing model trained docker start colima following colima start arch memory run image model see image creation example image image docker run platform,issue,negative,neutral,neutral,neutral,neutral,neutral
1732255896,"@earl-juanico Hi, how did you reproduce the results of their paper? Using the author's code? I tried to reproduce the results of R50 in the paper on coco dataset but did not reproduce its results. I hope you give detailed steps to reproduce it. Did you make your own training set containing only the base categories to train the model?
Thanks for your help and reply!",hi reproduce paper author code tried reproduce paper coco reproduce hope give detailed reproduce make training set base train model thanks help reply,issue,positive,negative,neutral,neutral,negative,negative
1732253493,"Hi @DMUgjj. I have so far been able to do it for the RN50 backbone. Next up, will be RN50x4 and RN50x16. 

Hello @weichengkuo, thank you so much for providing the precomputed embeddings. I am still curious if you have tried other padding sizes, e.g., less than 1601 or more than 1601, like a hyperparameter perhaps?",hi far able backbone next hello thank much providing still curious tried padding size le like perhaps,issue,positive,positive,positive,positive,positive,positive
1732246029,"@deepinact Hi, have you successfully reproduced the experimental results from the paper? Including COCO or LVIS?",hi successfully experimental paper coco,issue,negative,positive,neutral,neutral,positive,positive
1732178720,@rybakov streaming ds_tc_resnet evaluated with reset_state=False shows significant degradation compare to reset_state=True. is it expected?,streaming significant degradation compare,issue,negative,positive,positive,positive,positive,positive
1727000346,"Hi, sorry to bother you, but may I ask if your code can run? I always report errors and missing various things here (such as stacked_fc.so, and the AverageLayer class in model/coverage. py). Is the version I downloaded incorrect?

btw, I used the LLFF dataset, so I didn't meet the problem you met, sorry.",hi sorry bother may ask code run always report missing various class version incorrect used meet problem met sorry,issue,negative,negative,negative,negative,negative,negative
1724684211,"Thanks for the question @buzuyun. So for example in the top-left tile, we visualize the similarity value between the positional embedding (PE) of the top-left location (1xD) and PEs of all other locations (14x14xD) which results in the 14x14 values. Hope this helps clarifying.",thanks question example tile visualize similarity value positional location pe hope,issue,positive,positive,positive,positive,positive,positive
1722348152,"Hi @earl-juanico, they are padded to 1601 due to the background (first entry) and empty embeddings (remaining entries). FYI - https://github.com/google-research/google-research/issues/1731. Thanks!",hi due background first entry empty thanks,issue,negative,positive,neutral,neutral,positive,positive
1722347648,"No problem, yes the shape comes from adding the background and empty embeddings. Thanks! ",problem yes shape come background empty thanks,issue,negative,positive,neutral,neutral,positive,positive
1721979416,"Hi @ahmad-mirsalari, thank you for your feedback! Sorry but we do not extend public models repository now (but we still support open sourced models and streaming aware layers).  Please feel free to add streaming version of tcn. You could use [ds_tc_resnet](kws_streaming/models/ds_tc_resnet.py) as an example.",hi thank feedback sorry extend public repository still support open streaming aware please feel free add streaming version could use example,issue,positive,positive,neutral,neutral,positive,positive
1720351285,I have verified the evaluation on LVIS 1.0 with the embeddings. Thank you @weichengkuo. Any special reason why the precomputed category embeddings have for both COCO and LVIS datasets have 1601 rows? ,evaluation thank special reason category coco,issue,positive,positive,positive,positive,positive,positive
1717559183,"I had a similar error and turns out i had a couple of empty lines in my labels file, which contained pathes to images.",similar error turn couple empty file,issue,negative,negative,neutral,neutral,negative,negative
1717429850,"Upon investigating, the shape comes from adding the background embedding and additional empty embeddings. Thank you once again!",upon investigating shape come background additional empty thank,issue,negative,negative,neutral,neutral,negative,negative
1714142748,"Thank you for the prompt answer @weichengkuo, your answer definitely helps! I just had one last question regarding the pre-computed embeddings. Upon inspecting the resnet_50 precomputed  COCO embeddings, I notice they have a shape of `(1601, 1024)`, which is more than the number of classes in the dataset. May I know if there is some processing done to the class labels? ",thank prompt answer answer definitely one last question regarding upon coco notice shape number class may know done class,issue,positive,neutral,neutral,neutral,neutral,neutral
1712979596,Thank you so much @weichengkuo . I will test the embeddings soon. :),thank much test soon,issue,negative,positive,positive,positive,positive,positive
1712976436,"Thanks for the question @luisdavid64! Yes - I'll recommend using our reproduced CLIP (see example [here](https://github.com/google-research/google-research/blob/b7cb09ba04bf018f1ba1131ccd4c69be617e8e2f/fvlm/demo.py#L75) rather than the official CLIP for optimal performance, as there may be some minor/slight differences. The [demo.py](https://github.com/google-research/google-research/blob/master/fvlm/demo.py) shows an example of how to compute custom text embeddings for F-VLM, which are based on this input [argument](https://github.com/google-research/google-research/blob/b7cb09ba04bf018f1ba1131ccd4c69be617e8e2f/fvlm/demo.py#L57). [Here](https://github.com/google-research/google-research/blob/b7cb09ba04bf018f1ba1131ccd4c69be617e8e2f/fvlm/demo.py#L101) is where the text embedding computation starts from a list of category names. This [line](https://github.com/google-research/google-research/blob/b7cb09ba04bf018f1ba1131ccd4c69be617e8e2f/fvlm/demo.py#L120) is where the text embeddings are fully computed. Hope these help!",thanks question yes recommend clip see example rather official clip optimal performance may example compute custom text based input argument text computation list category line text fully hope help,issue,positive,positive,positive,positive,positive,positive
1712974096,"Hi @earl-juanico, thanks for verifying the evaluation on COCO using the provided embeddings. I have prepared the LVIS precomputed embeddings at this URL: [https://storage.googleapis.com/cloud-tpu-checkpoints/detection/projects/fvlm/lvis_embeddings.zip](https://storage.googleapis.com/cloud-tpu-checkpoints/detection/projects/fvlm/lvis_embeddings.zip). You can download it for LVIS evaluation. Hope that helps!",hi thanks evaluation coco provided prepared evaluation hope,issue,positive,positive,positive,positive,positive,positive
1711331051,"> Hi, I am relatively confident that the problem lies in the data and data processing. I noticed that the number of converted insertion examples and converted tagging examples was sometimes not the same. To fix this, I changed line 112 of preprocess_main.py to `if example is not None and insertion_example is not None:` (and deleted line 115). This way, only data points are created, where both insertion and tagging example can be processed. But the NaN still occurred with my own data, and it turned out that some rare data points caused this; I had to manually identify these sentences using binary search on the data set. I did not find out what common factor could have caused this, but it appeared that some input sentences caused the NaN loss. I hope this helps a least a bit, good luck!

I did as your instruction, yet the NaN still occurred when I was running discofuse. The most strange thing was that the first epoch could be successfully trained, but NaN occurred in the second epoch, and this problem was occurring before your instruction. Do you have any idea about it, and did you try running discofuse, or just with your own data?  I desperately need further instruction, thank you very much!!  ╥﹏╥",hi relatively confident problem data data number converted insertion converted sometimes fix line example none none line way data insertion example nan still data turned rare data manually identify binary search data set find common factor could input nan loss hope least bit good luck instruction yet nan still running strange thing first epoch could successfully trained nan second epoch problem instruction idea try running data desperately need instruction thank much,issue,positive,positive,positive,positive,positive,positive
1710856794,"Hi, I am relatively confident that the problem lies in the data and data processing. I noticed that the number of converted insertion examples and converted tagging examples was sometimes not the same. To fix this, I changed line 112 of preprocess_main.py  to ```if example is not None and insertion_example is not None:``` (and deleted line 115). This way, only data points are created, where both insertion and tagging example can be processed. But the NaN still occurred with my own data, and it turned out that some rare data points caused this; I had to manually identify these sentences using binary search on the data set. I did not find out what common factor could have caused this, but it appeared that some input sentences caused the NaN loss. I hope this helps a least a bit, good luck! ",hi relatively confident problem data data number converted insertion converted sometimes fix line example none none line way data insertion example nan still data turned rare data manually identify binary search data set find common factor could input nan loss hope least bit good luck,issue,positive,positive,positive,positive,positive,positive
1706984597,"Great, thanks for verifying. Will send a fix soon.",great thanks send fix soon,issue,positive,positive,positive,positive,positive,positive
1705904321,"Hey, have you solved this problem? I have met the same problem as you.",hey problem met problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1705554406,"Thanks for the report. From the error, it looks like the default max number classes was not set correctly. Could you verify if it works when you run:
python3 demo.py --model=resnet_50 --max_num_classes=91",thanks report error like default number class set correctly could verify work run python,issue,negative,positive,positive,positive,positive,positive
1697915616,"> I can confirm I also have this issue.
> 
> Specifically this occurs in Colab when running a GPU instance. It does not happen if I run a CPU instance with the exact same code.
> 
> I also tried all other possible modes and they work perfectly ('int', 'multi_hot' and 'count').
> 
> Details:
> 
> Tesla K80 GPU, NVIDIA-SMI 460.32.03, Driver Version: 460.32.03, CUDA Version: 11.2, Python 3.7, Tensorflow 2

I face with similar problem when use Colab with output_mode = 'int'.
Did you solve it by running on other GPU (not Colab)?",confirm also issue specifically running instance happen run instance exact code also tried possible work perfectly driver version version python face similar problem use solve running,issue,positive,positive,positive,positive,positive,positive
1683575505,"Thanks for the update and correction. 

The TFX 1.13.0 (latest) release still targets tensorflow 2.12 explicitly. I guess we can wait for a future TFX release which will which will hopefully target tensorflow 2.13. Based on the last two releases, the TFX project seems good at not skipping minor tensorflow versions.  ",thanks update correction latest release still explicitly guess wait future release hopefully target based last two project good skipping minor,issue,positive,positive,positive,positive,positive,positive
1682255720,"There seems to have been an oversight in the latest release, stating to support 2.12 but requiring 2.13 in the install. I recon it's a hassle to reverse the requirements of that release, so I updated the releases.md instead in #1719 . Not sure if that helps you at all?",oversight latest release support install recon hassle reverse release instead sure,issue,positive,positive,positive,positive,positive,positive
1681560530,"> Have you figured out the solution by any chance?
> It would be very helpful if you could guide me past the error.

after i test ,this error must come from the dataset, the code of construct dmvr dataset ,but i don't know how to fix it ",figured solution chance would helpful could guide past error test error must come code construct know fix,issue,negative,negative,negative,negative,negative,negative
1680817137,Hi I got same issue here on M1 chip.,hi got issue chip,issue,negative,neutral,neutral,neutral,neutral,neutral
1679885126,"> `modality_1_all = modality_1`
> `modality_2_all = modality_2`

hello i have replace the line with 
modality_1_all = modality_1
modality_2_all = modality_2
but still have this problem ,and when i replace all this function,i still have the slice index 0 of dimension 0 out of bounds",hello replace line still problem replace function still slice index dimension,issue,negative,neutral,neutral,neutral,neutral,neutral
1679169025,"> Hi @hassanhub, Thanks for your reply, i have replaced the function with strategy.gather(tensor, axis=0). However I got the following error instead. RuntimeError: tf.distribute.Strategy.gather method requires cross-replica context, use get_replica_context().all_gather() instead. It seems that tensor is not cross-replica context. Any idea to fix the problem? Thanks in advance.

excuse me, can you tell me how you replace  this function, i also need to pass the strategy?thank you very much
",hi thanks reply function tensor however got following error instead method context use instead tensor context idea fix problem thanks advance excuse tell replace function also need pas strategy thank much,issue,negative,positive,positive,positive,positive,positive
1666932880,"The same issue, I hope the authors provide us with implementation and the dataset at their earliest convenience.",issue hope provide u implementation convenience,issue,negative,neutral,neutral,neutral,neutral,neutral
1666476612,"> Thanks for the question. You can use the `model.save_weights()` function instead. After that you can initialize the model and then load_weights.

Thanks a lot
",thanks question use function instead initialize model thanks lot,issue,positive,positive,positive,positive,positive,positive
1666270750,"> 

@simondoesstuff
Unfortunately no, I gave up. They just don't care how many people already using M1 chip.",unfortunately gave care many people already chip,issue,negative,neutral,neutral,neutral,neutral,neutral
1666244920,"> Not sure why google like to push people away from their product. This issue let me spent lot of time for nothing just to install this om M1 chip 👎

but, you **did** manage to install it on the m1?",sure like push people away product issue let spent lot time nothing install om chip manage install,issue,positive,positive,positive,positive,positive,positive
1659832257,"



> Hi, @hassanhub. I encountered this problem and failed to solve it after searching for several solutions to similar(just look like similar) issues.
> 
> As you mentioned in #962, I modified the `objectives.py` for conducting a multi-gpu pre-training experiment and encountered this problem. Then I tried to inplement a single-gpu version and also encountered this problem.
> 
> The error information is as:
> 
> ```
> 2022-02-11 15:07:20.266179: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.266326: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.266722: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.268130: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.268490: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> Traceback (most recent call last):
>   File ""main.py"", line 97, in <module>
>     app.run(main)
>   File ""anaconda3/envs/vatt/lib/python3.9/site-packages/absl/app.py"", line 303, in run
>     _run_main(main, args)
>   File ""anaconda3/envs/vatt/lib/python3.9/site-packages/absl/app.py"", line 251, in _run_main
>     sys.exit(main(argv))
>   File ""main.py"", line 93, in main
>     return executor.run(mode=params.mode)
>   File ""experiments/base.py"", line 439, in run
>     self.train()
>   File ""experiments/base.py"", line 280, in train
>     metrics = train_step(data_iterator, num_iterations)
>   File ""anaconda3/envs/vatt/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
>     raise e.with_traceback(filtered_tb) from None
>   File ""anaconda3/envs/vatt/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute
>     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
> tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
>   (0) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
>          [[{{node strided_slice_3}}]]
>          [[while/body/_1/while/IteratorGetNext]]
>          [[while/body/_1/while/model/tx_mlp_fac/backbone_stack/audio_module/wat_base/RandomShuffle/_72]]
>   (1) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
>          [[{{node strided_slice_3}}]]
>          [[while/body/_1/while/IteratorGetNext]]
> 0 successful operations.
> 0 derived errors ignored. [Op:__inference_train_step_97988]
> 
> Function call stack:
> train_step -> train_step
> 
> 2022-02-11 15:07:20.269632: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.269706: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.271021: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.271261: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.271735: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.274001: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.274614: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.275279: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.275625: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 15:07:20.276116: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds
> 
> I mainly modified the `base.py` and `objectives.py`. Since I'm not familiar with Tensorflow framework, the code base_and_objectives.tar.gz may exists a lot of  ridiculous problems. Do you have some idea about this problem?
> 
> Thanks.
> ```

hi,@AlyssaYoung have you fix this problem ,i'm also encounter this problem.
",hi problem solve searching several similar look like similar experiment problem tried version also problem error information slice index dimension slice index dimension slice index dimension slice index dimension slice index dimension recent call last file line module main file line run main file line main file line main return file line run file line train metric file line raise none file line root error found slice index dimension node slice index dimension node successful derived function call stack slice index dimension slice index dimension slice index dimension slice index dimension slice index dimension slice index dimension slice index dimension slice index dimension slice index dimension slice index dimension mainly since familiar framework code may lot ridiculous idea problem thanks hi fix problem also encounter problem,issue,negative,positive,positive,positive,positive,positive
1658252530,"> Are there more detailed instructions on how to set everything up and train a model to detect objects?
> 

[This](https://www.tensorflow.org/hub/tutorials/tf2_object_detection) resource has detailed instructions for setting up test environments to experiment with object detection models. The site includes a couple platforms that demonstrate the material, including **Google Colab** & **GitHub**. 

The **Tensorflow** documentation is a formidable educational resource to get started with machine learning, worth looking over. 

Hope this can be of some help getting started. ",detailed set everything train model detect resource detailed setting test experiment object detection site couple demonstrate material documentation formidable educational resource get machine learning worth looking hope help getting,issue,positive,positive,positive,positive,positive,positive
1657377758,"sorry, I made a mistake. I faced the ""flax has no attribute optim"" too. But I changed the version to '0.5.1' and fixed the problem. You can try it!
",sorry made mistake faced flax attribute version fixed problem try,issue,negative,negative,negative,negative,negative,negative
1657363329,"Hi, I used the ""flax=0.6.9""  and fixed the problem!",hi used fixed problem,issue,negative,positive,neutral,neutral,positive,positive
1656557004,"was able to build but getting this error
`pip install scann-1.2.9-cp38-cp38-macosx_13_0_x86_64.whl`

`ERROR: scann-1.2.9-cp38-cp38-macosx_13_0_x86_64.whl is not a supported wheel on this platform.`",able build getting error pip install error wheel,issue,negative,positive,positive,positive,positive,positive
1652983502,Not sure why google likes to push people away from their product. This issue let me spent lot of time for nothing just to install this om M1 chip 👎 ,sure push people away product issue let spent lot time nothing install om chip,issue,negative,positive,positive,positive,positive,positive
1651382960,"no it can't add any new embeddings into the index,  you have to rebuild the index using the new data every time in one-shot ",ca add new index rebuild index new data every time,issue,negative,positive,positive,positive,positive,positive
1651376493,"it seems not. the int8 dataset for reordering can't be added in runtime , that's the key point ",ca added key point,issue,negative,neutral,neutral,neutral,neutral,neutral
1645167427,"excuse me ,can you help me convert the raw videos and text to TFRecords,i have downloaded the two datasets
![image](https://github.com/google-research/google-research/assets/55910045/6b5dccfc-7f01-4198-88b2-f34379af8478)
how can i convert them? thank you",excuse help convert raw text two image convert thank,issue,positive,negative,negative,negative,negative,negative
1644086638,"Have you tried creating node embeddings -> pooling to create graph embeddings? These might cluster naturally depending on your data. Use something like node2vec for node embeddings, no labels needed.",tried node create graph might cluster naturally depending data use something like node,issue,positive,positive,neutral,neutral,positive,positive
1642653863,"thanks for the response. you hit upon a different issue. there are 2 issues:

- not having all the data available in advance (original issue)
- have the data available in advance but it will not fit in RAM

i need solutions to both

i guess the second issue might be addressed using streaming inserts using
index_from_dataset but I am not using tensorflow API.

these are the methods I am seeing:

```

>>> import scann


>>> dir(scann.scann_ops)

['ScannSearcher', 'ScannState', '__builtins__', '__cached__', '__doc__',
'__file__', '__loader__', '__name__', '__package__', '__spec__',
'_scann_ops_so', 'builder', 'create_searcher', 'os', 'scann_builder',
'*scann_create_searcher',
'scann_search', 'scann_search_batched*', 'scann_to_tensors',
'searcher_from_module', 'tensors_to_scan', 'tf', 'uuid']


>>> dir(scann.scann_ops.ScannSearcher)

['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__',
'__format__', '__ge__', '__getattribute__', '__gt__', '__hash__',
'__init__', '__init_subclass__', '__le__', '__lt__', '__module__',
'__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__',
'__setattr__', '__sizeof__', '__str__', '__subclasshook__',
'__weakref__', '*search',
'search_batched', 'search_batched_parallel*', 'serialize_to_module']


>>> dir(scann.scann_ops._scann_ops_so)

['Scann_ScannCreateSearcher', 'Scann_ScannSearch',
'Scann_ScannSearchBatched', 'Scann_ScannToTensors', 'Scann_TensorsToScann',
'TypeVar', '_IS_TENSORFLOW_PLUGIN', '_Scann_ScannSearchBatchedOutput',
'_Scann_ScannSearchOutput', '_Scann_ScannToTensorsOutput', '__builtins__',
'__doc__', '__loader__', '__name__', '__package__', '__spec__', '_context',
'_core', '_dispatch', '_dispatcher_for_scann_scann_create_searcher',
'_dispatcher_for_scann_scann_search',
'_dispatcher_for_scann_scann_search_batched',
'_dispatcher_for_scann_scann_to_tensors',
'_dispatcher_for_scann_tensors_to_scann', '_dtypes', '_execute',
'_op_def_library', '_op_def_registry', '_ops', 'collections',
'deprecated_endpoints', 'pywrap_tfe', 'scann_scann_create_searcher',
'scann_scann_create_searcher_eager_fallback', '*scann_scann_search',
'scann_scann_search_batched*', 'scann_scann_search_batched_eager_fallback',
'scann_scann_search_eager_fallback', 'scann_scann_to_tensors',
'scann_scann_to_tensors_eager_fallback', 'scann_tensors_to_scann',
'scann_tensors_to_scann_eager_fallback', 'tf_export']
```

On Tue, Jul 18, 2023 at 11:16 PM Izak Marais ***@***.***>
wrote:

> No, this is not possible. From the docs
> <https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/factorized_top_k/ScaNN>
> for *index*:
>
> When called multiple times the existing index will be dropped and a new
> one created.
>
> If you have a large dataset, you can use index_from_dataset to do a
> streaming insert, but you still need to have it all available a-priori.
>
> I am not a contributor, but I suggest you close this issue. What you are
> requesting is outside the scope of ScaNN as offered. You need to look at a
> more fully featured vector database.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1679#issuecomment-1641481101>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/A6NWEK4ZWKVVUG7UXCI2XJDXQ53TBANCNFSM6AAAAAA2H2VYNM>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",thanks response hit upon different issue data available advance original issue data available advance fit ram need guess second issue might streaming seeing import search tue wrote possible index multiple time index new one large use streaming insert still need available contributor suggest close issue outside scope need look fully featured vector reply directly view id,issue,positive,positive,positive,positive,positive,positive
1641488242,"When using scann from tensorflow recommenders the domentation is [here](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/factorized_top_k/ScaNN).  From the docs, you provide a default for k at index build time and you can override that at call time.",provide default index build time override call time,issue,negative,neutral,neutral,neutral,neutral,neutral
1641482467,See my comment [here](https://github.com/google-research/google-research/issues/1679). From the docs linked there you can see that the ScaNN API is very limited: this is not possible. Please close this issue and consider using a vector datbase for your use case. ,see comment linked see limited possible please close issue consider vector use case,issue,negative,negative,neutral,neutral,negative,negative
1641481101,"No, this is not possible. From the [docs](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/factorized_top_k/ScaNN) for **index**:
> When called multiple times the existing index will be dropped and a new one created.

If you have a large dataset, you can use index_from_dataset to do a streaming insert, but you still need to have it all available a-priori.

I am not a contributor, but I suggest you close this issue. What you are requesting is outside the scope of ScaNN as offered. You need to look at a more fully featured vector database. ",possible index multiple time index new one large use streaming insert still need available contributor suggest close issue outside scope need look fully featured vector,issue,negative,positive,positive,positive,positive,positive
1641474127,"Python 3.7 is now past end of life, so we would like to move on to the 3.9 based TFX 1.13 Docker image.

Also, https://github.com/tensorflow/tfx/issues/5777#issuecomment-1640564951 requests that I test a bug fix with TFX 1.13, but this issue is blocking that test.",python past end life would like move based docker image also test bug fix issue blocking test,issue,negative,negative,negative,negative,negative,negative
1637679118,"This problem can be fixed by pinning the version of pandas like 

`pandas==0.25.3`

in the `requirements.txt` file. 

`pandas` unfortunately is not backward compatible and changed its behaviour, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike 

And the authors of tft unfortunately ignored the warning one gets when running their code with `pandas 0.25.3`:

    Passing list-likes to .loc or [] with any missing label will raise
    KeyError in the future, you can use .reindex() as an alternative.
",problem fixed pinning version like file unfortunately backward compatible behaviour see unfortunately warning one running code passing missing label raise future use alternative,issue,negative,negative,negative,negative,negative,negative
1635258014,Hi could you allow me to work on it.,hi could allow work,issue,negative,neutral,neutral,neutral,neutral,neutral
1631819628,I close the issues since it's my fault. I accidentally added a metric which slow the training process through time.,close since fault accidentally added metric slow training process time,issue,negative,negative,negative,negative,negative,negative
1627908142,"There is a small change to the flag here.

Change to: flags.DEFINE_integer('num_blocks', 4, 'number of mixer blocks')
Comment out: # flags.DEFINE_integer('num_blocks', 6, 'number of mixer blocks')

This should work on both single and multi-GPUs.",small change flag change mixer comment mixer work single,issue,negative,negative,negative,negative,negative,negative
1621444523,I used AVA checkpoint and I faced a similar problem. ,used ava faced similar problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1613528397,"Good point. We copied our implementation off Algorithm 1 in https://arxiv.org/abs/2303.03451 and didn't notice the point about symmetric noise in the proof of Theorem A.3.

I re-ran the k=5 BAS, L-BAS, and K-BAS experiments with the symmetric noise fix and didn't see a significant change in the R^2 values or the resulting rankings relative to the other methods, so I don't think this bug meaningfully affects the experiment conclusions. Nonetheless, we'll include this fix in the next code update.",good point copied implementation algorithm notice point symmetric noise proof theorem ba symmetric noise fix see significant change resulting relative think bug meaningfully experiment nonetheless include fix next code update,issue,positive,positive,positive,positive,positive,positive
1609923129,"> > Hey! Even I faced this issue. Using pip install flax==0.6.9 fixed the issue for me.
> 
> I try your suggestions and i got this,do you konw how to solve it? ![微信截图_20230530210856](https://user-images.githubusercontent.com/94942498/241977346-9acbce52-7417-4445-9ba4-994c0d7c462e.png)

met same issue, did you solve it?",hey even faced issue pip install fixed issue try got solve met issue solve,issue,negative,positive,neutral,neutral,positive,positive
1601024747,"The issues were related to the TensorFlow version, right now I'm using the implementation from pytorch forecasting, but i would like to use your source code",related version right implementation forecasting would like use source code,issue,negative,positive,positive,positive,positive,positive
1600972096,"Electricity worked for me.
I just checked the link of volatility, and it seems it got removed/is no longer accessible. ",electricity worked checked link volatility got longer accessible,issue,negative,positive,positive,positive,positive,positive
1598209281,"ah, i can open the RICO home page now.",ah open home page,issue,negative,neutral,neutral,neutral,neutral,neutral
1598180213,"> https://www.kaggle.com/datasets/onurgunes1993/rico-dataset

Hi halimaqadah,

I checked this dataset, but it doesn't contain interaction traces. I hope to download the complete RICO dataset. But anyway, thanks for your answer.

Cheers,
Ryan",hi checked contain interaction hope complete anyway thanks answer,issue,positive,positive,positive,positive,positive,positive
1596197119,"Hi @rybakov, here are some of my observations:
- [audio_processor.get_data](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#LL148C46-L148C70) take ~0.5s on step 2, ~1s on step 400, ~1.7s on step > 1000 and it keeps increasing. (batch size = 128)
- [model.train_on_batch](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L167) take ~0.06s everytime.

I have tried:
- Read all the wav into memory instead of [read file and decode wav](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py#L413), but the problem still persist, so it's not IO problem. 
- Run on CPU only, but the time loading each batch still increase, so it's not CPU to GPU problem.
- Measure the GPU time processing each batch. It isn't change, so it's not GPU problem.
- Run on different machine

As my measure, the mean time need to handle each wav file increases over time (it only takes approximately 0.005s for each wav on the first 100 steps, but increase larger than 0.01s when step > 1000), and the problem lie in [session.run() /input_data.py#L672](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py#L672). Maybe running `sess.run()` over and over again make something bigger, and slower over time? Is this problem related to [this issue](https://github.com/tensorflow/tensorflow/issues/1439)? I'm not very experienced debugging Tensorflow graph, so I still can't find the core problem. Could you look more into this problem? Thank you.
",hi take step step step increasing batch size take tried read memory instead read file decode problem still persist io problem run time loading batch still increase problem measure time batch change problem run different machine measure mean time need handle file time approximately first increase step problem lie maybe running make something bigger time problem related issue experienced graph still ca find core problem could look problem thank,issue,negative,positive,positive,positive,positive,positive
1595568872,"Hi @HKAB I have not seen this kind of issue before. I trained all kws_streaming models on laptop with GTX 1060 6GB. RTX 3090 is much more powerful, so compute should not be a problem. I would suggest to check GPU usage and GPU memory using nvidia-smi and see how it changes in time; also check hard drive, may be there is fragmentation issue (I would assume that bottleneck is in IO)? Currently TF will not use all GPU memory with [gpu_options.allow_growth](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L50)",hi seen kind issue trained much powerful compute problem would suggest check usage memory see time also check hard drive may fragmentation issue would assume bottleneck io currently use memory,issue,negative,positive,positive,positive,positive,positive
1594931439,"The last token is the [CLS] token (for captioning purpose), so
contrastive_text_embed is effectively the [CLS] token from the unimodal
text decoder.

On Fri, Jun 16, 2023 at 6:38 AM Fatemeh Behrad ***@***.***>
wrote:

> Hello,
> Dear @junjiek <https://github.com/junjiek>,
> Thank you for releasing the code of VILA.
> On the fifth page of the paper, it is written that
> we use “good image” as the prompt, and extract its normalized frozen text
> embedding from the wp [CLS] position of the unimodal text decoder
>
> However, I found the following line
> <https://github.com/google-research/google-research/blob/77a6e1bc63eea192498a2142c930154462b1754b/vila/coca_vila.py#L235>
> in the code which shows [CLS] output is removed.
> contrastive_text_embed = unimodal_emb[:, -1]
>
> I just want to check which one is correct. Am I misunderstood?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1628>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACHYBJ5YG4BVLPT7K2LHQTLXLROTHANCNFSM6AAAAAAZJHOXCA>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",last token token purpose effectively token unimodal text wrote hello dear thank code vila fifth page paper written use good image prompt extract frozen text position unimodal text however found following line code output removed want check one correct misunderstood reply directly view id,issue,positive,positive,positive,positive,positive,positive
1585037550,Thanks for the question. You can use the ```model.save_weights()``` function instead. After that you can initialize the model and then load_weights.,thanks question use function instead initialize model,issue,negative,positive,positive,positive,positive,positive
1577742131,"Hello @AhmedTElthakeb,

TFLite converter in [utils.py](https://github.com/google-research/google-research/blob/cd76f8373df19f3268d150c467cf4d9241b7017b/kws_streaming/models/utils.py)
with parameters:
```
  converter.experimental_new_quantizer = True
  converter.experimental_enable_resource_variables = True
  converter._experimental_variable_quantization = True
```
should remove Quantize/Dequantize between ops (at least between streaming states) for streaming with internal state.
You will need to train ds_tc_resnet with QAT as in example test for [ds_tc_resnet](https://github.com/google-research/google-research/blob/master/kws_streaming/models/ds_tc_resnet_qat_nbit_test.py)
Not that this feature is experimental.

There are two examples with streaming model calibration: [cnn_nbit_test.py](https://github.com/google-research/google-research/blob/cd76f8373df19f3268d150c467cf4d9241b7017b/kws_streaming/models/cnn_nbit_test.py) and [cnn_test.py](https://github.com/google-research/google-research/blob/cd76f8373df19f3268d150c467cf4d9241b7017b/kws_streaming/models/cnn_test.py). Both are designed for calibrating model with external streaming states. But in this example inputs for streaming states will stay in float.

For internal streaming state the test would be the same, but calibration data will have only input data (no need to feed input streaming states). Also will need to above flags (_experimental_variable_quantization, experimental_new_quantizer, experimental_enable_resource_variables).
These features are experimental.",hello converter true true true remove least streaming streaming internal state need train example test feature experimental two streaming model calibration designed model external streaming example streaming stay float internal streaming state test would calibration data input data need feed input streaming also need experimental,issue,positive,positive,positive,positive,positive,positive
1574067594,"@AlyssaYoung 
Have you figured out the solution by any chance?
It would be very helpful if you could guide me past the error.
",figured solution chance would helpful could guide past error,issue,positive,negative,negative,negative,negative,negative
1568408598,"> Hey! Even I faced this issue. Using pip install flax==0.6.9 fixed the issue for me.

I try your suggestions and i got this,do you konw how to solve it?
![微信截图_20230530210856](https://github.com/google-research/google-research/assets/94942498/9acbce52-7417-4445-9ba4-994c0d7c462e)
 

",hey even faced issue pip install fixed issue try got solve,issue,negative,positive,neutral,neutral,positive,positive
1567003367,"It seems like the error you're encountering might be caused by using the ""reorder"" option when querying but not when creating the index. This option requires the original dataset to reorder using exact vectors when doing queries. If you don't have the original dataset, the error might occur when loading it. To avoid this, make sure to have both ""dataset.npy"" and ""hashed_dataset.npy"" if you're using the hashed and reorder method. If you save the searcher without the .reorder option and use HA it will not save the ""dataset.npy""!

If your goal is to have faster queries, keeping both datasets can provide good precision and speed. However, if you want to reduce the dataset size (to not keep the heavy ""dataset.npy""), HA without reorder might give poor results. You can try using "".tree"" with good num_leaves and num_leaves_to_search to fix this issue :)",like error might reorder option querying index option original reorder exact original error might occur loading avoid make sure reorder method save searcher without option use ha save goal faster keeping provide good precision speed however want reduce size keep heavy ha without reorder might give poor try good fix issue,issue,positive,positive,positive,positive,positive,positive
1565614708,Still an issue.  Any plan to fix?  I can't create a proper conda package of `rouge` without a patch.,still issue plan fix ca create proper package rouge without patch,issue,negative,neutral,neutral,neutral,neutral,neutral
1562876251,"PS: The output dimension of CLIP's image encoder resnet50 is `7*7*2048`, but the standard input dimension of F-VLM detector head's R50 is `224 * 224`, The two dimensions don't match, so do I misunderstand something wrong?",output dimension clip image standard input dimension detector head two match misunderstand something wrong,issue,negative,negative,negative,negative,negative,negative
1555100835,"@ffffatgoose  I'm having trouble with the input formatting for layout-blt model just like in issue #1423 .
I noticed in above mentioned issue that you were able to solve the problem. I was wondering if you could share your solution with me, including your code base. It would be really appreciable.

",trouble input model like issue issue able solve problem wondering could share solution code base would really appreciable,issue,negative,negative,neutral,neutral,negative,negative
1555025406,"Unfortunately, jax2tf does not fully support many ops we use in the model (e.g. gather) with enable_xla=false. This would require further investigation of work-arounds. We will update here when we have the bandwidth to resolve this. ",unfortunately fully support many use model gather would require investigation update resolve,issue,negative,neutral,neutral,neutral,neutral,neutral
1554641031,Hi～Could you please tell me where I can get the model checkpoint？The code says it has not been released yet.,please tell get model code yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1553805936,Thanks all for flagging this issue. We will work on it and let you know if there's any update. ,thanks flagging issue work let know update,issue,negative,positive,positive,positive,positive,positive
1552526032,"I got this，too. I run this code on the Tesla V100-PCIE-32GB， but it caused OP_REQUIRES failed at xla_ops.cc:362, which seems just on the special device.",got run code special device,issue,negative,positive,positive,positive,positive,positive
1547434715,"First, thank you all for getting these created!
Wanted to ping @vanzytay and @patrickvonplaten in case those evaluation results had been published somewhere. I am looking at the T5-efficient models and wanted to know how they compared.",first thank getting ping case evaluation somewhere looking know,issue,negative,positive,positive,positive,positive,positive
1539184237,"Thank you for your answer, but I think this output embeddings is not whole embedding vector. 
I want to get embeddings like (batch, length, dimension), which include time-series outputs. ",thank answer think output whole vector want get like batch length dimension include,issue,positive,positive,positive,positive,positive,positive
1538958788,"hi, maybe you can try this,

https://tfhub.dev/google/nonsemantic-speech-benchmark/trillsson1/1
```
# TF2 version
import tensorflow as tf
import tensorflow_hub as hub

m = hub.KerasLayer('https://tfhub.dev/google/trillsson1/1')
# NOTE: Audio should be floats in [-1, 1], sampled at 16kHz. Model input is of
# the shape [batch size, time].
audio_samples = tf.zeros([3, 64000])
embeddings = m(audio_samples)['embedding']```",hi maybe try version import import hub note audio model input shape batch size time,issue,negative,neutral,neutral,neutral,neutral,neutral
1532654390,"Just to clarify, I was able to reproduce results similar to those presented in the paper after downgrading the environments to those given above by @dibbla (namely `gym==0.21.0` and `tensorflow==2.4.1`). I'm not too familiar with tensorflow so the problems I faced previously may be possibly due to the difference in version. The evaluation grid for `wide` using `seed=1` is 
![image](https://user-images.githubusercontent.com/59470295/235869249-d5f4cb96-409c-4e83-89d2-887cfb8da3a0.png)

Thanks!
",clarify able reproduce similar paper given namely familiar faced previously may possibly due difference version evaluation grid wide image thanks,issue,negative,positive,neutral,neutral,positive,positive
1530911787,"Any updates on this? I am trying to use knowledge distillation in a regression problem. But its very difficult to find any paper or project on it.

I tired using `(MSE(student_preds, GT) + MSE(student_preds, teacher_preds))/2` but this doesn't seem to work. Currently, I am planing to do `a * MSE(student_preds, GT) + (1-a) * MSE(student_preds, teacher_preds)`. However, I am not sure whether it will work or not.",trying use knowledge distillation regression problem difficult find paper project tired seem work currently however sure whether work,issue,negative,negative,neutral,neutral,negative,negative
1527972885,"Hey!
Even I faced this issue. Using pip install flax==0.6.9 fixed the issue for me.",hey even faced issue pip install fixed issue,issue,negative,positive,neutral,neutral,positive,positive
1526649441,"@naomibb similar question was discussed in [link](https://github.com/google-research/google-research/issues/809#issuecomment-940908231). Threshold can depend on your criteria, e.g. optimize precision vs recall; or False accept rate vs miss rate as in [paper](https://assets.amazon.science/7e/94/b850e377478fada03a82751f5535/low-bit-quantization-and-quantization-aware-training-for-small-footprint-keyword-spotting.pdf)",similar question link threshold depend criterion optimize precision recall false accept rate miss rate paper,issue,positive,negative,negative,negative,negative,negative
1525225317,"Hi @rybakov, 
One more question: I am training the model just for single word (wakeup word ) detection (just 2 classes: wake-word and unknown classes). Which model do you suggest to use to have a reasonable accuracy and can be deployed on a microcontroller. 
Thank you in advace. ",hi one question training model single word word detection class unknown class model suggest use reasonable accuracy thank,issue,negative,positive,neutral,neutral,positive,positive
1525066136,"> @smhsn I would suggest to use [TFLite-micro](https://github.com/tensorflow/tflite-micro) and follow [example1](https://medium.com/@dmytro.korablyov/first-steps-with-esp32-and-tensorflow-lite-for-microcontrollers-c2d8e238accf) or [example2](https://www.tensorflow.org/lite/microcontrollers)

Thank you @rybakov for quick reply. Any idea how small the model should be ?
",would suggest use follow example example thank quick reply idea small model,issue,negative,positive,neutral,neutral,positive,positive
1523691597,@smhsn I would suggest to use [TFLite-micro](https://github.com/tensorflow/tflite-micro) and follow [example1](https://medium.com/@dmytro.korablyov/first-steps-with-esp32-and-tensorflow-lite-for-microcontrollers-c2d8e238accf) or [example2](https://www.tensorflow.org/lite/microcontrollers),would suggest use follow example example,issue,negative,neutral,neutral,neutral,neutral,neutral
1521337987,"Thanks for the quick reply!
The only changes I made were the ones discussed in this issue (namely the `reset()` function for `gym` to work) and I'm running the code as is. I did follow the hyperparameters mentioned in the paper appendix but right now, it doesn't seem to work well. I will try to play around with the hyperparameters and coefficients to see if I can get something close to the paper!",thanks quick reply made issue namely reset function gym work running code follow paper appendix right seem work well try play around see get something close paper,issue,positive,positive,positive,positive,positive,positive
1520650211,"I haven't also touched this codebase in some time but I'd suggest maybe try running in a colab and playing around with the loss coefficients -- the training is really fast and runs on a cpu. Make sure the hyperparameters match the ones mentioned in the paper appendix. 

",also touched time suggest maybe try running around loss training really fast make sure match paper appendix,issue,negative,positive,positive,positive,positive,positive
1520647038,"> Hi @dibbla and @agarwl, I'm not sure if i am doing it right but i'm not getting anything remotely close to what was presented in the paper. The only changes that I made was to include the `env.environment.reset()`. The evaluation grid for the `wide` configuration from using `seed=1` looks like this ![image](https://user-images.githubusercontent.com/59470295/233958324-06a3136d-15e5-4f30-8f99-63f3a9feb025.png) One issue that I noticed is that the `alignment_loss` does not seem to be dropping much, starting from approximately 8 and dropping quickly to 6 before fluctuating around this range until the end of training. For comparison, the `cross_entropy_loss` is in the range of `1e-6`. The evaluation grid for `random grid` looks equally off. (Also using the `seed=1`) with multiple training environments not solved in the final trained model. ![image](https://user-images.githubusercontent.com/59470295/233961670-3fc2d1e0-24f2-48d7-89db-b73f0f700d35.png)
> 
> It will be great if you guys could point out any error in my runs. Thanks!

It's been a while and I am no longer working on the pse🤣.

By the change you made, do you mean the one discusses in this issue? If you didn't encouter any bug, maybe you can try to reproduce the result with the very original code.",hi sure right getting anything remotely close paper made include evaluation grid wide configuration like image one issue seem dropping much starting approximately dropping quickly around range end training comparison range evaluation grid random grid equally also multiple training final trained model image great could point error thanks longer working change made mean one issue bug maybe try reproduce result original code,issue,positive,positive,neutral,neutral,positive,positive
1520111339,"> @smhsn, I think the issues is streaming pooling, please try ds_pool='1, 1, 1, 1, 1, 1'.

@rybakov Thanks alot for the reply. The issue is resolved after I set flags.use_tf_fft to 0 and ds_pool='1, 1, 1, 1, 1, 1'.",think streaming please try thanks reply issue resolved set,issue,positive,positive,positive,positive,positive,positive
1519788599,"Hi @dibbla and @agarwl, I'm not sure if i am doing it right but i'm not getting anything remotely close to what was presented in the paper. The only changes that I made was to include the `env.environment.reset()`. The evaluation grid for the `wide` configuration from using `seed=1` looks like this
![image](https://user-images.githubusercontent.com/59470295/233958324-06a3136d-15e5-4f30-8f99-63f3a9feb025.png)
One issue that I noticed is that the `alignment_loss` does not seem to be dropping much, starting from approximately 8 and dropping quickly to 6 before fluctuating around this range until the end of training. For comparison, the `cross_entropy_loss` is in the range of `1e-6`.
The evaluation grid for `random grid` looks equally off. (Also using the `seed=1`) with multiple training environments not solved in the final trained model.
![image](https://user-images.githubusercontent.com/59470295/233961670-3fc2d1e0-24f2-48d7-89db-b73f0f700d35.png)

It will be great if you guys could point out any error in my runs. Thanks!
",hi sure right getting anything remotely close paper made include evaluation grid wide configuration like image one issue seem dropping much starting approximately dropping quickly around range end training comparison range evaluation grid random grid equally also multiple training final trained model image great could point error thanks,issue,positive,positive,neutral,neutral,positive,positive
1519412106,I'm sorry but I also haven't get solution of this problem. ,sorry also get solution problem,issue,negative,negative,negative,negative,negative,negative
1518373855,"Hello @nekosan314 Was this question answered? Did you figure out how to get whole embedding vector of TRILLsson? I am also looking for temporal audio feature generation.
",hello question figure get whole vector also looking temporal audio feature generation,issue,negative,positive,positive,positive,positive,positive
1518103090,"@smhsn, I think the issues is streaming pooling, please try ds_pool='1, 1, 1, 1, 1, 1'. 

It is possible to stream models with pooling > 1 wiht ds_pool='1, 2, 1, 1, 1, 1' but you will need to modify the size of the input tensor in streaming mode. For example with ds_pool='1, 2, 1, 1, 1, 1' you need to set [data_stride](https://github.com/google-research/google-research/blob/master/kws_streaming/train/base_parser.py#L517)=2. Input tensor in streaming mode is set in [line](https://github.com/google-research/google-research/blob/master/kws_streaming/models/utils.py#L285) by
```
input_data_shape = modes.get_input_data_shape(flags, mode)
```

@tuannvhust kws_streaming lib is always at sync with HEAD of tf_nightly , tensorflow_addons and tensorflow_model_optimization (if you install the latest versions of these libs it should work). The error 
```
ImportError: cannot import name 'control_flow_util' from 'keras.utils' 
```
means that your TF version is old, e.g. the same issue in [link](https://github.com/tensorflow/tensor2tensor/issues/1167)",think streaming please try possible stream need modify size input tensor streaming mode example need set input tensor streaming mode set line mode always sync head install latest work error import name version old issue link,issue,negative,positive,positive,positive,positive,positive
1516029738,"@smhsn dear sir, I got trouble when re-training the model. Which is the conflict of the dependencies I'm installing. Can you specify the version of tf_nightly , tensorflow_addons and tensorflow_model_optimization you are using to train. That means a lot to me, cc @rybakov . Here are my bug : 
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/workspace/models1/kws_streaming/train/model_train_eval.py"", line 112, in <module>
    import kws_streaming.models.att_mh_rnn as att_mh_rnn
  File ""/workspace/models1/kws_streaming/models/att_mh_rnn.py"", line 18, in <module>
    from kws_streaming.layers import speech_features
  File ""/workspace/models1/kws_streaming/layers/speech_features.py"", line 24, in <module>
    from kws_streaming.layers import random_shift
  File ""/workspace/models1/kws_streaming/layers/random_shift.py"", line 18, in <module>
    from keras.utils import control_flow_util
ImportError: cannot import name 'control_flow_util' from 'keras.utils' (/usr/local/lib/python3.8/site-packages/keras/utils/__init__.py)
root@6f1a60dcf9df:/workspace/models1# export TF_ENABLE_ONEDNN_OPTS=0
root@6f1a60dcf9df:/workspace/models1# python3 -m kws_streaming.train.model_train_eval --data_url '' --data_dir ./data1/ --train_dir ./models1/dnn_1/ --mel_upper_edge_hertz 7000 --how_many_training_steps 100,100,100 --learning_rate 0.0005,0.0001,0.00002 --window_size_ms 40.0 --window_stride_ms 20.0 --mel_num_bins 40 --dct_num_features 20 --resample 0.15 --alsologtostderr --train 1 dnn --units1 '64,128' --act1 ""'relu','relu'"" --pool_size 2 --strides 2 --dropout1 0.1 --units2 '128,256' --act2 ""'linear','relu'""
2023-04-20 08:13:40.464407: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-04-20 08:13:40.505306: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-04-20 08:13:40.505873: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-20 08:13:41.408289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/workspace/models1/kws_streaming/train/model_train_eval.py"", line 112, in <module>
    import kws_streaming.models.att_mh_rnn as att_mh_rnn
  File ""/workspace/models1/kws_streaming/models/att_mh_rnn.py"", line 18, in <module>
    from kws_streaming.layers import speech_features
  File ""/workspace/models1/kws_streaming/layers/speech_features.py"", line 24, in <module>
    from kws_streaming.layers import random_shift
  File ""/workspace/models1/kws_streaming/layers/random_shift.py"", line 18, in <module>
    from keras.utils import control_flow_util
ImportError: cannot import name 'control_flow_util' from 'keras.utils' (/usr/local/lib/python3.8/site-packages/keras/utils/__init__.py)
```
",dear sir got trouble model conflict specify version train lot bug recent call last file line return code none file line code file line module import file line module import file line module import file line module import import name root export root python resample train act dropout act could find machine used could find machine used binary use available enable following rebuild appropriate compiler warning could find recent call last file line return code none file line code file line module import file line module import file line module import file line module import import name,issue,negative,positive,neutral,neutral,positive,positive
1515887222,"@rybakov . I followed your restructions but i still got the bug. It's so hard for people if author doesnt specify the version of dependencies. Can you specify the versions for me. I found out that `tf_nightly==2.3.0.dev20200515 `did not exist. But when i install the latest version of `tf_nightly`, I got the bug:
`AttributeError: module 'keras.api._v2.keras.experimental' has no attribute 'PeepholeLSTMCell'`



I tried `--use_peepholes 0` and `pip install tensorflow==2.8.0` but they dont work out. ",still got bug hard people author doesnt specify version specify found dev exist install latest version got bug module attribute tried pip install dont work,issue,negative,positive,positive,positive,positive,positive
1509028120,"hi @rybakov, I used kws_streaming/train/model_train_eval.py to retrain and save the internal stream model as TfLite. The model flags are as follows: 

""Namespace(data_url='', data_dir='my_data/', lr_schedule='linear', optimizer='adam', background_volume=0.1, l2_weight_decay=0.0, background_frequency=0.8, split_data=0, silence_percentage=10.0, unknown_percentage=10.0, time_shift_ms=100.0, sp_time_shift_ms=0.0, testing_percentage=10, validation_percentage=10, how_many_training_steps='2500,2500,2500,2500', eval_step_interval=400, learning_rate='0.001,0.0005,0.0001,2E-05', batch_size=100, wanted_words='label1,label2', train_dir='models/ds_tc_resnet/', save_step_interval=100, start_checkpoint='', verbosity=0, optimizer_epsilon=1e-08, resample=0.15, sp_resample=0.0, volume_resample=0.0, train=1, sample_rate=16000, clip_duration_ms=1500, window_size_ms=30.0, window_stride_ms=10.0, preprocess='raw', feature_type='mfcc_tf', preemph=0.0, window_type='hann', mel_lower_edge_hertz=20.0, mel_upper_edge_hertz=7600.0, micro_enable_pcan=True, micro_features_scale=0.0390625, micro_min_signal_remaining=0.05, micro_out_scale=1, log_epsilon=1e-12, dct_num_features=40, use_tf_fft=True, mel_non_zero_only=False, fft_magnitude_squared=False, mel_num_bins=80, use_spec_augment=1, time_masks_number=2, time_mask_max_size=25, frequency_masks_number=2, frequency_mask_max_size=7, use_spec_cutout=0, spec_cutout_masks_number=3, spec_cutout_time_mask_size=10, spec_cutout_frequency_mask_size=5, return_softmax=0, novograd_beta_1=0.95, novograd_beta_2=0.5, novograd_weight_decay=0.001, novograd_grad_averaging=0, pick_deterministically=1, causal_data_frame_padding=1, wav=1, quantize=0, use_quantize_nbit=0, nbit_activation_bits=8, nbit_weight_bits=8, data_stride=1, restore_checkpoint=0, model_name='ds_tc_resnet', activation='relu', dropout=0.0, ds_filters='128, 64, 64, 64, 128, 128', ds_repeat='1, 1, 1, 1, 1, 1', ds_filter_separable='1, 1, 1, 1, 1, 1', ds_residual='0, 1, 1, 1, 0, 0', ds_padding=""'causal', 'causal', 'causal', 'causal', 'causal', 'causal'"", ds_kernel_size='11, 5, 15, 17, 15, 1', ds_stride='1, 1, 1, 1, 1, 1', ds_dilation='1, 1, 1, 1, 2, 1', ds_pool='1, 2, 1, 1, 1, 1', ds_max_pool=0, ds_scale=1, nbit_8bit_until_block=1, nbit_8bit_last=1, label_count=2, desired_samples=24000, window_size_samples=480, window_stride_samples=160, spectrogram_length=148, data_frame_padding='causal', summaries_dir='models/ds_tc_resnet/logs/', training=True)""

I added the following lines to kws_streaming/train/model_train_eval.py:
        ""folder_name = opt_name + 'tflite_stream_state_internal'
        file_name = 'stream_state_internal.tflite'
        mode = modes.Modes.STREAM_INTERNAL_STATE_INFERENCE
        test.convert_model_tflite(flags, folder_name, mode, file_name,
                                  optimizations=optimizations)""

and replaced ""sess"" with ""None"" (eager mode) in kws_streaming/train/test.py/convert_model_tflite() function :
""kws_streaming.models.utils.model_to_tflite(None, model, flags, mode, path_model, optimizations))"" 

model is trained, however for saving the stream internal model as tflite it gives the following error:

FAILED to convert to mode STREAM_INTERNAL_STATE_INFERENCE, tflite: Negative dimension size caused by subtracting 2 from 1 for '{{node streaming/average_pooling2d/AvgPool}} = AvgPool[T=DT_FLOAT, data_format=""NHWC"", ksize=[1, 2, 1, 1], padding=""VALID"", strides=[1, 2, 1, 1]](streaming/dropout_1/Identity)' with input shapes: [1,1,1,64].

",hi used retrain save internal stream model model label added following mode mode sess none eager mode function none model mode model trained however saving stream internal model following error convert mode negative dimension size node valid input,issue,negative,negative,neutral,neutral,negative,negative
1508297890,"@rybakov Hi, thanks for the quick reply. Here are the details:

FLAGS.split_data = 0
FLAGS.mel_upper_edge_hertz = 7600
FLAGS.window_size_ms = 30.0
FLAGS.window_stride_ms = 10.0
FLAGS.mel_num_bins = 80
FLAGS.dct_num_features = 40
FLAGS.feature_type = 'mfcc_tf'
FLAGS.preprocess = 'raw'
FLAGS.causal_data_frame_padding = 1
FLAGS.use_tf_fft = True
FLAGS.mel_non_zero_only = not FLAGS.use_tf_fft
FLAGS.train = 1
FLAGS.how_many_training_steps = '10000,10000,10000,10000'
FLAGS.learning_rate = '0.001,0.0005,0.0001,0.00002'
FLAGS.lr_schedule = 'linear'
FLAGS.verbosity = logging.INFO

FLAGS.resample = 0.15
FLAGS.time_shift_ms = 100
FLAGS.use_spec_augment = 1
FLAGS.time_masks_number = 2
FLAGS.time_mask_max_size = 25
FLAGS.frequency_masks_number = 2
FLAGS.frequency_mask_max_size = 7
FLAGS.pick_deterministically = 1

FLAGS.model_name = 'ds_tc_resnet'
FLAGS.activation = 'relu'
FLAGS.dropout = 0.0
FLAGS.ds_filters = '128, 64, 64, 64, 128, 128'
FLAGS.ds_filter_separable = '1, 1, 1, 1, 1, 1'
FLAGS.ds_repeat = '1, 1, 1, 1, 1, 1'
FLAGS.ds_residual = '0, 1, 1, 1, 0, 0' # residual can not be applied with stride
FLAGS.ds_kernel_size = '11, 5, 15, 17, 15, 1'
FLAGS.ds_dilation = '1, 1, 1, 1, 2, 1'
FLAGS.ds_stride = '1, 1, 1, 1, 1, 1'
FLAGS.ds_pool = '1, 2, 1, 1, 1, 1'
FLAGS.ds_padding = ""'causal', 'causal', 'causal', 'causal', 'causal', 'causal'""

FLAGS.clip_duration_ms = 1500  
FLAGS.batch_size = 100

flags = model_flags.update_flags(FLAGS)

with open(os.path.join(flags.train_dir, 'flags.json'), 'wt') as f:
  json.dump(flags.__dict__, f)

train.train(flags)

model_non_stream_batch = models.MODELS[flags.model_name](flags)
weights_name = 'best_weights'
model_non_stream_batch.load_weights(os.path.join(train_dir, weights_name))

flags.data_shape = (total_stride * flags.window_stride_samples,)
window_stride = flags.data_shape[0]
mode = 'stream_internal'
inference_batch_size = 1
model_stream_internal = utils.to_streaming_inference(model_non_stream_batch, flags, Modes.STREAM_INTERNAL_STATE_INFERENCE)

tflite_streaming_model = utils.model_to_tflite(None, model_non_stream_batch, flags, Modes.STREAM_INTERNAL_STATE_INFERENCE)",hi thanks quick reply true residual applied stride open mode none,issue,positive,positive,positive,positive,positive,positive
1507393049,"@smhsn I have not seen this kind of error. If you change only data and use kws_streaming/train/model_train_eval.py then it supposed to save your model and convert it to TFLite. Please share more details about how your issue can be reproduced. 

If you use streaming with internal state then [saved_model](https://www.tensorflow.org/guide/saved_model) is a recommended approach for saving a model and converting it to TFLite. ",seen kind error change data use supposed save model convert please share issue use streaming internal state approach saving model converting,issue,positive,positive,positive,positive,positive,positive
1507078250,"> @bdytx5 please pull the lates version of kws_streaming: I added conversion of model (with internal state streaming stored in saved_model) to tflite.
> 
> There is no need to retrain your model. Folder with training artifacts should have a subfolder named ""stream_state_internal"". It contains model streaming with internal state stored in saved_model format. You can convert it to tflite with below command:
> 
> ```
> python -m kws_streaming.train.convert \
> --saved_model_path=/tmp/ds_tc_resnet/stream_state_internal \
> --tflite_model_path=/tmp/ds_tc_resnet/stream_state_internal/model_stream.tflite
> ```
> 
> I use above command because kws_streaming uses session mode (for backward compatibility), but conversion of saved_model to tflite has to be done in eager mode.

Hi, How can I save my internal streaming model to SaveModel format? When I try to save my model using <tf.saved_model.save(model_stream, save_model_path)> I get the following error: 

ValueError: Unable to save function b'__inference_speech_features_layer_call_and_return_conditional_losses_9410' because it captures graph tensor Tensor(""streaming_1/speech_features/windowing_2/hann_window/sub_2:0"", shape=(480,), dtype=float32) from a parent function which cannot be converted to a constant with `tf.get_static_value`.",please pull version added conversion model internal state streaming need retrain model folder training model streaming internal state format convert command python use command session mode backward compatibility conversion done eager mode hi save internal streaming model format try save model get following error unable save function graph tensor tensor parent function converted constant,issue,positive,negative,neutral,neutral,negative,negative
1506882645,"> hi @fukien , no I couldn't. I am not a C++ expert though. If you want to use a library in your project, you can look at https://github.com/facebookresearch/faiss/wiki/Fast-accumulation-of-PQ-and-AQ-codes-(FastScan). This is within FAISS library and inspired by SCANN.

Thanks. However, still do not find much C++ examples...... So sad  : ( ",hi could expert though want use library project look within library inspired thanks however still find much sad,issue,positive,negative,negative,negative,negative,negative
1502438677,"Hi is there a checkpoint file here?
I am not able to see, hence how to run the code?",hi file able see hence run code,issue,negative,positive,positive,positive,positive,positive
1502383017,"+1 to this issue. @Jmallins  - It seems like the models currently in use at https://github.com/google-research/bert (For example, https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip) have a very different checkpoint structure to the expected red-ace models (at least with the requested version of the code). 

The following rewrite rules seem to be necessary, and some of the variables are different shapes, for example, the self-attention which has shape (768, 12, 64) in the checkpoint, and (768,768) in RED-ACE (This is just reshaped for heads, but it seems like the underlying implementation is different). Is there a different version of the BERT code/tensorflow/tf-models-official that we should be using? The current requirements.txt specifies 2.9.1 (and 2.9.2 reprectively), but it seems like there's something off:

```
bert/embeddings/LayerNorm/beta                     ==> embeddings/layer_norm/beta                        
bert/embeddings/LayerNorm/gamma                    ==> embeddings/layer_norm/gamma                       
bert/embeddings/position_embeddings                ==> position_embedding/embeddings                     
bert/embeddings/token_type_embeddings              ==> type_embeddings/embeddings                        
bert/embeddings/word_embeddings                    ==> word_embeddings/embeddings                        
bert/encoder/layer_0/attention/output/LayerNorm/beta ==> transformer/layer_0/self_attention_layer_norm/beta
bert/encoder/layer_0/attention/output/LayerNorm/gamma ==> transformer/layer_0/self_attention_layer_norm/gamma
bert/encoder/layer_0/attention/output/dense/bias   ==> transformer/layer_0/self_attention/attention_output/bias
bert/encoder/layer_0/attention/output/dense/kernel ==> transformer/layer_0/self_attention/attention_output/kernel
bert/encoder/layer_0/attention/self/key/bias       ==> transformer/layer_0/self_attention/key/bias       
bert/encoder/layer_0/attention/self/key/kernel     ==> transformer/layer_0/self_attention/key/kernel     
bert/encoder/layer_0/attention/self/query/bias     ==> transformer/layer_0/self_attention/query/bias     
bert/encoder/layer_0/attention/self/query/kernel   ==> transformer/layer_0/self_attention/query/kernel   
bert/encoder/layer_0/attention/self/value/bias     ==> transformer/layer_0/self_attention/value/bias     
bert/encoder/layer_0/attention/self/value/kernel   ==> transformer/layer_0/self_attention/value/kernel   
bert/encoder/layer_0/intermediate/dense/bias       ==> transformer/layer_0/intermediate/bias             
bert/encoder/layer_0/intermediate/dense/kernel     ==> transformer/layer_0/intermediate/kernel           
bert/encoder/layer_0/output/LayerNorm/beta         ==> transformer/layer_0/output_layer_norm/beta        
bert/encoder/layer_0/output/LayerNorm/gamma        ==> transformer/layer_0/output_layer_norm/gamma       
bert/encoder/layer_0/output/dense/bias             ==> transformer/layer_0/output/bias                   
bert/encoder/layer_0/output/dense/kernel           ==> transformer/layer_0/output/kernel                 
bert/encoder/layer_1/attention/output/LayerNorm/beta ==> transformer/layer_1/self_attention_layer_norm/beta
bert/encoder/layer_1/attention/output/LayerNorm/gamma ==> transformer/layer_1/self_attention_layer_norm/gamma
bert/encoder/layer_1/attention/output/dense/bias   ==> transformer/layer_1/self_attention/attention_output/bias
bert/encoder/layer_1/attention/output/dense/kernel ==> transformer/layer_1/self_attention/attention_output/kernel
bert/encoder/layer_1/attention/self/key/bias       ==> transformer/layer_1/self_attention/key/bias       
bert/encoder/layer_1/attention/self/key/kernel     ==> transformer/layer_1/self_attention/key/kernel     
bert/encoder/layer_1/attention/self/query/bias     ==> transformer/layer_1/self_attention/query/bias     
bert/encoder/layer_1/attention/self/query/kernel   ==> transformer/layer_1/self_attention/query/kernel   
bert/encoder/layer_1/attention/self/value/bias     ==> transformer/layer_1/self_attention/value/bias     
bert/encoder/layer_1/attention/self/value/kernel   ==> transformer/layer_1/self_attention/value/kernel   
bert/encoder/layer_1/intermediate/dense/bias       ==> transformer/layer_1/intermediate/bias             
bert/encoder/layer_1/intermediate/dense/kernel     ==> transformer/layer_1/intermediate/kernel           
bert/encoder/layer_1/output/LayerNorm/beta         ==> transformer/layer_1/output_layer_norm/beta        
bert/encoder/layer_1/output/LayerNorm/gamma        ==> transformer/layer_1/output_layer_norm/gamma       
bert/encoder/layer_1/output/dense/bias             ==> transformer/layer_1/output/bias                   
bert/encoder/layer_1/output/dense/kernel           ==> transformer/layer_1/output/kernel                 
bert/encoder/layer_10/attention/output/LayerNorm/beta ==> transformer/layer_10/self_attention_layer_norm/beta
bert/encoder/layer_10/attention/output/LayerNorm/gamma ==> transformer/layer_10/self_attention_layer_norm/gamma
bert/encoder/layer_10/attention/output/dense/bias  ==> transformer/layer_10/self_attention/attention_output/bias
bert/encoder/layer_10/attention/output/dense/kernel ==> transformer/layer_10/self_attention/attention_output/kernel
bert/encoder/layer_10/attention/self/key/bias      ==> transformer/layer_10/self_attention/key/bias      
bert/encoder/layer_10/attention/self/key/kernel    ==> transformer/layer_10/self_attention/key/kernel    
bert/encoder/layer_10/attention/self/query/bias    ==> transformer/layer_10/self_attention/query/bias    
bert/encoder/layer_10/attention/self/query/kernel  ==> transformer/layer_10/self_attention/query/kernel  
bert/encoder/layer_10/attention/self/value/bias    ==> transformer/layer_10/self_attention/value/bias    
bert/encoder/layer_10/attention/self/value/kernel  ==> transformer/layer_10/self_attention/value/kernel  
bert/encoder/layer_10/intermediate/dense/bias      ==> transformer/layer_10/intermediate/bias            
bert/encoder/layer_10/intermediate/dense/kernel    ==> transformer/layer_10/intermediate/kernel          
bert/encoder/layer_10/output/LayerNorm/beta        ==> transformer/layer_10/output_layer_norm/beta       
bert/encoder/layer_10/output/LayerNorm/gamma       ==> transformer/layer_10/output_layer_norm/gamma      
bert/encoder/layer_10/output/dense/bias            ==> transformer/layer_10/output/bias                  
bert/encoder/layer_10/output/dense/kernel          ==> transformer/layer_10/output/kernel                
bert/encoder/layer_11/attention/output/LayerNorm/beta ==> transformer/layer_11/self_attention_layer_norm/beta
bert/encoder/layer_11/attention/output/LayerNorm/gamma ==> transformer/layer_11/self_attention_layer_norm/gamma
bert/encoder/layer_11/attention/output/dense/bias  ==> transformer/layer_11/self_attention/attention_output/bias
bert/encoder/layer_11/attention/output/dense/kernel ==> transformer/layer_11/self_attention/attention_output/kernel
bert/encoder/layer_11/attention/self/key/bias      ==> transformer/layer_11/self_attention/key/bias      
bert/encoder/layer_11/attention/self/key/kernel    ==> transformer/layer_11/self_attention/key/kernel    
bert/encoder/layer_11/attention/self/query/bias    ==> transformer/layer_11/self_attention/query/bias    
bert/encoder/layer_11/attention/self/query/kernel  ==> transformer/layer_11/self_attention/query/kernel  
bert/encoder/layer_11/attention/self/value/bias    ==> transformer/layer_11/self_attention/value/bias    
bert/encoder/layer_11/attention/self/value/kernel  ==> transformer/layer_11/self_attention/value/kernel  
bert/encoder/layer_11/intermediate/dense/bias      ==> transformer/layer_11/intermediate/bias            
bert/encoder/layer_11/intermediate/dense/kernel    ==> transformer/layer_11/intermediate/kernel          
bert/encoder/layer_11/output/LayerNorm/beta        ==> transformer/layer_11/output_layer_norm/beta       
bert/encoder/layer_11/output/LayerNorm/gamma       ==> transformer/layer_11/output_layer_norm/gamma      
bert/encoder/layer_11/output/dense/bias            ==> transformer/layer_11/output/bias                  
bert/encoder/layer_11/output/dense/kernel          ==> transformer/layer_11/output/kernel                
bert/encoder/layer_2/attention/output/LayerNorm/beta ==> transformer/layer_2/self_attention_layer_norm/beta
bert/encoder/layer_2/attention/output/LayerNorm/gamma ==> transformer/layer_2/self_attention_layer_norm/gamma
bert/encoder/layer_2/attention/output/dense/bias   ==> transformer/layer_2/self_attention/attention_output/bias
bert/encoder/layer_2/attention/output/dense/kernel ==> transformer/layer_2/self_attention/attention_output/kernel
bert/encoder/layer_2/attention/self/key/bias       ==> transformer/layer_2/self_attention/key/bias       
bert/encoder/layer_2/attention/self/key/kernel     ==> transformer/layer_2/self_attention/key/kernel     
bert/encoder/layer_2/attention/self/query/bias     ==> transformer/layer_2/self_attention/query/bias     
bert/encoder/layer_2/attention/self/query/kernel   ==> transformer/layer_2/self_attention/query/kernel   
bert/encoder/layer_2/attention/self/value/bias     ==> transformer/layer_2/self_attention/value/bias     
bert/encoder/layer_2/attention/self/value/kernel   ==> transformer/layer_2/self_attention/value/kernel   
bert/encoder/layer_2/intermediate/dense/bias       ==> transformer/layer_2/intermediate/bias             
bert/encoder/layer_2/intermediate/dense/kernel     ==> transformer/layer_2/intermediate/kernel           
bert/encoder/layer_2/output/LayerNorm/beta         ==> transformer/layer_2/output_layer_norm/beta        
bert/encoder/layer_2/output/LayerNorm/gamma        ==> transformer/layer_2/output_layer_norm/gamma       
bert/encoder/layer_2/output/dense/bias             ==> transformer/layer_2/output/bias                   
bert/encoder/layer_2/output/dense/kernel           ==> transformer/layer_2/output/kernel                 
bert/encoder/layer_3/attention/output/LayerNorm/beta ==> transformer/layer_3/self_attention_layer_norm/beta
bert/encoder/layer_3/attention/output/LayerNorm/gamma ==> transformer/layer_3/self_attention_layer_norm/gamma
bert/encoder/layer_3/attention/output/dense/bias   ==> transformer/layer_3/self_attention/attention_output/bias
bert/encoder/layer_3/attention/output/dense/kernel ==> transformer/layer_3/self_attention/attention_output/kernel
bert/encoder/layer_3/attention/self/key/bias       ==> transformer/layer_3/self_attention/key/bias       
bert/encoder/layer_3/attention/self/key/kernel     ==> transformer/layer_3/self_attention/key/kernel     
bert/encoder/layer_3/attention/self/query/bias     ==> transformer/layer_3/self_attention/query/bias     
bert/encoder/layer_3/attention/self/query/kernel   ==> transformer/layer_3/self_attention/query/kernel   
bert/encoder/layer_3/attention/self/value/bias     ==> transformer/layer_3/self_attention/value/bias     
bert/encoder/layer_3/attention/self/value/kernel   ==> transformer/layer_3/self_attention/value/kernel   
bert/encoder/layer_3/intermediate/dense/bias       ==> transformer/layer_3/intermediate/bias             
bert/encoder/layer_3/intermediate/dense/kernel     ==> transformer/layer_3/intermediate/kernel           
bert/encoder/layer_3/output/LayerNorm/beta         ==> transformer/layer_3/output_layer_norm/beta        
bert/encoder/layer_3/output/LayerNorm/gamma        ==> transformer/layer_3/output_layer_norm/gamma       
bert/encoder/layer_3/output/dense/bias             ==> transformer/layer_3/output/bias                   
bert/encoder/layer_3/output/dense/kernel           ==> transformer/layer_3/output/kernel                 
bert/encoder/layer_4/attention/output/LayerNorm/beta ==> transformer/layer_4/self_attention_layer_norm/beta
bert/encoder/layer_4/attention/output/LayerNorm/gamma ==> transformer/layer_4/self_attention_layer_norm/gamma
bert/encoder/layer_4/attention/output/dense/bias   ==> transformer/layer_4/self_attention/attention_output/bias
bert/encoder/layer_4/attention/output/dense/kernel ==> transformer/layer_4/self_attention/attention_output/kernel
bert/encoder/layer_4/attention/self/key/bias       ==> transformer/layer_4/self_attention/key/bias       
bert/encoder/layer_4/attention/self/key/kernel     ==> transformer/layer_4/self_attention/key/kernel     
bert/encoder/layer_4/attention/self/query/bias     ==> transformer/layer_4/self_attention/query/bias     
bert/encoder/layer_4/attention/self/query/kernel   ==> transformer/layer_4/self_attention/query/kernel   
bert/encoder/layer_4/attention/self/value/bias     ==> transformer/layer_4/self_attention/value/bias     
bert/encoder/layer_4/attention/self/value/kernel   ==> transformer/layer_4/self_attention/value/kernel   
bert/encoder/layer_4/intermediate/dense/bias       ==> transformer/layer_4/intermediate/bias             
bert/encoder/layer_4/intermediate/dense/kernel     ==> transformer/layer_4/intermediate/kernel           
bert/encoder/layer_4/output/LayerNorm/beta         ==> transformer/layer_4/output_layer_norm/beta        
bert/encoder/layer_4/output/LayerNorm/gamma        ==> transformer/layer_4/output_layer_norm/gamma       
bert/encoder/layer_4/output/dense/bias             ==> transformer/layer_4/output/bias                   
bert/encoder/layer_4/output/dense/kernel           ==> transformer/layer_4/output/kernel                 
bert/encoder/layer_5/attention/output/LayerNorm/beta ==> transformer/layer_5/self_attention_layer_norm/beta
bert/encoder/layer_5/attention/output/LayerNorm/gamma ==> transformer/layer_5/self_attention_layer_norm/gamma
bert/encoder/layer_5/attention/output/dense/bias   ==> transformer/layer_5/self_attention/attention_output/bias
bert/encoder/layer_5/attention/output/dense/kernel ==> transformer/layer_5/self_attention/attention_output/kernel
bert/encoder/layer_5/attention/self/key/bias       ==> transformer/layer_5/self_attention/key/bias       
bert/encoder/layer_5/attention/self/key/kernel     ==> transformer/layer_5/self_attention/key/kernel     
bert/encoder/layer_5/attention/self/query/bias     ==> transformer/layer_5/self_attention/query/bias     
bert/encoder/layer_5/attention/self/query/kernel   ==> transformer/layer_5/self_attention/query/kernel   
bert/encoder/layer_5/attention/self/value/bias     ==> transformer/layer_5/self_attention/value/bias     
bert/encoder/layer_5/attention/self/value/kernel   ==> transformer/layer_5/self_attention/value/kernel   
bert/encoder/layer_5/intermediate/dense/bias       ==> transformer/layer_5/intermediate/bias             
bert/encoder/layer_5/intermediate/dense/kernel     ==> transformer/layer_5/intermediate/kernel           
bert/encoder/layer_5/output/LayerNorm/beta         ==> transformer/layer_5/output_layer_norm/beta        
bert/encoder/layer_5/output/LayerNorm/gamma        ==> transformer/layer_5/output_layer_norm/gamma       
bert/encoder/layer_5/output/dense/bias             ==> transformer/layer_5/output/bias                   
bert/encoder/layer_5/output/dense/kernel           ==> transformer/layer_5/output/kernel                 
bert/encoder/layer_6/attention/output/LayerNorm/beta ==> transformer/layer_6/self_attention_layer_norm/beta
bert/encoder/layer_6/attention/output/LayerNorm/gamma ==> transformer/layer_6/self_attention_layer_norm/gamma
bert/encoder/layer_6/attention/output/dense/bias   ==> transformer/layer_6/self_attention/attention_output/bias
bert/encoder/layer_6/attention/output/dense/kernel ==> transformer/layer_6/self_attention/attention_output/kernel
bert/encoder/layer_6/attention/self/key/bias       ==> transformer/layer_6/self_attention/key/bias       
bert/encoder/layer_6/attention/self/key/kernel     ==> transformer/layer_6/self_attention/key/kernel     
bert/encoder/layer_6/attention/self/query/bias     ==> transformer/layer_6/self_attention/query/bias     
bert/encoder/layer_6/attention/self/query/kernel   ==> transformer/layer_6/self_attention/query/kernel   
bert/encoder/layer_6/attention/self/value/bias     ==> transformer/layer_6/self_attention/value/bias     
bert/encoder/layer_6/attention/self/value/kernel   ==> transformer/layer_6/self_attention/value/kernel   
bert/encoder/layer_6/intermediate/dense/bias       ==> transformer/layer_6/intermediate/bias             
bert/encoder/layer_6/intermediate/dense/kernel     ==> transformer/layer_6/intermediate/kernel           
bert/encoder/layer_6/output/LayerNorm/beta         ==> transformer/layer_6/output_layer_norm/beta        
bert/encoder/layer_6/output/LayerNorm/gamma        ==> transformer/layer_6/output_layer_norm/gamma       
bert/encoder/layer_6/output/dense/bias             ==> transformer/layer_6/output/bias                   
bert/encoder/layer_6/output/dense/kernel           ==> transformer/layer_6/output/kernel                 
bert/encoder/layer_7/attention/output/LayerNorm/beta ==> transformer/layer_7/self_attention_layer_norm/beta
bert/encoder/layer_7/attention/output/LayerNorm/gamma ==> transformer/layer_7/self_attention_layer_norm/gamma
bert/encoder/layer_7/attention/output/dense/bias   ==> transformer/layer_7/self_attention/attention_output/bias
bert/encoder/layer_7/attention/output/dense/kernel ==> transformer/layer_7/self_attention/attention_output/kernel
bert/encoder/layer_7/attention/self/key/bias       ==> transformer/layer_7/self_attention/key/bias       
bert/encoder/layer_7/attention/self/key/kernel     ==> transformer/layer_7/self_attention/key/kernel     
bert/encoder/layer_7/attention/self/query/bias     ==> transformer/layer_7/self_attention/query/bias     
bert/encoder/layer_7/attention/self/query/kernel   ==> transformer/layer_7/self_attention/query/kernel   
bert/encoder/layer_7/attention/self/value/bias     ==> transformer/layer_7/self_attention/value/bias     
bert/encoder/layer_7/attention/self/value/kernel   ==> transformer/layer_7/self_attention/value/kernel   
bert/encoder/layer_7/intermediate/dense/bias       ==> transformer/layer_7/intermediate/bias             
bert/encoder/layer_7/intermediate/dense/kernel     ==> transformer/layer_7/intermediate/kernel           
bert/encoder/layer_7/output/LayerNorm/beta         ==> transformer/layer_7/output_layer_norm/beta        
bert/encoder/layer_7/output/LayerNorm/gamma        ==> transformer/layer_7/output_layer_norm/gamma       
bert/encoder/layer_7/output/dense/bias             ==> transformer/layer_7/output/bias                   
bert/encoder/layer_7/output/dense/kernel           ==> transformer/layer_7/output/kernel                 
bert/encoder/layer_8/attention/output/LayerNorm/beta ==> transformer/layer_8/self_attention_layer_norm/beta
bert/encoder/layer_8/attention/output/LayerNorm/gamma ==> transformer/layer_8/self_attention_layer_norm/gamma
bert/encoder/layer_8/attention/output/dense/bias   ==> transformer/layer_8/self_attention/attention_output/bias
bert/encoder/layer_8/attention/output/dense/kernel ==> transformer/layer_8/self_attention/attention_output/kernel
bert/encoder/layer_8/attention/self/key/bias       ==> transformer/layer_8/self_attention/key/bias       
bert/encoder/layer_8/attention/self/key/kernel     ==> transformer/layer_8/self_attention/key/kernel     
bert/encoder/layer_8/attention/self/query/bias     ==> transformer/layer_8/self_attention/query/bias     
bert/encoder/layer_8/attention/self/query/kernel   ==> transformer/layer_8/self_attention/query/kernel   
bert/encoder/layer_8/attention/self/value/bias     ==> transformer/layer_8/self_attention/value/bias     
bert/encoder/layer_8/attention/self/value/kernel   ==> transformer/layer_8/self_attention/value/kernel   
bert/encoder/layer_8/intermediate/dense/bias       ==> transformer/layer_8/intermediate/bias             
bert/encoder/layer_8/intermediate/dense/kernel     ==> transformer/layer_8/intermediate/kernel           
bert/encoder/layer_8/output/LayerNorm/beta         ==> transformer/layer_8/output_layer_norm/beta        
bert/encoder/layer_8/output/LayerNorm/gamma        ==> transformer/layer_8/output_layer_norm/gamma       
bert/encoder/layer_8/output/dense/bias             ==> transformer/layer_8/output/bias                   
bert/encoder/layer_8/output/dense/kernel           ==> transformer/layer_8/output/kernel                 
bert/encoder/layer_9/attention/output/LayerNorm/beta ==> transformer/layer_9/self_attention_layer_norm/beta
bert/encoder/layer_9/attention/output/LayerNorm/gamma ==> transformer/layer_9/self_attention_layer_norm/gamma
bert/encoder/layer_9/attention/output/dense/bias   ==> transformer/layer_9/self_attention/attention_output/bias
bert/encoder/layer_9/attention/output/dense/kernel ==> transformer/layer_9/self_attention/attention_output/kernel
bert/encoder/layer_9/attention/self/key/bias       ==> transformer/layer_9/self_attention/key/bias       
bert/encoder/layer_9/attention/self/key/kernel     ==> transformer/layer_9/self_attention/key/kernel     
bert/encoder/layer_9/attention/self/query/bias     ==> transformer/layer_9/self_attention/query/bias     
bert/encoder/layer_9/attention/self/query/kernel   ==> transformer/layer_9/self_attention/query/kernel   
bert/encoder/layer_9/attention/self/value/bias     ==> transformer/layer_9/self_attention/value/bias     
bert/encoder/layer_9/attention/self/value/kernel   ==> transformer/layer_9/self_attention/value/kernel   
bert/encoder/layer_9/intermediate/dense/bias       ==> transformer/layer_9/intermediate/bias             
bert/encoder/layer_9/intermediate/dense/kernel     ==> transformer/layer_9/intermediate/kernel           
bert/encoder/layer_9/output/LayerNorm/beta         ==> transformer/layer_9/output_layer_norm/beta        
bert/encoder/layer_9/output/LayerNorm/gamma        ==> transformer/layer_9/output_layer_norm/gamma       
bert/encoder/layer_9/output/dense/bias             ==> transformer/layer_9/output/bias                   
bert/encoder/layer_9/output/dense/kernel           ==> transformer/layer_9/output/kernel                 
bert/pooler/dense/bias                             ==> pooler_transform/bias                             
bert/pooler/dense/kernel                           ==> pooler_transform/kernel                           
cls/predictions/output_bias                        ==> cls/predictions/output_bias                       
cls/predictions/transform/LayerNorm/beta           ==> cls/predictions/transform/LayerNorm/beta          
cls/predictions/transform/LayerNorm/gamma          ==> cls/predictions/transform/LayerNorm/gamma         
cls/predictions/transform/dense/bias               ==> cls/predictions/transform/dense/bias              
cls/predictions/transform/dense/kernel             ==> cls/predictions/transform/dense/kernel            
cls/seq_relationship/output_bias                   ==> cls/seq_relationship/output_bias                  
cls/seq_relationship/output_weights                ==> cls/seq_relationship/output_weights
```

",issue like currently use example different structure least version code following rewrite seem necessary different example shape like underlying implementation different different version current like something,issue,positive,negative,neutral,neutral,negative,negative
1500661135,"+1 for this issue, it looks like the [notebook](https://github.com/google-research/google-research/blob/master/coref_mt5/colabs/coreference_decoder.ipynb) is missing a critical function to enable coreference prediction.",issue like notebook missing critical function enable prediction,issue,negative,negative,neutral,neutral,negative,negative
1497733459,"hi @fukien , no I couldn't. I am not a C++ expert though. If you want to use a library in your project, you can look at https://github.com/facebookresearch/faiss/wiki/Fast-accumulation-of-PQ-and-AQ-codes-(FastScan). This is within FAISS library and inspired by SCANN.",hi could expert though want use library project look within library inspired,issue,positive,neutral,neutral,neutral,neutral,neutral
1497629884,"@vamossagar12 @Shambricool @chasingegg @chenzaiping-dev 
Just wondering, if any of you have managed to use this in a separate C++ project? If so, can you share some examples?
Thanks.",wondering use separate project share thanks,issue,positive,positive,positive,positive,positive,positive
1493462810,"I am facing a similar issue at this block at the same line, which the error message is:

'XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.'",facing similar issue block line error message library look,issue,negative,neutral,neutral,neutral,neutral,neutral
1467801465,"Hi, were you able to get the training scripts?
",hi able get training,issue,negative,positive,positive,positive,positive,positive
1466985911,"> Hi,
> 
> We just release the script/data for generating Amazon2M. Please download it [here](http://web.cs.ucla.edu/~chohsieh/data/Amazon2M.tar.gz), in which you can see how this data set is processed from the raw metadata.

Hello @infwinston,

Do you, or anyone above, have a copy of Amazon-3M dataset on which this script actually works. I'm interested in the raw text files.

Best regards,
Hani",hi release generating please see data set raw hello anyone copy script actually work interested raw text best,issue,positive,positive,positive,positive,positive,positive
1464849179,"Same issue in 2023 with python 3.7, 3.8, 3.9. Please help me I'm using Mac m1.
ERROR: Could not find a version that satisfies the requirement tensorflow-cpu==2.8.1 (from versions: none)
ERROR: No matching distribution found for tensorflow-cpu==2.8.1
",issue python please help mac error could find version requirement none error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
1462543629,"
Saraiki is spoken by more than 26 M peoples and 57th language according to https://www.ethnologue.com/guides/ethnologue200
There is a community ready for working in this project, as Saraiki is taught in many universities.

1. Saraiki has ISO 639-3 valid code skr. Saraiki is compulsory subject from grade 1.

2 Saraiki is language in the census of Pakistan. Saraiki is written language.

3. Saraiki is a school, college, and University subject.

4. Saraiki is taught from primary to Ph.D level.

5. Saraiki has TV channels.

6. Govt: of Pakistan has made website in Saraiki also. see Multiple language site: http://saraiki.app.com.pk/saraiki/

7. Saraiki is recognized language by Govt: of Pakistan.

8. Awards are awarded by Govt: of Pakistan in Saraiki languages books in addition to Punjabi language.

9. Saraiki news and programs are presented on Public and Private Radio in Saraiki also.

10. Saraiki language Dictionaries are also available since 1881 A.D. An online is this https://www.ijunoon.com/saraiki/dictionary.aspx?word=when

11. Saraiki has its own Grammar, Idioms, Proverbs, alphabets, and Tenses.

12. Saraiki translations of Bible and Quran are also available.

13. Saraiki is also language of WordPress. see https://skr.wordpress.org/

14. More work has been done in Saraiki language at: http://translate.wordpress.com/projects/wpcom/

15. Saraiki language work has been done in also : https://translatewiki.net/

16. More than 60000 pages in Saraiki language has been made in Wiktionary and Wikipedia.

17. Saraiki is also added in Google translate.",spoken th language according community ready working project taught many iso valid code compulsory subject grade language census written language school college university subject taught primary level made also see multiple language site language addition language news public private radio also language also available since grammar also available also language see work done language language work done also language made also added translate,issue,negative,positive,positive,positive,positive,positive
1461158949,"My config is as follows


@dataclasses.dataclass
class Finetune(Dataset):
  """"""Fine-tune dataset configuration.""""""

  name: str ='kinetics400'
  batch_size: int = 64
  num_frames: int = 32
  frame_size: int = 320
  video_stride: int = 1
  audio_stride: int = 2
  mixup: bool = False  # True when finetuning audio (audioset)
  mixup_alpha: float = 5
  color_augment: bool = True
  audio_noise: float = 0.0
  label_smoothing: float = 0.1
  num_windows_test: int = 2
  multi_crop: bool = True
  max_num_words: int = 32
  max_context_sentences: int = 1


class ViTBase(ModelConfig):
  """"""Configuration of the Base Vision Transformer model.""""""

  name: str = 'vit_base'
  temporal_patch_size: int = 4
  spatial_patch_size: int = 16
  max_temporal_buckets: int = 8  # 32 // 4
  max_vertical_buckets: int = 20  # 224 // 16
  max_horizontal_buckets: int = 20  # 224 // 16
  random_patch_sampling: bool = True  # apply DropToken
  patch_sampling_rate: float = 0.9  # DropToken rate
  pre_projection: bool = True  # True if finetuning a modality-agnostic model
  d_model: int = 768
  d_kv: int = 64
  d_ff: int = 3072
  num_layers: int = 12
  num_heads: int = 12
  pre_norm: bool = True
  use_bias: bool = True
  activation: str = 'gelu'
  dropout_rate: float = 0.1
  layer_norm_epsilon: float = 1e-6
  use_masking: bool = False
  mask_rate: float = 0.2
  post_projection: bool =True  # True if finetuning a modality-agnostic model
  d_post_proj: int = 1024
  post_proj_activation: str = activation
  final_endpoint: str = 'predictions_3d'
  num_classes: Optional[int] = None
",class configuration name bool false true audio float bool true float float bool true class configuration base vision transformer model name bool true apply float rate bool true true model bool true bool true activation float float bool false float bool true model activation optional none,issue,positive,positive,positive,positive,positive,positive
1459534399,"I0308 03:08:43.208900 139913490419840 base.py:350] Train Step: 900/100000  / training metric = {'model_loss': 5.4955220222473145, 'regularization_loss': 0.0, 'total_loss': 5.4955220222473145, 'categorical_cross_entropy': 5.472332000732422, 'top_1_accuracy': 0.05281250178813934, 'top_5_accuracy': 0.1693750023841858, 'learning_rate': 0.0018000001}
I0308 03:09:38.097480 139913490419840 base.py:350] Train Step: 950/100000  / training metric = {'model_loss': 5.608379364013672, 'regularization_loss': 0.0, 'total_loss': 5.608379364013672, 'categorical_cross_entropy': 5.488901138305664, 'top_1_accuracy': 0.04531250149011612, 'top_5_accuracy': 0.16906249523162842, 'learning_rate': 0.0018999999}
I0308 03:10:32.985699 139913490419840 base.py:350] Train Step: 1000/100000  / training metric = {'model_loss': 5.597506046295166, 'regularization_loss': 0.0, 'total_loss': 5.597506046295166, 'categorical_cross_entropy': 5.569050312042236, 'top_1_accuracy': 0.03812500089406967, 'top_5_accuracy': 0.1393750011920929, 'learning_rate': 0.002}
I0308 03:11:27.894989 139913490419840 base.py:350] Train Step: 1050/100000  / training metric = {'model_loss': 4.783196449279785, 'regularization_loss': 0.0, 'total_loss': 4.783196449279785, 'categorical_cross_entropy': 5.074387073516846, 'top_1_accuracy': 0.04500000178813934, 'top_5_accuracy': 0.1809374988079071, 'learning_rate': 0.0021}
I0308 03:12:22.821660 139913490419840 base.py:350] Train Step: 1100/100000  / training metric = {'model_loss': 4.984374523162842, 'regularization_loss': 0.0, 'total_loss': 4.984374523162842, 'categorical_cross_entropy': 4.929326057434082, 'top_1_accuracy': 0.046875, 'top_5_accuracy': 0.17374999821186066, 'learning_rate': 0.0022}
I0308 03:13:17.740966 139913490419840 base.py:350] Train Step: 1150/100000  / training metric = {'model_loss': 5.1442766189575195, 'regularization_loss': 0.0, 'total_loss': 5.1442766189575195, 'categorical_cross_entropy': 4.9118123054504395, 'top_1_accuracy': 0.04593750089406967, 'top_5_accuracy': 0.1537500023841858, 'learning_rate': 0.0023}
I0308 03:14:12.622952 139913490419840 base.py:350] Train Step: 1200/100000  / training metric = {'model_loss': 4.648950099945068, 'regularization_loss': 0.0, 'total_loss': 4.648950099945068, 'categorical_cross_entropy': 4.839740753173828, 'top_1_accuracy': 0.05375000089406967, 'top_5_accuracy': 0.1681250035762787, 'learning_rate': 0.0023999999}
I0308 03:15:07.510002 139913490419840 base.py:350] Train Step: 1250/100000  / training metric = {'model_loss': 4.486297607421875, 'regularization_loss': 0.0, 'total_loss': 4.486297607421875, 'categorical_cross_entropy': 4.74024772644043, 'top_1_accuracy': 0.06406249850988388, 'top_5_accuracy': 0.19499999284744263, 'learning_rate': 0.0025}
I0308 03:16:02.427894 139913490419840 base.py:350] Train Step: 1300/100000  / training metric = {'model_loss': 4.514057159423828, 'regularization_loss': 0.0, 'total_loss': 4.514057159423828, 'categorical_cross_entropy': 4.633248805999756, 'top_1_accuracy': 0.08531250059604645, 'top_5_accuracy': 0.2199999988079071, 'learning_rate': 0.0025999998}
I0308 03:16:57.332457 139913490419840 base.py:350] Train Step: 1350/100000  / training metric = {'model_loss': 4.502958297729492, 'regularization_loss': 0.0, 'total_loss': 4.502958297729492, 'categorical_cross_entropy': 4.436383247375488, 'top_1_accuracy': 0.1184374988079071, 'top_5_accuracy': 0.27156248688697815, 'learning_rate': 0.0027}
I0308 03:17:52.228013 139913490419840 base.py:350] Train Step: 1400/100000  / training metric = {'model_loss': 4.75914192199707, 'regularization_loss': 0.0, 'total_loss': 4.75914192199707, 'categorical_cross_entropy': 4.458169937133789, 'top_1_accuracy': 0.10125000029802322, 'top_5_accuracy': 0.25687500834465027, 'learning_rate': 0.0028}
I0308 03:18:47.108559 139913490419840 base.py:350] Train Step: 1450/100000  / training metric = {'model_loss': 4.908833026885986, 'regularization_loss': 0.0, 'total_loss': 4.908833026885986, 'categorical_cross_entropy': 4.711931228637695, 'top_1_accuracy': 0.09437499940395355, 'top_5_accuracy': 0.2381249964237213, 'learning_rate': 0.0029}
I0308 03:19:42.000681 139913490419840 base.py:350] Train Step: 1500/100000  / training metric = {'model_loss': 4.633822441101074, 'regularization_loss': 0.0, 'total_loss': 4.633822441101074, 'categorical_cross_entropy': 4.7647809982299805, 'top_1_accuracy': 0.08843749761581421, 'top_5_accuracy': 0.2503125071525574, 'learning_rate': 0.003}
I0308 03:20:36.911430 139913490419840 base.py:350] Train Step: 1550/100000  / training metric = {'model_loss': 4.396494388580322, 'regularization_loss': 0.0, 'total_loss': 4.396494388580322, 'categorical_cross_entropy': 4.6485915184021, 'top_1_accuracy': 0.10499999672174454, 'top_5_accuracy': 0.28562501072883606, 'learning_rate': 0.0030999999}
I0308 03:21:31.799952 139913490419840 base.py:350] Train Step: 1600/100000  / training metric = {'model_loss': 4.517045021057129, 'regularization_loss': 0.0, 'total_loss': 4.517045021057129, 'categorical_cross_entropy': 4.5990471839904785, 'top_1_accuracy': 0.11343750357627869, 'top_5_accuracy': 0.28062498569488525, 'learning_rate': 0.0032}
I0308 03:22:26.694483 139913490419840 base.py:350] Train Step: 1650/100000  / training metric = {'model_loss': 3.842207193374634, 'regularization_loss': 0.0, 'total_loss': 3.842207193374634, 'categorical_cross_entropy': 4.290555953979492, 'top_1_accuracy': 0.14249999821186066, 'top_5_accuracy': 0.3581250011920929, 'learning_rate': 0.0033}
I0308 03:23:21.586740 139913490419840 base.py:350] Train Step: 1700/100000  / training metric = {'model_loss': 4.626027584075928, 'regularization_loss': 0.0, 'total_loss': 4.626027584075928, 'categorical_cross_entropy': 4.483630657196045, 'top_1_accuracy': 0.12687499821186066, 'top_5_accuracy': 0.3199999928474426, 'learning_rate': 0.0034}
I0308 03:24:16.491297 139913490419840 base.py:350] Train Step: 1750/100000  / training metric = {'model_loss': 4.26643705368042, 'regularization_loss': 0.0, 'total_loss': 4.26643705368042, 'categorical_cross_entropy': 4.233344078063965, 'top_1_accuracy': 0.16843749582767487, 'top_5_accuracy': 0.36031249165534973, 'learning_rate': 0.0034999999}
I0308 03:25:11.385244 139913490419840 base.py:350] Train Step: 1800/100000  / training metric = {'model_loss': 3.9381637573242188, 'regularization_loss': 0.0, 'total_loss': 3.9381637573242188, 'categorical_cross_entropy': 4.265650272369385, 'top_1_accuracy': 0.15781250596046448, 'top_5_accuracy': 0.36937499046325684, 'learning_rate': 0.0036000002}
I0308 03:26:06.292655 139913490419840 base.py:350] Train Step: 1850/100000  / training metric = {'model_loss': 4.2460737228393555, 'regularization_loss': 0.0, 'total_loss': 4.2460737228393555, 'categorical_cross_entropy': 4.252326011657715, 'top_1_accuracy': 0.20218749344348907, 'top_5_accuracy': 0.4037500023841858, 'learning_rate': 0.0037}
I0308 03:27:01.180989 139913490419840 base.py:350] Train Step: 1900/100000  / training metric = {'model_loss': 4.091322898864746, 'regularization_loss': 0.0, 'total_loss': 4.091322898864746, 'categorical_cross_entropy': 4.382428169250488, 'top_1_accuracy': 0.18250000476837158, 'top_5_accuracy': 0.4050000011920929, 'learning_rate': 0.0037999998}
I0308 03:27:56.066036 139913490419840 base.py:350] Train Step: 1950/100000  / training metric = {'model_loss': 4.248667240142822, 'regularization_loss': 0.0, 'total_loss': 4.248667240142822, 'categorical_cross_entropy': 4.429052829742432, 'top_1_accuracy': 0.1821874976158142, 'top_5_accuracy': 0.37437498569488525, 'learning_rate': 0.0038999997}
I0308 03:28:51.014935 139913490419840 base.py:350] Train Step: 2000/100000  / training metric = {'model_loss': 4.067220211029053, 'regularization_loss': 0.0, 'total_loss': 4.067220211029053, 'categorical_cross_entropy': 4.1249494552612305, 'top_1_accuracy': 0.2068749964237213, 'top_5_accuracy': 0.4115625023841858, 'learning_rate': 0.004}
I0308 03:29:45.926014 139913490419840 base.py:350] Train Step: 2050/100000  / training metric = {'model_loss': 3.7538018226623535, 'regularization_loss': 0.0, 'total_loss': 3.7538018226623535, 'categorical_cross_entropy': 3.895267963409424, 'top_1_accuracy': 0.17874999344348907, 'top_5_accuracy': 0.3606249988079071, 'learning_rate': 0.0041}
I0308 03:30:40.843265 139913490419840 base.py:350] Train Step: 2100/100000  / training metric = {'model_loss': 3.8598146438598633, 'regularization_loss': 0.0, 'total_loss': 3.8598146438598633, 'categorical_cross_entropy': 3.7325329780578613, 'top_1_accuracy': 0.19374999403953552, 'top_5_accuracy': 0.37937501072883606, 'learning_rate': 0.0042}
I0308 03:31:35.754410 139913490419840 base.py:350] Train Step: 2150/100000  / training metric = {'model_loss': 3.5715322494506836, 'regularization_loss': 0.0, 'total_loss': 3.5715322494506836, 'categorical_cross_entropy': 3.7204041481018066, 'top_1_accuracy': 0.1837500035762787, 'top_5_accuracy': 0.38499999046325684, 'learning_rate': 0.0043}
I0308 03:32:30.634554 139913490419840 base.py:350] Train Step: 2200/100000  / training metric = {'model_loss': 3.2765071392059326, 'regularization_loss': 0.0, 'total_loss': 3.2765071392059326, 'categorical_cross_entropy': 3.4621105194091797, 'top_1_accuracy': 0.2381249964237213, 'top_5_accuracy': 0.43937501311302185, 'learning_rate': 0.0044}
I0308 03:33:25.539886 139913490419840 base.py:350] Train Step: 2250/100000  / training metric = {'model_loss': 3.13212251663208, 'regularization_loss': 0.0, 'total_loss': 3.13212251663208, 'categorical_cross_entropy': 3.4471142292022705, 'top_1_accuracy': 0.23624999821186066, 'top_5_accuracy': 0.46281251311302185, 'learning_rate': 0.0045}
I0308 03:34:20.519623 139913490419840 base.py:350] Train Step: 2300/100000  / training metric = {'model_loss': 2.6200642585754395, 'regularization_loss': 0.0, 'total_loss': 2.6200642585754395, 'categorical_cross_entropy': 3.430809736251831, 'top_1_accuracy': 0.23781250417232513, 'top_5_accuracy': 0.4637500047683716, 'learning_rate': 0.0046}
I0308 03:35:15.431127 139913490419840 base.py:350] Train Step: 2350/100000  / training metric = {'model_loss': 3.0866472721099854, 'regularization_loss': 0.0, 'total_loss': 3.0866472721099854, 'categorical_cross_entropy': 3.266249895095825, 'top_1_accuracy': 0.28437501192092896, 'top_5_accuracy': 0.5293750166893005, 'learning_rate': 0.0047}
I0308 03:36:10.326174 139913490419840 base.py:350] Train Step: 2400/100000  / training metric = {'model_loss': 3.6148757934570312, 'regularization_loss': 0.0, 'total_loss': 3.6148757934570312, 'categorical_cross_entropy': 3.4584662914276123, 'top_1_accuracy': 0.24562500417232513, 'top_5_accuracy': 0.5049999952316284, 'learning_rate': 0.0047999998}
I0308 03:37:05.235336 139913490419840 base.py:350] Train Step: 2450/100000  / training metric = {'model_loss': 3.340536117553711, 'regularization_loss': 0.0, 'total_loss': 3.340536117553711, 'categorical_cross_entropy': 3.573549270629883, 'top_1_accuracy': 0.32875001430511475, 'top_5_accuracy': 0.573437511920929, 'learning_rate': 0.0049}
I0308 03:38:00.139814 139913490419840 base.py:350] Train Step: 2500/100000  / training metric = {'model_loss': 3.2848339080810547, 'regularization_loss': 0.0, 'total_loss': 3.2848339080810547, 'categorical_cross_entropy': 3.420278549194336, 'top_1_accuracy': 0.35624998807907104, 'top_5_accuracy': 0.5975000262260437, 'learning_rate': 0.005}
I0308 03:38:55.061367 139913490419840 base.py:350] Train Step: 2550/100000  / training metric = {'model_loss': 3.3088197708129883, 'regularization_loss': 0.0, 'total_loss': 3.3088197708129883, 'categorical_cross_entropy': 3.372468948364258, 'top_1_accuracy': 0.3556250035762787, 'top_5_accuracy': 0.6106250286102295, 'learning_rate': 0.0049999966}
I0308 03:39:49.954227 139913490419840 base.py:350] Train Step: 2600/100000  / training metric = {'model_loss': 3.2217209339141846, 'regularization_loss': 0.0, 'total_loss': 3.2217209339141846, 'categorical_cross_entropy': 3.129551410675049, 'top_1_accuracy': 0.3893750011920929, 'top_5_accuracy': 0.6643750071525574, 'learning_rate': 0.004999987}
I0308 03:40:44.854340 139913490419840 base.py:350] Train Step: 2650/100000  / training metric = {'model_loss': 3.379288673400879, 'regularization_loss': 0.0, 'total_loss': 3.379288673400879, 'categorical_cross_entropy': 3.258040189743042, 'top_1_accuracy': 0.3709374964237213, 'top_5_accuracy': 0.6262500286102295, 'learning_rate': 0.0049999706}
I0308 03:41:39.757709 139913490419840 base.py:350] Train Step: 2700/100000  / training metric = {'model_loss': 3.2116501331329346, 'regularization_loss': 0.0, 'total_loss': 3.2116501331329346, 'categorical_cross_entropy': 3.3587276935577393, 'top_1_accuracy': 0.3265624940395355, 'top_5_accuracy': 0.5962499976158142, 'learning_rate': 0.004999948}
I0308 03:42:34.656723 139913490419840 base.py:350] Train Step: 2750/100000  / training metric = {'model_loss': 2.7890779972076416, 'regularization_loss': 0.0, 'total_loss': 2.7890779972076416, 'categorical_cross_entropy': 3.19785737991333, 'top_1_accuracy': 0.37312498688697815, 'top_5_accuracy': 0.6324999928474426, 'learning_rate': 0.004999919}
I0308 03:43:29.556088 139913490419840 base.py:350] Train Step: 2800/100000  / training metric = {'model_loss': 3.5488884449005127, 'regularization_loss': 0.0, 'total_loss': 3.5488884449005127, 'categorical_cross_entropy': 3.3697900772094727, 'top_1_accuracy': 0.3243750035762787, 'top_5_accuracy': 0.6075000166893005, 'learning_rate': 0.004999883}
I0308 03:44:24.445503 139913490419840 base.py:350] Train Step: 2850/100000  / training metric = {'model_loss': 3.2052078247070312, 'regularization_loss': 0.0, 'total_loss': 3.2052078247070312, 'categorical_cross_entropy': 3.26669979095459, 'top_1_accuracy': 0.3543750047683716, 'top_5_accuracy': 0.6578124761581421, 'learning_rate': 0.0049998406}
I0308 03:45:19.344279 139913490419840 base.py:350] Train Step: 2900/100000  / training metric = {'model_loss': 2.6761646270751953, 'regularization_loss': 0.0, 'total_loss': 2.6761646270751953, 'categorical_cross_entropy': 3.201599359512329, 'top_1_accuracy': 0.40937501192092896, 'top_5_accuracy': 0.6768749952316284, 'learning_rate': 0.0049997927}
I0308 03:46:14.252767 139913490419840 base.py:350] Train Step: 2950/100000  / training metric = {'model_loss': 2.9753217697143555, 'regularization_loss': 0.0, 'total_loss': 2.9753217697143555, 'categorical_cross_entropy': 3.228616237640381, 'top_1_accuracy': 0.3868750035762787, 'top_5_accuracy': 0.6634374856948853, 'learning_rate': 0.0049997373}
I0308 03:47:09.161418 139913490419840 base.py:350] Train Step: 3000/100000  / training metric = {'model_loss': 2.8316872119903564, 'regularization_loss': 0.0, 'total_loss': 2.8316872119903564, 'categorical_cross_entropy': 3.0746676921844482, 'top_1_accuracy': 0.38499999046325684, 'top_5_accuracy': 0.6100000143051147, 'learning_rate': 0.004999676}
I0308 03:48:04.046491 139913490419840 base.py:350] Train Step: 3050/100000  / training metric = {'model_loss': 3.0471696853637695, 'regularization_loss': 0.0, 'total_loss': 3.0471696853637695, 'categorical_cross_entropy': 2.950817584991455, 'top_1_accuracy': 0.3528124988079071, 'top_5_accuracy': 0.5615624785423279, 'learning_rate': 0.0049996073}
I0308 03:48:58.954879 139913490419840 base.py:350] Train Step: 3100/100000  / training metric = {'model_loss': 2.907987594604492, 'regularization_loss': 0.0, 'total_loss': 2.907987594604492, 'categorical_cross_entropy': 3.072218894958496, 'top_1_accuracy': 0.3284375071525574, 'top_5_accuracy': 0.5490624904632568, 'learning_rate': 0.0049995324}
I0308 03:49:53.859304 139913490419840 base.py:350] Train Step: 3150/100000  / training metric = {'model_loss': 2.8692567348480225, 'regularization_loss': 0.0, 'total_loss': 2.8692567348480225, 'categorical_cross_entropy': 3.0506222248077393, 'top_1_accuracy': 0.3253124952316284, 'top_5_accuracy': 0.5425000190734863, 'learning_rate': 0.0049994513}
I0308 03:50:48.761835 139913490419840 base.py:350] Train Step: 3200/100000  / training metric = {'model_loss': 3.1024861335754395, 'regularization_loss': 0.0, 'total_loss': 3.1024861335754395, 'categorical_cross_entropy': 3.2175652980804443, 'top_1_accuracy': 0.31468749046325684, 'top_5_accuracy': 0.5550000071525574, 'learning_rate': 0.004999364}
I0308 03:51:43.670536 139913490419840 base.py:350] Train Step: 3250/100000  / training metric = {'model_loss': 3.8260300159454346, 'regularization_loss': 0.0, 'total_loss': 3.8260300159454346, 'categorical_cross_entropy': 3.615595817565918, 'top_1_accuracy': 0.2878125011920929, 'top_5_accuracy': 0.5087500214576721, 'learning_rate': 0.0049992697}
I0308 03:52:38.575314 139913490419840 base.py:350] Train Step: 3300/100000  / training metric = {'model_loss': 4.204483985900879, 'regularization_loss': 0.0, 'total_loss': 4.204483985900879, 'categorical_cross_entropy': 4.228389739990234, 'top_1_accuracy': 0.3021875023841858, 'top_5_accuracy': 0.5062500238418579, 'learning_rate': 0.004999169}
I0308 03:53:33.468915 139913490419840 base.py:350] Train Step: 3350/100000  / training metric = {'model_loss': 5.958434104919434, 'regularization_loss': 0.0, 'total_loss': 5.958434104919434, 'categorical_cross_entropy': 5.183603286743164, 'top_1_accuracy': 0.29374998807907104, 'top_5_accuracy': 0.5309374928474426, 'learning_rate': 0.0049990625}
I0308 03:54:28.423225 139913490419840 base.py:350] Train Step: 3400/100000  / training metric = {'model_loss': 8.740215301513672, 'regularization_loss': 0.0, 'total_loss': 8.740215301513672, 'categorical_cross_entropy': 9.447718620300293, 'top_1_accuracy': 0.32249999046325684, 'top_5_accuracy': 0.5525000095367432, 'learning_rate': 0.004998949}
I0308 03:55:23.327500 139913490419840 base.py:350] Train Step: 3450/100000  / training metric = {'model_loss': 8.015877723693848, 'regularization_loss': 0.0, 'total_loss': 8.015877723693848, 'categorical_cross_entropy': 9.662934303283691, 'top_1_accuracy': 0.3803125023841858, 'top_5_accuracy': 0.6356250047683716, 'learning_rate': 0.0049988288}
I0308 03:56:18.225341 139913490419840 base.py:350] Train Step: 3500/100000  / training metric = {'model_loss': 17.339242935180664, 'regularization_loss': 0.0, 'total_loss': 17.339242935180664, 'categorical_cross_entropy': 20.583890914916992, 'top_1_accuracy': 0.35718750953674316, 'top_5_accuracy': 0.6268749833106995, 'learning_rate': 0.004998702}
I0308 03:57:13.133769 139913490419840 base.py:350] Train Step: 3550/100000  / training metric = {'model_loss': 38.57574462890625, 'regularization_loss': 0.0, 'total_loss': 38.57574462890625, 'categorical_cross_entropy': 30.916563034057617, 'top_1_accuracy': 0.3581250011920929, 'top_5_accuracy': 0.6581249833106995, 'learning_rate': 0.0049985694}
I0308 03:58:08.089207 139913490419840 base.py:350] Train Step: 3600/100000  / training metric = {'model_loss': 52.35525894165039, 'regularization_loss': 0.0, 'total_loss': 52.35525894165039, 'categorical_cross_entropy': 52.556846618652344, 'top_1_accuracy': 0.3881250023841858, 'top_5_accuracy': 0.6850000023841858, 'learning_rate': 0.0049984297}
I0308 03:59:02.981985 139913490419840 base.py:350] Train Step: 3650/100000  / training metric = {'model_loss': 53.197120666503906, 'regularization_loss': 0.0, 'total_loss': 53.197120666503906, 'categorical_cross_entropy': 105.25386810302734, 'top_1_accuracy': 0.32906249165534973, 'top_5_accuracy': 0.635937511920929, 'learning_rate': 0.004998284}
I0308 03:59:57.901176 139913490419840 base.py:350] Train Step: 3700/100000  / training metric = {'model_loss': 101.9737319946289, 'regularization_loss': 0.0, 'total_loss': 101.9737319946289, 'categorical_cross_entropy': 129.548828125, 'top_1_accuracy': 0.3606249988079071, 'top_5_accuracy': 0.6568750143051147, 'learning_rate': 0.004998131}
I0308 04:00:52.783193 139913490419840 base.py:350] Train Step: 3750/100000  / training metric = {'model_loss': 95.17208862304688, 'regularization_loss': 0.0, 'total_loss': 95.17208862304688, 'categorical_cross_entropy': 243.89248657226562, 'top_1_accuracy': 0.3121874928474426, 'top_5_accuracy': 0.6446874737739563, 'learning_rate': 0.0049979724}
I0308 04:01:47.690040 139913490419840 base.py:350] Train Step: 3800/100000  / training metric = {'model_loss': 194.10791015625, 'regularization_loss': 0.0, 'total_loss': 194.10791015625, 'categorical_cross_entropy': 260.0191345214844, 'top_1_accuracy': 0.33281248807907104, 'top_5_accuracy': 0.6540625095367432, 'learning_rate': 0.004997807}
I0308 04:02:42.587450 139913490419840 base.py:350] Train Step: 3850/100000  / training metric = {'model_loss': 156.3447723388672, 'regularization_loss': 0.0, 'total_loss': 156.3447723388672, 'categorical_cross_entropy': 249.74295043945312, 'top_1_accuracy': 0.35218751430511475, 'top_5_accuracy': 0.675000011920929, 'learning_rate': 0.0049976353}
I0308 04:03:37.471593 139913490419840 base.py:350] Train Step: 3900/100000  / training metric = {'model_loss': 156.0306396484375, 'regularization_loss': 0.0, 'total_loss': 156.0306396484375, 'categorical_cross_entropy': 230.09837341308594, 'top_1_accuracy': 0.38093748688697815, 'top_5_accuracy': 0.6553124785423279, 'learning_rate': 0.0049974564}
I0308 04:04:32.384941 139913490419840 base.py:350] Train Step: 3950/100000  / training metric = {'model_loss': 3.2000417709350586, 'regularization_loss': 0.0, 'total_loss': 3.2000417709350586, 'categorical_cross_entropy': 261.2913513183594, 'top_1_accuracy': 0.3840624988079071, 'top_5_accuracy': 0.6775000095367432, 'learning_rate': 0.0049972716}
I0308 04:05:27.272176 139913490419840 base.py:350] Train Step: 4000/100000  / training metric = {'model_loss': 511.6324157714844, 'regularization_loss': 0.0, 'total_loss': 511.6324157714844, 'categorical_cross_entropy': 548.4517211914062, 'top_1_accuracy': 0.3943749964237213, 'top_5_accuracy': 0.6899999976158142, 'learning_rate': 0.0049970807}
I0308 04:06:22.186349 139913490419840 base.py:350] Train Step: 4050/100000  / training metric = {'model_loss': 235.9940643310547, 'regularization_loss': 0.0, 'total_loss': 235.9940643310547, 'categorical_cross_entropy': 744.6589965820312, 'top_1_accuracy': 0.37062498927116394, 'top_5_accuracy': 0.6603124737739563, 'learning_rate': 0.0049968828}
I0308 04:07:17.107798 139913490419840 base.py:350] Train Step: 4100/100000  / training metric = {'model_loss': 1283.82275390625, 'regularization_loss': 0.0, 'total_loss': 1283.82275390625, 'categorical_cross_entropy': 1076.8538818359375, 'top_1_accuracy': 0.3540624976158142, 'top_5_accuracy': 0.659375011920929, 'learning_rate': 0.0049966783}
I0308 04:08:12.009613 139913490419840 base.py:350] Train Step: 4150/100000  / training metric = {'model_loss': 1154.7998046875, 'regularization_loss': 0.0, 'total_loss': 1154.7998046875, 'categorical_cross_entropy': 1571.990966796875, 'top_1_accuracy': 0.33500000834465027, 'top_5_accuracy': 0.6659374833106995, 'learning_rate': 0.004996468}
I0308 04:09:06.887839 139913490419840 base.py:350] Train Step: 4200/100000  / training metric = {'model_loss': 2144.884765625, 'regularization_loss': 0.0, 'total_loss': 2144.884765625, 'categorical_cross_entropy': 1329.6629638671875, 'top_1_accuracy': 0.33500000834465027, 'top_5_accuracy': 0.6521875262260437, 'learning_rate': 0.0049962504}
I0308 04:10:01.780827 139913490419840 base.py:350] Train Step: 4250/100000  / training metric = {'model_loss': 442.94390869140625, 'regularization_loss': 0.0, 'total_loss': 442.94390869140625, 'categorical_cross_entropy': 2048.0166015625, 'top_1_accuracy': 0.31937500834465027, 'top_5_accuracy': 0.6368749737739563, 'learning_rate': 0.004996027}
I0308 04:10:56.692492 139913490419840 base.py:350] Train Step: 4300/100000  / training metric = {'model_loss': 684.1417236328125, 'regularization_loss': 0.0, 'total_loss': 684.1417236328125, 'categorical_cross_entropy': 2079.395751953125, 'top_1_accuracy': 0.3565624952316284, 'top_5_accuracy': 0.6575000286102295, 'learning_rate': 0.0049957964}
I0308 04:11:51.604972 139913490419840 base.py:350] Train Step: 4350/100000  / training metric = {'model_loss': 2357.19970703125, 'regularization_loss': 0.0, 'total_loss': 2357.19970703125, 'categorical_cross_entropy': 3100.297607421875, 'top_1_accuracy': 0.3412500023841858, 'top_5_accuracy': 0.6462500095367432, 'learning_rate': 0.0049955593}
I0308 04:12:46.507195 139913490419840 base.py:350] Train Step: 4400/100000  / training metric = {'model_loss': 2030.1817626953125, 'regularization_loss': 0.0, 'total_loss': 2030.1817626953125, 'categorical_cross_entropy': 3899.798095703125, 'top_1_accuracy': 0.3721874952316284, 'top_5_accuracy': 0.6812499761581421, 'learning_rate': 0.0049953163}
I0308 04:13:41.400074 139913490419840 base.py:350] Train Step: 4450/100000  / training metric = {'model_loss': 1352.78515625, 'regularization_loss': 0.0, 'total_loss': 1352.78515625, 'categorical_cross_entropy': 6134.16015625, 'top_1_accuracy': 0.37031251192092896, 'top_5_accuracy': 0.6634374856948853, 'learning_rate': 0.0049950667}
I0308 04:14:36.310041 139913490419840 base.py:350] Train Step: 4500/100000  / training metric = {'model_loss': 5806.3115234375, 'regularization_loss': 0.0, 'total_loss': 5806.3115234375, 'categorical_cross_entropy': 6415.31005859375, 'top_1_accuracy': 0.3462499976158142, 'top_5_accuracy': 0.6703125238418579, 'learning_rate': 0.0049948106}
I0308 04:15:31.217277 139913490419840 base.py:350] Train Step: 4550/100000  / training metric = {'model_loss': 5466.34619140625, 'regularization_loss': 0.0, 'total_loss': 5466.34619140625, 'categorical_cross_entropy': 9514.080078125, 'top_1_accuracy': 0.3412500023841858, 'top_5_accuracy': 0.6646875143051147, 'learning_rate': 0.004994548}
I0308 04:16:26.123792 139913490419840 base.py:350] Train Step: 4600/100000  / training metric = {'model_loss': 9588.677734375, 'regularization_loss': 0.0, 'total_loss': 9588.677734375, 'categorical_cross_entropy': 12791.7978515625, 'top_1_accuracy': 0.34437501430511475, 'top_5_accuracy': 0.6499999761581421, 'learning_rate': 0.0049942792}
I0308 04:17:21.018759 139913490419840 base.py:350] Train Step: 4650/100000  / training metric = {'model_loss': 8102.20703125, 'regularization_loss': 0.0, 'total_loss': 8102.20703125, 'categorical_cross_entropy': 14987.2998046875, 'top_1_accuracy': 0.30000001192092896, 'top_5_accuracy': 0.6324999928474426, 'learning_rate': 0.0049940036}
I0308 04:18:15.901978 139913490419840 base.py:350] Train Step: 4700/100000  / training metric = {'model_loss': 12103.6962890625, 'regularization_loss': 0.0, 'total_loss': 12103.6962890625, 'categorical_cross_entropy': 19662.26171875, 'top_1_accuracy': 0.31468749046325684, 'top_5_accuracy': 0.6403124928474426, 'learning_rate': 0.004993721}
I0308 04:19:10.811988 139913490419840 base.py:350] Train Step: 4750/100000  / training metric = {'model_loss': 12397.5, 'regularization_loss': 0.0, 'total_loss': 12397.5, 'categorical_cross_entropy': 19813.69921875, 'top_1_accuracy': 0.2746874988079071, 'top_5_accuracy': 0.5990625023841858, 'learning_rate': 0.0049934327}
I0308 04:20:05.705072 139913490419840 base.py:350] Train Step: 4800/100000  / training metric = {'model_loss': 15363.01171875, 'regularization_loss': 0.0, 'total_loss': 15363.01171875, 'categorical_cross_entropy': 26973.54296875, 'top_1_accuracy': 0.29593750834465027, 'top_5_accuracy': 0.6387500166893005, 'learning_rate': 0.0049931374}
I0308 04:21:00.601633 139913490419840 base.py:350] Train Step: 4850/100000  / training metric = {'model_loss': 51458.17578125, 'regularization_loss': 0.0, 'total_loss': 51458.17578125, 'categorical_cross_entropy': 39540.90625, 'top_1_accuracy': 0.31187498569488525, 'top_5_accuracy': 0.645312488079071, 'learning_rate': 0.004992836}
I0308 04:21:55.491498 139913490419840 base.py:350] Train Step: 4900/100000  / training metric = {'model_loss': 60048.9609375, 'regularization_loss': 0.0, 'total_loss': 60048.9609375, 'categorical_cross_entropy': 43602.64453125, 'top_1_accuracy': 0.3071874976158142, 'top_5_accuracy': 0.6353124976158142, 'learning_rate': 0.0049925284}
I0308 04:22:50.399409 139913490419840 base.py:350] Train Step: 4950/100000  / training metric = {'model_loss': 121630.28125, 'regularization_loss': 0.0, 'total_loss': 121630.28125, 'categorical_cross_entropy': 75506.765625, 'top_1_accuracy': 0.3050000071525574, 'top_5_accuracy': 0.6393749713897705, 'learning_rate': 0.004992214}
I0308 04:23:45.285839 139913490419840 base.py:350] Train Step: 5000/100000  / training metric = {'model_loss': 165580.40625, 'regularization_loss': 0.0, 'total_loss': 165580.40625, 'categorical_cross_entropy': 101910.328125, 'top_1_accuracy': 0.2809374928474426, 'top_5_accuracy': 0.6200000047683716, 'learning_rate': 0.004991893}
I0308 04:23:58.799107 139913490419840 base.py:360] Checkpoint saved at step 5000 at path: gs://t-e/k-0/ckpt-5000
I0308 04:24:53.357223 139913490419840 base.py:350] Train Step: 5050/100000  / training metric = {'model_loss': 52282.98828125, 'regularization_loss': 0.0, 'total_loss': 52282.98828125, 'categorical_cross_entropy': 117427.15625, 'top_1_accuracy': 0.2512499988079071, 'top_5_accuracy': 0.5915625095367432, 'learning_rate': 0.004991566}
I0308 04:25:48.249959 139913490419840 base.py:350] Train Step: 5100/100000  / training metric = {'model_loss': 99773.609375, 'regularization_loss': 0.0, 'total_loss': 99773.609375, 'categorical_cross_entropy': 154106.546875, 'top_1_accuracy': 0.23499999940395355, 'top_5_accuracy': 0.5849999785423279, 'learning_rate': 0.004991232}
I0308 04:26:43.147445 139913490419840 base.py:350] Train Step: 5150/100000  / training metric = {'model_loss': 361076.65625, 'regularization_loss': 0.0, 'total_loss': 361076.65625, 'categorical_cross_entropy': 233711.5, 'top_1_accuracy': 0.21843749284744263, 'top_5_accuracy': 0.5603125095367432, 'learning_rate': 0.0049908916}
I0308 04:27:38.035207 139913490419840 base.py:350] Train Step: 5200/100000  / training metric = {'model_loss': 120112.875, 'regularization_loss': 0.0, 'total_loss': 120112.875, 'categorical_cross_entropy': 116416.3984375, 'top_1_accuracy': 0.20937499403953552, 'top_5_accuracy': 0.5224999785423279, 'learning_rate': 0.004990545}
I0308 04:28:32.920276 139913490419840 base.py:350] Train Step: 5250/100000  / training metric = {'model_loss': 330068.875, 'regularization_loss': 0.0, 'total_loss': 330068.875, 'categorical_cross_entropy': 190905.65625, 'top_1_accuracy': 0.20531250536441803, 'top_5_accuracy': 0.49125000834465027, 'learning_rate': 0.004990192}
I0308 04:29:27.824043 139913490419840 base.py:350] Train Step: 5300/100000  / training metric = {'model_loss': 131.4397430419922, 'regularization_loss': 0.0, 'total_loss': 131.4397430419922, 'categorical_cross_entropy': 224205.234375, 'top_1_accuracy': 0.1990624964237213, 'top_5_accuracy': 0.484375, 'learning_rate': 0.004989832}
I0308 04:30:22.733699 139913490419840 base.py:350] Train Step: 5350/100000  / training metric = {'model_loss': 327857.4375, 'regularization_loss': 0.0, 'total_loss': 327857.4375, 'categorical_cross_entropy': 234616.765625, 'top_1_accuracy': 0.2068749964237213, 'top_5_accuracy': 0.5078125, 'learning_rate': 0.004989466}
I0308 04:31:17.619194 139913490419840 base.py:350] Train Step: 5400/100000  / training metric = {'model_loss': 202573.125, 'regularization_loss': 0.0, 'total_loss': 202573.125, 'categorical_cross_entropy': 314582.84375, 'top_1_accuracy': 0.19968749582767487, 'top_5_accuracy': 0.5062500238418579, 'learning_rate': 0.0049890936}
I0308 04:32:12.509630 139913490419840 base.py:350] Train Step: 5450/100000  / training metric = {'model_loss': 260085.046875, 'regularization_loss': 0.0, 'total_loss': 260085.046875, 'categorical_cross_entropy': 375604.40625, 'top_1_accuracy': 0.17374999821186066, 'top_5_accuracy': 0.47093749046325684, 'learning_rate': 0.004988714}
I0308 04:33:07.390564 139913490419840 base.py:350] Train Step: 5500/100000  / training metric = {'model_loss': 1199113.625, 'regularization_loss': 0.0, 'total_loss': 1199113.625, 'categorical_cross_entropy': 407754.6875, 'top_1_accuracy': 0.17343750596046448, 'top_5_accuracy': 0.4737499952316284, 'learning_rate': 0.004988329}
I0308 04:34:02.287076 139913490419840 base.py:350] Train Step: 5550/100000  / training metric = {'model_loss': 580391.625, 'regularization_loss': 0.0, 'total_loss': 580391.625, 'categorical_cross_entropy': 497039.875, 'top_1_accuracy': 0.16124999523162842, 'top_5_accuracy': 0.4762499928474426, 'learning_rate': 0.004987937}
I0308 04:34:57.182688 139913490419840 base.py:350] Train Step: 5600/100000  / training metric = {'model_loss': 282014.09375, 'regularization_loss': 0.0, 'total_loss': 282014.09375, 'categorical_cross_entropy': 715665.25, 'top_1_accuracy': 0.12156250327825546, 'top_5_accuracy': 0.39750000834465027, 'learning_rate': 0.004987539}
I0308 04:35:52.104306 139913490419840 base.py:350] Train Step: 5650/100000  / training metric = {'model_loss': 153833.25, 'regularization_loss': 0.0, 'total_loss': 153833.25, 'categorical_cross_entropy': 864711.375, 'top_1_accuracy': 0.1159375011920929, 'top_5_accuracy': 0.4059374928474426, 'learning_rate': 0.0049871337}
I0308 04:36:47.007554 139913490419840 base.py:350] Train Step: 5700/100000  / training metric = {'model_loss': 1215362.25, 'regularization_loss': 0.0, 'total_loss': 1215362.25, 'categorical_cross_entropy': 771002.375, 'top_1_accuracy': 0.10437499731779099, 'top_5_accuracy': 0.3578124940395355, 'learning_rate': 0.0049867225}
I0308 04:37:41.921612 139913490419840 base.py:350] Train Step: 5750/100000  / training metric = {'model_loss': 522426.375, 'regularization_loss': 0.0, 'total_loss': 522426.375, 'categorical_cross_entropy': 833038.5, 'top_1_accuracy': 0.10062500089406967, 'top_5_accuracy': 0.33281248807907104, 'learning_rate': 0.0049863043}
I0308 04:38:36.816616 139913490419840 base.py:350] Train Step: 5800/100000  / training metric = {'model_loss': 911706.625, 'regularization_loss': 0.0, 'total_loss': 911706.625, 'categorical_cross_entropy': 920400.25, 'top_1_accuracy': 0.0625, 'top_5_accuracy': 0.29624998569488525, 'learning_rate': 0.0049858806}
I0308 04:39:31.702463 139913490419840 base.py:350] Train Step: 5850/100000  / training metric = {'model_loss': 1052067.5, 'regularization_loss': 0.0, 'total_loss': 1052067.5, 'categorical_cross_entropy': 1014729.625, 'top_1_accuracy': 0.049687501043081284, 'top_5_accuracy': 0.24906249344348907, 'learning_rate': 0.00498545}
I0308 04:40:26.626227 139913490419840 base.py:350] Train Step: 5900/100000  / training metric = {'model_loss': 148873.15625, 'regularization_loss': 0.0, 'total_loss': 148873.15625, 'categorical_cross_entropy': 1167621.0, 'top_1_accuracy': 0.03812500089406967, 'top_5_accuracy': 0.22875000536441803, 'learning_rate': 0.0049850126}
I0308 04:41:21.540576 139913490419840 base.py:350] Train Step: 5950/100000  / training metric = {'model_loss': 1136518.5, 'regularization_loss': 0.0, 'total_loss': 1136518.5, 'categorical_cross_entropy': 1638601.0, 'top_1_accuracy': 0.02500000037252903, 'top_5_accuracy': 0.20406250655651093, 'learning_rate': 0.004984569}
I0308 04:42:16.438541 139913490419840 base.py:350] Train Step: 6000/100000  / training metric = {'model_loss': 934404.6875, 'regularization_loss': 0.0, 'total_loss': 934404.6875, 'categorical_cross_entropy': 1522227.5, 'top_1_accuracy': 0.023749999701976776, 'top_5_accuracy': 0.19687500596046448, 'learning_rate': 0.004984119}
I0308 04:43:11.359108 139913490419840 base.py:350] Train Step: 6050/100000  / training metric = {'model_loss': 1608090.125, 'regularization_loss': 0.0, 'total_loss': 1608090.125, 'categorical_cross_entropy': 1950979.375, 'top_1_accuracy': 0.0062500000931322575, 'top_5_accuracy': 0.15968750417232513, 'learning_rate': 0.004983662}
I0308 04:44:06.269903 139913490419840 base.py:350] Train Step: 6100/100000  / training metric = {'model_loss': 1019114.6875, 'regularization_loss': 0.0, 'total_loss': 1019114.6875, 'categorical_cross_entropy': 3171475.75, 'top_1_accuracy': 0.005937499925494194, 'top_5_accuracy': 0.16218750178813934, 'learning_rate': 0.0049831998}
I0308 04:45:01.173259 139913490419840 base.py:350] Train Step: 6150/100000  / training metric = {'model_loss': 1277093.375, 'regularization_loss': 0.0, 'total_loss': 1277093.375, 'categorical_cross_entropy': 3348038.75, 'top_1_accuracy': 0.002812500111758709, 'top_5_accuracy': 0.13843749463558197, 'learning_rate': 0.00498273}
I0308 04:45:56.090717 139913490419840 base.py:350] Train Step: 6200/100000  / training metric = {'model_loss': 418131.5625, 'regularization_loss': 0.0, 'total_loss': 418131.5625, 'categorical_cross_entropy': 2211395.0, 'top_1_accuracy': 0.0018749999580904841, 'top_5_accuracy': 0.08781249821186066, 'learning_rate': 0.0049822545}
I0308 04:46:50.990113 139913490419840 base.py:350] Train Step: 6250/100000  / training metric = {'model_loss': 1013870.625, 'regularization_loss': 0.0, 'total_loss': 1013870.625, 'categorical_cross_entropy': 2314980.5, 'top_1_accuracy': 0.0062500000931322575, 'top_5_accuracy': 0.08187499642372131, 'learning_rate': 0.004981772}
I0308 04:47:45.899729 139913490419840 base.py:350] Train Step: 6300/100000  / training metric = {'model_loss': 857057.625, 'regularization_loss': 0.0, 'total_loss': 857057.625, 'categorical_cross_entropy': 1761380.75, 'top_1_accuracy': 0.0037499999161809683, 'top_5_accuracy': 0.061250001192092896, 'learning_rate': 0.0049812836}
I0308 04:48:40.776562 139913490419840 base.py:350] Train Step: 6350/100000  / training metric = {'model_loss': 1539291.25, 'regularization_loss': 0.0, 'total_loss': 1539291.25, 'categorical_cross_entropy': 1815184.75, 'top_1_accuracy': 0.002812500111758709, 'top_5_accuracy': 0.08781249821186066, 'learning_rate': 0.004980788}
I0308 04:49:35.663771 139913490419840 base.py:350] Train Step: 6400/100000  / training metric = {'model_loss': 1977799.5, 'regularization_loss': 0.0, 'total_loss': 1977799.5, 'categorical_cross_entropy': 2075230.875, 'top_1_accuracy': 0.0024999999441206455, 'top_5_accuracy': 0.08562500029802322, 'learning_rate': 0.004980287}
I0308 04:50:30.601303 139913490419840 base.py:350] Train Step: 6450/100000  / training metric = {'model_loss': 1785081.5, 'regularization_loss': 0.0, 'total_loss': 1785081.5, 'categorical_cross_entropy': 2287603.25, 'top_1_accuracy': 0.0003124999930150807, 'top_5_accuracy': 0.09781250357627869, 'learning_rate': 0.0049797785}
I0308 04:51:25.551883 139913490419840 base.py:350] Train Step: 6500/100000  / training metric = {'model_loss': 1369190.5, 'regularization_loss': 0.0, 'total_loss': 1369190.5, 'categorical_cross_entropy': 2639144.0, 'top_1_accuracy': 0.0012499999720603228, 'top_5_accuracy': 0.09437499940395355, 'learning_rate': 0.004979264}
I0308 04:52:20.439293 139913490419840 base.py:350] Train Step: 6550/100000  / training metric = {'model_loss': 8053418.0, 'regularization_loss': 0.0, 'total_loss': 8053418.0, 'categorical_cross_entropy': 3027302.5, 'top_1_accuracy': 0.0015625000232830644, 'top_5_accuracy': 0.08406250178813934, 'learning_rate': 0.004978743}
I0308 04:53:15.363229 139913490419840 base.py:350] Train Step: 6600/100000  / training metric = {'model_loss': 262206.1875, 'regularization_loss': 0.0, 'total_loss': 262206.1875, 'categorical_cross_entropy': 2338608.5, 'top_1_accuracy': 0.0037499999161809683, 'top_5_accuracy': 0.08906249701976776, 'learning_rate': 0.004978216}
I0308 04:54:10.266209 139913490419840 base.py:350] Train Step: 6650/100000  / training metric = {'model_loss': 17855364.0, 'regularization_loss': 0.0, 'total_loss': 17855364.0, 'categorical_cross_entropy': 3746925.0, 'top_1_accuracy': 0.0037499999161809683, 'top_5_accuracy': 0.09468749910593033, 'learning_rate': 0.004977682}
I0308 04:55:05.161387 139913490419840 base.py:350] Train Step: 6700/100000  / training metric = {'model_loss': 4191285.25, 'regularization_loss': 0.0, 'total_loss': 4191285.25, 'categorical_cross_entropy': 3739659.25, 'top_1_accuracy': 0.0009374999790452421, 'top_5_accuracy': 0.09812500327825546, 'learning_rate': 0.004977142}
I0308 04:56:00.063202 139913490419840 base.py:350] Train Step: 6750/100000  / training metric = {'model_loss': 1084895.0, 'regularization_loss': 0.0, 'total_loss': 1084895.0, 'categorical_cross_entropy': 1908365.5, 'top_1_accuracy': 0.0003124999930150807, 'top_5_accuracy': 0.06875000149011612, 'learning_rate': 0.0049765958}
I0308 04:56:54.957091 139913490419840 base.py:350] Train Step: 6800/100000  / training metric = {'model_loss': 3624456.0, 'regularization_loss': 0.0, 'total_loss': 3624456.0, 'categorical_cross_entropy': 2167073.75, 'top_1_accuracy': 0.0034374999813735485, 'top_5_accuracy': 0.06093750149011612, 'learning_rate': 0.0049760425}
I0308 04:57:49.963701 139913490419840 base.py:350] Train Step: 6850/100000  / training metric = {'model_loss': 1864859.75, 'regularization_loss': 0.0, 'total_loss': 1864859.75, 'categorical_cross_entropy': 1897945.5, 'top_1_accuracy': 0.0018749999580904841, 'top_5_accuracy': 0.0625, 'learning_rate': 0.004975483}
I0308 04:58:44.864124 139913490419840 base.py:350] Train Step: 6900/100000  / training metric = {'model_loss': 678997.8125, 'regularization_loss': 0.0, 'total_loss': 678997.8125, 'categorical_cross_entropy': 2214354.0, 'top_1_accuracy': 0.0024999999441206455, 'top_5_accuracy': 0.06750000268220901, 'learning_rate': 0.004974917}
I0308 04:59:39.746266 139913490419840 base.py:350] Train Step: 6950/100000  / training metric = {'model_loss': 2738107.25, 'regularization_loss': 0.0, 'total_loss': 2738107.25, 'categorical_cross_entropy': 3622996.25, 'top_1_accuracy': 0.004062499850988388, 'top_5_accuracy': 0.10593750327825546, 'learning_rate': 0.0049743447}
I0308 05:00:34.655885 139913490419840 base.py:350] Train Step: 7000/100000  / training metric = {'model_loss': 12828540.0, 'regularization_loss': 0.0, 'total_loss': 12828540.0, 'categorical_cross_entropy': 4821058.0, 'top_1_accuracy': 0.0021875000093132257, 'top_5_accuracy': 0.12343750149011612, 'learning_rate': 0.004973766}
I0308 05:01:29.552266 139913490419840 base.py:350] Train Step: 7050/100000  / training metric = {'model_loss': 23238402.0, 'regularization_loss': 0.0, 'total_loss': 23238402.0, 'categorical_cross_entropy': 6946179.0, 'top_1_accuracy': 0.0018749999580904841, 'top_5_accuracy': 0.11124999821186066, 'learning_rate': 0.0049731806}
I0308 05:02:24.478363 139913490419840 base.py:350] Train Step: 7100/100000  / training metric = {'model_loss': 18391932.0, 'regularization_loss': 0.0, 'total_loss': 18391932.0, 'categorical_cross_entropy': 6081235.0, 'top_1_accuracy': 0.004999999888241291, 'top_5_accuracy': 0.12593750655651093, 'learning_rate': 0.004972589}
I0308 05:03:19.376034 139913490419840 base.py:350] Train Step: 7150/100000  / training metric = {'model_loss': 3728803.25, 'regularization_loss': 0.0, 'total_loss': 3728803.25, 'categorical_cross_entropy': 11084411.0, 'top_1_accuracy': 0.0018749999580904841, 'top_5_accuracy': 0.15031249821186066, 'learning_rate': 0.0049719913}
I0308 05:04:14.287791 139913490419840 base.py:350] Train Step: 7200/100000  / training metric = {'model_loss': 7393465.5, 'regularization_loss': 0.0, 'total_loss': 7393465.5, 'categorical_cross_entropy': 7843142.0, 'top_1_accuracy': 0.0018749999580904841, 'top_5_accuracy': 0.14249999821186066, 'learning_rate': 0.004971387}
I0308 05:05:09.213889 139913490419840 base.py:350] Train Step: 7250/100000  / training metric = {'model_loss': 18708090.0, 'regularization_loss': 0.0, 'total_loss': 18708090.0, 'categorical_cross_entropy': 8508862.0, 'top_1_accuracy': 0.0003124999930150807, 'top_5_accuracy': 0.13062499463558197, 'learning_rate': 0.004970776}
I0308 05:06:04.104018 139913490419840 base.py:350] Train Step: 7300/100000  / training metric = {'model_loss': 4797357.0, 'regularization_loss': 0.0, 'total_loss': 4797357.0, 'categorical_cross_entropy': 10867162.0, 'top_1_accuracy': 0.004999999888241291, 'top_5_accuracy': 0.13718749582767487, 'learning_rate': 0.004970159}
I0308 05:06:59.019053 139913490419840 base.py:350] Train Step: 7350/100000  / training metric = {'model_loss': 1360129.625, 'regularization_loss': 0.0, 'total_loss': 1360129.625, 'categorical_cross_entropy': 8285337.5, 'top_1_accuracy': 0.0021875000093132257, 'top_5_accuracy': 0.1287499964237213, 'learning_rate': 0.004969535}
I0308 05:07:53.907558 139913490419840 base.py:350] Train Step: 7400/100000  / training metric = {'model_loss': 5140473.0, 'regularization_loss': 0.0, 'total_loss': 5140473.0, 'categorical_cross_entropy': 7616045.0, 'top_1_accuracy': 0.002812500111758709, 'top_5_accuracy': 0.11656250059604645, 'learning_rate': 0.004968905}
I0308 05:08:48.804555 139913490419840 base.py:350] Train Step: 7450/100000  / training metric = {'model_loss': 12302493.0, 'regularization_loss': 0.0, 'total_loss': 12302493.0, 'categorical_cross_entropy': 9210756.0, 'top_1_accuracy': 0.009687500074505806, 'top_5_accuracy': 0.12343750149011612, 'learning_rate': 0.0049682683}
I0308 05:09:43.740948 139913490419840 base.py:350] Train Step: 7500/100000  / training metric = {'model_loss': 4733610.0, 'regularization_loss': 0.0, 'total_loss': 4733610.0, 'categorical_cross_entropy': 11823826.0, 'top_1_accuracy': 0.0015625000232830644, 'top_5_accuracy': 0.10343749821186066, 'learning_rate': 0.0049676257}
I0308 05:10:38.634593 139913490419840 base.py:350] Train Step: 7550/100000  / training metric = {'model_loss': 17190046.0, 'regularization_loss': 0.0, 'total_loss': 17190046.0, 'categorical_cross_entropy': 9572840.0, 'top_1_accuracy': 0.0015625000232830644, 'top_5_accuracy': 0.08781249821186066, 'learning_rate': 0.004966976}
This is the training record",train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric saved step path train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric train step training metric training record,issue,negative,neutral,neutral,neutral,neutral,neutral
1457127843,The error is only occurring when I'm using script. searcher.search_batched() method is working fine when used in a notebook. I need to use a script though. What could be the issue? ,error script method working fine used notebook need use script though could issue,issue,negative,positive,positive,positive,positive,positive
1455113636,"We are investigating such issues about tf.function, and your answer will be of great help to our work. Thank you!",investigating answer great help work thank,issue,positive,positive,positive,positive,positive,positive
1452067320,"Apparently, the issue is not occuring due to the 1.2.9 version:
https://github.com/google-research/google-research/issues/355#issuecomment-1452037088",apparently issue due version,issue,negative,negative,neutral,neutral,negative,negative
1452037088,Yes I'm able to load for this example and even the search_batched() method is working.,yes able load example even method working,issue,negative,positive,positive,positive,positive,positive
1451696321,@pseudonymmm try to load again the [example](https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb) with any input. Are you able to load it with the latest version (1.2.9)?,try load example input able load latest version,issue,negative,positive,positive,positive,positive,positive
1451154453,Works fine in local environment.,work fine local environment,issue,negative,positive,positive,positive,positive,positive
1450938993,"@Charlyo Yes, I'm able to load the searcher. It gives me error in searcher.search_batched()",yes able load searcher error,issue,negative,positive,positive,positive,positive,positive
1450457528,"> @MFajcik I have used scann in version 1.0. But the problem is you will store your results in `.scann_artefacts/serialize_test` but this directory was not created yet.
> 
> ```python
> os.makedirs('.scann_artefacts/serialize_test', exist_ok=True)
> searcher.serialize(f'.scann_artefacts/serialize_test')
> ```
> 
> will do the job

By doing so, then can you load again the searcher? With score_ah present in the creation, it complains about a missing dataset if exact reorder is used.",used version problem store directory yet python job load searcher present creation missing exact reorder used,issue,negative,positive,neutral,neutral,positive,positive
1447997724,@dbieber @dmrd  @sammymax Could you please help in the matter?,could please help matter,issue,positive,neutral,neutral,neutral,neutral,neutral
1447993731,FYI: Only searchers that score_ah are affected. Brute force ones are not and can be saved / loaded correctly.,affected brute force saved loaded correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
1446473733,"After saving, I load the numpy searcher and then use the searched_batched method:
neighbors, distances = searcher.search_batched(data)

But it's giving me this error: 
AttributeError: 'ScannSearcher' object has no attribute 'searched_batched'

Any idea why this is happening? I'm loading this searcher within a python function.
ScaNN version: 1.2.9

```
def mock_function(data, dir_path):
    searcher = scann.scann_ops_pybind.load_searcher(dir_path)
    neighbors, distances = searcher.search_batched(data)      ### This is where it gives error
    return distances
```",saving load searcher use method data giving error object attribute idea happening loading searcher within python function version data searcher data error return,issue,negative,neutral,neutral,neutral,neutral,neutral
1446220605,Still Happening on current version! Could you please clarify the loading procedure?,still happening current version could please clarify loading procedure,issue,negative,neutral,neutral,neutral,neutral,neutral
1442416167,"@AhmedTElthakeb please find comments below:

""From this [code](https://github.com/google-research/google-research/blob/master/kws_streaming/train/test.py), it seems the streaming accuracy calculation [L169](https://github.com/google-research/google-research/blob/master/kws_streaming/train/test.py#L169) occurs outside the iteration over frames loop [L158](https://github.com/google-research/google-research/blob/master/kws_streaming/train/test.py#L158), although the network outputting new result per frame. Could you please comment on this? Could this be missing potential false alarms?""
The goal here is to validate that streaming function has the same accuracy with non streaming one: so I run streaming model in streaming mode and use the last prediction as final one for accuracy computation. This way you can compare non streaming and streaming accuracy. For general evaluation of streaming approach you will need to compute accuracy at every step to measure false alarm in time.

""If that is the case, why would streaming accuracy be different compared to non-streaming accuracy given the same model? where is the difference coming from?""
There will be numerical difference in TF in order of 1e-5, it is due to floating point multiplication as in [line](https://source.corp.google.com/piper///depot/google3/third_party/google_research/google_research/kws_streaming/models/ds_tc_resnet_qat_nbit_test.py;l=53)..

There will be numerical difference when you run it with TFLite because there is numerical difference between TFLite and TF as in [line](https://source.corp.google.com/piper///depot/google3/third_party/google_research/google_research/kws_streaming/models/cnn_test.py;l=162), it is expected around 1e-3 or more.

""Lastly, is this streaming inference simulation dependent on the execution engine/hardware?""
No, it does not depend on execution engine or hardware.
",please find code streaming accuracy calculation outside iteration loop although network new result per frame could please comment could missing potential false goal validate streaming function accuracy non streaming one run streaming model streaming mode use last prediction final one accuracy computation way compare non streaming streaming accuracy general evaluation streaming approach need compute accuracy every step measure false alarm time case would streaming accuracy different accuracy given model difference coming numerical difference order due floating point multiplication line numerical difference run numerical difference line around lastly streaming inference simulation dependent execution depend execution engine hardware,issue,negative,negative,neutral,neutral,negative,negative
1439363658,"> The MOS score is the average of the scores given to an image by many people, with higher scores generally indicating better quality of the image.",score average given image many people higher generally better quality image,issue,negative,positive,positive,positive,positive,positive
1437834070,"Hi [maniatis](https://github.com/maniatis),
I opened this pull request to fix a bug in the source code. I also modified the `run_classifier.py` to extract the embedding from CuBERT model and I would like to contributed to this repo. 

Kamel",hi pull request fix bug source code also extract model would like,issue,negative,neutral,neutral,neutral,neutral,neutral
1435436317,"@googlebot  I signed it!
Any update on my pull request. The [issue](https://github.com/google-research/google-research/issues/571#issuecomment-1160769111) ",update pull request issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1435375048,Found where the padding mask is applied to the values in performer/fast_attention/jax/attention_module.py line 121. It doesn't look like it carried over to the tensorflow version for anyone that has this same question. ,found padding mask applied line look like carried version anyone question,issue,negative,neutral,neutral,neutral,neutral,neutral
1434010672,Is there a way to use TFT for time series classification? ,way use time series classification,issue,negative,neutral,neutral,neutral,neutral,neutral
1425547677,"Hi,can you send me  the 200M parameter model checkpoint? My email is zhoujieyunzjy@163.com.
thank you so much!!!!!!!",hi send parameter model thank much,issue,negative,positive,positive,positive,positive,positive
1421486032,Actually it makes more sense to mask data_dash and data_diag before they go through the exp function to remove influence of padding. Thoughts?,actually sense mask go function remove influence padding,issue,negative,neutral,neutral,neutral,neutral,neutral
1420354809,"@DavidWiesner , @XiaoConstantine @YJYJLee and @Soroosh-aval did you manage to add additional embedding to the scanner without re-initialization ? ",manage add additional scanner without,issue,negative,neutral,neutral,neutral,neutral,neutral
1409395672,"I ran into this issue as well, and I don't have a good solution. I tried removing all newlines from the input files, but then it took longer to fail and I got an error about an ""UNK"" token. I don't know why it fails like this...",ran issue well good solution tried removing input took longer fail got error token know like,issue,negative,positive,neutral,neutral,positive,positive
1408659892,Could you provide a link to the checkpoint you used with model?,could provide link used model,issue,negative,neutral,neutral,neutral,neutral,neutral
1406001986,"Hi Severin,

Thanks for your interest in our work and apologies for the late reply. We have not encountered issues like this before, and the symptom you described makes me suspect image size mismatch, too.

Would it work if you resize/crop all your inputs to 512x512 before running the code? I don't remember exactly whether it was necessary (it's been a while and I've already left the team), but it's probably worth trying?

Thanks,
Qiurui",hi thanks interest work late reply like symptom suspect image size mismatch would work running code remember exactly whether necessary already left team probably worth trying thanks,issue,positive,positive,neutral,neutral,positive,positive
1401113037,"Updating issue after ping interval. See its status below.

---

Project is out of compliance with Binary Artifacts policy: binaries present in source code

**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.

**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.

**Artifacts Found**

- tf3d/ops/packages/tensorflow_sparse_conv_ops-0.0.1-cp36-cp36m-linux_x86_64.whl
- tf3d/ops/packages/tensorflow_sparse_conv_ops-0.0.1-cp37-cp37m-linux_x86_64.whl

**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.",issue ping interval see status project compliance binary policy present source code rule description binary security risk repository binary introduction possibly obsolete maliciously information see security documentation binary remediation remove executable repository found additional information policy drawn security tool project adherence security best may wish run scan directly repository,issue,positive,positive,positive,positive,positive,positive
1399222330,"Hi! 
I have the same problem. I was trying to install scann as requirement for tflite-model-maker under enviroments with python 3.10, 3.19, 3.8 in windows 10. 
I read that scann is a linux lib. Ok, I tried to install from my hosted linux server which doesnt many libraries installed. The result was exactly the as as in windows:
**ERROR: Could not find a version that satisfies the requirement scann==1.2.6 (from versions: none)
ERROR: No matching distribution found for scann==1.2.6**

I read that it works in WSL, great, I tried to install scann from a virtual env on Cygwin. Same result.
Bottom line: pip cannot fint scann netiher from Windows, Cygwin nor Linux server.

I dont know how to check if the lib is available, or maybe which kind of requirement do I need to install.

Any light to this poor lost soul?",hi problem trying install requirement python read tried install server doesnt many result exactly error could find version requirement none error matching distribution found read work great tried install virtual result bottom line pip server dont know check available maybe kind requirement need install light poor lost soul,issue,negative,positive,positive,positive,positive,positive
1398323610,"I have a similar error ""OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds."" with an additional message 
""slice index 0 of dimension 0 out of bounds.
         [[{{node strided_slice_2}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[IteratorGetNext]] [Op:__inference_inference_step_26864]""
that comes from [this line](https://github.com/google-research/google-research/blob/2b2c362f274e801ea4af19e68f0154dcbe1496da/vatt/experiments/base.py#L386). Was anyone able to fix it? ",similar error slice index dimension additional message slice index dimension node come line anyone able fix,issue,negative,positive,positive,positive,positive,positive
1398255623,"Hello. I am facing the same error message with **pip install scann** on my Windows 11.
ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann

Did anyone found a solution how to install properly **scann** for Windows? Thanks in advance!",hello facing error message pip install error could find version requirement none error matching distribution found anyone found solution install properly thanks advance,issue,negative,positive,positive,positive,positive,positive
1376370472,Can you clarify this statement? The exported models match the architecture defined in the paper and do contain dilated convolutions.,clarify statement match architecture defined paper contain dilated,issue,negative,neutral,neutral,neutral,neutral,neutral
1376366690,"If the model does not have the field, it is not possible to predict right disparity image. The sceneflow models may have it available. You may try a simple image flipping trick: Swap left and right images and mirror them along vertical access before feeding into the network. the prediction should have an inverse operation.",model field possible predict right disparity image may available may try simple image trick swap left right mirror along vertical access feeding network prediction inverse operation,issue,negative,positive,positive,positive,positive,positive
1373487384,@ybkim95 Sorry I may not be much of help as its been a long time,sorry may much help long time,issue,negative,negative,negative,negative,negative,negative
1373301423,"To add one more question, when I did a few-shot ICL experiment with T5.1.1-XXL and UL2 20B, the performance was generally better in one-shot than in zero-shot. 
However, the performance tended to decrease from a certain number of shots, and the number of shots at which this performance began to decrease was smaller in UL2 than in T5. 

I wonder if the authors also experienced this trend.

Thank you.

cc: @MostafaDehghani ",add one question experiment performance generally better however performance decrease certain number number performance decrease smaller wonder also experienced trend thank,issue,positive,positive,positive,positive,positive,positive
1373110571,@sainivedh19pt may I ask where you put your tf.function() decorator? I couldn't solve the above error when trying to use my own dataset which is transformed into tf_record format as dmvr example describes.,may ask put decorator could solve error trying use format example,issue,negative,neutral,neutral,neutral,neutral,neutral
1368171922,"I faced the same problem with  Python  from 3.7 and 3.8 , pip 19  the matter has something to do with pip ,the solution was to upgrade pip
pip install --upgrade pip",faced problem python pip matter something pip solution upgrade pip pip install upgrade pip,issue,negative,neutral,neutral,neutral,neutral,neutral
1365352501,"> I can reproduce the results via the pre-trained models, but I cannot train custom data. Most likely, this is related to #1096

I'm sorry that I can't help you. I just ran this code a few times to understand some data processing flow. However, issues with custom datasets may not be related to #1096 since it just states that there is no NLL Loss in the code.",reproduce via train custom data likely related sorry ca help ran code time understand data flow however custom may related since loss code,issue,negative,negative,negative,negative,negative,negative
1365334439,"I can reproduce the results via the pre-trained models, but I cannot train custom data. Most likely, this is related to https://github.com/google-research/google-research/issues/1096 ",reproduce via train custom data likely related,issue,negative,neutral,neutral,neutral,neutral,neutral
1365313209,"Haven't tried yet, but I will do some experiments in the next few days. Is there any problem?",tried yet next day problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1365149818,True. A quick question. Can you replicate the results in the paper? ,true quick question replicate paper,issue,negative,positive,positive,positive,positive,positive
1365144231,"> AFAICT, based on my testing, both cases have the same outcome.

The metric may be the same. However, if `random_poses.append(viewmatrix(z_axis, up, position))` is in the for-loop. the `random_poses` will be a list contains `n_poses` of view-matrix, but in this case, it is a list which only contains one view-matrix.",based testing outcome metric may however position list case list one,issue,negative,neutral,neutral,neutral,neutral,neutral
1364845144,"try this:
```
 pip install --upgrade jaxlib==0.1.68+cuda110 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```",try pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
1361508377,"Hi.

I am facing these problem too. I can not reproduce results reported in main paper.
first I tried configs in this [link](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_12_labels.md#bc_resnet_1) but maximum accuracy for training dataset was about 35%.
after that, I tried to reproduce result by running code with changing hyper parameters such learning rate and iteration number. the best accuracy I reached was 83% for training dataset and 89% for validation dataset.

`--how_many_training_steps 200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200,200 
--learning_rate 2e-2,1e-2,1e-3,1e-4,1e-5,1e-6,2e-2,1e-2,1e-3,1e-4,1e-5,1e-6,2e-2,1e-2,1e-3,1e-4,1e-5,1e-6'`",hi facing problem reproduce main paper first tried link maximum accuracy training tried reproduce result running code hyper learning rate iteration number best accuracy training validation,issue,negative,positive,positive,positive,positive,positive
1360945404,"unsubscribe me

On Wed, 21 Dec 2022 at 12:37, google-allstar-prod[bot] <
***@***.***> wrote:

> Updating issue after ping interval. See its status below.
> ------------------------------
>
> Project is out of compliance with Binary Artifacts policy: binaries
> present in source code
>
> *Rule Description*
> Binary Artifacts are an increased security risk in your repository. Binary
> artifacts cannot be reviewed, allowing the introduction of possibly
> obsolete or maliciously subverted executables. For more information see the Security
> Scorecards Documentation
> <https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts>
> for Binary Artifacts.
>
> *Remediation Steps*
> To remediate, remove the generated executable artifacts from the
> repository.
>
> *Artifacts Found*
>
>    -
>    tf3d/ops/packages/tensorflow_sparse_conv_ops-0.0.1-cp36-cp36m-linux_x86_64.whl
>    -
>    tf3d/ops/packages/tensorflow_sparse_conv_ops-0.0.1-cp37-cp37m-linux_x86_64.whl
>
> *Additional Information*
> This policy is drawn from Security Scorecards
> <https://github.com/ossf/scorecard/>, which is a tool that scores a
> project's adherence to security best practices. You may wish to run a
> Scorecards scan directly on this repository for more details.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1421#issuecomment-1360933277>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/A4LYAGSE55CDD5EVUQGNI4DWOKUCDANCNFSM6AAAAAAS7VFZ4U>
> .
> You are receiving this because you are subscribed to this thread.Message
> ID: ***@***.***>
>
",wed bot wrote issue ping interval see status project compliance binary policy present source code rule description binary security risk repository binary introduction possibly obsolete maliciously information see security documentation binary remediation remove executable repository found additional information policy drawn security tool project adherence security best may wish run scan directly repository reply directly view id,issue,positive,positive,positive,positive,positive,positive
1360068208,@DavidMChan : I am trying this https://github.com/tensorflow/recommenders/issues/597. basically trying to infer a scann powered savedmodel in Java through native TF. Is it possible? Does installing scann like above would help since this will only install scann in a python venv.?,trying basically trying infer powered native possible like would help since install python,issue,positive,neutral,neutral,neutral,neutral,neutral
1360001344,"Possibly/probably... I don't think there were any M1 specific optimizations that I had to make, but I don't really know. The best thing to do would be to try it and see if it works. ",think specific make really know best thing would try see work,issue,positive,positive,positive,positive,positive,positive
1358862154,Hey thanks and apols it just took me about 10 mins to work out 'causal' padding :),hey thanks took work padding,issue,negative,positive,positive,positive,positive,positive
1358857971,"Yeah not that sure how with the streaming subclassing, but yeah have read that article and will give it a go.",yeah sure streaming yeah read article give go,issue,positive,positive,positive,positive,positive,positive
1358841061,"To run this model in streaming mode you need to set:
```
paddings 'causal' \
```

You can stream non causal models with 
```
--paddings 'same' \
```
but you will need to modify the model and introduce delay as in [example](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/delay_test.py#L106)
",run model streaming mode need set stream non causal need modify model introduce delay example,issue,negative,neutral,neutral,neutral,neutral,neutral
1358836637,"I would suggest to follow an [example](https://www.tensorflow.org/lite/examples/on_device_training/overview) and use TF V2 in kws_streaming, this way the above example should be compatible with kws_streaming and the reaming question would be about TFLite supported ops.",would suggest follow example use way example compatible question would,issue,negative,neutral,neutral,neutral,neutral,neutral
1358678155,"You just need to make the changes to the code in the patch files (either manually, or using the `git apply`, see [here](https://everythingdevops.dev/how-to-create-and-apply-a-git-patch-with-git-diff-and-git-apply-commands/)), and then replace the `bazel ... :build_pip_pkg`  in the instructions with the modified build command above (while making sure you're using bazel 4.2.2).",need make code patch either manually git apply see replace build command making sure,issue,negative,positive,positive,positive,positive,positive
1358495000,@DavidMChan : Can you please elaborate the steps to install using the patch file?,please elaborate install patch file,issue,negative,positive,positive,positive,positive,positive
1357756533,"These versions work perfectly for me:

`Successfully installed flatbuffers-1.12 keras-2.9.0 scann-1.2.7 tensorboard-2.9.1 tensorflow-2.9.3 tensorflow-estimator-2.9.0`

I uninstalled `tensorflow-gpu` and installed `scann==1.2.7` via pip

My versions of Tensorflow Recommenders and Tensorflow Datasets are 0.7.0 and 4.7.0, respectively

My pip version is 22.0.2

The sequence for a container is:

```
scann==1.2.7
tensorflow-recommenders==0.7.0
tensorflow==2.9.1
```",work perfectly successfully uninstalled via pip respectively pip version sequence container,issue,positive,positive,positive,positive,positive,positive
1356139997,I too think there is some pre-processing required for the datasets to be consumed by the BLT code ([more on this](https://github.com/google-research/google-research/issues/1423)). I hope the authors would respond. ,think code hope would respond,issue,negative,neutral,neutral,neutral,neutral,neutral
1345950027,"@YingchaojieFeng Was your issue resolved? If yes, could you please let me know the required dependencies. Thanks. ",issue resolved yes could please let know thanks,issue,positive,positive,positive,positive,positive,positive
1345948064,"@wuchiuwong Has your error resolved? If yes, could you please let me know the fix. Thanks.",error resolved yes could please let know fix thanks,issue,positive,positive,positive,positive,positive,positive
1345469223,"> You can find `mel_features.py` in `tensorflow_models/audioset/vggish/`. To fix this and oother import problems just create `tensorflow_models/audioset/__init__.py` and add the line: `from .vggish import mel_features, vggish_params, vggish_slim` Also, in file `tensorflow_models/audioset/vggish/vggish_slim.py` you should change the import to `from . import vggish_params as params`.
> 
> You will also have problems with Tensorflow version: for me it worked r1.15 and changing in file `frechet_audio_distance/fad_utils.py` line 37 to `tf_record = tf.python_io.tf_record_iterator(filename).__next__()`

An update here on recent dig-in on FAD - TF 2.+ should work, I am using the more recent TF 2.11.0 but only changing line 37 as suggested by Javier, it works fine on my side. The import related changes by Javier are still needed though.

Thanks again @javiernistal for this comment, it's golden!",find fix import create add line import also file change import import also version worked file line update recent fad work recent line work fine side import related still though thanks comment golden,issue,positive,positive,positive,positive,positive,positive
1345456182,"> Solved by author response via email

Could you give me  the author solution ? Thank you very much ! ",author response via could give author solution thank much,issue,positive,positive,positive,positive,positive,positive
1344253790,"Thanks @rybakov looks like I got the wrong end of the stick with 'micro' it just looked very much like [the micro front-end](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/microfrontend) and made an assumption.

Thanks for all the work and if you ever get time to add a few more examples then please do :)
 ",thanks like got wrong end stick much like micro made assumption thanks work ever get time add please,issue,positive,positive,neutral,neutral,positive,positive
1343415106,"Please have a look at  [speech-feature-extraction-configs](https://github.com/google-research/google-research/tree/master/kws_streaming#speech-feature-extraction-configs)
In your command feature_type  will be ignored and only preprocess='micro' will be used:
```
--feature_type 'mfcc_op' \
--fft_magnitude_squared 1 \
--preprocess 'micro' \
```

I would suggest to use default speech feature extractor with preprocess='raw';
feature_type='mfcc_tf', it is better also to set use_tf_fft=1 (so that you can quantize your model). This way you have full control over the whole model and can run it end to end. 'mfcc_op' is a special op in TFLite (it can run only on CPU) and you can only set some parameters there.  'micro' is a very special use case (it can run only on CPU), model will be divided into two parts, the first part will have 'micro' speech feature extractor and second part is the model itself, so during inference you will have to run feature extractor using 'micro' and then feed extracted features into your model.
",please look command used would suggest use default speech feature extractor better also set quantize model way full control whole model run end end special run set special use case run model divided two first part speech feature extractor second part model inference run feature extractor feed extracted model,issue,positive,positive,positive,positive,positive,positive
1343330936,Seconded. I was trying to migrate from https://github.com/tagucci/pythonrouge whose SU4 has the highest variance on my summary evaluations (and so is an important metric in the PCA/eigenvector sense) but you don't provide it. Is there a reason?,trying migrate whose highest variance summary important metric sense provide reason,issue,negative,positive,positive,positive,positive,positive
1341979453,"With the RTX card and newer TF 
```
2022-12-08 04:14:49.318105: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:1075 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/dense_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
```
The GRU was with a CRNN but if its a quick and easy reply then please do but will look into that as pretty sure never saw these with the older 1070 card and think it was tf2.8 when I last used kws_streaming",card operation name id device assigned node setting attribute run session mutation effect trigger error future either modify running create new session quick easy reply please look pretty sure never saw older card think last used,issue,positive,positive,positive,positive,positive,positive
1338968367,"Apologies for the delay as none of the authors of this work get notifications on issues here. For faster responses you can either tag me using @charlesjin, or just email me directly at ccj@csail.mit.edu

Regarding your question, each config contains a graph which can be executed as a JAX model using the abstract_nas.model.Model class.

Unfortunately, our codebase only provides support for native JAX training and inference.

You could always convert our models to Tensorflow manually by inspecting the ops in the graph, and then reimplement the model in Tensorflow. If you have questions about interpreting the graph representation, feel free to follow up with me.

There also appear to be some tools that would do this for you automatically (e.g., jax2tf in jax.experimental might be what you're looking for), but I have no experience with them.

Hope this helps!",delay none work get faster either tag directly regarding question graph executed model class unfortunately support native training inference could always convert manually graph model graph representation feel free follow also appear would automatically might looking experience hope,issue,positive,neutral,neutral,neutral,neutral,neutral
1336826919,"When you give colab example, it is assumed it will run as is.

On Mon, 5 Dec 2022, 04:36 rybakov, ***@***.***> wrote:

> Please install dependencies as described in link
> <https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#install-tensorflow-with-deps>
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1382#issuecomment-1336652034>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AC7IWCZY2MQXS7QG3EFEL5TWLVILXANCNFSM6AAAAAASHRX6GY>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",give example assumed run mon wrote please install link reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1336323973,"Fortunately, a peer was able to provide me with the code for the full ul2 objective.

```python
# found this function and modified from https://github.com/GoogleCloudPlatform/t5x-on-vertex-ai/blob/main/tasks/custom_tasks.py#L78
# UL2 paper appendix code missed this function
def prepend_prompt(dataset: tf.data.Dataset,
                   output_features: seqio.preprocessors.OutputFeaturesType,
                   sequence_length: Optional[
                       seqio.preprocessors.SequenceLengthType] = None,
                   prompt_mode: str = """",
                   key: str = ""inputs"",
                   mode: str = """") -> tf.data.Dataset:
    """"""Prepends a prompt at the beginning of an input sequence.""""""
    del sequence_length
    if prompt_mode and mode:
        # output_features may not have inputs key
        out_keys = list(output_features.keys())
        prompt_tokens = output_features[out_keys[0]
                                        ].vocabulary.encode_tf(prompt_mode)

        def add_to_inputs(x):
            x[key] = tf.concat([prompt_tokens, x[key]], axis=0)
            return x

        dataset = dataset.map(add_to_inputs)
    return dataset
```",fortunately peer able provide code full objective python found function paper appendix code function optional none key mode prompt beginning input sequence mode may key list key key return return,issue,negative,positive,positive,positive,positive,positive
1328173346,"I only changed the config.py. Here is my config
```
def get_config():

  return config_dict(
      seed=0,
      dataset=config_dict(
          name='MockCIFAR10',
          args=config_dict(
              class_conditional=False,
              randflip=True,
          ),
      ),
      model=config_dict(
          # architecture, see main.py and model.py
          name='unet0',
          args=config_dict(
              ch=128,
              out_ch=3,
              ch_mult=[1, 2, 2, 2],
              num_res_blocks=2,
              attn_resolutions=[16],
              num_heads=1,
              dropout=0.1,
              model_output='logits',  # logits or logistic_pars
          ),
          # diffusion betas, see diffusion_categorical.get_diffusion_betas
          diffusion_betas=config_dict(
              type='linear',
              # start, stop only relevant for linear, power, jsdtrunc schedules.
              start=0.02,  # 1e-4 gauss, 0.02 uniform
              stop=1.,  # 0.02, gauss, 1. uniform
              num_timesteps=1000,
          ),
          # Settings used in diffusion_categorical.py
          model_prediction='x_start',  # 'x_start','xprev'
          # 'gaussian','uniform','absorbing'
          transition_mat_type='uniform',
          transition_bands=None,
          loss_type='kl',  # kl,cross_entropy_x_start, hybrid
          hybrid_coeff=0.001,  # only used for hybrid loss type.
      ),
      train=config_dict(
          # optimizer
          batch_size=10,
          optimizer='adam',
          learning_rate=2e-4,
          learning_rate_warmup_steps=0,
          weight_decay=0.0,
          ema_decay=0.9999,
          grad_clip=1.0,
          substeps=1,
          num_train_steps=1500000,  # multiple of substeps
          # logging
          log_loss_every_steps=1000,
          checkpoint_every_secs=900,  # 15 minutes
          retain_checkpoint_every_steps=100000,
          eval_every_steps=50000,
      ))
```",return architecture see diffusion see start stop relevant linear power gauss uniform gauss uniform used hybrid used hybrid loss type multiple logging,issue,negative,positive,positive,positive,positive,positive
1295837469,"@mln-wave Hi, I met the same error when using the colab notebook:
`UnfilteredStackTrace: TypeError: Can't call __hash__ on modules that hold variables.`
Have you solved this problem?",hi met error notebook ca call hold problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1289959878,"After re-reading the authors' paper for Pr-VIPE, I was able to answer my own question.  This work was never intended to predict 3D key points.  Instead, it only aims to embed the poses such that similar 3D poses would produce similar embeddings.  This is meant for applications like action recognition, pose retrieval and syncing of videos.  So there is no capability to decode these embeddings into 3D key points, but this shouldn't be hard to extend from the author's work.",paper able answer question work never intended predict key instead embed similar would produce similar meant like action recognition pose retrieval capability decode key hard extend author work,issue,negative,positive,neutral,neutral,positive,positive
1289707296,I'd also love to have access to the checkpoints from the smaller versions of the model.,also love access smaller model,issue,positive,positive,positive,positive,positive,positive
1287246509,I've found a solution to install it manually. See: https://github.com/CompVis/latent-diffusion/issues/147#issuecomment-1279934409,found solution install manually see,issue,negative,neutral,neutral,neutral,neutral,neutral
1287231422,scann works when you install it via wsl. It doesn't seem to have windows support atm,work install via seem support,issue,negative,neutral,neutral,neutral,neutral,neutral
1286766624,"Hi @JixiangGao, @AlyssaYoung, @alizaree , @sainivedh19pt and @hassanhub ,

Have you figured out the solution by any chance?
It would be very helpful if you could guide me past the error.

Best",hi figured solution chance would helpful could guide past error best,issue,positive,positive,positive,positive,positive,positive
1285488336,Could you try making your datasets (train/eval) size (currently 8) bigger than your batch size (currently 32).,could try making size currently bigger batch size currently,issue,negative,neutral,neutral,neutral,neutral,neutral
1285474285,"I also tried 2.9.1 but the error is the same.

```bash
+ python run_felix.py --train_file=data/output/train.tfrecord --eval_file=data/output/tune.tfrecord --model_dir_tagging=data/output/model_tagging --bert_config_tagging=/home/vimos/Data/NFS/corpus/discofuse/discofuse_v1/wikipedia/felix_config.json --max_seq_length=128 --num_train_epochs=500 --num_train_examples=8 --num_eval_examples=8 --train_batch_size=32 --eval_batch_size=32 --log_steps=100 --steps_per_loop=100 --train_insertion=False --use_pointing=True --learning_rate=0.00003 --pointing_weight=1 --use_weighted_labels=True --init_checkpoint=data/models/uncased_L-12_H-768_A-12/bert_model.ckpt
2022-10-20 20:30:29.765338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-20 20:30:29.792027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-20 20:30:29.792335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-20 20:30:29.803386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-10-20 20:30:29.803798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-20 20:30:29.804118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-20 20:30:29.804264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-20 20:30:30.241821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-20 20:30:30.242105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-20 20:30:30.242309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-10-20 20:30:30.242487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10399 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1
I1020 20:30:32.546360 140492064822208 optimization.py:87] using Adamw optimizer
I1020 20:30:32.546533 140492064822208 optimization.py:140] gradient_clip_norm=1.000000
I1020 20:30:32.734020 140492064822208 run_felix.py:169] Initializing from a BERT checkpoint...
I1020 20:30:33.160571 140492064822208 run_felix.py:186] Starting training from iteration: 0.
Epoch 1/500
Traceback (most recent call last):
  File ""/home/vimos/Data/Text/chn_nlp/TextEditing/felix/run_felix.py"", line 239, in <module>
    app.run(main)
  File ""/home/vimos/anaconda3/lib/python3.9/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/home/vimos/anaconda3/lib/python3.9/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""/home/vimos/Data/Text/chn_nlp/TextEditing/felix/run_felix.py"", line 228, in main
    return run_train(bert_config, FLAGS.max_seq_length,
  File ""/home/vimos/Data/Text/chn_nlp/TextEditing/felix/run_felix.py"", line 196, in run_train
    pretrain_model.fit(
  File ""/home/vimos/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/vimos/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 1420, in fit
    raise ValueError('Unexpected result of `train_function` '
ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.
```

I have suceessfully created `tfrecords`.
```bash
(base)  ➜  felix vimos@vimos-Z270MX-Gaming5  % ll data/output/*.tfrecord
-rw-rw-r-- 1 vimos vimos 35G Oct  9 14:04 data/output/train.tfrecord
-rw-rw-r-- 1 vimos vimos 98M Oct  9 15:54 data/output/tune.tfrecord
```",also tried error bash python successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero binary deep neural network library use following enable rebuild appropriate compiler successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero device memory device name ti bus id compute capability starting training iteration epoch recent call last file line module main file line run main file line main file line main return file line file line raise none file line fit raise result unexpected result empty please use true information went wrong file bash base,issue,positive,positive,neutral,neutral,positive,positive
1284884058,"I analyze that nets.py code suppose that making MPI for 32 layers. That's
why I'd like to see training process of this model. But I can't found that
train single view MPI in here.
So I left the issue of training process code of this repository.

2022년 10월 20일 (목) 오후 12:47, 夕月 ***@***.***>님이 작성:

> You can check mpi in net.py_ from_ Image function, which is used to
> generate mpi.Have you implemented the training code? I want to refer to the
> training process，I have some difficulties in training code implementation
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1161#issuecomment-1284881439>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AJPACNA7SAAIBSAM2S6UDQTWEC6G3ANCNFSM5Y5COFOQ>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>

-- 

",analyze code suppose making like see training process model ca found train single view left issue training process code repository check image function used generate training code want refer training training code implementation reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1284881439,"You can check mpi in net.py_ from_ Image function, which is used to generate mpi.Have you implemented the training code? I want to refer to the training process，I have some difficulties in training code implementation",check image function used generate training code want refer training training code implementation,issue,negative,neutral,neutral,neutral,neutral,neutral
1284875024,"In this repository, we could only use 32 MPI layers. I'd like to train MPI
64, 128 layers so that it could express well at 3D scene.

2022년 10월 20일 (목) 오후 12:36, 夕月 ***@***.***>님이 작성:

> yes，it‘s right
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1161#issuecomment-1284873284>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AJPACNCT727PBSEE5XHUFBTWEC44LANCNFSM5Y5COFOQ>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>

-- 

",repository could use like train could express well scene right reply directly view id,issue,positive,positive,positive,positive,positive,positive
1284868142,"> Hello, have you realized the training process?

I understand that compare view synthesis output and target image for training MPI.
Is it right?",hello training process understand compare view synthesis output target image training right,issue,negative,positive,positive,positive,positive,positive
1281031149,"Hi, is there a reason to use 2.9.2 and not 2.9.1? 

But it might be worth checking that your dev/train tfrecords arn't empty. There should be a text file that specifies the size. ",hi reason use might worth ar empty text file size,issue,negative,positive,neutral,neutral,positive,positive
1274473381,"Hi @liuzhengzhe 
Thanks for the quick reply!

I think jax or some library version has changed since you last ran. Can you once run it now? Or share your notebook?

[This](https://colab.research.google.com/drive/1Sq_Bua2_O5ThFjr5KiP3WCkELA8larBM#scrollTo=PrCmSqTIAor-) is the notebook with the error I am facing. 
",hi thanks quick reply think library version since last ran run share notebook notebook error facing,issue,negative,positive,positive,positive,positive,positive
1274433639,"Yes, I use this colab demo version and it works fine.

Zhengzhe

mln-wave ***@***.***> 于2022年10月11日周二 17:45写道：

> Hi @liuzhengzhe <https://github.com/liuzhengzhe>
> Do you mean that you were able to run colab notebook provided in this
> project page <https://ajayj.com/dreamfields>?
> Actually I am facing the below error when running the trainng setp in that
> notebook.
> UnfilteredStackTrace: TypeError: Can't call __hash__ on modules that hold
> variables.
>
> If you have working colab version, can u share the link here?
>
> Thank you!
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1077#issuecomment-1274421750>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AB7WPDCISR33HTHO2QFVUWTWCUZLXANCNFSM5ULTGZCA>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",yes use version work fine hi mean able run notebook provided project page actually facing error running notebook ca call hold working version share link thank reply directly view id,issue,positive,positive,positive,positive,positive,positive
1274432204,"Hi
Does anyone know how to overcome this?
Was anybody successful in trying this out?",hi anyone know overcome anybody successful trying,issue,positive,positive,positive,positive,positive,positive
1274421750,"Hi @liuzhengzhe 
Do you mean that you were able to run colab notebook provided in [this project page](https://ajayj.com/dreamfields)?
Actually I am facing the below error in the training step in that notebook.
`UnfilteredStackTrace: TypeError: Can't call __hash__ on modules that hold variables.` 

If you have working colab version, can u share the link here?

Thank you!",hi mean able run notebook provided project page actually facing error training step notebook ca call hold working version share link thank,issue,negative,positive,neutral,neutral,positive,positive
1274409671,"Hi @hanhung 
Why did u close this thread? Were you able to reproduce with good results?
 ",hi close thread able reproduce good,issue,negative,positive,positive,positive,positive,positive
1274240924,"Each sampled path is trained on a distinct core, so all the parameters and
activations of a path need to fit in the memory of 1 ""core"".
Each core trains in parallel on the same batch of data.
FYI the TPUv4 experiments were using 32 megacores each having ~32G (not 64)
Yes I'm not sure why it wouldn't fit in 80G if it fits on 32G in my
experiments
A single A100 machine has 80G but how many cores are on 1 machine? (I think
we are assuming there is only 1 core per machine but maybe it is more, e.g.
if there are 4 cores then each core can use only 80/4=20G of memory)


On Thu, Jun 30, 2022 at 10:57 AM adamantboy ***@***.***>
wrote:

> oh great, then I'll add that function in the colab. What's the highest
> batch size you can reach now?
> … <#m_6047855296256924580_>
> On Thu, Jun 30, 2022 at 10:21 AM adamantboy *@*.***> wrote: Could you try
> to add this function to the colab def delete_live_buffers(): for device in
> jax.local_devices(): lbs = device.live_buffers() for lb in lbs: lb.delete()
> and add a call to it just before each call to gc.collect() And check if
> this allows you to increase the batch size to 4 or higher. If not would be
> interesting to know if you see the same error or a different one. I tried
> it. The error is same, but the memory preallocated is reduced to 70-90GB
> when the batch size is 64 indeed.And the batch size can be set to 4 with no
> OOM error. - Error message, batch size 64: 2022-06-30 15:50:02.179408: W
> external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:479]
> Allocator (GPU_3_bfc) ran out of memory trying to allocate 74.65GiB
> (rounded to 80159329536)requested by op 2022-06-30 15:50:06.549646: W
> external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:491]
> *_________________________________________________________________________________________________
> 2022-06-30 15:50:33.277741: E
> external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2141]
> Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while
> trying to allocate 80159329472 bytes. BufferAssignment OOM Debugging.
> BufferAssignment stats: parameter allocation: 1.33GiB constant allocation:
> 8B maybe_live_out allocation: 288.38MiB preallocated temp allocation:
> 74.65GiB preallocated temp fragmentation: 212B (0.00%) total allocation:
> 75.98GiB total fragmentation: 19.2KiB (0.00%) Peak buffers: Buffer 1: Size:
> 1.27GiB XLA Label: custom-call Shape: f32[64,16,577,577]
> ========================== Buffer 2: Size: 1.27GiB XLA Label: custom-call
> Shape: f32[64,16,577,577] ========================== — Reply to this email
> directly, view it on GitHub <#1187 (comment)
> <https://github.com/google-research/google-research/issues/1187#issuecomment-1170915826>>,
> or unsubscribe
> https://github.com/notifications/unsubscribe-auth/AADGZ5CBMDIRMRYMVQXC3Y3VRVKHTANCNFSM52HXRSJA
> <https://github.com/notifications/unsubscribe-auth/AADGZ5CBMDIRMRYMVQXC3Y3VRVKHTANCNFSM52HXRSJA>
> . You are receiving this because you were mentioned.Message ID: @.*>
>
> I'm still confused.
> Your total TPUv4 memory is 2048GB(32GBX64 megacore), my total A100 memory
> is 640GB(80GBX8).I'm confused about the memory number one train_step
> compiled can use in your experiment, 32GB or 2048GB?.If it was 32GB, then
> my 80GB is sufficient but why my batch size is smaller then 64, otherwise
> it's not enough.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1187#issuecomment-1170953598>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AADGZ5DRQZ7ZCOXN2B77ZD3VRVOQXANCNFSM52HXRSJA>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",path trained distinct core path need fit memory core core parallel batch data yes sure would fit single machine many machine think assuming core per machine maybe core use memory wrote oh great add function highest batch size reach wrote could try add function device add call call check increase batch size higher would interesting know see error different one tried error memory reduced batch size batch size set error error message batch size allocator ran memory trying allocate rounded execution replica memory trying allocate parameter allocation constant allocation allocation temp allocation temp fragmentation total allocation total fragmentation peak buffer size label shape buffer size label shape reply directly view comment id still confused total memory total memory confused memory number one use experiment sufficient batch size smaller otherwise enough reply directly view id,issue,positive,positive,positive,positive,positive,positive
1272874618,"@IvyTang even if you find version tf_nightly-2.3.0.dev20200515 you will have to use the corresponding version of kws_streaming at github. For example you can pick a version which is close to tf_nightly-2.3.0.dev20200515  at [link](https://github.com/tensorflow/tensorflow/releases/tag/v2.3.1) and kws_streaming version of Oct 23, 2020, e.g. [commit](https://github.com/google-research/google-research/commit/aed055e0e8b635e215dd8c24690db6a162e9d245)). kws_streaming is always at sync with the latest version of TF, so it is better to use TF nightly version. I would suggest to try another model e.g. 'gru' and let me know if it works (there is no need to train it long: train for 1000 iterations and check that it can be converted).",even find version dev use corresponding version example pick version close dev link version commit always sync latest version better use nightly version would suggest try another model let know work need train long train check converted,issue,positive,positive,positive,positive,positive,positive
1272445993,"Same question, it's better to add a C++ example in the code.",question better add example code,issue,negative,positive,positive,positive,positive,positive
1272278874,"> I try to install scann using _`pip install scann`_, it give me the following error:
> 
> _ERROR: Could not find a version that satisfies the requirement scann (from versions: none) ERROR: No matching distribution found for scann_
> 
> Here is package version in my env:
> 
> **tensorflow=2.6.0 tensorflow recommender=v0.5.2 tensorflow dataset=4.4.0**
> 
> It can install the scann on Colab and I used the same env in my local computer, BUT it did not work. Any suggestions are appreciated.

Did you solve this problem?",try install pip install give following error could find version requirement none error matching distribution found package version install used local computer work solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1272274467,"No matching distribution found for scann==1.2.6 (from tflite-model-maker)
I have the same problem, can anyone help me?",matching distribution found problem anyone help,issue,negative,neutral,neutral,neutral,neutral,neutral
1269381356,"@B-rkh-verma after some more Googling, I discovered that the weights are actually available  [here](https://sites.google.com/view/pr-vipe/model-checkpoints).  It would be great if the Pr-VIPE README was updated with this link.",discovered actually available would great link,issue,positive,positive,positive,positive,positive,positive
1269371306,"I doubt that POEM and Pr-VIPE accidentally omitted the checkpoint files, but if this is the case, I too would like to obtain the file(s).",doubt poem accidentally case would like obtain file,issue,negative,neutral,neutral,neutral,neutral,neutral
1259851056,"The issue is due to the default compression behavior of PIL.Image.save(). By disabling the compression of Image.save(), I am able to reproduce the results. ",issue due default compression behavior compression able reproduce,issue,negative,positive,positive,positive,positive,positive
1257242575,"Please unsubscribe me from the mailing list!!!!!
Please!!!!

dependabot[bot] ***@***.***> schrieb am So. 25. Sept. 2022 um
19:39:

> Bumps protobuf <https://github.com/protocolbuffers/protobuf> from 3.11.3
> to 3.18.3.
> Release notes
>
> *Sourced from protobuf's releases
> <https://github.com/protocolbuffers/protobuf/releases>.*
>
> Protocol Buffers v3.18.3 C++
>
>    - Reduce memory consumption of MessageSet parsing
>    - This release addresses a Security Advisory for C++ and Python users
>    <https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-8gq9-2x98-w8hf>
>
> Protocol Buffers v3.16.1 Java
>
>    - Improve performance characteristics of UnknownFieldSet parsing (#9371
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9371>
>    )
>
> Protocol Buffers v3.18.2 Java
>
>    - Improve performance characteristics of UnknownFieldSet parsing (#9371
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9371>
>    )
>
> Protocol Buffers v3.18.1 Python
>
>    - Update setup.py to reflect that we now require at least Python 3.5 (
>    #8989
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8989>
>    )
>    - Performance fix for DynamicMessage: force GetRaw() to be inlined (
>    #9023
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9023>
>    )
>
> Ruby
>
>    - Update ruby_generator.cc to allow proto2 imports in proto3 (#9003
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9003>
>    )
>
> Protocol Buffers v3.18.0 C++
>
>    - Fix warnings raised by clang 11 (#8664
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8664>
>    )
>    - Make StringPiece constructible from std::string_view (#8707
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8707>
>    )
>    - Add missing capability attributes for LLVM 12 (#8714
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8714>
>    )
>    - Stop using std::iterator (deprecated in C++17). (#8741
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8741>
>    )
>    - Move field_access_listener from libprotobuf-lite to libprotobuf (
>    #8775
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8775>
>    )
>    - Fix #7047
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/7047>
>    Safely handle setlocale (#8735
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8735>
>    )
>    - Remove deprecated version of SetTotalBytesLimit() (#8794
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8794>
>    )
>    - Support arena allocation of google::protobuf::AnyMetadata (#8758
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8758>
>    )
>    - Fix undefined symbol error around SharedCtor() (#8827
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8827>
>    )
>    - Fix default value of enum(int) in json_util with proto2 (#8835
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/8835>
>    )
>    - Better Smaller ByteSizeLong
>    - Introduce event filters for inject_field_listener_events
>    - Reduce memory usage of DescriptorPool
>    - For lazy fields copy serialized form when allowed.
>    - Re-introduce the InlinedStringField class
>    - v2 access listener
>    - Reduce padding in the proto's ExtensionRegistry map.
>    - GetExtension performance optimizations
>    - Make tracker a static variable rather than call static functions
>    - Support extensions in field access listener
>    - Annotate MergeFrom for field access listener
>    - Fix incomplete types for field access listener
>    - Add map_entry/new_map_entry to SpecificField in MessageDifferencer.
>    They record the map items which are different in MessageDifferencer's
>    reporter.
>    - Reduce binary size due to fieldless proto messages
>    - TextFormat: ParseInfoTree supports getting field end location in
>    addition to start.
>
> ... (truncated)
> Commits
>
>    - a902b39
>    <https://github.com/protocolbuffers/protobuf/commit/a902b39270841beafc307dfa709610aa1cac2f06>
>    No-op whitespace change
>    - ae62acd
>    <https://github.com/protocolbuffers/protobuf/commit/ae62acd7293e5a80378faeaac67b3baadba810d5>
>    Updating version.json and repo version numbers to: 18.3
>    - f43ac49
>    <https://github.com/protocolbuffers/protobuf/commit/f43ac49b91007501ce1683967b04dcfb47183478>
>    Merge pull request #10542
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/10542>
>    from deannagarcia/3.18.x
>    - 9efdf55
>    <https://github.com/protocolbuffers/protobuf/commit/9efdf55814add154d3c2646652f527a51d4a21ea>
>    Add missing includes
>    - d1635e1
>    <https://github.com/protocolbuffers/protobuf/commit/d1635e1496f51e0d5653d856211e8821bc47adc4>
>    Apply patch
>    - 5b37c91
>    <https://github.com/protocolbuffers/protobuf/commit/5b37c91d62d7fe097253ac64518b993b096e9386>
>    Update version.json with ""lts"": true (#10534
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/10534>
>    )
>    - c39d622
>    <https://github.com/protocolbuffers/protobuf/commit/c39d622dba8343c50876770e6c2cc580781f6264>
>    Merge pull request #10529
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/10529>
>    from protocolbuffers/deannagarcia-patch-5
>    - f77d3b6
>    <https://github.com/protocolbuffers/protobuf/commit/f77d3b643209437f186755737499a841886706cd>
>    Update version.json
>    - 8178b06
>    <https://github.com/protocolbuffers/protobuf/commit/8178b06523f67923f538c479c148af765b09b628>
>    Merge pull request #10503
>    <https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/10503>
>    from deannagarcia/3.18.x
>    - 24ca839
>    <https://github.com/protocolbuffers/protobuf/commit/24ca839fffdd2f795acb26686a312de9892f2c6b>
>    Add version file
>    - Additional commits viewable in compare view
>    <https://github.com/protocolbuffers/protobuf/compare/v3.11.3...v3.18.3>
>
>
> [image: Dependabot compatibility score]
> <https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores>
>
> Dependabot will resolve any conflicts with this PR as long as you don't
> alter it yourself. You can also trigger a rebase manually by commenting @dependabot
> rebase.
> ------------------------------
> Dependabot commands and options
>
> You can trigger Dependabot actions by commenting on this PR:
>
>    - @dependabot rebase will rebase this PR
>    - @dependabot recreate will recreate this PR, overwriting any edits
>    that have been made to it
>    - @dependabot merge will merge this PR after your CI passes on it
>    - @dependabot squash and merge will squash and merge this PR after
>    your CI passes on it
>    - @dependabot cancel merge will cancel a previously requested merge
>    and block automerging
>    - @dependabot reopen will reopen this PR if it is closed
>    - @dependabot close will close this PR and stop Dependabot recreating
>    it. You can achieve the same result by closing it manually
>    - @dependabot ignore this major version will close this PR and stop
>    Dependabot creating any more for this major version (unless you reopen the
>    PR or upgrade to it yourself)
>    - @dependabot ignore this minor version will close this PR and stop
>    Dependabot creating any more for this minor version (unless you reopen the
>    PR or upgrade to it yourself)
>    - @dependabot ignore this dependency will close this PR and stop
>    Dependabot creating any more for this dependency (unless you reopen the PR
>    or upgrade to it yourself)
>    - @dependabot use these labels will set the current labels as the
>    default for future PRs for this repo and language
>    - @dependabot use these reviewers will set the current reviewers as
>    the default for future PRs for this repo and language
>    - @dependabot use these assignees will set the current assignees as
>    the default for future PRs for this repo and language
>    - @dependabot use this milestone will set the current milestone as the
>    default for future PRs for this repo and language
>
> You can disable automated security fix PRs for this repo from the Security
> Alerts page
> <https://github.com/google-research/google-research/network/alerts>.
> ------------------------------
> You can view, comment on, or merge this pull request online at:
>
>   https://github.com/google-research/google-research/pull/1302
> Commit Summary
>
>    - 2ffd933
>    <https://github.com/google-research/google-research/pull/1302/commits/2ffd933c98a674a5c1b71daee7c8f841c2a57190>
>    Bump protobuf from 3.11.3 to 3.18.3 in /cache_replacement
>
> File Changes
>
> (1 file
> <https://github.com/google-research/google-research/pull/1302/files>)
>
>    - *M* cache_replacement/requirements.txt
>    <https://github.com/google-research/google-research/pull/1302/files#diff-e85b94e22a06983a95cb97bb0448087263c75ef74ab12042b93f4ea257c11088>
>    (2)
>
> Patch Links:
>
>    - https://github.com/google-research/google-research/pull/1302.patch
>    - https://github.com/google-research/google-research/pull/1302.diff
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/pull/1302>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AINARVNLNUXO5N57EJJOJMDWACE6NANCNFSM6AAAAAAQVFTUKQ>
> .
> You are receiving this because you are subscribed to this thread.Message
> ID: ***@***.***>
>
",please list please bot um release protocol reduce memory consumption release security advisory python protocol improve performance protocol improve performance protocol python update reflect require least python performance fix force ruby update allow proto proto protocol fix raised clang make stringpiece constructible add missing capability stop move fix safely handle remove version support arena allocation fix undefined symbol error around fix default value proto better smaller introduce event reduce memory usage lazy copy form class access listener reduce padding proto map performance make tracker static variable rather call static support field access listener annotate field access listener fix incomplete field access listener add record map different reporter reduce binary size due proto getting field end location addition start truncated change version merge pull request add missing de apply patch update true merge pull request update merge pull request ca add version file additional viewable compare view image compatibility score resolve long alter also trigger rebase manually rebase trigger rebase rebase recreate recreate made merge merge squash merge squash merge cancel merge cancel previously merge block reopen reopen closed close close stop achieve result manually ignore major version close stop major version unless reopen upgrade ignore minor version close stop minor version unless reopen upgrade ignore dependency close stop dependency unless reopen upgrade use set current default future language use set current default future language use set current default future language use milestone set current milestone default future language disable security fix security page view comment merge pull request commit summary bump file file patch link reply directly view id,issue,positive,positive,neutral,neutral,positive,positive
1253120188,"> So, fixing random seeds in TF is possible but has a significant speed & engineering cost (as you have to switch to deterministic ops) and use libraries like the one you mentioned. That said, you can possibly fix the random seeds of the environment and other things (but GPU randomness would still remain).
> 
> Re evaluation, you can utilize the library in [agarwl.github.io/rliable](https://agarwl.github.io/rliable) (NeurIPS'21 best paper) and simply report aggregate metrics with 95% CIs as they capture the uncertainty in results. See the section on reporting results on DM Control 100k/ 500k in the paper and hopefully you can utilize some of those protocols for reporting results with small uncertainty. Does that help?

Yes, it seems fixing randomness on GPU is hard. And the library, judging from the latest uppdate date, is obviously out-of-date. Thanks for your information! I'll go & check it out.",fixing random possible significant speed engineering cost switch deterministic use like one said possibly fix random environment randomness would still remain evaluation utilize library best paper simply report aggregate metric capture uncertainty see section control paper hopefully utilize small uncertainty help yes fixing randomness hard library latest date obviously thanks information go check,issue,positive,positive,neutral,neutral,positive,positive
1251079996,"So, fixing random seeds in TF is possible but has a significant speed & engineering cost (as you have to switch to deterministic ops) and use libraries like the one you mentioned. That said, you can possibly fix the random seeds of the environment and other things (but GPU randomness would still remain).

Re evaluation, you can utilize the library in [agarwl.github.io/rliable](https://agarwl.github.io/rliable) (NeurIPS'21 best paper) and simply report aggregate metrics with 95% CIs as they capture the uncertainty in results. See the section on reporting results on DM Control 100k/ 500k in the paper and hopefully you can utilize some of those protocols for reporting results with small uncertainty. Does that help?",fixing random possible significant speed engineering cost switch deterministic use like one said possibly fix random environment randomness would still remain evaluation utilize library best paper simply report aggregate metric capture uncertainty see section control paper hopefully utilize small uncertainty help,issue,positive,positive,neutral,neutral,positive,positive
1248801110,"> You, Will, Get Notified When Code is Released, Thank you.

Do you plan to release the code anytime soon?",get notified code thank plan release code soon,issue,negative,neutral,neutral,neutral,neutral,neutral
1248425552,"@sammymax Make sense, I completely understand the efforts maintaining whls cross platform. Do you feel comfortable that we made more explicit steps in the README regarding how to built Scann locally? I think with your house keeping change (removing `hash_set`), the minimum change for folks to build on macos, the `tensorflow dylib` also needs to be updated, and pruned bazel build command.  Lemme know if you think it's helpful, Thanks!",make sense completely understand cross platform feel comfortable made explicit regarding built locally think house keeping change removing minimum change build also need build command know think helpful thanks,issue,positive,positive,positive,positive,positive,positive
1248405304,"You, Will, Get Notified When Code is Released, Thank you.",get notified code thank,issue,negative,neutral,neutral,neutral,neutral,neutral
1248377652,"It's unfortunately a fairly big commitment for us to test and release binaries on MacOS, so we're hesitant to accept this change. Based off this PR though, I removed the `hash_set` includes from the ScaNN codebase in https://github.com/google-research/google-research/commit/06604970e69f9f79a2ff32e07742890bcfc6c268 -- hope this at least helps a bit.",unfortunately fairly big commitment u test release hesitant accept change based though removed hope least bit,issue,positive,negative,negative,negative,negative,negative
1248369998,"ScaNN 1.2.8 was just released, and it was compiled with just `-mavx` rather than `-mavx2`; hopefully that fixes your issue. Granted, I could never reproduce this issue (I tested ScaNN 1.2.7 on non-AVX2 machines and it never crashed) so I'm not sure if this will be sufficient.",rather hopefully issue could never reproduce issue tested never sure sufficient,issue,negative,negative,negative,negative,negative,negative
1245973539,"@sammymax Apologies for pining you directly, but I kind wanna hear your thoughts on this one, have seen a bunch of folks complaining about missing whls for macosx. Lemme know if this one makes sens to you, thanks in advance!",pining directly kind wan na hear one seen bunch missing know one thanks advance,issue,positive,positive,neutral,neutral,positive,positive
1240750460,"Same issue, can't find a solution",issue ca find solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1238850330,"+Chen Liang ***@***.***>


On Fri, Aug 12, 2022 at 10:39 AM Romi Kuntsman ***@***.***>
wrote:

> @3dvg <https://github.com/3dvg>
> please try the fix in my PR here:
> #1222 <https://github.com/google-research/google-research/pull/1222>
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/647#issuecomment-1213358150>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AB56VRQKHAQSQQHOLDX7YTLVY2D4FANCNFSM4ZTGYPAQ>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",liang wrote please try fix reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1236874179,"> Hi, I think i found a bug in regnerf datasets.py row 1131, the code should be in the for-loops?
> 
> https://github.com/google-research/google-research/blob/4808a726f4b126ea38d49cdd152a6bb5d42efdf0/regnerf/internal/datasets.py#L1131

Hi, dii you train regnerf successfully?",hi think found bug row code hi train successfully,issue,negative,positive,positive,positive,positive,positive
1236230139,"There is not special parameter for low-shot learning, but you can play with parameters for spectrogram and audio data augmentation: time_shift_ms, use_spec_augment, use_spec_cutout with theirs settings in [line1](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/speech_features.py#L167) and [line2](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/speech_features.py#L176)",special parameter learning play spectrogram audio data augmentation line line,issue,positive,positive,positive,positive,positive,positive
1231149573,"I'd like to know what you've done，
 What is the version number of Pillow, Keras and TensFlow?  and jax

thank you so much",like know version number pillow thank much,issue,positive,positive,positive,positive,positive,positive
1229535296,"So originally I solved the problem described here: https://github.com/google-research/scenic/issues/213. However, I have also tried with newer versions of jax and scenic (0.5.3) and this error no longer pops up.",originally problem however also tried scenic error longer,issue,negative,positive,positive,positive,positive,positive
1226051748,"Hi, I am sorry for this issue. It is not related to this colab - see https://github.com/googlecolab/colabtools/issues/3009 for a minimal repro.",hi sorry issue related see minimal,issue,negative,negative,negative,negative,negative,negative
1225226186,"Thanks for the update, [restore_checkpoint](https://github.com/google-research/google-research/blob/master/kws_streaming/train/base_parser.py#L479) option saved my life. Is there any low-shot learning parameters approaches can be applied to the current schema?  ",thanks update option saved life learning applied current schema,issue,positive,positive,neutral,neutral,positive,positive
1225171968,"You can use [restore_checkpoint](https://github.com/google-research/google-research/blob/master/kws_streaming/train/base_parser.py#L479) option, so that it will use the latest checkpoint for model initialization and at the same time you can point model training to a new data sets, by specifying [data_dir](https://github.com/google-research/google-research/blob/master/kws_streaming/train/base_parser.py#L40)",use option use latest model time point model training new data,issue,negative,positive,positive,positive,positive,positive
1224238963,"Hey @CatStark, how did you hardcoded this framework on Mac environment?
Can you share this rows from configure.py file?
",hey framework mac environment share file,issue,negative,neutral,neutral,neutral,neutral,neutral
1221251440,"Okay, yes I confirm. It's link to AVX2 support.

AMD FX-8320E Eight-Core Processor didn't support AVX2 but only AVX :-(
```
$ cat /proc/cpuinfo |grep avx
```
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave **`avx`** f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold




",yes confirm link support processor support cat de pat monitor aes,issue,positive,neutral,neutral,neutral,neutral,neutral
1221028656,"My hypothesis (from [here](https://github.com/tensorflow/recommenders/issues/328#issuecomment-1214314586)) is that ScaNN is using AVX2 instructions due to it being compiled with the `-mavx2` flag, and your processor doesn't support AVX2. We will look into compiling the ScaNN wheels the next release without the `-mavx2` flag so that this issue is resolved. You can try compiling ScaNN yourself without that flag in the meantime to see if that fixes the problem.",hypothesis due flag processor support look next release without flag issue resolved try without flag see problem,issue,positive,negative,neutral,neutral,negative,negative
1220793163,"Same problem for me :-(

OS : Linux Mint 21
CPU : AMD FX-8320E Eight-Core Processor
GPU : Geforce1080 Ti
Python : 3.8.5 and 3.10.4
cuda : 11-2
scann : 1.2.7 and 1.2.6

```
$ python3
Python 3.10.4 (main, Jun 29 2022, 12:14:53) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import scann
Segmentation fault (core dumped)
```

```
$ python3
Python 3.8.5 (default, Sep  4 2020, 07:30:14) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import scann
Instruction non permise (core dumped)
```",problem o mint processor ti python python python main type help copyright license information import segmentation fault core python python default anaconda type help copyright license information import instruction non core,issue,negative,positive,positive,positive,positive,positive
1219843694,"Thanks for asking, I agree the description is ambiguous. We'll update the paper and README. Task IDs 11-510 are the 500 test problems. We use 90 problems (511-600) for validation and then remaining 374 for fine-tuning (601-974). The other problems can be used as desired, either for training or few-shot prompting (although this should be specified).

For the edited dataset, yes, you're exactly correct. I've updated the README with these details. It's simply inherited by the subset.",thanks agree description ambiguous update paper task test use validation used desired either training although yes exactly correct simply subset,issue,positive,positive,positive,positive,positive,positive
1219697645,"There's no binary distribution for macos, if you need scann on mac, feel free to try out instructions in this PR: https://github.com/google-research/google-research/pull/830 (Build from source on mac), once you finished all the steps, you are good to go.",binary distribution need mac feel free try build source mac finished good go,issue,positive,positive,positive,positive,positive,positive
1218430544,I'm facing the same problem. Does anyone have a solution? ,facing problem anyone solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1218153031,"Thanks very much for the fix, I've pushed this change and it should be updated shortly. ",thanks much fix change shortly,issue,negative,positive,positive,positive,positive,positive
1217939544,"Also, the changelog shows all these changes in character encodings that are an artifact of my saving this notebook from colab -- these can be ignored (unless you see an issue with this); only the pip install line is actually different.",also character artifact saving notebook unless see issue pip install line actually different,issue,negative,neutral,neutral,neutral,neutral,neutral
1214906832,"> https://github.com/james77777778/metis_python

Had problem with metis as well, was having the following error with the test code:

```
for i in H.nodes:
TypeError: 'method' object is not iterable
```

This helped with that issue as well, thanks a lot!",problem metis well following error test code object iterable issue well thanks lot,issue,negative,positive,neutral,neutral,positive,positive
1214577629,"Same here with 
- `Python 3.9.13`
- `pip-22.2.2`
- `tensorflow==2.9.2`
- `tensorflow-recommenders==0.7.0`

```sh
❯ pip install scann
ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann
```

😢 ",python sh pip install error could find version requirement none error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
1211349929,"Thank you for the clarification.

Rishabh Agarwal ***@***.***> 于2022年8月10日周三 15:26写道：

> Calculating the PSM metric requires the trajectory. If you have access to
> the metric values, you can sample minibatches.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1234#issuecomment-1211167867>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF6FY22ECLYL3YQNOBFCO43VYP65ZANCNFSM56D6KCPA>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",thank clarification agarwal calculating metric trajectory access metric sample reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1211167867,"Calculating the PSM metric requires the trajectory. If you have access to the metric values, you can sample minibatches. ",calculating metric trajectory access metric sample,issue,negative,neutral,neutral,neutral,neutral,neutral
1211156262,"Can I ask another question? Do we have to sample two trajectories whose
(s,s', a,r) are in order? What if we randomly select  (s,s', a,r) from two
tasks? Like batch1 for task1 and batch2 for task2, then calculate the
contrastive loss based on batch1 and batch2.

Rishabh Agarwal ***@***.***> 于2022年8月10日周三 15:13写道：

> The only thing we did in the original work was to find well performing
> trajectories across 2 tasks from a single policy and assumed that some
> states across these two tasks would be similar. Extending beyond that
> requires some additional work (not sure what it entails).
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1234#issuecomment-1211149517>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF6FY262PEP6ML2G22FBR33VYP5MVANCNFSM56D6KCPA>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",ask another question sample two whose order randomly select two like batch task batch task calculate contrastive loss based batch batch agarwal thing original work find well across single policy assumed across two would similar extending beyond additional work sure reply directly view id,issue,positive,positive,neutral,neutral,positive,positive
1211151390,"Thanks for the clarification!

Best,
Jiawei

Rishabh Agarwal ***@***.***> 于2022年8月10日周三 15:13写道：

> The only thing we did in the original work was to find well performing
> trajectories across 2 tasks from a single policy and assumed that some
> states across these two tasks would be similar. Extending beyond that
> requires some additional work (not sure what it entails).
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1234#issuecomment-1211149517>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF6FY262PEP6ML2G22FBR33VYP5MVANCNFSM56D6KCPA>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",thanks clarification best agarwal thing original work find well across single policy assumed across two would similar extending beyond additional work sure reply directly view id,issue,positive,positive,positive,positive,positive,positive
1211149517,The only thing we did in the original work was to find well performing trajectories across 2 tasks from a single policy and assumed that some states across these two tasks would be similar. Extending beyond that requires some additional work (not sure what it entails).,thing original work find well across single policy assumed across two would similar extending beyond additional work sure,issue,positive,positive,positive,positive,positive,positive
1211146447,"Thanks. Then if is there a mechanism to ensure the existence of similar
state pair when selecting the trajectories?

Rishabh Agarwal ***@***.***> 于2022年8月10日周三 15:06写道：

> So, the above approximation was based on the assumption that there states
> in the environment that are behaviorally similar. If this assumption
> doesn't hold, then PSM may not capture the right kind of state similarity.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1234#issuecomment-1211143396>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF6FY22XHDT5HY7ZW3AMQMLVYP4UDANCNFSM56D6KCPA>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",thanks mechanism ensure existence similar state pair agarwal approximation based assumption environment similar assumption hold may capture right kind state similarity reply directly view id,issue,positive,positive,positive,positive,positive,positive
1211143396,"So, the above approximation was based on the assumption that there states across the environments that are behaviorally similar. If this assumption doesn't hold, then PSM may not capture the right kind of state similarity. ",approximation based assumption across similar assumption hold may capture right kind state similarity,issue,positive,positive,positive,positive,positive,positive
1211141031,"Thanks for the kind response!
Sorry, maybe I didn't make it clear. I wonder what if there are no similar
states among the two trajectories at all? In this case, there should be no
positive pairs, but according to PSM, you must determine one positive pair
by selecting the smallest one to calculate the contrastive loss.

Rishabh Agarwal ***@***.***> 于2022年8月10日周三 10:54写道：

> The PSM distance over pairs is computed across all states in two
> trajectories {x}*{i=1 to N} and {y}*^{j=1 to M}, which gives a pairwise
> similarity matrix S, where S_{ij} = PSM (x_i, y_j). For each of the rows /
> columns, we pick the pair with the smallest distance as positive pair while
> rest as negative pair. This pair is valid for a given state in first
> trajectory relative to all states in the other trajectory. This seemed like
> a reasonable enough approximation and worked reasonably.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1234#issuecomment-1210790687>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF6FY2YGEC5BGWEFRHGXVETVYO7BBANCNFSM56D6KCPA>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",thanks kind response sorry maybe make clear wonder similar among two case positive according must determine one positive pair one calculate contrastive loss agarwal distance across two pairwise similarity matrix pick pair distance positive pair rest negative pair pair valid given state first trajectory relative trajectory like reasonable enough approximation worked reasonably reply directly view id,issue,positive,positive,positive,positive,positive,positive
1210790687,"The PSM distance over pairs is computed across all states in two trajectories {x}^{i=1 to N} and {y}_^{j=1 to M}, which gives a pairwise similarity matrix S, where S_{ij} = PSM (x_i, y_j). For each of the rows / columns, we pick the pair with the smallest distance as positive pair while rest as negative pair.  This pair is valid for a given state in first trajectory relative to all states in the other trajectory. This seemed like a reasonable enough approximation and worked well. ",distance across two pairwise similarity matrix pick pair distance positive pair rest negative pair pair valid given state first trajectory relative trajectory like reasonable enough approximation worked well,issue,positive,positive,neutral,neutral,positive,positive
1209999521,"> I updated it. The new link is https://tinyurl.com/coltranfewclicksv2. Let me know if it works.

The colab has the wrong permissions, in the link sharing on Google Drive you need to select ""Everyone with this link"", currently it's set to require email invite",new link let know work wrong link drive need select everyone link currently set require invite,issue,negative,negative,negative,negative,negative,negative
1205614091,"I spun up a Ubuntu 20.04 VM and installed pip packages until I got to a superset of your `pip freeze`; mine is

```
absl-py==1.2.0
astunparse==1.6.3
attrs==19.3.0
Automat==0.8.0
blinker==1.4
cachetools==5.2.0
certifi==2022.6.15
chardet==3.0.4
charset-normalizer==2.1.0
Click==7.0
cloud-init==22.2
colorama==0.4.3
command-not-found==0.3
configobj==5.0.6
constantly==15.1.0
cryptography==2.8
dbus-python==1.2.16
distro==1.4.0
distro-info===0.23ubuntu1
entrypoints==0.3
flatbuffers==2.0
gast==0.5.3
google-auth==2.9.1
google-auth-oauthlib==0.4.6
google-pasta==0.2.0
grpcio==1.47.0
h5py==3.7.0
httplib2==0.14.0
hyperlink==19.0.0
idna==3.3
importlib-metadata==4.12.0
incremental==16.10.1
Jinja2==2.10.1
jsonpatch==1.22
jsonpointer==2.0
jsonschema==3.2.0
keras==2.8.0
Keras-Preprocessing==1.1.2
keyring==18.0.1
language-selector==0.1
launchpadlib==1.10.13
lazr.restfulclient==0.14.2
lazr.uri==1.0.3
libclang==14.0.6
Markdown==3.4.1
MarkupSafe==2.1.1
more-itertools==4.2.0
netifaces==0.10.4
numpy==1.23.1
oauthlib==3.2.0
opt-einsum==3.3.0
pexpect==4.6.0
protobuf==3.19.4
pyarrow==8.0.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
PyGObject==3.36.0
PyHamcrest==1.9.0
PyJWT==1.7.1
pymacaroons==0.13.0
PyNaCl==1.3.0
pyOpenSSL==19.0.0
pyrsistent==0.15.5
pyserial==3.4
python-apt==2.0.0+ubuntu0.20.4.7
python-debian===0.1.36ubuntu1
PyYAML==5.3.1
requests==2.28.1
requests-oauthlib==1.3.1
requests-unixsocket==0.2.0
rsa==4.9
scann==1.2.6
SecretStorage==2.3.1
service-identity==18.1.0
simplejson==3.16.0
six==1.16.0
sos==4.3
ssh-import-id==5.10
systemd-python==234
tensorboard==2.8.0
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.1
tensorflow==2.8.2
tensorflow-estimator==2.8.0
tensorflow-io-gcs-filesystem==0.26.0
termcolor==1.1.0
Twisted==18.9.0
typing-extensions==4.3.0
ubuntu-advantage-tools==27.9
ufw==0.36
unattended-upgrades==0.1
urllib3==1.26.11
wadllib==1.3.3
Werkzeug==2.2.1
wrapt==1.14.1
zipp==3.8.1
zope.interface==4.7.1
```

I wasn't able to reproduce your error, but it seems like you're experiencing something similar to [this](https://github.com/tensorflow/tensorflow/issues/24976). Best of luck with `faiss`.",spun pip got pip freeze mine able reproduce error like something similar best luck,issue,positive,positive,positive,positive,positive,positive
1205611419,I will have to defer to using `faiss` instead.  Would love to use `scann` at some point in the future when it's available.,defer instead would love use point future available,issue,negative,positive,positive,positive,positive,positive
1205611406,"It's unfortunately not possible to build ScaNN without TensorFlow, because we use a number of internal functions provided by the TensorFlow library (for example, TF [Status](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/status.h) and [StatusOr](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/statusor.h)).",unfortunately possible build without use number internal provided library example status,issue,negative,neutral,neutral,neutral,neutral,neutral
1204573506,"It seems that version 1.2.7 is now accessible from `pip` without having to perform a local build.  Unfortunately, I'm still receiving SIGILL when running `import scann`.  When I run my python script with `gdb`:

```bash
gdb python3
...
run src/hivemind/server.py
```

I get the following backtrace once the SIGILL occurs:

```
0x00007fff9b0ca10a in google::protobuf::FieldDescriptorProto::FieldDescriptorProto() ()
--Type <RET> for more, q to quit, c to continue without paging--
from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
(gdb) bt
#0  0x00007fff9b0ca10a in google::protobuf::FieldDescriptorProto::FieldDescriptorProto() ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#1  0x00007fff9b0bc2fb in InitDefaultsscc_info_FieldDescriptorProto_google_2fprotobuf_2fdescriptor_2eproto() ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#2  0x00007fff9b153c45 in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) [clone .llvm.10051976107214125249] ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#3  0x00007fff9b153c3b in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) [clone .llvm.10051976107214125249] ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
```",version accessible pip without perform local build unfortunately still running import run python script bash python run get following type ret quit continue without anonymous clone anonymous clone,issue,negative,negative,neutral,neutral,negative,negative
1204431568,**Please note:** This issue persists with both `pip install scann` and a local build of `scann`.,please note issue pip install local build,issue,negative,neutral,neutral,neutral,neutral,neutral
1203403955,"I attempted to get this working with a local build of `scann`, but I am running into the same issues locally.

You can find the build steps I had to take, including downgrading `bazel` to 3.7.2 due to protobuf issues: https://github.com/toddwildey/google-research/blob/6df480615340a4f7e15a2db12ea06e56f8d68a31/scann/build.sh

I also had to install this package globally on my machine, and then install it locally in my virtual environment:

In `google-research/scann`:

```bash
python3 setup.py install
```

In my project:

```bash
#!/bin/bash

[[ ! -d .venv ]] && python -m venv .venv

USER_PYTHON_LIB_PATH=""$HOME/.pyenv/versions/3.8.6/lib/python3.8/site-packages""

find ""$USER_PYTHON_LIB_PATH"" -name ""scann*"" -maxdepth 1 \
    | xargs -I{} cp -Rpf {} .venv/lib/python3.8/site-packages

source .venv/bin/activate

pip3 install -r requirements.txt
```

The `protobuf` issues I encountered with `bazel` 5.2.0 are documented here: https://github.com/bazelbuild/bazel/issues/12887",get working local build running locally find build take due also install package globally machine install locally virtual environment bash python install project bash python find source pip install,issue,negative,negative,neutral,neutral,negative,negative
1203295693,"Glad you figured it out! Essentially the layer ""SimpleModel/OutputLogits/C0/embedding_stddevs/bias"" should only exist for Pr-VIPE, but you were using a L2-VIPE checkpoint. L2-VIPE checkpoints do not include this layer, since the embeddings don't have standard deviations.

Let us know if there's anything else!",glad figured essentially layer exist include layer since standard let u know anything else,issue,negative,positive,positive,positive,positive,positive
1201355156,It turns out that I need to check all_flag.json and setting the correct flag in infer.py,turn need check setting correct flag,issue,negative,neutral,neutral,neutral,neutral,neutral
1192990858,OK I've verified that 0.1.2 works in colab. Today I learned that colab uses python 3.7.,work today learned python,issue,negative,neutral,neutral,neutral,neutral,neutral
1192267130,"The same problem in our CI on Hugging Face Datasets: https://github.com/huggingface/datasets/runs/7463218591?check_suite_focus=true

The package seems not importable:
```
>   from rouge_score import rouge_scorer
E   ModuleNotFoundError: No module named 'rouge_score'
```
despite having been installed:
```
 Collecting rouge_score
  Downloading rouge_score-0.0.7.tar.gz (3.3 kB)

    Running setup.py install for rouge-score: started
    Running setup.py install for rouge-score: finished with status 'done'


Successfully installed: … rouge-score-0.0.7 …
```",problem hugging face package importable import module despite running install running install finished status successfully,issue,positive,positive,positive,positive,positive,positive
1192039773,"@braveheartwithlove ""--preprocess ""mfcc"""" option uses [custom audio ops](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/speech_features.py#L225) which is non streamable, so if you need to use it then you have to decopule you model from preprocessor, so that you will manage preprocessing on your own and stream the rest of the model which will take mfcc features as input (you will need to modify the model inputs). I would suggest not to use any preprocessing and feed raw audio, this way you can train and stream your model end to end: ""--preprocess ""raw""""",option custom audio non need use model manage stream rest model take input need modify model would suggest use feed raw audio way train stream model end end raw,issue,negative,negative,negative,negative,negative,negative
1191770707,"@gariel-google 
Where can i find the euroc the euroc training dataset used in this paper ?",find training used paper,issue,negative,neutral,neutral,neutral,neutral,neutral
1191102537,"The command I used:
python -m kws_streaming.train.model_train_eval 
--data_dir ./data/ 
--train_dir ./models/ds_tc_resnet/ 
--mel_upper_edge_hertz 7600 
--feature_type ""mfcc_tf"" 
--preprocess ""mfcc""  
--causal_data_frame_padding 0 
--how_many_training_steps 1000,1000,500,500   
--learning_rate 0.001,0.0005,0.0002,0.0001 
--window_size_ms 30.0 
--window_stride_ms 10.0 
--mel_num_bins 40 
--dct_num_features 40 
--resample 0.15 
--alsologtostderr 
--train 1 
--time_shift_ms 100 
--use_spec_augment 1 
--time_masks_number 2 
--time_mask_max_size 25 
--frequency_masks_number 2 
--frequency_mask_max_size 7 
--pick_deterministically 1 
--unknown_percentage 200 
--silence_percentage 20 
--clip_duration_ms 1000 
--wanted_words yes 
ds_tc_resnet 
--dropout 0.0 
--ds_filters 128,64,64,64,128,128 
--ds_filter_separable 1,1,1,1,1,1 
--ds_repeat 1,1,1,1,1,1 
--ds_residual 0,1,1,1,0,0 
--ds_kernel_size 11,5,15,17,15,1 
--ds_stride 1,1,1,1,1,1 
--ds_dilation 1,1,1,1,2,1 
--ds_pool 1,2,1,1,1,1 
--activation ""relu"" 
--ds_padding ""'causal','causal','causal','causal','causal','causal'""",command used python resample train yes dropout activation,issue,negative,neutral,neutral,neutral,neutral,neutral
1188549465,"Hi! It depends on which model you are using - if you are using the Pr-VIPE models we released, then there's no need to normalize",hi model need normalize,issue,negative,neutral,neutral,neutral,neutral,neutral
1188408494,"@junjiek 

Thanks,  I think that is the problem. I am trying to now figure out how to tile the grayscale image to 3 channels.

Tim",thanks think problem trying figure tile image,issue,negative,positive,positive,positive,positive,positive
1188393952,"Currently the code only handles RGB images with 3 channels, you can tile the gray scale image to 3 channels if you want to handle it. ",currently code tile gray scale image want handle,issue,negative,neutral,neutral,neutral,neutral,neutral
1188356847,"@junjiek 

I think my problem may be the image mode which is 'L' meaning it is gray scale and not RGB.
",think problem may image mode meaning gray scale,issue,negative,neutral,neutral,neutral,neutral,neutral
1188324033,"@junjiek 

I just saw this note in the code. 
""We only handle 3-channel images for now.""

I wonder if the images I have are 4 channel or perhaps 1 channel.  I'll explore that next.

Tim",saw note code handle wonder channel perhaps channel explore next,issue,negative,neutral,neutral,neutral,neutral,neutral
1188320832,"@junjiek 

Thanks for getting back to me.  If I shared an image with you that fails would that help?
![2645084](https://user-images.githubusercontent.com/2401628/179618968-98930dbd-e80a-43a3-ad70-0fd714248414.jpeg)

",thanks getting back image would help,issue,positive,positive,neutral,neutral,positive,positive
1188297988,"Sorry I cannot reproduce your error. I tried putting `image = tf.ones([1, 728, 2048, 3]` before [prepreocessing.py#L336](https://github.com/google-research/google-research/blob/master/musiq/model/preprocessing.py#L336) but the program finished successfully. ",sorry reproduce error tried image program finished successfully,issue,negative,positive,positive,positive,positive,positive
1186970744,"
@yuan0101213 , Hi I want to ask how we can give our own keypoints to the pr-vipe model?

",yuan hi want ask give model,issue,negative,neutral,neutral,neutral,neutral,neutral
1186964327,"@yuan0101213 Hi, I have one question how we can give our known 2D keypoints to the pr-vipe model.",yuan hi one question give known model,issue,negative,neutral,neutral,neutral,neutral,neutral
1186507524,"@MechCoder coud you please make the colab notebook public, I think it currently has wrong permissions and comes up with the following message when trying to open the notebook.

_You need access
Ask for access, or switch to an account with access._ ",please make notebook public think currently wrong come following message trying open notebook need access ask access switch account,issue,negative,negative,negative,negative,negative,negative
1185047085,"> I met the same problem and I solved it by slightly modifying the original metis wrapper to support the lastest networkx. You can check my repo: https://github.com/james77777778/metis_python

Thanks a lot! It works for me.",met problem slightly original metis wrapper support check thanks lot work,issue,positive,positive,positive,positive,positive,positive
1184146727,"Hi, As I am using pr-vibe and we are getting an embedding file of the 13 body key points so I have a question that how we are calculating the embeddings file in embedding CSV we are getting 16 columns so my question is how they are representing the 13 body keypoint.",hi getting file body key question calculating file getting question body,issue,negative,neutral,neutral,neutral,neutral,neutral
1184135739,"Indeed, a release would be much appreciated!",indeed release would much,issue,negative,positive,positive,positive,positive,positive
1181408128,"In our case, we had to install metaworld, d4rl and dm-acme[jax, tensorflow] after all the other packages to get rid of some weird package issues. Another great news is that it also works on ubuntu 20.04. Thx for updating everything so efficiently and marvellous work!",case install get rid weird package another great news also work everything efficiently work,issue,positive,positive,positive,positive,positive,positive
1181333677,"> OK, spent some more time looking into this. I think the root cause is a missing `tensorflow` dependency for `dm-launchpad` (note that all the code in the contrastive-rl repository is written in JAX, not TensorFlow). Can you try following the instructions above again, using this updated [`requirements.txt`](https://github.com/google-research/google-research/files/9041874/requirements.3.txt) file?

Well it did work. Thank you so much!",spent time looking think root cause missing dependency note code repository written try following file well work thank much,issue,negative,negative,neutral,neutral,negative,negative
1180804886,"Hi, I followed the same steps and I'm getting an error. Clean ubuntu literally the same steps as above. I get the following (attached image) 
![image](https://user-images.githubusercontent.com/32895945/178346946-fbf18bd1-7f5c-419e-a0bc-1b8c2fadde22.png)
",hi getting error clean literally get following attached image image,issue,negative,positive,positive,positive,positive,positive
1180318277,"Hi @sML-90 ,

Can you tell me if you manage to figure it out. If so, can you tell me the right steps that you follow to get the code embeddings from CuBERT ??

Thanks in advance!",hi tell manage figure tell right follow get code thanks advance,issue,negative,positive,positive,positive,positive,positive
1179233628,"I have identified the source of the download issue. In the time between when we first published the manuscript and now, there has been an organizational policy change that affected public access to data in the location where we stored the data. I am working on a different hosting solution right now, and I will be in touch ASAP.",source issue time first manuscript organizational policy change affected public access data location data working different hosting solution right touch,issue,negative,positive,positive,positive,positive,positive
1179208107,"Hi @nicain! I appreciate your quick response. That would be wonderful, thank you! ",hi appreciate quick response would wonderful thank,issue,positive,positive,positive,positive,positive,positive
1179191133,Hello @shumanpng! Thank you for your interest in this project. I will get started on addressing your issue. ,hello thank interest project get issue,issue,positive,neutral,neutral,neutral,neutral,neutral
1176044694,"I have the same issue when running on our own server. 
Full stacktrace looks like this:
```
Traceback (most recent call last):
  File ""run.py"", line 136, in <module>
    run(train=lib.run_train, eval=lib.run_eval)
  File ""run.py"", line 132, in run
    app.run(functools.partial(main, executable_dict))
  File ""/home/jovyan/.conda_envs/dreamfieldsgg/lib/python3.7/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/home/jovyan/.conda_envs/dreamfieldsgg/lib/python3.7/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""run.py"", line 124, in main
    **extra_args)
  File ""/home/jovyan/alz/dreamfields_gg/gh/dreamfields/dreamfields/lib.py"", line 809, in run_train
    yield_results=False):
  File ""/home/jovyan/alz/dreamfields_gg/gh/dreamfields/dreamfields/lib.py"", line 426, in run_train
    state, rays_in, keys_pstep, lrs, scs, sns, mrs, betas, accts, acclams)
  File ""/home/jovyan/alz/dreamfields_gg/gh/dreamfields/dreamfields/lib.py"", line 220, in train_step
    state, aux = body_fn(state, np.squeeze(multistep_constants))
  File ""/home/jovyan/alz/dreamfields_gg/gh/dreamfields/dreamfields/lib.py"", line 209, in body_fn
    (_, aux), grad = grad_fn(state.target, grad_fn_key, *step_constants)
  File ""/home/jovyan/alz/dreamfields_gg/gh/dreamfields/dreamfields/lib.py"", line 151, in loss_fn
    augs = augment.augment_rendering(config, rgb_est, acc_est, aug_key)
  File ""/home/jovyan/alz/dreamfields_gg/gh/dreamfields/dreamfields/augment.py"", line 144, in augment_rendering
    return jax.vmap(random_aug)(random.split(key, config.n_local_aug))
  File ""/home/jovyan/alz/dreamfields_gg/gh/dreamfields/dreamfields/augment.py"", line 139, in random_aug
    bg = random_bg(bg_key)
  File ""/home/jovyan/alz/dreamfields_gg/gh/dreamfields/dreamfields/augment.py"", line 96, in random_bg
    bg = random.choice(bg_sel_key, bgs, p=np.array(probs))
  File ""/home/jovyan/.conda_envs/dreamfieldsgg/lib/python3.7/site-packages/jax/_src/random.py"", line 459, in choice
    raise ValueError(""a must be an integer or 1-dimensional"")
ValueError: a must be an integer or 1-dimensional
```",issue running server full like recent call last file line module run file line run main file line run main file line main file line main file line file line state file line state state file line grad file line file line return key file line file line file line choice raise must integer must integer,issue,negative,positive,positive,positive,positive,positive
1175533926,"any other idea? I'm trying to compile a project with c++20, so I need a more recent bazel version.",idea trying compile project need recent version,issue,negative,neutral,neutral,neutral,neutral,neutral
1174272536,"OK, spent some more time looking into this. I think the root cause is a missing `tensorflow` dependency for `dm-launchpad` (note that all the code in the contrastive-rl repository is written in JAX, not TensorFlow). Can you try following the instructions above again, using this updated [`requirements.txt`](https://github.com/google-research/google-research/files/9041874/requirements.3.txt) file?",spent time looking think root cause missing dependency note code repository written try following file,issue,negative,negative,negative,negative,negative,negative
1173639175,"> oh sorry I think I mis-read you last comment. you are saying that you don't see a positive effect from ""delete_live_buffers"" right? (so it is not needed).



> oh sorry I think I mis-read you last comment. you are saying that you don't see a positive effect from ""delete_live_buffers"" right? (so it is not needed).

It's not need, i think.",oh sorry think last comment saying see positive effect right oh sorry think last comment saying see positive effect right need think,issue,positive,positive,neutral,neutral,positive,positive
1173582948,"oh sorry I think I mis-read you last comment.
you are saying that you don't see a positive effect from ""delete_live_buffers"" right? (so it is not needed).",oh sorry think last comment saying see positive effect right,issue,positive,positive,neutral,neutral,positive,positive
1173576441,"great! thank you for confirming!
I have committed the change.",great thank confirming change,issue,positive,positive,positive,positive,positive,positive
1173460311,"> 



> regarding the def ""delete_live_buffers()"" method. could you confirm that it helps reducing the memory usage? in the case I'll commit that change also to the published code. Thanks! On Thu, Jun 30, 2022 at 11:28 AM Andrea Gesmundo ***@***.***> wrote:
> […](#)
> Each sampled path is trained on a distinct core, so all the parameters and activations of a path need to fit in the memory of 1 ""core"". Each core trains in parallel on the same batch of data. FYI the TPUv4 experiments were using 32 megacores each having ~32G (not 64) Yes I'm not sure why it wouldn't fit in 80G if it fits on 32G in my experiments A single A100 machine has 80G but how many cores are on 1 machine? (I think we are assuming there is only 1 core per machine but maybe it is more, e.g. if there are 4 cores then each core can use only 80/4=20G of memory) On Thu, Jun 30, 2022 at 10:57 AM adamantboy ***@***.***> wrote: > oh great, then I'll add that function in the colab. What's the highest > batch size you can reach now? > … <#m_9119296503949766251_m_6047855296256924580_> > On Thu, Jun 30, 2022 at 10:21 AM adamantboy *@*.***> wrote: Could you > try to add this function to the colab def delete_live_buffers(): for device > in jax.local_devices(): lbs = device.live_buffers() for lb in lbs: > lb.delete() and add a call to it just before each call to gc.collect() And > check if this allows you to increase the batch size to 4 or higher. If not > would be interesting to know if you see the same error or a different one. > I tried it. The error is same, but the memory preallocated is reduced to > 70-90GB when the batch size is 64 indeed.And the batch size can be set to 4 > with no OOM error. - Error message, batch size 64: 2022-06-30 > 15:50:02.179408: W > external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:479] > Allocator (GPU_3_bfc) ran out of memory trying to allocate 74.65GiB > (rounded to 80159329536)requested by op 2022-06-30 15:50:06.549646: W > external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:491] > *_________________________________________________________________________________________________ > 2022-06-30 15:50:33.277741: E > external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2141] > Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while > trying to allocate 80159329472 bytes. BufferAssignment OOM Debugging. > BufferAssignment stats: parameter allocation: 1.33GiB constant allocation: > 8B maybe_live_out allocation: 288.38MiB preallocated temp allocation: > 74.65GiB preallocated temp fragmentation: 212B (0.00%) total allocation: > 75.98GiB total fragmentation: 19.2KiB (0.00%) Peak buffers: Buffer 1: Size: > 1.27GiB XLA Label: custom-call Shape: f32[64,16,577,577] > ========================== Buffer 2: Size: 1.27GiB XLA Label: custom-call > Shape: f32[64,16,577,577] ========================== — Reply to this email > directly, view it on GitHub <#1187 (comment) > <[#1187 (comment)](https://github.com/google-research/google-research/issues/1187#issuecomment-1170915826)>>, > or unsubscribe > https://github.com/notifications/unsubscribe-auth/AADGZ5CBMDIRMRYMVQXC3Y3VRVKHTANCNFSM52HXRSJA > <https://github.com/notifications/unsubscribe-auth/AADGZ5CBMDIRMRYMVQXC3Y3VRVKHTANCNFSM52HXRSJA> > . You are receiving this because you were mentioned.Message ID: @.*> > > I'm still confused. > Your total TPUv4 memory is 2048GB(32GBX64 megacore), my total A100 memory > is 640GB(80GBX8).I'm confused about the memory number one train_step > compiled can use in your experiment, 32GB or 2048GB?.If it was 32GB, then > my 80GB is sufficient but why my batch size is smaller then 64, otherwise > it's not enough. > > — > Reply to this email directly, view it on GitHub > <[#1187 (comment)](https://github.com/google-research/google-research/issues/1187#issuecomment-1170953598)>, > or unsubscribe > <https://github.com/notifications/unsubscribe-auth/AADGZ5DRQZ7ZCOXN2B77ZD3VRVOQXANCNFSM52HXRSJA> > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >

I trained it again with batch size 32 and no delete_live_buffers function, it could run too.",regarding method could confirm reducing memory usage case commit change also code thanks wrote path trained distinct core path need fit memory core core parallel batch data yes sure would fit single machine many machine think assuming core per machine maybe core use memory wrote oh great add function highest batch size reach wrote could try add function device add call call check increase batch size higher would interesting know see error different one tried error memory reduced batch size batch size set error error message batch size allocator ran memory trying allocate rounded execution replica memory trying allocate parameter allocation constant allocation allocation temp allocation temp fragmentation total allocation total fragmentation peak buffer size label shape buffer size label shape reply directly view comment comment id still confused total memory total memory confused memory number one use experiment sufficient batch size smaller otherwise enough reply directly view comment id trained batch size function could run,issue,positive,positive,positive,positive,positive,positive
1173084703,"> After some more hours, the installation fails:
> 
> ```
> ERROR: Exception:
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py"", line 180, in _main
>     status = self.run(options, args)
>   File ""/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py"", line 199, in wrapper
>     return func(self, options, args)
>   File ""/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py"", line 319, in run
>     reqs, check_supported_wheels=not options.target_dir
>   File ""/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 128, in resolve
>     requirements, max_rounds=try_to_avoid_resolution_too_deep
>   File ""/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py"", line 473, in resolve
>     state = resolution.resolve(requirements, max_rounds=max_rounds)
>   File ""/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py"", line 384, in resolve
>     raise ResolutionTooDeep(max_rounds)
> pip._vendor.resolvelib.resolvers.ResolutionTooDeep: 2000000
> ```

This is caused by pip's dependency resolver. See [pip-install-rasa-x-takes-forever](https://stackoverflow.com/questions/65806524/pip-install-rasa-x-takes-forever).
I had the same error with pip 21.1.3, I upgraded to the latest version as of writing this, 22.1.3 by running
``pip install --upgrade pip`` which fixed the error.",installation error exception recent call last file line status file line wrapper return self file line run file line resolve file line resolve state file line resolve raise pip dependency resolver see error pip latest version writing running pip install upgrade pip fixed error,issue,negative,positive,positive,positive,positive,positive
1173066229,"> Hmm. This seems like a problem with some of the dependencies. Can you try creating a new Conda environment using these alternative instructions?
> 
> * Delete the old environment: `conda env remove -n contrastive_rl`
> * Download [`requirements.txt`](https://github.com/google-research/google-research/files/9033178/requirements.txt)
> * `conda create -n contrastive_rl python=3.9 -y`
> * `conda activate contrastive_rl`
> * `pip install -r requirements.txt --no-deps`

Thanks for your reply. I reinstalled the conda environment following your instructions, but still the same problem... ",like problem try new environment alternative delete old environment remove create activate pip install thanks reply environment following still problem,issue,negative,positive,positive,positive,positive,positive
1172912347,"Hmm. This seems like a problem with some of the dependencies. Can you try creating a new Conda environment using these alternative instructions?

* Delete the old environment: `conda env remove -n contrastive_rl`
* Download [`requirements.txt`](https://github.com/google-research/google-research/files/9033178/requirements.txt)
* `conda create -n contrastive_rl python=3.9 -y`
* `conda activate contrastive_rl`
* `pip install -r requirements.txt --no-deps`",like problem try new environment alternative delete old environment remove create activate pip install,issue,negative,positive,positive,positive,positive,positive
1172668908,"Also FYI
Google can give free access to TPUs for research if you are interested.
You can find more information and apply here in the case you are interested:
https://sites.research.google/trc/about/
I'm not very familiar with the process but let me know if I can help.


On Fri, Jul 1, 2022 at 9:47 PM Andrea Gesmundo ***@***.***> wrote:

> regarding the def ""delete_live_buffers()"" method.
> could you confirm that it helps reducing the memory usage?
> in the case I'll commit that change also to the published code.
> Thanks!
>
> On Thu, Jun 30, 2022 at 11:28 AM Andrea Gesmundo ***@***.***>
> wrote:
>
>> Each sampled path is trained on a distinct core, so all the parameters
>> and activations of a path need to fit in the memory of 1 ""core"".
>> Each core trains in parallel on the same batch of data.
>> FYI the TPUv4 experiments were using 32 megacores each having ~32G (not
>> 64)
>> Yes I'm not sure why it wouldn't fit in 80G if it fits on 32G in my
>> experiments
>> A single A100 machine has 80G but how many cores are on 1 machine? (I
>> think we are assuming there is only 1 core per machine but maybe it is
>> more, e.g. if there are 4 cores then each core can use only 80/4=20G of
>> memory)
>>
>>
>> On Thu, Jun 30, 2022 at 10:57 AM adamantboy ***@***.***>
>> wrote:
>>
>>> oh great, then I'll add that function in the colab. What's the highest
>>> batch size you can reach now?
>>> … <#m_-1470306883118412883_m_9119296503949766251_m_6047855296256924580_>
>>> On Thu, Jun 30, 2022 at 10:21 AM adamantboy *@*.***> wrote: Could you
>>> try to add this function to the colab def delete_live_buffers(): for device
>>> in jax.local_devices(): lbs = device.live_buffers() for lb in lbs:
>>> lb.delete() and add a call to it just before each call to gc.collect() And
>>> check if this allows you to increase the batch size to 4 or higher. If not
>>> would be interesting to know if you see the same error or a different one.
>>> I tried it. The error is same, but the memory preallocated is reduced to
>>> 70-90GB when the batch size is 64 indeed.And the batch size can be set to 4
>>> with no OOM error. - Error message, batch size 64: 2022-06-30
>>> 15:50:02.179408: W
>>> external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:479]
>>> Allocator (GPU_3_bfc) ran out of memory trying to allocate 74.65GiB
>>> (rounded to 80159329536)requested by op 2022-06-30 15:50:06.549646: W
>>> external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:491]
>>> *_________________________________________________________________________________________________
>>> 2022-06-30 15:50:33.277741: E
>>> external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2141]
>>> Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while
>>> trying to allocate 80159329472 bytes. BufferAssignment OOM Debugging.
>>> BufferAssignment stats: parameter allocation: 1.33GiB constant allocation:
>>> 8B maybe_live_out allocation: 288.38MiB preallocated temp allocation:
>>> 74.65GiB preallocated temp fragmentation: 212B (0.00%) total allocation:
>>> 75.98GiB total fragmentation: 19.2KiB (0.00%) Peak buffers: Buffer 1: Size:
>>> 1.27GiB XLA Label: custom-call Shape: f32[64,16,577,577]
>>> ========================== Buffer 2: Size: 1.27GiB XLA Label: custom-call
>>> Shape: f32[64,16,577,577] ========================== — Reply to this email
>>> directly, view it on GitHub <#1187 (comment)
>>> <https://github.com/google-research/google-research/issues/1187#issuecomment-1170915826>>,
>>> or unsubscribe
>>> https://github.com/notifications/unsubscribe-auth/AADGZ5CBMDIRMRYMVQXC3Y3VRVKHTANCNFSM52HXRSJA
>>> <https://github.com/notifications/unsubscribe-auth/AADGZ5CBMDIRMRYMVQXC3Y3VRVKHTANCNFSM52HXRSJA>
>>> . You are receiving this because you were mentioned.Message ID: @.*>
>>>
>>> I'm still confused.
>>> Your total TPUv4 memory is 2048GB(32GBX64 megacore), my total A100
>>> memory is 640GB(80GBX8).I'm confused about the memory number one train_step
>>> compiled can use in your experiment, 32GB or 2048GB?.If it was 32GB, then
>>> my 80GB is sufficient but why my batch size is smaller then 64, otherwise
>>> it's not enough.
>>>
>>> —
>>> Reply to this email directly, view it on GitHub
>>> <https://github.com/google-research/google-research/issues/1187#issuecomment-1170953598>,
>>> or unsubscribe
>>> <https://github.com/notifications/unsubscribe-auth/AADGZ5DRQZ7ZCOXN2B77ZD3VRVOQXANCNFSM52HXRSJA>
>>> .
>>> You are receiving this because you were mentioned.Message ID:
>>> ***@***.***>
>>>
>>
",also give free access research interested find information apply case interested familiar process let know help wrote regarding method could confirm reducing memory usage case commit change also code thanks wrote path trained distinct core path need fit memory core core parallel batch data yes sure would fit single machine many machine think assuming core per machine maybe core use memory wrote oh great add function highest batch size reach wrote could try add function device add call call check increase batch size higher would interesting know see error different one tried error memory reduced batch size batch size set error error message batch size allocator ran memory trying allocate rounded execution replica memory trying allocate parameter allocation constant allocation allocation temp allocation temp fragmentation total allocation total fragmentation peak buffer size label shape buffer size label shape reply directly view comment id still confused total memory total memory confused memory number one use experiment sufficient batch size smaller otherwise enough reply directly view id,issue,positive,positive,positive,positive,positive,positive
1172666350,"regarding the def ""delete_live_buffers()"" method.
could you confirm that it helps reducing the memory usage?
in the case I'll commit that change also to the published code.
Thanks!

On Thu, Jun 30, 2022 at 11:28 AM Andrea Gesmundo ***@***.***>
wrote:

> Each sampled path is trained on a distinct core, so all the parameters and
> activations of a path need to fit in the memory of 1 ""core"".
> Each core trains in parallel on the same batch of data.
> FYI the TPUv4 experiments were using 32 megacores each having ~32G (not 64)
> Yes I'm not sure why it wouldn't fit in 80G if it fits on 32G in my
> experiments
> A single A100 machine has 80G but how many cores are on 1 machine? (I
> think we are assuming there is only 1 core per machine but maybe it is
> more, e.g. if there are 4 cores then each core can use only 80/4=20G of
> memory)
>
>
> On Thu, Jun 30, 2022 at 10:57 AM adamantboy ***@***.***>
> wrote:
>
>> oh great, then I'll add that function in the colab. What's the highest
>> batch size you can reach now?
>> … <#m_9119296503949766251_m_6047855296256924580_>
>> On Thu, Jun 30, 2022 at 10:21 AM adamantboy *@*.***> wrote: Could you
>> try to add this function to the colab def delete_live_buffers(): for device
>> in jax.local_devices(): lbs = device.live_buffers() for lb in lbs:
>> lb.delete() and add a call to it just before each call to gc.collect() And
>> check if this allows you to increase the batch size to 4 or higher. If not
>> would be interesting to know if you see the same error or a different one.
>> I tried it. The error is same, but the memory preallocated is reduced to
>> 70-90GB when the batch size is 64 indeed.And the batch size can be set to 4
>> with no OOM error. - Error message, batch size 64: 2022-06-30
>> 15:50:02.179408: W
>> external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:479]
>> Allocator (GPU_3_bfc) ran out of memory trying to allocate 74.65GiB
>> (rounded to 80159329536)requested by op 2022-06-30 15:50:06.549646: W
>> external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:491]
>> *_________________________________________________________________________________________________
>> 2022-06-30 15:50:33.277741: E
>> external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2141]
>> Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while
>> trying to allocate 80159329472 bytes. BufferAssignment OOM Debugging.
>> BufferAssignment stats: parameter allocation: 1.33GiB constant allocation:
>> 8B maybe_live_out allocation: 288.38MiB preallocated temp allocation:
>> 74.65GiB preallocated temp fragmentation: 212B (0.00%) total allocation:
>> 75.98GiB total fragmentation: 19.2KiB (0.00%) Peak buffers: Buffer 1: Size:
>> 1.27GiB XLA Label: custom-call Shape: f32[64,16,577,577]
>> ========================== Buffer 2: Size: 1.27GiB XLA Label: custom-call
>> Shape: f32[64,16,577,577] ========================== — Reply to this email
>> directly, view it on GitHub <#1187 (comment)
>> <https://github.com/google-research/google-research/issues/1187#issuecomment-1170915826>>,
>> or unsubscribe
>> https://github.com/notifications/unsubscribe-auth/AADGZ5CBMDIRMRYMVQXC3Y3VRVKHTANCNFSM52HXRSJA
>> <https://github.com/notifications/unsubscribe-auth/AADGZ5CBMDIRMRYMVQXC3Y3VRVKHTANCNFSM52HXRSJA>
>> . You are receiving this because you were mentioned.Message ID: @.*>
>>
>> I'm still confused.
>> Your total TPUv4 memory is 2048GB(32GBX64 megacore), my total A100 memory
>> is 640GB(80GBX8).I'm confused about the memory number one train_step
>> compiled can use in your experiment, 32GB or 2048GB?.If it was 32GB, then
>> my 80GB is sufficient but why my batch size is smaller then 64, otherwise
>> it's not enough.
>>
>> —
>> Reply to this email directly, view it on GitHub
>> <https://github.com/google-research/google-research/issues/1187#issuecomment-1170953598>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/AADGZ5DRQZ7ZCOXN2B77ZD3VRVOQXANCNFSM52HXRSJA>
>> .
>> You are receiving this because you were mentioned.Message ID:
>> ***@***.***>
>>
>
",regarding method could confirm reducing memory usage case commit change also code thanks wrote path trained distinct core path need fit memory core core parallel batch data yes sure would fit single machine many machine think assuming core per machine maybe core use memory wrote oh great add function highest batch size reach wrote could try add function device add call call check increase batch size higher would interesting know see error different one tried error memory reduced batch size batch size set error error message batch size allocator ran memory trying allocate rounded execution replica memory trying allocate parameter allocation constant allocation allocation temp allocation temp fragmentation total allocation total fragmentation peak buffer size label shape buffer size label shape reply directly view comment id still confused total memory total memory confused memory number one use experiment sufficient batch size smaller otherwise enough reply directly view id,issue,positive,positive,positive,positive,positive,positive
1171874067,"> Hi there! I don't know what the people from the Bart paper are reporting but as to low, mid and high, you can see that they are just percentiles:
> 
> https://github.com/google-research/google-research/blob/478efa18a3a1fef144fac03fcd034829f348794e/rouge/scoring.py#L127
> 
> ```
>       intervals = tuple(
>           (scores[0].__class__(*percentiles[j, :]) for j in range(3)))
>       result[score_type] = AggregateScore(
>           low=intervals[0], mid=intervals[1], high=intervals[2])
> ```
> 
> ```
> Mid is always the mean, while low and high
> bounds are specified by self._confidence_interval (which defaults to 0.95
> meaning it will return the 2.5th and 97.5th percentiles for a 95%
> confidence interval on the mean).
> ```
> 
> so basically in `q = 100 * np.array([percentile_delta, 0.5, 1 - percentile_delta])`
> 
> * low is the percentile_delta percentile
> * mid is the 0.5 percentile
> * high is the 1 - percentile_delta percentile
> 
> and this is based on all of the scores you have added.
> 
> I think that's it from a quick glance but I could be wrong.

By ""mean"" you actually meant ""median"", right?",hi know people paper low mid high see range result mid always mean low high meaning return confidence interval mean basically low percentile mid percentile high percentile based added think quick glance could wrong mean actually meant median right,issue,negative,negative,neutral,neutral,negative,negative
1170953598,"> oh great, then I'll add that function in the colab. What's the highest batch size you can reach now?
> […](#)
> On Thu, Jun 30, 2022 at 10:21 AM adamantboy ***@***.***> wrote: Could you try to add this function to the colab def delete_live_buffers(): for device in jax.local_devices(): lbs = device.live_buffers() for lb in lbs: lb.delete() and add a call to it just before each call to gc.collect() And check if this allows you to increase the batch size to 4 or higher. If not would be interesting to know if you see the same error or a different one. I tried it. The error is same, but the memory preallocated is reduced to 70-90GB when the batch size is 64 indeed.And the batch size can be set to 4 with no OOM error. - Error message, batch size 64: 2022-06-30 15:50:02.179408: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_3_bfc) ran out of memory trying to allocate 74.65GiB (rounded to 80159329536)requested by op 2022-06-30 15:50:06.549646: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:491] ***_________________________________________________________________________________________________ 2022-06-30 15:50:33.277741: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2141] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 80159329472 bytes. BufferAssignment OOM Debugging. BufferAssignment stats: parameter allocation: 1.33GiB constant allocation: 8B maybe_live_out allocation: 288.38MiB preallocated temp allocation: 74.65GiB preallocated temp fragmentation: 212B (0.00%) total allocation: 75.98GiB total fragmentation: 19.2KiB (0.00%) Peak buffers: Buffer 1: Size: 1.27GiB XLA Label: custom-call Shape: f32[64,16,577,577] ========================== Buffer 2: Size: 1.27GiB XLA Label: custom-call Shape: f32[64,16,577,577] ========================== — Reply to this email directly, view it on GitHub <[#1187 (comment)](https://github.com/google-research/google-research/issues/1187#issuecomment-1170915826)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AADGZ5CBMDIRMRYMVQXC3Y3VRVKHTANCNFSM52HXRSJA> . You are receiving this because you were mentioned.Message ID: ***@***.***>

My highest batch size is 32.My time cost of one generation is 9 hours, is it normal?
```bash
time    LOOP    COMPevl  COMPtrn   PREP    TRN+EVL    1stEVAL
(s)     91.7    189.5    635.2     32.2    33853.0    123.8
```
I'm still confused.Your total TPUv4 memory is 2048GB(32GBX64 megacore), my total A100 memory is 640GB(80GBX8).I'm confused about the memory number one train_step compiled can use in your experiment, 32GB or 2048GB?.If it was 32GB, then my 80GB is sufficient but why is my batch size smaller then 64, otherwise my memory is not enough to use batch size 64.",oh great add function highest batch size reach wrote could try add function device add call call check increase batch size higher would interesting know see error different one tried error memory reduced batch size batch size set error error message batch size allocator ran memory trying allocate rounded execution replica memory trying allocate parameter allocation constant allocation allocation temp allocation temp fragmentation total allocation total fragmentation peak buffer size label shape buffer size label shape reply directly view comment id highest batch size time cost one generation normal bash time loop prep still total memory total memory confused memory number one use experiment sufficient batch size smaller otherwise memory enough use batch size,issue,negative,positive,neutral,neutral,positive,positive
1170951057,"Hi @Kamel773 , @maniatis 

Can you tell me if you manage to figure it out. If so, can you tell me the right steps that you follow to get the code embeddings from CuBERT ??

Thank you for your reactivity.

",hi tell manage figure tell right follow get code thank reactivity,issue,negative,positive,positive,positive,positive,positive
1170940234,"oh great,
then I'll add that function in the colab.
What's the highest batch size you can reach now?


On Thu, Jun 30, 2022 at 10:21 AM adamantboy ***@***.***>
wrote:

> Could you try to add this function to the colab
>
> def delete_live_buffers():
>   for device in jax.local_devices():
>     lbs = device.live_buffers()
>     for lb in lbs:
>       lb.delete()
>
> and add a call to it just before each call to
>
> gc.collect()
>
> And check if this allows you to increase the batch size to 4 or higher. If
> not would be interesting to know if you see the same error or a different
> one.
>
> I tried it. The error is same, but the memory preallocated is reduced to
> 70-90GB when the batch size is 64 indeed.And the batch size can be set to 4
> with no OOM error.
>
>    - Error message, batch size 64:
>
> 2022-06-30 15:50:02.179408: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_3_bfc) ran out of memory trying to allocate 74.65GiB (rounded to 80159329536)requested by op
> 2022-06-30 15:50:06.549646: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:491] ***_________________________________________________________________________________________________
> 2022-06-30 15:50:33.277741: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2141] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 80159329472 bytes.
> BufferAssignment OOM Debugging.
> BufferAssignment stats:
>              parameter allocation:    1.33GiB
>               constant allocation:         8B
>         maybe_live_out allocation:  288.38MiB
>      preallocated temp allocation:   74.65GiB
>   preallocated temp fragmentation:       212B (0.00%)
>                  total allocation:   75.98GiB
>               total fragmentation:    19.2KiB (0.00%)
> Peak buffers:
>         Buffer 1:
>                 Size: 1.27GiB
>                 XLA Label: custom-call
>                 Shape: f32[64,16,577,577]
>                 ==========================
>
>         Buffer 2:
>                 Size: 1.27GiB
>                 XLA Label: custom-call
>                 Shape: f32[64,16,577,577]
>                 ==========================
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/1187#issuecomment-1170915826>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AADGZ5CBMDIRMRYMVQXC3Y3VRVKHTANCNFSM52HXRSJA>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",oh great add function highest batch size reach wrote could try add function device add call call check increase batch size higher would interesting know see error different one tried error memory reduced batch size batch size set error error message batch size allocator ran memory trying allocate rounded execution replica memory trying allocate parameter allocation constant allocation allocation temp allocation temp fragmentation total allocation total fragmentation peak buffer size label shape buffer size label shape reply directly view id,issue,negative,positive,positive,positive,positive,positive
1170915826,"> Could you try to add this function to the colab
> 
> ```
> def delete_live_buffers():
>   for device in jax.local_devices():
>     lbs = device.live_buffers()
>     for lb in lbs:
>       lb.delete()
> ```
> 
> and add a call to it just before each call to
> 
> ```
> gc.collect()
> ```
> 
> And check if this allows you to increase the batch size to 4 or higher. If not would be interesting to know if you see the same error or a different one.

I tried it. The error is same, but the memory preallocated is reduced to 70-90GB when the batch size is 64 indeed.And the batch size can be set to 4 with no OOM error.

- Error message, batch size 64:
```bash
2022-06-30 15:50:02.179408: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_3_bfc) ran out of memory trying to allocate 74.65GiB (rounded to 80159329536)requested by op 
2022-06-30 15:50:06.549646: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:491] ***_________________________________________________________________________________________________
2022-06-30 15:50:33.277741: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2141] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 80159329472 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:    1.33GiB
              constant allocation:         8B
        maybe_live_out allocation:  288.38MiB
     preallocated temp allocation:   74.65GiB
  preallocated temp fragmentation:       212B (0.00%)
                 total allocation:   75.98GiB
              total fragmentation:    19.2KiB (0.00%)
Peak buffers:
        Buffer 1:
                Size: 1.27GiB
                XLA Label: custom-call
                Shape: f32[64,16,577,577]
                ==========================

        Buffer 2:
                Size: 1.27GiB
                XLA Label: custom-call
                Shape: f32[64,16,577,577]
                ==========================
```",could try add function device add call call check increase batch size higher would interesting know see error different one tried error memory reduced batch size batch size set error error message batch size bash allocator ran memory trying allocate rounded execution replica memory trying allocate parameter allocation constant allocation allocation temp allocation temp fragmentation total allocation total fragmentation peak buffer size label shape buffer size label shape,issue,negative,positive,positive,positive,positive,positive
1170861875,"Could you try to add this function to the colab
```
def delete_live_buffers():
  for device in jax.local_devices():
    lbs = device.live_buffers()
    for lb in lbs:
      lb.delete()
```

and add a call to it just before each call to
```
gc.collect()
```

And check if this allows you to increase the batch size to 4 or higher.
If not would be interesting to know if you see the same error or a different one.",could try add function device add call call check increase batch size higher would interesting know see error different one,issue,negative,positive,positive,positive,positive,positive
1170834632,"> oh ok, 80G should be enough. How long does it take to OOM? For example, does it crash at the very first train step?

yes, it crashes at first train step when the batch size is big.It's weird that it wants to preallocate 100GB+ memory.",oh enough long take example crash first train step yes first train step batch size weird memory,issue,negative,negative,neutral,neutral,negative,negative
1170828017,"oh ok, 80G should be enough.
How long does it take to OOM?
For example, does it crash at the very first train step?",oh enough long take example crash first train step,issue,negative,positive,neutral,neutral,positive,positive
1170817885,"> Hi! Does it still run out of memory if the batch size is reduced (maybe try batch size of 1 as a test).
> 
> ViT Large experiments have high accelerator memory requirement. In my experience the peak memory usage happens during the forward pass as the activations are stored. (activations are the outputs of each layer that is stored to compute gradients during backward pass). The first dimension of the activations is the batch size, so if you reduce the batch size you may be able to run this experiment on hardware with smaller accelerator memory.
> 
> For example, as described in the paper, the first segment of ViT/L experiment was run on TPUv3 that provides ~16GB per core, for this part of the training batch size was set to 32 (higher values would OOM). The rest of the experiment was run on TPUv4 that provides ~32GB per mega-core, that allowed to increase batch size to 64. Keep in mind that experiments with smaller batch size will take more time to complete.
> 
> Thanks for asking this question that can be useful also for other users! Let me know if you have any other question.

Thank you for your immediate reply.
It's very slow when the batch size was set to 2, spent 5 hours on one generation, but it could run indeed.I used 8 A100 gpus, 80GB per gpu. Your total TPUv4 memory is 2048GB(32GBX64 megacore), my total A100 memory is 640GB(80GBX8).I'm confused about the memory number one train_step compiled can use  in your experiment, 32GB or 2048GB?.If it was 32GB, then my 80GB is sufficient, otherwise it's not enough.",hi still run memory batch size reduced maybe try batch size test large high accelerator memory requirement experience peak memory usage forward pas layer compute backward pas first dimension batch size reduce batch size may able run experiment hardware smaller accelerator memory example paper first segment experiment run per core part training batch size set higher would rest experiment run per increase batch size keep mind smaller batch size take time complete thanks question useful also let know question thank immediate reply slow batch size set spent one generation could run used per total memory total memory confused memory number one use experiment sufficient otherwise enough,issue,positive,positive,neutral,neutral,positive,positive
1170799874,"Hi!
Does it still run out of memory if the batch size  is reduced
(maybe try batch size of 1 as a test).

ViT Large experiments have high accelerator memory requirement.
In my experience the peak memory usage happens during the forward pass as the activations are stored.
(activations are the outputs of each layer that is stored to compute gradients during backward pass).
The first dimension of the activations is the batch size, so if you reduce the batch size you may be able to run this experiment on hardware with smaller accelerator memory.

For example, as described in the paper, the first segment of ViT/L experiment was run on TPUv3 that provides ~16GB per core, for this part of the training batch size was set to 32 (higher values would OOM).
The rest of the experiment was run on TPUv4 that provides ~32GB per mega-core, that allowed to increase batch size to 64.
Keep in mind that experiments with smaller batch size will take more time to complete.

Thanks for asking this question that can be useful also for other users!
Let me know if you have any other question.
",hi still run memory batch size reduced maybe try batch size test large high accelerator memory requirement experience peak memory usage forward pas layer compute backward pas first dimension batch size reduce batch size may able run experiment hardware smaller accelerator memory example paper first segment experiment run per core part training batch size set higher would rest experiment run per increase batch size keep mind smaller batch size take time complete thanks question useful also let know question,issue,positive,positive,positive,positive,positive,positive
1170060031,"Thank you for your reply @rybakov, your reply has solved my questions.
We are temporarily unable to access Google colab due to network problems.

In 2014, end-to-end deep KWS for streaming inference has been proposed to divided into three parts, including feature extraction, deep neural network, posterior handling[1]. This framework has been adopted by many subsequent studies. In 2020, Google proposed KWS_ Streaming. Did this overturn the research published in 2014? 
Both the research of 2014 and kws_streaming  can realize streaming inference. Which one is better, or more worthy of further research and application?

Thank you.

[1] Chen, Guoguo, Carolina Parada, and Georg Heigold. ""Small-footprint keyword spotting using deep neural networks."" 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.
",thank reply reply temporarily unable access due network deep streaming inference divided three feature extraction deep neural network posterior handling framework adopted many subsequent streaming overturn research research realize streaming inference one better worthy research application thank spotting deep neural international conference acoustic speech signal,issue,positive,positive,neutral,neutral,positive,positive
1169555275,"After some more hours, the installation fails:

```
ERROR: Exception:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py"", line 180, in _main
    status = self.run(options, args)
  File ""/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py"", line 199, in wrapper
    return func(self, options, args)
  File ""/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py"", line 319, in run
    reqs, check_supported_wheels=not options.target_dir
  File ""/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 128, in resolve
    requirements, max_rounds=try_to_avoid_resolution_too_deep
  File ""/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py"", line 473, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
  File ""/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py"", line 384, in resolve
    raise ResolutionTooDeep(max_rounds)
pip._vendor.resolvelib.resolvers.ResolutionTooDeep: 2000000
```

",installation error exception recent call last file line status file line wrapper return self file line run file line resolve file line resolve state file line resolve raise,issue,positive,neutral,neutral,neutral,neutral,neutral
1167895578,"Hi @panchengww, the evaluation tool for model streaming was designed for non strided models (and was not extended, please feel free to update it). I would suggest to run the evaluation with [colab](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/02_inference.ipynb)",hi evaluation tool model streaming designed non extended please feel free update would suggest run evaluation,issue,positive,positive,positive,positive,positive,positive
1167094805,@kamalkraj there is one file (May '22) available at archive.org on the gsbucket url: https://web.archive.org/web/20220509043347/https://storage.googleapis.com/scenic-bucket/ul2/ul220b/config.gin,one file may available,issue,negative,positive,positive,positive,positive,positive
1166429609,"As it produces the similar results in the paper, I believe this is a good way to run the code. Also thank you, Agarwl :smile:",similar paper believe good way run code also thank smile,issue,positive,positive,positive,positive,positive,positive
1166270827,"While running run.sh, It comes the error like this:
Traceback (most recent call last):
  File ""/home3/yc/Workspaces/jzx/contrastive_rl/lp_contrastive.py"", line 31, in <module>
    import contrastive
  File ""/home3/yc/Workspaces/jzx/contrastive_rl/contrastive/__init__.py"", line 18, in <module>
    from contrastive.agents import DistributedContrastive
  File ""/home3/yc/Workspaces/jzx/contrastive_rl/contrastive/agents.py"", line 21, in <module>
    from acme import specs
  File ""/home/yc/anaconda3/envs/contrastive_rl/lib/python3.9/site-packages/acme/__init__.py"", line 35, in <module>
    from acme.environment_loop import EnvironmentLoop
  File ""/home/yc/anaconda3/envs/contrastive_rl/lib/python3.9/site-packages/acme/environment_loop.py"", line 26, in <module>
    from acme.utils import signals
  File ""/home/yc/anaconda3/envs/contrastive_rl/lib/python3.9/site-packages/acme/utils/signals.py"", line 22, in <module>
    import launchpad
  File ""/home/yc/anaconda3/envs/contrastive_rl/lib/python3.9/site-packages/launchpad/__init__.py"", line 36, in <module>
    from launchpad.nodes.courier.node import CourierHandle
  File ""/home/yc/anaconda3/envs/contrastive_rl/lib/python3.9/site-packages/launchpad/nodes/courier/node.py"", line 21, in <module>
    import courier
  File ""/home/yc/anaconda3/envs/contrastive_rl/lib/python3.9/site-packages/courier/__init__.py"", line 26, in <module>
    from courier.python.client import Client  # pytype: disable=import-error
  File ""/home/yc/anaconda3/envs/contrastive_rl/lib/python3.9/site-packages/courier/python/client.py"", line 30, in <module>
    from courier.python import py_client
ImportError: libtensorflow_framework.so.2: cannot open shared object file: No such file or directory


It seems to be the wrong edition of dm-acme and dm-lanuchpad. But my environment is consistent with yml. Can anyone help?",running come error like recent call last file line module import contrastive file line module import file line module acme import spec file line module import file line module import file line module import file line module import file line module import courier file line module import client file line module import open object file file directory wrong edition environment consistent anyone help,issue,negative,negative,neutral,neutral,negative,negative
1164762167,"flax.linen has a different way of defining modules. The documentation is here: https://flax.readthedocs.io/en/latest/flax.linen.html
For example, you will need to put the arguments as class variables and override the __call__() function. Calling the model is also a bit different (see the `model.apply` example in the documentation). In short, the current code does not work with if we just change the library from flax.nn to flax.linen. Hope it helps.

Best,
Junjie",different way documentation example need put class override function calling model also bit different see example documentation short current code work change library hope best,issue,positive,positive,positive,positive,positive,positive
1164727297,"We never open-sourced code to extract embeddings from the pre-trained model. We did try something out that largely looks like https://github.com/google-research/bert/blob/master/extract_features.py, but customized to the CuBERT tokenized code. I can try to open-source that code, if you'd find that useful, but as you say, it's tough to work on TF 1.14, and we have no plans to port this code forward to TF 2.

Others who used the CuBERT pre-trained models are using the huggingface framework to load the CuBERT checkpoints. That might also be an option for you.",never code extract model try something largely like code try code find useful say tough work port code forward used framework load might also option,issue,positive,positive,neutral,neutral,positive,positive
1164698844,@mohzhang - were you able to find an answer to this? I am having a similar question,able find answer similar question,issue,negative,positive,positive,positive,positive,positive
1163667247,"@junjiek 

When I run a prediction I get a couple deprecated code warnings.  

The first 2 are the same issue.

`
WARNING:tensorflow:From /Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py:157: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
`

`
W0622 16:08:59.331060 4690888192 deprecation.py:350] From /Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py:157: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
`

`
/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/flax/nn/base.py:273: DeprecationWarning: The `flax.nn` module is Deprecated, use `flax.linen` instead. Learn more and find an upgrade guide at https://github.com/google/flax/blob/master/flax/linen/README.md
  warnings.warn(""The `flax.nn` module is Deprecated, use `flax.linen` instead. Learn more and find an upgrade guide at https://github.com/google/flax/blob/master/flax/linen/README.md"", DeprecationWarning)
`

I tried to upgrade to flax.linen but then it wouldn't run for me.  Do you have any suggestions? 

Thanks,

Tim",run prediction get couple code first issue warning removed future version use removed future version use module use instead learn find upgrade guide module use instead learn find upgrade guide tried upgrade would run thanks,issue,negative,positive,positive,positive,positive,positive
1163664327,"@junjiek 

Ok I got it running with a prediction.    

Thanks",got running prediction thanks,issue,negative,positive,positive,positive,positive,positive
1163565922,"@junjiek 

Thanks for responding.  I have to look at what I am doing wrong then.  I need to obviously come up with a better question for you.  Thanks.",thanks look wrong need obviously come better question thanks,issue,positive,positive,neutral,neutral,positive,positive
1163489962,Sorry for the late reply. I used tensorflow==2.9.1 and it worked for me.,sorry late reply used worked,issue,negative,negative,negative,negative,negative,negative
1163202050,"Sorry for the delay. 
It seems like the problem is 'TimeLimit' Wrapper. Without the wrapper the env is good with `_reset`. So I changed [here](https://github.com/google-research/google-research/blob/4808a726f4b126ea38d49cdd152a6bb5d42efdf0/pse/jumping_task/data_helpers.py#L53) with 
`initial_obs = env.environment.env._reset(  # pylint: disable=protected-access
      obstacle_position=obstacle_position,
      floor_height=floor_height)`

This `_reset` works. But it still reported error that I shoud take a reset before step. So I added `env.environment.reset()` before the previous changed line. It seems to work well. Maybe it is a proper change that matches your idea? ",sorry delay like problem wrapper without wrapper good work still error take reset step added previous line work well maybe proper change idea,issue,negative,positive,neutral,neutral,positive,positive
1162207436,"Hi @alexkarargyris 

I modify the source code to get the embedding from CuBERT. It was a bit hard to work on TF 1.14. 

Kamel

",hi modify source code get bit hard work,issue,negative,negative,negative,negative,negative,negative
1162086107,@Kamel773 did you manage to figure it out? I was under the impression that the embeddings are here https://github.com/google-research/google-research/blob/master/cubert/run_classifier.py#L600 but unfortunately i am getting very weird results.,manage figure impression unfortunately getting weird,issue,negative,negative,negative,negative,negative,negative
1160769111,"Hi @ElchaabiMohamed 
I solved in such a way switching `import tensorflow.compat.v1 as tf` to  `import tensorflow as tf`",hi way switching import import,issue,negative,neutral,neutral,neutral,neutral,neutral
1160676875,"@junjiek

I changed my flax to be 0.3.3, but I am getting conflicts with my tensorflow>=2.9.1 and a related library daal==2021.2.3

What version of tensor flow are you running?  Could you share with me the updated requirements.txt file or update it here?

This is what is in the requirements file now tensorflow>=2.0.0-beta1.  That version has been deprecated.

Thanks
 Tim",flax getting related library version tensor flow running could share file update file version thanks,issue,positive,positive,neutral,neutral,positive,positive
1160543733,"So the `_reset` method needs to be called like it was called earlier i suppose: `env.environment._reset` (also, the `reset` function internally uses `_reset`: https://github.com/google-research/jumping-task/blob/master/gym_jumping_task/envs/jumping_task.py#L210)",method need like suppose also reset function internally,issue,negative,neutral,neutral,neutral,neutral,neutral
1160539395,"> Btw, you want to use the jumping task repo: https://github.com/google-research/jumping-task as the original one didn't have extra support for some of the experiments in the paper.

Thanks for super fast reply.

Yes, the problem is the version I used for jumping task doesn't have the `_reset` method at all. I went to the repo you mentioned and cloned it. I placed the dir 'gym_jumping_task' to under the dir 'jumping_task'. However I got the same error. 

I now directly import all modules in the files (for example, remove `from pse.jumping_task`), and run with `python train.py --train_dir .. --training_epochs 1`. I still got error.

I made a testEnv code:
```python
import gym
import gym_jumping_task

env = gym.make(""jumping-task-v0"")

print(env.reset()) # good
print(env._reset()) # error
```

And idea?",want use task original one extra support paper thanks super fast reply yes problem version used task method went however got error directly import example remove run python still got error made code python import gym import print good print error idea,issue,positive,positive,positive,positive,positive,positive
1160432132,"Btw, you want to use the jumping task repo: https://github.com/google-research/jumping-task as the original one didn't have extra support for some of the experiments in the paper.",want use task original one extra support paper,issue,positive,positive,positive,positive,positive,positive
1160424820,"Well, seems like I'm not really calling the _reset for gym-jumping-task",well like really calling,issue,positive,positive,positive,positive,positive,positive
1160190281,"Hi @ElchaabiMohamed 

Thank you for your answer, but i still have the same error even i run CuBERT in TF 1.14.

Any other suggestion ???

Or can you tell me the right steps that you follow to fine-tune CuBERT ??


",hi thank answer still error even run suggestion tell right follow,issue,negative,positive,positive,positive,positive,positive
1160157891,"Hi @ElchaabiMohamed 

Yeah, it was a bit complicated to fine tune Cubert. To solve this issue, you should run CuBERT in TensorFlow 1.14.

I would recommend to have a look to other models such as [CodeBERT](https://github.com/microsoft/CodeXGLUE) and [CodeT5](https://github.com/Kamel773/CodeT5), these models build in PyTorch and much easier to fine-tune and modify their source code. ",hi yeah bit complicated fine tune solve issue run would recommend look build much easier modify source code,issue,positive,positive,neutral,neutral,positive,positive
1160145637,"Hi @Kamel773 ,

I am getting the same error, did you resolve it??",hi getting error resolve,issue,negative,neutral,neutral,neutral,neutral,neutral
1157865238,"Thanks @junjiek

I did upgrade the flax, but I'll make sure I use this version and see if that fixes the problem.  Thanks.

Tim",thanks upgrade flax make sure use version see problem thanks,issue,positive,positive,positive,positive,positive,positive
1156980209,"Hi,

Thank you for the interest in the project. I suspect the issue is because of the flax migration from flax.nn to flax.linen. We were using the deprecated API, it is moved from flax.nn to flax.deprecated.nn. The partial function is part of the flax.nn.Module parent class. 

I just tried the code again with flax==0.3.3 and it was working for me. Could you try `pip3 install flax==0.3.3` and see if it fixes the issue?

Best",hi thank interest project suspect issue flax migration partial function part parent class tried code working could try pip install see issue best,issue,positive,positive,positive,positive,positive,positive
1156684468,I had made a mistake and was able to resolve it.,made mistake able resolve,issue,negative,positive,positive,positive,positive,positive
1154453815,"@junjiek

I was taken off of this project for a little while and just got reassigned.  I am trying to run a prediction with the MUSIQ code here using one of the existing checkpoints.  

I discovered quickly that some of the library references were depreciated and I have update the libraries as well as TensorFlow to 2.9.1.  I am trying to get an MOS score on 1 image and when the code gets to line 143 of run_predict_image.py
'model = model_mod.Model.partial(
      num_classes=num_classes, train=False, **model_config)'
 it is trying to call the .partial attribute of the Model class from multiscale_transformer.py and it fails.  When I look in the code for multiscale_transformer.py there is no class function partial.
 
When the prediction code for MUSIQ was posted to the GitHub, was there some code left out that was supposed to be here to run the prediction or am I missing something?

Thanks,   Tim",taken project little got trying run prediction code one discovered quickly library update well trying get score image code line trying call attribute model class look code class function partial prediction code posted code left supposed run prediction missing something thanks,issue,negative,positive,neutral,neutral,positive,positive
1154127224,"Just a quick pointer for anyone who is still interested in this - I was able to build a working CPU-only (not metal enabled) version of Scann on M1 using the patches [here](https://gist.github.com/DavidMChan/934b4a7025652554ae99ee65809df51d) (use the ALL.patch file, or you can look at the individual changes in the other patches) , BAZEL version 4.2.2 (I used bazelisk, hence the USE_BAZEL_VERSION command below), and the following modified build command:

```
USE_BAZEL_VERSION=4.2.2 bazel build --incompatible_blacklisted_protos_requires_proto_info=false -c opt --features=thin_lto --copt=-mavx2 --copt=-mfma --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg
```




",quick pointer anyone still interested able build working metal version use file look individual version used hence command following build command build opt,issue,negative,positive,positive,positive,positive,positive
1153116852,"@DanielHesslow gotcha, thanks for pointing out. we'll update the paper in the next version to reflect this so as to not confuse others too! thanks! ",thanks pointing update paper next version reflect confuse thanks,issue,positive,positive,positive,positive,positive,positive
1152983237,"Ahh okay thanks! This phrasing in the paper confused me `we feed the model an extra paradigm token`, then everything should be good :) ",thanks phrasing paper confused feed model extra paradigm token everything good,issue,positive,positive,positive,positive,positive,positive
1152980916,yeah they are not literally one token per se. it's just a mode prefix. ,yeah literally one token per se mode prefix,issue,negative,neutral,neutral,neutral,neutral,neutral
1152963085,"Thanks, the issue is that ` [NLG]`, `[NLU]` and `[S2S]` are not defined in the default t5 tokenizer. And I can't seem to find any publicly released tokenizer where they are defined. 

Well, unless eg. `[S2S]` is supposed to be tokenized as the sequence [784, 134, 357, 134, 908] 
",thanks issue defined default ca seem find publicly defined well unless supposed sequence,issue,positive,positive,neutral,neutral,positive,positive
1152956010,"hey! the prompt tokens we use are [NLG], [NLU] and [S2S]. This information is in the appendix. NLG corresponds to X-denoisers, NLU to R-denoisers and S2S to S-Denoisers. ",hey prompt use information appendix,issue,negative,neutral,neutral,neutral,neutral,neutral
1152755117,"Just kidding, I can't just accept your PR, I need pull in your changes myself and re-export them. The headers will reset automatically. Hold on...",ca accept need pull reset automatically hold,issue,negative,neutral,neutral,neutral,neutral,neutral
1149547032,"@kurehawaru  Hello,

I am working on action detection . I have outputs of openpose model and i am trying to make embeddings from the output of openpose which are contained in csv. Can you please let me know how to get embeddings from infer.py from scratch. Will be very glad if got to know this",hello working action detection model trying make output please let know get scratch glad got know,issue,positive,positive,positive,positive,positive,positive
1148079311,"Hi @DaichenWang please install the latest version of TF by running ""pip install tf_nightly""",hi please install latest version running pip install,issue,negative,positive,positive,positive,positive,positive
1146353422,"Looks like tensorflow is no longer a dependency, so this is no longer needed.",like longer dependency longer,issue,negative,neutral,neutral,neutral,neutral,neutral
1143943449,"Hi @dungnguyen98, kws models are small and not compute heavy, you could add data prefetching, or increase batch size as discussed in [thread](https://github.com/google-research/google-research/issues/679).",hi small compute heavy could add data increase batch size thread,issue,negative,negative,negative,negative,negative,negative
1141858869,"I am on python 3.9 - using tensorflow 2.8
my pip version 22.1.1
and get the same error 
`ERROR: Could not find a version that satisfies the requirement scann (from versions: none)`",python pip version get error error could find version requirement none,issue,negative,neutral,neutral,neutral,neutral,neutral
1141201024,"Actually I converted it to HFs T5 implementation instead 😅 , I could maybe upload an unofficial port if it's of interest. ",actually converted implementation instead could maybe unofficial port interest,issue,negative,neutral,neutral,neutral,neutral,neutral
1141192883,"I'm new to T5X in general, could you share how you are doing inference at all? Don't you need additional `.gin` files, like the base architecture?",new general could share inference need additional like base architecture,issue,positive,negative,negative,negative,negative,negative
1140725917,"have you every check pip version? 
I tried upgrade and solved it 
pip install -U pip",every check pip version tried upgrade pip install pip,issue,negative,neutral,neutral,neutral,neutral,neutral
1139892958,Apologies for the late response - we have just the Tensorflow implementation currently. ,late response implementation currently,issue,negative,negative,negative,negative,negative,negative
1136983150,"It will be great if we can have the complete gin config. 

is the base model config t5 or t5_1_1 ? @vanzytay",great complete gin base model,issue,positive,positive,neutral,neutral,positive,positive
1136454365,"I seem to have gotten it working now. Had a bug in my cp conversion.  However, regardless of what token I prefix the input with, it appears to be doing `R` denoising. 
> <pad><extra_id_0> fruiting<extra_id_1> above ground on the ground<extra_id_2> fleshy fruiting bodies of<extra_id_3> ",seem gotten working bug conversion however regardless token prefix input pad fruiting ground ground fleshy fruiting,issue,negative,neutral,neutral,neutral,neutral,neutral
1136063575,"Tried with: `<pad>`, `<extra_id_0>`,  `<extra_id_1>`, `<extra_id_2>`, `<extra_id_97>`, `<extra_id_98>`, `<extra_id_99>`  in the t5 tokenizer but pretty much just getting garbage out so far:

Using a random wiki passage on mushrooms with some masked out words, which works fine with t5.

> A mushroom or toadstool is the fleshy, spore-bearing <extra_id_0> body of a fungus, typically produced <extra_id_1>, on soil, or on its food source.
> 
> The standard for the name ""mushroom"" is the cultivated white button mushroom, Agaricus bisporus; hence the word ""mushroom"" is most often applied to those fungi (Basidiomycota, Agaricomycetes) that have a stem (stipe), a cap (pileus), and gills (lamellae, sing. lamella) on the underside of the cap. ""Mushroom"" also describes a variety of other <extra_id_2> fungi, with or without stems, therefore the term is used to describe the fleshy fruiting bodies of some Ascomycota. These gills produce microscopic spores that help the fungus spread across the ground or its occupant surface.
> 

I'm getting the following output with the different conditioning tokens mentioned above:

```
-------------------------------
<pad><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0>s</s>
-------------------------------
<extra_id_0>;<extra_id_0>sis put<extra_id_0>is<extra_id_0>,<extra_id_0>isis<extra_id_0> être Real cross Signh
-------------------------------
<extra_id_1>s Erasmus or"" no  mondiale être.. Erasmus...+1ka’u<extra_id_29>
-------------------------------
<extra_id_2>d<extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0> être"" that<extra_id_5><extra_id_27>t être êtres
-------------------------------
<extra_id_97><extra_id_0><extra_id_0><extra_id_0>TM garanteaz studyiestatesiesteiano<unk> mega<unk>i<extra_id_0><extra_id_0>
-------------------------------
<extra_id_98><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0>y<extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0><extra_id_0>
-------------------------------
<extra_id_99> domestic<extra_id_11>,,is cheddar</s>
```



Could of course also be an issue the way I'm running the model. If you could provide some example usage of the model that would be very helpful. 

Ping @vanzytay
",tried pad pretty much getting garbage far random passage masked work fine mushroom toadstool fleshy body fungus typically produced soil food source standard name mushroom cultivated white button mushroom hence word mushroom often applied fungi stem stipe cap pileus sing lamella underside cap mushroom also variety fungi without therefore term used describe fleshy fruiting produce microscopic help fungus spread across ground occupant surface getting following output different pad si put real cross domestic could course also issue way running model could provide example usage model would helpful ping,issue,positive,positive,neutral,neutral,positive,positive
1135520469,"Hi @Wanggcong , we actually even discuss this in the github repo itself, please see the [Running the code - Training a new model](https://github.com/google-research/google-research/tree/master/regnerf#training-an-new-model) section.",hi actually even discus please see running code training new model section,issue,negative,positive,neutral,neutral,positive,positive
1134628964,"Hey all! 

zarrays have been reuploaded and a config file can be also found at gs://scenic-bucket/ul2/ul220b

Thanks! ",hey file also found thanks,issue,negative,positive,positive,positive,positive,positive
1133965446,"i think there is a typo in the readme, can you try adding a space between ""${prediction_file}"" and the slash?",think typo try space slash,issue,negative,neutral,neutral,neutral,neutral,neutral
1133821907,"in WORKSPACE file, replace these:

```
http_archive(
  name = ""com_google_absl"",
  urls = [""https://github.com/abseil/abseil-cpp/archive/98eb410c93ad059f9bba1bf43f5bb916fc92a5ea.zip""],
  strip_prefix = ""abseil-cpp-98eb410c93ad059f9bba1bf43f5bb916fc92a5ea"",
)

http_archive(
    name = ""com_google_googletest"",
    strip_prefix = ""googletest-release-1.11.0"",
    urls = [""https://github.com/google/googletest/archive/refs/tags/release-1.11.0.zip""],
)

http_archive(
    name = ""rules_cc"",
    strip_prefix = ""rules_cc-main"",
    urls = [""https://github.com/bazelbuild/rules_cc/archive/refs/heads/main.zip""],
)
```",file replace name name name,issue,negative,neutral,neutral,neutral,neutral,neutral
1127610777,"@yangxl-2014-fe @weierwan  Hi, thanks for your solution, but I meet a problem when initialize the model with imagenet pretrained weights, I get the error message below:
`
2022-05-16 20:26:14.298942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.
Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.
Warning: To use the threejs_vizualization, please install the colabtools package following the instructions detailed in the README at github.com/tensorflow/graphics.
Traceback (most recent call last):
  File ""/mnt/bd/lq-zhangtaolucky7/dataset/AbsDepth/google-research/depth_and_motion_learning/parameter_container.py"", line 189, in parse_object
    object_or_json_or_file = json.loads(object_or_json_or_file)
  File ""/home/tiger/anaconda3/envs/tf/lib/python3.8/json/__init__.py"", line 357, in loads
    return _default_decoder.decode(s)
  File ""/home/tiger/anaconda3/envs/tf/lib/python3.8/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/home/tiger/anaconda3/envs/tf/lib/python3.8/json/decoder.py"", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting ',' delimiter: line 9 column 7 (char 156)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/mnt/bd/lq-zhangtaolucky7/dataset/AbsDepth/google-research/depth_and_motion_learning/parameter_container.py"", line 222, in parse_object
    f = open(object_or_json_or_file)
FileNotFoundError: [Errno 2] No such file or directory: '{\n    ""model"": {\n      ""input"": {\n        ""data_path"": ""vid2depth/KITTI_procesed/train.txt""\n      }\n    },\n    ""trainer"": {\n      ""max_steps"": 450000\n      ""init_ckpt"": ""depth_and_motion_learning/init/model.ckpt"",\n      ""init_ckpt_type"": ""imagenet""\n    }\n  }'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/tiger/anaconda3/envs/tf/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/tiger/anaconda3/envs/tf/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/mnt/bd/lq-zhangtaolucky7/dataset/AbsDepth/google-research/depth_and_motion_learning/depth_motion_field_train.py"", line 38, in <module>
    app.run(main)
  File ""/home/tiger/.local/lib/python3.8/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/home/tiger/.local/lib/python3.8/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""/mnt/bd/lq-zhangtaolucky7/dataset/AbsDepth/google-research/depth_and_motion_learning/depth_motion_field_train.py"", line 32, in main
    training_utils.train(depth_motion_field_model.input_fn,
  File ""/mnt/bd/lq-zhangtaolucky7/dataset/AbsDepth/google-research/depth_and_motion_learning/training_utils.py"", line 290, in train
    params.override(FLAGS.param_overrides)
  File ""/mnt/bd/lq-zhangtaolucky7/dataset/AbsDepth/google-research/depth_and_motion_learning/parameter_container.py"", line 429, in override
    self._override(
  File ""/mnt/bd/lq-zhangtaolucky7/dataset/AbsDepth/google-research/depth_and_motion_learning/parameter_container.py"", line 470, in _override
    params_dict = parse_dict(params_dict)
  File ""/mnt/bd/lq-zhangtaolucky7/dataset/AbsDepth/google-research/depth_and_motion_learning/parameter_container.py"", line 266, in parse_dict
    return parse_object(dict_or_json_or_file, allow_dict=True)
  File ""/mnt/bd/lq-zhangtaolucky7/dataset/AbsDepth/google-research/depth_and_motion_learning/parameter_container.py"", line 247, in parse_object
    raise ValueError(message)
ValueError: Unable to parse override parameters either as a literal JSON or as a python dictionary literal or as the name of a file that exists.

GFile error: [Errno 2] No such file or directory: '{\n    ""mode...

JSON parsing error: Expecting ',' delimiter: line 9 column 7 (char 156)

Python dict parsing error: invalid syntax (<unknown>, line 9)

Override parameters:
{
    ""model"": {
      ""input"": {
        ""data_path"": ""vid2depth/KITTI_procesed/train.txt""
      }
    },
    ""trainer"": {
      ""max_steps"": 450000
      ""init_ckpt"": ""depth_and_motion_learning/init/model.ckpt"",
      ""init_ckpt_type"": ""imagenet""
    }
  }.
Debug Info:
Expected counts of "":"" (7) to be half of string delimiters: ' (0) and "" (20)
Char  Count

' ==> 0
"" ==> 20
{ ==> 4
} ==> 4
: ==> 7
, ==> 2
`

and the params I used:
`
  --param_overrides='{
    ""model"": {
      ""input"": {
        ""data_path"": ""vid2depth/KITTI_procesed/train.txt""
      }
    },
    ""trainer"": {
      ""max_steps"": 450000
      ""init_ckpt"": ""depth_and_motion_learning/init/model.ckpt"",
      ""init_ckpt_type"": ""imagenet""
    }
  }'
`
Any idea why this happen? Thanks",fe hi thanks solution meet problem initialize model get error message successfully dynamic library warning deprecation disabled set warning use data format please install package following detailed warning use please install package following detailed recent call last file line file line return file line decode end file line end delimiter line column char handling exception another exception recent call last file line open file directory model input trainer handling exception another exception recent call last file line return code none file line code file line module main file line run main file line main file line main file line train file line override file line file line return file line raise message unable parse override either literal python dictionary literal name file error file directory mode error delimiter line column char python error invalid syntax unknown line override model input trainer half string char count used model input trainer idea happen thanks,issue,negative,positive,neutral,neutral,positive,positive
1127578812,"I have the same issue here, just trying to run the demo. A new window would pop up, print the above output and close immediately. 

Windows 10
Bazel 5.1.1
GCC 11.3.0
",issue trying run new window would pop print output close immediately,issue,negative,positive,positive,positive,positive,positive
1126707900,Hi! We're tidying this up and getting the config in shape for use. ETA: next week! ,hi getting shape use eta next week,issue,negative,neutral,neutral,neutral,neutral,neutral
1125462052,"


> 

Agreed, that sounds good to me. At the very least I think the jax version should be fixed, so I'll leave this quick PR open. I'll let you know if an external repo gets created. ",agreed good least think version fixed leave quick open let know external,issue,positive,positive,positive,positive,positive,positive
1125430463,"Hi Ryan, thanks for putting this together.

I think it probably makes more sense to maintain the code (and examples) for JAX DFT in a separate repository, outside of the shared google-research repository. Accepting pull requests here involves a fair amount of overhead related to Google infrastructure and isn't worth the trouble given that nobody at Google is current running this code. This would also let you and @aikinogard directly merge changes. If there are really only 1 or 2 PRs that you will ever want to merge we could keep it here, but otherwise I think the separate new repository will be much more maintainable. 

What do you think? The code is all open source, so it is free to copy into any new repository that you create. I can also add a prominent link to the README here pointing to the externally maintained version.",hi thanks together think probably sense maintain code separate repository outside repository pull fair amount overhead related infrastructure worth trouble given nobody current running code would also let directly merge really ever want merge could keep otherwise think separate new repository much maintainable think code open source free copy new repository create also add prominent link pointing externally version,issue,positive,positive,positive,positive,positive,positive
1125195739,"Hi @shoyer , I have a few minor-ish updates to the JAX DFT code (formerly developed by Li Li at GAS) to make it usable. I have been contacted by a few people who are trying to use the code and are running into difficulties. Please let me know who we can assign these PRs to.

Thanks,
Ryan",hi code formerly li li gas make usable people trying use code running please let know assign thanks,issue,positive,positive,neutral,neutral,positive,positive
1124740853,"Thanks for your reply. Now I used the google-colab, which works fine. ",thanks reply used work fine,issue,positive,positive,positive,positive,positive,positive
1124730633,"@liuzhengzhe A month ago dream fields was running much faster on my GPUs. I couldn't figure out what changed, e.g. if it was something related to JAX. 

On TPUs inferrence and training are normal for me",month ago dream running much faster could figure something related training normal,issue,negative,positive,positive,positive,positive,positive
1122030197,"Same issue here, is there anyone still doing maintenance to this project?",issue anyone still maintenance project,issue,negative,neutral,neutral,neutral,neutral,neutral
1118737521,ScaNN doesn't have support for isolating the various phases of its search process. ScaNN also depends on having all of its embeddings resident in RAM; not sure if this goes against what you were proposing in terms of fetching partitions from a distributed database.,support isolating various phase search process also resident ram sure go fetching distributed,issue,positive,positive,positive,positive,positive,positive
1118732442,"If you're referring to this line https://github.com/google-research/google-research/blob/fa0dbe14bfaf361cefdec12b5a6bdbb965cbcace/scann/scann/scann_ops/cc/scann.cc#L349

It's not in a loop. (Is that what you mean by a circle?)",line loop mean circle,issue,negative,negative,negative,negative,negative,negative
1118730123,ScaNN does not include dynamic update support.,include dynamic update support,issue,positive,neutral,neutral,neutral,neutral,neutral
1118729019,"Feel free to use ScaNN with Python 3.10 if you find it's stable. I'm surprised you were able to install it though, given ScaNN only has wheels on PyPI for Python 3.7 - 3.9 ([link](https://pypi.org/project/scann/1.2.6/#files)). Official Python 3.10 support will be added in the next release.",feel free use python find stable able install though given python link official python support added next release,issue,positive,positive,positive,positive,positive,positive
1118725031,"ScaNN doesn't support custom similarity metrics--it supports dot product and squared L2 distance only. Perhaps there's a way to map your vectors so that your custom similarity in the original space is equivalent to dot product search in the mapped space, though (although depending on the mapping you use, this may not be very efficient).",support custom similarity metric dot product squared distance perhaps way map custom similarity original space equivalent dot product search space though although depending use may efficient,issue,positive,positive,positive,positive,positive,positive
1118723059,"ScaNN doesn't support MacOS and likely also has problems with the M1 chip, because ScaNN uses x86-specific intrinsics.",support likely also chip,issue,negative,neutral,neutral,neutral,neutral,neutral
1117667338,"@sammymax Hi, I have a related question. Does ScaNN support running on GPU?",hi related question support running,issue,negative,neutral,neutral,neutral,neutral,neutral
1116167265,"I can confirm I also have this issue.

Specifically this occurs in Colab when running a GPU instance. It does not happen if I run a CPU instance with the exact same code.

I also tried all other possible modes and they work perfectly ('int', 'multi_hot' and 'count').

Details:

Tesla K80 GPU, NVIDIA-SMI 460.32.03, Driver Version: 460.32.03, CUDA Version: 11.2, Python 3.7, Tensorflow 2.8.0",confirm also issue specifically running instance happen run instance exact code also tried possible work perfectly driver version version python,issue,positive,positive,positive,positive,positive,positive
1115042950," 

> Thank you for the good work of SMURF. I am trying to replicate the reported performance on the training set of the Sintel dataset, namely 1.71 EPE on Clean and 2.58 EPE on Final.
> 
> I used apply_smurf.py on the sintel sequences but cannot get the same performance. Could you please provide some instructions on what commands to run?
> 
> Thank you.

It turns out that the provided Sintel model is Sintel-Test (which is useful to me as well). In case this information is useful @remimar. In addition, adjusting max_rec_iters can make a difference, say 16.",thank good work trying replicate performance training set namely clean final used get performance could please provide run thank turn provided model useful well case information useful addition make difference say,issue,positive,positive,positive,positive,positive,positive
1115032314,"Hi, I am trying to replicate results on FlyingChairs testing set and on KITTI training set. Here are the commands I ran and the corresponding outputs:

## Eval on provided checkpoints

### FlyingChairs
I ran:
```
python -m smurf_evaluator --eval_on=""chairs:/data/FlyingChairs_tfrecord/test"" --global_gpu_batch_size=1 --num_gpus=1 --checkpoint_dir='/path/to/checkpoint'
```
I got:
``` 
I0407 13:16:56.373906 139860221076224 smurf_evaluator.py:82] New checkpoint found: /home/users/rmarsal/dev/smurf/checkpoints/smurf/chairs/ckpt-76
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
chairs-EPE: 1.966786, chairs-ER: 0.058409, chairs-best-occl-thresh: -1.000000, chairs-census: 1.770922, chairs-eval-time(s): 271.058564, chairs-inf-time(ms): 308.539359, chairs-occl-f-max: -1.000000, chairs-smooth1: 0.000000, chairs-smooth2: 0.008453, chairs-total_loss: 1.779375
```
Should I had to expect EPE 1.72 with this checkpoint as in Table 2 in the paper ? I also tried to set the height and the weight to 368 and 496 as in the paper but the performance are slightly worse.

### KITTI
I ran:
```
python -m smurf_evaluator --eval_on=""kitti:/data/kitti_data/data_scene_flow_training-tfrecords"" --global_gpu_batch_size=1 --num_gpus=1 --checkpoint_dir='/path/to/checkpoint'
```
It returns:
```
Waiting for a new checkpoint, at /home/users/rmarsal/dev/smurf/checkpoints/smurf/kitti, latest is /home/users/rmarsal/dev/smurf/checkpoints/smurf/kitti/ckpt-31
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
kitti-EPE(noc): 30.468662, kitti-EPE(occ): 39.341362, kitti-ER(noc): 0.955546, kitti-ER(occ): 0.958396, kitti-best-occl-thresh: 0.000000, kitti-eval-time(s): 194.786156, kitti-inf-time(ms): 402.838845, kitti-occl-f-max: 0.240043
```
I tried different image dimension : 384 1280 and 296 696 but the result is still roughly the same.

## Re-training models

### FlyingChairs

I ran this with the settings of the repo:
```
python -m smurf_main --from_scratch=True \
--train_on=""chairs:/data/FlyingChairs_tfrecord/train"" --eval_on=""chairs:/data/FlyingChairs_tfrecord/test"" \
--plot_dir=$PATH_CHAIRS --checkpoint_dir=$PATH_CHAIRS \
--global_gpu_batch_size=8 --num_gpus=8  \
--height=384 --width=512 \
--weight_smooth1=2.0 --weight_smooth2=0.0 \
--evaluate_during_train=True \
```
As settings of the repo are slightly different from those of the paper I tried them too:
```
python -m smurf_main --from_scratch=True \
--train_on=""chairs:/data/FlyingChairs_tfrecord/train"" --eval_on=""chairs:/data/FlyingChairs_tfrecord/test"" \
--plot_dir=$PATH_CHAIRS --checkpoint_dir=$PATH_CHAIRS \
--global_gpu_batch_size=8 --num_gpus=8  \
--selfsup_after_num_steps=30000 --selfsup_ramp_up_steps=7500 \
--lr_decay_after_num_steps=60000 \
--weight_smooth1=4.0 --weight_smooth2=0.0 \
--selfsup_mask='none' \
```
But the model did not learn anything in both cases: EPE remain above 11.


### KITTI

I fine-tuned the FlyingChairs model provided for KITTI with the setting of the repo:
```
python -m smurf_main
--train_on=""kitti:/data/kitti/data_scene_flow/_test_384x1280_fullseq-tfrecords"" --eval_on=""kitti:/data/kitti/data_scene_flow/_training-tfrecords"" \
--init_checkpoint_dir='/path/to/chairs/checkpoints'
--global_gpu_batch_size=8 --num_gpus=8  \
--height=296 --width=696 \
--occlusion_estimation='brox'
```
I got:
```
Step is 76000
75 -- total-loss: 12.162772, census-loss: 11.476587, data-time: 4.658461, learning-rate: 0.000000, selfsup-loss: 0.651478, smooth1-loss: 0.000000, smooth2-loss: 0.034706, supervision-loss: 0.000000, train-time: 3135.333538
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
kitti-EPE(noc): 1.903955, kitti-EPE(occ): 2.641327, kitti-ER(noc): 0.075511, kitti-ER(occ): 0.086005, kitti-best-occl-thresh: 0.974359, kitti-eval-time(s): 126.104367, kitti-inf-time(ms): 186.461623, kitti-occl-f-max: 0.906196
```
I also tried the same training from scratch and the result become:
```
Step is 76000
75 -- total-loss: 10.951851, census-loss: 10.805418, data-time: 4.501820, learning-rate: 0.000006, selfsup-loss: 0.129271, smooth1-loss: 0.000000, smooth2-loss: 0.017162, supervision-loss: 0.000000, train-time: 3094.687700
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
kitti-EPE(noc): 1.975226, kitti-EPE(occ): 2.862183, kitti-ER(noc): 0.077030, kitti-ER(occ): 0.090028, kitti-best-occl-thresh: 0.974359, kitti-eval-time(s): 126.648766, kitti-inf-time(ms): 194.172957, kitti-occl-f-max: 0.903836
```

As settings of the repo are slightly different from those of the paper I tried them too:
```
python -m smurf_main \
    --reset_global_step=True --reset_optimizer=True \
    --evaluate_during_train=True \
    --train_on=""kitti:/data/kitti/data_scene_flow/_test_384x1280_fullseq-tfrecords"" --eval_on=""kitti:/data/kitti/data_scene_flow/_training-tfrecords"" \
    --checkpoint_dir=$PATH_KITTI \
    --plot_dir=$PATH_KITTI \
    --init_checkpoint_dir='/path/to/chairs/checkpoints' \
    --global_gpu_batch_size=8 \
    --num_gpus=8 \
    --height=296 \
    --width=696 \
    --occlusion_estimation='brox' \
    --weight_smooth1=0.0 \
    --weight_smooth2=4.0 \
    --num_train_steps=75000 \
    --selfsup_after_num_steps=30000 \
    --selfsup_ramp_up_steps=7500 \
    --lr_decay_after_num_steps=60000 \
    --lr_decay_steps=1500 \
    --dropout_rate=0 \
    --selfsup_mask='none' \
    --occ_after_num_steps_brox=0 \
```
I got:
```
Step is 76000
75 -- total-loss: 12.162772, census-loss: 11.476587, data-time: 4.658461, learning-rate: 0.000000, selfsup-loss: 0.651478, smooth1-loss: 0.000000, smooth2-loss: 0.034706, supervision-loss: 0.000000, train-time: 3135.333538
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
kitti-EPE(noc): 1.903955, kitti-EPE(occ): 2.641327, kitti-ER(noc): 0.075511, kitti-ER(occ): 0.086005, kitti-best-occl-thresh: 0.974359, kitti-eval-time(s): 126.104367, kitti-inf-time(ms): 186.461623, kitti-occl-f-max: 0.906196
```
I expected at least one of the previous configuration to match the performance of the paper without multi-frame supervision (i.e EPE=2.45 and ER(occ)=7.53) but I am actually far from them. 

@AustinCStone how to reproduce the performance of your paper? Should I trust the instructions of the ""Experimental Details"" section of your repo or the hyperparameters indicated in your paper? Do you notice any mistake in the commands above that could explain such disparities between the results I got and those of your paper?

Thank you",hi trying replicate testing set training set ran corresponding provided ran python got new found expect table paper also tried set height weight paper performance slightly worse ran python waiting new latest tried different image dimension result still roughly ran python slightly different paper tried python model learn anything remain model provided setting python got step also tried training scratch result become step slightly different paper tried python got step least one previous configuration match performance paper without supervision er actually far reproduce performance paper trust experimental section paper notice mistake could explain got paper thank,issue,negative,positive,neutral,neutral,positive,positive
1114303123,"> I've implemented a 3D version of the non max suppression algorithm in C following the tensorflow tutorial to create custom operation. I've compiled, run the basic tests (similar to those in 2D that can be seen found in tensoflow sources) and used it with success within a deep learning algorithm. I'm going to publish this 3D NMS in the tensorflow repositories in the next few days: I'll then add a link here.

Hello, I would be really gratefuly if you could send the link to the 3D NMS repository. Thank you in advance!",version non suppression algorithm following tutorial create custom operation run basic similar seen found used success within deep learning algorithm going publish next day add link hello would really could send link repository thank advance,issue,positive,positive,neutral,neutral,positive,positive
1110575930,"@INF800 Thanks for the report! Looks like this error comes up with some versions of Jax. Changing line 109 in your version of augment.py should fix it:
```
# Fixed version
bg_idx = random.choice(bg_sel_key, bgs.shape[0], p=np.array(probs))
bg = bgs[bg_idx]
```

```
# Old version
bg = random.choice(bg_sel_key, bgs, p=np.array(probs))
```

I'll try to release a fix next week.",thanks report like error come line version fix fixed version old version try release fix next week,issue,negative,positive,neutral,neutral,positive,positive
1109602822,"Yes, correct, we updated this in the second paper version but the arxiv version is still the old one. Thanks for the pointer, we will update the arxiv paper soon!",yes correct second paper version version still old one thanks pointer update paper soon,issue,positive,positive,positive,positive,positive,positive
1100499294,"python 3.9.7 - April 16, 2022 still got this issue
pip install -q scann
ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann

",python still got issue pip install error could find version requirement none error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
1099851896,"@G-Apple1 Hi, got the same error but I can surpass it when we use tf.funtion() decorator.",hi got error surpass use decorator,issue,negative,neutral,neutral,neutral,neutral,neutral
1098322334,audio will be trimmed by 1000ms in above I pointed you to the line in the code where trimming will happen. [tf.audio.decode_wav](https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav) reads audio and trims/crops it.,audio pointed line code trimming happen audio,issue,negative,neutral,neutral,neutral,neutral,neutral
1098097192,"If I'm not wrong, this one is during the decoding?
I have trained a model, keeping ""--clip_duration_ms=1000"", with the data having 0.5 seconds to 3seconds of audio.
In this,  whether the model was trained on the entire utterance between the audios of 0.5 to 3 seconds or, it trim the 1000 ms of audio, train on it, and discard the rest of the utterance?",wrong one trained model keeping data audio whether model trained entire utterance trim audio train discard rest utterance,issue,negative,negative,negative,negative,negative,negative
1096369540,"hi @rybakov 

I successfully train a small model with training steps (400,400,400,400) based on att_mh_rnn architecture.
My dataset contains audio between 0.5 to 3 seconds. I used ""--clip_duration_ms=1000"". 
Any idea, in this case, whether the model was trained on the entire utterance between the audios of 0.5 to 3 seconds or, it trim the 1000 ms of audio, train on it, and discard the rest of the utterance?
The test accuracy during training is 93.82%",hi successfully train small model training based architecture audio used idea case whether model trained entire utterance trim audio train discard rest utterance test accuracy training,issue,negative,positive,positive,positive,positive,positive
1095463294,"Hi @csetanmayjain, this lib does not support training samples with different audio length: all data should have the same length with defined parameter e.g. ""--clip_duration_ms=1000"" for audio with 1000ms length.",hi support training different audio length data length defined parameter audio length,issue,negative,neutral,neutral,neutral,neutral,neutral
1092783913,"Hi @hassanhub,

Thanks for the wonderful work. 

I'm trying to reproduce the pretraining step of VATT but facing the same error as everyone above. 
` OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.`

It would be very helpful if you could guide us past the error.

Best
",hi thanks wonderful work trying reproduce pretraining step facing error everyone slice index dimension would helpful could guide u past error best,issue,positive,positive,positive,positive,positive,positive
1092304289,"Hi @derrick-xwp, I don't see this error with our Colab notebook. Could you provide a full stack trace? Are you running the code on your own server?",hi see error notebook could provide full stack trace running code server,issue,negative,positive,positive,positive,positive,positive
1092178881,"### ImageNet FID Scores not matching with the paper

What I did
1. Used the pretrained checkpoints given
2. Finetune them on a custom Imagenet dataset (a smaller version of 1000 train images) for all 3 steps, for around 10 epochs using `coltran.run` script 
3. Generate output using `coltran.custom_colorize` script
4. Calculate FID using this package [pytorch-fid](https://pypi.org/project/pytorch-fid/) 

Got FID score of around `59` while the FID score mentioned in paper is around `19`.

While calculating FID, both ground truth and generated images (count 436) are of res 256x256.

GroundTruth Images
![image](https://user-images.githubusercontent.com/42700922/162290277-32d4e856-1317-46c2-ad07-a0e6651c0ec1.png)

Generated Images
![image](https://user-images.githubusercontent.com/42700922/162290363-203e1eeb-77a3-43f6-adf9-33f8440f385f.png)


- Am I missing any step or anything I am not taking into consideration?
- Anything to do with the fid package used? (I tried the script mentioned earlier here, but TF versions are creating issues and this one is a lot simpler to use)
",fid matching paper used given custom smaller version train around script generate output script calculate fid package got fid score around fid score paper around calculating fid ground truth count image image missing step anything taking consideration anything fid package used tried script one lot simpler use,issue,negative,negative,neutral,neutral,negative,negative
1091111737,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

For more information, open the [CLA check for this pull request](https://github.com/google-research/google-research/pull/1063/checks?check_run_id=5863233339).",thanks pull request like may first contribution open source project look pull request need sign contributor license agreement information open check pull request,issue,positive,positive,positive,positive,positive,positive
1089143934,"Hi @AlyssaYoung and @hassanhub  , 
Have you figured out the solution by any chance? I am encountering the same problem using cpu ( for the moment) and I can't figure out why I am getting this error.
Best,
",hi figured solution chance problem moment ca figure getting error best,issue,negative,positive,positive,positive,positive,positive
1086888924,"> Hi. A big thank-you to @jalayrac. The [hmdb example of dmvr](https://github.com/deepmind/dmvr/blob/master/examples/linear_mmv_hmdb.py) can run successfully.
> 
> However, despite using the new version of tfrecord, vatt still output following error:
> 
> I was running finetune/test mode using:
> 
> ```
> python -m vatt.main --task=finetune --mode=eval --model_dir=./my_output --model_arch=vit_base --strategy_type=mirrored --override_checkpoint=./Checkpoint/Kinetics-400_BBS/vision_vatt_finetune_vit_base_k400_modality_specific_pretrain_ckpt-100000
> ```
> 
> And following is the error:
> 
> ```
> I0211 14:25:43.123416 139658407814144 video_dataset.py:456] Dataset created successfully
> 2022-02-11 14:25:50.560914: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 14:25:50.560957: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 14:25:50.564367: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 14:25:50.564423: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> 2022-02-11 14:25:50.565001: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
> ```
> 
> ```
> tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.
>   (0) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
> 	 [[{{node strided_slice_9}}]]
> 	 [[MultiDeviceIteratorGetNextFromShard]]
> 	 [[RemoteCall]]
> 	 [[IteratorGetNext_3]]
> 	 [[model/video_module/vit_base/spatio_temporal_embeddings/strided_slice_3/_3]]
>   (1) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
> 	 [[{{node strided_slice_9}}]]
> 	 [[MultiDeviceIteratorGetNextFromShard]]
> 	 [[RemoteCall]]
> 	 [[IteratorGetNext_3]]
>   (2) CANCELLED:  Function was cancelled before it was started
> 0 successful operations.
> 0 derived errors ignored. [Op:__inference_evaluation_step_21087]
> 
> Function call stack:
> evaluation_step -> evaluation_step -> evaluation_step
> ```
> 
> What should I do? @hassanhub
> 
> cc @adaniefei

I have the same problem, is the problem solved?",hi big example run successfully however despite new version still output following error running mode python following error successfully slice index dimension slice index dimension slice index dimension slice index dimension slice index dimension root error found slice index dimension node slice index dimension node function successful derived function call stack problem problem,issue,negative,positive,positive,positive,positive,positive
1075158664,@MechCoder  provided I train the model again or will it work on the current model?,provided train model work current model,issue,negative,neutral,neutral,neutral,neutral,neutral
1075154680,"This is the final dense layer (https://github.com/google-research/google-research/blob/master/coltran/models/upsampler.py#L194). The 256 dimensions output are a distribution over 256 RGB symbols.

You can modify it depending on the dimension of your output space.",final dense layer output distribution modify depending dimension output space,issue,negative,neutral,neutral,neutral,neutral,neutral
1073187188,"I'm having some problems running this project. I can't imagine that this project from Google has reported 496 problems so far, when in fact the project is not that complicated.",running project ca imagine project far fact project complicated,issue,negative,negative,negative,negative,negative,negative
1072113847,"@lazrak-mouad @AustinCStone  hey,while I use the convert_video_to_dataset.py. I use a 640x368 video about 30 second and generate a document.I use it and 'chair' as the train_on flag and I get the same error with @infi2021 .like tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: [<tf.Tensor 'pwc_flow/sequential_5/leaky_re_lu_10/LeakyRelu:0' shape=(1, 96, 128, 32) dtype=float32>, <tf.Tensor 'pwc_flow/add_13:0' shape=(1, 96, 128, 2) dtype=float32>]
Consider rewriting this model with the Functional API.
but this is a WARNING and the program can be run.I'd like to ask you about this.",hey use use video second generate use flag get error sequential model single input tensor receive class input consider model functional warning program like ask,issue,negative,negative,neutral,neutral,negative,negative
1069065314,"Sorry, I mixed up PoseNet and OpenPose. I see that you use OpenPose since the DGS corpus comes pre-processed with OpenPose landmarks. However, you use PoseNet for the demo, since that is significantly faster than OpenPose and can run in realtime.
However, you only present results using OpenPose in the table. However, it is not very useful since it does not run in real time. Since the useful method is using PoseNet, could you share the accuracy results for the PoseNet version/demo version?",sorry mixed see use since corpus come however use since significantly faster run however present table however useful since run real time since useful method could share accuracy version,issue,positive,positive,neutral,neutral,positive,positive
1069034294,"3] Also, why does the demo only use 17 points out of OpenPose's 25? Is it to reduce latency?",also use reduce latency,issue,negative,neutral,neutral,neutral,neutral,neutral
1066748269,"hello, I meet the same problem. How to fix this?",hello meet problem fix,issue,negative,neutral,neutral,neutral,neutral,neutral
1066495557,I updated it. The new link is https://tinyurl.com/coltranfewclicksv2. Let me know if it works.,new link let know work,issue,negative,positive,positive,positive,positive,positive
1066485710,"@haiderasad Can you try the hack mentioned in this blogpost? (https://habr.com/ru/company/ruvds/blog/563858/)

```
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')
```

You can save some memory by changing from float32 to float16.",try hack import save memory float float,issue,negative,neutral,neutral,neutral,neutral,neutral
1066160541,hi @MechCoder  can the final output dimension  be changed to 512 or 1024 from 256?,hi final output dimension,issue,negative,neutral,neutral,neutral,neutral,neutral
1066153302,"hi i am having the same probelm with spatial upsampler part, it gives OOM so i have to run it on cpu by uncommenting the line 6 of custom_colorise, i get following error with gpu

`OOM when allocating tensor with shape[1,256,256,4,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Softmax]`

`2022-03-13 23:04:55.443444: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 256.00MiB (rounded to 268435456)requested by op Softmax
`

i did try batch size 1 in custom_colorize still same error",hi spatial part run uncommenting line get following error tensor shape type float allocator allocator ran memory trying allocate rounded try batch size still error,issue,negative,neutral,neutral,neutral,neutral,neutral
1066125501,"UPDATE: i fixed it by setting the path of checkpoint manually in default field of logdir in custom_colorize.py, somehow it was not picking up the flag in the main command",update fixed setting path manually default field somehow flag main command,issue,negative,positive,positive,positive,positive,positive
1066120772,"yes i checked again and again, my pre trained model are inside the main dir ""coltran/coltran_coltran/coltran/colorizer"" , can you please help to debug what can be the problem,do i need to modify configs/colorizer.py?",yes checked trained model inside main please help problem need modify,issue,positive,positive,positive,positive,positive,positive
1066119802,"This line indicates that you are not loading the correct checkpoint directory.

```
 I0312 17:01:44.065676 140209677809472 train_utils.py:185] Restoring from /tmp/svt
```

Are your flags passed correctly? logdir inside the script should point to coltran_coltran/coltran/colorize",line loading correct directory correctly inside script point,issue,negative,neutral,neutral,neutral,neutral,neutral
1065851704,"I didn't find the official one, but this [repo](https://github.com/AmrMKayid/nam) and this [library](https://interpret.ml/docs/ebm.html) might help",find official one library might help,issue,negative,neutral,neutral,neutral,neutral,neutral
1065496698,"suspect this is due to breaking changes in jax

would be v helpful if there were some degree of version pinning here https://github.com/google-research/google-research/blob/master/ipagnn/requirements.txt",suspect due breaking would helpful degree version pinning,issue,negative,negative,negative,negative,negative,negative
1064284217,"In the middle of February PeepholeLSTMCell was removed from keras api, so if you need 
tf_nightly then you have to set ""--use_peepholes 0"", else use older version of tf, for example: 
```
pip install tensorflow==2.8.0
```",middle removed need set else use older version example pip install,issue,negative,positive,neutral,neutral,positive,positive
1063731715,"> (pip install tf_nightly

hi the lastest tf_nightly can't work, my bug is:
`AttributeError: module 'keras.api._v2.keras.experimental' has no attribute 'PeepholeLSTMCell'
`",pip install hi ca work bug module attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
1061651661,"1. the value of FLAGS.samples equals 5000
2. skip_samples can be anything greater than 5000.
3. batch_size can be anything and dependent on the memory.

I generate 5000 colored images conditioned on the first 5000 grayscale images.
Greater than 5000 images are skipped from the real data, to ensure there is no overlap between the ground truth and generated images. Does that help?",value anything greater anything dependent memory generate colored conditioned first greater real data ensure overlap ground truth help,issue,positive,positive,positive,positive,positive,positive
1061634536,"Thanks for replying!
I wonder how many samples are skipped in real_dataset, i.e., 1) the value of FLAGS.samples, 2) skip_samples 3)FLAGS.batch_size in your scrips.

And also, I want to verify that if the above fid is calculated between a: gen_data the whole validation set of ImageNet and b: the real_data for skip some samples ?  ",thanks wonder many value also want verify fid calculated whole validation set skip,issue,positive,positive,positive,positive,positive,positive
1061515815,"Thanks for your interest in our paper. Here is the FID Script that I used, you can just use a .skip call in the dataset iterator.

```
def normalize(x):
  # inception checkpoints expects inputs to be in [-1, 1]
  # https://codesearch.corp.google.com/piper///depot/google3/third_party/py/tensorflow_gan/examples/cifar/eval_lib.py?dr=CSs&g=0&l=34.
  # https://codesearch.corp.google.com/piper///depot/google3/third_party/py/tensorflow_gan/examples/cifar/data_provider.py?dr=CSs&g=0&l=40
  x = tf.squeeze(x['image'], axis=0)
  logging.info(x.shape)
  x = tf.to_float(x)
  # Normalize from [0, 255] to [-1.0, 1.0]
  x = (x / 128.0) - 1.0
  return x


# Real dataset.
real_dataset = datasets.get_dataset(
    name=FLAGS.dataset, subset='test', config=config, batch_size=1)
real_dataset = real_dataset.map(normalize, num_parallel_calls=100)
real_dataset = real_dataset.skip(FLAGS.samples)
real_dataset = real_dataset.batch(batch_size=FLAGS.batch_size)
real_dataset = real_dataset.skip(skip_samples // FLAGS.batch_size)
real_iterator = tf.compat.v1.data.make_initializable_iterator(real_dataset)
real_dataset = real_iterator.get_next()

gen_dataset = datasets.get_dataset(
    name=FLAGS.dataset, subset='test', config=config, batch_size=1)
gen_dataset = gen_dataset.map(normalize, num_parallel_calls=100)
gen_dataset = gen_dataset.batch(batch_size=FLAGS.batch_size)
gen_iterator = tf.compat.v1.data.make_initializable_iterator(gen_dataset)
gen_dataset = gen_iterator.get_next()

fid_stream = tfgan.eval.frechet_inception_distance_streaming
distance, update_op = fid_stream(real_dataset, gen_dataset)
logging.info(distance)
logging.info(update_op)

batch_size = FLAGS.batch_size
with tf.Session() as sess:
  init_ops = ([real_iterator.initializer, gen_iterator.initializer,
               tf.initialize_local_variables()])
  sess.run(init_ops)

  for epoch in range(1, num_epochs + 1):
    sess.run(update_op)

    if epoch % 10 == 0:
      dist_np = sess.run(distance)
      fid_str = f'Number of samples: {epoch * batch_size}, fid: {dist_np}'
      logging.info(fid_str)
  distance_np = sess.run(distance)
  logging.info(distance_np)
```",thanks interest paper fid script used use call normalize inception normalize return real normalize normalize distance distance sess epoch range epoch distance epoch fid distance,issue,positive,positive,positive,positive,positive,positive
1060827842,"Hello,

I'd also be interested in the code. Could you provide a link?",hello also interested code could provide link,issue,negative,positive,positive,positive,positive,positive
1060108811,"Okay, so some digging around I find that reshaping works best.

(Batch, Width, Height, Channel) -> (Batch, Width\*Height\*Channel)",digging around find work best batch width height channel batch channel,issue,positive,positive,positive,positive,positive,positive
1059947837,"Hello, 

I also have a problem while getting the attention weights from the model:
![image](https://user-images.githubusercontent.com/68455198/156921673-e407c37c-7949-4fcf-a9a2-590130d961a5.png)

I already have a model:
tmp/tft_outputs/saved_models/volatility/fixed/TemporalFusionTransformer.ckpt
which is produced by the commend:
!python3 -m script_train_fixed_params $EXPT $OUTPUT_FOLDER $USE_GPU

Anyone can help on this?

Thanks,
Erica
",hello also problem getting attention model image already model produced commend python anyone help thanks,issue,positive,positive,positive,positive,positive,positive
1059944036,"Never mind, it was a minor mistake. I had to select the correct options when I ran `./configure.sh`. For example, `pip` installed tensorflow as `tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl` so I was supposed to choose `manylinux2010` but I had selected the other option. ",never mind minor mistake select correct ran example pip supposed choose selected option,issue,negative,negative,neutral,neutral,negative,negative
1059931638,"> @HRLTY Hi, I got the point on the error. When i created the tf3d/ops folder inside there were only the files copied with the cp 
> ...
> Execution platform: @local_execution_config_platform//:platform
> /usr/bin/ld.gold: error: cannot find python/_pywrap_tensorflow_internal.so
> collect2: error: ld returned 1 exit status
> Target //:sparse_conv_ops_py_test failed to build
> INFO: Elapsed time: 273.785s, Critical Path: 61.35s
> INFO: 77 processes: 77 local.
> FAILED: Build did NOT complete successfully
> FAILED: Build did NOT complete successfully
> ```

@FrancescoMandru could you please share how you had fixed this error. I face the exact same issue. I am already using docker for the bazel build step as recommended in the installation guide. I do not get the `404 not found` error. I don't know if  it matters, but my host is a Ubuntu-20.04 machine. I attached my error log below.
```
root@a7eaf9f3f356:~/ops# bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures
WARNING: /root/.cache/bazel/_bazel_root/3627e55338facf6d3159a1187ba75fa2/external/local_config_tf/BUILD:5674:1: target 'libtensorflow_framework.so' is both a rule and a file; please choose another name for the rule
INFO: Analyzed target //:sparse_conv_ops_py_test (45 packages loaded, 1049 targets configured).
INFO: Found 1 target...
INFO: From Compiling submanifold_sparse_conv_utils.cc:
In file included from submanifold_sparse_conv_utils.cc:15:0:
submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
INFO: From Compiling submanifold_sparse_conv_grad_launcher.cc:
In file included from submanifold_sparse_conv_grad_launcher.cc:17:0:
submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
INFO: From Compiling submanifold_sparse_conv_launcher.cc:
In file included from submanifold_sparse_conv_launcher.cc:19:0:
submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
INFO: From Compiling submanifold_sparse_conv_ops.cc:
In file included from submanifold_sparse_conv_ops.cc:21:0:
submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
INFO: From Compiling submanifold_sparse_conv_grad.cc:
In file included from submanifold_sparse_conv_grad.cc:22:0:
submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
ERROR: /root/ops/BUILD:161:1: Linking of rule '//:tensorflow_sparse_conv_ops/_sparse_conv_ops.so' failed (Exit 1): gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/3627e55338facf6d3159a1187ba75fa2/execroot/__main__ && \
  exec env - \
    PATH=/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF_HEADER_DIR=/usr/local/lib/python3.6/dist-packages/tensorflow/include \
    TF_NEED_CUDA=0 \
    TF_SHARED_LIBRARY_DIR=/usr/local/lib/python3.6/dist-packages/tensorflow \
    TF_SHARED_LIBRARY_NAME=libtensorflow_framework.so.2 \
  /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow_sparse_conv_ops/_sparse_conv_ops.so-2.params)
Execution platform: @local_execution_config_platform//:platform
/usr/bin/ld.gold: error: cannot find python/_pywrap_tensorflow_internal.so
collect2: error: ld returned 1 exit status
Target //:sparse_conv_ops_py_test failed to build
INFO: Elapsed time: 112.424s, Critical Path: 37.90s
INFO: 77 processes: 77 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
Any help is much appreciated!! @HRLTY @afathi3",hi got point error folder inside copied execution platform platform error find collect error returned exit status target build time critical path local build complete successfully build complete successfully could please share fixed error face exact issue already docker build step installation guide get found error know host machine attached error log root run warning target rule file please choose another name rule target loaded found target file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll error linking rule exit error command execution platform platform error find collect error returned exit status target build time critical path local build complete successfully build complete successfully help much,issue,negative,positive,positive,positive,positive,positive
1059403665,Awesome! Appreciate the quick response! ,awesome appreciate quick response,issue,positive,positive,positive,positive,positive,positive
1059389914,"@bdytx5 please pull the lates version of kws_streaming: I added conversion of model (with internal state streaming stored in saved_model) to tflite.

There is no need to retrain your model. Folder with training artifacts should have a subfolder named ""stream_state_internal"". It contains model streaming with internal state stored in saved_model format.
You can convert it to tflite with below command:
```
python -m kws_streaming.train.convert \
--saved_model_path=/tmp/ds_tc_resnet/stream_state_internal \
--tflite_model_path=/tmp/ds_tc_resnet/stream_state_internal/model_stream.tflite
```
I use above command because kws_streaming uses session mode (for backward compatibility), but conversion of saved_model to tflite has to be done in eager mode.
",please pull version added conversion model internal state streaming need retrain model folder training model streaming internal state format convert command python use command session mode backward compatibility conversion done eager mode,issue,positive,neutral,neutral,neutral,neutral,neutral
1058302364,"@bdytx5 TFLite did not suport streaming with internal state in the past (when kws_streaming was open sourced), but it does support it now. So you can convert any streaming models to TFLite. I will add conversion later.",streaming internal state past open support convert streaming add conversion later,issue,negative,negative,neutral,neutral,negative,negative
1057748883,"@FUJI-W It is not an essential solution, but it could be deduced by downsampling the input image.
The resolution criteria at that time were not clear, and 1080/720 could be inferred, but not 720/480, so experimental downsampling was necessary.",essential solution could input image resolution criterion time clear could experimental necessary,issue,positive,positive,neutral,neutral,positive,positive
1057728042,"I tried HITNET with GPU: Nvidia GeForce RTX 3060, Memory Size : 6GB. 
And I get the same error. Have you solved it now? 
",tried memory size get error,issue,negative,neutral,neutral,neutral,neutral,neutral
1056441784,"try this:


import tensorflow.compat.v1 as tf
 





------------------&nbsp;原始邮件&nbsp;------------------
发件人: ""Kamel ***@***.***&gt;; 
发送时间: 2022年3月2日(星期三) 下午3:15
收件人: ***@***.***&gt;; 
抄送: ***@***.***&gt;; 
主题: Re: [google-research/google-research] Instructions to load and use CuBERT pretrained and fine-tuned models (#571)





 
Hi @DNGros ,
 
I tried to run cubert on TensorFlow 2, and I am getting the error below. TensorFlow 2 do not support the contrib. Should I run cubert on TensorFlow 1? or any sugestion?
 
from tensorflow.contrib import layers as contrib_layers ModuleNotFoundError: No module named 'tensorflow.contrib'
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
Triage notifications on the go with GitHub Mobile for iOS or Android. 
You are receiving this because you are subscribed to this thread.Message ID: ***@***.***&gt;",try import load use hi tried run getting error support run import module reply directly view triage go mobile android id,issue,negative,positive,neutral,neutral,positive,positive
1056431390,"Hi @DNGros ,

I tried to run cubert on TensorFlow 2, and I am getting the error below. TensorFlow 2 do not support the contrib. Should I run cubert on TensorFlow 1? or any sugestion?

`from tensorflow.contrib import layers as contrib_layers
ModuleNotFoundError: No module named 'tensorflow.contrib'`",hi tried run getting error support run import module,issue,negative,neutral,neutral,neutral,neutral,neutral
1050999256,"For anyone who has similar problems, another solution was just downgrading tensorflow to 2.3.0. The problem was that even if I change every c++ compiler to 14 when building and it builds, if the code was wrote considering tensorflow 2.3.0, it is dependent on c++11. So it might have problems later when running the python file. Of course you would have to downgrade other things to match it. I used gcc 8.4.0 and cuda 10.2.",anyone similar another solution problem even change every compiler building code wrote considering dependent might later running python file course would downgrade match used,issue,negative,neutral,neutral,neutral,neutral,neutral
1050515852,"Hi @khaghanijavad 

We did a pytorch reimplementation of this repo and I believe it does not have the issue as mentioned. You may be interested in checking it and have a discussion with us. Thanks!
https://github.com/bolianchen/pytorch_depth_from_videos_in_the_wild

Best,
Bolian",hi believe issue may interested discussion u thanks best,issue,positive,positive,positive,positive,positive,positive
1050515005,"Hi @adizhol 

We acknowledged what you mentioned and add --use_weighted_l1 option to only average over effective pixels. You may be interested to check the repo: 
https://github.com/bolianchen/pytorch_depth_from_videos_in_the_wild

        self.parser.add_argument('--use_weighted_l1',
                                 action = 'store_true',
                                 help='use weighted_l1 error which would not'
                                      ' include the masked out regions into '
                                      'the denominator')
Best,
Bolian",hi acknowledged add option average effective may interested check action error would include masked denominator best,issue,positive,positive,positive,positive,positive,positive
1050512919,"Hi @hcv1027 

We did a pytorch reimplementation of this repo. You may be interested in checking it and have discussion with us. Thanks!
https://github.com/bolianchen/pytorch_depth_from_videos_in_the_wild

Best,
Bolian",hi may interested discussion u thanks best,issue,positive,positive,positive,positive,positive,positive
1049076910,"Well I was building it for a github repo, which is this
https://github.com/MiguelMonteiro/CRFasRNNLayer
and the build completed with a bunch or warnings but without error. I haven't ran any code yet. If it creates another error I'll be devastated :(",well building build bunch without error ran code yet another error,issue,positive,neutral,neutral,neutral,neutral,neutral
1049051422,Well the problem was indeed so.2 and the system manager just created a soft link to .so from .so.2 and it solved the problem.,well problem indeed system manager soft link problem,issue,negative,positive,neutral,neutral,positive,positive
1049022325,I will once I solve the problem. Thank you for all the help!,solve problem thank help,issue,positive,neutral,neutral,neutral,neutral,neutral
1049021542,"Hope you will be able to solve the problem. If you can, please can you describe here what you did? May help other people",hope able solve problem please describe may help people,issue,positive,positive,positive,positive,positive,positive
1049016450,"I did try downloading the libtensorflow_framework.so from the official site and and adding it to LD_LIBRARY_PATH along with CMAKE_LIBRARY_PATH, which didn't seem to work. I haven't tried adding that particular directory, so might as well try it. Thank you!",try official site along seem work tried particular directory might well try thank,issue,positive,positive,positive,positive,positive,positive
1049005625,Yes. But `libtensorflow_framework.so` file does not exist in that directory. Only `libtensorflow_framework.so.2`,yes file exist directory,issue,negative,neutral,neutral,neutral,neutral,neutral
1049002823,"I would think that the link path (where it has the `so.2`) is the culprit here. You should check that part. But it is a bit odd at the same time, Does that file, `libtensorflow_framework.so.2` exist in the path `/N/soft/rhel7/deeplearning/Python-3.9.8/lib/python3.9/site-packages/tensorflow/`? ",would think link path culprit check part bit odd time file exist path,issue,negative,negative,negative,negative,negative,negative
1048977860,"```
In [2]: print("" "".join(tf.sysconfig.get_compile_flags()))
-I/N/soft/rhel7/deeplearning/Python-3.9.8/lib/python3.9/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0 -DEIGEN_MAX_ALIGN_BYTES=64

In [3]: print("" "".join(tf.sysconfig.get_link_flags()))
-L/N/soft/rhel7/deeplearning/Python-3.9.8/lib/python3.9/site-packages/tensorflow -l:libtensorflow_framework.so.2
```

It gives this code. Do you think it might be a problem from using libtensorflow_framework.so.2 instead of libtensorflow_framework.so ? I don't have permission to create a soft link so might have to talk with the system manager if that's the case.",print print code think might problem instead permission create soft link might talk system manager case,issue,negative,positive,neutral,neutral,positive,positive
1048951388,"if you are failing to find that means either compile flags or link flags were not found. What happens when you do the following? (from a python REPL)

```
import tensorflow as tf
print("" "".join(tf.sysconfig.get_compile_flags()))
print("" "".join(tf.sysconfig.get_link_flags()))
```
What do they print?",failing find either compile link found following python import print print print,issue,negative,neutral,neutral,neutral,neutral,neutral
1048816873,"Well first I was using the wrong g++ version. Then, I changed my code to use c++14, now it's a whole new error of failing to find libtensorflow.so. It's just harder since we're not on a personal machine but a server from an institution. I was just wondering because that server also uses Red Hat Fedora, and I might be just headbanging to a wrong wall.",well first wrong version code use whole new error failing find harder since personal machine server institution wondering server also red hat might wrong wall,issue,negative,negative,neutral,neutral,negative,negative
1048447469,"I don't think it's the -std=c++14 flag issue, as I don't have that std=c++14 error on my side. Thank you for the suggestions though! @rcshubhadeep ",think flag issue error side thank though,issue,negative,neutral,neutral,neutral,neutral,neutral
1048442887,"@pjsjongsung try to compile it in a Colab instance or something like that. Usually they have updated versions of the libs and gcc

```
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
g++ -std=c++14 -shared neural_trees_ops.cc neural_trees_kernels.cc neural_trees_helpers.cc -o neural_trees_ops.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2
```
This command above worked for me. Notice that the difference from the command given in the `README` is that I have substituted `c++11` with `c++14` I guess TF has implemented few things that does not have backward compatibility. Then, I am not a TF expert in any sense. When I will use the ideas I will use this code-base just as a basis to translate this entire thing to PyTorch.",try compile instance something like usually python print python print command worked notice difference command given substituted guess backward compatibility expert sense use use basis translate entire thing,issue,negative,negative,negative,negative,negative,negative
1048071055,@rcshubhadeep How did you work around the first problem? I am trying to compile on a system that uses RH Fedora too. It seems to be causing the problem for me.,work around first problem trying compile system causing problem,issue,negative,positive,positive,positive,positive,positive
1042680275,"I think the error is come from line 513, since ""oil"" missing data on date ""2015-01-03"" ...
so when try ""oil.loc[]"" on these days, it raise KeyError
![image](https://user-images.githubusercontent.com/22293352/154432910-6241a3a2-814a-4c3c-bead-e610786fc35d.png)
",think error come line since oil missing data date try day raise image,issue,negative,negative,negative,negative,negative,negative
1042646811,"Yes, Thanks. 
I feel stupid for not checking that before. 🤦‍♂️😂",yes thanks feel stupid,issue,negative,negative,negative,negative,negative,negative
1042622860,"This is my `coltran/configs/spatial_upsampler.py`
Seems the batch_size is 1

```
# coding=utf-8
# Copyright 2021 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Test configurations for color upsampler.""""""
from ml_collections import ConfigDict


def get_config():
  """"""Experiment configuration.""""""
  config = ConfigDict()

  # Data.
  config.dataset = 'imagenet'
  config.downsample = True
  config.downsample_res = 64
  config.resolution = [256, 256]
  config.random_channel = True

  # Training.
  config.batch_size = 1
  config.max_train_steps = 300000
  config.save_checkpoint_secs = 900
  config.num_epochs = -1
  config.polyak_decay = 0.999
  config.eval_num_examples = 20000
  config.eval_batch_size = 16
  config.eval_checkpoint_wait_secs = -1

  config.optimizer = ConfigDict()
  config.optimizer.type = 'rmsprop'
  config.optimizer.learning_rate = 3e-4

  # Model.
  config.model = ConfigDict()
  config.model.hidden_size = 512
  config.model.ff_size = 512
  config.model.num_heads = 4
  config.model.num_encoder_layers = 3
  config.model.resolution = [64, 64]
  config.model.name = 'spatial_upsampler'

  config.sample = ConfigDict()
  config.sample.gen_data_dir = ''
  config.sample.log_dir = 'samples_sweep'
  config.sample.batch_size = 1
  config.sample.mode = 'argmax'
  config.sample.num_samples = 1
  config.sample.num_outputs = 1
  config.sample.skip_batches = 0
  config.sample.gen_file = 'gen0'

  return config
```",copyright research licensed apache license version license may use file except compliance license may obtain copy license unless applicable law agreed writing distributed license distributed basis without kind either express see license specific language governing license test color import experiment configuration data true true training model return,issue,positive,positive,positive,positive,positive,positive
1042620629,"try setting batch size=1, in the spatial upsampler config?",try setting batch spatial,issue,negative,neutral,neutral,neutral,neutral,neutral
1042616395,"I custom trained the 3 models, `Colorizer`, `Color Upsampler` and `Spatial Upsampler` for a custom dataset.
Then used the `custom_colorize` script to get the results.
The First 2 stages went smooth, got the following error for the 3rd `Spatial Upsampler` step.

I used this command
```
!python -m coltran.custom_colorize --config=coltran/configs/spatial_upsampler.py \
--logdir=$SPATIAL_UPSMPLR_LOGDIR --img_dir=$IMG_DIR --store_dir=$STORE_DIR \
--gen_data_dir=$STORE_DIR/stage2 --mode=$MODE
``` 

I am using google colab. 
Is this because of colab GPU limits?
Any help on how to solve this issue?



```
2022-02-16 08:20:45.489115: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/moving_averages.py:548: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0216 08:20:51.389240 140616530913152 deprecation.py:343] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/moving_averages.py:548: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0216 08:20:52.797949 140616530913152 train_utils.py:91] Built with exponential moving average.
I0216 08:20:52.813167 140616530913152 train_utils.py:185] Restoring from /content/drive/MyDrive/Colab_Work/HONORS/coltran-v3/coltran-cityscapes-v2-finetune-3/google-research/coltran/logs/cityscapes_ft_spatial_upsampler.
I0216 08:20:56.561913 140616530913152 custom_colorize.py:207] Producing sample after 37600 training steps.
I0216 08:20:56.562508 140616530913152 custom_colorize.py:210] 100
2022-02-16 08:21:08.242997: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.25GiB (rounded to 1342177280)requested by op Softmax
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2022-02-16 08:21:08.243377: W tensorflow/core/common_runtime/bfc_allocator.cc:474] *__**_****_*******________********************************************_______*************__________
2022-02-16 08:21:08.246940: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at softmax_op_gpu.cu.cc:219 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[5,256,256,4,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/content/drive/MyDrive/Colab_Work/HONORS/coltran-v3/coltran-cityscapes-v2-finetune-3/google-research/coltran/custom_colorize.py"", line 244, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/content/drive/MyDrive/Colab_Work/HONORS/coltran-v3/coltran-cityscapes-v2-finetune-3/google-research/coltran/custom_colorize.py"", line 227, in main
    out = model.sample(gray_cond=gray, inputs=prev_gen, mode='argmax')
  File ""/content/drive/MyDrive/Colab_Work/HONORS/coltran-v3/coltran-cityscapes-v2-finetune-3/google-research/coltran/models/upsampler.py"", line 254, in sample
    logits = self.upsampler(inputs, gray_cond, training=False)
  File ""/content/drive/MyDrive/Colab_Work/HONORS/coltran-v3/coltran-cityscapes-v2-finetune-3/google-research/coltran/models/upsampler.py"", line 245, in upsampler
    context = self.encoder(channel, training=training)
  File ""/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/content/drive/MyDrive/Colab_Work/HONORS/coltran-v3/coltran-cityscapes-v2-finetune-3/google-research/coltran/models/layers.py"", line 668, in call
    output = layer(inputs)
  File ""/content/drive/MyDrive/Colab_Work/HONORS/coltran-v3/coltran-cityscapes-v2-finetune-3/google-research/coltran/models/layers.py"", line 611, in call
    weights = tf.nn.softmax(alphas)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling layer ""self_attention_nd"" (type SelfAttentionND).

OOM when allocating tensor with shape[5,256,256,4,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Softmax]

Call arguments received:
  • inputs=tf.Tensor(shape=(5, 256, 256, 512), dtype=float32)
CPU times: user 294 ms, sys: 59.3 ms, total: 353 ms
Wall time: 47.6 s
```",custom trained color spatial custom used script get first went smooth got following error spatial step used command python mode help solve issue setting environment variable set original value warning removed future version use automatically eager graph inside removed future version use automatically eager graph inside built exponential moving average sample training allocator ran memory trying allocate rounded cause memory fragmentation maybe environment variable improve situation current allocation summary current allocation summary tensor shape type float allocator recent call last file line file line code file line module main file line run main file line main file line main file line sample file line context channel file line raise none file line call output layer file line call exception calling layer type tensor shape type float allocator call received time user total wall time,issue,positive,positive,neutral,neutral,positive,positive
1041458628,"That sounds great - yes exactly I meant a spreadsheet with all the evaluation results and maybe also the computational complexity (speed). Think we could easily extract the important numbers for each model checkpoint and add it to the model card which should be very important information for people wondering which of the 160 checkpoints to use. 

If you could share some kind of spreadsheet / table this would be super nice",great yes exactly meant evaluation maybe also computational complexity speed think could easily extract important model add model card important information people wondering use could share kind table would super nice,issue,positive,positive,positive,positive,positive,positive
1041421542,"Hey! @patrickvonplaten sorry for the delayed response. Awesome work! thanks a lot :) Model cards look excellent.

About the last question. Do you mean to release a table or spreadsheet of some sort that reproduce the results on the graphs in the paper? If so I can share that too, along with maybe a colab to play with it. Lmk if that would be helpful.  

I didn't really have time to look at the shared configs (i trained them almost a year or so actually) yet. Will ping back when I get the time. Do let me know if there's also any issues along the way or requests that you think could help. 
",hey sorry response awesome work thanks lot model look excellent last question mean release table sort reproduce paper share along maybe play would helpful really time look trained almost year actually yet ping back get time let know also along way think could help,issue,positive,positive,positive,positive,positive,positive
1040149002,"@vanzytay - we've finalized the port of all model checkpoints except **SH** and **SKV**. We've also converted each checkpoint to PyTorch and JAX/Flax besides TF and added a model card. You can see all checkpoints here: 
https://huggingface.co/models?other=deep-narrow (We'll probably move the checkpoints soon to the google organization).

Does that sound good to you? Do the model cards look good to you? 

Also, do you have a table where one could retrieve the downstream performance of a specific checkpoint and the computational complexity by any chance? The graphs in the paper are super nice, but it's difficult to retrieve the exact numbers of a checkpoint. I think this could be very useful for the users :-)",port model except sh also converted besides added model card see probably move soon organization sound good model look good also table one could retrieve downstream performance specific computational complexity chance paper super nice difficult retrieve exact think could useful,issue,positive,positive,positive,positive,positive,positive
1039662072,"Hi @hassanhub @Adam-fei , sorry for the late update. I have tried latest hmdb51 tfrecords with the following cmd for the fine-tune step, and I got a different error here:
             
        python -m vatt.main --task=finetune --mode=train --model_dir=pretrain_model\ --model_arch=Vit_Large --strategy_type=mirrored
        ....
        ....
        ValueError: Inputs must be an iterable of at least one Tensor/IndexedSlices with the same dtype and shape.

P.S: I'm using one GPU, so I modified the [line 97](https://github.com/google-research/google-research/blob/master/vatt/experiments/base.py#L97) using ```tf.distribute.mirrorstrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())``` based on [this post](https://github.com/tensorflow/tensorflow/issues/33656).
      ",hi sorry late update tried latest following step got different error python must iterable least one shape one line based post,issue,negative,negative,negative,negative,negative,negative
1039276328,"Thanks a lot for the quick answer! I will then go ahead and publish the checkpoints without the **SH** and **SKV** ones :-) 

One last question (sorry for the spam). 
I couldn't really figure out what the folder: `bi_v1_bs_law_03-18-08-28/` corresponded to? Can I just ignore this one? ",thanks lot quick answer go ahead publish without sh one last question sorry could really figure folder ignore one,issue,negative,positive,neutral,neutral,positive,positive
1036741135,"Hi daidedou,

""So I would find it weird to align 2 videos that aren't similar (like pouring/tennis serve)."" You are right it would be weird to align those videos. In our [sampling code](https://github.com/google-research/google-research/blob/master/tcc/datasets.py#L355) we always sample videos of the same class in one batch. That is in a single batch you would only have videos of one class. Hence, aligning them is fine. We do so by creating per-action datasets and sampling exactly batch_size number of elements from each dataset. In the experiments conducted in the paper, we trained a separate model for each action. Hence, we never aligned videos of different classes. Hope this clears things up. ",hi would find weird align similar like serve right would weird align sampling code always sample class one batch single batch would one class hence fine sampling exactly number paper trained separate model action hence never different class hope,issue,negative,negative,neutral,neutral,negative,negative
1036012812,"Since I failed to attach files, I copy the modified code here.

I mainly modified the `def replicated_train_step()`, `def create_train_step()` in `base.py` and `def _calculate_nce()` in `objectives.py` and code about distributed setting.
```
def replicated_train_step(self, model):
    """"""Constructs an op that runs one train step on a replica.""""""

    optimizer = model.optimizer
    trainable_variables = model.trainable_variables
    gradient_clip_norm = self.params.train.gradient_clip_norm
 
    # inputs, labels = self.prepare_inputs(inputs)
    inputs = {'video': tf.random.normal((4, 32, 224, 224, 3)), 'audio': tf.random.normal((4, 153600, 1)), 'text':tf.ones((4, 16))}
    labels = {'audio_mask': tf.random.normal((4,1)), 'text_mask': tf.random.normal((4,1))}

    with tf.GradientTape() as tape:
      outputs = model(inputs, training=True)
      all_losses = model.loss_fn(labels=labels,
                                 outputs=outputs)
      losses = {}
      for k, v in all_losses.items():
        losses[k] = tf.reduce_mean(v)
        losses[k] = tf.where(tf.math.is_nan(v), 0., v)

      per_replica_loss = losses['total_loss']
      print(per_replica_loss)
      grads = tf.gradients(per_replica_loss, trainable_variables)
      grads = [tf.where(tf.math.is_nan(g), 0., g) for g in grads]
      if gradient_clip_norm > 0:
        grads, _ = tf.clip_by_global_norm(grads, gradient_clip_norm)
      optimizer.apply_gradients(zip(grads, trainable_variables))

    return losses
```
```
  def create_train_step(self):
    """"""Constructs an op that runs a train step and returns the metrics.""""""

    model = self.model

    @tf.function
    def train_step(iterator, num_iterations):
      
      for iter in range(num_iterations):
        # inputs = next(iterator)
        metrics = self.replicated_train_step(model)
      
      return metrics

    return train_step
```

```
  def _calculate_nce(self,
                     embed_1,
                     embed_2,
                     embed_2_mask=None):
    """"""Calculate NCE / MIL-NCE loss between embed_1 and embed_2.""""""
    batch_size = embed_1.shape.as_list()[0]
    x = self._calculate_similarity(embed_1, embed_2, batch_size, batch_size, self._temperature)
    id_m = tf.eye(batch_size)  # [B, B]
    sim_pos = tf.reduce_sum(x * id_m[:, :, None], axis=1)
    sim_all = tf.concat([x, tf.transpose(x, perm=[1, 0, 2])], axis=1)
    sim_all = tf.reshape(sim_all, [batch_size, -1])
    # print(x, sim_pos, sim_all)
    # Compute the log sum exp (numerator) of the NCE loss.
    logsumexp_pos = tf.reduce_logsumexp(sim_pos, axis=1)
    # Compute the log sum exp (denominator) of the NCE loss.
    logsumexp_all = tf.reduce_logsumexp(sim_all, axis=1)
    # print(logsumexp_pos, logsumexp_all)
    # Compute the loss.
    loss = logsumexp_all - logsumexp_pos
    # print('loss:', loss)
    
    # If there are no valid examples, logsumexp_all could be a NaN.
    loss = tf.where(tf.math.is_nan(loss),
                    tf.zeros(loss.shape),
                    tf.identity(loss))
    # Average over batch samples.
    loss = tf.reduce_mean(loss)
    
    return loss

```
When I use the data generated from dmvr, it always prints the same error just as in the title. I change it to pre-defined inputs to test:
```
inputs = {'video': tf.random.normal((4, 32, 224, 224, 3)), 'audio': tf.random.normal((4, 153600, 1)), 'text':tf.ones((4, 16))}
    labels = {'audio_mask': tf.random.normal((4,1)), 'text_mask': tf.random.normal((4,1))}
```
Then it works, the training experiment is running.

I guess there must be errors somewhere and I can't successfully generate correct inputs from any datasets. 

Do you have any idea for the error?

Thanks.
",since attach copy code mainly code distributed setting self model one train step replica tape model print zip return self train step metric model iter range next metric model return metric return self calculate loss none print compute log sum numerator loss compute log sum denominator loss print compute loss loss print loss valid could nan loss loss loss average batch loss loss return loss use data always error title change test work training experiment running guess must somewhere ca successfully generate correct idea error thanks,issue,negative,positive,positive,positive,positive,positive
1035941111,"Hi. A big thank-you to @jalayrac. The [hmdb example of dmvr](https://github.com/deepmind/dmvr/blob/master/examples/linear_mmv_hmdb.py) can run successfully.

However, despite using the new version of tfrecord, vatt still output following error:

I was running finetune/test mode using:
```
python -m vatt.main --task=finetune --mode=eval --model_dir=./my_output --model_arch=vit_base --strategy_type=mirrored --override_checkpoint=./Checkpoint/Kinetics-400_BBS/vision_vatt_finetune_vit_base_k400_modality_specific_pretrain_ckpt-100000
```

And following is the error:
```
I0211 14:25:43.123416 139658407814144 video_dataset.py:456] Dataset created successfully
2022-02-11 14:25:50.560914: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 14:25:50.560957: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 14:25:50.564367: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 14:25:50.564423: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 14:25:50.565001: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
```

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.
  (0) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node strided_slice_9}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNext_3]]
	 [[model/video_module/vit_base/spatio_temporal_embeddings/strided_slice_3/_3]]
  (1) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node strided_slice_9}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNext_3]]
  (2) CANCELLED:  Function was cancelled before it was started
0 successful operations.
0 derived errors ignored. [Op:__inference_evaluation_step_21087]

Function call stack:
evaluation_step -> evaluation_step -> evaluation_step
```

What should I do?
@hassanhub 

cc @adaniefei ",hi big example run successfully however despite new version still output following error running mode python following error successfully slice index dimension slice index dimension slice index dimension slice index dimension slice index dimension root error found slice index dimension node slice index dimension node function successful derived function call stack,issue,negative,positive,positive,positive,positive,positive
1035051145,"Hey! I'll need to revisit how mesh did this again (I can't remember at this moment). Will get back in a bit. But from our results, both shared models didn't move the needle at all in terms of pareto optimality so I think they will also have limited practical utility. But for completeness I will get back to the thread soon. ",hey need revisit mesh ca remember moment get back bit move needle think also limited practical utility completeness get back thread soon,issue,negative,negative,neutral,neutral,negative,negative
1034461739,"@braveheartwithlove I guess it is due to
```
model_path + ""logs/train/""
```
It should use os.path.join instead, for compatibility with Windows.
I would try os.path.join, or just plug full file path directly into read_log, as shown below:
```
acc_train1 = read_log(fname_train, tag=""accuracy"")
acc_val1 = read_log(fname_validation, tag=""accuracy"")
```",guess due use instead compatibility would try plug full file path directly shown accuracy accuracy,issue,negative,positive,positive,positive,positive,positive
1034455651,"@rybakov I renamed the log file as you suggested. The file is clearly there.
fname=""C:\\Users\\xxxxx\\Desktop\\google-research\\kws_streaming\\models\\ds_tc_resnet\\logs\\train\\events_out_tfevents.gtc""
I tried this directly:
tf.train.summary_iterator(fname)
It will show the same error as above:
NotFoundError: NewRandomAccessFile failed to Create/Open: C:\Users\xxxxx\Desktop\google-research\kws_streaming\models\ds_tc_resnet\logs\train\events_out_tfevents.gtc : The system cannot find the path specified.
; No such process",log file file clearly tried directly show error system find path process,issue,negative,positive,neutral,neutral,positive,positive
1034362932,"Thanks for letting me know! Could you provide a dataset (e.g., hdmb51 or other small-size datasets) in DMVR format and details of running fine-tuning experiments on it? Based on the current readme, though I could generate DMVR files for my custom dataset, it's still hard to reproduce the finetuning code in a short time. Thanks!",thanks know could provide format running based current though could generate custom still hard reproduce code short time thanks,issue,positive,positive,neutral,neutral,positive,positive
1034319978,"Hi, thanks for your interest in VATT!
We do not intend to add a PyTorch version. However, we will provide a support for JAX very soon.",hi thanks interest intend add version however provide support soon,issue,positive,positive,positive,positive,positive,positive
1034221216,"@braveheartwithlove  it was tested on Ubuntu and it works ok. It looks like on Winbdows it generates log files with different extensions.
Please rename all log files with the new extension "".gtc"" (something like events_out_tfevents.gtc).
In the notebook you should specify path the model folder, for example model_path1 = ""/tmp/ds_tc_resnet/""
Then notebook will append ""logs/train/"" and look for files with "".gtc"" extension, the same with ""logs/validation/"".
The code for all above is included in the notebook, so please feel free to modify it for your needs.
",tested work like log different please rename log new extension something like notebook specify path model folder example notebook append look extension code included notebook please feel free modify need,issue,positive,positive,positive,positive,positive,positive
1033870848,"And **SKV** (tied key-values) means that key and value projection matrices are always tied, no? ",tied key value projection matrix always tied,issue,negative,neutral,neutral,neutral,neutral,neutral
1033865060,"Awesome - thanks for the quick answer. One question - I'm having some problems with the ""shared heads"" **SH** checkpoints when converting them. There is not too much information in the paper about what ""shared"" means here exactly and how the model architecture differs exactly to the non-shared ones. 

Does ""shared heads"" mean that each transformer block has the same q, v, k projection matrices or does it mean that in each bock, each head projection matrix is the same? ",awesome thanks quick answer one question sh converting much information paper exactly model architecture exactly mean transformer block projection matrix mean bock head projection matrix,issue,positive,positive,positive,positive,positive,positive
1033751918,"Hi! 

First of all, thanks for all the effort! We really appreciate it. 

For 1, I've double checked and found that we don't have a checkpoint for that. That folder was perhaps created by accident. So you can ignore (1).

For 2, 3, unfortunately the jobs might have been accidentally killed before they could finish pretraining and was only finished up till 300k steps. At a later time, we could retrain them. For now, it would be awesome if there is a way to mark them as 300k (or something similar). 

Thanks a lot! ",hi first thanks effort really appreciate double checked found folder perhaps accident ignore unfortunately might accidentally could finish pretraining finished till later time could retrain would awesome way mark something similar thanks lot,issue,positive,positive,positive,positive,positive,positive
1032310971,"NotFoundError: NewRandomAccessFile failed to Create/Open: C:\Users\xxxxx\Desktop\google-research\kws_streaming\models\ds_tc_resnet\logs\train\events.out.tfevents.1644273734.NA51000227 : The system cannot find the path specified.
; No such process

But the log file is actually there.",na system find path process log file actually,issue,negative,neutral,neutral,neutral,neutral,neutral
1031851403,The changes have been already incorporated into the current version.,already incorporated current version,issue,negative,neutral,neutral,neutral,neutral,neutral
1031565872,"Hi, was anyone able to successfully generate AndroidHowTo and RicoSCA?

I am running into issues with the preprocessed data where the data generation scripts seem to work but the files must be wrong or empty because the model loss isn't decreasing during training.",hi anyone able successfully generate running data data generation seem work must wrong empty model loss decreasing training,issue,negative,positive,positive,positive,positive,positive
1031019797,"> Seems like it will work out

For the example above. The actual number of search vectors is 6. But this function does return 7.",like work example actual number search function return,issue,negative,neutral,neutral,neutral,neutral,neutral
1030831577,"Hi @hassanhub,
Thanks for your reply, i have replaced the function with strategy.gather(tensor, axis=0).
However I got the following error instead.
RuntimeError: tf.distribute.Strategy.gather method requires cross-replica context, use get_replica_context().all_gather() instead.
It seems that tensor is not cross-replica context.
Any idea to fix the problem?
Thanks in advance.",hi thanks reply function tensor however got following error instead method context use instead tensor context idea fix problem thanks advance,issue,negative,positive,positive,positive,positive,positive
1030745293,"Hi! Apologies for the late response - you can use the pretrained CPN model, but since the detector is different (current provided pre-trained model is trained using CPN model finetuned on Human3.6M), it may not work as well. We do recommend using a detector that works well in general, but that also requires training Pr-VIPE with the same detector.",hi late response use model since detector different current provided model trained model may work well recommend detector work well general also training detector,issue,positive,negative,neutral,neutral,negative,negative
1029476794,"Training colab uses toy settings (FLAGS.clip_duration_ms = 400), please set FLAGS.clip_duration_ms = 1000 and re-train your model.",training toy please set model,issue,negative,neutral,neutral,neutral,neutral,neutral
1029361849,With batch_size=100 training will report reasonable accuracy in the log you shared,training report reasonable accuracy log,issue,negative,positive,positive,positive,positive,positive
1029342195,"It looks like it is restoring model from previous run, which was probably trained with 12 labels, so cleaning folder should help.",like model previous run probably trained cleaning folder help,issue,positive,negative,negative,negative,negative,negative
1029338521,"Please delete the folder with your model and re-run colab again. I would suggest to use command [line](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_12_labels.md#ds_tc_resnet---based-on-matchboxnet), it will complain if model folder already has a model.",please delete folder model would suggest use command line complain model folder already model,issue,negative,neutral,neutral,neutral,neutral,neutral
1029333507,"@rybakov 
You may see the model summary looks right with 3 classes output. I am wondering where it still stores the classes as 12.

 flatten (Flatten)              (1, 128)             0           ['stream_6[0][0]']               
                                                                                                  
 dense (Dense)                  (1, 3)               387         ['flatten[0][0]']                
                                                                                                  
==================================================================================================
Total params: 70,139
Trainable params: 68,603
Non-trainable params: 1,536


InvalidArgumentError: var and m do not have the same shape[12] [3]
	 [[{{node training/Adam/Adam/update_dense/bias/ResourceApplyAdam}}]]
",may see model summary right class output wondering still class flatten flatten dense dense total trainable shape node,issue,negative,positive,positive,positive,positive,positive
1029331390,"These colabs are used as fast playground, so most parameters are for toy example: e.g. FLAGS.batch_size=1. Please feel free to set it FLAGS.batch_size=100. I would suggest to use command line from [line](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_12_labels.md#ds_tc_resnet---based-on-matchboxnet)",used fast playground toy example please feel free set would suggest use command line line,issue,positive,positive,positive,positive,positive,positive
1029325358,"@rybakov Actually that is what I did. 
![single](https://user-images.githubusercontent.com/17091333/152414382-e23bdf37-c481-43af-a4cd-be089823540e.png)
I also updated the model_params.py.
  def __init__(self):
    # default parameters
    self.data_url = ''
    self.train_dir = ''
    self.wanted_words = 'stop'
",actually single also self default,issue,negative,negative,neutral,neutral,negative,negative
1029307622,"It looks like you set wanted_words in wrong place. 
You should set
```
FLAGS.wanted_words='stop' 
```
before 
```
flags = model_flags.update_flags(FLAGS)
```
update_flags will parse FLAGS and use wanted_words to compute ""label_count"" (silence and unknown labels will be added)",like set wrong place set parse use compute silence unknown added,issue,negative,negative,negative,negative,negative,negative
1028673550,Why the accuracy is 0.00%? I think it should keep increasing close to >95% as shown in your paper. What parameters should I change to reach a performance similar to your paper? Thanks!,accuracy think keep increasing close shown paper change reach performance similar paper thanks,issue,negative,positive,neutral,neutral,positive,positive
1028405093,"Yes, this happens because VATT is optimized for TPU kernels.
If you'd like to bypass this error, you can replace [this function](https://github.com/google-research/google-research/blob/f258e739b6a713c0d965f75535c76ab825a60006/vatt/utils/train/objectives.py#L51) with `strategy.gather(tensor, axis=0)`

OR simply replace [this line](https://github.com/google-research/google-research/blob/f258e739b6a713c0d965f75535c76ab825a60006/vatt/utils/train/objectives.py#L105) with the following:
`modality_1_all = modality_1`
`modality_2_all = modality_2`",yes like bypass error replace function tensor simply replace line following,issue,negative,neutral,neutral,neutral,neutral,neutral
1028222324,"Hi @jalayrac and @hassanhub, I've pulled the latest commit of dmvr, and I'm able to run the [hmdb example](https://github.com/deepmind/dmvr/tree/master/examples) now. The only change I have to do locally is to upgrade cuDNN from 8.0.5 to 8.1.0.

Thanks, @jalayrac for the latest commit, and @hassanhub later this week I will update about the local VATT running using the latest commit from dmvr. :D Thanks again guys!!!",hi latest commit able run example change locally upgrade thanks latest commit later week update local running latest commit thanks,issue,positive,positive,positive,positive,positive,positive
1028155078,"Ok, Figured it out. 

- You need Ubuntu system for compiling it, Amazon Linux is based on RH Fedora. So it was not working. 
- You also need to use c++14 standard otherwise with the newest version of tf it won't compile. 

After doing the above two, it compiled and I could run the demo.py without issues. ",figured need system based working also need use standard otherwise version wo compile two could run without,issue,negative,neutral,neutral,neutral,neutral,neutral
1028147163,"Thank you @jalayrac 
@adaniefei @Adam-fei please let us know how it goes.",thank please let u know go,issue,positive,neutral,neutral,neutral,neutral,neutral
1027228991,We should have fixed this on the DMVR code: https://github.com/deepmind/dmvr/commit/adf78b7a8edd4bb56c6ce1bffaa5003019761716. Could you retry after that commit? Sorry for this!,fixed code could retry commit sorry,issue,negative,negative,negative,negative,negative,negative
1024738746,"Hello @dante1024, thanks for your interest in the flare removal project.

Our code isn't supposed to have an explicit dependency on Keras. Instead, it uses the Keras API inside Tensorflow (namely, `tf.keras.*`). If you have the latest version of TF, you should be fine.

Does that answer your question?

Qiurui",hello thanks interest flare removal project code supposed explicit dependency instead inside namely latest version fine answer question,issue,positive,positive,positive,positive,positive,positive
1020708076,"Thanks, @adaniefei
This is very helpful!
I'll work with DMVR to understand where in the generation file things go wrong. Hopefully we can submit a patch to fix this soon.",thanks helpful work understand generation file go wrong hopefully submit patch fix soon,issue,positive,negative,negative,negative,negative,negative
1020675579,"@hassanhub 
Thanks for the reply! I have checked and found the other two generating tfrecord scripts work well following the [instruction](https://github.com/deepmind/dmvr/tree/master/examples). Then to run the [example](https://github.com/deepmind/dmvr/blob/master/examples/linear_mmv_hmdb.py), I have to modify the [line 48](https://github.com/deepmind/dmvr/blob/master/examples/linear_mmv_hmdb.py#L48) from

     shards = [f'hmdb51_{subset}_{split}-{i:05d}-of-{num_shards:05d}'

to

     shards = [f'{subset}_{split}-{i:05d}-of-{num_shards-1:05d}'

After this, I used the cmd:
     
      python linear_mmv_hmdb.py \
      --data_path=/path/to/hmdb_shards \
      --model_name=s3d \
      --hmdb51_split=1 (instead of **split_1**)

and got a similar INVALID_ARGUMENT error:
     
      ....
      2022-01-24 18:57:42.806099: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 86 but output shape: []
      2022-01-24 18:57:42.816347: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 26 but output shape: []
      2022-01-24 18:57:42.826208: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 35 but output shape: []
      2022-01-24 18:57:42.836729: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 15 but output shape: []
      ....

Also, the script [generate_from_file.py](https://github.com/deepmind/dmvr/blob/master/examples/generate_from_file.py) generates one less tfrecord file than it should have, For instance, the tfrecord files are named in the format of `train_1-xxxxx-of-00059`, while the `xxxxx` here is from `00000` to `00058`.

Hope this feedback helps. :)",thanks reply checked found two generating work well following instruction run example modify line subset split subset split used python instead got similar error name unknown key index number size output shape name unknown key index number size output shape name unknown key index number size output shape name unknown key index number size output shape also script one le file instance format hope feedback,issue,positive,negative,neutral,neutral,negative,negative
1020004542,I regret to announce that I lost the file. I will repost the link here when I have time,regret announce lost file repost link time,issue,negative,neutral,neutral,neutral,neutral,neutral
1019963021,@mhoangvslev Can you please fix the dead colab link?,please fix dead link,issue,negative,negative,negative,negative,negative,negative
1019450020,"Thank you very much! So you suggest to use CPN for the keypoint detection, would it be sufficient to use the pretrained CPN model which they provide in their [repo](https://github.com/chenyilun95/tf-cpn)? Or do I need to finetune it on Human3.6?",thank much suggest use detection would sufficient use model provide need human,issue,negative,positive,neutral,neutral,positive,positive
1018990478,"An update that the model checkpoint with dropout is up: https://sites.google.com/view/pr-vipe/home (see ""Dropout L2-VIPE w/ CPN Keypoints finetuned on Human 3.6M""). Note as Ting mentioned that these are trained on CPN keypoints so using keypoints from other models will not work as well.",update model dropout see dropout human note ting trained work well,issue,negative,neutral,neutral,neutral,neutral,neutral
1018813081,"I just pushed a commit with the 360 scene config and some bug fixes. Hope
that helps!

On Mon, Jan 17, 2022 at 8:55 PM Infinite Reality Solution <
***@***.***> wrote:

> Hi @hedpeter <https://github.com/hedpeter> ,
>
> Thank your sharing of release code.
> Btw can you share the llff config for 360 scene?
> I tried to use your shared llff config for 360 captured scene, and it
> won't work.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/948>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ATMA43P24U7SKZONVOBTWHLUWR65BANCNFSM5MFSB2KQ>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",commit scene bug hope mon infinite reality solution wrote hi thank release code share scene tried use scene wo work reply directly view triage go mobile android id,issue,positive,positive,neutral,neutral,positive,positive
1018332599,"I see, now everything is clear. Thanks a lot, your answer helped a lot! And thank you for your great work, it is amazing!
Have a nice day :) ",see everything clear thanks lot answer lot thank great work amazing nice day,issue,positive,positive,positive,positive,positive,positive
1018221893,"No problem at all. Thanks for your interest in our work!

Regarding keypoint normalization, sorry about the confusion. The bottom line is that the input 2D keypoints to [input_generator.create_model_input](https://github.com/google-research/google-research/blob/d220e37d33a96c021af3cd2a8e5df25bfc9db8bf/poem/core/input_generator.py#L365) should preserve the original aspect ratio (e.g., keypoint coordinates in the original image). The `denormalization` was included simply because our data pipeline assumes the 2D keypoints in the CSV file are normalized by image size (https://github.com/google-research/google-research/blob/d220e37d33a96c021af3cd2a8e5df25bfc9db8bf/poem/tools/gen_train_tfrecords.py#L33-L34), which has nothing to do with the ego-centric normalization in `input_generator.create_model_input` (https://github.com/google-research/google-research/blob/d220e37d33a96c021af3cd2a8e5df25bfc9db8bf/poem/core/input_generator.py#L542-L543).

We have not tried OpenPose keypoints, so our experience is perhaps limited here. The first question to ask here is whether you care about handling missing keypoints. If not, you do not need to worry about the keypoint scores at all (still need to include some values in the CSV if you use our `gen_train_tfrecords.py`), and simply set the [min_input_keypoint_score_2d](https://github.com/google-research/google-research/blob/d220e37d33a96c021af3cd2a8e5df25bfc9db8bf/poem/pr_vipe/infer.py#L114) to the default value -1.0 should be fine.

If you do care about handling missing keypoint, please refer the detailed methods in our follow-up [paper](https://arxiv.org/abs/2010.13321). To use `infer.py`, you need to: (1) select a reasonable threshold for [min_input_keypoint_score_2d](https://github.com/google-research/google-research/blob/d220e37d33a96c021af3cd2a8e5df25bfc9db8bf/poem/pr_vipe/infer.py#L114) based on your own pose estimator, below which a keypoint should be considered as ""unreliable""; (2) set [model_input_keypoint_mask_type](https://github.com/google-research/google-research/blob/d220e37d33a96c021af3cd2a8e5df25bfc9db8bf/poem/pr_vipe/infer.py#L110) to [common.MASK_KEYPOINTS_AND_AS_INPUT](https://github.com/google-research/google-research/blob/d220e37d33a96c021af3cd2a8e5df25bfc9db8bf/poem/core/common.py#L89-L90).

Note that the model checkpoint currently released at https://sites.google.com/corp/view/pr-vipe/home does **not** handle missing keypoints. We are working on releasing a new model checkpoint that handles missing keypoints. @jenjsun 

As a final note, if you consider using non-Human3.6-finetuned CPN keypoints (e.g., OpenPose keypoints), we **highly recommend** retraining the model, as it is a known issue that the model trained with the Human3.6M-finetuned CPN keypoints has degraded performance using keypoints from other models (e.g., PersonLab).",problem thanks interest work regarding normalization sorry confusion bottom line input preserve original aspect ratio original image included simply data pipeline file image size nothing normalization tried experience perhaps limited first question ask whether care handling missing need worry still need include use simply set default value fine care handling missing please refer detailed paper use need select reasonable threshold based pose estimator considered unreliable set note model currently handle missing working new model missing final note consider highly recommend model known issue model trained degraded performance,issue,positive,positive,neutral,neutral,positive,positive
1017438474,"I found the solution (I am using windows 10) - the solution is to install it from the URL from the PyPI website:

`C:\Users\onef0>pip install https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl`",found solution solution install pip install,issue,positive,neutral,neutral,neutral,neutral,neutral
1017399124,"Thanks a lot! I went step by step through the code and it seems like in the **input_generator.py** file you are normalizing the keypoints in the code. As I understand, you are doing the centering & scaling in **keypoint_utils.py** in the **normalize_points** method:  
`scale_distances = tf.math.maximum(1e-12, compute_scale_distances())`
`  normalized_points = (points - offset_points) / scale_distances * scale_unit`

as you have described in your paper: 
> For 2D poses, we translate the keypoints so that the center between left and right hip is at the origin. Then we normalize the pose such that the maximum distance between shoulder and hip joints is 0.5. This maximum distance is computed between all pairwise distances among left shoulder, right shoulder, left hip, and right hip.

Besides that, I realized that the keypoints are ""denormalized"" in the **read_input** method of the **infer.py** file. So therefore, the input csv containes normalized keypoints according to the image size, while the keypoints are denormalized and then again centered & normalized in the code. 

So thats the reason why I am a bit confused. I am very sorry if I am repeating my question, but what do I really need to do with the keypoints for preprocessing before using the infer.py with your pretrained model?

One more question...I am using OpenPose for the keypoint extraction and when examining your example tfrecords, I saw that the _score_ value is higher and isn't in the interval [0,1] as in OpenPose. Is that relevant? I checked your code and it seems like the _score_ is nowhere used.? 

My apologies for all these questions. Thank you again for your time and help!

",thanks lot went step step code like file code understand centering scaling method paper translate center left right hip origin normalize pose maximum distance shoulder hip maximum distance pairwise among left shoulder right shoulder left hip right hip besides method file therefore input according image size centered code thats reason bit confused sorry question really need model one question extraction examining example saw value higher interval relevant checked code like nowhere thank time help,issue,positive,positive,neutral,neutral,positive,positive
1017076212,"If you use a model checkpoint provided by us, you will need to do the proper normalization (ego-centric centering + scaling).",use model provided u need proper normalization centering scaling,issue,negative,neutral,neutral,neutral,neutral,neutral
1016968991,"Alright I see, great! Thank you very much for your answer.  I checked the comments in the other issues and remarked that some users only ""normalize"" the data by the image size (which is also written in the infer.py script). Do I need to center the keypoints or is it enough to only divide it by the image size before running the infer.py script?",alright see great thank much answer checked normalize data image size also written script need center enough divide image size running script,issue,positive,positive,positive,positive,positive,positive
1016954827,"yes, I'm pretty sure that the split file is correct. I've used that script and debugged it heavily.
Cache is fine. I've built the dataset from scratch to check this.",yes pretty sure split file correct used script heavily cache fine built scratch check,issue,positive,positive,positive,positive,positive,positive
1016941483,"Hello! Getting negative values is fine after centering since all the keypoints are defined relative to the center (at 0,0).",hello getting negative fine centering since defined relative center,issue,negative,positive,neutral,neutral,positive,positive
1016938176,"@marcociccone Can you confirm if the correct full zip package was downloaded? You may clear the [cache dir][cache], which will allow the script to redownload the package.

https://github.com/tensorflow/federated/blob/v0.19.0/tensorflow_federated/python/simulation/datasets/gldv2.py#L36-L38

[cache]: https://github.com/tensorflow/federated/blob/v0.19.0/tensorflow_federated/python/simulation/datasets/gldv2.py#L220",confirm correct full zip package may clear cache cache allow script package cache,issue,positive,positive,positive,positive,positive,positive
1015766160,"Thanks for the solution. I had pretty much given up. I'm definitely interested in TFT, but it doesn't seem incredibly popular in terms of getting support.",thanks solution pretty much given definitely interested seem incredibly popular getting support,issue,positive,positive,positive,positive,positive,positive
1015055495,"`sklearn.preprocessing.StandardScaler()` is fitted on `df[[target_column]].values` which has shape `(3488949, 1)`.
When using the inverse transform of the fitted standardscaler `predictions[col]` is a pandas series and has shape `(139635,)`.
So changing the original `self._target_scaler.inverse_transform(predictions[col])` to `self._target_scaler.inverse_transform(predictions[col].values.reshape(-1,1))` solves the issue!",fitted shape inverse transform fitted col series shape original col col issue,issue,negative,positive,positive,positive,positive,positive
1014082053,"Finetuneing Sst-2 Task on moblieBert, The dev acc is **0.8027523**, It's a huge difference 
hyperparameters: 
batch_size=64,
length=256, 
epoch=5,
learning rate=5e-5
",task dev huge difference learning,issue,negative,positive,positive,positive,positive,positive
1013835787,"> Hi, I’ve been confused about this as well. What solution did you find?

Issue is script_train_fixed_params.py doesn't print/save predicted values and displays only Mean loss value on terminal. To check  predicted values, print/save p90_forecast variable which is populating at following script.
https://github.com/google-research/google-research/blob/master/tft/script_train_fixed_params.py#L157
You can also refer following script which you can use after Model training. It loads trained model and save predicted values in  outputs/saved_models/<your exp>/targets/test_predictions.csv file.
https://github.com/ashishKAgg/tft/blob/master/script_load_predict.py
",hi confused well solution find issue mean loss value terminal check variable following script also refer following script use model training trained model save file,issue,positive,negative,negative,negative,negative,negative
1013685798,"Hi, thank you very much for publishing the code.
I have difficulties as well with the Normalization and shifting of the image source. I extract the keypoints of the input images with OpenPose and reorder them according to the order as you have described in infer.py. But I don't fully understand the normalization and shifting part (which you also used in your keypoints_utils.py part. It seems as you also get negative values. In my version, I computed the center between the hips as you describes and then subtracted the center_x from all x values and center_y from all y values. Afterwards if I compute the normalized values I get negative values as well (because of the shifting). Am I wrong? I checked your code but I didn't understand it at all, can you help me please?
Thanks in advance!",hi thank much code well normalization shifting image source extract input reorder according order fully understand normalization shifting part also used part also get negative version center afterwards compute get negative well shifting wrong checked code understand help please thanks advance,issue,negative,negative,negative,negative,negative,negative
1012540791,"Thanks, @Adam-fei for the confirmation, too. :D

And thanks @hassanhub for the reply. Look forward to the new tfrecord version. :D",thanks confirmation thanks reply look forward new version,issue,positive,positive,positive,positive,positive,positive
1010584538,I see. That makes sense. Thanks for the response ,see sense thanks response,issue,negative,positive,positive,positive,positive,positive
1010314778,"I will have to add support for the mode you are asking for, and I will take a look.",add support mode take look,issue,negative,neutral,neutral,neutral,neutral,neutral
1010302986,"Only two modes are currently supported, pmap and pjit. Pjit is experimental and currently can be used with Lingvo JAX framework. ",two currently experimental currently used framework,issue,negative,positive,neutral,neutral,positive,positive
1010298818,"@rohan-anil Forgot to tag you in the post, you seem to be the active maintainer",forgot tag post seem active maintainer,issue,negative,negative,negative,negative,negative,negative
1010284812,Thanks a lot! We will update the name.,thanks lot update name,issue,negative,positive,positive,positive,positive,positive
1010211175,"I haven't actually seen this exact error before so I'm guessing here. I think the data generation creates subfolders for the different splits (train, test) and when you pass the folder to the train script, it needs to be the lowest level folder which actually contains the TF-records. My guess is you are giving it a higher level folder containing a train and test subfolder.",actually seen exact error guessing think data generation different train test pas folder train script need level folder actually guess giving higher level folder train test,issue,negative,positive,neutral,neutral,positive,positive
1010205817,"To train on a custom dataset, you will need to produce your own TF-records with the expected fields. I would advise modeling off of one of the existing data generation scripts, such as: https://github.com/google-research/google-research/blob/master/smurf/data_conversion_scripts/convert_flying_chairs_to_tf_records.py

You should then be able to use the generic loader to train on your custom dataset: https://github.com/google-research/google-research/blob/master/smurf/data/generic_flow_dataset.py
",train custom need produce would advise modeling one data generation able use generic loader train custom,issue,negative,positive,positive,positive,positive,positive
1010203111,"There is an assumption in the code of batch size 1 per GPU. You run into memory issues trying to squeeze more images onto an individual V100 GPU anyway, so this is not very limiting.

In our work, we trained with an 8 GPU machine. In the released code, you should be able to train with a batch size equal to 8 if you have 8 GPUs available. ",assumption code batch size per run memory trying squeeze onto individual anyway limiting work trained machine code able train batch size equal available,issue,negative,positive,positive,positive,positive,positive
1008978644,"> Hi, thanks for this wonderful project. Are there any pretrained weights for IB-BERT(large) available for download, or planned to be?

Has it been settled",hi thanks wonderful project large available settled,issue,positive,positive,positive,positive,positive,positive
1008004668,"I don't think so, I'm afraid, but I've actually forgotten what I was trying to do...",think afraid actually forgotten trying,issue,negative,negative,negative,negative,negative,negative
1007961230,"It has been some time since you've opened this issue. I am now also having it, did you manage  to find any solution?
",time since issue also manage find solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1007904056,"Hi, I’ve been confused about this as well. What solution did you find?",hi confused well solution find,issue,negative,negative,negative,negative,negative,negative
1007842994,"Good to hear!

You should use latency from:
```
Inference timings in us: Init: 17851, First inference: 2104, Warmup (avg): 1041.15, Inference (avg): 1039.66
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=10.7383 overall=12.1055
```
Inference (microseconds): 1039.66
Memory(MB) 12.1055
",good hear use latency inference u first inference inference note tool memory footprint following approximate actual memory footprint model take information discretion memory footprint delta start tool inference memory,issue,negative,positive,positive,positive,positive,positive
1007821285,"@marcociccone Thanks for surfacing this issue! I have got a fix, will push an update to the affected CIFAR files soon.",thanks surfacing issue got fix push update affected soon,issue,negative,positive,positive,positive,positive,positive
1007819952,"You're very helpful:) I really appreciate your help. So, following your link I was able to build the `benchmark_model_plus_flex`.:)

I ran the commands as said suggested above, for both benchmark and profiling for svdf non stream. Now,I've too many numbers and i am not sure which numbers should be considered as latency time as presented in your paper. Here is the output:

`non_stream.tflite.benchmark`

```
STARTING!
Log parameter values verbosely: [0]
Min num runs: [1000]
Num threads: [1]
Min warmup runs: [1]
Graph: [/data/local/tmp/model.tflite]
#threads used for CPU inference: [1]
Loaded model /data/local/tmp/model.tflite
The input model file size (MB): 1.41812
Initialized session in 17.851ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=478 first=2104 curr=1017 min=1012 max=2104 avg=1041.15 std=63

Running benchmark for at least 1000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=1000 first=1040 curr=1018 min=1012 max=1190 avg=1039.66 std=22

Inference timings in us: Init: 17851, First inference: 2104, Warmup (avg): 1041.15, Inference (avg): 1039.66
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=10.7383 overall=12.1055
```
And 

`non_stream.tflite.benchmark.profile`

```
STARTING!
Log parameter values verbosely: [0]
Min num runs: [1000]
Num threads: [1]
Min warmup runs: [1]
Graph: [/data/local/tmp/model.tflite]
Enable op profiling: [1]
#threads used for CPU inference: [1]
Loaded model /data/local/tmp/model.tflite
The input model file size (MB): 1.41812
Initialized session in 17.264ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=472 first=2010 curr=1044 min=1018 max=2010 avg=1053.98 std=70

Running benchmark for at least 1000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=1000 first=1045 curr=1058 min=1036 max=1104 avg=1060.94 std=11

Inference timings in us: Init: 17264, First inference: 2010, Warmup (avg): 1053.98, Inference (avg): 1060.94
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=10.4727 overall=11.9141
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	 ModifyGraphWithDelegate	            0.000	    2.018	    2.018	 97.253%	 97.253%	  2964.000	        1	ModifyGraphWithDelegate/0
	         AllocateTensors	            1.990	    0.056	    0.029	  2.747%	100.000%	     0.000	        2	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	 ModifyGraphWithDelegate	            0.000	    2.018	    2.018	 97.253%	 97.253%	  2964.000	        1	ModifyGraphWithDelegate/0
	         AllocateTensors	            1.990	    0.056	    0.029	  2.747%	100.000%	     0.000	        2	AllocateTensors/0

Number of nodes executed: 2
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	 ModifyGraphWithDelegate	        1	     2.018	    97.253%	    97.253%	  2964.000	        1
	         AllocateTensors	        1	     0.057	     2.747%	   100.000%	     0.000	        2

Timings (microseconds): count=1 curr=2075
Memory (bytes): count=0
2 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	   TfLiteXNNPackDelegate	            0.541	    0.507	    0.518	 48.978%	 48.978%	     0.000	        1	[streaming/dense_11/BiasAdd]:25
	   TfLiteXNNPackDelegate	            0.001	    0.005	    0.004	  0.362%	 49.340%	     0.000	        1	[streaming/speech_features/transpose1]:24
	      TfLiteFlexDelegate	            0.005	    0.526	    0.536	 50.660%	100.000%	     0.000	        1	[streaming/speech_features/Mfcc]:23

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	      TfLiteFlexDelegate	            0.005	    0.526	    0.536	 50.660%	 50.660%	     0.000	        1	[streaming/speech_features/Mfcc]:23
	   TfLiteXNNPackDelegate	            0.541	    0.507	    0.518	 48.978%	 99.638%	     0.000	        1	[streaming/dense_11/BiasAdd]:25
	   TfLiteXNNPackDelegate	            0.001	    0.005	    0.004	  0.362%	100.000%	     0.000	        1	[streaming/speech_features/transpose1]:24

Number of nodes executed: 3
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	      TfLiteFlexDelegate	        1	     0.536	    50.710%	    50.710%	     0.000	        1
	   TfLiteXNNPackDelegate	        2	     0.521	    49.290%	   100.000%	     0.000	        2

Timings (microseconds): count=1000 first=1038 curr=1054 min=1033 max=1102 avg=1058.19 std=11
Memory (bytes): count=0
3 nodes observed

Delegate internal: 
============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	        DelegateOpInvoke	            0.292	    0.244	    0.248	 46.881%	 46.881%	     0.000	        1	Delegate/Mfcc:2
	        DelegateOpInvoke	            0.011	    0.274	    0.281	 53.119%	100.000%	     0.000	        1	Delegate/AudioSpectrogram:1

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	        DelegateOpInvoke	            0.011	    0.274	    0.281	 53.119%	 53.119%	     0.000	        1	Delegate/AudioSpectrogram:1
	        DelegateOpInvoke	            0.292	    0.244	    0.248	 46.881%	100.000%	     0.000	        1	Delegate/Mfcc:2

Number of nodes executed: 2
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	        DelegateOpInvoke	        2	     0.527	   100.000%	   100.000%	     0.000	        2

Timings (microseconds): count=1000 first=518 curr=523 min=513 max=573 avg=528.433 std=8
Memory (bytes): count=0
2 nodes observed
```
Since, I am writing the latency numbers into my paper, it would be great if you could pointout which numbers from here are used as `latency`. Also, if you could point me to any documentations where I could understand the interpretation of these outputs `Run Order`, `Top by Computation Time` , `Summary by node type` etc. that would be so helpful. 

Thank you @rybakov 














",helpful really appreciate help following link able build ran said non stream many sure considered latency time paper output starting log parameter verbosely min min graph used inference loaded model input model file size session running least least terminate exceeding running least least terminate exceeding inference u first inference inference note tool memory footprint following approximate actual memory footprint model take information discretion memory footprint delta start tool starting log parameter verbosely min min graph enable used inference loaded model input model file size session running least least terminate exceeding running least least terminate exceeding inference u first inference inference note tool memory footprint following approximate actual memory footprint model take information discretion memory footprint delta start tool run order node type start first mem time name top computation time node type start first mem time name number executed summary node type node type count mem time memory regular run order node type start first mem time name top computation time node type start first mem time name number executed summary node type node type count mem time memory delegate internal run order node type start first mem time name top computation time node type start first mem time name number executed summary node type node type count mem time memory since writing latency paper would great could used latency also could point could understand interpretation run order top computation time summary node type would helpful thank,issue,positive,positive,neutral,neutral,positive,positive
1007774960,"Hey there, very sorry about this. The models are actually at 

https://console.cloud.google.com/storage/browser/gresearch/smurf/kitti-smurf

And

https://console.cloud.google.com/storage/browser/gresearch/smurf/sintel-smurf

I'll update the README with the correct links!
",hey sorry actually update correct link,issue,negative,negative,negative,negative,negative,negative
1007718442,"Thank you for such a quick response :), Ok I will have a look at it.",thank quick response look,issue,negative,positive,positive,positive,positive,positive
1007715057,"It looks like it is missing absl dependence ""absl/types/optional.h"", or path to absl is not configured. I saw build issue with benchmark_model_plus_flex in [link](https://github.com/Samsung/ONE/issues/3033), may be it can help.
Please report this issue to [tf](https://github.com/tensorflow/tensorflow)",like missing dependence path saw build issue link may help please report issue,issue,positive,negative,negative,negative,negative,negative
1007705852,"@rybakov , I am not able to figure out what is causing this `benchmark_model_plus_flex` build fail. I am using every component as latest. It builds fine for `Target //tensorflow/lite/tools/benchmark:benchmark_model`. Would you please help me here. I've lost so much time to figure this out. Online resources are not much help as well :/.


```
srauniy@cmtcleu:~/bazel/tensorflow_src$ bazel build -c opt --config=android_arm64 --cxxopt='--std=c++17' tensorflow/lite/tools/benchmark:benchmark_model_plus_flex
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=183
INFO: Reading rc options for 'build' from /lhome/srauniy/bazel/tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /lhome/srauniy/bazel/tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from /lhome/srauniy/bazel/tensorflow_src/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages --python_path=/usr/bin/python3 --action_env ANDROID_NDK_HOME=/usr/local/android/android-ndk-r21e --action_env ANDROID_NDK_API_LEVEL=30 --action_env ANDROID_BUILD_TOOLS_VERSION=31.0.0 --action_env ANDROID_SDK_API_LEVEL=31 --action_env ANDROID_SDK_HOME=/usr/local/android/Android/Sdk
INFO: Reading rc options for 'build' from /lhome/srauniy/bazel/tensorflow_src/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /lhome/srauniy/bazel/tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /lhome/srauniy/bazel/tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:android_arm64 in file /lhome/srauniy/bazel/tensorflow_src/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /lhome/srauniy/bazel/tensorflow_src/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --define=with_xla_support=false
WARNING: API level 30 specified by android_ndk_repository 'androidndk' is not available. Using latest known API level 29
INFO: Analyzed target //tensorflow/lite/tools/benchmark:benchmark_model_plus_flex (98 packages loaded, 5848 targets configured).
INFO: Found 1 target...
ERROR: /lhome/srauniy/bazel/tensorflow_src/tensorflow/cc/saved_model/BUILD:155:11: Compiling tensorflow/cc/saved_model/loader_util.cc failed: (Exit 1): clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target ... (remaining 41 argument(s) skipped)
In file included from tensorflow/cc/saved_model/loader_util.cc:16:
In file included from ./tensorflow/cc/saved_model/loader_util.h:21:
In file included from ./tensorflow/core/lib/core/status.h:19:
./tensorflow/core/platform/status.h:26:10: fatal error: 'absl/types/optional.h' file not found
#include ""absl/types/optional.h""
         ^~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
Target //tensorflow/lite/tools/benchmark:benchmark_model_plus_flex failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.621s, Critical Path: 1.01s
INFO: 35 processes: 14 internal, 21 local.
FAILED: Build did NOT complete successfully
srauniy@cmtcleu:~/bazel/tensorflow_src$
```",able figure causing build fail every component latest fine target would please help lost much time figure much help well build opt provided client reading reading define opt reading reading found applicable definition build file found applicable definition build file found applicable definition build file found applicable definition build android file warning level available latest known level target loaded found target error exit clang error command argument file included file included file included fatal error file found include error target build use see command build time critical path internal local build complete successfully,issue,negative,positive,positive,positive,positive,positive
1007617502,"It seems that the tfrecord saved by your DMVR's example is not compatible with the parser in the pipeline.
I'll contact the POC in DMVR and get back to you with a solution for storing a compatible tfrecord.
Thanks for your patience. I hope this is the only bottleneck for smoothly running VATT.",saved example compatible parser pipeline contact get back solution compatible thanks patience hope bottleneck smoothly running,issue,positive,positive,positive,positive,positive,positive
1006464799,"Hi, @hassanhub sorry for the late reply, and big thanks to @adaniefei for offering more informations.

You can find my fork code [here](https://github.com/Adam-fei/google-research/tree/debug) in the branch `debug`.

I'll try to explain my process with the code as clearly as possible.
Besides, you can find my modification for the code in the **only commit** I pushed

I'm trying to reproduce the result of kinetics400, so I download the following checkpoint file.
![image](https://user-images.githubusercontent.com/18185114/146738933-69bedbed-d025-4ec1-ab4e-be506640847b.png)

To load this model correctly, I modified the `./vatt/configs/video.py` file: `14--->20`

I generate the tfrecord using dmvr. You can find the pipeline at `./vatt/dmvr/examples/run.sh`. I also upload the corresponding kinetics400 videos so you can generate tfrecord through my pipeline if you want.

I put the tfrecord in `./vatt/dmvr/examples/generated_dataset/valid`. To read this tfrecord, I modified the code in `./data/datasets/toy_dataset.py`, `data/datasets/factory.py`.
Furthermore, I ensured that the output of [this line](https://github.com/deepmind/dmvr/blob/dad75545a98a80a619edf6f1ec71d92f25e9e923/examples/generate_from_file.py#L135) is non-zero, and the `input_feature_name` name I used in dmvr is `""image/encoded""` (you can check at [this line](https://github.com/Adam-fei/google-research/blob/debug/vatt/dmvr/examples/generate_from_file.py#L147))

Besides all above, I also made some changes so the code can run properly:
1. modified `./configs/dataset.py` to meet the config of trained model according to [issue917](https://github.com/google-research/google-research/issues/917)
2. modified `data/processing.py` due to the version mismatch of dmvr according to issue917
3. modified `experiments/finetune.py`, so the code can load the model I specified in the command line.
4. downloaded ./misc folder and pre-trained model and put them in the right place.

After the modifications, I run the code using
`python -m vatt.main --task=finetune --mode=eval --model_dir=./my_output --model_arch=vit_base --strategy_type=mirrored --override_checkpoint=./vatt/Checkpoint/Kinetics-400_BBS/vision_vatt_finetune_vit_base_k400_modality_specific_pretrain_ckpt-100000`

Then I'll get the error message as follows:

```
I0106 18:39:41.460304 139730737300480 video_dataset.py:456] Dataset created successfully
2022-01-06 18:39:55.803064: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 48 but output shape: []
2022-01-06 18:39:55.806709: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 101 but output shape: []
2022-01-06 18:39:55.807336: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 91 but output shape: []
2022-01-06 18:39:55.807725: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 101 but output shape: []
2022-01-06 18:39:55.814186: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 101 but output shape: []
```

If you need more informations, please tell me
Thank you! :)
",hi sorry late reply big thanks offering find fork code branch try explain process code clearly possible besides find modification code commit trying reproduce result kinetics following file image load model correctly file generate find pipeline also corresponding kinetics generate pipeline want put read code furthermore output line name used check line besides also made code run properly meet trained model according issue due version mismatch according issue code load model command line folder model put right place run code python get error message successfully name unknown key index number size output shape name unknown key index number size output shape name unknown key index number size output shape name unknown key index number size output shape name unknown key index number size output shape need please tell thank,issue,positive,negative,neutral,neutral,negative,negative
1006284992,"Hi @bdytx5 thank you for using this library and your suggestion! In non custom split ""silence"" and ""unknown"" labels are added under the hood (for compatibility with previously published papers) and it can confuse some users. That is why we introduced custom split, where data split is defined explicitly by user (in this case this lib does not do anything under the hood): it removes any confusions and we would prefer to keep this distinction between custom and non custom splits as it is.",hi thank library suggestion non custom split silence unknown added hood compatibility previously confuse custom split data split defined explicitly user case anything hood would prefer keep distinction custom non custom,issue,negative,negative,negative,negative,negative,negative
1006211331,"Thank you, happy new year! 

It is a multiclass precision recall, so I would suggest to look at [link](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2), [link](https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co)",thank happy new year precision recall would suggest look link link,issue,positive,positive,positive,positive,positive,positive
1006208626,"Hi @ntyoshi for [12 labels experiment](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md) it will do data splitting automatically for you: by default 10 labels are defined in [line](https://github.com/google-research/google-research/blob/master/kws_streaming/train/base_parser.py#L162) and it will also add 2 additional labels UNKNOWN and SILENCE), so you will need all data with 35 labels (but not all ogf them will be used). Data split logic is well described in [paper](https://arxiv.org/pdf/1804.03209.pdf), [paper](https://arxiv.org/pdf/2005.06720.pdf)",hi experiment data splitting automatically default defined line also add additional unknown silence need data used data split logic well paper paper,issue,negative,negative,neutral,neutral,negative,negative
1006123360,"> Thanks a lot for reporting this. I have a clarifying question. What fields are you storing in your tfrecord? Please note that `input_feature_name` should match the one that is already in the tfrecord fields.
> 
> I assume you are storing your visual features under a field which does not match `input_feature_name=""image/encoded""`. So, you get `INVALID_ARGUMENT: Name: , Key: image/encoded`.

Thanks for your reply @hassanhub ! 

I used the [generate_from_file.py](https://github.com/deepmind/dmvr/blob/master/examples/generate_from_file.py#L147) from dmvr to generate the tfrecord. The key ""image/encoded"" at there is same as the field name in processing.add_vision() in the [dataset factory](https://github.com/adaniefei/vatt_mosi/blob/master/vatt/data/datasets/mosi_dataset.py#L471). I only modified the `input_feature_name=**` in processing.add_audio() and processing.add_label() in the [factory](/vatt/data/datasets/mosi_dataset.py) to make them consistent from both sides.

I have reproduced the tfrecord of the dataset to check the output of [this line](https://github.com/deepmind/dmvr/blob/dad75545a98a80a619edf6f1ec71d92f25e9e923/examples/generate_from_file.py#L135). So far there is no zero-length `img_encoded` list from the video data I'm using.

The content below is the partial error note I got when running vatt.main. The difference compared to @Adam-fei is that the `Name:` is **\<unknown\>** in my case. 

     2022-01-05 17:04:20.749125: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 49 but output shape: []
     2022-01-05 17:04:20.760604: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 76 but output shape: []
     2022-01-05 17:04:20.771905: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at example_parsing_ops.cc:480 : INVALID_ARGUMENT: Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 115 but output shape: []

Thanks!






",thanks lot question please note match one already assume visual field match get name key thanks reply used generate key field name factory factory make consistent side check output line far list video data content partial error note got running difference name case name unknown key index number size output shape name unknown key index number size output shape name unknown key index number size output shape thanks,issue,positive,positive,neutral,neutral,positive,positive
1005603231,"> > > @lucyyangrui 感觉fnet和realformer结构上还是有些区别的
> > 
> > 
> > fnet是把自注意力层砍了，换成了傅里叶变换，参数量少了，realformer我原理了解不多欸
> 
> 可以加下微信，交流一下嘛，微信: 13273814025

I also want to ask you a question",also want ask question,issue,negative,neutral,neutral,neutral,neutral,neutral
1005208100,Could you also please check if the output of [this line](https://github.com/deepmind/dmvr/blob/dad75545a98a80a619edf6f1ec71d92f25e9e923/examples/generate_from_file.py#L135) is non-zero?,could also please check output line,issue,negative,neutral,neutral,neutral,neutral,neutral
1005010624,"Thanks a lot for reporting this.
I have a clarifying question. What fields are you storing in your tfrecord?
Please note that `input_feature_name` should match the one that is already in the tfrecord fields.

I assume you are storing your visual features under a field which does not match `input_feature_name=""image/encoded""`. So, you get `INVALID_ARGUMENT: Name: , Key: image/encoded`.",thanks lot question please note match one already assume visual field match get name key,issue,positive,positive,neutral,neutral,positive,positive
1003760733,"@rybakov, I wish you a happy new year :)
I quickly wanted to ask you, is there anywhere you've also calculated the precision and recall of the models in the code? if not could you please give me any hints on the same.:)",wish happy new year quickly ask anywhere also calculated precision recall code could please give,issue,positive,positive,positive,positive,positive,positive
1003503466,"I believe I have met the identical problem as @Adam-fei described, though my setting is slightly different. Here is a [repository](https://github.com/adaniefei/vatt_mosi/tree/master) including the code (modified on configs/dataset.py and data/processing.py as described in #917 ) and two data samples. The repo doesn't contain the misc folder and pre-trained model because of the size constrain. It will be great if @hassanhub could also help me to check it. :D

I'm using the ""finetune"" mode on the generated dmvr tf-record of a different dataset. Currently, I only tried on for **video modality** with **vit_large** arch, starting from **Large-Base-Small** check-point. I used the default setting of ""generate_from_file.py"" in dmvr repo to generate the data samples in the repo above. 

P.S: Thank you guys and happy new year!





",believe met identical problem though setting slightly different repository code two data contain folder model size constrain great could also help check mode different currently tried video modality arch starting used default setting generate data thank happy new year,issue,positive,positive,positive,positive,positive,positive
1003486602,"Hi Adam,

Thanks for flagging this. Could you please fork the repo and point me to your commit for the data reader?
I prefer not to open any compressed and/or uploaded files.
Also, having a repo would make it easier for other people to understand the issue and point to it as a reference in future.

Thanks",hi thanks flagging could please fork point commit data reader prefer open compressed also would make easier people understand issue point reference future thanks,issue,positive,positive,neutral,neutral,positive,positive
1002801942,"Hi, MSc student here. Been learning about vision papers recently and this is such an interesting paper!!! I rarely spent this much time trying to understand every detail.  Thank you so much for the work!!! 

I was writing some notes and I ended up getting cross_entropy['s_on_l_old'] - cross_entropy['s_on_l_new'], which is the symmetric of what is in the code. There's also this other popular [implementation](kekmodel/MPL-pytorch) in pytorch that uses the same result I got. Is it indifferent?
Here's the sketch of my notes
![image](https://user-images.githubusercontent.com/47759921/147709060-04d93555-131c-4ffb-8d67-add18659ea81.png)

",hi student learning vision recently interesting paper rarely spent much time trying understand every detail thank much work writing ended getting symmetric code also popular implementation result got indifferent sketch image,issue,positive,positive,positive,positive,positive,positive
1002195075,"@junjiek 

Thank you.  You have been very helpful.  Again I am very impressed with your teams work.  Thanks,

Tim
",thank helpful work thanks,issue,positive,positive,positive,positive,positive,positive
1001301436,"Thx, I've created a new issue at [issues925](https://github.com/google-research/google-research/issues/925).

And I'll close this resolved issue. :)",new issue close resolved issue,issue,negative,positive,positive,positive,positive,positive
1000923557,"This is a DMVR-specific issue and requires a correct understanding of their pipeline.
Please create a new issue with `[VATT] Construct and load datasets using DMVR` as the title and I'd try to address the issues as much as possible.",issue correct understanding pipeline please create new issue construct load title try address much possible,issue,positive,positive,neutral,neutral,positive,positive
1000274650,"Hi, I encountered another problem.

I followed the [dmvr example](https://github.com/deepmind/dmvr/tree/master/examples) to generate the dataset.
And changed the corresponding code so the code can read the correct path of data.

But when I run the vatt evaluation process, it would raise following error:

> tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.
>   (0) INVALID_ARGUMENT:  Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 91 but output shape: []
>          [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]]
>          [[MultiDeviceIteratorGetNextFromShard]]
>          [[RemoteCall]]
>          [[IteratorGetNext_3]]
>          [[model/video_module/vit_base/spatio_temporal_embeddings/strided_slice_5/_1]]
>   (1) INVALID_ARGUMENT:  Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 91 but output shape: []
>          [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]]
>          [[MultiDeviceIteratorGetNextFromShard]]
>          [[RemoteCall]]
>          [[IteratorGetNext_3]]
>   (2) CANCELLED:  Function was cancelled before it was started
> 0 successful operations.
> 0 derived errors ignored. [Op:__inference_evaluation_step_21087]
> 
> Function call stack:
> evaluation_step -> evaluation_step -> evaluation_step

I located the error at line150 in file ./vatt/data/loading.py where the object generate by command `factory.make_dataset()` cannot be read. The minimize pseudocode is as follow:

```
dataset =  factory.make_dataset(
              shuffle=self.shuffle,
              num_epochs=self.num_epochs,
              batch_size=per_replica_batch_size,
              padded_batch=False,
              drop_remainder=True,
              keep_key=False,
              override_preprocess_fn=None,
              )
iter_data = iter(dataset)
next(iter_data) // this would trigger the error
```

-------------------------------------------------------

Here is the pipeline I used to generate the data file. I write some info in the README.txt.
[dmvr_example.tar.gz](https://github.com/google-research/google-research/files/7769292/dmvr_example.tar.gz)

I cannot identify whether I miss used the dmvr, or I miss code the vatt data reader.

What should I do?

Thanks



",hi another problem example generate corresponding code code read correct path data run evaluation process would raise following error root error found name unknown key index number size output shape node name unknown key index number size output shape node function successful derived function call stack error line file object generate command read minimize follow iter next would trigger error pipeline used generate data file write identify whether miss used miss code data reader thanks,issue,negative,positive,neutral,neutral,positive,positive
1000101940,Thank you for your quick apply! Now it works and I am training the model to check the result.,thank quick apply work training model check result,issue,negative,positive,positive,positive,positive,positive
1000088938,"Hi sorry I think I introduced a bug during the update for the second paper. @VicaYang 

Pls make the following changes (https://github.com/google-research/google-research/commit/bda411e8644173ae60b2997b062d47832aa8dcb5) and try again. I will do a formal update later on.",hi sorry think bug update second paper make following try formal update later,issue,negative,negative,negative,negative,negative,negative
1000073277,"@zizhaozhang  Thanks a lot! I run `CUDA_VISIBLE_DEVICES=0 python -m ieg.main --dataset=cifar100_uniform_0.4 --network_name=wrn28-10 --checkpoint_path=ieg/save` and got the error message.
",thanks lot run python got error message,issue,negative,positive,positive,positive,positive,positive
1000070810,"Hi @VicaYang and others,

Can you share me the script you are running? 
There are two methods in this codebase, one needs probe, one does not. If mixing up their configurations, there could make such an error. Which method were you trying to run?
",hi share script running two one need probe one could make error method trying run,issue,negative,neutral,neutral,neutral,neutral,neutral
999247704,"Thank you so much for the tips!

I will post the newest update, thx again :)",thank much post update,issue,negative,positive,positive,positive,positive,positive
999027764,"Please change it to the following:

video.py: ViTBase.spatial_patch_size=16
video.py: ViTBase.max_vertical_buckets=20
video.py: ViTBase.max_horizontal_buckets=20
dataset.py: Finetune.frame_size=224

`Side note`: Although we wrote the `Position Encoding Lookup` module to be dynamic in terms of the input size (using interpolation), it's recommended to use a frame size of 320 for the action recognition models, because they were trained with this size and the accuracy of pos embd lookups decreases by using interpolation. If you are fine-tuning, you don't need to worry about it. But if you just want to evaluate the model, please use `dataset.py: Finetune.frame_size=320`.",please change following side note although wrote position module dynamic input size interpolation use frame size action recognition trained size accuracy interpolation need worry want evaluate model please use,issue,positive,positive,neutral,neutral,positive,positive
998614563,"I see. I thought the config is pre-set and I could just hit the run Button :)

Now my config is as follow:

video.py: ViTBase.spatial_patch_size=16
video.py: ViTBase.max_vertical_buckets=14
video.py: ViTBase.max_horizontal_buckets=14
dataset.py: Finetune.frame_size=224

As for params.yaml, I did not use any *.yaml file as config file, since there is no config_file mentioned in the README.md. Where can I find it?

",see thought could hit run button follow use file file since find,issue,negative,neutral,neutral,neutral,neutral,neutral
998588479,"> tf.tpu.experimental.DeviceAssignment.build(
>       topology=topology,
>       num_replicas=num_replicas)

Thank you for your reply and i alter the code as your mention. But i still have some problem when i run the fixed code. As shown in below.
Q1: If i use tf.tpu.experimental.Topology to bulid Toplogy, i will need to change tensorflow version to '2.X' .  So the 'from tensorflow.contrib import image as image_ops' in augment.py will crash, because the version 2.X tf does not have the contrib lib. And then, i use ’import tensorflow_addons.image as image_ops‘  instead of contrib lib. However 'mesh_shape' parameter in tf.tpu.experimental.Topology() need 4 positive integers([2,2,2,2]) as input,but we only have 3([2,2,2]). So, i need to change the tf version to 2.2.0(only versions below 2.5.0 accept mesh_shape input of 3 dimension), but Addons( tensorflow_addons) supports using Python ops for all Tensorflow versions above or equal to 2.5.0. Thus, there  is a contradiction.
Q2: if i use tf 1.15 'from tensorflow.python import tpu as tpu_lib' to import tpu_lib, i need to reduce num_cores_to_shape(in main.py) dimension to 3([1,1,1]) since 1.X tf version only accept 3 dimensions of computation_shape. When the code is running, i have the same question with https://github.com/google-research/google-research/issues/651. In thread 2,we can not find the TPU device.
Q3: Could you please send me a latest code? i really appreciate and want to follow your work. Thank you very much! My email is liujiaming@bupt.edu.cn",thank reply alter code mention still problem run fixed code shown use need change version import image crash version use import instead however parameter need positive input need change version accept input dimension python equal thus contradiction use import import need reduce dimension since version accept code running question thread find device could please send latest code really appreciate want follow work thank much,issue,positive,positive,positive,positive,positive,positive
998236432,"You don't need to tweak uvatt.py for finetune. Finetune only uses video.py or audio.py

Can you please send me your config below:
- video.py: ViTBase.spatial_patch_size
- video.py: ViTBase.max_vertical_buckets
- video.py: ViTBase.max_horizontal_buckets
- dataset.py: Finetune.frame_size

Alternatively, please upload your `params.yaml` somewhere and send me the link so that I could take a look.

I will upload the pre-set configs in the future. But for now please bare with us with setting the config flags manually.",need tweak please send alternatively please somewhere send link could take look future please bare u setting manually,issue,positive,positive,neutral,neutral,positive,positive
998160698,"If you are using tensorflow 2.x, you can replace `import tensorflow as tf` by `import tensorflow.compat.v1 as tf`, at least, it works for me",replace import import least work,issue,negative,negative,negative,negative,negative,negative
998075092,"Thank you for your interest. Apologies for the broken imports and flags :( but please find the answers to your questions below:

> question 1: i can not find the parameter 'num_cores_per_replica' in main.py(line 71). And the paremeter is not defined in flag_utils.py.

Can you use the value of `1` instead?

>question 2: i donot know which tpu_lib is imported in main.py(line 66). thank you very much!
Please use `tf.tpu.experimental.Topology` to build the `Topology` object, and use
```
tf.tpu.experimental.DeviceAssignment.build(
      topology=topology,
      num_replicas=num_replicas)
```
to build the `DevicePlacement` object.
",thank interest broken please find question find parameter line defined use value instead question know line thank much please use build topology object use build object,issue,positive,negative,neutral,neutral,negative,negative
997730567,"Thank you for your reply! I met another issue here:

I downloaded the following checkpoint file, and try to use it to evaluate the Kinetics-400 result

![image](https://user-images.githubusercontent.com/18185114/146738933-69bedbed-d025-4ec1-ab4e-be506640847b.png)

I used this command line:
`python -m vatt.main --task=finetune --mode=eval --model_dir=./my_output --model_arch=vit_base --strategy_type=mirrored --override_checkpoint=/PATH/TO/FILE/vision_vatt_finetune_vit_base_k400_modality_specific_pretrain_ckpt-100000`

But I got this error message indicates that the value shape is incompatible with the model network:

> ValueError: Cannot assign value to variable ' video_module/vit_base/spatio_temporal_embeddings/horizontal_position_embeddings/embeddings:0': Shape mismatch.The variable shape (14, 768), and the assigned value shape (20, 768) are incompatible.

 I tried to change the ""14"" argument in video.py and uvatt.py, but the error still occur. What should I do?",thank reply met another issue following file try use evaluate result image used command line python got error message value shape incompatible model network assign value variable shape variable shape assigned value shape incompatible tried change argument error still occur,issue,negative,neutral,neutral,neutral,neutral,neutral
997516130,"Thanks for your feedback.
Regarding the add_audio issue, there is a mismatch between the latest internal version and the public version of DMVR. I will communicate this with DMVR authors to resolve the issue. In the meantime, you can simply bypass it by removing those args.

Regarding the embeddings and vocabs, I made the file public. You should have access now.
Please let me know if you ran into any other issues.",thanks feedback regarding issue mismatch latest internal version public version communicate resolve issue simply bypass removing regarding made file public access please let know ran,issue,positive,positive,positive,positive,positive,positive
997345344,"In first thread, the code can initial the model successful. But in the second thread, 'ConfigureDistributedTPU' can not find ‘TPU’?",first thread code initial model successful second thread find,issue,positive,positive,positive,positive,positive,positive
997344862,"> Traceback (most recent call last): File ""meta_pseudo_labels/main.py"", line 463, in main train_tpu(params, should_eval=should_eval) File ""meta_pseudo_labels/main.py"", line 237, in train_tpu set_tpu_info(params) File ""meta_pseudo_labels/main.py"", line 68, in set_tpu_info topology_proto = sess.run(tpu_init) File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run run_metadata_ptr) File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run run_metadata) File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by node ConfigureDistributedTPU (defined at /usr/local/lib/p ython3.7/dist-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [tpu_embedding_config="""", is_global_init=false, embedding_config=""""] Registered devices: [CPU, XLA_CPU] Registered kernels: [[ConfigureDistributedTPU]] I0322 13:53:47.967103 140539649898304 main.py:470] Failed 50 times. Retry!

Hi, have u solved this problem?",recent call last file line main file line file line file line run file line file line file line raise type message registered support used node defined registered registered time retry hi problem,issue,negative,positive,neutral,neutral,positive,positive
997344687,"i have the same problem. And i summarize some corresponding questions in mpl code.
 (btw, i use colab-tpu to run this work.)
question 1: i can not find the parameter 'num_cores_per_replica' in main.py(line 71). And the paremeter is not defined in flag_utils.py. 
question 2: i donot know which tpu_lib  is imported in main.py(line 66).
thank you very much!",problem summarize corresponding code use run work question find parameter line defined question know line thank much,issue,negative,positive,positive,positive,positive,positive
997344303,"Hi @hyhieu , your paper is great! and I want to follow your work.
But i have some questions, when i use colab-tpu to run this work.
question 1: i can not find the parameter 'num_cores_per_replica' in main.py(line 71). And the paremeter is not defined in flag_utils.py. @question 2: i donot know which tpu_lib  is imported in main.py(line 66).
thank you very much!",hi paper great want follow work use run work question find parameter line defined question know line thank much,issue,positive,positive,positive,positive,positive,positive
997341572,"change the import format. for example, 'from meta_pseudo_labels import xxx'   change to 'import xxx'",change import format example import change,issue,negative,neutral,neutral,neutral,neutral,neutral
997197652,"No problem, happy to help.

> Also, Is it okay to use these FID implementations?

I think it should be okay as long as you apply the same type of cropping to both generate and evaluate the images. We use central cropping to convert the high-res images into 256x256 (https://github.com/google-research/google-research/blob/master/coltran/datasets.py#L37)

> What is the config file that is passed as input to this and to gen_dataset as well?

The config in my script is just `  config = {'resolution': [FLAGS.resolution, FLAGS.resolution]}`. The above code snippet is to compute the baseline FID between two sets of ground truth images. 
",problem happy help also use fid think long apply type generate evaluate use central convert file input well script code snippet compute fid two ground truth,issue,positive,positive,positive,positive,positive,positive
997058577,"@MechCoder  Thanks a lot for this. I am around halfway done through making one for my dataset, actually running into errors related to TF versions. Hopefully, should be able to resolve this with some more efforts.

Sorry to bother with some more questions

> real_dataset = datasets.get_dataset(
>    name=FLAGS.dataset, subset='test', config=config, batch_size=1)

What is the config file that is passed as input to this and to gen_dataset as well?

Also, Is it okay to use these FID implementations?
https://github.com/mseitzer/pytorch-fid
https://github.com/toshas/torch-fidelity
I have used them to get results for some other models and were pretty straightforward to use.
",thanks lot around halfway done making one actually running related hopefully able resolve sorry bother file input well also use fid used get pretty straightforward use,issue,positive,positive,positive,positive,positive,positive
996568750,"Also, in ./vatt/README.md, Data section:

> Embeddings and Vocabulary
> Depending on the configuration, you might need the pre-trained text embeddings and vocuabulary. Please download this file and extract it under vatt/.

above hyperlink leads to **Forbidden Error 403**, Could you please fix it?

Thank you!",also data section vocabulary depending configuration might need text please file extract forbidden error could please fix thank,issue,negative,neutral,neutral,neutral,neutral,neutral
996096609,"Hello @DNGros.
I read these comments but I can not understand how to use CuBERT and how to fine-tuned it on my special dataset.
Have a nice day! ",hello read understand use special nice day,issue,positive,positive,positive,positive,positive,positive
995078367,"Hi Tim,

Thank you for the recognition for the paper and code.

1) pos_emb_hash_h = tf.cast(pos_emb_hash_h, dtype=tf.int32) should just be
called once. I will remove the redundant line. Thanks for pointing it out.

2) We save each (image, MOS) pair as tf records.
https://www.tensorflow.org/tutorials/load_data/tfrecord#reading_a_tfrecord_file.
More specifically, we use the API of tensorflow dataset. For example, this
is the publicly available imagenet dataset :
https://www.tensorflow.org/datasets/catalog/imagenet2012

Thanks again for the kind words. Hope the answers are helpful.

Best,
Junjie



On Tue, Dec 14, 2021 at 5:08 PM Tim Hunt ***@***.***> wrote:

> @junjiek <https://github.com/junjiek>
> Xie Xie Junjie,
>
> I appreciate you getting back to me.
>
> I saw the use of tf.image.decode_jpeg() in the preprocessor.py and I think
> I can change it to tf.image.decode_image to handle .png and .bmp along with
> .jpg or jpeg. That will save me from prepping images to be in a specific
> format. Provided that the encoding of those files don't require additional
> handling of the features in preprocessing. I haven't tested that yet so I
> will have to see if I can do that.
>
> I also have another question. Is there a reason in the preprocessor.py
> file under the get_hashed_spactial_pos_emb_index function your team
> repeated this line of code pos_emb_hash_h = tf.cast(pos_emb_hash_h,
> dtype=tf.int32). I'm not sure I understand the benefit of running the
> same code here twice.
>
> You mentioned above that you just sent in the (image, MOS) pair to the
> model for training. I assume the image was the ""image path"" to an image
> based on how the code processes it and the MOS or (mean opinion score) was
> just a float value between 1 and 5. To feed in a whole batch of images did
> you just feed in an array of image path/names and MOS scores in a list of
> tuples or did you feed each in as an array and zip them together or did you
> prepare a dictionary file of them to feed to the application? Sorry rather
> long question.
>
> Actually I am not even sure my question above is the right one. How the
> data is fed into the model is more determined by how I prepare it to train.
> I can see points in the code where possibly training type code was
> potentially removed. Since you aren't going to release that code I will
> have to figure out if I can come up with it on my own training code based
> on the paper and the code you have released.
>
> I have learned so much studying your team's code. I love your use of Jax
> to speed up GPU processing of images and I didn't even know until now that
> Tensorflow had a tf.image.extract_patches() function. That must the
> advantage of working with Google. You either learn quickly about the
> advances the Google Research team does in expanding Tensorflow or you are
> able to suggest and maybe help them build them. Either way it is very nice
> work.
>
> Please again accept my appreciation for your team's great skill in
> building such a clean simple model. It is very impressive. I may not be
> able to come up with a way to leverage it and extend it into other fields
> of use and may have to use another model to meet my short term needs but
> nice work and I am enjoying studying your code.
>
> Thanks
> Tim
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/914#issuecomment-994192977>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACHYBJYIYQA4NE243ZVXL5TUQ7TBRANCNFSM5JZX34SQ>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>
",hi thank recognition paper remove redundant line thanks pointing save image pair specifically use example publicly available thanks kind hope helpful best tue hunt wrote appreciate getting back saw use think change handle along save specific format provided require additional handling tested yet see also another question reason file function team repeated line code sure understand benefit running code twice sent image pair model training assume image image path image based code mean opinion score float value feed whole batch feed array image list feed array zip together prepare dictionary file feed application sorry rather long question actually even sure question right one data fed model determined prepare train see code possibly training type code potentially removed since going release code figure come training code based paper code learned much team code love use speed even know function must advantage working either learn quickly research team expanding able suggest maybe help build either way nice work please accept appreciation team great skill building clean simple model impressive may able come way leverage extend use may use another model meet short term need nice work enjoying code thanks reply directly view triage go mobile android,issue,positive,positive,positive,positive,positive,positive
994192977,"@junjiek  
Xie Xie Junjie,

I appreciate you getting back to me.  

I saw the use of tf.image.decode_jpeg() in the preprocessor.py and I think I can change it to tf.image.decode_image to handle .png and .bmp along with .jpg or jpeg.  That will save me from prepping images to be in a specific format. Provided that the encoding of those files don't require additional handling of the features in preprocessing.  I haven't tested that yet so I will have to see if I can do that.

I also have another question.  Is there a reason in the preprocessor.py file under the get_hashed_spactial_pos_emb_index function your team repeated this line of code `pos_emb_hash_h = tf.cast(pos_emb_hash_h, dtype=tf.int32)`. I'm not sure I understand the benefit of running the same code here twice.

You mentioned above that you just sent in the (image, MOS) pair to the model for training.  I assume the image was the ""image path"" to an image based on how the code processes it and the MOS or (mean opinion score) was just a float value between 1 and 5.  To feed in a whole batch of images did you just feed in an array of image path/names and MOS scores in a list of tuples or did you feed each in as an array and zip them together or did you prepare a dictionary file of them to feed to the application?  Sorry rather long question.

Actually I am not even sure my question above is the right one.  How the data is fed into the model is more determined by how I prepare it to train.  I can see points in the code where possibly training type code was potentially removed.  Since you aren't going to release that code I will have to figure out if I can come up with it on my own training code based on the paper and the code you have released.

I have learned so much studying your team's code.  I love your use of Jax to speed up GPU processing of images and I didn't even know until now that Tensorflow had a tf.image.extract_patches() function.  That must the advantage of working with Google.  You either learn quickly about the advances the Google Research team does in expanding Tensorflow or you are able to suggest and maybe help them build them.  Either way it is very nice work.

Please again accept my appreciation for your team's great skill in building such a clean simple model.  It is very impressive.  I may not be able to come up with a way to leverage it and extend it into other fields of use and may have to use another model to meet my short term needs but nice work and I am enjoying studying your code.

Thanks
Tim
",appreciate getting back saw use think change handle along save specific format provided require additional handling tested yet see also another question reason file function team repeated line code sure understand benefit running code twice sent image pair model training assume image image path image based code mean opinion score float value feed whole batch feed array image list feed array zip together prepare dictionary file feed application sorry rather long question actually even sure question right one data fed model determined prepare train see code possibly training type code potentially removed since going release code figure come training code based paper code learned much team code love use speed even know function must advantage working either learn quickly research team expanding able suggest maybe help build either way nice work please accept appreciation team great skill building clean simple model impressive may able come way leverage extend use may use another model meet short term need nice work enjoying code thanks,issue,positive,positive,positive,positive,positive,positive
992930108,"Hi Tim,

Thanks a lot for your interest in the paper and code. At this point we
don't have plans to open source the training code yet.

For training the model, the image format can be anything. In our code we
use tf.image.decode_jpeg() by default. The MOS is a single value for each
image. For AVA, the label for each image is a score distribution.  In
training, we just feed the (image, MOS) pair to the model.

Hope it helps,
Junjie

On Mon, Dec 13, 2021 at 12:48 PM Tim Hunt ***@***.***> wrote:

> @junjiek <https://github.com/junjiek>
> I understand if you can't share the script that you used to train the
> model. I have been studying the paper and the code and feel I can perhaps
> now come up with a script to do training. However, I have a question. I
> suspect that to train the model I would need to provide the images and our
> MOS scores. However, I am not sure what format they need to be in. Do the
> images need to be .jpg? How about the MOS scores. What is the best way to
> provide the model the MOS score for each image? If you can help I would
> appreciate it as I would like to see if I can make this model focus on a
> specific kind of image and use the existing models as a transformer to
> focus the MOS scores for these images. Thanks.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/914#issuecomment-992892720>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACHYBJ36KOV7RKCAITAH5FTUQZLZNANCNFSM5JZX34SQ>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>
",hi thanks lot interest paper code point open source training code yet training model image format anything code use default single value image ava label image score distribution training feed image pair model hope mon hunt wrote understand ca share script used train model paper code feel perhaps come script training however question suspect train model would need provide however sure format need need best way provide model score image help would appreciate would like see make model focus specific kind image use transformer focus thanks reply directly view triage go mobile android,issue,positive,positive,positive,positive,positive,positive
992892720,"@junjiek 
I understand if you can't share the script that you used to train the model.  I have been studying the paper and the code and feel I can perhaps now come up with a script to do training.  However, I have a question. I suspect that to train the model I would need to provide the images and our MOS scores.  However, I am not sure what format they need to be in. Do the images need to be .jpg?  How about the MOS scores.  What is the best way to provide the model the MOS score for each image?  If you can help I would appreciate it as I would like to see if I can make this model focus on a specific kind of image and use the existing models as a transformer to focus the MOS scores for these images.    Thanks.",understand ca share script used train model paper code feel perhaps come script training however question suspect train model would need provide however sure format need need best way provide model score image help would appreciate would like see make model focus specific kind image use transformer focus thanks,issue,positive,positive,positive,positive,positive,positive
991695212,"> I guess gbash.sh is internal to Google and we can’t get it, but it should be possible to remove this line of code and then change the code to runnable
> […](#)
> ------------------&nbsp;原始邮件&nbsp;------------------ 发件人: ""google-research/google-research"" ***@***.***&gt;; 发送时间:&nbsp;2021年12月11日(星期六) 中午1:04 ***@***.***&gt;; 抄送:&nbsp;""北京航空航天大学 ***@***.******@***.***&gt;; 主题:&nbsp;Re: [google-research/google-research] Question about seq2act (#658) Hello, where can we find gbash.sh file? — You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or unsubscribe. Triage notifications on the go with GitHub Mobile for iOS or Android.

Thank you so much I will try it",guess internal get possible remove line code change code runnable question hello find file thread reply directly view triage go mobile android thank much try,issue,negative,positive,neutral,neutral,positive,positive
991468455,"I guess gbash.sh is internal to Google and we can’t get it, but it should be possible to remove this line of code and then change the code to runnable




------------------&nbsp;原始邮件&nbsp;------------------
发件人:                                                                                                                        ""google-research/google-research""                                                                                    ***@***.***&gt;;
发送时间:&nbsp;2021年12月11日(星期六) 中午1:04
***@***.***&gt;;
抄送:&nbsp;""北京航空航天大学 ***@***.******@***.***&gt;;
主题:&nbsp;Re: [google-research/google-research] Question about seq2act (#658)





 
Hello, where can we find gbash.sh file?
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.
Triage notifications on the go with GitHub Mobile for iOS or Android.",guess internal get possible remove line code change code runnable question hello find file thread reply directly view triage go mobile android,issue,negative,positive,neutral,neutral,positive,positive
986182968,"> I have the same problem, could you solve the problem yet?

No, i can't. It's so sad!",problem could solve problem yet ca sad,issue,negative,negative,negative,negative,negative,negative
986178741,"Here is the FID Script that I used for ImageNet. Hope you can adapt it to your dataset. I used the TFGAN implementation.
(https://github.com/tensorflow/gan/blob/99bb93042520040dac401237616c10e54ab80a9f/tensorflow_gan/python/eval/inception_metrics.py#L130)

```
def normalize(x):
  # inception checkpoints expects inputs to be in [-1, 1]
  # https://codesearch.corp.google.com/piper///depot/google3/third_party/py/tensorflow_gan/examples/cifar/eval_lib.py?dr=CSs&g=0&l=34.
  # https://codesearch.corp.google.com/piper///depot/google3/third_party/py/tensorflow_gan/examples/cifar/data_provider.py?dr=CSs&g=0&l=40
  x = tf.squeeze(x['image'], axis=0)
  logging.info(x.shape)
  x = tf.to_float(x)
  # Normalize from [0, 255] to [-1.0, 1.0]
  x = (x / 128.0) - 1.0
  return x


# Real dataset.
real_dataset = datasets.get_dataset(
    name=FLAGS.dataset, subset='test', config=config, batch_size=1)
real_dataset = real_dataset.map(normalize, num_parallel_calls=100)
real_dataset = real_dataset.skip(FLAGS.samples)
real_dataset = real_dataset.batch(batch_size=FLAGS.batch_size)
real_dataset = real_dataset.skip(skip_samples // FLAGS.batch_size)
real_iterator = tf.compat.v1.data.make_initializable_iterator(real_dataset)
real_dataset = real_iterator.get_next()

gen_dataset = datasets.get_dataset(
    name=FLAGS.dataset, subset='test', config=config, batch_size=1)
gen_dataset = gen_dataset.map(normalize, num_parallel_calls=100)
gen_dataset = gen_dataset.batch(batch_size=FLAGS.batch_size)
gen_iterator = tf.compat.v1.data.make_initializable_iterator(gen_dataset)
gen_dataset = gen_iterator.get_next()

fid_stream = tfgan.eval.frechet_inception_distance_streaming
distance, update_op = fid_stream(real_dataset, gen_dataset)
logging.info(distance)
logging.info(update_op)

batch_size = FLAGS.batch_size
with tf.Session() as sess:
  init_ops = ([real_iterator.initializer, gen_iterator.initializer,
               tf.initialize_local_variables()])
  sess.run(init_ops)

  for epoch in range(1, num_epochs + 1):
    sess.run(update_op)

    if epoch % 10 == 0:
      dist_np = sess.run(distance)
      fid_str = f'Number of samples: {epoch * batch_size}, fid: {dist_np}'
      logging.info(fid_str)
  distance_np = sess.run(distance)
  logging.info(distance_np)
```",fid script used hope adapt used implementation normalize inception normalize return real normalize normalize distance distance sess epoch range epoch distance epoch fid distance,issue,negative,positive,positive,positive,positive,positive
986178462,"> How long do I need to train on a custom dataset?

The longer you train, the results will be better.  I would use the maximum batch-size that fits in memory and train for around 500K steps. There should be a `train_summaries` subdirectory in `/content/drive/MyDrive/Colab_Work/HONORS/ColTran-v2/google-research/coltran/logs/cityscapes_ckpt`. For a sanity check, you could point tensorboard to this directory to see if the train loss goes down. 

> How to get the result/output for a custom dataset?

For a custom dataset, as ling as your dataset directory is supported by `tf.io.decode_image` (https://www.tensorflow.org/api_docs/python/tf/io/decode_image), it should work.

> till GPU resource got exhausted

Btw, the GPU should not OOM during training. That seems a bit weird.
",long need train custom longer train better would use maximum memory train around sanity check could point directory see train loss go get custom custom ling directory work till resource got exhausted training bit weird,issue,negative,negative,negative,negative,negative,negative
986079330,"> I modified the run_squad.py script to remove the contrib module in order to export a quantized tflite model file in tf 2.4.0-dev20200712 version. I used two saved_model; one is provided (https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/mobilebert_squad_savedmodels.tar.gz), the other one is trained as guided. I got the same error during convert.
> 
> > loc(""bert/encoder/layer_2/attention/self/clip_by_value/Minimum""): error: 'tfl.minimum' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.32020080089569092:-67> vs. !quant.uniform<i8:f32, 0.18242761492729187:-77>
> > Traceback (most recent call last):
> > File ""export_tflite.py"", line 642, in 
> > tf.app.run()
> > File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
> > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
> > File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
> > _run_main(main, args)
> > File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
> > sys.exit(main(argv))
> > File ""export_tflite.py"", line 632, in main
> > tflite_model = converter.convert()
> > File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 1972, in convert
> > return super(TFLiteConverter, self).convert()
> > File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 1341, in convert
> > result = self._calibrate_quantize_model(result, **flags)
> > File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 444, in _calibrate_quantize_model
> > return _mlir_quantize(calibrated)
> > File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py"", line 147, in mlir_quantize
> > inference_type)
> > File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/wrap_toco.py"", line 52, in wrapped_experimental_mlir_quantize
> > inference_type)
> > RuntimeError: Failed to quantize: :0: error: loc(""bert/encoder/layer_2/attention/self/clip_by_value/Minimum""): 'tfl.minimum' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.32020080089569092:-67> vs. !quant.uniform<i8:f32, 0.18242761492729187:-77>
> > :0: note: loc(""bert/encoder/layer_2/attention/self/clip_by_value/Minimum""): see current operation: %3377 = ""tfl.minimum""(%3373, %36) : (tensor<1x4x384x384x!quant.uniform<i8:f32, 0.32020080089569092:-67>>, tensor<!quant.uniform<i8:f32, 0.18242761492729187:-77>>) -> tensor<1x4x384x384x!quant.uniform<i8:f32, 0.32020080089569092:-67>>

Hi, @nadongguri , I'm having the exact issue you had as the following:
```
Traceback (most recent call last):
  File ""mobilebert/run_squad.py"", line 32, in <module>
    from mobilebert import modeling
  File ""/home/khsu4/google-research/mobilebert/modeling.py"", line 32, in <module>
    from tensorflow.contrib import layers as contrib_layers
ModuleNotFoundError: No module named 'tensorflow.contrib'
```

What did you do exactly on modifying run_squad.py? thanks.",script remove module order export model file version used two one provided one trained got error convert error quantization violate scale constraint recent call last file line file line run file line run main file line main file line main file line convert return super self file line convert result result file line return file line file line quantize error quantization violate scale constraint note see current operation tensor tensor tensor hi exact issue following recent call last file line module import modeling file line module import module exactly thanks,issue,negative,positive,positive,positive,positive,positive
986079301,Hi! We've recently updated our repo with random keypoint dropout during training to handle missing keypoints (as well as added a temporal version of our embedding). Please see the updated repo and let us know if you have any questions. ,hi recently random dropout training handle missing well added temporal version please see let u know,issue,negative,negative,negative,negative,negative,negative
986079123,"How did you work when you use tf2.3/2.4/2.6? What did you modify exactly?

I tried tf2.x and got this error: 
```
Traceback (most recent call last):
  File ""mobilebert/run_squad.py"", line 32, in <module>
    from mobilebert import modeling
  File ""/home/khsu4/google-research/mobilebert/modeling.py"", line 32, in <module>
    from tensorflow.contrib import layers as contrib_layers
ModuleNotFoundError: No module named 'tensorflow.contrib'
```
It seems like the same error message as mentioned [here](https://github.com/google-research/google-research/issues/325), any idea?",work use modify exactly tried got error recent call last file line module import modeling file line module import module like error message idea,issue,negative,positive,neutral,neutral,positive,positive
985632649,"The sampling is stochastic by default, so each run should give you different results. See https://github.com/google-research/google-research/blob/master/coltran/models/colorizer.py#L282",sampling stochastic default run give different see,issue,negative,neutral,neutral,neutral,neutral,neutral
985630528,"> Also, the paper mentions 3 different coloured outputs for one input bnw image. How to get such results?

You can just run the sampling 3 times, and it will give you 3 different results.

",also paper different one input image get run sampling time give different,issue,negative,neutral,neutral,neutral,neutral,neutral
983999792,"Hi, @lazrak-mouad  can you please tell me how to go about evaluating a model trained on custom dataset? ",hi please tell go model trained custom,issue,negative,neutral,neutral,neutral,neutral,neutral
983233354,"@srewai please have a look at [issues/836](https://github.com/google-research/google-research/issues/836) it discusses post training quantization with calibration data. I am also working on quantization aware training which will not requre calibration data, but I do not have ETA yet.",please look post training quantization calibration data also working quantization aware training calibration data eta yet,issue,negative,positive,positive,positive,positive,positive
982529423,"Hey @sakatoable,
did you were able to extract the object motion?
Thanks in advance,
Michael
",hey able extract object motion thanks advance,issue,negative,positive,positive,positive,positive,positive
981851158,"@rybakov , while performing quantization there is talk about calibration dataset for full quantization. How do I achieve this, are there steps mentioned anywhere?",quantization talk calibration full quantization achieve anywhere,issue,negative,positive,positive,positive,positive,positive
980831420,"I have the same problem, could you solve the problem yet?",problem could solve problem yet,issue,negative,neutral,neutral,neutral,neutral,neutral
979686310,We don't expect there to be a big difference due to the buffer size flag. Could you provide the full sets of flags you used in both settings?,expect big difference due buffer size flag could provide full used,issue,negative,positive,neutral,neutral,positive,positive
977989774,"I solved the problem in the conda env with
- pyhton 3.6
- cuda 10.1.243 (update 2) 
- building tensorflow 1.15 from source (for this version bazel must be 0.26.1 at most). 
- also if there are problems with numpy, just downgrade to 1.18.5

I set the env variables:
- $LD_LIBRARY_PATH to /usr/local/cuda-10.1/targets/x86_64-linux/lib
- $PATH to /usr/local/cuda-10.1/bin

https://forums.developer.nvidia.com/t/cublas-for-10-1-is-missing/71015/18
The answer in this forum really helped me with the packages in the cuda folder that are not present when you install that version.",problem update building source version must also downgrade set path answer forum really folder present install version,issue,negative,positive,neutral,neutral,positive,positive
977531478,I also encountered some errors when I used tensorflow2. Then I changed to tensorflow 1.15 and everything worked out just fine.,also used everything worked fine,issue,negative,positive,positive,positive,positive,positive
975825581,"Does this comment fix your issue? (https://github.com/google-research/google-research/issues/838#issuecomment-930699980)

You can set the `--batch_size` flag to 1.",comment fix issue set flag,issue,negative,neutral,neutral,neutral,neutral,neutral
975490658,Looking closer there was also a change in -input_shuffle_buffer_size=2000000 for deterministic model. For probabilistic training we used -input_shuffle_buffer_size=1000000. Could that be the reason? All other flags are the same,looking closer also change deterministic model probabilistic training used could reason,issue,negative,neutral,neutral,neutral,neutral,neutral
973975533,"> Hello, have you found a solutions for ur questions? Can you share the notebook for custom dataset?

I have provided a link to the notebook above. 
Posting again,
[Link to Colab Notebook](https://colab.research.google.com/drive/1hG5t2djdrToWGyQdvGJ3Sf_DkDfWi5jE?usp=sharing)",hello found ur share notebook custom provided link notebook posting link notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
972792332,"Hello, have you found a solutions for ur questions? Can you share the notebook for custom dataset?",hello found ur share notebook custom,issue,negative,neutral,neutral,neutral,neutral,neutral
972778718,"Thanks for your reply, the work of Takács et al. is very effective in optimizing iALS loss, but unfortunately for some reason is not very well known.

> In our experiment, we used Cholesky as a solution method, but it can be replaced by a CG-based solution.

That's brilliant! I am looking forward to the updated version.",thanks reply work al effective loss unfortunately reason well known experiment used solution method solution brilliant looking forward version,issue,positive,positive,positive,positive,positive,positive
972462725,"the better solution is to modify tft_model.py from
import tensorflow as tf
to
import tensorflow.compat.v1 as tf
",better solution modify import import,issue,positive,positive,positive,positive,positive,positive
972460258,"Sorry, I found that change at the include is the better way",sorry found change include better way,issue,negative,neutral,neutral,neutral,neutral,neutral
971866717,"Hi @tohtsky
Thank you for bringing this work to our attention. We will update our paper to include a discussion/comparison with the CG-based method of Takács et al.
One can also combine the CG method with iALS++ as follows. In iALS++, we solve a sequence of least squares problems on different subspaces. In our experiments we used Cholesky as a solution method, but that can be replaced by a CG-based method. This could lead to a further speedup.",hi thank work attention update paper include method al one also combine method solve sequence least different used solution method method could lead,issue,positive,negative,negative,negative,negative,negative
970748046,"Hello! Could you provide the flags you used for training the probabilistic and deterministic model? Training the deterministic model should not be much slower than the probabilistic one.

@Shegun93 Unfortunately we did not see your issues. Could you make sure to use ""[POEM]"" in the title so we can find them easily? Thanks!",hello could provide used training probabilistic deterministic model training deterministic model much probabilistic one unfortunately see could make sure use poem title find easily thanks,issue,positive,positive,neutral,neutral,positive,positive
968869075,"Hello Valayramani,

I See that a few months have passed since you've opened this ticket,
have you by any chance made any progress and are able to share it?

thanks,
Omer.

",hello see since ticket chance made progress able share thanks omer,issue,positive,positive,positive,positive,positive,positive
967299892,"@vishalkevat007 You can run a test on new data: just change the path, but data reader has its own logic for data split and I do not know if that is what you want. The data reader with data splitting logic was taken from the original [speech commands paper](https://arxiv.org/abs/1804.03209) for compatibility with previously published results. 

Given that you are customizing your own data, I would suggest using [approach with training/testing on customized data](https://github.com/google-research/google-research/tree/master/kws_streaming#training-on-custom-data), discussed in [thread](https://github.com/google-research/google-research/issues/809). This way you have direct control over training and testing data sets - it will simplify your experiments.",run test new data change path data reader logic data split know want data reader data splitting logic taken original speech paper compatibility previously given data would suggest approach data thread way direct control training testing data simplify,issue,negative,positive,positive,positive,positive,positive
966174506,"> Hi, @Wayne-wonderai , Thanks for your question. We haven’t seen this symbol missing before. I guess it’s cuda version related. As you using the pip version of tensorflow 2.3, which is with cuda 10?
> 
> In any case, please try compiling the ops locally.
> 
> We will clarify in the instruction.

Hi, I understand how busy your schedule might be but i will appreciate it if you take some time to help me look into the issue I have been stuck with while trying to test the sparse network

INFO: Elapsed time: 0.517s, Critical Path: 0.00s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
INFO: Build completed successfully, 1 total action
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //:sparse_conv_ops_py_test
-----------------------------------------------------------------------------
2021-11-11 10:09:26.373640: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Running tests under Python 3.6.12: /usr/bin/python3
[ RUN      ] SparseConvOpTest.test_session
[  SKIPPED ] SparseConvOpTest.test_session
[ RUN      ] SparseConvOpTest.test_spar_conv_op
2021-11-11 10:09:27.683195: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-11-11 10:09:27.694249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-11 10:09:27.695031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0
coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s
2021-11-11 10:09:27.695048: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-11-11 10:09:27.696971: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-11-11 10:09:27.698662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-11-11 10:09:27.699302: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-11-11 10:09:27.701228: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-11-11 10:09:27.702321: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-11-11 10:09:27.706496: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-11-11 10:09:27.706764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-11 10:09:27.707604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-11 10:09:27.708390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-11-11 10:09:27.708698: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-11 10:09:27.735997: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2599990000 Hz
2021-11-11 10:09:27.736540: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52c7260 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-11-11 10:09:27.736563: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-11-11 10:09:27.769110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-11 10:09:27.769955: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x535c5e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-11-11 10:09:27.769982: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0
2021-11-11 10:09:27.770245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-11 10:09:27.770798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0
coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s
2021-11-11 10:09:27.770819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-11-11 10:09:27.770857: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-11-11 10:09:27.770874: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-11-11 10:09:27.770885: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-11-11 10:09:27.770899: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-11-11 10:09:27.770913: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-11-11 10:09:27.770927: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-11-11 10:09:27.771004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-11 10:09:27.771580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-11-11 10:09:27.772104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-11-11 10:09:27.772130: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
INFO:tensorflow:time(__main__.SparseConvOpTest.test_spar_conv_op): 0.28s
I1111 10:09:27.961207 140067725731584 test_util.py:1973] time(__main__.SparseConvOpTest.test_spar_conv_op): 0.28s
[  FAILED  ] SparseConvOpTest.test_spar_conv_op
======================================================================
ERROR: test_spar_conv_op (__main__.SparseConvOpTest)
SparseConvOpTest.test_spar_conv_op
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/39e26583fdab04cdafc9b2ab1c2b6d71/execroot/__main__/bazel-out/k8-opt/bin/sparse_conv_ops_py_test.runfiles/__main__/tensorflow_sparse_conv_ops/sparse_conv_ops_test.py"", line 32, in test_spar_conv_op
    dtype=tf.float32)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
    allow_broadcast=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 275, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 300, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 97, in convert_to_eager_tensor
    ctx.ensure_initialized()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py"", line 539, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid

----------------------------------------------------------------------
Ran 2 tests in 0.281s

FAILED (errors=1, skipped=1)
root@0bdfcf54f13f:/tf3d/ops# 

please help.",hi thanks question seen symbol missing guess version related pip version case please try locally clarify instruction hi understand busy schedule might appreciate take time help look issue stuck trying test sparse network time critical path build successfully total action build successfully total action pager exit successfully dynamic library running python run run successfully dynamic library successful node read negative value must least one node node zero found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible binary deep neural network library use following enable rebuild appropriate compiler frequency service platform host guarantee used device host default version successful node read negative value must least one node node zero service platform guarantee used device compute capability successful node read negative value must least one node node zero found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible successfully dynamic library time time error recent call last file line file line constant file line return value shape file line value file line file line implicit status device kernel image invalid ran root please help,issue,positive,positive,neutral,neutral,positive,positive
965990540,"```python
line 24 of felix_tagger.py
from official.nlp.modeling import layers
should be
from official.nlp.keras_nlp import layers
```
Above is what I meant by reverting back to old codes, which works too.
If you don't want to do this, you can clone the latest `google-models` codes and add them to your python path (or something similar):
```bash
# Need to clone: git clone https://github.com/tensorflow/models.git google-models
export PYTHONPATH=$PYTHONPATH:your/path/to/google-models
```",python line import import meant back old work want clone latest add python path something similar bash need clone git clone export,issue,negative,positive,positive,positive,positive,positive
964837633,"Here's a same mistake when I ""import jax"" in python, the error info says:
`import jax

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/jax/__init__.py"", line 93, in <module>
    from . import image
…………

TypeError: jit(): incompatible function arguments. The following argument types are supported:
    1. (fun: function, cache_miss: function, get_device: function, static_argnums: List[int], static_argnames: List[str] = [], donate_argnums: List[int] = [], cache: jaxlib.xla_extension.jax_jit.CompiledFunctionCache = None) -> object

Invoked with: <function _rfft_transpose at 0x7f5d7f4f70d0>, <function _cpp_jit.<locals>.cache_miss at 0x7f5d7f4f7160>, <function _cpp_jit.<locals>.get_device_info at 0x7f5d7f4f71f0>, <function _cpp_jit.<locals>.get_jax_enable_x64 at 0x7f5d7f4f7280>, <function _cpp_jit.<locals>.get_jax_disable_jit_flag at 0x7f5d7f4f7310>, (0, 2)`

and I don't know how to solve it.",mistake import python error import recent call last file line module file line module import image incompatible function following argument fun function function function list list list cache none object function function function function function know solve,issue,negative,positive,neutral,neutral,positive,positive
964429497,"Hi, thanks so much for your email

I had opened issues more than 4 Times couldn't get a response from the
authors. Would appreciate any form of assistance on it. I'm far behind
schedule in the implementation of the code.

Thanks.

On Tue, Nov 9, 2021, 6:54 PM Andres Herrera ***@***.***>
wrote:

> I've been training the probabilistic model and the deterministic one, and
> I noticed that the training in the deterministic one is much slower than
> the probabilistic. Is this behavior expected? Thanks in advance
>
> Hi andreherrera97, i have been having problem implementing the code
> contained in this repository seeing you commenting here only means you must
> have worked on it. would you mind giving me a helping hand? I'm stuck in
> step 6 of ops thanks best regards
>
> Hello Shgun93, I really appreciate your confidence in my understanding of
> this code but I would recommend opening an issue. The last time I opened
> one the authors answered me around a week later.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/877#issuecomment-964392890>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ARIHG5JOBUFZ2MNGHKJT4DDULFN43ANCNFSM5HVFG7HA>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>
",hi thanks much time could get response would appreciate form assistance far behind schedule implementation code thanks tue wrote training probabilistic model deterministic one training deterministic one much probabilistic behavior thanks advance hi problem code repository seeing must worked would mind giving helping hand stuck step thanks best hello really appreciate confidence understanding code would recommend opening issue last time one around week later reply directly view triage go mobile android,issue,positive,positive,positive,positive,positive,positive
964392890,"> > I've been training the probabilistic model and the deterministic one, and I noticed that the training in the deterministic one is much slower than the probabilistic. Is this behavior expected? Thanks in advance
> 
> Hi andreherrera97, i have been having problem implementing the code contained in this repository seeing you commenting here only means you must have worked on it. would you mind giving me a helping hand? I'm stuck in step 6 of ops thanks best regards

Hello Shgun93, I really appreciate your confidence in my understanding of this code but I would recommend opening an issue. The last time I opened one the authors answered me around a week later.  ",training probabilistic model deterministic one training deterministic one much probabilistic behavior thanks advance hi problem code repository seeing must worked would mind giving helping hand stuck step thanks best hello really appreciate confidence understanding code would recommend opening issue last time one around week later,issue,positive,positive,positive,positive,positive,positive
964381214,"On second thoughts, it may just be that the problem is so easy that even corrupt examples aren't damaging the performance?",second may problem easy even corrupt performance,issue,negative,negative,neutral,neutral,negative,negative
964217342,"Sounds good. I have a problem with fitting the model I got another problem. I followed the steps on this link (https://aihub.cloud.google.com/p/products%2F9f39ad8d-ad81-4fd9-8238-5186d36db2ec) 
![image](https://user-images.githubusercontent.com/69587655/140945302-87121cd5-8563-4920-9100-d62e7509f4e8.png)

",good problem fitting model got another problem link image,issue,negative,positive,positive,positive,positive,positive
964211183,"Hi,
yes, I got very promising results, but it took a while to change all the parameters (e.g. valid set and especially input parameters and -type) to get it. Totally depends on dataseth though...",hi yes got promising took change valid set especially input get totally though,issue,positive,positive,neutral,neutral,positive,positive
964188734,"Thanks, it works fine now.
Another question, did you test the TFT model and get any results? I am struggling with this.
",thanks work fine another question test model get struggling,issue,negative,positive,positive,positive,positive,positive
964152436,"> > > > i konw it
> > > > > > > i change bert config.json vocab_size = 30522 , make success! but a new problem is i don't have a vocabulary of size 30522. can you provide one？ after, i will share torch bin model. thank you very much！
> > > > > > 
> > > > > > 
> > > > > > Is there any time need the vocabulary which size of 30522? Cause i download the 'bert-base-uncased' which is the pytorch version, and it's config show that ""vocab_size"": 30522... Also, i have fine-tune the bert model to train QQP dataset, and don't need use such that vocabulary...
> > > > > 
> > > > > 
> > > > > OOO, I figure it out! When making dataloader, may have to use that! You can get it from https://huggingface.co/bert-base-uncased/blob/main/vocab.txt Hope this can help you As for loading data, I think tranformer's tokenizer is a very good tool...Recommend!
> > > > 
> > > > 
> > > > Because I have been using Chinese vocab before, ignoring the English vocab size is 30522, this is a low-level error .😂
> > > 
> > > 
> > > can I ask the pytorch-version model?hhh by the way, maybe we can communicate in Chinese
> > 
> > 
> > I have found the pytorch-version model already~
> 
> 不好意思，没看到你的回复，我马上上传Google网盘发给你

我找到啦~~有现成的，地址在这： https://huggingface.co/google/fnet-base",change make success new problem vocabulary size provide share torch bin model thank time need vocabulary size cause version show also model train need use vocabulary figure making may use get hope help loading data think good tool recommend size error ask model way maybe communicate found model,issue,positive,positive,positive,positive,positive,positive
964118455,"> I've been training the probabilistic model and the deterministic one, and I noticed that the training in the deterministic one is much slower than the probabilistic. Is this behavior expected? Thanks in advance

Hi andreherrera97, i have been having problem implementing the code contained in this repository seeing you commenting here only means you must have worked on it. would you mind giving me a helping hand? I'm stuck in step 6 of ops
thanks best regards",training probabilistic model deterministic one training deterministic one much probabilistic behavior thanks advance hi problem code repository seeing must worked would mind giving helping hand stuck step thanks best,issue,positive,positive,positive,positive,positive,positive
963936566,"> i konw it
> 
> > > > i change bert config.json vocab_size = 30522 , make success! but a new problem is i don't have a vocabulary of size 30522. can you provide one？ after, i will share torch bin model. thank you very much！
> > > 
> > > 
> > > Is there any time need the vocabulary which size of 30522? Cause i download the 'bert-base-uncased' which is the pytorch version, and it's config show that ""vocab_size"": 30522... Also, i have fine-tune the bert model to train QQP dataset, and don't need use such that vocabulary...
> > 
> > 
> > OOO, I figure it out! When making dataloader, may have to use that! You can get it from https://huggingface.co/bert-base-uncased/blob/main/vocab.txt Hope this can help you As for loading data, I think tranformer's tokenizer is a very good tool...Recommend!
> 
> Because I have been using Chinese vocab before, ignoring the English vocab size is 30522, this is a low-level error .😂

can I ask the pytorch-version model?hhh
by the way, maybe we can communicate in Chinese",change make success new problem vocabulary size provide share torch bin model thank time need vocabulary size cause version show also model train need use vocabulary figure making may use get hope help loading data think good tool recommend size error ask model way maybe communicate,issue,positive,positive,positive,positive,positive,positive
963740647,"i konw it 

> > > i change bert config.json vocab_size = 30522 , make success! but a new problem is i don't have a vocabulary of size 30522. can you provide one？ after, i will share torch bin model. thank you very much！
> > 
> > 
> > Is there any time need the vocabulary which size of 30522? Cause i download the 'bert-base-uncased' which is the pytorch version, and it's config show that ""vocab_size"": 30522... Also, i have fine-tune the bert model to train QQP dataset, and don't need use such that vocabulary...
> 
> OOO, I figure it out! When making dataloader, may have to use that! You can get it from https://huggingface.co/bert-base-uncased/blob/main/vocab.txt Hope this can help you As for loading data, I think tranformer's tokenizer is a very good tool...Recommend!

Because I have been using Chinese vocab before, ignoring the English vocab size is 30522, this is a low-level error .😂",change make success new problem vocabulary size provide share torch bin model thank time need vocabulary size cause version show also model train need use vocabulary figure making may use get hope help loading data think good tool recommend size error,issue,positive,positive,positive,positive,positive,positive
963735055,"> > > i change bert config.json vocab_size = 30522 , make success! but a new problem is i don't have a vocabulary of size 30522. can you provide one？ after, i will share torch bin model. thank you very much！
> > 
> > 
> > Is there any time need the vocabulary which size of 30522? Cause i download the 'bert-base-uncased' which is the pytorch version, and it's config show that ""vocab_size"": 30522... Also, i have fine-tune the bert model to train QQP dataset, and don't need use such that vocabulary...
> 
> OOO, I figure it out! When making dataloader, may have to use that! You can get it from https://huggingface.co/bert-base-uncased/blob/main/vocab.txt Hope this can help you As for loading data, I think tranformer's tokenizer is a very good tool...Recommend!
thank you very much for your help! ",change make success new problem vocabulary size provide share torch bin model thank time need vocabulary size cause version show also model train need use vocabulary figure making may use get hope help loading data think good tool recommend thank much help,issue,positive,positive,positive,positive,positive,positive
963182113,"> > i change bert config.json vocab_size = 30522 , make success! but a new problem is i don't have a vocabulary of size 30522. can you provide one？ after, i will share torch bin model. thank you very much！
> 
> Is there any time need the vocabulary which size of 30522? Cause i download the 'bert-base-uncased' which is the pytorch version, and it's config show that ""vocab_size"": 30522... Also, i have fine-tune the bert model to train QQP dataset, and don't need use such that vocabulary...

OOO, I figure it out!  When making dataloader, may have to use that!
You can get it from https://huggingface.co/bert-base-uncased/blob/main/vocab.txt
Hope this can help you 
As for loading data, I think tranformer's tokenizer is a very good tool...Recommend!",change make success new problem vocabulary size provide share torch bin model thank time need vocabulary size cause version show also model train need use vocabulary figure making may use get hope help loading data think good tool recommend,issue,positive,positive,positive,positive,positive,positive
963170387,"> i change bert config.json vocab_size = 30522 , make success! but a new problem is i don't have a vocabulary of size 30522. can you provide one？ after, i will share torch bin model. thank you very much！

Also, can i ask to have the pytorch version pre-trained model....looking forward to your apply!",change make success new problem vocabulary size provide share torch bin model thank also ask version model looking forward apply,issue,positive,positive,positive,positive,positive,positive
963165954,"> i change bert config.json vocab_size = 30522 , make success! but a new problem is i don't have a vocabulary of size 30522. can you provide one？ after, i will share torch bin model. thank you very much！

Is there any time need the vocabulary which size of 30522?   Cause i download the 'bert-base-uncased' which is the pytorch version, and it's config show that ""vocab_size"": 30522...   Also, i have fine-tune the bert model to train QQP dataset, and don't need use such that vocabulary...",change make success new problem vocabulary size provide share torch bin model thank time need vocabulary size cause version show also model train need use vocabulary,issue,positive,positive,positive,positive,positive,positive
963113178,"Hi,
got a comparable problem by using tf 2.x (where there is no ConfigProto anymore). I solved it by using the automatic update mechanism: https://www.tensorflow.org/guide/migrate/upgrade. It goes through all scripts and ensures the compatibility.

Hope that helps.",hi got comparable problem automatic update mechanism go compatibility hope,issue,negative,neutral,neutral,neutral,neutral,neutral
962520449,"Hi, 

I updated the utils.py. Still, I got the same error.
I would like to train the model using the CPU.
Any recommendations, please?
Thanks
",hi still got error would like train model please thanks,issue,positive,positive,positive,positive,positive,positive
960837315,"> Hi,
> 
> I would like to report that, the recent commits to FELIX ([22119bc](https://github.com/google-research/google-research/commit/22119bcefacaf7ae696d3c9cb57f9760e135a622)), and to `tensorflow/models` ([tensorflow/models@9b23daf#diff-163581806ba7ae5f9a9dc1f68f99befae3e361d4e702f5a31d0fceb4ae9a45bf](https://github.com/tensorflow/models/commit/9b23daf9daaa1b44aee32e9f2e3b7ee5c9462b11#diff-163581806ba7ae5f9a9dc1f68f99befae3e361d4e702f5a31d0fceb4ae9a45bf)) result in a breaking change in the import of `layers` modules.
> 
> FELIX asks for `tf-models-official==2.4.0` and imports `official.nlp.modeling.layers.position_embedding import PositionEmbedding`, but such class is not present in versions `2.4.0`, nor `2.5.0` and `2.6.0` (latest release in Aug.). It is only present in the current master version of the repository (https://github.com/tensorflow/models/blob/ac7f9e7f2d0508913947242bad3e23ef7cae5a43/official/nlp/modeling/layers/__init__.py#L37) which is not released yet.
> 
> So for the FELIX codes to work, one should either use the old version of the codes or manually clone the master version.


line 24 of felix_tagger.py
from official.nlp.modeling import layers
should be
from official.nlp.keras_nlp import layers

",hi would like report recent result breaking change import import class present latest release present current master version repository yet work one either use old version manually clone master version line import import,issue,negative,positive,neutral,neutral,positive,positive
955845070,"I have the same question, seems like this model use the targets to predict the targets!!",question like model use predict,issue,negative,neutral,neutral,neutral,neutral,neutral
955628299,"> Hi
> 
> For me there are two items on [https://github.com/google-research/google-research/blob/master/tabnet/tabnet_model.py](url) that are different from what was reported in the paper [https://arxiv.org/pdf/1908.07442.pdf](url)
> 
> On **interpretability** section the author @soarik says that: ![](https://render.githubusercontent.com/render/math?math=\sum_{c=1}^{N_d} ReLU(d_{b,c}[i])) But the given code in github is dividing this sum by the number of steps: (line 177) `# Aggregated masks are used for visualization of the` ` # feature importance attributes.` ` scale_agg = tf.reduce_sum( decision_out, axis=1, keep_dims=True) / ( self.num_decision_steps - 1)` Also, for the aggregate feature importance mask, it looks like the normalization is missing. The paper states: ![](https://render.githubusercontent.com/render/math?math=\frac{ \sum_{i=1}^{N_{steps}} \eta_b[i]M_{b,j}[i]} {\sum_{j=1}^{D}\sum_{i=1}^{N_{steps}} \eta_b[i]M_{b,j}[i]}) But the give code is: (line 182) `aggregated_mask_values += mask_values * scale_agg` Inside a for looping over each step, which will give only the top part of the equation.
> 
> Am I missing something or the paper and code are really different?

Hello,as the README.md said,I use `python -m test_experiment_covertype.py` for simple test,but its result is obviously wrong,The traing loss and val accuracy are irregular,just because it's  in the low-resource environment?How can I fix this problem?
![KH HVLC(7N@1KB$A@Y%FBDF](https://user-images.githubusercontent.com/41032255/139565232-955f7e1f-b1e8-41bf-b6ac-4df88770aa7b.png)
",hi two different paper interpretability section author given code dividing sum number line used visualization feature importance also aggregate feature importance mask like normalization missing paper give code line inside looping step give top part equation missing something paper code really different hello said use python simple test result obviously wrong loss accuracy irregular environment fix problem,issue,negative,negative,neutral,neutral,negative,negative
955628191,"> > First of, I'd like to thank the authors for their interesting work! I'm not sure how to make a pull request to this repository, so I'll leave my thoughts here regarding TabNet training.
> > **Problem**: TabNet training is slow because the parsing of the CSV is performed on the entire file each time, instead of performed over a single batch each time (see [line](https://github.com/google-research/google-research/blob/432ad195830f3722e2d2bc6dfe5f39991c384806/tabnet/data_helper_covertype.py#L126)). This results in overhead in CPU processing, reducing the utilization of the GPU.
> > **Solution**: In order to improve the training speed, we set the batching operation before the mapping operation so that the mapping operation only operates on a smaller set of data each iteration as follows:
> > ```
> >     dataset = dataset.batch(batch_size, drop_remainder=True)
> >     dataset = dataset.map(parse_csv, num_parallel_calls=n_parallel)
> > 
> >     # Repeat after shuffling, to prevent separate epochs from blending together.
> >     dataset = dataset.repeat(num_epochs)
> >     return dataset
> > ```
> > 
> > 
> >     
> >       
> >     
> > 
> >       
> >     
> > 
> >     
> >   
> > In terms of GPU utilization on an RTX2080Ti, I am getting about 70% utilization compared to 28% previously. More could potentially be done but this is a start.
> > **Caveat**: The unfortunate side-effect of this is that the last small batch would be dropped. Fortunately this has not impacted accuracy on a test set.
> > Padding was tried, but I think I am not getting the shapes right, so I've used this as a solution in the interim.
> > **Testing**: Shown here are the test accuracy and test loss when tested on the CoverType dataset. When tested on the CoverType dataset, the accuracy reached on the test set is 96.23% for the final model.
> > ![image](https://user-images.githubusercontent.com/1995167/79059722-bcb89580-7cc0-11ea-9bb6-3087f2a58fdf.png)
> > ![image](https://user-images.githubusercontent.com/1995167/79059768-2f297580-7cc1-11ea-9027-2acf825d0eb3.png)
> 
> HI! I met the same problem. The TabNet Training is slow coping with [38*130000] data. Did you process the raw data or do sth. special to increase the speed? How you get it? Highly appreciated for any information and suggestions

Hello,as the README.md said,I use `python -m test_experiment_covertype.py` for simple test,but its result is obviously wrong,The traing loss and val accuracy are irregular,just because it's  in the low-resource environment?How can I fix this problem?
![KH HVLC(7N@1KB$A@Y%FBDF](https://user-images.githubusercontent.com/41032255/139565137-ac318333-3c6e-4833-84a3-33320e4357b0.png)",first like thank interesting work sure make pull request repository leave regarding training problem training slow entire file time instead single batch time see line overhead reducing utilization solution order improve training speed set operation operation operation smaller set data iteration repeat shuffling prevent separate blending together return utilization getting utilization previously could potentially done start caveat unfortunate last small batch would fortunately impacted accuracy test set padding tried think getting right used solution interim testing shown test accuracy test loss tested tested accuracy test set final model image image hi met problem training slow coping data process raw data special increase speed get highly information hello said use python simple test result obviously wrong loss accuracy irregular environment fix problem,issue,negative,positive,neutral,neutral,positive,positive
954341726,"> @vishalkevat007 thank you validating the checkpointing! Please pull the latest version: checkpointing is moved into different folder, so there will be no overwriting from model.save_weights.

@rybakov Thank you, its working totally fine now👍",thank please pull latest version different folder thank working totally fine,issue,positive,positive,positive,positive,positive,positive
954038963,"@srewai models weights initialization to resume training in [line](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L133). Initialization for inference in [line](https://github.com/google-research/google-research/blob/master/kws_streaming/train/test.py#L66), can be called in other places too, just look for load_weights",resume training line inference line look,issue,negative,neutral,neutral,neutral,neutral,neutral
953759411,"@rybakov , could you please redirect me to code where you are initializing the weights for the models?",could please redirect code,issue,negative,neutral,neutral,neutral,neutral,neutral
952971508,How can i use the split of this dataset? The id of mini-imagenet-annotations.json dont match of split file,use split id dont match split file,issue,negative,neutral,neutral,neutral,neutral,neutral
952059852,"i change bert config.json vocab_size = 30522 , make success!
but a new problem is i don't have a vocabulary of size 30522. can you provide one？ after, i will share torch bin model.
thank you very much！",change make success new problem vocabulary size provide share torch bin model thank,issue,positive,positive,positive,positive,positive,positive
951377304,"@vishalkevat007 thank you validating the checkpointing!
Please pull the latest version: checkpointing is moved into different folder, so there will be no overwriting from model.save_weights.",thank please pull latest version different folder,issue,positive,positive,positive,positive,positive,positive
950932378,"> Hello, thanks for your interventions above, i have a similar issue regarding undefined symbol. but this time a different one. ![image](https://user-images.githubusercontent.com/47244743/121070884-3e109880-c79d-11eb-938c-82ebf93c0e99.png)

Hi, I'm in step 6 now, any help would be greatly appreciated. Thanks. ",hello thanks similar issue regarding undefined symbol time different one image hi step help would greatly thanks,issue,positive,positive,neutral,neutral,positive,positive
950155383,"> First of, I'd like to thank the authors for their interesting work! I'm not sure how to make a pull request to this repository, so I'll leave my thoughts here regarding TabNet training.
> 
> **Problem**: TabNet training is slow because the parsing of the CSV is performed on the entire file each time, instead of performed over a single batch each time (see [line](https://github.com/google-research/google-research/blob/432ad195830f3722e2d2bc6dfe5f39991c384806/tabnet/data_helper_covertype.py#L126)). This results in overhead in CPU processing, reducing the utilization of the GPU.
> 
> **Solution**: In order to improve the training speed, we set the batching operation before the mapping operation so that the mapping operation only operates on a smaller set of data each iteration as follows:
> 
> ```
>     dataset = dataset.batch(batch_size, drop_remainder=True)
>     dataset = dataset.map(parse_csv, num_parallel_calls=n_parallel)
> 
>     # Repeat after shuffling, to prevent separate epochs from blending together.
>     dataset = dataset.repeat(num_epochs)
>     return dataset
> ```
> 
> In terms of GPU utilization on an RTX2080Ti, I am getting about 70% utilization compared to 28% previously. More could potentially be done but this is a start.
> 
> **Caveat**: The unfortunate side-effect of this is that the last small batch would be dropped. Fortunately this has not impacted accuracy on a test set.
> 
> Padding was tried, but I think I am not getting the shapes right, so I've used this as a solution in the interim.
> 
> **Testing**: Shown here are the test accuracy and test loss when tested on the CoverType dataset. When tested on the CoverType dataset, the accuracy reached on the test set is 96.23% for the final model.
> 
> ![image](https://user-images.githubusercontent.com/1995167/79059722-bcb89580-7cc0-11ea-9bb6-3087f2a58fdf.png)
> 
> ![image](https://user-images.githubusercontent.com/1995167/79059768-2f297580-7cc1-11ea-9027-2acf825d0eb3.png)

HI! I met the same problem. The TabNet Training is slow coping with [38*130000] data. Did you process the raw data or do sth. special to increase the speed? How you get it? Highly appreciated for any information and suggestions
",first like thank interesting work sure make pull request repository leave regarding training problem training slow entire file time instead single batch time see line overhead reducing utilization solution order improve training speed set operation operation operation smaller set data iteration repeat shuffling prevent separate blending together return utilization getting utilization previously could potentially done start caveat unfortunate last small batch would fortunately impacted accuracy test set padding tried think getting right used solution interim testing shown test accuracy test loss tested tested accuracy test set final model image image hi met problem training slow coping data process raw data special increase speed get highly information,issue,positive,positive,neutral,neutral,positive,positive
950136577,Hi @Shambricool / @chenzaiping-dev  How did you go about adding this as a dependency in a separat c++ project?,hi go dependency project,issue,negative,neutral,neutral,neutral,neutral,neutral
950112253,"@rybakov The checkpointer which you have added is working fine, but the file `checkpoint` which is saved by the checkpointer using checkpoint.save [here](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L213) is getting overwritten by the model.save_weights [here](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L200) which also saves `checkpoint` file type of the same format. So the best checkpointer file gets lost and we cannot resume the training properly. I would suggest to change the checkpointer directory specified in this [line](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L129) different from model.save_weights directory. Temporarily, I solved this problem by commenting the [line](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L200) and its working fine now.",added working fine file saved getting also file type format best file lost resume training properly would suggest change directory line different directory temporarily problem line working fine,issue,positive,positive,positive,positive,positive,positive
949424025,"
Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.

----

#### What to do if you already signed the CLA

##### Individual signers

*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).

##### Corporate signers

*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).
*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).
		

ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle-research%2Fgoogle-research%2Fpull%2F852) for more info**.

<!-- need_sender_cla -->",thanks pull request like may first contribution open source project look help look pull request need sign contributor license agreement memo please visit sign fixed please reply verify already individual possible different address commit check data verify set git corporate company point contact authorized participate ask added group authorized know point contact direct project maintainer public version used register authorized contributor must used git commit check data verify set git used register authorized contributor must also attached account information go,issue,positive,positive,neutral,neutral,positive,positive
949285633,"> @Hzy98 Check this [this](https://github.com/VladimirYugay/diw), it's a refactored version of this paper + fixes

Thank you. I'll try this.",check version paper thank try,issue,negative,neutral,neutral,neutral,neutral,neutral
949188610,"@vishalkevat007 The only option I can suggest is to use different optimizer, for example --optimizer 'momentum', shown below (it is a hack, but this way you should be able to load model weights and resume training):
```
$CMD_TRAIN \
...
--optimizer 'momentum' \
crnn \
--cnn_filters '16,16' \
...
```

I already commented about it in the past: you should use a checkpointer, model weights are not enough to resume model training: the error message you shared complains about adam optimizer and its states which were not saved and can not be restored. In the latest version I added an example with model checkpointing, you can enable it with flag restore_checkpoint, example shown below:
```
...
--frequency_mask_max_size 5 \
--restore_checkpoint 1 \
crnn \
--cnn_filters '16,16' \
...
```
If you plan to train your model at scale on large data, then it is better to design own data loader and training script with scaling support. We could make further improvements in training script, but in the future.
",option suggest use different example shown hack way able load model resume training already past use model enough resume model training error message saved latest version added example model enable flag example shown plan train model scale large data better design data loader training script scaling support could make training script future,issue,positive,positive,positive,positive,positive,positive
948834419,"> @vishalkevat007 If you are using the same tf version for training and loading then it looks like a bug in h5 weights saver/loader, please feel free to file it for keras team. kws_streaming uses default version (not h5) of save_weights and [load_weights](https://github.com/google-research/google-research/blob/master/kws_streaming/train/test.py#L67) and it works ok, I would suggest to try it in your use case.

If I use the default tensorflow saving and loading weights format its still showing error.
`InvalidArgumentError: Cannot colocate nodes node training/Adam/stream/conv2d/kernel/m (defined at C:\Users\Vishal\anaconda3\envs\tf-nightly\lib\site-packages\keras\optimizer_v2\optimizer_v2.py:916) placed on device No device assignments were active during op 'training/Adam/stream/conv2d/kernel/m' creation.  and node stream/conv2d/kernel/Initializer/random_uniform/shape (defined at C:\Users\Vishal\anaconda3\envs\tf-nightly\lib\site-packages\keras\backend.py:1831) placed on device Device assignments active during op 'stream/conv2d/kernel/Initializer/random_uniform/shape' creation:
  with tf.device(None): <C:\Users\Vishal\anaconda3\envs\tf-nightly\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py:1774> : Cannot merge devices with incompatible types: '/job:localhost/replica:0/task:0/device:GPU:0' and '/device:CPU:0'
	 [[node training/Adam/stream/conv2d/kernel/m
 (defined at C:\Users\Vishal\anaconda3\envs\tf-nightly\lib\site-packages\keras\optimizer_v2\optimizer_v2.py:916)
]]`

Firstly, I was trying the load weights using the default one, as this error occurred, I switched to .h5 format. But some models like crnn cannot load weights in .h5 format. ",version training loading like bug please feel free file team default version work would suggest try use case use default saving loading format still showing error node defined device device active creation node defined device device active creation none merge incompatible node defined firstly trying load default one error switched format like load format,issue,positive,positive,neutral,neutral,positive,positive
948644646,"> @cognitiveRobot If your question is to restore and train the checkpoint that the author provided. Then, you could try to add a file named ""checkpoint"" in your checkpoint folder (the folder contains .index, .meta .data-xxxx). The content in ""checkpoint"" file can be the following: model_checkpoint_path:""path_to_kitti_learned_intrinsics/model-248900""

I have downloaded the checkpoints provided by the author, extracted them and put them in the folder, and also did as you said，added a checkpoint. In the run.sh, I wrote as follows:
""--imagenet_ckpt=/root/depth_from_video_in_the_wild/MY_IMAGENET_CHECKPOINT/model-248900""
But it's still wrong：
Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
2 root error(s) found.
(0) Not found: Key conv1/bn/beta not found in checkpoint
[[node save_1/RestoreV2 (defined at /miniconda3/envs/huawei/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
[[save_1/RestoreV2/_1067]]
(1) Not found: Key conv1/bn/beta not found in checkpoint
[[node save_1/RestoreV2 (defined at /miniconda3/envs/huawei/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Is my path wrong？Can you help me，thank you.",question restore train author provided could try add file folder folder content file following provided author extracted put folder also wrote still likely due variable name graph key missing please ensure graph based original error root error found found key found node defined found key found node defined path help,issue,positive,positive,neutral,neutral,positive,positive
948375801,"> Maybe your input image is too large. Try to shrink the image width to 1400px keeping aspect ratio.
> 
> The memory usage of HITNET model depends on the input image size. I don't know why it is so.
> 
> The width of the following `half-resolution` is 1400px.
> 
> > [Three models are provided, with maximum disparity set to 160, 288, 400. The models are trained and designed to run on half-resolution input images.](https://github.com/google-research/google-research/tree/master/hitnet#middlebury)

Okay, so I tried to check resolution below : 
```
In [1]: import cv2

In [2]: img_l = cv2.imread('/home/ubuntu/Flowers-perfect/im0.png')

In [3]: img_l.shape[0]
Out[3]: 1980

In [4]: img_l.shape[1]
Out[4]: 2880
```
and I have created the following shell script that crops and then inputs image-pair into HITNET like this.
```
MODEL_PATH=""/tmp/hitnet/models""

# max_disparity = 400
MODEL_NAME=""middlebury_d400.pb""

DATA_PATTERN=""/workspace/hitnet/Flowers-perfect/""

LEFT_PATTERN=""/workspace/hitnet/Flowers-perfect/im0.png""
RIGHT_PATTERN=""/workspace/hitnet/Flowers-perfect/im1.png""
GT_LEFT_PATTERN=""/workspace/hitnet/Flowers-perfect/disp0.pfm""

#!/bin/bash
set -e
set -x


pip3 install -r requirements.txt

mkdir -p $MODEL_PATH
wget -P $MODEL_PATH -N https://storage.googleapis.com/tensorflow-graphics/models/hitnet/default_models/$MODEL_NAME

python3 predict_test.py \
  --data_pattern=$DATA_PATTERN \
  --model_path=$MODEL_PATH/$MODEL_NAME \
  --png_disparity_factor=128 \
  --iml_pattern=$LEFT_PATTERN \
  --imr_pattern=$RIGHT_PATTERN \
  --gtl_pattern=$GT_LEFT_PATTERN \
  --input_channels=3 \
  --predict_right=false \
  --save_png=true \
  --save_pfm=true \
  --evaluate=false \
  --max_test_number=10000 \
  --crop_left 990 \
  --crop_bottom 1440
```

But I got same errors...

",maybe input image large try shrink image width keeping aspect ratio memory usage model input image size know width following three provided maximum disparity set trained designed run input tried check resolution import following shell script like set set pip install python got,issue,negative,positive,neutral,neutral,positive,positive
947100573,I used  convert_kitti_to_tf_records.py  to generate the TF Records for KITTI and tried both the KITTI dataset as well as the TF records generated save files but still I got the same issue as yours. Please let me know how you solved the problem.,used generate tried well save still got issue please let know problem,issue,positive,neutral,neutral,neutral,neutral,neutral
947085189,I am facing the same issue but for KITTI. Have you found any solution?,facing issue found solution,issue,negative,neutral,neutral,neutral,neutral,neutral
946956561,"I agree that quantize-aware training should be a better option. You can do quantization aware training with current version too, an example is shown in [cnn](https://github.com/google-research/google-research/blob/master/kws_streaming/models/cnn.py).",agree training better option quantization aware training current version example shown,issue,positive,positive,positive,positive,positive,positive
946467249,"@rybakov Thanks a lot. This problem is solved. 
BTW I found the tflite model with  post-quantization can't match the h5 model output  if only one feature is used for calibration.  Quantize-aware training is better to get a more accuracy quantized model.

 **tflite model with  post-quantization vs h5 model** 
Not equal to tolerance rtol=1e-06, atol=0.001
Mismatched value: a is different from b.
not close where = (array([0, 0]), array([0, 1]))
not close lhs = [0.39453125 0.60546875]
not close rhs = [0.21052642 0.78947353]
not close dif = [0.18400483 0.18400478]
not close tol = [0.00100021 0.00100079]
dtype = float32, shape = (1, 2)
Mismatched elements: 2 / 2 (100%)
Max absolute difference: 0.18400483
Max relative difference: 0.8740225
x: array([[0.394531, 0.605469]], dtype=float32)
y: array([[0.210526, 0.789474]], dtype=float32)",thanks lot problem found model ca match model output one feature used calibration training better get accuracy model model model equal tolerance value different close array array close close close close tol float shape absolute difference relative difference array array,issue,positive,positive,positive,positive,positive,positive
946462301,"Maybe your input image is too large.
Try to shrink the image width to 1400px keeping aspect ratio.

The memory usage of HITNET model depends on the input image size.
I don't know why it is so.

The width of the following `half-resolution` is 1400px.
> [Three models are provided, with maximum disparity set to 160, 288, 400. The models are trained and designed to run on half-resolution input images.](https://github.com/google-research/google-research/tree/master/hitnet#middlebury)",maybe input image large try shrink image width keeping aspect ratio memory usage model input image size know width following three provided maximum disparity set trained designed run input,issue,negative,positive,positive,positive,positive,positive
946108417,"I would also suggest to train a model on small/toy data sets first, validate that it all works on small data and then scale it to larger one.",would also suggest train model data first validate work small data scale one,issue,negative,neutral,neutral,neutral,neutral,neutral
946094494,"@vishalkevat007 If you are using the same tf version for training and loading then it looks like a bug in h5 weights saver/loader, please feel free to file it for keras team. kws_streaming uses default version (not h5) of save_weights and [load_weights](https://github.com/google-research/google-research/blob/master/kws_streaming/train/test.py#L67) and it works ok, I would suggest to try it in your use case.",version training loading like bug please feel free file team default version work would suggest try use case,issue,positive,positive,positive,positive,positive,positive
946078397,"@uloveqian2021 the reason why streaming accuracy is low: by default ds_tc_resnet uses 'same' paddings - this kind of model is not streamable with the current model config (it is possible to stream it, but model topology has to be modified with [delay layer](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/delay_test.py#L55)). 

So the simplest solution is to re-train the model with 'causal' padings(conversion to streaming works out of the box), with command:
```
$CMD_TRAIN
--data_url ''
--data_dir $DATA_PATH/
--train_dir $MODELS_PATH/ds_tc_resnet/
--mel_upper_edge_hertz 7600
--how_many_training_steps 40000,40000,20000,20000
--learning_rate 0.001,0.0005,0.0002,0.0001
--window_size_ms 30.0
--window_stride_ms 10.0
--mel_num_bins 80
--dct_num_features 40
--resample 0.15
--alsologtostderr
--train 1
--use_spec_augment 1
--time_masks_number 2
--time_mask_max_size 25
--frequency_masks_number 2
--frequency_mask_max_size 7
--pick_deterministically 1
ds_tc_resnet
--activation 'relu'
--dropout 0.0
--ds_filters '128, 64, 64, 64, 128, 128'
--ds_repeat '1, 1, 1, 1, 1, 1'
--ds_residual '0, 1, 1, 1, 0, 0'
--ds_kernel_size '11, 13, 15, 17, 29, 1'
--ds_stride '1, 1, 1, 1, 1, 1'
--ds_dilation '1, 1, 1, 1, 2, 1'
--ds_padding ""'causal', 'causal', 'causal', 'causal', 'causal', 'causal'"" 
```

",reason streaming accuracy low default kind model current model possible stream model topology delay layer solution model conversion streaming work box command resample train activation dropout,issue,negative,positive,positive,positive,positive,positive
945923915,"AH, That's the bazel binary I used locally, you can follow the instructions here:
https://docs.bazel.build/versions/main/install-os-x.html
To install bazel",ah binary used locally follow install,issue,negative,neutral,neutral,neutral,neutral,neutral
945352784,"@rybakov 
Hello. I try to reproduce the experimental result about ds_tc_resnet, but I got bad result in stream model:
tflite Final test accuracy, non stream model = 97.91% (N=4890)
tflite Final test accuracy, stream model state external = 50.43% (N=4890)

$CMD_TRAIN \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/ds_tc_resnet/ \
--mel_upper_edge_hertz 7600 \
--how_many_training_steps 40000,40000,20000,20000 \
--learning_rate 0.001,0.0005,0.0002,0.0001 \
--window_size_ms 30.0 \
--window_stride_ms 10.0 \
--mel_num_bins 80 \
--dct_num_features 40 \
--resample 0.15 \
--alsologtostderr \
--train 1 \
--use_spec_augment 1 \
--time_masks_number 2 \
--time_mask_max_size 25 \
--frequency_masks_number 2 \
--frequency_mask_max_size 7 \
--pick_deterministically 1 \
ds_tc_resnet \
--activation 'relu' \
--dropout 0.0 \
--ds_filters '128, 64, 64, 64, 128, 128' \
--ds_repeat '1, 1, 1, 1, 1, 1' \
--ds_residual '0, 1, 1, 1, 0, 0' \
--ds_kernel_size '11, 13, 15, 17, 29, 1' \
--ds_stride '1, 1, 1, 1, 1, 1' \
--ds_dilation '1, 1, 1, 1, 2, 1'",hello try reproduce experimental result got bad result stream model final test accuracy non stream model final test accuracy stream model state external resample train activation dropout,issue,negative,negative,negative,negative,negative,negative
945274694,"@wenjingyang you are using the same format of representative data as in [cnn_test](https://github.com/google-research/google-research/blob/master/kws_streaming/models/cnn_test.py), but your ds_tc_resnet model has different number of input parameters, so you need to create calibration data for all inputs of your model. 
I am working with TFLite team on enabling quantization of the models with internal states, but it can take some time.",format representative data model different number input need create calibration data model working team quantization internal take time,issue,negative,neutral,neutral,neutral,neutral,neutral
944999652,"On running step 4.
I get the following error: `no such file or directory: ./bazel`
Any solution? Thanks.
",running step get following error file directory solution thanks,issue,negative,positive,neutral,neutral,positive,positive
944470278,"In case of ""--split 1"", it just multiplies input wav in [line](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py#L447) by zero set [here](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py#L659)",case split input line zero set,issue,negative,neutral,neutral,neutral,neutral,neutral
944301790,Thank you so much! But how does this work in terms of the dataset for --split 1?,thank much work split,issue,negative,positive,positive,positive,positive,positive
943561603,"Yes, you will need to create ""silence"" folder with samples containing low level noise and run it as --split 0 with --wanted words as ""hey_babuni"", ""other"", ""silence"".",yes need create silence folder low level noise run split silence,issue,negative,neutral,neutral,neutral,neutral,neutral
943396617,"In case of `--split 1` , you are using additional labels : `SILENCE_LABEL = '_silence_'` and `UNKNOWN_WORD_LABEL = '_unknown_'` But here how does the model gets `SILENCE_LABEL`  dataset? 

In my case now I have wanted words `""hey_babuni"", ""other"" ` but I think I also need to add `SILENCE_LABEL`. Do I need to create another folder with silence audio files and run as  `--split 0`  and `--wanted words` as `""hey_babuni"", ""other"", ""silence""`?

Or is there any other alternative for custom data split for silence label?",case split additional model case think also need add need create another folder silence audio run split silence alternative custom data split silence label,issue,negative,neutral,neutral,neutral,neutral,neutral
941222546,For example you can collect many hours of speech which does not have any key command. Then compute the distributions of output scores and set a threshold to satisfy your conditions for example probability of false alarm.,example collect many speech key command compute output set threshold satisfy example probability false alarm,issue,negative,positive,neutral,neutral,positive,positive
940908231,"@rybakov , sorry I wasn't well so couldn't respond. 
Thank you for answering my questions. Could you please elaborate a bit on 'thresholding neural net output score'? What do you think a real-time keyword spotter threshold should be? Is this something we have to experiment and define the threshold limit?

 ",sorry well could respond thank could please elaborate bit neural net output score think spotter threshold something experiment define threshold limit,issue,positive,neutral,neutral,neutral,neutral,neutral
940554078,"> @uloveqian2021 kws_streaming was validated with tf_nightly-2.3.0.dev20200515 version, but kws_streaming is always at sync with TF HEAD and works with the latest tf_nightly version. So please use the latest version of tf_nightly (pip install tf_nightly)

Thanks! I have solved it. ",dev version always sync head work latest version please use latest version pip install thanks,issue,positive,positive,positive,positive,positive,positive
940480883,"@uloveqian2021 kws_streaming was validated with tf_nightly-2.3.0.dev20200515 version, but kws_streaming is always at sync with TF HEAD and works with the latest tf_nightly version. So please use the latest version of tf_nightly (pip install tf_nightly)",dev version always sync head work latest version please use latest version pip install,issue,negative,positive,positive,positive,positive,positive
937849176,"Actually I don't have much experience in TensorFlow. 
I realized that for checkpoint_path the input should just be ""model.ckpt-02013963"". ",actually much experience input,issue,negative,positive,positive,positive,positive,positive
937590400,"Hi, I also encountered a similar problem, have you solved it?",hi also similar problem,issue,negative,neutral,neutral,neutral,neutral,neutral
933482706,"Hi, I'm now in step 5  preparing the 3d sparse convolution ops.

I will appreciate your help

Thanks.

On Tue, Sep 21, 2021, 8:05 AM Holadele Shegun ***@***.***>
wrote:

> Hi, I hope you are doing well. Still finding it hard to work around the
> problem, you might be busy, Understandably, it would be great to use a paid
> hour of your time, I can see our areas of interest in CV, we could use some
> project together, and beyond that, starting a career journey together. Let
> me know when you are available.
>
> Thanks.
> Segun
> 2348139345173
>
> On Fri, Sep 17, 2021, 10:00 PM Holadele Shegun ***@***.***>
> wrote:
>
>> Hey Hamza. I haven't received your response yet. I hope you are doing well
>>
>> On Wed, Sep 15, 2021, 8:04 AM Holadele Shegun ***@***.***>
>> wrote:
>>
>>> Hi,
>>>
>>> Its days now, haven't received any response from your side, I hope you
>>> are doing well. Pls, let me know when you are available. I'm still trying
>>> to work around the project, no headway yet. Pls get back to me as soon as
>>> you can.
>>>
>>> Best regards
>>>
>>> On Sat, Sep 11, 2021, 8:22 AM Holadele Shegun ***@***.***>
>>> wrote:
>>>
>>>> Hi, I have kept my eyes on my mail particularly on this hoping to get
>>>> your response, lol, guess you have been busy, understandably, I will
>>>> appreciate if you can let me know when you are available.
>>>> Thanks
>>>>
>>>> On Fri, Sep 10, 2021, 8:49 AM Holadele Shegun ***@***.***>
>>>> wrote:
>>>>
>>>>> Thank you for your reply, I take it very seriously.
>>>>>
>>>>> I guess I was able to prepare the setup successfully, but moving
>>>>> forward to a 3D sparse convo has been so challenging.
>>>>>
>>>>> I guess the real problem started from step 3 Download the Tensorflow
>>>>> custom op repo to local folder tf_custom_op
>>>>>
>>>>> I couldn't get the step described in that step 3 so what I did was to
>>>>> use the command in step 4, the command cloned the custom_op folder into my
>>>>> VM and I copy the necessary files out to the required folder.
>>>>>
>>>>> I will appreciate it if you respond as soon as you can.
>>>>>
>>>>> Thanks and best regards
>>>>>
>>>>> On Thu, Sep 9, 2021, 9:08 PM Hamza Alaoui ***@***.***>
>>>>> wrote:
>>>>>
>>>>>> Sure, but show me at what step of the process exactly do you have
>>>>>> difficulties ?
>>>>>>
>>>>>> —
>>>>>> You are receiving this because you were mentioned.
>>>>>> Reply to this email directly, view it on GitHub
>>>>>> <https://github.com/google-research/google-research/issues/614#issuecomment-916553698>,
>>>>>> or unsubscribe
>>>>>> <https://github.com/notifications/unsubscribe-auth/ARIHG5I66X4BDVQTIUVVBYTUBFLBZANCNFSM4Y3DZIWA>
>>>>>> .
>>>>>> Triage notifications on the go with GitHub Mobile for iOS
>>>>>> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
>>>>>> or Android
>>>>>> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>>>>>>
>>>>>>
>>>>>
",hi step sparse convolution appreciate help thanks tue wrote hi hope well still finding hard work around problem might busy understandably would great use hour time see interest could use project together beyond starting career journey together let know available thanks wrote hey hamza received response yet hope well wed wrote hi day received response side hope well let know available still trying work around project headway yet get back soon best sat wrote hi kept mail particularly get response guess busy understandably appreciate let know available thanks wrote thank reply take seriously guess able prepare setup successfully moving forward sparse guess real problem step custom local folder could get step step use command step command folder copy necessary folder appreciate respond soon thanks best hamza wrote sure show step process exactly reply directly view triage go mobile android,issue,positive,positive,positive,positive,positive,positive
933453540,"> I was able to solve this one on my own, for anyone having the same issue, take a look at : echo ""_ZN10tensorflow7strings6StrCatB5cxx11ERKNS0_8AlphaNumES3_S3_"" | c++filt then nm -D /usr/local/lib/python3.6/dist-packages/tensorflow/libtensorflow_framework.so.2 |c++filt | grep ""tensorflow::strings::StrCat"" You'll see the different tags, from there you can either rebuild your tensorflow, or if using bazel add --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" (i'm using tensorflow 2.3.2 in a docker container under ppc64le) ![Debug_n+3](https://user-images.githubusercontent.com/47244743/121227825-03206a80-c85a-11eb-8f7e-3ca46c3f6fad.PNG) ![SOLUTION XD](https://user-images.githubusercontent.com/47244743/121227827-03b90100-c85a-11eb-9af2-3cab064c1705.PNG)

Hello, I still haven't heard from you, I had sent messages to you, guess you have been busy. I managed to go through the other steps, im not stuck at step 5. I pulled the docker image, when I try to map it with the tf3d folder, I got the attached error
![Uploading 20211004_134822.jpg…]()
",able solve one anyone issue take look echo see different either rebuild add docker container solution hello still sent guess busy go stuck step docker image try map folder got attached error,issue,negative,positive,positive,positive,positive,positive
933013301,Hi! I saw a discussion here: https://github.com/google-research/google-research/issues/611#issuecomment-930022033 for this issue.,hi saw discussion issue,issue,negative,neutral,neutral,neutral,neutral,neutral
931978160,"First step of setup instructions in README file says to ""Download the pre-trained BERT model from here and unzip them inside the bert directory"". What there isn't said is that flag definitions has to be manually set to reflect path and file names of the downloaded files.

I faced the same issue with `bert_classifier.py` (PWD in `goemotions` folder) and here are the changes which made it working: 
```diff
-from goemotions.bert import modeling
-from goemotions.bert import optimization
-from goemotions.bert import tokenization
+from bert import modeling
+from bert import optimization
+from bert import tokenization
...
-flags.DEFINE_string(""emotion_file"", ""goemotions/data/emotions.txt"",
+flags.DEFINE_string(""emotion_file"", ""data/emotions.txt"",
...
-    ""data_dir"", ""goemotions/data"",
+    ""data_dir"", ""data"",
...
-    ""bert_config_file"", None,
+    ""bert_config_file"", 'bert/bert_config.json',
...
-flags.DEFINE_string(""vocab_file"", None,
+flags.DEFINE_string(""vocab_file"", 'bert/vocab.txt',
...
-    ""output_dir"", None,
+    ""output_dir"", 'bert',
...
-flags.DEFINE_string(""sentiment_file"", ""goemotions/data/sentiment_dict.json"",
+flags.DEFINE_string(""sentiment_file"", ""data/sentiment_dict.json"",
```
Patch file: [bert_classifier.patch.txt](https://github.com/google-research/google-research/files/7265037/bert_classifier.patch.txt) ",first step setup file model inside directory said flag manually set reflect path file faced issue folder made working import modeling import optimization import import modeling import optimization import data none none none patch file,issue,positive,positive,positive,positive,positive,positive
930699980,"I hardcoded the battch-size for the spatial upsampler here to be 5. Could you change it and see if it works? (https://github.com/google-research/google-research/blob/master/coltran/custom_colorize.py#L168)

You could also set the `--batch_size` flag instead,",spatial could change see work could also set flag instead,issue,negative,neutral,neutral,neutral,neutral,neutral
930058515,"@yuan0101213 , thanks again for a thorough reply. I will apply your suggestions and update you here. Thanks",yuan thanks thorough reply apply update thanks,issue,positive,positive,positive,positive,positive,positive
930022033,"> @yuan0101213 , thank you for quick response. From blog this is the code for DTW:
> 
> ```
> from numpy import array, zeros, argmin, inf, equal, ndim
> # from scipy.spatial.distance import cdist
> from sklearn.metrics.pairwise import manhattan_distances
> #在这里我用到的是曼哈顿距离(求绝对值距离)
> #如果比较的是二维数组，则用欧几里得距离
> 
> s1 = [1, 2, 3, 4, 5, 5, 5, 4]
> s2 = [3, 4, 5, 5, 5, 4]
> 
> r, c = len(s1), len(s2)
> D0 = zeros((r+1,c+1))
> D0[0,1:] = inf
> D0[1:,0] = inf
> D1 = D0[1:,1:]
> #浅复制
> # print D1
> 
> for i in range(r):
>     for j in range(c):
>         D1[i,j] = manhattan_distances(s1[i],s2[j])
> #生成原始距离矩阵
> 
> M = D1.copy()
> for i in range(r):
>     for j in range(c):
>         D1[i,j] += min(D0[i,j],D0[i,j+1],D0[i+1,j])
> #代码核心，动态计算最短距离
> 
> 
> i,j = array(D0.shape) - 2
> #最短路径
> # print i,j
> p,q = [i],[j]
> while(i>0 or j>0):
>     tb = argmin((D0[i,j],D0[i,j+1],D0[i+1,j]))
>     if tb==0 :
>         i-=1
>         j-=1
>     elif tb==1 :
>         i-=1
>     else:
>         j-=1
>     p.insert(0,i)
>     q.insert(0,j)
> 
> print M
> #原始距离矩阵
> print zip(p,q)
> #匹配路径过程
> print D1
> #Cost Matrix或者叫累积距离矩阵
> print D1[-1,-1]
> #序列距离
> ```
> 
> What ive understood from your reply is that I should replace S1 and S2 with embeddings(320 dimension output by Pr-Vipe) and replace distance function in the above code with distance function in distance_utils.py. But there is no distance function in the above code and there are multiple distance functions in distance_utlis.py? Can you please elaborate alittle bit more... Thanks and much appreciated

According to the paper, I use compute_gaussian_likelihoods. The author also gave me some suggestions. Finally, I used acompute_gaussian_likelihoods.The following are the author's suggestions：
Hi Ducheng,

The `compute_gaussian_likelihoods` function is not what we used in the paper. I recommend referring to Eq 5 in our paper for how the matching probability p(m|z_i, z_j) between two embeddings z_i and z_j are computed. You can refer to this function for how the matching probability computed:
https://github.com/google-research/google-research/blob/678de1febc782cd76e75e9591fa3b5b9687b083c/poem/core/loss_utils.py#L70

Once you have the matching probability, the distance is simply -log p(m|z_i, z_j).

You can also directly use the L2 distance between the embedding means of the two predicted Gaussians (""unnormalized_embeddings.csv""), which should also work is much simpler.

Thanks,
Ting",yuan thank quick response code import array equal import import print range range range range min array print else print print zip print cost print understood reply replace dimension output replace distance function code distance function distance function code multiple distance please elaborate bit thanks much according paper use author also gave finally used following author hi function used paper recommend paper matching probability two refer function matching probability matching probability distance simply also directly use distance two also work much simpler thanks ting,issue,positive,positive,positive,positive,positive,positive
929981843,"@yuan0101213 , thank you for quick response.
From blog this is the code for DTW:



```
from numpy import array, zeros, argmin, inf, equal, ndim
# from scipy.spatial.distance import cdist
from sklearn.metrics.pairwise import manhattan_distances
#在这里我用到的是曼哈顿距离(求绝对值距离)
#如果比较的是二维数组，则用欧几里得距离

s1 = [1, 2, 3, 4, 5, 5, 5, 4]
s2 = [3, 4, 5, 5, 5, 4]

r, c = len(s1), len(s2)
D0 = zeros((r+1,c+1))
D0[0,1:] = inf
D0[1:,0] = inf
D1 = D0[1:,1:]
#浅复制
# print D1

for i in range(r):
    for j in range(c):
        D1[i,j] = manhattan_distances(s1[i],s2[j])
#生成原始距离矩阵

M = D1.copy()
for i in range(r):
    for j in range(c):
        D1[i,j] += min(D0[i,j],D0[i,j+1],D0[i+1,j])
#代码核心，动态计算最短距离


i,j = array(D0.shape) - 2
#最短路径
# print i,j
p,q = [i],[j]
while(i>0 or j>0):
    tb = argmin((D0[i,j],D0[i,j+1],D0[i+1,j]))
    if tb==0 :
        i-=1
        j-=1
    elif tb==1 :
        i-=1
    else:
        j-=1
    p.insert(0,i)
    q.insert(0,j)

print M
#原始距离矩阵
print zip(p,q)
#匹配路径过程
print D1
#Cost Matrix或者叫累积距离矩阵
print D1[-1,-1]
#序列距离

```
What ive understood from your reply is that I should replace S1 and S2 with embeddings(320 dimension output by Pr-Vipe)
and replace distance function in the above code with distance function in distance_utils.py. But there is no distance function in the above code and there are multiple distance functions in distance_utlis.py? Can you please elaborate alittle bit more...
Thanks and much appreciated ",yuan thank quick response code import array equal import import print range range range range min array print else print print zip print cost print understood reply replace dimension output replace distance function code distance function distance function code multiple distance please elaborate bit thanks much,issue,positive,positive,positive,positive,positive,positive
929967354,"> @yuan0101213 , I am trying to do video alignment using DTW, it seems that you have implemented this already. Is it possible for you to share your implementation or any resources from where I can quickly figure out this algorithm? thanks...

You can go to Google to search the implementation of DTW algorithm, and then modify two places: 1. replace the data with the embedding of two videos; 2. replace the distance function with the distance function in the file(https://github.com/google-research/google-research/blob/master/poem/core/distance_utils.py).In this way, two video aligned frames and their similarity (distance) can be obtained.

Finally, the DTW blog I refer to is https://www.jianshu.com/p/05bee48cc6a2. Chinese VPN may be required.",yuan trying video alignment already possible share implementation quickly figure algorithm thanks go search implementation algorithm modify two replace data two replace distance function distance function file way two video similarity distance finally refer may,issue,positive,positive,positive,positive,positive,positive
929928357,"@yuan0101213 , I am trying to do video alignment using DTW, it seems that you have implemented this already. Is it possible for you to share your implementation or any resources from where I can quickly figure out this algorithm? thanks...",yuan trying video alignment already possible share implementation quickly figure algorithm thanks,issue,positive,positive,positive,positive,positive,positive
929842404,"@rybakov  Thanks. I'd like to use post-quantization now. 
I generated ds_tc_resnet with your example (--preprocess micro). 
My ds_tc_resnet folder:  [ds_tc_resnet](https://github.com/google-research/google-research/files/7248950/ds_tc_resnet.zip)
representative_dataset: [feature.npz](https://github.com/google-research/google-research/files/7242587/feature.zip)

I changed the [train/test.py](https://github.com/google-research/google-research/blob/master/kws_streaming/train/test.py) with adding ""representative_dataset"".  Updated the fully quantized parameters in [models/utils.py](https://github.com/google-research/google-research/blob/master/kws_streaming/models/utils.py).   I uploaded my test.py and untils.py as [test.txt](https://github.com/google-research/google-research/files/7248973/test.txt) and [utils.txt](https://github.com/google-research/google-research/files/7249031/utils.txt)

The issue is: stream_state_external.tflite in quantize_opt_for_size_tflite_stream_state_external isn't be quantized at all. 

You can reproduce this issue when you run the script below. I have highlighted the difference with your script. 

$CMD_TRAIN \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/ds_tc_resnet/ \
--mel_upper_edge_hertz 7600 \
--how_many_training_steps 4,4,2,2 \
--learning_rate 0.001,0.0005,0.0002,0.0001 \
--window_size_ms 30.0 \
--window_stride_ms 10.0 \
--mel_num_bins 80 \
--dct_num_features 40 \
--resample 0.15 \
--alsologtostderr \
**--train 0** \
--use_spec_augment 1 \
--time_masks_number 2 \
--time_mask_max_size 25 \
--frequency_masks_number 2 \
--frequency_mask_max_size 7 \
--pick_deterministically 1 \
**--preprocess micro** \
ds_tc_resnet 
--activation 'relu' \
--dropout 0.0 \
--ds_filters '128, 64, 64, 64, 128, 128' \
--ds_repeat '1, 1, 1, 1, 1, 1' \
--ds_residual '0, 1, 1, 1, 0, 0' \
--ds_kernel_size '11, 13, 15, 17, 29, 1' \
--ds_stride '1, 1, 1, 1, 1, 1' \
--ds_dilation '1, 1, 1, 1, 2, 1'


The text.py difference.  
![image](https://user-images.githubusercontent.com/4407779/135206401-4ac1101f-eece-417b-bfb7-c29b5caab81d.png)

",thanks like use example micro folder fully issue reproduce issue run script difference script resample train micro activation dropout difference image,issue,positive,positive,positive,positive,positive,positive
929798029,"@wenjingyang quantize.quantize_layer is used for quantization aware training and then it will be used by post-training quantization in tflite.
So, if you are interested in post-training quantization only, then you do not need quantize.quantize_layer with quantization aware training.
Please let me know what error you observe and how to reproduce it.

I would also suggest to try [ds_tc_resnet model](https://github.com/google-research/google-research/blob/master/kws_streaming/models/ds_tc_resnet.py), with training [example](https://github.com/google-research/google-research/blob/624e3196b6fbfa429480027606387ae682b2a207/kws_streaming/experiments/kws_experiments_12_labels.md#ds_tc_resnet---based-on-matchboxnet). It can be faster and more accurate than ds-cnn.

",used quantization aware training used quantization interested quantization need quantization aware training please let know error observe reproduce would also suggest try model training example faster accurate,issue,negative,positive,positive,positive,positive,positive
929779497,"@rybakov Thanks for your help. 
My target is to get a fully quantized model. 
I have tried the [example ](https://github.com/google-research/google-research/blob/master/kws_streaming/models/cnn_test.py) and it works fine. But I can't get any quantization on my [ds-cnn model](https://github.com/google-research/google-research/files/7242640/stream_state_external.zip). 
 I checked the code and found the layer should be quantized  when [cnn model](https://github.com/google-research/google-research/blob/master/kws_streaming/models/cnn.py) is built. So my understanding is that I should use quantized layer when build ds-cnn model. Right?   

",thanks help target get fully model tried example work fine ca get quantization model checked code found layer model built understanding use layer build model right,issue,positive,positive,positive,positive,positive,positive
929509679,"@wenjingyang I do not know what are you going to achieve with your coversion parameters
If you follow [example](https://github.com/google-research/google-research/blob/master/kws_streaming/models/cnn_test.py#L68) and set converter parameters:
```
  converter.inference_type = tf.int8
  converter.experimental_new_quantizer = True
  converter.experimental_enable_resource_variables = True
  converter.representative_dataset = representative_dataset

  # this will enable audio_spectrogram and mfcc in TFLite
  converter.target_spec.supported_ops = [
      tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS
  ]
  converter.allow_custom_ops = True
  converter.inference_input_type = tf.int8
  converter.inference_output_type = tf.int8
  converter.optimizations = [tf.lite.Optimize.DEFAULT]
```
then you will get a fully quantized model with quantized inputs and outputs. All conv ops will use quantized version.

If you set parameters:
```
  converter.inference_type = tf.int8
  converter.experimental_new_quantizer = True
  converter.experimental_enable_resource_variables = True
  converter.representative_dataset = representative_dataset

  # this will enable audio_spectrogram and mfcc in TFLite
  converter.target_spec.supported_ops = [
      tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS
  ]
  converter.allow_custom_ops = True
  converter.optimizations = [tf.lite.Optimize.DEFAULT]
```
then your model will be partially quantized: all inputs and outputs will be float including input/output states. All inputs/outputs will be quantized and de-quantized inside of the model. All conv ops will use quantized version.

Please let me know what way do you plan to quantize your model. 
I would suggest to use the second option, as the easy one (it also works well out of the box).

If you create an issue at google-research without mentioning the owner of the project, then the owner will not be notified and there is no guarantee that the issue will be addressed. I would encourage you to mention the project owner in an issue, for example in case of kws_streaming you can use @rybakov, this way I will get notification and will not miss it.
",know going achieve follow example set converter true true enable true get fully model use version set true true enable true model partially float inside model use version please let know way plan quantize model would suggest use second option easy one also work well box create issue without owner project owner notified guarantee issue would encourage mention project owner issue example case use way get notification miss,issue,positive,positive,positive,positive,positive,positive
928513116,"> @peterjliu Did you mean to add somebody else? I don't have anything to do with that project.

Sorry, I saw you as a contributor here -- perhaps it was some formatting or re-organization that you did",mean add somebody else anything project sorry saw contributor perhaps,issue,negative,negative,negative,negative,negative,negative
927682262,"I've implemented a 3D version of the non max suppression algorithm in C following the tensorflow tutorial to create custom operation. I've compiled, run the basic tests (similar to those in 2D that can be seen found in tensoflow sources) and used it with success within a deep learning algorithm. I'm going to publish this 3D NMS in the tensorflow repositories in the next few days: I'll then add a link here.",version non suppression algorithm following tutorial create custom operation run basic similar seen found used success within deep learning algorithm going publish next day add link,issue,positive,positive,neutral,neutral,positive,positive
927245372,"Hi,  @rybakov 
Thanks again. The problem is solved with tf-night(2.7.0.dev20210824). The version tensorflow2.3 had this issue. ",hi thanks problem dev version issue,issue,negative,positive,positive,positive,positive,positive
927241466,"@rybakov . Thanks for your help. The run_convert.py is below. 


import tensorflow as tf
from kws_streaming.layers import stream
converter = tf.lite.TFLiteConverter.from_saved_model( 'models1/ds_cnn_stream1/stream_state_internal')
converter.experimental_enable_resource_variables = True
converter.target_spec.supported_ops = [
      tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS
  ]
converter.allow_custom_ops = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tfmodel = converter.convert()
open (""model.tflite"" , ""wb"") .write(tfmodel)
",thanks help import import stream converter true true open,issue,positive,positive,positive,positive,positive,positive
927220440,"I would suggest to try several options:
* You use very small number of training steps: FLAGS.how_many_training_steps = '5000,5000', I would suggest to try something similar with [example](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_35_labels.md#based-on-matchboxnet): how_many_training_steps = 20000,20000,20000,20000,20000,20000;  learning_rate = 0.01,0.005,0.002,0.001,0.0005,0.0002
* You disable spectrum augmentation: use_spec_augment = 0, I would enable it.
* Your speech feature framing parameters are large: FLAGS.window_size_ms = 160.0 FLAGS.window_stride_ms = 80.0, I would try smaller numbers if phonemes in your key words have smaller time resolution
* Explore different model [ds_tc_resnet](https://github.com/google-research/google-research/blob/master/kws_streaming/models/ds_tc_resnet.py) model, here is a model training [example](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_35_labels.md#based-on-matchboxnet) this model has only 75K parameters, I would increase it to 300K or more if you need to increase accuracy. This model also streamable and gives SOTA accuracy for small model size, shown in table at [experimental-results](https://github.com/google-research/google-research/tree/master/kws_streaming#experimental-results)",would suggest try several use small number training would suggest try something similar example disable spectrum augmentation would enable speech feature framing large would try smaller key smaller time resolution explore different model model model training example model would increase need increase accuracy model also accuracy small model size shown table,issue,positive,negative,neutral,neutral,negative,negative
926856250,"Here is a [test](https://github.com/google-research/google-research/blob/master/kws_streaming/models/utils_test.py#L136) validating that conversion works with any frontend: it shows that [model with external state](https://github.com/google-research/google-research/blob/master/kws_streaming/models/utils_test.py#L177) can be converted. Frontend type should not matter.

Streaming with internal state is still in dev mode in tflite, but streaming with external state works ok. If you are interested in converting your model to streaming tflite with internal state, then you should use this [example](https://github.com/google-research/google-research/blob/master/kws_streaming/models/ds_tc_resnet_eager_test.py#L29). This feature was added recently, it works only in **eager mode**. You can use above example for conversion to streaming tflite with internal state, but it has to be executed separately from kws_streaming (kws_streaming runs in session mode for backward compatibility with tf v1 and streaming with external state). Streaming with internal state also has some issues with quantization and we are working on it. Please confirm that you use tf-nightly, so that all latest features are supported.

[ds_cnn_stream.zip](https://github.com/google-research/google-research/files/7223822/ds_cnn_stream.zip) is not enought to reproduce your issues, I also need to see how you convert the model in ""run_convert.py"".
",test conversion work model external state converted type matter streaming internal state still dev mode streaming external state work interested converting model streaming internal state use example feature added recently work eager mode use example conversion streaming internal state executed separately session mode backward compatibility streaming external state streaming internal state also quantization working please confirm use latest reproduce also need see convert model,issue,positive,positive,neutral,neutral,positive,positive
926544874,"One more comments. I also tried 'raw' preprocess, the tflite can be generated successfully. ",one also tried successfully,issue,negative,positive,positive,positive,positive,positive
924998049,"Thanks for the notification, I added the fix here: https://github.com/google-research/google-research/commit/eb436440636d7950d75cec6c75e1e738488fa897
(Since it appears you might not be authorized to push commits to the `google_research` repo)",thanks notification added fix since might authorized push,issue,negative,positive,positive,positive,positive,positive
923917928,"Hi, I hope you are doing well. Still finding it hard to work around the
problem, you might be busy, Understandably, it would be great to use a paid
hour of your time, I can see our areas of interest in CV, we could use some
project together, and beyond that, starting a career journey together. Let
me know when you are available.

Thanks.
Segun
2348139345173

On Fri, Sep 17, 2021, 10:00 PM Holadele Shegun ***@***.***>
wrote:

> Hey Hamza. I haven't received your response yet. I hope you are doing well
>
> On Wed, Sep 15, 2021, 8:04 AM Holadele Shegun ***@***.***>
> wrote:
>
>> Hi,
>>
>> Its days now, haven't received any response from your side, I hope you
>> are doing well. Pls, let me know when you are available. I'm still trying
>> to work around the project, no headway yet. Pls get back to me as soon as
>> you can.
>>
>> Best regards
>>
>> On Sat, Sep 11, 2021, 8:22 AM Holadele Shegun ***@***.***>
>> wrote:
>>
>>> Hi, I have kept my eyes on my mail particularly on this hoping to get
>>> your response, lol, guess you have been busy, understandably, I will
>>> appreciate if you can let me know when you are available.
>>> Thanks
>>>
>>> On Fri, Sep 10, 2021, 8:49 AM Holadele Shegun ***@***.***>
>>> wrote:
>>>
>>>> Thank you for your reply, I take it very seriously.
>>>>
>>>> I guess I was able to prepare the setup successfully, but moving
>>>> forward to a 3D sparse convo has been so challenging.
>>>>
>>>> I guess the real problem started from step 3 Download the Tensorflow
>>>> custom op repo to local folder tf_custom_op
>>>>
>>>> I couldn't get the step described in that step 3 so what I did was to
>>>> use the command in step 4, the command cloned the custom_op folder into my
>>>> VM and I copy the necessary files out to the required folder.
>>>>
>>>> I will appreciate it if you respond as soon as you can.
>>>>
>>>> Thanks and best regards
>>>>
>>>> On Thu, Sep 9, 2021, 9:08 PM Hamza Alaoui ***@***.***>
>>>> wrote:
>>>>
>>>>> Sure, but show me at what step of the process exactly do you have
>>>>> difficulties ?
>>>>>
>>>>> —
>>>>> You are receiving this because you were mentioned.
>>>>> Reply to this email directly, view it on GitHub
>>>>> <https://github.com/google-research/google-research/issues/614#issuecomment-916553698>,
>>>>> or unsubscribe
>>>>> <https://github.com/notifications/unsubscribe-auth/ARIHG5I66X4BDVQTIUVVBYTUBFLBZANCNFSM4Y3DZIWA>
>>>>> .
>>>>> Triage notifications on the go with GitHub Mobile for iOS
>>>>> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
>>>>> or Android
>>>>> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>>>>>
>>>>>
>>>>
",hi hope well still finding hard work around problem might busy understandably would great use hour time see interest could use project together beyond starting career journey together let know available thanks wrote hey hamza received response yet hope well wed wrote hi day received response side hope well let know available still trying work around project headway yet get back soon best sat wrote hi kept mail particularly get response guess busy understandably appreciate let know available thanks wrote thank reply take seriously guess able prepare setup successfully moving forward sparse guess real problem step custom local folder could get step step use command step command folder copy necessary folder appreciate respond soon thanks best hamza wrote sure show step process exactly reply directly view triage go mobile android,issue,positive,positive,positive,positive,positive,positive
923910927,"> Thanks for updating.
> I tried [Op compilation instructions](https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md), and now stop at step 6.
> 
> ```
> bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures
> ```
> 
> Here's the logs.
> 
> ```
> 2021-03-18 02:02:29.285314: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
> Running tests under Python 3.6.12: /usr/bin/python3
> [ RUN      ] SparseConvOpTest.test_session
> [  SKIPPED ] SparseConvOpTest.test_session
> [ RUN      ] SparseConvOpTest.test_spar_conv_op
> 2021-03-18 02:02:30.022262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
> 2021-03-18 02:02:30.058706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.059657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
> pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
> coreClock: 1.755GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s
> 2021-03-18 02:02:30.059699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.060507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
> pciBusID: 0000:03:00.0 name: GeForce RTX 3090 computeCapability: 8.6
> coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
> 2021-03-18 02:02:30.060519: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
> 2021-03-18 02:02:30.061462: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
> 2021-03-18 02:02:30.062386: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
> 2021-03-18 02:02:30.062544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
> 2021-03-18 02:02:30.063485: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
> 2021-03-18 02:02:30.064012: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
> 2021-03-18 02:02:30.066111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
> 2021-03-18 02:02:30.066171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.067002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.067805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.068599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.069395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
> 2021-03-18 02:02:30.069622: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> 2021-03-18 02:02:30.074065: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3799900000 Hz
> 2021-03-18 02:02:30.074340: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5719590 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
> 2021-03-18 02:02:30.074347: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
> 2021-03-18 02:02:30.172593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.213582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.214498: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57858d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
> 2021-03-18 02:02:30.214506: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
> 2021-03-18 02:02:30.214508: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 3090, Compute Capability 8.6
> 2021-03-18 02:02:30.215138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.215928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
> pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
> coreClock: 1.755GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s
> 2021-03-18 02:02:30.215960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.216749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
> pciBusID: 0000:03:00.0 name: GeForce RTX 3090 computeCapability: 8.6
> coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
> 2021-03-18 02:02:30.216765: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
> 2021-03-18 02:02:30.216777: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
> 2021-03-18 02:02:30.216783: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
> 2021-03-18 02:02:30.216791: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
> 2021-03-18 02:02:30.216799: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
> 2021-03-18 02:02:30.216806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
> 2021-03-18 02:02:30.216814: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
> 2021-03-18 02:02:30.216844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.217628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.218417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.219196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.219963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
> 2021-03-18 02:02:30.219978: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
> INFO:tensorflow:time(__main__.SparseConvOpTest.test_spar_conv_op): 29.54s
> I0318 02:02:59.557888 140562656798464 test_util.py:1973] time(__main__.SparseConvOpTest.test_spar_conv_op): 29.54s
> [  FAILED  ] SparseConvOpTest.test_spar_conv_op
> ======================================================================
> ERROR: test_spar_conv_op (__main__.SparseConvOpTest)
> SparseConvOpTest.test_spar_conv_op
> ----------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/execroot/__main__/bazel-out/k8-opt/bin/sparse_conv_ops_py_test.runfiles/__main__/tensorflow_sparse_conv_ops/sparse_conv_ops_test.py"", line 32, in test_spar_conv_op
>     dtype=tf.float32)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
>     allow_broadcast=True)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 275, in _constant_impl
>     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 300, in _constant_eager_impl
>     t = convert_to_eager_tensor(value, ctx, dtype)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 97, in convert_to_eager_tensor
>     ctx.ensure_initialized()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py"", line 539, in ensure_initialized
>     context_handle = pywrap_tfe.TFE_NewContext(opts)
> tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid
> 
> ----------------------------------------------------------------------
> Ran 2 tests in 29.538s
> 
> FAILED (errors=1, skipped=1)
> ```
> 
> Please if there's any suggestions to fix the problem, thanks!

Hi, I want to ask if you have been able to train 3D model following the steps discussed in this repo, would you be interested in taking me through the process of training this model, I do not mind paying for the hours. But beyond payment, we could start a career relationship from there. Let me know if that sounds interesting to you. 

Thanks and I hope to hear from you. 
Segun",thanks tried compilation stop step run successfully dynamic library running python run run successfully dynamic library successful node read negative value must least one node node zero found device name successful node read negative value must least one node node zero found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible binary deep neural network library use following enable rebuild appropriate compiler frequency service platform host guarantee used device host default version successful node read negative value must least one node node zero successful node read negative value must least one node node zero service platform guarantee used device compute capability device compute capability successful node read negative value must least one node node zero found device name successful node read negative value must least one node node zero found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible successfully dynamic library time time error recent call last file line file line constant file line return value shape file line value file line file line implicit status device kernel image invalid ran please fix problem thanks hi want ask able train model following would interested taking process training model mind paying beyond payment could start career relationship let know interesting thanks hope hear,issue,positive,positive,neutral,neutral,positive,positive
923125520,"It sounds reasonable to add segments to the “unknown” bucket for reducing false positives. You could also use them as separate labels.
Another standard approach for controlling false positive is thresholding neural net output score (positive rate also will be impacted), in addition you could check that the score is passing a threshold several times before making a final decision.
",reasonable add unknown bucket reducing false could also use separate another standard approach false positive neural net output score positive rate also impacted addition could check score passing threshold several time making final decision,issue,positive,negative,neutral,neutral,negative,negative
922837913,"@rybakov . Thank you so much for the clarification. Here, I want to use the phonetic segmentation of the `--ẁanted_word hey babuni` to be treated as  UNKNOWN_WORD_LABEL so that the model is more robust to avoid false positives. So all the contents of the folder `other` are individual sounds (phonetic segmentation) of `ẁanted word hey babuni` which means these are *not* the list of my wanted words but I want the model to learn them as  UNKNOWN_WORD_LABEL. 

Q.1. Is this a good approach for a robust model to avoid false positives, please suggest?
Q.2  I still want to have 3 output labels, or there is a better approach: 

'hey babuni'
“silence”
“unknown” (all individual sounds, phonetic segmentation).

Thank you for your understanding!




",thank much clarification want use phonetic segmentation hey model robust avoid false content folder individual phonetic segmentation word hey list want model learn good approach robust model avoid false please suggest still want output better approach silence unknown individual phonetic segmentation thank understanding,issue,positive,positive,neutral,neutral,positive,positive
922551243,I saw that there was a discussion here: https://github.com/google-research/google-research/issues/716#issuecomment-919711761 and another user shared a CSV file. ,saw discussion another user file,issue,negative,neutral,neutral,neutral,neutral,neutral
922078710,"Hey Hamza. I haven't received your response yet. I hope you are doing well

On Wed, Sep 15, 2021, 8:04 AM Holadele Shegun ***@***.***>
wrote:

> Hi,
>
> Its days now, haven't received any response from your side, I hope you are
> doing well. Pls, let me know when you are available. I'm still trying to
> work around the project, no headway yet. Pls get back to me as soon as you
> can.
>
> Best regards
>
> On Sat, Sep 11, 2021, 8:22 AM Holadele Shegun ***@***.***>
> wrote:
>
>> Hi, I have kept my eyes on my mail particularly on this hoping to get
>> your response, lol, guess you have been busy, understandably, I will
>> appreciate if you can let me know when you are available.
>> Thanks
>>
>> On Fri, Sep 10, 2021, 8:49 AM Holadele Shegun ***@***.***>
>> wrote:
>>
>>> Thank you for your reply, I take it very seriously.
>>>
>>> I guess I was able to prepare the setup successfully, but moving forward
>>> to a 3D sparse convo has been so challenging.
>>>
>>> I guess the real problem started from step 3 Download the Tensorflow
>>> custom op repo to local folder tf_custom_op
>>>
>>> I couldn't get the step described in that step 3 so what I did was to
>>> use the command in step 4, the command cloned the custom_op folder into my
>>> VM and I copy the necessary files out to the required folder.
>>>
>>> I will appreciate it if you respond as soon as you can.
>>>
>>> Thanks and best regards
>>>
>>> On Thu, Sep 9, 2021, 9:08 PM Hamza Alaoui ***@***.***>
>>> wrote:
>>>
>>>> Sure, but show me at what step of the process exactly do you have
>>>> difficulties ?
>>>>
>>>> —
>>>> You are receiving this because you were mentioned.
>>>> Reply to this email directly, view it on GitHub
>>>> <https://github.com/google-research/google-research/issues/614#issuecomment-916553698>,
>>>> or unsubscribe
>>>> <https://github.com/notifications/unsubscribe-auth/ARIHG5I66X4BDVQTIUVVBYTUBFLBZANCNFSM4Y3DZIWA>
>>>> .
>>>> Triage notifications on the go with GitHub Mobile for iOS
>>>> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
>>>> or Android
>>>> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>>>>
>>>>
>>>
",hey hamza received response yet hope well wed wrote hi day received response side hope well let know available still trying work around project headway yet get back soon best sat wrote hi kept mail particularly get response guess busy understandably appreciate let know available thanks wrote thank reply take seriously guess able prepare setup successfully moving forward sparse guess real problem step custom local folder could get step step use command step command folder copy necessary folder appreciate respond soon thanks best hamza wrote sure show step process exactly reply directly view triage go mobile android,issue,positive,positive,positive,positive,positive,positive
922056834,"Hello @DNGros for such detailed discussion. Actually I am looking for the pytorch_model.bin file of the CuBert Model that I can simply load into my code for fine tuning using AutoModel.from_pretrained( ). Can you explain what files to give as input to this code below  for transformation to PyTorch files? I am new with Bert Models and would appreciate if you can explain what I am missing here and how I can get a pre trained model file of CuBert Model that can be easily loaded.

export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
transformers-cli convert --model_type bert \
  --tf_checkpoint $BERT_BASE_DIR/bert_model.ckpt \
  --config $BERT_BASE_DIR/bert_config.json \
  --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin",hello detailed discussion actually looking file model simply load code fine tuning explain give input code transformation new would appreciate explain missing get trained model file model easily loaded export convert,issue,positive,positive,positive,positive,positive,positive
922006887,"This effect is well described in our [paper](https://arxiv.org/pdf/2005.06720.pdf), please have a look at paragraph ""4.3. Streaming and non-streaming latency with accuracy"". If you train RNN based model with default settings (CRNN is based on RNN) then it resets RNN state at every minibatch and trained in ""stateless"" mode - this model can not be used for streaming, because it expects that RNN state is reseted in the beginning of the input sequence, but in the streaming mode we do not know when is the beginning of the key word in the input audio sequence. That is why for streaming mode, RNN based model, have to be trained in ""statefull"" mode as shown in [crnn_state](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#crnn_state). In this case you should expect that overall accuracy of CRNN model can go down: as shown in ""Table 2: Accuracy on data V2 with latency and model size"" in comparison to non streaming version in ""Table 1: Baseline models accuracy on data V1* and V2* with paper references and our models accuracy on data V1 and V2"" of the paper. For streaming mode model has to learn on its own how to process RNN state.

Non streaming accuracy reported at ""Table 1: Baseline models accuracy on data V1* and V2* with
paper references and our models accuracy on data V1 and V2"" can be reproduced by running [crnn](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#crnn). This model can be used for non streaming inference. You can use it in streaming mode, but you will have to reset manually RNN state in the beginning of the key word, so it is very limited use case because you will need to design a beginning of the key word detector and connect it with CRNN etc - it is not end to end approach.

Streaming accuracy reported at ""Table 2: Accuracy on data V2 with latency and model size"" can be reproduced by [crnn_state](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#crnn_state). This model can be used for both streaming and non streaming inference.
",effect well paper please look paragraph streaming latency accuracy train based model default based state every trained stateless mode model used streaming state beginning input sequence streaming mode know beginning key word input audio sequence streaming mode based model trained mode shown case expect overall accuracy model go shown table accuracy data latency model size comparison non streaming version table accuracy data paper accuracy data paper streaming mode model learn process state non streaming accuracy table accuracy data paper accuracy data running model used non streaming inference use streaming mode reset manually state beginning key word limited use case need design beginning key word detector connect end end approach streaming accuracy table accuracy data latency model size model used streaming non streaming inference,issue,negative,negative,neutral,neutral,negative,negative
921988636,"My previous [answer](https://github.com/google-research/google-research/issues/809#issuecomment-916235510) was for a case when lib will split data for you with option 
```
--split_data 1 \
```
But here you are using custom data split with 
```
--split_data 0 \
```
so you have to control all data on your own: data split and labels. As it is shown [here](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data_utils.py#L96) SILENCE_LABEL and UNKNOWN_WORD_LABEL will not be added to your wanted_words as it was with split_data=1.
In this case (split_data=0) folders ""testing""/""training""/""validation"" should have only wanted. 
For example if you have wanted_words: ""hey_babuni"", ""other"" then testing folders will contain
```
testing  
      < other     ""you can copy here all wav files from other folders 'down', 'eight', 'five' etc...""
               gen-12.wav 
               gen-1.wav 
               xeyz.wav
                ..
         hey_babuni 
```
Sub-folders names (in ""testing"" folder) have to be the same with wanted_words. Number of wanted_words is the number of categories in the classifier.

""training"" and ""validation"" folders should have the same structure as ""testing"" above.
",previous answer case split data option custom data split control data data split shown added case testing training validation example testing contain testing copy testing folder number number classifier training validation structure testing,issue,negative,negative,negative,negative,negative,negative
921560761,"Hi @rybakov, 

I am trying to train `sdvf` model with custom data split with the below directory structure and training parameters.

```
(venv) srewai@las:~/path/final_dataset$ ls -l

_background_noise_
       < doing_the_dishes.wav   
          dude_miaowing.wav  
         ...
testing  
      < down  
               gen-12.wav 
                ..
         eight  
              gen-1.wav 
               ...
         five  
              xyz.wav
              ... 
         four  
             ...
         hey_babuni 
         nine  
         no  
         on  
         one  
         seven  
         six  
         three  
         two  
         up  
         zero
             abc.wav
training  
          < down  
               gen-3.wav 
                ..
         eight  
              gen-2.wav 
               ...
         five  
              xyzr.wav
              ... 
         four  
             ...
         hey_babuni 
         nine  
         no  
         on  
         one  
         seven  
         six  
         three  
         two  
         up  
         zero
             abcx.wav
validation
           < down  
               gen-3.wav 
                ..
         eight  
              gen-12.wav 
               ...
         five  
              xeyz.wav
              ... 
         four  
             ...
         hey_babuni 
         nine  
         no  
         on  
         one  
         seven  
         six  
         three  
         two  
         up  
         zero
              abcd.wav
 ```
```
$CMD_TRAIN \
--data_url '' \
--wanted_words hey_babuni \
--split_data 0 \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/svdf/ \
--mel_upper_edge_hertz 7600 \
--how_many_training_steps 20000,20000,20000,20000 \
--learning_rate 0.001,0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 80 \
--dct_num_features 30 \
--resample 0.15 \
--alsologtostderr \
--time_shift_ms 100 \
--train 1 \
svdf \
--svdf_memory_size 4,10,10,10,10,10 \
--svdf_units1 256,256,256,256,256,256 \
--svdf_act ""'relu','relu','relu','relu','relu','relu'"" \
--svdf_units2 128,128,128,128,128,-1 \
--svdf_dropout 0.0,0.0,0.0,0.0,0.0,0.0 \
--svdf_pad 0 \
--dropout1 0.0 \
--units2 '' \
--act2 ''

```
But I am getting the exception error as:

```
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/lhome/srauniy/test2/kws_streaming/train/model_train_eval.py"", line 367, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/lhome/srauniy/test2/sonu-venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/lhome/srauniy/test2/sonu-venv/lib/python3.6/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/lhome/srauniy/test2/sonu-venv/lib/python3.6/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""/lhome/srauniy/test2/kws_streaming/train/model_train_eval.py"", line 147, in main
    train.train(flags)
  File ""/lhome/srauniy/test2/kws_streaming/train/train.py"", line 51, in train
    audio_processor = input_data.AudioProcessor(flags)
  File ""/lhome/srauniy/test2/kws_streaming/data/input_data.py"", line 124, in __init__
    self.prepare_split_data_index(wanted_words, flags.split_data, file_ext)
  File ""/lhome/srauniy/test2/kws_streaming/data/input_data.py"", line 324, in prepare_split_data_index
    raise Exception('Unknown word ' + word)
Exception: Unknown word six

```

All the folders files (gen-12.wav..)  `one two three..` in the training, testing, and validation are phonetic segmentation of `--wanted_word hey_babuni` with 1-sec length. I am considering these as contrasting examples of the wanted word. Would you please help me with what is missing here and causing the error? Also, do you think creating contrasting examples using phonetic segmentation would help for a robust model by reducing the number of false positives?

Thank you!

",hi trying train model custom data split directory structure training la testing eight five four nine one seven six three two zero training eight five four nine one seven six three two zero validation eight five four nine one seven six three two zero resample train dropout act getting exception error recent call last file line file line code file line module unparsed file line run file line run main file line main file line main file line train file line file line raise exception word word exception unknown word six one two three training testing validation phonetic segmentation length considering word would please help missing causing error also think phonetic segmentation would help robust model reducing number false thank,issue,positive,negative,neutral,neutral,negative,negative
921275131,Did you mean to add somebody else? I don't have anything to do with that project.,mean add somebody else anything project,issue,negative,negative,negative,negative,negative,negative
921242072,@sun51 @peterjliu any chance you could check this PR out?,sun chance could check,issue,negative,neutral,neutral,neutral,neutral,neutral
919959129,"Hi,

Its days now, haven't received any response from your side, I hope you are
doing well. Pls, let me know when you are available. I'm still trying to
work around the project, no headway yet. Pls get back to me as soon as you
can.

Best regards

On Sat, Sep 11, 2021, 8:22 AM Holadele Shegun ***@***.***>
wrote:

> Hi, I have kept my eyes on my mail particularly on this hoping to get your
> response, lol, guess you have been busy, understandably, I will appreciate
> if you can let me know when you are available.
> Thanks
>
> On Fri, Sep 10, 2021, 8:49 AM Holadele Shegun ***@***.***>
> wrote:
>
>> Thank you for your reply, I take it very seriously.
>>
>> I guess I was able to prepare the setup successfully, but moving forward
>> to a 3D sparse convo has been so challenging.
>>
>> I guess the real problem started from step 3 Download the Tensorflow
>> custom op repo to local folder tf_custom_op
>>
>> I couldn't get the step described in that step 3 so what I did was to use
>> the command in step 4, the command cloned the custom_op folder into my VM
>> and I copy the necessary files out to the required folder.
>>
>> I will appreciate it if you respond as soon as you can.
>>
>> Thanks and best regards
>>
>> On Thu, Sep 9, 2021, 9:08 PM Hamza Alaoui ***@***.***>
>> wrote:
>>
>>> Sure, but show me at what step of the process exactly do you have
>>> difficulties ?
>>>
>>> —
>>> You are receiving this because you were mentioned.
>>> Reply to this email directly, view it on GitHub
>>> <https://github.com/google-research/google-research/issues/614#issuecomment-916553698>,
>>> or unsubscribe
>>> <https://github.com/notifications/unsubscribe-auth/ARIHG5I66X4BDVQTIUVVBYTUBFLBZANCNFSM4Y3DZIWA>
>>> .
>>> Triage notifications on the go with GitHub Mobile for iOS
>>> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
>>> or Android
>>> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>>>
>>>
>>
",hi day received response side hope well let know available still trying work around project headway yet get back soon best sat wrote hi kept mail particularly get response guess busy understandably appreciate let know available thanks wrote thank reply take seriously guess able prepare setup successfully moving forward sparse guess real problem step custom local folder could get step step use command step command folder copy necessary folder appreciate respond soon thanks best hamza wrote sure show step process exactly reply directly view triage go mobile android,issue,positive,positive,positive,positive,positive,positive
919843473,"> As this seems like another issue, I open a new one.
> I am currently at step 6 according to https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md
> at line ""bazel run sparse_conv_ops_py_test --experimental_repo_remote_exec --verbose_failures""
> 
> i get this error:
> 
> ```
> WARNING: Download from https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/fcc4b966f1265f466e82617020af93670141b009.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
> DEBUG: /root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:5: 
> Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
> WARNING: /root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/external/local_config_tf/BUILD:5674:1: target 'libtensorflow_framework.so' is both a rule and a file; please choose another name for the rule
> ```
> 
> Is there any regional restriction for this url `https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/fcc4b966f1265f466e82617020af93670141b009.tar.gz` ?

Hi, I am currently working on implementing a 3D sparse for a 3D object detection, I will appreciate if you can give me a hand. Let me know how to connect with you. Thanks. ",like another issue open new one currently step according line run get error warning class get returned found warning environment variable set default warning target rule file please choose another name rule regional restriction hi currently working sparse object detection appreciate give hand let know connect thanks,issue,negative,positive,neutral,neutral,positive,positive
919782992,"@n-yoshida14 
thank you very much, you saved me a lot of time. Much appreciated...",thank much saved lot time much,issue,positive,positive,positive,positive,positive,positive
919776622,"@farazBhatti 
I divided x values and y values of each keypoint by the width and the height of whole images, respectivelly.",divided width height whole,issue,negative,positive,positive,positive,positive,positive
919716041,"@n-yoshida14  Thanks for your reply, one more question, how did you normalized x and y values? Thanks",thanks reply one question thanks,issue,positive,positive,positive,positive,positive,positive
919711761,"@farazBhatti 
I'm very sorry the reply was late.
I attached the sample_input.csv, so please check it.
Best.
[sample_input.csv](https://github.com/google-research/google-research/files/7166960/sample_input.csv)
",sorry reply late attached please check best,issue,positive,positive,neutral,neutral,positive,positive
919692104,"@n-yoshida14 , is it possible for you to share sample ""input.csv"" file. I am having trouble in making one. I need to get inference to varify results. Thanks ",possible share sample file trouble making one need get inference thanks,issue,negative,neutral,neutral,neutral,neutral,neutral
919189204,"> Hi, I'm trying to install [tf3d](https://github.com/google-research/google-research/tree/master/tf3d) but encounter a problem of step 1 [Preparing and Compiling the Sparse Conv Op](https://github.com/google-research/google-research/tree/master/tf3d/ops).
> My virtual environment (managed by using miniconda) is `tensorflow==2.3.0, Python=3.7` running on RTX-3090.
> I tried `pip3 install tf3d/ops/packages/tensorflow_sparse_conv_ops-0.0.1-cp37-cp37m-linux_x86_64.whl `.
> Then when I ` import tf3d.ops.tensorflow_sparse_conv_ops as sparse_conv_ops` in python shell, I got error message as bellow:
> 
> ```shell
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/media/xxx/357B56513FD6C042/dockers/tf3d/google-research/tf3d/ops/tensorflow_sparse_conv_ops/__init__.py"", line 19, in <module>
>     from tensorflow_sparse_conv_ops import sparse_conv_ops
>   File ""/home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow_sparse_conv_ops/__init__.py"", line 18, in <module>
>     from tensorflow_sparse_conv_ops.sparse_conv_ops import submanifold_sparse_conv3d, submanifold_sparse_conv2d
>   File ""/home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow_sparse_conv_ops/sparse_conv_ops.py"", line 27, in <module>
>     resource_loader.get_path_to_datafile('_sparse_conv_ops.so'))
>   File ""/home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
>     lib_handle = py_tf.TF_LoadLibrary(library_filename)
> tensorflow.python.framework.errors_impl.NotFoundError: /home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow_sparse_conv_ops/_sparse_conv_ops.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb
> >>> 
> ```

Hi, I want to ask if you were able to solve the sparse conv problem, I have got one here that I couldn't move around it. Pls let me know as soon as you are available. ",hi trying install encounter problem step sparse virtual environment running tried pip install import python shell got error message bellow shell recent call last file line module file line module import file line module import file line module file line undefined symbol hi want ask able solve sparse problem got one could move around let know soon available,issue,negative,positive,positive,positive,positive,positive
918862430,"In my case this is just due to not install cuda and cudnn, after properly install them, everything goes ok",case due install properly install everything go,issue,negative,negative,neutral,neutral,negative,negative
918531073,"@tansaku I see, here's what I suggest you try:
Reinstall python@3.8, note: python version need to be greater than 3.8.10
```
brew reinstall python@3.8
```
(if you use homebrew, you should end up with 3.8.12)
Make sure your virtual env/any kind of env management system use python3.8.12 as your virtualenv python interpreter and then pip install scann

reference: https://bugs.python.org/issue41100
",see suggest try reinstall python note python version need greater brew reinstall python use end make sure virtual kind management system use python python interpreter pip install reference,issue,positive,positive,positive,positive,positive,positive
918339977,"thanks @XiaoConstantine - however that does not appear to have helped:

```
. /opt/anaconda3/bin/activate && conda activate /opt/anaconda3; 
❯ . /opt/anaconda3/bin/activate && conda activate /opt/anaconda3;
❯ export SYSTEM_VERSION_COMPAT=1
❯ pip install scann
ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann
```",thanks however appear activate activate export pip install error could find version requirement none error matching distribution found,issue,negative,positive,positive,positive,positive,positive
918036077,"> Hi @Wayne-wonderai and @michaellin99999,
> Since 3090 is a relatively new card I suggest checking the minimum CUDA and driver version (the 2.3.0 tf docker is cuda 10.1).
> Besides, please try a few lines of code that use tf on gpu to test it out inside the docker first before retrying our sparse conv test.
> Thanks

Hi, I have been stuck at step 5 for a while in sparse conv ops. Would greatly appreciate your help. 
Thanks. ",hi since relatively new card suggest minimum driver version docker besides please try code use test inside docker first sparse test thanks hi stuck step sparse would greatly appreciate help thanks,issue,positive,positive,positive,positive,positive,positive
918034891,"> Hi, I am running into the same issue. any help would be greatly appreciated.

Hi, I want to know if you were able to solve the sparse conv problem. I couldn't implement the convolutional neural network here. ",hi running issue help would greatly hi want know able solve sparse problem could implement convolutional neural network,issue,negative,positive,positive,positive,positive,positive
918033546,"> Sure, but show me at what step of the process exactly do you have difficulties ?

Hi, I have been waiting for your response. Pls let me know when you are available. ",sure show step process exactly hi waiting response let know available,issue,negative,positive,positive,positive,positive,positive
917463887,"There is no need to use colab. Please follow instructions from [experiment description](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md) and let me know if you have any issues/questions. You could train a model on any laptop or PC without GPU: models are small(<1MB) and data sets are <5GB. Sorry, I do not keep old pre-trained models because with new feature requests they can become incopatible with the lates kws_streaming version.",need use please follow experiment description let know could train model without small data sorry keep old new feature become version,issue,negative,negative,negative,negative,negative,negative
917398479,"Hi, I have kept my eyes on my mail particularly on this hoping to get your
response, lol, guess you have been busy, understandably, I will appreciate
if you can let me know when you are available.
Thanks

On Fri, Sep 10, 2021, 8:49 AM Holadele Shegun ***@***.***>
wrote:

> Thank you for your reply, I take it very seriously.
>
> I guess I was able to prepare the setup successfully, but moving forward
> to a 3D sparse convo has been so challenging.
>
> I guess the real problem started from step 3 Download the Tensorflow
> custom op repo to local folder tf_custom_op
>
> I couldn't get the step described in that step 3 so what I did was to use
> the command in step 4, the command cloned the custom_op folder into my VM
> and I copy the necessary files out to the required folder.
>
> I will appreciate it if you respond as soon as you can.
>
> Thanks and best regards
>
> On Thu, Sep 9, 2021, 9:08 PM Hamza Alaoui ***@***.***>
> wrote:
>
>> Sure, but show me at what step of the process exactly do you have
>> difficulties ?
>>
>> —
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/google-research/google-research/issues/614#issuecomment-916553698>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/ARIHG5I66X4BDVQTIUVVBYTUBFLBZANCNFSM4Y3DZIWA>
>> .
>> Triage notifications on the go with GitHub Mobile for iOS
>> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
>> or Android
>> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>>
>>
>
",hi kept mail particularly get response guess busy understandably appreciate let know available thanks wrote thank reply take seriously guess able prepare setup successfully moving forward sparse guess real problem step custom local folder could get step step use command step command folder copy necessary folder appreciate respond soon thanks best hamza wrote sure show step process exactly reply directly view triage go mobile android,issue,positive,positive,positive,positive,positive,positive
917362800,"> @Shegun93 what problems did you find exactly?

Hi Hamza, couldn't get your response again. Now stuck in step 5. I did 2-4 by copying files manually, this error I got in step 5 is here attached as an image. Pls, I will appreciate you giving me a helping hand. Thanks. 
![20210911_034857](https://user-images.githubusercontent.com/72381301/132940867-239f8e56-61a4-4c9a-a27e-d7f2e57a466c.jpg)
",find exactly hi hamza could get response stuck step manually error got step attached image appreciate giving helping hand thanks,issue,positive,positive,positive,positive,positive,positive
917221075,"@rybakov , you are an amazing person! Thanks again! You have always helped me extra mile :)
Yes, the difference is in the flag parameter that might have caused the varying results. I am going to use the models generated post model training and will re-implement part of inference.py in c++ as suggested.


",amazing person thanks always extra mile yes difference flag parameter might going use post model training part,issue,positive,positive,positive,positive,positive,positive
917184719,"If you trained your model, then all conversion to streaming is already done (you can use it as it is) so there is no need to convert the model again. Your example 1 and example 2 should be numerically the same, I guess there is a difference in flags parameters, which are used for converting a model in colab (I would suggest to use model generated by training script, utils.model_to_tflite is part of it any way).

I do not have open sourced c++ demo. But there are a lot of [examples](https://stackoverflow.com/questions/56837288/tensorflow-lite-c-api-example-for-inference) of running tflite module in c++. TFLite c++ api is similar to python, you will need to re-implement [py](https://github.com/google-research/google-research/blob/e5e376157ef4e5c425cf9ee1da06537d930067fe/kws_streaming/train/inference.py#L104) in c++.
",trained model conversion streaming already done use need convert model example example numerically guess difference used converting model would suggest use model training script part way open lot running module similar python need,issue,negative,neutral,neutral,neutral,neutral,neutral
917149616,"Following the [inference notebook](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/02_inference.ipynb)

I see that the model is converted into streaming mode as below, (Run streaming inference with TFLite):

```
tflite_streaming_model = utils.model_to_tflite(sess, model_non_stream_batch, flags, Modes.STREAM_EXTERNAL_STATE_INFERENCE)
tflite_stream_fname = 'tflite_stream.tflite'



with open(os.path.join(MODEL_PATH, tflite_stream_fname), 'wb') as fd:
  fd.write(tflite_streaming_model)
```

But after training a model, we already have the `tflite_stream.tflite` file, so not sure why not use that directly?

The notebook uses `model_content`, for `tf.lite.Interpreter` as an argument,
```
example 1:interpreter = tf.lite.Interpreter(***model_content***=tflite_non_streaming_model)
```

Isn't it ok to just use the streaming `.tflite` file from the trained models folder like below using the path and run it for inference?

```
example 2: interpreter = tf.lite.Interpreter(***model_path***='path_to_/tflite_stream.tflite')
```

I tried example 1 and example 2 and both of them predict different labels for the same input by the same `.tflite` model. 
I want to implement `.tflite` model on an embedded platform in C++. 

Do you have some recommendations to directly implement the available .tflite  in C++ and skip converting functions like```utils.model_to_tflite```  and others from python to C++ that have been used in the inference notebook.",following inference notebook see model converted streaming mode run streaming inference sess open training model already file sure use directly notebook argument example interpreter use streaming file trained folder like path run inference example interpreter tried example example predict different input model want implement model platform directly implement available skip converting like python used inference notebook,issue,positive,positive,positive,positive,positive,positive
916956329,"Try:
```bash
export SYSTEM_VERSION_COMPAT=1
```
and then proceed",try bash export proceed,issue,negative,neutral,neutral,neutral,neutral,neutral
916879636,"Thank you for your reply, I take it very seriously.

I guess I was able to prepare the setup successfully, but moving forward to
a 3D sparse convo has been so challenging.

I guess the real problem started from step 3 Download the Tensorflow custom
op repo to local folder tf_custom_op

I couldn't get the step described in that step 3 so what I did was to use
the command in step 4, the command cloned the custom_op folder into my VM
and I copy the necessary files out to the required folder.

I will appreciate it if you respond as soon as you can.

Thanks and best regards

On Thu, Sep 9, 2021, 9:08 PM Hamza Alaoui ***@***.***> wrote:

> Sure, but show me at what step of the process exactly do you have
> difficulties ?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/614#issuecomment-916553698>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ARIHG5I66X4BDVQTIUVVBYTUBFLBZANCNFSM4Y3DZIWA>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>
",thank reply take seriously guess able prepare setup successfully moving forward sparse guess real problem step custom local folder could get step step use command step command folder copy necessary folder appreciate respond soon thanks best hamza wrote sure show step process exactly reply directly view triage go mobile android,issue,positive,positive,positive,positive,positive,positive
916872591,"> Sure, but show me at what step of the process exactly do you have difficulties?

Step 1 3D sparse. ",sure show step process exactly step sparse,issue,negative,positive,positive,positive,positive,positive
916553698,"Sure, but show me at what step of the process exactly do you have difficulties ?",sure show step process exactly,issue,negative,positive,positive,positive,positive,positive
916552695,"> I was able to solve this one on my own, for anyone having the same issue, take a look at :
> echo ""_ZN10tensorflow7strings6StrCatB5cxx11ERKNS0_8AlphaNumES3_S3_"" | c++filt
> then
> nm -D /usr/local/lib/python3.6/dist-packages/tensorflow/libtensorflow_framework.so.2 |c++filt | grep ""tensorflow::strings::StrCat""
> You'll see the different tags, from there you can either rebuild your tensorflow, or if using bazel add --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""
> (i'm using tensorflow 2.3.2 in a docker container under ppc64le)
> ![Debug_n+3](https://user-images.githubusercontent.com/47244743/121227825-03206a80-c85a-11eb-8f7e-3ca46c3f6fad.PNG)
>![SOLUTION XD](https://user-images.githubusercontent.com/47244743/121227827-03b90100-c85a-11eb-9af2-3cab064c1705.PNG)

Hi I see you have been able to prepare the ops for 3d sparse convolutional neural network, may I ask if you can guide me to it. The problem is m lost, can't even diagnose where I have problem. ",able solve one anyone issue take look echo see different either rebuild add docker container solution hi see able prepare sparse convolutional neural network may ask guide problem lost ca even diagnose problem,issue,negative,positive,positive,positive,positive,positive
916550938,"Hi, it's 2 weeks I have been trying to compile the 3d ops for 3D object detection following all steps here, I couldn't, is there someone that can guide me out of it. Thanks",hi trying compile object detection following could someone guide thanks,issue,negative,positive,neutral,neutral,positive,positive
916253634,"@rybakov , got more than my answers, simply great! Thanks again!",got simply great thanks,issue,positive,positive,positive,positive,positive,positive
916235510,"I would suggest to play with [colab](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/00_check_data.ipynb) which analyzes the data and shows labels distribution.

""Q.1 if we train a model with --wanted-word 'srewai', it will consider everything else in the data folder (bed,bird,dog,house,marvin,sheila,tree,wow) as unknown words but still use these words for training the model, right?""
Yes, if you have only one wanted word 'srewai', then number of output labels will include:
'srewai'
“silence”  
“unknown” (it will use all other remaining words).

""Q.2 How do I add additional background noise for the training? Do I just add the noise .wav files into the _background_noise_ folder? Is there a limitation on the number of files , filenames and audio file length in _background_noise_ folder?""
Yes, you can add more files in _background_noise_ there is no limit on the number of files.
Audio length has to be the same on all files.

All data split logic is in [link](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py), it is based on original [TF speech_commads paper](https://arxiv.org/abs/1804.03209) for backward compatibility and results comparison.
For publishing you can use it as it is, for production applications I would encourage you to modify it for your needs.
",would suggest play data distribution train model consider everything else data folder bed bird dog house tree wow unknown still use training model right yes one word number output include silence unknown use add additional background noise training add noise folder limitation number audio file length folder yes add limit number audio length data split logic link based original paper backward compatibility comparison use production would encourage modify need,issue,positive,positive,positive,positive,positive,positive
916095503,"If you don't mind I have few questions as below, just for confirmation:

Consider the below data folder :
```
data folder:
_background_noise_  bed  bird  cat  dog  happy  srewai  house  marvin   sheila  tree  wow
```
Q.1 if we train a model with `--wanted-word 'srewai'`, it will consider everything else in the data folder `(bed,bird,dog,house,marvin,sheila,tree,wow)` as unknown words but still use these words for training the model, right?

Q.2 How do I add additional background noise for the training? Do I just add the noise .wav files into the `_background_noise_` folder?  Is there a limitation on the number of `files` , `filenames` and `audio file length`  in ` _background_noise_`  folder?
",mind confirmation consider data folder data folder bed bird cat dog happy house tree wow train model consider everything else data folder bed bird dog house tree wow unknown still use training model right add additional background noise training add noise folder limitation number audio file length folder,issue,positive,positive,positive,positive,positive,positive
916075744,"@rybakov,  you're amazing! Thank you so much for your help! It works :)
I really appreciate it. Very quick response!",amazing thank much help work really appreciate quick response,issue,positive,positive,positive,positive,positive,positive
916064677,"Hi, we don't plan to release the TF one version, as it tied to internal Google infrastructure. Could you maybe try using the most recent TF2 nightly rather than the one specified in the requirments.txt ",hi plan release one version tied internal infrastructure could maybe try recent nightly rather one,issue,negative,neutral,neutral,neutral,neutral,neutral
916061882,"Hi @BlackBearBiscuit  I've never noticed anything like this before. Could you maybe post your logs/tensorboard? Also you say you set the num of examples to 450,000, how many examples does the dataset actually contain(are you using all of DF)?",hi never anything like could maybe post also say set many actually contain,issue,negative,positive,positive,positive,positive,positive
915428018,"If you trained model with description from [experiments](https://github.com/google-research/google-research/tree/master/kws_streaming/experiments) then your model already converted to streaming mode and stored in folder (both model artifacts with test accuracy) which you used for training: folder structure is described in the end on [link](https://github.com/google-research/google-research/tree/master/kws_streaming)

If you need more details about how conversion to streaming is done, then please have a look at [function](https://github.com/google-research/google-research/blob/master/kws_streaming/models/utils.py#L266)",trained model description model already converted streaming mode folder model test accuracy used training folder structure end link need conversion streaming done please look function,issue,negative,neutral,neutral,neutral,neutral,neutral
915419356,"svdf_resnet was added after paper was published. It was not streamable, so in the last CL I made it streamable. Please try
```
$CMD_TRAIN \
--wanted_words 'srewai' \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/svdf_resnet/ \
--mel_upper_edge_hertz 7600 \
--how_many_training_steps 20000,20000,20000,20000 \
--learning_rate 0.001,0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 80 \
--dct_num_features 40 \
--resample 0.15 \
--time_shift_ms 100 \
--feature_type 'mfcc_tf' \
--fft_magnitude_squared 1 \
--preprocess 'raw' \
--train 1 \
--lr_schedule 'exp' \
--data_stride 4 \
svdf_resnet \
--block1_memory_size '7' \
--block2_memory_size '7' \
--block3_memory_size '11,11' \
--block1_units1 '32' \
--block2_units1 '50' \
--block3_units1 '50,128' \
--blocks_pool '2,2,1' \
--use_batch_norm 1 \
--bn_scale 1 \
--activation 'relu' \
--svdf_dropout 0.0 \
--svdf_pad 1 \
--svdf_use_bias 0 \
--dropout1 0.0 \
--units2 '64' \
--flatten 0
```",added paper last made please try resample train activation dropout flatten,issue,negative,neutral,neutral,neutral,neutral,neutral
915219566,"@rybakov, thanks so much for your help.  kws_experiments_paper_12_labels do not have svdf_resnet training example and svdf works fine for me. So, once you have time to implement the changes, please add a comment here for the heads up! :)

Thanks again!",thanks much help training example work fine time implement please add comment thanks,issue,positive,positive,positive,positive,positive,positive
914118907,"Problem solved - I did not saw, that the id is kind of batched too. ",problem saw id kind,issue,negative,positive,positive,positive,positive,positive
913521748,"that's good to hear @kim-sardine - can you be more specific about which version works for you?  3.8.7 not working for me:

```
❯ python
Python 3.8.7 (default, Feb  3 2021, 17:06:40) 
[Clang 12.0.0 (clang-1200.0.32.28)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> 
❯ pip install scann
ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann
```

or you mean you've tried with all versions between 3.6 and 3.9 and they all work?",good hear specific version work working python python default clang type help copyright license information pip install error could find version requirement none error matching distribution found mean tried work,issue,negative,positive,positive,positive,positive,positive
913241210,"There is also issue with striding parameter in the model. I will submit a fix on Monday.
After that you should also add:
```
--data_stride 4 \
```
",also issue parameter model submit fix also add,issue,negative,neutral,neutral,neutral,neutral,neutral
913016234,"With option ""mfcc_tf"" you should follow [kws_experiments_paper_12_labels](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md): these are not quantized models.

If you need quantized models then it has to be [kws_experiments_quantized_12_labels](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_quantized_12_labels.md) in this case you will need tflite flex, as descried in [stackoverflow](https://stackoverflow.com/questions/57658509/how-to-invoke-the-flex-delegate-for-tflite-interpreters).

It was designed this way because TFLite did not support some tf ops and I had to use custom TFLite ops with ""mfcc_op"" (for quantized models) or their direct tf re-implementation with ""mfcc_tf"" (for non quantized models). In the last case if you enable quantization it will also quantize DFT weights which will degrade model accuracy. Now TFLite supports all ops in this lib, so I can try to enable quantization of ""mfcc_tf"" too.

",option follow need case need flex designed way support use custom direct non last case enable quantization also quantize degrade model accuracy try enable quantization,issue,negative,positive,neutral,neutral,positive,positive
913012857,"> I submitted above fix, please pull the latest version of [kws_streaming](https://github.com/google-research/google-research/tree/master/kws_streaming).

Thank you @rybakov. Its working now.",fix please pull latest version thank working,issue,positive,positive,positive,positive,positive,positive
913003408,"Thanks @rybakov , when I replace the suggested , I still get the error the second error ``` FAILED to run TFLite streaming: Mmap of '7' at offset '0' failed with error '22'.```:

```
2021-09-04 18:41:59.625944: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.
2021-09-04 18:41:59.625969: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.
2021-09-04 18:41:59.636301: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2021-09-04 18:41:59.698157: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1922] Estimated count of arithmetic ops: 66.155 M  ops, equivalently 33.077 M  MACs

Estimated count of arithmetic ops: 66.155 M  ops, equivalently 33.077 M  MACs
I0904 18:42:00.552432 139941496817472 test.py:510] tflite test accuracy, non stream model = 32.34% 200 out of 221
I0904 18:42:00.605312 139941496817472 test.py:514] tflite Final test accuracy, non stream model = 32.13% (N=221)
I0904 18:42:00.606071 139941496817472 model_train_eval.py:236] run TF evalution only without optimization/quantization
I0904 18:42:01.808442 139941496817472 model_train_eval.py:255] FAILED to run TF streaming: Negative dimension size caused by subtracting 3 from 1 for '{{node streaming/max_pooling1d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=""NHWC"", explicit_paddings=[], ksize=[1, 3, 1, 1], padding=""VALID"", strides=[1, 2, 1, 1]](streaming/max_pooling1d/ExpandDims)' with input shapes: [1,1,1,32].
I0904 18:42:01.809512 139941496817472 model_train_eval.py:257] run TFlite streaming model accuracy evaluation
W0904 18:42:02.433501 139941496817472 test.py:560] FAILED to convert to mode STREAM_EXTERNAL_STATE_INFERENCE, tflite: Negative dimension size caused by subtracting 3 from 1 for '{{node streaming/max_pooling1d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=""NHWC"", explicit_paddings=[], ksize=[1, 3, 1, 1], padding=""VALID"", strides=[1, 2, 1, 1]](streaming/max_pooling1d/ExpandDims)' with input shapes: [1,1,1,32].
I0904 18:42:02.433842 139941496817472 test.py:364] tflite stream model state external with reset_state 1
I0904 18:42:02.692718 139941496817472 model_train_eval.py:282] FAILED to run TFLite streaming: Mmap of '7' at offset '0' failed with error '22'.
I0904 18:42:02.693176 139941496817472 model_train_eval.py:194] feature type mfcc_tf needs quantization aware training for quantization - it is not implemented
```


",thanks replace still get error second error run streaming offset error crash reproducer set enable count arithmetic equivalently count arithmetic equivalently test accuracy non stream model final test accuracy non stream model run without run streaming negative dimension size node valid input run streaming model accuracy evaluation convert mode negative dimension size node valid input stream model state external run streaming offset error feature type need quantization aware training quantization,issue,negative,negative,neutral,neutral,negative,negative
912981961,"> It should be ok to use .h5 format, but you still need to checkpoint trainer state (using tf.train.Checkpoint) and load it back together with model weights to resume the training as described in [link](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save_weights)

Thank you @rybakov ",use format still need trainer state load back together model resume training link thank,issue,negative,neutral,neutral,neutral,neutral,neutral
912727337,"I submitted above fix, please pull the latest version of [kws_streaming](https://github.com/google-research/google-research/tree/master/kws_streaming).",fix please pull latest version,issue,negative,positive,positive,positive,positive,positive
912708529,"It is a known issue [stackoverflow](https://stackoverflow.com/questions/57658509/how-to-invoke-the-flex-delegate-for-tflite-interpreters), there are are two options to solve it: 1 rebuild tf with bazel with enabled flex delegate; 2 use tf as it is but do not use flex ops in your model and replace 
```
--feature_type 'mfcc_op' \
```
by 
```
--feature_type 'mfcc_tf' \
```
The last one will use standard tf ops (this is what we used in the paper). 'mfcc_op' is a good option if you plan to quantize you model.",known issue two solve rebuild flex delegate use use flex model replace last one use standard used paper good option plan quantize model,issue,positive,positive,positive,positive,positive,positive
912701635,Any android or non android (ios) phone supported by TFLite.,android non android phone,issue,negative,neutral,neutral,neutral,neutral,neutral
912679060,"You're amazing! Thanks, tons!
The android phone can be any and doesn't necessarily need to be pixel4, right!?
",amazing thanks android phone necessarily need right,issue,positive,positive,positive,positive,positive,positive
912670927,"It looks like a bug in tf, introduced in the latest version, please feel free to file a bug for [tf](https://github.com/tensorflow/tensorflow/issues). As error suggested, try using tf.compat.v1.experimental.output_all_intermediates(True).
Please insert:
```
  if flags.model_name=='crnn':
    tf.compat.v1.experimental.output_all_intermediates(True) 
```
into [line 72](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L72)
",like bug latest version please feel free file bug error try true please insert true line,issue,positive,positive,positive,positive,positive,positive
912648039,"Sure, it is all in [experiments](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md), you should [compile-tflite-benchmarking-tools](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#compile-tflite-benchmarking-tools) then train a model and run benchmark on phone, for [example](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#svdf). From above link, benchmarking of a non streaming model:
```
adb shell rm -f /data/local/tmp/model.tflite
adb push $MODELS_PATH/svdf/tflite_non_stream/non_stream.tflite /data/local/tmp/model.tflite
adb shell taskset f0 /data/local/tmp/benchmark_model_plus_flex --graph=/data/local/tmp/model.tflite --warmup_runs=1 --num_threads=1 --num_runs=1000 > $MODELS_PATH/svdf/tflite_non_stream/non_stream.tflite.benchmark
adb shell taskset f0 /data/local/tmp/benchmark_model_plus_flex --graph=/data/local/tmp/model.tflite --warmup_runs=1 --num_threads=1 --num_runs=1000 --enable_op_profiling=true > $MODELS_PATH/svdf/tflite_non_stream/non_stream.tflite.benchmark.profile
```
and benchmark of streaming model:
```
adb shell rm -f /data/local/tmp/model.tflite
adb push $MODELS_PATH/svdf/tflite_stream_state_external/stream_state_external.tflite /data/local/tmp/model.tflite
adb shell taskset f0 /data/local/tmp/benchmark_model_plus_flex --graph=/data/local/tmp/model.tflite --warmup_runs=1 --num_threads=1 --num_runs=1000 > $MODELS_PATH/svdf/tflite_stream_state_external/stream_state_external.tflite.benchmark
adb shell taskset f0 /data/local/tmp/benchmark_model_plus_flex --graph=/data/local/tmp/model.tflite --warmup_runs=1 --num_threads=1 --num_runs=1000 --enable_op_profiling=true > $MODELS_PATH/svdf/tflite_stream_state_external/stream_state_external.tflite.benchmark.profile
```
An example of running tflite model on mobile app you can find at [link](https://www.tensorflow.org/lite/examples/audio_classification/overview)",sure train model run phone example link non streaming model shell push shell shell streaming model shell push shell shell example running model mobile find link,issue,negative,positive,positive,positive,positive,positive
912466736,"I found the same problem, which is changing `anisotropic_quantization_threshold` from 0.001 to 0.4 results in the same recall-rate, and i have no idea how to fix it. It would be very helpful if the author or contributor can reply.",found problem idea fix would helpful author contributor reply,issue,negative,neutral,neutral,neutral,neutral,neutral
912359735,"Thank you @rybakov . In the paper, it says the KWS experiments were tested in real time for latency and accuracy on pixel4 phones. But I couldn't find that in the repo or did I miss something? I want to produce the same results as paper on phones. Any leads, please!",thank paper tested real time latency accuracy could find miss something want produce paper please,issue,positive,positive,positive,positive,positive,positive
912336847,For any one looking for  pre trained model. Here is the link : https://github.com/google-research/google-research/issues/457#issuecomment-863501473,one looking trained model link,issue,negative,neutral,neutral,neutral,neutral,neutral
910948957,I would suggestr to look at [link](https://stackoverflow.com/questions/47189624/maintain-a-streaming-microphone-input-in-python). Easy to start and build a demo in python.,would look link easy start build python,issue,negative,positive,positive,positive,positive,positive
910895763,"colab examples were created just for demo purposes and getting familiar with this lib. 
You should follow any example in [experiments](https://github.com/google-research/google-research/tree/master/kws_streaming/experiments), e.g. to reproduce results from our paper you need to run [kws_experiments_paper_12_labels.md](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md). 
Most models have small size, so you could train them even on laptop cpu (you can control training time by changing how_many_training_steps). 
I attached svdf model trained with recipe from [kws_experiments_paper_12_labels.md](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md).
I trained it with below command:
```shell
$CMD_TRAIN \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/svdf/ \
--mel_upper_edge_hertz 7600 \
--how_many_training_steps 20000,20000,20000,20000 \
--learning_rate 0.001,0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 80 \
--dct_num_features 30 \
--resample 0.15 \
--alsologtostderr \
--time_shift_ms 100 \
--train 1 \
svdf \
--svdf_memory_size 4,10,10,10,10,10 \
--svdf_units1 256,256,256,256,256,256 \
--svdf_act ""'relu','relu','relu','relu','relu','relu'"" \
--svdf_units2 128,128,128,128,128,-1 \
--svdf_dropout 0.0,0.0,0.0,0.0,0.0,0.0 \
--svdf_pad 0 \
--dropout1 0.0 \
--units2 '' \
--act2 ''
``` 

Trained model is in [svdf.zip](https://github.com/google-research/google-research/files/7095031/svdf.zip)",getting familiar follow example reproduce paper need run small size could train even control training time attached model trained recipe trained command shell resample train dropout act trained model,issue,negative,positive,neutral,neutral,positive,positive
908897242,"it says `ScaNN supports Linux environments running Python versions 3.6-3.9`
I found it works on docker with python 3.6-3.9",running python found work docker python,issue,negative,neutral,neutral,neutral,neutral,neutral
908809556,"Hello, you can contact me by email (jjsun@caltech.edu) for a pre-trained model. We have the inference script here: https://github.com/google-research/google-research/blob/master/poem/pr_vipe/infer.py",hello contact model inference script,issue,negative,neutral,neutral,neutral,neutral,neutral
908560391,"It should be ok to use .h5 format, but you still need to checkpoint trainer state (using tf.train.Checkpoint) and load it back together with model weights to resume the training as described in [link](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save_weights)",use format still need trainer state load back together model resume training link,issue,negative,neutral,neutral,neutral,neutral,neutral
908421651,"Is it okay to save the model weights in .h5 format and load it in same format?
I have inserted `model.save_weights(flags.train_dir + 'best_weights.h5')` after this [line](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L194) and trained it on less trainings steps.

Then I had inserted `model.load_weights(flags.train_dir + 'best_weights.h5')` just before the training loop [here](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L123).

Now it is successfully loading the previous weights while training. I wanted to know that will this method of loading weights can affect the other model parameters and inference or its just fine to use this way of loading weights.",save model format load format inserted line trained le inserted training loop successfully loading previous training know method loading affect model inference fine use way loading,issue,positive,positive,positive,positive,positive,positive
907630799,"wish quick please



---Original---
From: ***@***.***&gt;
Date: Sat, Aug 28, 2021 22:00 PM
To: ***@***.***&gt;;
Cc: ***@***.******@***.***&gt;;
Subject: Re: [google-research/google-research] When will the musiq network code be open source (#800)




 
also waiting
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.
Triage notifications on the go with GitHub Mobile for iOS or Android.",wish quick please date sat subject network code open source also waiting thread reply directly view triage go mobile android,issue,positive,positive,neutral,neutral,positive,positive
907535962,"@vishalkevat007 all dependencies are described in [experiment](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_12_labels.md#install-tensorflow-with-deps)

You should install:
```
pip install --upgrade pip
pip install tf_nightly
pip install tensorflow_addons
pip install tensorflow_model_optimization

# install libs:
pip install pydot
pip install graphviz
pip install numpy
pip install absl-py
```

I run colab using jupyter notebook
So I also install it, by running:
```
pip install notebook
```
Then start a notebook by running:
```
jupyter notebook
```
After that load colab from [folder](https://github.com/google-research/google-research/tree/master/kws_streaming/colab) and run it step by step. This way there is no need to install anything in colab.


You can also install dependencies directly in colab by executing:
```
!pip install tf_nightly
```
But I prefer to avoid it, so that you don run this installation in every colab.
More details about this option are described in [link](https://stackoverflow.com/questions/54223282/installing-tf-nightly-gpu-2-0-preview-on-google-colab)",experiment install pip install upgrade pip pip install pip install pip install install pip install pip install pip install pip install run notebook also install running pip install notebook start notebook running notebook load folder run step step way need install anything also install directly pip install prefer avoid run installation every option link,issue,negative,positive,neutral,neutral,positive,positive
907532589,"This is the problem I mentioned earlier optimizer was not checkpointed.
I would suggest to check a [description](https://stackoverflow.com/questions/33759623/how-to-save-restore-a-model-after-training). kws_streaming training is based on tf v1, so you should use ""Tensorflow < 2"" option described in [description](https://stackoverflow.com/questions/33759623/how-to-save-restore-a-model-after-training)",problem would suggest check description training based use option description,issue,negative,neutral,neutral,neutral,neutral,neutral
907530471,"@vishalkevat007 please share error message.
You should follow steps described in [experiments](https://github.com/google-research/google-research/tree/master/kws_streaming/experiments)
For example install all dependencies before running your colab:
```
pip install --upgrade pip
pip install tf_nightly
pip install tensorflow_addons
pip install tensorflow_model_optimization

# install libs:
pip install pydot
pip install graphviz
pip install numpy
pip install absl-py
```
After all above you can execute
```
jupyter notebook
```
and run [colabs](https://github.com/google-research/google-research/tree/master/kws_streaming/colab)",please share error message follow example install running pip install upgrade pip pip install pip install pip install install pip install pip install pip install pip install execute notebook run,issue,negative,neutral,neutral,neutral,neutral,neutral
907286827,"Thankyou @rybakov 
I have tried to insert model.load_weights() after the line number specified by you.
I trained the model first on fews training steps (100,100,100), then inserted the below code in train.py
`
weights_name = 'best_weights'`
`model.load_weights(os.path.join(flags.train_dir, weights_name))
`
I am getting an error,

InvalidArgumentError: Cannot colocate nodes node training/Adam/svdf_0/dense/kernel/m (defined at ./google-research/kws_streaming/train/trains.py:150) placed on device No device assignments were active during op 'training/Adam/svdf_0/dense/kernel/m' creation.  and node svdf_0/dense/kernel/Initializer/random_uniform/shape (defined at ./google-research/kws_streaming/layers/svdf.py:127) placed on device Device assignments active during op 'svdf_0/dense/kernel/Initializer/random_uniform/shape' creation:
  with tf.device(None): </usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1738> : Cannot merge devices with incompatible types: '/job:localhost/replica:0/task:0/device:GPU:0' and '/device:CPU:0'
	 [[node training/Adam/svdf_0/dense/kernel/m (defined at ./google-research/kws_streaming/train/trains.py:150) ]]Additional information about colocations:Node-device colocations active during op 'training/Adam/svdf_0/dense/kernel/m' creation:
  with tf.colocate_with(svdf_0/dense/kernel): </usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:900>
  with tf.colocate_with(svdf_0/dense/kernel:0): </usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:900>
No device assignments were active during op 'training/Adam/svdf_0/dense/kernel/m' creation.
No node-device colocations were active during op 'svdf_0/dense/kernel/Initializer/random_uniform/shape' creation.
Device assignments active during op 'svdf_0/dense/kernel/Initializer/random_uniform/shape' creation:
  with tf.device(None): </usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1738>
",tried insert line number trained model first training inserted code getting error node defined device device active creation node defined device device active creation none merge incompatible node defined additional information active creation device active creation active creation device active creation none,issue,positive,negative,neutral,neutral,negative,negative
907270319,"@rybakov 
How can I install tf-nightly in colab? I was using pip install tf-nightly and I was getting error. Can you tell how you installed tf-nightly in colab to run all notebooks check_data, train and inference? ",install pip install getting error tell run train inference,issue,negative,neutral,neutral,neutral,neutral,neutral
906978268,"Hello, 

I installed tf-nightly and ran all colabs in jupyter notebook today: all worked ok.
Please confirm that you executed colab step by step including:
```
!git clone https://github.com/google-research/google-research.git
import sys
import os
import tarfile
import urllib
sys.path.append('./google-research')
```
Your error is pointing to missing kws_streaming module, it means that below command was not executed:
```
!git clone https://github.com/google-research/google-research.git
```
or the path was not set
```
sys.path.append('./google-research')
```

If above does not help, please let me know more details about your environment and how to reproduce the issue.",hello ran notebook today worked please confirm executed step step git clone import import o import import error pointing missing module command executed git clone path set help please let know environment reproduce issue,issue,negative,negative,negative,negative,negative,negative
906965627,"Hello, 
Yes as omerc1 suggested please use tf-nightly. kws_streaming is always in sync with the latest version of tf-nightly, and there can be some new changes in tf or keras which are part of tf-nightly, but not part of the official release yet.",hello yes please use always sync latest version new part part official release yet,issue,positive,positive,positive,positive,positive,positive
906942970,"Hello, 
kws_streaming is check-pointing only model weights in two places [here](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L186) and [here](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L194). You could restore model weights by inserting model.load_weights() in this [line](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L121). There is one problem with such approach: it does not restore optimizer state, because we did not checkpointing it. Overall I would suggest just to re-train the model (data and models are small). We assume that data and training are owned by user and our focus was mostly on streaming aware models/layers design, so training script here is for a demo purpose.",hello model two could restore model line one problem approach restore state overall would suggest model data small assume data training user focus mostly streaming aware design training script purpose,issue,negative,positive,positive,positive,positive,positive
906940361,"Please make me assignee and after that Merge pull request: #799, Thanks ",please make assignee merge pull request thanks,issue,positive,positive,positive,positive,positive,positive
906938489,"This was the error because of the TensorFlow model optimization, I solved this issue now all the cells are running properly in colab I will make a pull request, verify that and merge that.",error model optimization issue running properly make pull request verify merge,issue,negative,neutral,neutral,neutral,neutral,neutral
906922861,"I would like to work on it, Please assign it to me.
",would like work please assign,issue,positive,neutral,neutral,neutral,neutral,neutral
905185586,"Yes, I've regenerated the .pb files using current codebase and it behaves closer to the paper. Older versions from Dec 2020 are available as middlebury_dXXX_v1.pb. 
The metrics implemented in the flyingthings script don't match the official ones from the middlebury website. 

See this thread for more details.  https://github.com/google-research/google-research/issues/613#issuecomment-903609552",yes current closer paper older available metric script match official see thread,issue,negative,positive,positive,positive,positive,positive
905184897,@vtankovich Thank you for your effort! I will take a try with the new model.,thank effort take try new model,issue,negative,positive,positive,positive,positive,positive
905183347,"The preprocess is part of the .pb file and script. 
I've regenerated the .pb files using current codebase and it behaves closer to the paper. The training set became a bit worse (bad2.0: 6.12 ->6.34), and most of the difference is in PianoL lamp, so hopefully the old checkpoint was loaded correctly and the difference is due to runtime changes. 
I also verified that flying things and eth3d models are not affected, so this may be related to image normalization, which contains division ops.

Summary: middlebury_dXXX.pb now contain models generated with current codebase. Older versions from Dec 2020 are available as middlebury_dXXX_v1.pb.",part file script current closer paper training set bit worse bad difference lamp hopefully old loaded correctly difference due also flying affected may related image normalization division summary contain current older available,issue,negative,negative,neutral,neutral,negative,negative
904737431,"@minghaoBD this issue has not been resolved. Please open a new issue.

@tgale96 @codgeek",issue resolved please open new issue,issue,negative,positive,neutral,neutral,positive,positive
904608982,"Hi Lutz, 

I have exactly the same issue with you when parsing this tflite model but don't how to solve it. Could you share your solutions to it? Thank you.

Minghao ",hi exactly issue model solve could share thank,issue,positive,positive,positive,positive,positive,positive
904294465,"@vtankovich  thanks for your immediate reply.

you mean even the plant case is also not as good as previous result, and it might due to the different libraries.
  
oh, I forget to mention that I use **tensorflow==2.4.0**, which is not exactly correspond to **tensorflow>=1.11.0 in requirements.txt**. I will give it a try for  other**tensorflow>=1.11.0** setup.

=====================================================================
It seems tensorflow==2.2 behave the same and predict.py is use tensorflow 2.x by default.

maybe preprocess need to be taken like normalization (mean std)which is not add in inference?",thanks immediate reply mean even plant case also good previous result might due different oh forget mention use exactly correspond give try setup behave use default maybe need taken like normalization mean add inference,issue,positive,positive,neutral,neutral,positive,positive
904176747,Thanks! Do you mean you also found such issue?,thanks mean also found issue,issue,negative,negative,neutral,neutral,negative,negative
904130587,"@mazeyu Those metrics are implemented and used to evaluate flyingthings models. For middlebury metrics please refer to middelbury eval site. Currently the models are not being run correctly, and using a machine with older versions (Dec 2020) of python/tensorflow/numpy may help. I'm looking at what exactly causing the issue.",metric used evaluate metric please refer site currently run correctly machine older may help looking exactly causing issue,issue,positive,positive,positive,positive,positive,positive
904004215,"@fafancier 
Thanks for your interest in the work. It seems that the model behaves significantly different now (even the plant is not as good as it should be) , compared to Dec 2020. Can you try running it with older versions of the libraries while I'm investigating this? ",thanks interest work model significantly different even plant good try running older investigating,issue,positive,positive,positive,positive,positive,positive
903609552,"Hi, @vtankovich  thanks for your amazing work.  Recently, I'm working on the HITNet for stereo matching in **real scene**. So, I am  discovering the **generalization** of it.


The predicted disparity of plant and some other test data are excellent.  
![2021-08-20 13-34-21 的屏幕截图](https://user-images.githubusercontent.com/39083471/130425689-7e286746-b36b-4756-a6ee-5b625bc6f6cb.png)

However, I found that  the result of the pre_trained middlebury model (three models of different disparity are all tested) on the **test set is not so good as paper shows**.  as the following figure shows, left is from Figure 7 in the paper,  where right is the  test result of the model. 

![2021-08-23 17-16-15 的屏幕截图](https://user-images.githubusercontent.com/39083471/130424232-67d67127-2840-454f-b4df-e2546e6bb06c.png)


I also found that the published models not well adapted to illumination change and hue change like this.  

![2021-08-20 13-46-52 的屏幕截图](https://user-images.githubusercontent.com/39083471/130425873-74278038-dadc-48a4-b84a-03d1dc7f1b32.png)

**I'm curious that is the published model not well trained or not the best model? Are the models published not corresponding to the models produce the paper results**. 

Looking forward to your reply.

Thank you!



",hi thanks amazing work recently working stereo matching real scene generalization disparity plant test data excellent however found result model three different disparity tested test set good paper following figure left figure paper right test result model also found well illumination change hue change like curious model well trained best model corresponding produce paper looking forward reply thank,issue,positive,positive,positive,positive,positive,positive
903168394,"Try to update tf-nightly package. 
I had a similar problem and it helped.",try update package similar problem,issue,negative,neutral,neutral,neutral,neutral,neutral
902641309,"All Julia code has been removed. I also had to put in a couple of unittest fixes (and hacks).
I will get the other author to respond.",code removed also put couple get author respond,issue,negative,neutral,neutral,neutral,neutral,neutral
902641103,"All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.

We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.

*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).

ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle-research%2Fgoogle-research%2Fpull%2F785) for more info**.

<!-- need_author_consent -->",pull request submitter commit one someone pull request submitter need confirm project please confirm leaving comment pull request note project maintainer may author leave comment comment properly consent manually confirm consent commit author set label yes project information go,issue,positive,neutral,neutral,neutral,neutral,neutral
902263488,"If we are building an index offline, shouldn't it cover all the index samples ?",building index cover index,issue,negative,neutral,neutral,neutral,neutral,neutral
901724966,".???

El jue., 19 de agosto de 2021 3:26 a. m., cclvr ***@***.***>
escribió:

> Dear authors,
> I really appreciate your DualDICE paper and it gives me much insight.
> Recently I try to reproduce it on Reacher, but I can't get the results as
> you presented in the paper. The phenomena is quite strange: the estimate
> average step reward is quite close to the behavior policy's step reward
> rather than the target policy's. Since there is no source code for
> continuous action setting, could you help to provide more details?
> i) when calculating \nu(s',a'), I use (1/N)*\sum_{i=1}^{N}nu(s',\pi(s'))
> and try N=1,10,100 respectively, and the results are the same. How you
> calculate it?
> ii) For this setting, do you choose Fenchel conjugate trick or not?
> iii) Is any other tips I need to know?
>
> Look forward to your reply.
> Best wishes.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/787>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AVH3EEXPLKZBT3LLSI4DYBDT5S55FANCNFSM5CNWM6GQ>
> .
>
",el de de dear really appreciate paper much insight recently try reproduce reacher ca get paper phenomenon quite strange estimate average step reward quite close behavior policy step reward rather target policy since source code continuous action setting could help provide calculating use nu try respectively calculate setting choose conjugate trick need know look forward reply best thread reply directly view,issue,positive,positive,positive,positive,positive,positive
900758325,"We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.
In order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.

ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle-research%2Fgoogle-research%2Fpull%2F785) for more info**.

<!-- need_author_cla -->",found contributor license agreement sender pull request unable find commit author maybe used different address git used sign login double check someone else need sign well confirm order pas check please resolve problem comment fixed bot comment think anything information go,issue,positive,negative,neutral,neutral,negative,negative
899100413,"> @mhoangvslev Could you please make this edit to your notebook? Thanks in advance!

Done!",could please make edit notebook thanks advance done,issue,positive,positive,positive,positive,positive,positive
899099905,@mhoangvslev Could you please make this edit to your notebook? Thanks in advance!,could please make edit notebook thanks advance,issue,positive,positive,positive,positive,positive,positive
896369969,"Thank you @rybakov
Sorry for leaving this issue for a long time.
I understood the situation.
Let me close this issue!",thank sorry leaving issue long time understood situation let close issue,issue,negative,negative,negative,negative,negative,negative
895409471,"Hi - thanks for your interest! First, the main idea is to embed the 2D poses such that if they are projected from similar 3D poses, then they are close together in the embedding space (See Fig 1 of our paper: https://arxiv.org/pdf/1912.01001.pdf). 

To use 2D keypoints only, there are a few things you need to do:
- Set this flag to -1 (https://github.com/google-research/google-research/blob/d26ed752ca5f276f0c35a2d55e213b7cb2ea9339/poem/pr_vipe/train_base.py#L100-L101). The input will still try to read in 3D keypoints, so you can add some dummy ones (ex: set 3D keypoints to all 0s).
- Set this flag to 2D_INPUT (https://github.com/google-research/google-research/blob/d26ed752ca5f276f0c35a2d55e213b7cb2ea9339/poem/pr_vipe/train_base.py#L49) to disable camera augmentation.",hi thanks interest first main idea embed similar close together space see fig paper use need set flag input still try read add dummy ex set set flag disable camera augmentation,issue,positive,positive,positive,positive,positive,positive
893872331,"> > Hello,
> > I am trying to apply FAD but facing the Fatal Python error: Segmentation fault here.
> > Currently I am using tf 2.5.0 and apache-beam 2.24.0 with python 3.7.
> > I am stuck on the step > **Compute embeddings and eastimate multivariate Gaussians**
> > `python -m frechet_audio_distance.create_embeddings_main --input_files test_audio/test_files_background.cvs --stats stats/background_stats`
> > I found the problem **Fatal Python error** happened in `def create_pipeline()` of `create_embeddings_beam.py` :
> > > if files_input_list:
> > > examples = (
> > > pipeline
> > > | 'Read File List' >> ReadFromText(files_input_list)
> > > | 'Read Files' >> beam.ParDo(ReadWavFiles()))
> > 
> > 
> > Cause I am really not an expert using apache so couldn't figure it out what's going on.
> > Also by googling it, it seems something wrong with stack memory which may cause the **Segmentation fault** problem,
> > so I have tried to increase the stack that my operating system allocates for the python process ended by `ulimit -s 262140`.
> > But it's still not working.
> > I will be appreciate if someone has any ideas.
> > Thank you!!
> 
> I attached my log here:
> 
> > (fad-env) kp1@nipa2021-1226:~/google-research$ python -m frechet_audio_distance.create_embeddings_main --input_files test_audio/test_files_background.cvs --stats stats/background_stats  2021-07-26 15:34:57.534827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
> > 2021-07-26 15:34:57.534877: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
> > I0726 15:34:58.903872 140039776012032 pipeline.py:190] Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.
> > Fatal Python error: Segmentation fault
> > Current thread 0x00007f5d8d1a8700 (most recent call first):
> > File ""/usr/local/lib/python3.7/bz2.py"", line 346 in decompress
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/internal/pickler.py"", line 274 in loads
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 787 in **init**
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/core.py"", line 1266 in **init**
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/io/iobase.py"", line 1544 in expand
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 228 in apply_PTransform
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 198 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 664 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pvalue.py"", line 141 in **or**
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/io/iobase.py"", line 892 in expand
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 228 in apply_PTransform
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 198 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 664 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 561 in **ror**
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/io/textio.py"", line 564 in expand
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 228 in apply_PTransform
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 198 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 664 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 621 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 561 in **ror**
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 997 in **ror**
> > File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_beam.py"", line 359 in create_pipeline
> > File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_main.py"", line 69 in main
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/absl/app.py"", line 258 in _run_main
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/absl/app.py"", line 312 in run
> > File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_main.py"", line 75 in 
> > File ""/usr/local/lib/python3.7/runpy.py"", line 85 in _run_code
> > File ""/usr/local/lib/python3.7/runpy.py"", line 193 in _run_module_as_main
> > Segmentation fault (core dumped)

Same issue for me",hello trying apply fad facing fatal python error segmentation fault currently python stuck step compute python found problem fatal python error pipeline file list cause really expert apache could figure going also something wrong stack memory may cause segmentation fault problem tried increase stack operating system python process ended still working appreciate someone thank attached log python could load dynamic library open object file file directory ignore set machine missing pipeline option runner pipeline default runner fatal python error segmentation fault current thread recent call first file line decompress file line file line file line file line expand file line file line apply file line apply file line file line expand file line file line apply file line apply file line file line expand file line file line apply file line apply file line apply file line file line file line file line main file line file line run file line file line file line segmentation fault core issue,issue,negative,negative,neutral,neutral,negative,negative
893871732,"> I am also stacking on this step, when I fix the import problem. I got core dumped error as below... (I am using tf 2.5.0)
> Anyone can help?
> 
> > (fad-env) kp1@nipa2021-1226:~/google-research$ python -m frechet_audio_distance.create_embeddings_main --input_files test_audio/test_files_background.cvs --stats stats/background_stats
> > 2021-07-23 16:52:30.124584: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
> > 2021-07-23 16:52:30.124631: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
> > I0723 16:52:31.658765 140422069139200 pipeline.py:186] Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.
> > Fatal Python error: Segmentation fault
> > Current thread 0x00007fb68f8cb700 (most recent call first):
> > File ""/usr/local/lib/python3.7/bz2.py"", line 346 in decompress
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/internal/pickler.py"", line 277 in loads
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 857 in **init**
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/core.py"", line 1222 in **init**
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/core.py"", line 1539 in FlatMap
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/core.py"", line 1593 in Map
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/io/iobase.py"", line 895 in expand
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 215 in apply_PTransform
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 185 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 694 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 601 in **ror**
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/io/textio.py"", line 569 in expand
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 215 in apply_PTransform
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 185 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 694 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 651 in apply
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 601 in **ror**
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 1086 in **ror**
> > File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_beam.py"", line 353 in create_pipeline
> > File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_main.py"", line 69 in main
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/absl/app.py"", line 258 in _run_main
> > File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/absl/app.py"", line 312 in run
> > File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_main.py"", line 75 in 
> > File ""/usr/local/lib/python3.7/runpy.py"", line 85 in _run_code
> > File ""/usr/local/lib/python3.7/runpy.py"", line 193 in _run_module_as_main
> > Segmentation fault (core dumped)

Darn Im also starting to experience the same issue now, here's my logs:

2021-08-05 22:49:02.936735: F tensorflow/stream_executor/cuda/cuda_dnn.cc:533] Check failed: cudnnSetTensorNdDescriptor(handle_.get(), elem_type, nd, dims.data(), strides.data()) == CUDNN_STATUS_SUCCESS (9 vs. 0)batch_descriptor: {count: 5979 feature_map_count: 64 spatial: 96 64  value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX}
Fatal Python error: Aborted

Thread 0x00007ff65048b700 (most recent call first):
  File ""/opt/conda/lib/python3.8/threading.py"", line 870 in run
  File ""/opt/conda/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""/opt/conda/lib/python3.8/threading.py"", line 890 in _bootstrap

Thread 0x00007ff64dc8a700 (most recent call first):
  File ""/opt/conda/lib/python3.8/threading.py"", line 306 in wait
  File ""/opt/conda/lib/python3.8/threading.py"", line 558 in wait
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/data_plane.py"", line 207 in run
  File ""/opt/conda/lib/python3.8/threading.py"", line 932 in _bootstrap_inner
  File ""/opt/conda/lib/python3.8/threading.py"", line 890 in _bootstrap

Thread 0x00007ff674084740 (most recent call first):
  File ""/opt/conda/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1441 in _call_tf_sessionrun
  File ""/opt/conda/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1349 in _run_fn
  File ""/opt/conda/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1365 in _do_call
  File ""/opt/conda/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1358 in _do_run
  File ""/opt/conda/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1180 in _run
  File ""/opt/conda/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 957 in run
  File ""/home/jovyan/scratch-01/rs002/tt-vae-gan/fad/frechet_audio_distance/audioset_model.py"", line 69 in process_batch
  File ""/home/jovyan/scratch-01/rs002/tt-vae-gan/fad/frechet_audio_distance/create_embeddings_beam.py"", line 247 in _flush_buffer
  File ""/home/jovyan/scratch-01/rs002/tt-vae-gan/fad/frechet_audio_distance/create_embeddings_beam.py"", line 283 in finish_bundle
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 999 in process_bundle
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 639 in process_bundle
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 601 in do_instruction
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py"", line 378 in push
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 905 in process_bundle
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 603 in _run_bundle
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 557 in _run_stage
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 366 in run_stages
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 202 in run_via_runner_api
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 191 in run_pipeline
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/runners/direct/direct_runner.py"", line 131 in run_pipeline
  File ""/opt/conda/lib/python3.8/site-packages/apache_beam/pipeline.py"", line 564 in run
  File ""/home/jovyan/scratch-01/rs002/tt-vae-gan/fad/frechet_audio_distance/create_embeddings_main.py"", line 70 in main
  File ""/opt/conda/lib/python3.8/site-packages/absl/app.py"", line 251 in _run_main
  File ""/opt/conda/lib/python3.8/site-packages/absl/app.py"", line 303 in run
  File ""/home/jovyan/scratch-01/rs002/tt-vae-gan/fad/frechet_audio_distance/create_embeddings_main.py"", line 75 in <module>
  File ""/opt/conda/lib/python3.8/runpy.py"", line 87 in _run_code
  File ""/opt/conda/lib/python3.8/runpy.py"", line 194 in _run_module_as_main

",also step fix import problem got core error anyone help python could load dynamic library open object file file directory ignore set machine missing pipeline option runner pipeline default runner fatal python error segmentation fault current thread recent call first file line decompress file line file line file line file line file line map file line expand file line file line apply file line apply file line file line expand file line file line apply file line apply file line apply file line file line file line file line main file line file line run file line file line file line segmentation fault core darn also starting experience issue check count spatial layout fatal python error aborted thread recent call first file line run file line file line thread recent call first file line wait file line wait file line run file line file line thread recent call first file line file line file line file line file line file line run file line file line file line file line file line file line file line push file line file line file line file line file line file line file line file line run file line main file line file line run file line module file line file line,issue,negative,positive,neutral,neutral,positive,positive
893733317,"Reading further the code I saw that it is used to generate more 2D keypoints. However, changing the code to only use 2D input gives issues in some parts such as the triplet loss calculation, how can I adapt the code to use 2D keypoints in that part? Because using only 2D keypoints I don't have anchor_keypoints_3d, positive_keypoints_3d ot the masks.

` triplet_loss, triplet_loss_summaries = (
                             loss_utils.compute_keypoint_triplet_losses(
                                                 anchor_embeddings=triplet_anchor_embeddings,
                                                 positive_embeddings=triplet_positive_embeddings,
                                                 match_embeddings=triplet_positive_embeddings,
                                                 anchor_keypoints=anchor_keypoints_3d,
                                                 match_keypoints=positive_keypoints_3d,
                                                 margin=FLAGS.triplet_loss_margin,
                                                 min_negative_keypoint_distance=(
                                                     configs['min_negative_keypoint_distance']),
                                                 use_semi_hard=FLAGS.use_semi_hard_triplet_negatives,
                                                 exclude_inactive_triplet_loss=(
                                                     FLAGS.exclude_inactive_triplet_loss),
                                                 anchor_keypoint_masks=anchor_keypoint_masks_3d,
                                                 match_keypoint_masks=positive_keypoint_masks_3d,
                                                 embedding_sample_distance_fn=(
                                                     configs['triplet_embedding_sample_distance_fn']),
                                                 keypoint_distance_fn=configs['keypoint_distance_fn'],
                                                 anchor_mining_embeddings=triplet_anchor_mining_embeddings,
                                                 positive_mining_embeddings=triplet_positive_mining_embeddings,
                                                 match_mining_embeddings=triplet_positive_mining_embeddings,
                                                 summarize_percentiles=FLAGS.summarize_percentiles
                                                 )
                             )`",reading code saw used generate however code use input triplet loss calculation adapt code use part,issue,negative,neutral,neutral,neutral,neutral,neutral
890693410,"I think I found a solution.  I missed the line below.

>For the tall variant, we also used the GLU variant described in [Shazeer, 2020] which is commonly referred to as the V1.1 variant in the T5 library

 So, I added lines below.
```
DenseReluDense.activation = [""gelu"", ""linear""]
dropout_rate = 0.0
Unitransformer.shared_embedding_and_softmax_weights = False
```

Then I got 134M params.
```
INFO:tensorflow:Trainable Variables            count: 240     Total size: 133860352        Total slice_size: 133860352      
INFO:tensorflow:All Variables                  count: 253     Total size: 134280960        Total slice_size: 134280960    
```",think found solution line tall variant also used variant commonly variant library added linear false got trainable count total size total count total size total,issue,negative,negative,negative,negative,negative,negative
889909716,"Ya sure...

You can contact me on WhatsApp: +91 9833668436

Or

if you prefer any other communication medium than just let me know.

On Tue, 27 Jul 2021, 10:40 Ed0flower93, ***@***.***> wrote:

> Hello,
>
> I'm following your same goal, but I'm having trouble using waymo datasets.
> Can i contact you, please?
>
> Thank you
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/693#issuecomment-887326341>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AT7WVHLUINUWSZACOWQHY2LTZZWI5ANCNFSM44L2C6VA>
> .
>
",ya sure contact prefer communication medium let know tue wrote hello following goal trouble contact please thank thread reply directly view,issue,positive,positive,neutral,neutral,positive,positive
887326341,"Hello,

I'm following your same goal, but I'm having trouble using waymo datasets.
Can i contact you, please?

Thank you ",hello following goal trouble contact please thank,issue,negative,negative,neutral,neutral,negative,negative
886419682,"> Hello,
> I am trying to apply FAD but facing the Fatal Python error: Segmentation fault here.
> Currently I am using tf 2.5.0 and apache-beam 2.24.0 with python 3.7.
> 
> I am stuck on the step > **Compute embeddings and eastimate multivariate Gaussians**
> `python -m frechet_audio_distance.create_embeddings_main --input_files test_audio/test_files_background.cvs --stats stats/background_stats`
> 
> I found the problem **Fatal Python error** happened in `def create_pipeline()` of `create_embeddings_beam.py` :
> 
> > if files_input_list:
> > examples = (
> > pipeline
> > | 'Read File List' >> ReadFromText(files_input_list)
> > | 'Read Files' >> beam.ParDo(ReadWavFiles()))
> 
> Cause I am really not an expert using apache so couldn't figure it out what's going on.
> 
> Also by googling it, it seems something wrong with stack memory which may cause the **Segmentation fault** problem,
> so I have tried to increase the stack that my operating system allocates for the python process ended by `ulimit -s 262140`.
> But it's still not working.
> 
> I will be appreciate if someone has any ideas.
> Thank you!!

I attached my log here:
> (fad-env) kp1@nipa2021-1226:~/google-research$ python -m frechet_audio_distance.create_embeddings_main --input_files test_audio/test_files_background.cvs --stats stats/background_stats  2021-07-26 15:34:57.534827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-07-26 15:34:57.534877: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
I0726 15:34:58.903872 140039776012032 pipeline.py:190] Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.
Fatal Python error: Segmentation fault
Current thread 0x00007f5d8d1a8700 (most recent call first):
  File ""/usr/local/lib/python3.7/bz2.py"", line 346 in decompress
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/internal/pickler.py"", line 274 in loads
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 787 in __init__
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/core.py"", line 1266 in __init__
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/io/iobase.py"", line 1544 in expand
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 228 in apply_PTransform
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 198 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 664 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pvalue.py"", line 141 in __or__
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/io/iobase.py"", line 892 in expand
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 228 in apply_PTransform
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 198 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 664 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 561 in __ror__
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/io/textio.py"", line 564 in expand
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 228 in apply_PTransform
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 198 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 664 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 621 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 561 in __ror__
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 997 in __ror__
  File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_beam.py"", line 359 in create_pipeline
  File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_main.py"", line 69 in main
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/absl/app.py"", line 258 in _run_main
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/absl/app.py"", line 312 in run
  File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_main.py"", line 75 in <module>
  File ""/usr/local/lib/python3.7/runpy.py"", line 85 in _run_code
  File ""/usr/local/lib/python3.7/runpy.py"", line 193 in _run_module_as_main
Segmentation fault (core dumped)

",hello trying apply fad facing fatal python error segmentation fault currently python stuck step compute python found problem fatal python error pipeline file list cause really expert apache could figure going also something wrong stack memory may cause segmentation fault problem tried increase stack operating system python process ended still working appreciate someone thank attached log python could load dynamic library open object file file directory ignore set machine missing pipeline option runner pipeline default runner fatal python error segmentation fault current thread recent call first file line decompress file line file line file line file line expand file line file line apply file line apply file line file line expand file line file line apply file line apply file line file line expand file line file line apply file line apply file line apply file line file line file line file line main file line file line run file line module file line file line segmentation fault core,issue,negative,negative,neutral,neutral,negative,negative
885466312,"I am also stacking on this step, when I fix the import problem. I got core dumped error as below... (I am using tf 2.5.0)
Anyone can help?

> (fad-env) kp1@nipa2021-1226:~/google-research$ python -m frechet_audio_distance.create_embeddings_main --input_files test_audio/test_files_background.cvs --stats stats/background_stats
2021-07-23 16:52:30.124584: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-07-23 16:52:30.124631: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
I0723 16:52:31.658765 140422069139200 pipeline.py:186] Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.
Fatal Python error: Segmentation fault
Current thread 0x00007fb68f8cb700 (most recent call first):
  File ""/usr/local/lib/python3.7/bz2.py"", line 346 in decompress
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/internal/pickler.py"", line 277 in loads
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 857 in __init__
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/core.py"", line 1222 in __init__
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/core.py"", line 1539 in FlatMap
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/core.py"", line 1593 in Map
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/io/iobase.py"", line 895 in expand
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 215 in apply_PTransform
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 185 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 694 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 601 in __ror__
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/io/textio.py"", line 569 in expand
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 215 in apply_PTransform
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/runners/runner.py"", line 185 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 694 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 651 in apply
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 601 in __ror__
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/apache_beam/transforms/ptransform.py"", line 1086 in __ror__
  File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_beam.py"", line 353 in create_pipeline
  File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_main.py"", line 69 in main
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/absl/app.py"", line 258 in _run_main
  File ""/home/kp1/google-research/fad-env/lib/python3.7/site-packages/absl/app.py"", line 312 in run
  File ""/home/kp1/google-research/frechet_audio_distance/create_embeddings_main.py"", line 75 in <module>
  File ""/usr/local/lib/python3.7/runpy.py"", line 85 in _run_code
  File ""/usr/local/lib/python3.7/runpy.py"", line 193 in _run_module_as_main
Segmentation fault (core dumped)",also step fix import problem got core error anyone help python could load dynamic library open object file file directory ignore set machine missing pipeline option runner pipeline default runner fatal python error segmentation fault current thread recent call first file line decompress file line file line file line file line file line map file line expand file line file line apply file line apply file line file line expand file line file line apply file line apply file line apply file line file line file line file line main file line file line run file line module file line file line segmentation fault core,issue,negative,positive,neutral,neutral,positive,positive
884688405,"Have you tried it with one GPU? What is the minimum batch size, you can fit?",tried one minimum batch size fit,issue,negative,positive,positive,positive,positive,positive
880202469,"@vsippola how do you downgrade bazel? tried brew install bazel@3.4.1, and it didn't work",downgrade tried brew install work,issue,negative,neutral,neutral,neutral,neutral,neutral
880070179,"Updated the WORKSPACE with ""main"" instead of ""master"" only for rules_cc and rules_proto and now it's back to normal (in my situation for automl_zero)

http_archive(
    name = ""rules_cc"",
    strip_prefix = ""rules_cc-main"",
    urls = [""https://github.com/bazelbuild/rules_cc/archive/main.zip""],
)

http_archive(
    name = ""rules_proto"",
    strip_prefix = ""rules_proto-main"",
    urls = [
        ""https://github.com/bazelbuild/rules_proto/archive/main.zip"",
    ],
)
",main instead master back normal situation name name,issue,negative,positive,positive,positive,positive,positive
879759042,"One (untested) code snippet to try would be:

```
# output is the output of the spatial upsampler.
# gray is the high resolution grayscale iamge
output = tf.expand_dims(output, axis=0)
resized = tf.image.resize(output, size=(new_h, new_w), preserve_aspect_ratio=True)
resized_yuv = tf.image.rgb_to_yuv(resized)
y, u, v = tf.unstack(resized_yuv, axis=-1)

high_res = tf.stack([gray, u, v], axis=-1)
```",one untested code snippet try would output output spatial gray high resolution output output output gray,issue,negative,positive,positive,positive,positive,positive
879711618,"@MechCoder Gladly. I would like to make it work with image of any size, not just square and length as power of 2.",gladly would like make work image size square length power,issue,positive,positive,positive,positive,positive,positive
879666428,"@mhoangvslev Thanks for your work! Would you like your colab to be part of the repository?

In our paper and in the released checkpoints the final output resolution is 256x256.

Even higher resolution output can be obtained by some tricks. In Y Cb Cr space, you could upsample the 256x256 Cb and Cr channels using standard tf.image.resize methods and combine them with the high resolution 512x512 Y channel. ",thanks work would like part repository paper final output resolution even higher resolution output space could standard combine high resolution channel,issue,positive,positive,positive,positive,positive,positive
879524991,"Hello, how about setting the shape of the input Tensor to the None?",hello setting shape input tensor none,issue,negative,neutral,neutral,neutral,neutral,neutral
879386627,"It looks like `master.zip` has been renamed to `main.zip`. Changing that in `WORKSPACE` however, doesn't fix the problem as there seem to be other references with the old naming.",like however fix problem seem old naming,issue,negative,positive,neutral,neutral,positive,positive
877782078,"> I added support for training and finetuneing on custom datasets. You can set the FLAGS `dataset=custom` and `data_dir` to be a directory containing RGB images.
> 
> See: [db430ca](https://github.com/google-research/google-research/commit/db430ca3f9575e05245898880a95889e8025a92e)
> 
> If you are using the same resolutions as in our paper, finetuneing is recommended. You can set `pretrain_dir` to be a pretrained ImageNet (colorizer/upsampler) checkpoint.

Thank you ",added support training custom set directory see paper set thank,issue,positive,neutral,neutral,neutral,neutral,neutral
877772800,"I added support for training and finetuneing on custom datasets. You can set the FLAGS dataset=custom and data_dir to be a directory containing RGB images.

See: db430ca

If you are using the same resolutions as in our paper, finetuneing is recommended. You can set pretrain_dir to be a pretrained ImageNet (colorizer/upsampler) checkpoint.",added support training custom set directory see paper set,issue,negative,neutral,neutral,neutral,neutral,neutral
875729117,"Thanks

> You can find `mel_features.py` in `tensorflow_models/audioset/vggish/`. To fix this and oother import problems just create `tensorflow_models/audioset/__init__.py` and add the line: `from .vggish import mel_features, vggish_params, vggish_slim`
> Also, in file `tensorflow_models/audioset/vggish/vggish_slim.py` you should change the import to `from . import vggish_params as params`.
> 
> You will also have problems with Tensorflow version: for me it worked r1.15 and changing in file `frechet_audio_distance/fad_utils.py` line 37 to `tf_record = tf.python_io.tf_record_iterator(filename).__next__()`

Really helped! Thanks",thanks find fix import create add line import also file change import import also version worked file line really thanks,issue,positive,positive,positive,positive,positive,positive
874341620,"We didn't work with the SMPL model in our paper but you can either match up the joint names with an existing one in our keypoint profiles (https://github.com/google-research/google-research/blob/master/poem/core/keypoint_profiles.py) or create a new keypoint profile. The function ""compute_procrustes_aligned_mpjpes"" should work on general points, but let us know if you have any issues with it.",work model paper either match joint one create new profile function work general let u know,issue,negative,positive,neutral,neutral,positive,positive
873425350,"Hi Jinwoo @jw9730! Thanks for the clear explanations!
Applied to Q: Line 102 makes Q', but when calculating attention later, Q' is also in the denominator, so it doesn't affect the result.
BTW, I think line 106 also not affecting the results. We are using the maximum value of all elements(specifically, along `axis=last_dims_t + attention_dims_t`) in the `data_dash` tensor.  And K' is also in the denominator(Q'((K')T1)) when calculating attention later. 
Am I mistaken?",hi thanks clear applied line calculating attention later also denominator affect result think line also affecting maximum value specifically along tensor also denominator calculating attention later mistaken,issue,positive,positive,neutral,neutral,positive,positive
873026645,"I think it is for numerical stability, as query and key features can overflow with exp().

Line 102 (applied to Q) is equivalent to dividing each element of query (Q') and normalizer (Q'((K')T1)) with a same scalar, so shouldn't affect the result.
Line 106 (applied to K) is an often-used trick for stable normalized exp (see [this](https://stackoverflow.com/questions/23665599/numerically-stable-implementation)).",think numerical stability query key overflow line applied equivalent dividing element query normalizer scalar affect result line applied trick stable see,issue,negative,neutral,neutral,neutral,neutral,neutral
870692015,"Yes I am using ground truth keypoints from 3DHP. I am taking the mean of all the 20 embeddings for each pose before calculating the euclidean distance. Considering all the 110 pairs, I am getting Hit@1 = 35.14, Hit@10 = 69.87 and Hit@20 = 78.57. I am using NP-MPJPE to calculate the matching function for 3D poses. 


Also, how do I use the 3D coordinates provided by SMPL. I want to evaluate on 3D poses in the wild dataset. Their dataset provides 2D keypoints and 3D joint coordinates. But they provide 24 joint coordinates and I don't know which coordinate corresponds to which joint. 

Also, is it fine to use the function ""compute_procrustes_aligned_mpjpes"" to compute the distance between 3D annotations from SMPL ? ",yes ground truth taking mean pose calculating distance considering getting hit hit hit calculate matching function also use provided want evaluate wild joint provide joint know joint also fine use function compute distance,issue,positive,positive,neutral,neutral,positive,positive
870214053,"No, the results in the paper are from the PersonLab pose estimator (see Section 3.6 of our paper: https://arxiv.org/pdf/1912.01001.pdf). 

For publicly available pose estimators, HRNet is a good choice (https://github.com/HRNet/HRNet-Human-Pose-Estimation) although we have not tested our model with HRNet. 

The retrieval results are probably good because you are using Hit@10 and ground truth keypoints from 3DHP it sounds like. What are you getting for Hit@1? We found that results are better when we use ground truth keypoints (described in our paper) and Hit@10 will always be better than Hit@1.",paper pose estimator see section paper publicly available pose good choice although tested model retrieval probably good hit ground truth like getting hit found better use ground truth paper hit always better hit,issue,positive,positive,positive,positive,positive,positive
869298557,how much time does it usually take to get reviewed?,much time usually take get,issue,negative,negative,neutral,neutral,negative,negative
868497172,"I am using the keypoints provided by 3DHP. And I am using the pre-trained model from the website which is from CPN keypoints. So, the results won't match with the paper ?
In order to match the results with the paper, are you suggesting the I should use https://github.com/leoxiaobin/deep-high-resolution-net.pytorch for getting the keypoints on the 3DHP images ?

And also by taking mean you are saying that taking the mean of 20 embeddings for each sample right ?

Using the frames in ""3dhp-train-C0.txt"" and ""3dhp-train-C1.txt"" and using all the 20 embeddings for all the frames, I calculated top 10 nearest neighbours for each frame in ""3dhp-train-C0.txt"" and using the 3D annotations provided by 3DHP I calculated the Hit@10 value. For the pair C0 and C1, I am getting the value 96.7, I think it is very high, is there something I am missing. I am using the following code to calculate:

import time
import numpy as np 
import pandas as pd
import tensorflow as tf
import sys
from multiprocessing import Pool
from sklearn.metrics import pairwise_distances as pwd
sys.path.append(""../"")
from poem.core.keypoint_utils import compute_procrustes_aligned_mpjpes

K = 10

def calcDistance(data):
	a = samples0[data[0]].reshape(20, 16)
	b = samples1[data[1]].reshape(20, 16)
	distance = pwd(a, b)
	return distance.mean()

if __name__ == ""__main__"":
	samples0 = np.array(pd.read_csv(""./c0/unnormalized_embedding_samples.csv"", header=None))
	samples1 = np.array(pd.read_csv(""./c1/unnormalized_embedding_samples.csv"", header=None))

	indices = []
	for i in range(len(samples0)):
		for j in range(len(samples1)):
			indices.append((i, j))

	start = time.time()

	with Pool(10) as p:
		res = p.map(calcDistance, indices)

	scores = np.array(res).reshape(len(samples0), len(samples1))
	indices = np.argsort(scores, axis=1)
	top_k = indices[:, :K]

	s0_3d = np.load(""/home/akshay/poem/pr_vipe/eval/frames/3dhp/3dhp-train-C0.3D.npy"")
	s1_3d = np.load(""/home/akshay/poem/pr_vipe/eval/frames/3dhp/3dhp-train-C1.3D.npy"")

	query = s0_3d
	keys = s1_3d[top_k]

	hit = 0

	for i in range(len(query)):
		flag = 0
		for j in range(len(keys[i])):
			q = tf.convert_to_tensor(query[i])
			k = tf.convert_to_tensor(keys[i][j])
			score = compute_procrustes_aligned_mpjpes(k, q).numpy()
			if score < 0.1:
				flag = 1
				break
		if flag:
			hit += 1

	print(""Hit Ratio"", 100*hit/len(query))

",provided model wo match paper order match paper suggesting use getting also taking mean saying taking mean sample right calculated top nearest frame provided calculated hit value pair getting value think high something missing following code calculate import time import import import import import pool import import data data data distance return index range range start pool index index index query hit range query flag range query score score flag break flag hit print hit ratio query,issue,positive,positive,neutral,neutral,positive,positive
868152250,"Hmm I think the easiest solution for quick tests for now is to just take the mean (to reduce the computation by 400x). Just want to highlight the first point in the previous comment again, that the pre-trained model on the website is from CPN keypoints fine-tuned on Human3.6M, so probably won't work well for 3DHP. ",think easiest solution quick take mean reduce computation want highlight first point previous comment model probably wo work well,issue,positive,positive,neutral,neutral,positive,positive
867965077,"I am not using loops to calculate distance, since the distance has to be calculated by considering all the 20 samples therefore I had to modify KNNs distance function, I have written the below code:

def calcDistance(x, y):
	a = x.reshape(20, 16)
	b = y.reshape(20, 16)
	distance = pwd(a, b)
	return distance.mean()

samples0 = np.array(pd.read_csv(""./c0/unnormalized_embedding_samples.csv"", header=None))
samples1 = np.array(pd.read_csv(""./c1/unnormalized_embedding_samples.csv"", header=None))
y = np.ones(len(samples0))

start = time.time()
neigh = KNeighborsClassifier(n_neighbors=5, metric=calcDistance, n_jobs=20)
neigh.fit(samples1, y)
knn = neigh.kneighbors(samples0, 5, return_distance=False)

This code takes almost 2 Hrs to produce output. I have used embeddings from camera view C1 to fir the KNN and then I have used it to predict the nearest K neighbours of embedding in C0. K is 5. The inputs ""sample0"" and ""sample1"" are the outputs of the inference script and they are of the shape -> [6842, 320] hence in order to calculate the distance I have reshaped it. 

I wrote another code for the same thing that takes less time than the above one but still it takes an hour to give the nearest neighbours. The code is as follows:

def calcDistance(data):
	a = samples0[data[0]].reshape(20, 16)
	b = samples1[data[1]].reshape(20, 16)
	distance = pwd(a, b)
	return distance.mean()

if __name__ == ""__main__"":
	samples0 = np.array(pd.read_csv(""./c0/unnormalized_embedding_samples.csv"", header=None))
	samples1 = np.array(pd.read_csv(""./c1/unnormalized_embedding_samples.csv"", header=None))

	indices = []
	for i in range(len(samples0)):
		for j in range(len(samples1)):
			indices.append((i, j))

	with Pool(20) as p:
		res = p.map(calcDistance, indices)

	scores = np.array(res).reshape(len(samples0), len(samples1))
	indices = np.argsort(scores, axis=1)

I also saw the blogpost that you have mentioned but there is one issue with that. Their library do not allow to define custom distance computation function hence in this case where there are 2 matrices of shape [6842, 320], then the output will be the euclidean distance between the vectors of dimension [320], but that is not the case here. 
Is there any other approach for speeding up the calculation, an hour for single pair is a lot and there are a total of 110 pairs.
",calculate distance since distance calculated considering therefore modify distance function written code distance return start neigh code almost produce output used camera view fir used predict nearest sample sample inference script shape hence order calculate distance wrote another code thing le time one still hour give nearest code data data data distance return index range range pool index index also saw one issue library allow define custom distance computation function hence case matrix shape output distance dimension case approach speeding calculation hour single pair lot total,issue,negative,negative,neutral,neutral,negative,negative
867900650,"Hi! We just want to note that the pre-trained model on the website is from CPN keypoints **fine-tuned on Human3.6M**, and is likely to not work as well for other datasets. We would recommend using a more robust pose estimator such as HRNet (https://github.com/leoxiaobin/deep-high-resolution-net.pytorch) if you would like to test across more datasets. 

Are you doing a lot of loops in your distance calculation? It will be faster if it is vectorized. Also, I just saw this blogpost https://towardsdatascience.com/make-knn-300-times-faster-than-scikit-learns-in-20-lines-5e29d74e76bb that says this KNN computation is faster compared to sklearn. However, I have not tried out Faiss before so cannot comment on whether that will be the case. ",hi want note model likely work well would recommend robust pose estimator would like test across lot distance calculation faster also saw computation faster however tried comment whether case,issue,positive,neutral,neutral,neutral,neutral,neutral
867650332,"Hey, for your first question here is an alternative solution :
You need to change the line 116 on uflow_data.py file to the following : elif 'chairs' in data_format or 'custom' in data_format:
And then use the command provided in the readme to train your network.

For the second one I think you have an input shape issue. I think it's related to the way you converted your video to a dataset. I don't know if you have noticed that you should change line 81 in uflow/misc/convert_video_to_dataset.py to the following : 
convert_video(FLAGS.video_path, FLAGS.output_path). 

Also I don't understand why you have a list of two tensors as input, normally you should have a single tensor with shape (batch_size, 2, height, width, 3).

If you can share more I would be glad to help as much as I can.",hey first question alternative solution need change line file following use command provided train network second one think input shape issue think related way converted video know change line following also understand list two input normally single tensor shape height width share would glad help much,issue,positive,positive,positive,positive,positive,positive
867604538,"@lazrak-mouad While training using the chairs as the train_on flag im getting this error, any help appreciated 

Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: [<tf.Tensor 'pwc_flow/mul_2000:0' shape
=(1, 96, 128, 32) dtype=float32>, <tf.Tensor 'pwc_flow/add_88:0' shape=(1, 96, 128, 2) dtype=float32>] 

@AustinCStone ",training flag getting error help sequential model single input tensor receive class input shape,issue,negative,negative,neutral,neutral,negative,negative
867602249,"@lazrak-mouad Im unable to train on custom dataset with either train_on flag as custom or chairs, created the TF records using the convert_video_to_dataset.py,. Can you please let me know how you trained on your custom dataset",unable train custom either flag custom please let know trained custom,issue,negative,negative,negative,negative,negative,negative
866480928,"One more thing, why is there an extra `--` on [this line](https://github.com/google-research/google-research/blob/a6fb1bd26b296510134f6f430a6e74743469ccb7/coltran/custom_colorize.py#L33)  (and others steps too)?
```python
python -m coltran.custom_colorize -- --config=configs/colorizer.py \
``` 
For me, it gave some errors, which were resolved after removing them.",one thing extra line python python gave resolved removing,issue,negative,neutral,neutral,neutral,neutral,neutral
866175973,"Found my Mistake
I did not specify the checkpoints in the command properly. 
I used [this](https://github.com/google-research/google-research/blob/a6fb1bd26b296510134f6f430a6e74743469ccb7/coltran/custom_colorize.py#L23-L35), but did not enter step specific checkpoint directory.

The correct command should include 

`--logdir=coltran/checkpoints/coltran/colorizer`
`--logdir=coltran/checkpoints/coltran/color_upsampler`
`--logdir=coltran/checkpoints/coltran/spatial_upsampler`
for each of the different steps.

I earlier had
```python
LOGDIR = 'coltran/checkpoints/coltran'
# and in command
--logdir=$LOGDIR
```
and it showed 0 training steps.

Yes, TF version is 2.5.
![image](https://user-images.githubusercontent.com/42700922/122968500-3b22c400-d3a9-11eb-88ff-7173c90f07a3.png)

I will check with the custom dataset train part.
Thanks.


",found mistake specify command properly used enter step specific directory correct command include different python command training yes version image check custom train part thanks,issue,negative,positive,neutral,neutral,positive,positive
865283247,"@ddemszky This is rather a preliminary question: After getting the tags and making all of what you mentioned. Do I add the <person> and <religion> tokens as using ``` tokenizer.add_tokens([""NEW_TOKEN""]) ``` as discussed in here ( https://github.com/huggingface/transformers/issues/1413 ) , then train the model, or are there any pitfalls before getting to training?

 Thank you again for your time and effort ^^",rather preliminary question getting making add person religion train model getting training thank time effort,issue,negative,neutral,neutral,neutral,neutral,neutral
865212005,"An update regarding the masked tokens: we won't be able to release that list for now unfortunately, but you can recreate the tokens based on the Reddit comment ids. You can use diff to compare the comments in the dataset to the raw comments and you should be able to recreate those lists!",update regarding masked wo able release list unfortunately recreate based comment use compare raw able recreate,issue,negative,positive,neutral,neutral,positive,positive
864767631,"@ketan-lambat Could you verify that your TensorFlow version is indeed 2.5? @hufeng0912 reports the TF version of their version to be 1.1.0. 

Could you also verify that your checkpoints are restored correctly?, i.e the output of this line? (https://github.com/google-research/google-research/blob/master/coltran/custom_colorize.py#L220).

It should be possible to train/finetune on a custom dataset using functions over  https://github.com/google-research/google-research/blob/master/coltran/custom_colorize.py#L220 and https://github.com/google-research/google-research/blob/master/coltran/datasets.py but I did not have the time to pack them into a script.

",could verify version indeed version version could also verify correctly output line possible custom time pack script,issue,negative,neutral,neutral,neutral,neutral,neutral
864692900,"Can someone please provide me with a link to install Tensorflow version greater than 2.6.
I get this error (see attached img) when installing packages in Google Colab notebook.
With the default TF 2.5, I got a similar noise output as mentioned above.

@hufeng0912  Can you please let me know how and where did you run this code, (https://github.com/google-research/google-research/issues/710#issuecomment-846851319), particularly with the TensorFlow version.

@MechCoder Is it possible to train ColTran on a custom dataset?

![image](https://user-images.githubusercontent.com/42700922/122702000-1ecd3d00-d26c-11eb-8d9d-5b0810661d16.png)
",someone please provide link install version greater get error see attached notebook default got similar noise output please let know run code particularly version possible train custom image,issue,positive,positive,positive,positive,positive,positive
864615787,"> Thanks for these questions! For NER, we used the model described in the paper by [Tsai et al. (2019)](https://arxiv.org/abs/1909.00100). We used an internal version of it, but I found a public-facing Github [repository](https://github.com/DomHudson/bert-in-production), which seems to include this model, so you might find it useful. We'll get back to you regarding the other masked tokens. Hope this helps!

Thank you ^^ I'll check it out and hopefully get back to you.",thanks used model paper al used internal version found repository include model might find useful get back regarding masked hope thank check hopefully get back,issue,positive,positive,neutral,neutral,positive,positive
864611671,"Thanks for these questions! For NER, we used the model described in the paper by [Tsai et al. (2019)](https://arxiv.org/abs/1909.00100). We used an internal version of it, but I found a public-facing Github [repository](https://github.com/DomHudson/bert-in-production), which seems to include this model, so you might find it useful. We'll get back to you regarding the other masked tokens. Hope this helps!",thanks used model paper al used internal version found repository include model might find useful get back regarding masked hope,issue,positive,positive,positive,positive,positive,positive
864607713,"@jenjsun Thank you for your reply. I got the pre-trained model from the website. I am now trying to match the results on 3DHP dataset. I have created the CSVs using the 11 cameras and the frames as mentioned in the repository. The pre-trained model gives 20 samples for each row. How do I calculate Hit@K value ? 
From what I understand, I first select a pair of camera. For the 3DHP dataset, for each camera view 6824 frames are listed, for all the 6824 frames I get the 20 samples using the infer.py script.  The script outputs the samples which have the shape - [6824, 320]. Now, I am considering the pair of cameras as C0 and C1, for both camera pairs I have the samples of size [6824, 320]. I am using sklearn's KNN classifier for calculating the distance between the two, but I have to change the metric because first [6824, 320] needs to be converted to [6824, 20, 16] and then the distance is calculated between all the 400 pair of samples and final result is averaged out. But isn't this computationally expensive because both the views have the shape 6824 which means that KNN has to compute the distances 6824 * 6824 times ? 
My code is taking a lot of time for a single pair, how can I speed it up ?

 ",thank reply got model trying match repository model row calculate hit value understand first select pair camera camera view listed get script script shape considering pair camera size classifier calculating distance two change metric first need converted distance calculated pair final result expensive shape compute time code taking lot time single pair speed,issue,positive,negative,neutral,neutral,negative,negative
864555589,"Nope, also asked for the BERT NER model directly to the authors no answer there as well. 
I've used my own list of religions to tag the religions and another NER model to tag the names ¯\_(ツ)_/¯",nope also model directly answer well used list tag another model tag,issue,negative,positive,neutral,neutral,positive,positive
864471234,"@vtankovich  Sir your paper was really good, is there any source code for your model, even I want to reproduce the same kind of results. but I am not able to code the architecture. It would be of great help if you could share your code for training the model. 


Thank you",sir paper really good source code model even want reproduce kind able code architecture would great help could share code training model thank,issue,positive,positive,positive,positive,positive,positive
864405639,"> You need to unzip the checkpoints (coltran_cotran.zip) into a separate directory.
> 
> And then set logdir to be the path to the colorizer subdirectory.

Thank You, I will try to do so!
The given solutin worked it our perfectly. Thanks once again!",need separate directory set path thank try given worked perfectly thanks,issue,positive,positive,positive,positive,positive,positive
864405100,"You need to unzip the checkpoints (coltran_cotran.zip) into a separate directory.

And then set logdir to be the path to the colorizer subdirectory.",need separate directory set path,issue,negative,neutral,neutral,neutral,neutral,neutral
863501473,"Hi! We looked into this and will upload a new table that fits the description. We want to note that this testdata file is not real data - it is just for you to try running the script. You shouldn't use the provided testdata in any evaluations.

The index order depends on the keypoint profile, and a list of all the keypoint profiles with names of each index is in: https://github.com/google-research/google-research/blob/master/poem/core/keypoint_profiles.py (Note the keypoint_names field).

We released a pretrained model based on CPN keypoints on our website: https://sites.google.com/view/pr-vipe. There's also a data processing script on our website to generate the tfrecord used by our model. You can substitute the input files with your own keypoints. See https://github.com/google-research/google-research/blob/master/poem/core/keypoint_utils.py for some normalization functions that we used for our work.",hi new table description want note file real data try running script use provided index order profile list index note field model based also data script generate used model substitute input see normalization used work,issue,negative,positive,positive,positive,positive,positive
860232109,"Hi @sabarinathh, do you mind sharing your predictions output after using the trained weight?
It does seem to me that the pretrained SMITH checkpoint is not loaded properly into the script, I am getting 0.8 to 1 for all predicted scores, and only class 1 as the predicted class.

Does the fix above give you a reasonable performance for pretrained model?
",hi mind output trained weight seem smith loaded properly script getting class class fix give reasonable performance model,issue,negative,positive,neutral,neutral,positive,positive
860169285,"Thanks I will try this. In the infer.py script it is written that 
""Note: The input 2D keypoint coordinate values are required to be normalized by
image sizes to within [0, 1].""
So I was running the inference on the testdata - tfe-2.tfrecords, when I load this data I found the 2D keypoints to have negative values, for exapmle:
[ 0.        ,  0.        ],
[ 0.07026762, -0.71297413],
[ 0.06136627, -0.5364032 ],
[-0.01427505, -0.5316725 ],
[ 0.01604421, -0.5012069 ],
[-0.00570184, -0.4617674 ],
[ 0.14517584, -0.26618955],
[ 0.05502375, -0.16674794],
[ 0.1798002 , -0.18768266],
[ 0.24993953, -0.11369239],
[-0.02704499, -0.23463024],
[ 0.04197613, -0.01131284],
[-0.02284318,  0.00342976]]
This contains negative values.
In the pipeline_utils.py script it is mentioned that the data is assumed to be normalized, so the above values are normalized but they do not fall in the range [0, 1]. How can I normalize the data ?

Also in this 13 * 2 keypoint tensor, which index represents which keypoint, does the 0th index represent nose tip x and y ?
Why does the first coordinate [0, 0] for all the poses ? Also, I plotted these coordinates using a scatter plot but they do not resemble a valid pose. (While plotting I assumed that first index corresponds to nose tip, second corresponds to left shoulder and so on). 
I want to evaluate on some other dataset. What steps do I have to follow ?  As per the paper, I am using PoseNet to get the 2D keypoints for the images, but how should I normalise them and use the normalised keypoints as input to the model. 
Is there any pretrained model available that I can download and do Inference ?

",thanks try script written note input image size within running inference load data found negative negative script data assumed fall range normalize data also tensor index th index represent nose tip first also plotted scatter plot resemble valid pose plotting assumed first index nose tip second left shoulder want evaluate follow per paper get use input model model available inference,issue,negative,positive,neutral,neutral,positive,positive
860134638,"Hi, infer.py was moved to here: https://github.com/google-research/google-research/blob/master/poem/pr_vipe/infer.py. 

run.sh only provides a sample of how to run the code (it only trains a few steps). You can see this issue for more tips on how to run for more steps: https://github.com/google-research/google-research/issues/449. 

Hit@K is computed based on K nearest neighbors in the embedding space, and you can see the comment above on the steps: https://github.com/google-research/google-research/issues/457#issuecomment-781008721. To find K nearest neighbors, you don't need to use Tensorflow, there are other implementations such as from scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.",hi sample run code see issue run hit based nearest space see comment find nearest need use,issue,negative,neutral,neutral,neutral,neutral,neutral
859617341,"The infer.py file needs the input as a csv file. I am able to run the sample train script provided in run.sh and got the checkpoints, I want to evaluate the checkpoints. How can I generate the csv file and how can I visualize the input images to the infer.py file. Also, I want to evaluate using cross view pose retrieval taks by calculating Hit@K, how can I do that ? I am new to tensorflow.
",file need input file able run sample train script provided got want evaluate generate file visualize input file also want evaluate cross view pose retrieval calculating hit new,issue,negative,positive,positive,positive,positive,positive
856940540,"I was able to solve this one on my own, for anyone having the same issue, take a look at :
echo ""_ZN10tensorflow7strings6StrCatB5cxx11ERKNS0_8AlphaNumES3_S3_"" | c++filt
then
nm -D /usr/local/lib/python3.6/dist-packages/tensorflow/libtensorflow_framework.so.2  |c++filt  | grep ""tensorflow::strings::StrCat""
You'll see the different tags, from there you can either rebuild your tensorflow, or if using bazel add --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""
(i'm using tensorflow 2.3.2 in a docker container under ppc64le)
![Debug_n+3](https://user-images.githubusercontent.com/47244743/121227825-03206a80-c85a-11eb-8f7e-3ca46c3f6fad.PNG)
![SOLUTION XD](https://user-images.githubusercontent.com/47244743/121227827-03b90100-c85a-11eb-9af2-3cab064c1705.PNG)

",able solve one anyone issue take look echo see different either rebuild add docker container solution,issue,negative,positive,positive,positive,positive,positive
856167156,"Hello, thanks for your interventions above, i have a similar issue regarding undefined symbol. but this time a different one.
![image](https://user-images.githubusercontent.com/47244743/121070884-3e109880-c79d-11eb-938c-82ebf93c0e99.png)
",hello thanks similar issue regarding undefined symbol time different one image,issue,negative,positive,neutral,neutral,positive,positive
853867135,Hello @FrancescoMandru did you got your model to train ? ,hello got model train,issue,negative,neutral,neutral,neutral,neutral,neutral
853859294,"not sure that I am getting what you mean with the balance thing, sorry this thread can be remove. I figure I wont get much help here but I am slowly starting to understand while digging into the code, so that is ok. 
",sure getting mean balance thing sorry thread remove figure wont get much help slowly starting understand digging code,issue,negative,negative,neutral,neutral,negative,negative
851720624,"Thank you for your kind response!
I understood that recent version did not support missing keypoints.
I immediately check the report, and I'm very looking forward to new version!",thank kind response understood recent version support missing immediately check report looking forward new version,issue,positive,positive,positive,positive,positive,positive
851638173,Hello! Thanks for your interest! We've looked into missing keypoints as well in our recent paper on occlusion-robust embeddings https://arxiv.org/pdf/2010.13321.pdf and we are working on releasing code that can support missing keypoints. Let us know if there is anything else we can help with. ,hello thanks interest missing well recent paper working code support missing let u know anything else help,issue,positive,negative,neutral,neutral,negative,negative
851617217,Long as we understand each other I’m good man a living like this until it upsets the balance,long understand good man living like balance,issue,positive,positive,positive,positive,positive,positive
847554069,"I have also faced the same problem and I have no idea how to fix it. 

But you can disable LPIPS by commented the following line and it works fine for me.

https://github.com/google-research/google-research/blob/0344f02961613178e1d119e7ac3a16f77916fb55/jaxnerf/eval.py#L73

https://github.com/google-research/google-research/blob/0344f02961613178e1d119e7ac3a16f77916fb55/jaxnerf/eval.py#L131

https://github.com/google-research/google-research/blob/0344f02961613178e1d119e7ac3a16f77916fb55/jaxnerf/eval.py#L135

https://github.com/google-research/google-research/blob/0344f02961613178e1d119e7ac3a16f77916fb55/jaxnerf/eval.py#L147

https://github.com/google-research/google-research/blob/0344f02961613178e1d119e7ac3a16f77916fb55/jaxnerf/eval.py#L154-L155

https://github.com/google-research/google-research/blob/0344f02961613178e1d119e7ac3a16f77916fb55/jaxnerf/eval.py#L160-L161

If you still need LPIPS you may have to find another alternative library to compute LPIPS . 

you may save the output as `npy` format and then use the [official implementation of LPIPS](https://github.com/richzhang/PerceptualSimilarity)",also faced problem idea fix disable following line work fine still need may find another alternative library compute may save output format use official implementation,issue,negative,positive,positive,positive,positive,positive
847071808,@hufeng0912 I just pushed https://github.com/google-research/google-research/commit/c1c89f282cf6afaa6b07bebc2e3e3ed53fba5827 this which ensures that the TF version is greater than 2.6.0 and it automatically closed the issue. Feel free to let me know if you have other questions.,version greater automatically closed issue feel free let know,issue,positive,positive,positive,positive,positive,positive
847044345,"Yes indeed, they are as expected. Could you please close the issue if it is resolved?",yes indeed could please close issue resolved,issue,positive,neutral,neutral,neutral,neutral,neutral
846851319,"Thank you so much for your help! I try it again for the new version, here is my result after the first command:
![(1)](https://user-images.githubusercontent.com/50228574/119315504-7fd31680-bca8-11eb-98d6-e116e38d8bc2.jpg)
![(2)](https://user-images.githubusercontent.com/50228574/119315511-81044380-bca8-11eb-97b2-545a238c665f.jpg)
![(3)](https://user-images.githubusercontent.com/50228574/119315514-81044380-bca8-11eb-82a3-655225b800c3.jpg)
![(4)](https://user-images.githubusercontent.com/50228574/119315516-819cda00-bca8-11eb-8f13-36b561074969.jpg)
![(5)](https://user-images.githubusercontent.com/50228574/119315520-82357080-bca8-11eb-9e0d-a3f56e71e96e.jpg)
![(6)](https://user-images.githubusercontent.com/50228574/119315521-82357080-bca8-11eb-99ff-8a173aa03ee9.jpg)
![(7)](https://user-images.githubusercontent.com/50228574/119315524-82ce0700-bca8-11eb-9a3f-ed231004bed0.jpg)
![(8)](https://user-images.githubusercontent.com/50228574/119315527-83669d80-bca8-11eb-9896-5ab80506820c.jpg)
![(9)](https://user-images.githubusercontent.com/50228574/119315530-83669d80-bca8-11eb-8216-428b203b91e4.jpg)
![(10)](https://user-images.githubusercontent.com/50228574/119315533-83ff3400-bca8-11eb-8087-c7f9b89b9c2a.jpg)
I think it is the correct result for your codes, right?",thank much help try new version result first command think correct result right,issue,positive,positive,positive,positive,positive,positive
846802018,"Your tensorflow version is outdated. I trained my models with 2.6.0. Could you update to Tensorflow 2.6.0 and report your results again?
",version outdated trained could update report,issue,negative,negative,negative,negative,negative,negative
846770125,Yeah those seem correct to me. I did not make any modifications from the script for the results I just posted.,yeah seem correct make script posted,issue,negative,neutral,neutral,neutral,neutral,neutral
846754818,"actually, I just modify the path of the custom_colorize.py, and others just keep original, Right?
and I don't find any error about the logs.",actually modify path keep original right find error,issue,negative,positive,positive,positive,positive,positive
846539053,"Wow，your results are amazing！I will follow your advice to check the option，thanks for your help so much. We keep conversation later. ------------------&nbsp;原始邮件&nbsp;------------------
***@***.***&gt;
发送时间:&nbsp;2021年5月23日(星期天) 晚上6:10
***@***.***&gt;;
***@***.******@***.***&gt;;
主题:&nbsp;Re: [google-research/google-research] about coltran, I have some questions. (#710)",follow advice check help much keep conversation later,issue,negative,positive,neutral,neutral,positive,positive
846537659,"I just made a new workspace and ran the commands again.  Here are my final colorized images.

![image](https://user-images.githubusercontent.com/1867024/119255848-25f62200-bbbe-11eb-8674-cc35eaa7e3b7.png)

![image](https://user-images.githubusercontent.com/1867024/119255857-31494d80-bbbe-11eb-9192-45d98f32a146.png)
![image](https://user-images.githubusercontent.com/1867024/119255869-3efed300-bbbe-11eb-9aca-31bd7076dbbb.png)
![image](https://user-images.githubusercontent.com/1867024/119255871-42925a00-bbbe-11eb-8cc8-0d5ff0eb0629.png)
![image](https://user-images.githubusercontent.com/1867024/119255884-50e07600-bbbe-11eb-900c-17dd505fc16b.png)
![image](https://user-images.githubusercontent.com/1867024/119255893-5dfd6500-bbbe-11eb-9c48-8dea6d9fcb3e.png)
![image](https://user-images.githubusercontent.com/1867024/119255910-6e154480-bbbe-11eb-85c7-85f97090b4f6.png)
![image](https://user-images.githubusercontent.com/1867024/119255915-72d9f880-bbbe-11eb-9800-9bf12dccc9b6.png)
![image](https://user-images.githubusercontent.com/1867024/119255921-75d4e900-bbbe-11eb-9ea8-c3bdd7000f81.png)
![image](https://user-images.githubusercontent.com/1867024/119255925-7bcaca00-bbbe-11eb-999f-132fcc64b1e7.png)

Unfortunately (or fortunately), I cannot reproduce any of your outputs, so it is difficult for me to offer help. 

1, Could you check if all flags are passed correctly
- For example, FLAGS.mode shoule be colorize in this line (https://github.com/google-research/google-research/blob/master/coltran/custom_colorize.py#L117)
- name should be 'coltran_core' in this line (https://github.com/google-research/google-research/blob/master/coltran/custom_colorize.py#L162)

2. Could you post your TF, NumPy and matplotlib versions?
3. Could you save the samples here (https://github.com/google-research/google-research/blob/master/coltran/custom_colorize.py#L250) as NumPy arrays? And then can you load the numpy arrays as images in a separate notebook to see if there is the same noisy output. I wonder if it has something to do with how matplotlib stores images.

",made new ran final image image image image image image image image image image unfortunately fortunately reproduce difficult offer help could check correctly example colorize line name line could post could save load separate notebook see noisy output wonder something,issue,negative,positive,neutral,neutral,positive,positive
846513330,"now my gpu is RTX2080. Here is the out logs of the GPU after the first command:
`

WARNING:tensorflow:From` C:\Users\GLDYD-002\Anaconda3\envs\tensorflow2.0\lib\site-packages\tensorflow\python\training\moving_averages.py:444: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0523 14:24:35.690207 29252 deprecation.py:323] From C:\Users\GLDYD-002\Anaconda3\envs\tensorflow2.0\lib\site-packages\tensorflow\python\training\moving_averages.py:444: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.

I0523 14:24:35.925577 29252 train_utils.py:91] Built with exponential moving average.
I0523 14:24:35.941554 29252 train_utils.py:185] Restoring from coltran/coltran/colorizer.
I0523 14:24:38.793386 29252 custom_colorize.py:220] Producing sample after 601000 training steps.
I0523 14:24:38.794382 29252 custom_colorize.py:223] 1
I0523 14:26:21.048899 29252 custom_colorize.py:245] ['(1).jpg', '(10).jpg', '(2).jpg', '(3).jpg', '(4).jpg', '(5).jpg', '(6).jpg', '(7).jpg', '(8).jpg', '(9).jpg']
I0523 14:26:21.051920 29252 custom_colorize.py:249] coltran/results\stage1\(1).jpg
I0523 14:26:21.051920 29252 custom_colorize.py:251] (64, 64, 3)
I0523 14:26:21.082807 29252 custom_colorize.py:249] coltran/results\stage1\(10).jpg
I0523 14:26:21.082807 29252 custom_colorize.py:251] (64, 64, 3)
I0523 14:26:21.110733 29252 custom_colorize.py:249] coltran/results\stage1\(2).jpg
I0523 14:26:21.110733 29252 custom_colorize.py:251] (64, 64, 3)
I0523 14:26:21.136664 29252 custom_colorize.py:249] coltran/results\stage1\(3).jpg
I0523 14:26:21.136664 29252 custom_colorize.py:251] (64, 64, 3)
I0523 14:26:21.160599 29252 custom_colorize.py:249] coltran/results\stage1\(4).jpg
I0523 14:26:21.161597 29252 custom_colorize.py:251] (64, 64, 3)
I0523 14:26:21.178551 29252 custom_colorize.py:249] coltran/results\stage1\(5).jpg
I0523 14:26:21.179549 29252 custom_colorize.py:251] (64, 64, 3)
I0523 14:26:21.202487 29252 custom_colorize.py:249] coltran/results\stage1\(6).jpg
I0523 14:26:21.202487 29252 custom_colorize.py:251] (64, 64, 3)
I0523 14:26:21.228418 29252 custom_colorize.py:249] coltran/results\stage1\(7).jpg
I0523 14:26:21.228418 29252 custom_colorize.py:251] (64, 64, 3)
I0523 14:26:21.252354 29252 custom_colorize.py:249] coltran/results\stage1\(8).jpg
I0523 14:26:21.253352 29252 custom_colorize.py:251] (64, 64, 3)
I0523 14:26:21.273298 29252 custom_colorize.py:249] coltran/results\stage1\(9).jpg
I0523 14:26:21.273298 29252 custom_colorize.py:251] (64, 64, 3)`",first command warning removed future version use automatically eager graph inside removed future version use automatically eager graph inside built exponential moving average sample training,issue,negative,positive,neutral,neutral,positive,positive
846511311,"YES，I just stored my grayscale images  as .jpg files.
and I never modify the code, I just modify the command.",never modify code modify command,issue,negative,neutral,neutral,neutral,neutral,neutral
846424373,"Yes

![image](https://user-images.githubusercontent.com/1867024/119231907-60f74780-bb23-11eb-94fc-8b70cac2fef2.png)

Are your grayscale images stored as .jpg files?
Could you please post all the modifications you made to the code and also the output logs of your GPU?

Thanks!",yes image could please post made code also output thanks,issue,positive,positive,positive,positive,positive,positive
846417887,"I just put my grayscale images in the directory. And modify the path of custome_colorize code。 You sure the model of you performed are the same with the code of github？------------------&nbsp;原始邮件&nbsp;------------------
***@***.***&gt;
发送时间:&nbsp;2021年5月22日(星期六) 晚上9:59
***@***.***&gt;;
***@***.******@***.***&gt;;
主题:&nbsp;Re: [google-research/google-research] about coltran, I have some questions. (#710)",put directory modify path sure model code,issue,negative,positive,positive,positive,positive,positive
846412379,"I'm unable to reproduce your noisy images. I just tried the script on one of your grayscale images

![image](https://user-images.githubusercontent.com/1867024/119228937-bc223d80-bb15-11eb-9823-94a0b9f3115c.png)

and I get this output from the first command line:

![image](https://user-images.githubusercontent.com/1867024/119228965-d9efa280-bb15-11eb-971c-4694e9b2e9d2.png)

How are you storing the grayscale images in your `coltran/test-qifengle` directory?
Could you copy-paste your GPU logs as well?",unable reproduce noisy tried script one image get output first command line image directory could well,issue,negative,negative,negative,negative,negative,negative
846406953,"Below is the result of using the first line of command：
According to your instructions, the output should be a 64×64 rough picture
![(1)](https://user-images.githubusercontent.com/50228574/119227907-e2120700-bb42-11eb-80f3-26f4c174be27.jpg)
![(2)](https://user-images.githubusercontent.com/50228574/119227910-e3433400-bb42-11eb-9295-ca45c02145d9.jpg)
![(3)](https://user-images.githubusercontent.com/50228574/119227911-e3433400-bb42-11eb-8e7f-032932dc7c60.jpg)
![(4)](https://user-images.githubusercontent.com/50228574/119227912-e3dbca80-bb42-11eb-9973-d07f274502f5.jpg)
![(5)](https://user-images.githubusercontent.com/50228574/119227913-e4746100-bb42-11eb-8e13-895b620cf62a.jpg)
![(6)](https://user-images.githubusercontent.com/50228574/119227914-e4746100-bb42-11eb-9625-d2d235867bd6.jpg)
![(7)](https://user-images.githubusercontent.com/50228574/119227915-e50cf780-bb42-11eb-9041-3752164361e5.jpg)
![(8)](https://user-images.githubusercontent.com/50228574/119227916-e5a58e00-bb42-11eb-95db-2050e2090f1d.jpg)
![(9)](https://user-images.githubusercontent.com/50228574/119227917-e5a58e00-bb42-11eb-991f-12d19c898bfc.jpg)
![(10)](https://user-images.githubusercontent.com/50228574/119227918-e63e2480-bb42-11eb-99a8-52177a77d7cc.jpg)
",result first line according output rough picture,issue,negative,positive,neutral,neutral,positive,positive
846406105,"Thanks! It looks fine to me.

Could you remove all the files from the store_dir and post the results and output from just the first line?

```
python -m custom_colorize --config=coltran/configs/colorizer.py --logdir=coltran/coltran/colorizer --img_dir=coltran/test-qifengle --store_dir=coltran/results --mode=colorize
```",thanks fine could remove post output first line python,issue,positive,positive,positive,positive,positive,positive
846405501,"My client does not have permission to access this URL. Please confirm if I have downloaded your pre-trained model correctly
![image](https://user-images.githubusercontent.com/50228574/119227585-3d42fa00-bb41-11eb-8b80-b0994ab0ed38.png)
![image](https://user-images.githubusercontent.com/50228574/119227592-446a0800-bb41-11eb-8ab0-6225e928d5a4.png)
![image](https://user-images.githubusercontent.com/50228574/119227598-49c75280-bb41-11eb-82fe-4ae72bbc50d7.png)
![image](https://user-images.githubusercontent.com/50228574/119227606-5055ca00-bb41-11eb-9269-e53200cc95fe.png)
",client permission access please confirm model correctly image image image image,issue,negative,neutral,neutral,neutral,neutral,neutral
846402075,"Thank you very much for your reply. The output on that line should be correct. But what I don’t understand is why the output result is a noise graph, because the GPU I use is Tesla v100",thank much reply output line correct understand output result noise graph use,issue,negative,positive,positive,positive,positive,positive
846401702,"I converted some sample RGB test images from imagenet to grayscale images in a directory and the colorizer script colorized those images successfully (reproduced results from the paper).

Thanks for sharing your grayscale images. I will try to test your grayscale images using my setup later this week.",converted sample test directory script successfully paper thanks try test setup later week,issue,positive,positive,positive,positive,positive,positive
846400270,"If your model is guaranteed to be valid, can it be tested with the grayscale input image I just provided?",model valid tested input image provided,issue,negative,neutral,neutral,neutral,neutral,neutral
846400171,"By the way, the output result of this line in my impression is:
Producing sample after 301000 training steps",way output result line impression sample training,issue,negative,neutral,neutral,neutral,neutral,neutral
846399647,"Thank you very much for your quick reply。The following is my execution code command line：
```
python -m custom_colorize --config=coltran/configs/colorizer.py --logdir=coltran/coltran/colorizer --img_dir=coltran/test-qifengle --store_dir=coltran/results --mode=colorize

python -m custom_colorize --config=coltran/configs/color_upsampler.py --logdir=coltran/coltran/color_upsampler --img_dir=coltran/test-qifengle --store_dir=coltran/results --gen_data_dir=coltran/results/stage1 --mode=colorize

python -m custom_colorize --config=coltran/configs/spatial_upsampler.py --logdir=coltran/coltran/spatial_upsampler --img_dir=coltran/test-qifengle --store_dir=coltran/results --gen_data_dir=coltran/results/stage2 --mode=colorize
```
My test image is a grayscale image with a size of 256×256。
![(1)](https://user-images.githubusercontent.com/50228574/119226083-1e8d3500-bb3a-11eb-9307-369fb71462ee.jpg)
![(2)](https://user-images.githubusercontent.com/50228574/119226087-1fbe6200-bb3a-11eb-9b02-3e0a943d35b2.jpg)
![(3)](https://user-images.githubusercontent.com/50228574/119226090-20ef8f00-bb3a-11eb-8cb1-b0deb297bdf4.jpg)
![(4)](https://user-images.githubusercontent.com/50228574/119226091-21882580-bb3a-11eb-9328-f432daf0fe7b.jpg)
![(5)](https://user-images.githubusercontent.com/50228574/119226093-21882580-bb3a-11eb-92aa-ef005815bdab.jpg)
![(6)](https://user-images.githubusercontent.com/50228574/119226094-2220bc00-bb3a-11eb-8805-3f45e5e9f108.jpg)
![(7)](https://user-images.githubusercontent.com/50228574/119226095-22b95280-bb3a-11eb-88b9-8c4e3ce5cbd0.jpg)
![(8)](https://user-images.githubusercontent.com/50228574/119226098-22b95280-bb3a-11eb-8b55-bbb2d21af253.jpg)
![(9)](https://user-images.githubusercontent.com/50228574/119226099-2351e900-bb3a-11eb-8f7f-16d68d245bb5.jpg)
![(10)](https://user-images.githubusercontent.com/50228574/119226100-23ea7f80-bb3a-11eb-9428-d98d6c4b9286.jpg)
I have read all the questions from other users. At present, they are relevant, but you have provided a code that can be tested by inputting a grayscale image, right?
My second question is can I train my custom dataset? But from your code point of view, it is using the downloaded ImageNet dataset. Modify the custom dataset for your code, I still have no way to achieve it",thank much quick following execution code command python python python test image image size read present relevant provided code tested image right second question train custom code point view modify custom code still way achieve,issue,negative,positive,positive,positive,positive,positive
846398392,"Hi, thanks for the interest in our paper

Are the checkpoints restored correctly? What is the output of this line?
https://github.com/google-research/google-research/blob/master/coltran/custom_colorize.py#L220

More generally, are these issues relevant?

https://github.com/google-research/google-research/issues/583
https://github.com/google-research/google-research/issues/619
",hi thanks interest paper correctly output line generally relevant,issue,positive,positive,positive,positive,positive,positive
843787764,"Assuming 1) problem is resolved, but `gmm.GenericKmeans(...)` alway set parameter `spherical` as `false`. The new center points of each k-means iteration are not normalized, this will cause the k-means clustering is imbalance and not converged.

I think `RecomputeCentroidsWithParallelCostMultiplier(...)` should support parameter `spherical` and normalize the centers. 

@sammymax @YJYJLee 
",assuming problem resolved alway set parameter spherical false new center iteration cause clustering imbalance think support parameter spherical normalize,issue,negative,negative,negative,negative,negative,negative
843439911,"Yes you are right. That is the reason the eval is off.

I rechecked the second suggestion and you don't need to change anything if you are using the version of TF mentioned in requirements.txt.

For newer versions, tf.keras.backend.learning_phase() will be deprecated and you need to pass training=True/False explicity. The part you pasted already does this, so that's ok. The part that will need changing in the future is https://github.com/google-research/google-research/blob/master/tcc/utils.py#L302 and https://github.com/google-research/google-research/blob/master/tcc/utils.py#L396 where training=False needs to be passed. 

Let me know if eval works for you now.",yes right reason second suggestion need change anything version need pas part pasted already part need future need let know work,issue,negative,positive,neutral,neutral,positive,positive
843288679,Thank you everyone for watching my back on this,thank everyone watching back,issue,negative,neutral,neutral,neutral,neutral,neutral
842645936,Does this mean it's OK to normalize data set even if it's not cutting through?￼,mean normalize data set even cutting,issue,negative,negative,negative,negative,negative,negative
842058545,"Hi @debidatta, 

Thanks for the suggestions. 

I think I resolved the issue. I had an issue in the following line: 
`restore_ckpt(logdir=CONFIG.LOGDIR, optimizer=optimizer, **algo.model)` in `evaluate_once` function.

`CONFIG.LOGDIR` is being updated in `evaluate()` by assigning `CONFIG.LOGDIR = FLAGS.logdif` but in `evaluate_once()` when loading the checkpoint, `CONFIG.LOGDIR` was not actually `FLAGS.logdir` so I happened to refer to a wrong `logdir`.  I had to add `CONFIG.LOGDIR = FLAGS.logdir` again at the beginning of `evaluate_once` function.

Regarding your second suggestion, I have questions. 

Shouldn't the following update make the base_model be operated in eval mode?     
```
training = (tf.keras.backend.learning_phase() and CONFIG.MODEL.TRAIN_BASE != 'frozen')
x = self.base_model(images, training=training)
```

Should I additionally modify ConvEmbedder for the eval mode? For example, turning off fc_dropout and/or inputting training=false for batch normalization? 

Any feedback would be appreciated. Thank you!",hi thanks think resolved issue issue following line function evaluate loading actually refer wrong add beginning function regarding second suggestion following update make mode training additionally modify mode example turning batch normalization feedback would thank,issue,negative,negative,neutral,neutral,negative,negative
841731330,"> Hi, have you figured it out in the meantime? I have the same question..

Nope, I have just left it as `250000` for the time being!",hi figured question nope left time,issue,negative,neutral,neutral,neutral,neutral,neutral
841642609,"@PhilippSchmaelzle Thank you so much for the information! I use the same dataset and same initialization, maybe I will just wait until it is trained for enough steps since the predictions look acceptable now. May I ask whether you use the given camera intrinsics, or just let the network learn it? I am also training another model using the given intrinsics but the model appears even worse. Super grateful if you can share some experience on it!",thank much information use maybe wait trained enough since look acceptable may ask whether use given camera let network learn also training another model given model even worse super grateful share experience,issue,positive,positive,neutral,neutral,positive,positive
841640828,"Hii @ZetongZhangZ, I did some training as well and it performs quite well on the Cityscape too.
I used the following dataset: 'leftImg8bit_sequence_trainvaltest.zip'. I initialized the mono_depth net with the [this resnet-18](https://github.com/dalgu90/resnet-18-tensorflow) which helped alot.

Further I trained on the named dataset for 100.000 steps (followd by some steps on my own data) which gave good results.
The artifacts you stated, I guess you mean some dotted or striped areas, will resolve themself after couple 10.000 steps. ",training well quite well cityscape used following net trained data gave good stated guess mean dotted striped resolve couple,issue,positive,positive,neutral,neutral,positive,positive
841630516,Not sure what to post but many thanks,sure post many thanks,issue,positive,positive,positive,positive,positive,positive
841613523,"""Unroll"" just removes the symbolic loops in the model, it does not affect anything else, I tested it on tf 2.6 nightly and it works ok.

Without ""Unroll"", I confirm that InvalidArgumentError happens on tf 2.6 (so it looks like this issues happens on 2.5.1 and above).
Please report this issue at tf github.

""any chance background noise could be matched to the RMS of classification so you can have more control without it becoming the foreground?""
This customization is up to you: current set up was selected for comparing with other papers.",unroll symbolic model affect anything else tested nightly work without unroll confirm like please report issue chance background noise could classification control without becoming foreground current set selected,issue,negative,positive,positive,positive,positive,positive
841592732,"Its OK @rybakov prob will prefer to use 2.5 and was just eager to test some of the tflite post quant changes and see if and what...

I am OK with 2.4.1 and was just reporting back as 2.5 is still hot from the press.

Thanks for g-kws seriously great PS any chance background noise could be matched to the RMS of classification so you can have more control without it becoming the foreground?

Does unroll have any affect other than fix?

Is it a matter of 2.5.1 or above when it happens?",prob prefer use eager test post quant see back still hot press thanks seriously great chance background noise could classification control without becoming foreground unroll affect fix matter,issue,positive,positive,positive,positive,positive,positive
841583590,Thank you for sharing the log. It looks like there was an issue with symbolic loops / control flow. I would suggest to try tf nightly or if you prefer to use tf 2.5 then I would try to set unroll=True in the [code](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/gru.py#L63),thank log like issue symbolic control flow would suggest try nightly prefer use would try set code,issue,positive,neutral,neutral,neutral,neutral,neutral
841574385,"```
W0515 01:51:17.825679 140248402396992 module_wrapper.py:153] From /home/stuart/google-kws/kws_streaming/train/train.py:49: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.

I0515 01:51:17.928516 140248402396992 train.py:69] Namespace(act1=""'linear','relu'"", background_frequency=0.8, background_volume=0.1, batch_size=100, causal_data_frame_padding=0, clip_duration_ms=1000, cnn_act=""'relu','relu'"", cnn_dilation_rate='(1,1),(1,1)', cnn_filters='16,16', cnn_kernel_size='(3,3),(5,3)', cnn_strides='(1,1),(1,1)', data_dir='/home/stuart/Dataset-builder/dataset/', data_frame_padding=None, data_stride=1, data_url='', dct_num_features=20, desired_samples=16000, dropout1=0.1, eval_step_interval=400, feature_type='mfcc_op', fft_magnitude_squared=True, frequency_mask_max_size=5, frequency_masks_number=2, gru_units='256', how_many_training_steps='6000,6000,6000,6000', l2_weight_decay=0.0, label_count=3, learning_rate='0.001,0.0005,0.0001,0.00002', log_epsilon=1e-12, lr_schedule='exp', mel_lower_edge_hertz=20.0, mel_non_zero_only=1, mel_num_bins=40, mel_upper_edge_hertz=7600.0, model_name='crnn', novograd_beta_1=0.95, novograd_beta_2=0.5, novograd_grad_averaging=0, novograd_weight_decay=0.001, optimizer='adam', optimizer_epsilon=1e-08, pick_deterministically=0, preemph=0.0, preprocess='raw', quantize=0, resample=0.0, return_sequences='0', return_softmax=0, sample_rate=16000, save_step_interval=100, silence_percentage=10.0, sp_resample=0.0, sp_time_shift_ms=0.0, spec_cutout_frequency_mask_size=5, spec_cutout_masks_number=3, spec_cutout_time_mask_size=10, spectrogram_length=49, split_data=0, start_checkpoint='', stateful=1, summaries_dir='/home/stuart/google-kws/models2/crnn_state/logs/', testing_percentage=10, time_mask_max_size=10, time_masks_number=2, time_shift_ms=100.0, train=1, train_dir='/home/stuart/google-kws/models2/crnn_state/', training=True, units1='128,256', unknown_percentage=10.0, use_spec_augment=1, use_spec_cutout=0, use_tf_fft=0, validation_percentage=10, verbosity=0, volume_resample=0.0, wanted_words='silence,notkw,kw', wav=1, window_size_ms=40.0, window_size_samples=640, window_stride_ms=20.0, window_stride_samples=320, window_type='hann')
2021-05-15 01:51:18.472312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-15 01:51:18.472656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 104.43GiB/s
2021-05-15 01:51:18.472766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-15 01:51:18.473114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-15 01:51:18.473378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-05-15 01:51:18.695905: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3392225000 Hz
2021-05-15 01:51:18.977733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-15 01:51:18.978064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 104.43GiB/s
2021-05-15 01:51:18.978127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-15 01:51:18.978416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-15 01:51:18.978667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-05-15 01:51:18.978698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-05-15 01:51:18.978708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-05-15 01:51:18.978716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-05-15 01:51:18.978791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-15 01:51:18.979082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-05-15 01:51:18.979403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3129 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:Layer cell will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
W0515 01:51:18.979778 140248402396992 recurrent_v2.py:413] Layer cell will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(100, 16000)]            0         
_________________________________________________________________
speech_features (SpeechFeatu (100, 49, 20)             0         
_________________________________________________________________
tf_op_layer_ExpandDims (Tens [(100, 49, 20, 1)]        0         
_________________________________________________________________
stream (Stream)              (100, 47, 18, 16)         160       
_________________________________________________________________
stream_1 (Stream)            (100, 43, 16, 16)         3856      
_________________________________________________________________
reshape (Reshape)            (100, 43, 256)            0         
_________________________________________________________________
gru (GRU)                    (100, 1, 256)             394752    
_________________________________________________________________
stream_2 (Stream)            (100, 256)                0         
_________________________________________________________________
dropout (Dropout)            (100, 256)                0         
_________________________________________________________________
dense (Dense)                (100, 128)                32896     
_________________________________________________________________
dense_1 (Dense)              (100, 256)                33024     
_________________________________________________________________
dense_2 (Dense)              (100, 3)                  771       
=================================================================
Total params: 465,459
Trainable params: 465,459
Non-trainable params: 0
_________________________________________________________________
I0515 01:51:19.321968 140248402396992 train.py:71] None
I0515 01:51:19.619409 140248402396992 train.py:107] Training from step: 1 
2021-05-15 01:51:21.140477: W tensorflow/c/c_api.cc:335] Operation '{name:'gru/cell/while' id:661 op device:{} def:{{{node gru/cell/while}} = While[T=[DT_INT32, DT_INT32, DT_INT32, DT_VARIANT, DT_FLOAT, ..., DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=53, _read_only_resource_inputs=[7, 8, 9], body=gru_cell_while_body_1256_rewritten[], cond=gru_cell_while_cond_1255_rewritten[], output_shapes=[[], [], [], [], [100,256], ..., [], [], [], [], []], parallel_iterations=32](gru/cell/while/loop_counter, gru/cell/while/maximum_iterations, gru/cell/time, gru/cell/TensorArrayV2_1, gru/cell/ReadVariableOp, gru/cell/strided_slice, gru/cell/TensorArrayUnstack/TensorListFromTensor, gru/cell/gru_cell/bias, gru/cell/gru_cell/kernel, gru/cell/gru_cell/recurrent_kernel, gru/cell/while/EmptyTensorList, gru/cell/while/EmptyTensorList_1, gru/cell/while/EmptyTensorList_2, gru/cell/while/EmptyTensorList_3, gru/cell/while/EmptyTensorList_4, gru/cell/while/EmptyTensorList_5, gru/cell/while/EmptyTensorList_6, gru/cell/while/EmptyTensorList_7, gru/cell/while/EmptyTensorList_8, gru/cell/while/EmptyTensorList_9, gru/cell/while/EmptyTensorList_10, gru/cell/while/EmptyTensorList_11, gru/cell/while/EmptyTensorList_12, gru/cell/while/EmptyTensorList_13, gru/cell/while/EmptyTensorList_14, gru/cell/while/EmptyTensorList_15, gru/cell/while/EmptyTensorList_16, gru/cell/while/EmptyTensorList_17, gru/cell/while/EmptyTensorList_18, gru/cell/while/EmptyTensorList_19, gru/cell/while/EmptyTensorList_20, gru/cell/while/EmptyTensorList_21, gru/cell/while/EmptyTensorList_22, gru/cell/while/EmptyTensorList_23, gru/cell/while/EmptyTensorList_24, gru/cell/while/EmptyTensorList_25, gru/cell/while/EmptyTensorList_26, gru/cell/while/EmptyTensorList_27, gru/cell/while/EmptyTensorList_28, gru/cell/while/EmptyTensorList_29, gru/cell/while/EmptyTensorList_30, gru/cell/while/EmptyTensorList_31, gru/cell/while/EmptyTensorList_32, gru/cell/while/EmptyTensorList_33, gru/cell/while/EmptyTensorList_34, gru/cell/while/EmptyTensorList_35, gru/cell/while/EmptyTensorList_36, gru/cell/while/EmptyTensorList_37, gru/cell/while/EmptyTensorList_38, gru/cell/while/EmptyTensorList_39, gru/cell/while/EmptyTensorList_40, training/Adam/gradients/gradients/gru/cell/while_grad/gradients/TensorArrayV2Read/TensorListGetItem_grad/TensorListElementShape_0/accumulator:0, training/Adam/gradients/gradients/gru/cell/while_grad/gradients/TensorArrayV2Read/TensorListGetItem_grad/TensorListLength_0/accumulator:0)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Traceback (most recent call last):
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1375, in _do_call
    return fn(*args)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1358, in _run_fn
    self._extend_graph()
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1398, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'training/Adam/gradients/gradients/gru/cell/while_grad/gru/cell/while_grad': Connecting to invalid output 51 of source node gru/cell/while which has 51 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/stuart/google-kws/kws_streaming/train/model_train_eval.py"", line 366, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/stuart/google-kws/kws_streaming/train/model_train_eval.py"", line 147, in main
    train.train(flags)
  File ""/home/stuart/google-kws/kws_streaming/train/train.py"", line 147, in train
    result = model.train_on_batch(train_fingerprints, train_ground_truth)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 1094, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/keras/backend.py"", line 4020, in __call__
    session = get_session(inputs)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/keras/backend.py"", line 745, in get_session
    _initialize_variables(session)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/keras/backend.py"", line 1193, in _initialize_variables
    is_initialized = session.run(
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1190, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1368, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1394, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'training/Adam/gradients/gradients/gru/cell/while_grad/gru/cell/while_grad': Connecting to invalid output 51 of source node gru/cell/while which has 51 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).
```
tensorflow-2.5.0-cp38-cp38-manylinux2010_x86_64.whl

```

$CMD_TRAIN \

--data_url '' \

--data_dir $DATA_PATH/ \

--train_dir $MODELS_PATH/crnn_state/ \

--mel_upper_edge_hertz 7600 \

--how_many_training_steps 20000,20000,20000,20000 \

--learning_rate 0.001,0.0005,0.0001,0.00002 \

--window_size_ms 40.0 \

--window_stride_ms 20.0 \

--mel_num_bins 40 \

--dct_num_features 20 \

--resample 0.15 \

--alsologtostderr \

--train 1 \

--lr_schedule 'exp' \

--use_spec_augment 1 \

--time_masks_number 2 \

--time_mask_max_size 10 \

--frequency_masks_number 2 \

--frequency_mask_max_size 5 \

--feature_type 'mfcc_op' \

--fft_magnitude_squared 1 \

crnn \

--cnn_filters '16,16' \

--cnn_kernel_size '(3,3),(5,3)' \

--cnn_act ""'relu','relu'"" \

--cnn_dilation_rate '(1,1),(1,1)' \

--cnn_strides '(1,1),(1,1)' \

--gru_units 256 \

--return_sequences 0 \

--dropout1 0.1 \

--units1 '128,256' \

--act1 ""'linear','relu'"" \

--stateful 1

```

1st step bangs out and apols thought it would be univeral as some sort of API change causing this with 2.5?
",name please use instead successful node read negative value must least one node node zero found device name ti successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible frequency successful node read negative value must least one node node zero found device name ti successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible device interconnect strength edge matrix successful node read negative value must least one node node zero successful node read negative value must least one node node zero device memory physical device name ti bus id compute capability warning layer cell use since meet criterion use generic kernel fallback running layer cell use since meet criterion use generic kernel fallback running model model layer type output shape param stream stream stream reshape reshape stream dropout dropout dense dense dense dense total trainable none training step operation name id device node setting attribute run session mutation effect trigger error future either modify running create new session recent call last file line return file line file line node invalid output source node try true handling exception another exception recent call last file line return code none file line code file line module unparsed file line run file line run main file line main file line main file line train result file line file line session file line session file line file line run result none file line handle file line return file line raise type message node invalid output source node try true resample train dropout act stateful st step thought would sort change causing,issue,positive,positive,neutral,neutral,positive,positive
841339647,please share more details about how to reproduce the error.,please share reproduce error,issue,negative,neutral,neutral,neutral,neutral,neutral
841016637,"I thought it might be because of the older fork but works on 2.4.1 but not 2.5

crnn stateful",thought might older fork work stateful,issue,negative,positive,positive,positive,positive,positive
840911128,"Hi @yongwanlim thanks for reaching out. 

Can you check the logs of the evaluate script to see if the checkpoint is getting loaded properly? Another thing to check would be if the function calls in eval script are being run in eval mode (training=False in Keras model calls). This used to happen implicitly before due to tf.keras.backend.set_learning_phase(0) but this feature was supposed to be deprecated in newer TF versions.",hi thanks reaching check evaluate script see getting loaded properly another thing check would function script run mode model used happen implicitly due feature supposed,issue,positive,positive,neutral,neutral,positive,positive
840454848,"@Lostinecho Hi, thank you for the information! I've tried to train and produce depth images several times but the predictions are not so good. I use Cityscapes as my dataset and train the model for about 10000 steps but I find the loss doesn't converge, swinging around 0.5, and there are severe artifacts in the depth images produced.  When I use a few hundreds of Cityscapes image sequences to do the training, it then performs much better. As you said, it works on simple datasets.

Although you used KITTI as the dataset, I believe it should be similar to my Cityscapes case. May I ask whether you got a well-performed model trained on full KITTI images later? I will be super grateful if you can offer some details of your training model. It will be very helpful for me to figure out what the problem is here. Thank you!",hi thank information tried train produce depth several time good use train model find loss converge swinging around severe depth produced use image training much better said work simple although used believe similar case may ask whether got model trained full later super grateful offer training model helpful figure problem thank,issue,positive,positive,positive,positive,positive,positive
839541890,"I found the same problem. 
So, scann default does not use ""anisotropic hashing/quantization"".",found problem default use anisotropic,issue,negative,neutral,neutral,neutral,neutral,neutral
834687627,"Thank you for reminding, I edited the link, a hyphen was missing. The code runs however only the advantages of the last task are normalized. ",thank link hyphen missing code however last task,issue,negative,negative,neutral,neutral,negative,negative
834499289,"@ecada  nice catch, i wonder if this file even runs, becaause task_indx seem undefined,

and the link is [MAMLReinforcement](https://github.com/google-research/google-research/blob/8c62c43029efc0c49d9dfc019bd9af11578cc1ae/norml/maml_rl.py#L581) . Your link needs editing",nice catch wonder file even seem undefined link link need,issue,negative,positive,positive,positive,positive,positive
834384971,"Hi Elad,

I'm also looking for a model checkpoint trained on the KITTI dataset. It will be highly appreciated if you could kindly share the files. 

Thank you for the great work!

Best,
Honglin
 ",hi also looking model trained highly could kindly share thank great work best,issue,positive,positive,positive,positive,positive,positive
832882253,"About ""noise resilient"".
It is hard to comment on it without seeing the code or getting more details about the issues.

About microcontroller front end:
""Also with the microcontroller front end are the validation results so bad because for testing its using a wrong MFCC method?
I haven't got round to exporting to microcontroller and using the frontend but wondered in the training part end testing results are due the model created with experimental frontend but testing is via the standard python ops MFCC.""
If you select microcontroller front end, then you have to use it during training and inference, because it is not numerically compatible with tf MFCC.",noise resilient hard comment without seeing code getting front end also front end validation bad testing wrong method got round training part end testing due model experimental testing via standard python select front end use training inference numerically compatible,issue,negative,negative,negative,negative,negative,negative
832865144,"[input_data](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py) loads wav files into batch and does some pre-processing. The simplest approach to replace it is to load all training data and validation data and call [model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit). This way it will work. Accuracy will not be the same because [input_data](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py) is pre-processing the data by stretching and shifting audio in time, it also adds noise and adds ""unknown"" label.

To reproduce the accuracy you could to add learning rate schedule and enable [rand_stretch_squeeze](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/speech_features.py#L233) and [rand_shift](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/speech_features.py#L234) in [speech_features](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/speech_features.py#L234) also will need to add noise to training data and create ""unknown"" label.

Another option is to convert [input_data](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py) to eager mode: will need to remove all sessions and validate that it works
",batch approach replace load training data validation data call way work accuracy data shifting audio time also noise unknown label reproduce accuracy could add learning rate schedule enable also need add noise training data create unknown label another option convert eager mode need remove session validate work,issue,positive,negative,neutral,neutral,negative,negative
832836651,"Many thanks as yeah that is prob it as didn't change the spectrum aug size.
I shall try again.

@rybakov Just another thing that would be so amazing as currently I am turning off split data and creating my own testing, validation & training folders.
I am getting really great noise resilient custom KWS where I am match the amplitude of _back_ground_noise_ to the KW & !KW samples.
This allows me free range of a mix volume of 0-1 and also I can vary the foreground volume without worry of the background being far to predominant with varying inputs.

The end result as said works exceptionally well with noisy environs but because there is no 'volume check' found I am limited to the default 0.3 mix level as higher starts returning too many garbage entries of mainly background in the foreground.

I ended up creating my own split dataset with mixing as in the code struggled to work out how to fit that into the framework, any chance that could be possible as it really does allow more noise resilient and more variance on input volume.

Also with the microcontroller front end are the validation results so bad because for testing its using a wrong MFCC method?
I haven't got round to exporting to microcontroller and using the frontend but wondered in the training part end testing results are due the model created with experimental frontend but testing is via the standard python ops MFCC.

 ",many thanks yeah prob change spectrum size shall try another thing would amazing currently turning split data testing validation training getting really great noise resilient custom match amplitude free range mix volume also vary foreground volume without worry background far predominant end result said work exceptionally well noisy environs check found limited default mix level higher many garbage mainly background foreground ended split code work fit framework chance could possible really allow noise resilient variance input volume also front end validation bad testing wrong method got round training part end testing due model experimental testing via standard python,issue,positive,positive,positive,positive,positive,positive
832793843,"I see.
Do you know any codes I can see as the example of eager mode compatible input?",see know see example eager mode compatible input,issue,negative,neutral,neutral,neutral,neutral,neutral
832442804,"About eager execution, [models_sub](https://github.com/google-research/google-research/tree/master/kws_streaming/models_sub) already eager mode compatible: you could see it in [conv_model_test.py](https://github.com/google-research/google-research/blob/master/kws_streaming/models_sub/conv_model_test.py): eager mode in this file is not disabled. So you can run inference of this model in eager mode.
But training utility is running in session/graph mode because of [input_data](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py) - it is not eager mode compatible. So if you make [input_data](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py) eager mode compatible then you could train [models_sub](https://github.com/google-research/google-research/tree/master/kws_streaming/models_sub) in eager mode.

There are also other models in [folder](https://github.com/google-research/google-research/tree/master/kws_streaming/models) but they are all not eager compatible, so eager mode is disabled explicitly in this [example test](https://github.com/google-research/google-research/blob/master/kws_streaming/models/ds_tc_resnet_test.py#L141).",eager execution already eager mode compatible could see eager mode file disabled run inference model eager mode training utility running mode eager mode compatible make eager mode compatible could train eager mode also folder eager compatible eager mode disabled explicitly example test,issue,positive,negative,negative,negative,negative,negative
832418473,"@rybakov 
Thank you for your advices. I reviewed my environment especially lib related to CUDA, eventually I could confirm Volatile GPU util working!
In my case, I use conda as a virtual environment and conda install tensorflow-gpu did work for this issue.

And then in terms of eager execution, I'm going to try to convert [models_sub](https://github.com/google-research/google-research/tree/master/kws_streaming/models_sub) to eager mode. If you have any other info, please kindly share with me. Any information will help me.",thank environment especially related eventually could confirm volatile working case use virtual environment install work issue eager execution going try convert eager mode please kindly share information help,issue,positive,positive,positive,positive,positive,positive
831931318,"> Hi, did you find a solution to this? I also did not find any files about inference.

You can find another issue with the same problem: 
[#issue 474](https://github.com/google-research/google-research/issues/474)",hi find solution also find inference find another issue problem issue,issue,negative,neutral,neutral,neutral,neutral,neutral
831878929,"Hi!,

first of all thank for your time and effort!. I want to use the tensorfow serving with ScaNN Op and I would like to be able to pass any of these parameters.

```python
tfrs.layers.factorized_top_k.ScaNN(
    query_model: Optional[tf.keras.Model] = None,
    k: int = 10,
    distance_measure: Text = 'dot_product',
    num_leaves: int = 100,
    num_leaves_to_search: int = 10,
    dimensions_per_block: int = 2,
    num_reordering_candidates: Optional[int] = None,
    parallelize_batch_searches: bool = True,
    name: Optional[Text] = None
)
```

 I did'nt see any clear way to do it throught the REST API. Is it possible ?
 
Thanks!,
Xavi",hi first thank time effort want use serving would like able pas python optional none text optional none bool true name optional text none see clear way rest possible thanks,issue,positive,positive,positive,positive,positive,positive
831312368,"Thanks for the soluton!

> The pre-trained ResNet-18 model used by struct2depth / depth_from_video_in_the_wild / depth_and_motion_learning is the same as [this](https://github.com/dalgu90/resnet-18-tensorflow.git)
> 
> ```shell
> git clone https://github.com/dalgu90/resnet-18-tensorflow.git
> cd resnet-18-tensorflow
> wget https://d2j0dndfm35trm.cloudfront.net/resnet-18.t7
> 
> # Optional if necessary:
> #   pip install torchfile
> #
> #   edit extract_torch_t7.py:
> #     import cPickle as pickle --> import pickle  # https://github.com/JuliaPy/PyCallJLD.jl/issues/6#issuecomment-421732697
> 
> python extract_torch_t7.py
> 
> # The generated:
> #    ./init/
> #        model.ckpt.data-00000-of-00001
> #        model.ckpt.index
> #        model.ckpt.meta
> # could be used for training depth_and_motion_learning.
> ```

",thanks model used shell git clone optional necessary pip install edit import pickle import pickle python could used training,issue,negative,positive,neutral,neutral,positive,positive
830848773,"It is hard to say what is the root cause, without seeing what command did you run and how to reproduce it.
But, I guess this issue can be related to spectrum augmentation: it applies mask on input signal and if signal length is smaller than mask size then it can crash. (with clip_duration_ms=200, you reduced signal length by 5x in comparison to original setting of 1000)
Tow options to fix it:
* Disable spectrum augmentation with ""--use_spec_augment 0"".
* Change settings of spectrum augmentation by reducing time_mask_max_size and frequency_mask_max_size
",hard say root cause without seeing command run reproduce guess issue related spectrum augmentation mask input signal signal length smaller mask size crash reduced signal length comparison original setting tow fix disable spectrum augmentation change spectrum augmentation reducing,issue,negative,positive,neutral,neutral,positive,positive
830774560,"Before optimizing model training with data pre-fetching and modifying the code with model.fit etc, I would suggest to check that your tf and nvidia cuda are set up correctly. Per this [description](https://www.tensorflow.org/install/pip) I installed [tf with gpu support](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl) and installed [cuda 11](https://www.tensorflow.org/install/gpu). I run the training and confirm that gpu is used and it is fluctuating between 3% and 50%, and if I increase the batch size (to 512) then peak gpu usage goes to 70%. The changes in gpu usage is due to the lack of data pre-fetching in this lib. When you train the model please check printed logs in the beginning of training: it should say that cuda libs are loaded, if there is any error in *.so loading then training will fall back to cpu training.",model training data code would suggest check set correctly per description support run training confirm used increase batch size peak usage go usage due lack data train model please check printed beginning training say loaded error loading training fall back training,issue,negative,negative,neutral,neutral,negative,negative
828922866,"> > @hyhieu Thanks for your great work! I really admire you!
> > Could you please explain more about the Equation (10), how can I derive it from REINFORCE equation?
> > Many thanks!
> > ![Untitled](https://user-images.githubusercontent.com/71539555/106589173-4dc56980-657e-11eb-9eff-219f21e8d547.png)
> 
> The REINFORCE equation is applied to go from the first line to the second line, moving the $\frac{\partial}{\partial \theta_T}$ from outside the expectation to inside the expectation with the extra $\log$ term.

As I saw the trick in your [link](http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/), the expectation of a function f is formulated as follow:
![Screenshot from 2021-04-29 10-26-51](https://user-images.githubusercontent.com/71539555/116499150-9e2a3a00-a8d5-11eb-8f0e-9bb10ffb68f0.png)
Because computing this formular is difficult, they exchanged the derivative and the integral as Leibniz integral rule:
![Screenshot from 2021-04-29 10-28-24](https://user-images.githubusercontent.com/71539555/116499163-ad10ec80-a8d5-11eb-9e46-e02edc8334a0.png)
But to perform this transformation, the prerequisite condition is that the function `f(z)` has no dependency on the variable `theta`.

As stated in your paper, ![Screenshot from 2021-04-29 10-43-47](https://user-images.githubusercontent.com/71539555/116500018-d29ef580-a8d7-11eb-8717-b7515eaa6e9e.png) has dependency on `theta_T` via `y^_u`:
![Screenshot from 2021-04-29 10-50-32](https://user-images.githubusercontent.com/71539555/116500437-c49da480-a8d8-11eb-824a-36e00c19f738.png)

Therefore, I think that the aforementioned condition is kinda broken here. So how can you still apply the above trick?



",thanks great work really admire could please explain equation derive reinforce equation many thanks untitled reinforce equation applied go first line second line moving outside expectation inside expectation extra term saw trick link expectation function follow formular difficult derivative integral integral rule perform transformation prerequisite condition function dependency variable theta stated paper dependency via therefore think condition broken still apply trick,issue,positive,positive,positive,positive,positive,positive
828152404,"![image](https://user-images.githubusercontent.com/38975880/116350427-8181f980-a824-11eb-8e8c-5370bca3233e.png)
Besides this one, I got another error when I‘m handling bwaves.",image besides one got another error handling,issue,negative,neutral,neutral,neutral,neutral,neutral
827710671,"The generalization experiment was done with data augmentation turned on, similar to other pre-training setups in the table. The flyingthings model does not have data augmentations enabled.",generalization experiment done data augmentation turned similar table model data,issue,negative,neutral,neutral,neutral,neutral,neutral
827429107,"Thanks very much for your reply @vtankovich,

I was interested in the generalization performance as reported in the Additional Evaluations B.2 Generalization. I would like to know how I can obtain similar values to those reported in Table 5. When I tried to run KITTI images with the model trained on SceneFlow, results are very far from the ones of the table (I get EPE around 40). Am I missing something?

Thanks
Sergio",thanks much reply interested generalization performance additional generalization would like know obtain similar table tried run model trained far table get around missing something thanks,issue,positive,positive,positive,positive,positive,positive
827335975,Thanks for noticing this. This issue should be fixed now by this [commit](https://github.com/google-research/google-research/commit/f3332cf769f20b1a4fffa9fecc630af00aab902a). Please pull the newest version and have a try. ,thanks issue fixed commit please pull version try,issue,positive,positive,positive,positive,positive,positive
827189434,"> Hi,
> 
> Thanks for publishing this amazing work.
> I have downloaded the FlyingThings model using the provided script. When I feed the model with FlyingThings data I obtain correct and sharp disparity predictions. Nevertheless, if I feed it KITTI data, I obtain incredibly wrong outputs, not even close to the expected predictions.
> 
> Is there any reason your model does not work with images from a different dataset? Do I have to apply some sort of preprocessing before feeding the data to the network?
> 
> Thanks in advance,
> Sergio

The flyingthings model is trained on synthetic only, and is not expected to work on kitti or other real datasets. Currently eth3d and middlebury models are available and are the same ones that were used to generate data for benchmark submission. The middlebury model is pre-trained on flyingthings, as described in paper.",hi thanks amazing work model provided script feed model data obtain correct sharp disparity nevertheless feed data obtain incredibly wrong even close reason model work different apply sort feeding data network thanks advance model trained synthetic work real currently available used generate data submission model paper,issue,positive,positive,positive,positive,positive,positive
827188542,"> I meet the same problem.
> The error of pb model on sceneflow dataset is higher than the result reported in the hitnet paper.
> @serizba Do you meet the same thing?

Have you ran the evaluate_flyingthings.sh? against TEST set of flyingthings? What numbers have you got at the end?  ",meet problem error model higher result paper meet thing ran test set got end,issue,negative,positive,positive,positive,positive,positive
827010276,"@rybakov 
>I would suggest to run ""watch nvidia-smi"" to see how GPU usage is changing in time to validate that it is using GPU.
>
>To increase the gpu usage you will need to modify data provider and training loop, for example use model.fit() instead of train_on_batch().

I did `watch nvidia-smi` and confirmed the GPU running a training of kws_streaming kept 0%.
And it seems not to work just by changing from train_on_batch() to fit().
I think it's as you mentioned.

>There are two options for streaming conversion:

I can't see the link of data provider you gave me but do you mean if I want to train some models eagerly with high GPU usage, I need to make a training loop by scratch following 1?
If so, can you give me some example to write?
 (I guess your project has the script that can be an example)",would suggest run watch see usage time validate increase usage need modify data provider training loop example use instead watch confirmed running training kept work fit think two streaming conversion ca see link data provider gave mean want train eagerly high usage need make training loop scratch following give example write guess project script example,issue,positive,positive,positive,positive,positive,positive
826623325,"Sorry for the late reply. We are not intending to training the first tuple extract model for now, so we just skip the data acquisition step. The  [checkpoint](https://storage.googleapis.com/gresearch/seq2act/ccg3-transformer-6-dot_product_attention-lr_0.003_rd_0.1_ad_0.1_pd_0.2.tar.gz) provided in the [README.md](https://github.com/google-research/google-research/tree/master/seq2act) contains both the two model and can do all work we wanted, you can try that too. If you want to discuss about this work,  just send me a email: 452784131@qq.com",sorry late reply intending training first extract model skip data acquisition step provided two model work try want discus work send,issue,negative,negative,negative,negative,negative,negative
826147272,"In kws_streaming our focus was mostly on streaming functionality, so you can plug it in any training pipeline. Training loop in this lib was not optimized: there is no data pre-fetching that is why GPU usage will vary in time.
I would suggest to run ""watch nvidia-smi"" to see how GPU usage is changing in time to validate that it is using GPU.

To increase the gpu usage you will need to modify [data provider](https://source.corp.google.com/piper///depot/google3/third_party/google_research/google_research/kws_streaming/data/input_data.py) and training loop, for example use model.fit() instead of train_on_batch().

We stick with above training loop and data provider for compatibility with [tf example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands) and [hello edge](https://github.com/ARM-software/ML-KWS-for-MCU) work, so that we could compare them side by side.

There are two options for streaming conversion:
1 Based on keras functional api, shown in [models](https://github.com/google-research/google-research/tree/master/kws_streaming/models) folder. It propagates streaming buffer automatically through the model. But it works only in session/graph mode (due to constrains in keras: input layers creation is not allowed inside of a layer in eager mode).

2 Based on keras subclass api, shown in [models_sub](https://github.com/google-research/google-research/tree/master/kws_streaming/models_sub) folder. User will have to manage and propagate streaming buffers manually as show in [function](https://github.com/google-research/google-research/blob/master/kws_streaming/models_sub/conv_model.py#L95). It works in eager mode. But it is not integrated with the training loop of this lib, because current training loop is based on session/graph mode due to [data provider](https://source.corp.google.com/piper///depot/google3/third_party/google_research/google_research/kws_streaming/data/input_data.py) implementation, which can be converted to eager mode too - it is out of scope of our current project.",focus mostly streaming functionality plug training pipeline training loop data usage vary time would suggest run watch see usage time validate increase usage need modify data provider training loop example use instead stick training loop data provider compatibility example hello edge work could compare side side two streaming conversion based functional shown folder streaming buffer automatically model work mode due input creation inside layer eager mode based subclass shown folder user manage propagate streaming manually show function work eager mode training loop current training loop based mode due data provider implementation converted eager mode scope current project,issue,positive,positive,neutral,neutral,positive,positive
826079079,"Greetings Elad,

 if possible it would be great if you could upload the tensorflow checkpoints to replicate the results  for the model (T) in table 14 of your paper https://arxiv.org/pdf/2006.04902.pdf.

The files should be two and similar to the checkpoints of this repository https://github.com/ppliuboy/SelFlow/tree/master/models/Sintel

thanks for your reply,
Stefano",possible would great could replicate model table paper two similar repository thanks reply,issue,positive,positive,positive,positive,positive,positive
825743477,"Hi Jo,

This is actually not my code, my name there only appears for
technical reasons. Please refer to this
<https://github.com/google-research/google-research/tree/master/graph_embedding/dmon#readme>
page for authors .

Elad

On Wed, Apr 21, 2021 at 2:41 AM Jo Richards✅ ***@***.***>
wrote:

> Dear Mr. @eladeban <https://github.com/eladeban>, I would like to follow
> your unsupervised algorithm *DMoN* from the Repo
> <https://github.com/google-research/google-research/tree/master/graph_embedding/dmon>,
> but found that it only supports node feature inputs. Does the model support
> modification to support input of *edge* features for learning. As from
> our data perspective, both edge and node features are critical. Really
> appreciate your work!
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/678>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AJTI4NRTGGFFHIXSJHXXLGLTJ2MTRANCNFSM43J7UIUQ>
> .
>


-- 

Elad Eban  |  Research Scientist  |  ***@***.*** |  go/reModel
",hi jo actually code name technical please refer page wed jo wrote dear would like follow unsupervised algorithm found node feature model support modification support input edge learning data perspective edge node critical really appreciate work reply directly view research scientist,issue,positive,positive,neutral,neutral,positive,positive
825742500,"Hi Stefano,

Thanks for taking a look at our work.

I am not sure we have the files and more, what exactly are you looking for?

Best,

Elad

On Wed, Apr 21, 2021 at 5:20 AM Stefano Savian ***@***.***>
wrote:

> Dear authors @AustinCStone <https://github.com/AustinCStone> @eladeban
> <https://github.com/eladeban>,
>
> I would like to benchmark your model, however I cannot find the trained
> model checkpoints.
> Would you be willing to share the files?
>
> Kind Regards,
> Stefano
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/675>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AJTI4NSUBOXYANYV6BTIC33TJ27JLANCNFSM427U7RAQ>
> .
>


-- 

Elad Eban  |  Research Scientist  |  ***@***.*** |  go/reModel
",hi thanks taking look work sure exactly looking best wed wrote dear would like model however find trained model would willing share kind reply directly view research scientist,issue,positive,positive,positive,positive,positive,positive
825430771,"I have the same problems. I found that if I increase the batch size to 1000, only one of the GPUs has increase the Memory usage, yet the GPU_util remains to be 0%. And the training process has been too long. Hope there is a way to increase the GPU usage.",found increase batch size one increase memory usage yet remains training process long hope way increase usage,issue,positive,negative,neutral,neutral,negative,negative
824923556,"@PhilippSchmaelzle Hey, I use `1 / depth[:, :]` because I find that when the pixel is deeper the color predicted shows lighter, which is opposite to their performance, and also many people use this code for inference in this team's another code [depth_from_video_in_the_wild](https://github.com/google-research/google-research/tree/master/depth_from_video_in_the_wild) which I think makes sense.

I have just succeeded in reproduce depth map but the result of scene in wild is not so good. I think the reason is that I just train it in 5000 steps or I did not use Imagenet. But when I use it to train some simple images, it produces a nice result.

**DATASET:** I create my dataset by download [KITTI dataset](http://www.cvlibs.net/datasets/kitti/raw_data.php) and then preprocess these images by [GenDataKITTI_gray](https://github.com/go125/depth_from_video_in_the_wild_eval_2020Winter/blob/main/GenDataKITTI_gray.py), which makes the images gray and put three frames together. 

Besides, you can refer to [depth_from_video_in_the_wild](https://github.com/google-research/google-research/tree/master/depth_from_video_in_the_wild), which is similar to this program and they are made by a same team. Their method to get dataset is same. Also it has a example of training that really inspires me, so I think you can try it and make your dataset like [example](https://github.com/google-research/google-research/tree/master/depth_from_video_in_the_wild/data_example). 

By the way, maybe you'd better comment out the code about `mask` and `intrinsics` in `dataset/reader_cityscapes.py` if you just want to produce a depth map. 

I am not sure whether I have shared my experience clearly, so if you have any other questions, you can ask me. I'll be happy to reply to you.(^_^)",hey use depth find color lighter opposite performance also many people use code inference team another code think sense reproduce depth map result scene wild good think reason train use use train simple nice result create gray put three together besides refer similar program made team method get also example training really think try make like example way maybe better comment code mask want produce depth map sure whether experience clearly ask happy reply,issue,positive,positive,positive,positive,positive,positive
824806917,"@Lostinecho Hey. Np glad it worked out for you!
And thx for your additions. I also used the plasma visualization in my final application. But why do you use 
`(1 / depth[:, :])`? Could you elaborate the reason for this please?

Secondly, have you been able to reproduce their performance and how have you trained your network? I would be interested in sharing experiences if possible :)
Best regards Philipp",hey glad worked also used plasma visualization final application use depth could elaborate reason please secondly able reproduce performance trained network would interested possible best,issue,positive,positive,positive,positive,positive,positive
823853423,"@PhilippSchmaelzle Hey bro. Thanks for your code! I find that your code is to produce grayscale image, so I've done some change to your code to produce depth map like what is shown in the thesis. In function
`def run_local_inference(input_fn, trainer_params_overrides, model_params, vars_to_restore_fn=None)`
after 
`predict_output = estimator.predict(input_fn=input_fn, predict_keys=None, hooks=[init_hook])`
you can `import matplotlib.pyplot as plt` 
and add the code below
 ` depth = predict_output[0, :, :, :]`
    `depth = cv2.resize(depth, (128, 416))`
    `plt.figure(figsize=(30,15))`
    `plt.imshow(1 / depth[:, :], cmap='plasma')`
    `plt.axis('off')`
    `plt.savefig(""infer.png"")`
    `plt.show()` 
and then the code can produce color depth map.
Best Regards
Echo",hey thanks code find code produce image done change code produce depth map like shown thesis function import add code depth depth depth depth code produce color depth map best echo,issue,positive,positive,positive,positive,positive,positive
823460587,"I inserted following code [here](https://github.com/google-research/google-research/blob/2f73ba5cdb6a829f5458f58f94d893e969bf74e8/kws_streaming/train/train.py#L153) and I could see data on TensorBoard.
Actually I know this is not good codes but for now let me share just in case.
I wrote for only the 1st batch of the 1st layer's output.

```
if training_step == start_step
  inp = model.input                                           # input placeholder
  outputs = [layer.output for layer in model.layers][1:]          # all layer outputs except for input layer
  functors = [K.function([inp, K.learning_phase()], [out]) for out in outputs]    # evaluation functions
  layer_outs = [func([train_fingerprints,1.]) for func in functors]
  
  # input data
  loop=0
  for i in train_fingerprints[0]:
    train_writer.add_summary(
        tf.Summary(value=[
                          tf.Summary.Value(
                              tag='input_' +  str(training_step),
                              simple_value=i)
                    ]) ,loop)
    loop = loop + 1
  
  # 1st layer's output
  loop = 0
  for o in layer_outs[0][0][0]:
    train_writer.add_summary(
                  tf.Summary(value=[
                          tf.Summary.Value(
                              tag='output_' +  str(training_step),
                              simple_value=o)
                    ]) ,loop)
    loop = loop + 1
```

I'm going to close this issue soon unless there are any problems but if anyone have better ideas, please share it.
Thank you.",inserted following code could see data actually know good let share case wrote st batch st layer output input layer layer except input layer evaluation input data loop loop loop st layer output loop loop loop loop going close issue soon unless anyone better please share thank,issue,positive,positive,positive,positive,positive,positive
821995708,"> Hi! sorry for the late response, could you send us (Yang Li and me) an email listing your questions, so that we could track the discussion better. Looking forward to it!
> Our emails are listing at the paper https://arxiv.org/abs/2005.03776

Thank you for your reply. I have sent an email to Professor Li, but I have not received a reply yet. I would like to ask if you can provide the crawled_instructions.json file. Although the instructions in README.md have listed all the steps about how to generate the AndroidHowTo Dataset, but the CommonCrawl Dataset is very large and difficult for me the deal with. So if it is convenient for you guys to supply the final crawled_instructions.json file? That will help a lot of people who have situations like me.",hi sorry late response could send u yang li listing could track discussion better looking forward listing paper thank reply sent professor li received reply yet would like ask provide file although listed generate large difficult deal convenient supply final file help lot people like,issue,positive,negative,neutral,neutral,negative,negative
821994655,"> The instructions in [README.md](https://github.com/google-research/google-research/blob/master/seq2act/data_generation/README.md) have listed all the steps about how to generate the `AndroidHowTo` Dataset, but the `CommonCrawl` Dataset is very large and difficult for me the deal with. So if it is convenient for you guys to supply the final `crawled_instructions.json` file? That will help a lot of people who have situations like me.

hi，Is this problem solved? I’m currently studying the paper Mapping-Natural-Language-Instructions-to-Mobile-UI-Action-Sequences. I guess you are too. Can you leave an email? Let’s discuss it.",listed generate large difficult deal convenient supply final file help lot people like problem currently paper guess leave let discus,issue,negative,negative,neutral,neutral,negative,negative
821917476,"I meet the same problem.
The error of pb model on sceneflow dataset is higher than the result reported in the hitnet paper.
@serizba Do you meet the same thing?",meet problem error model higher result paper meet thing,issue,negative,positive,positive,positive,positive,positive
821767975,"OK. Let me try it. It might take for a while.
If you see other examples that look good, please share it kindly. 
Anyway thanks for your advice!",let try might take see look good please share kindly anyway thanks advice,issue,positive,positive,positive,positive,positive,positive
821754649,"There is no need to modify the model to access the outputs of every layer because the model is based on functional keras api so you can iterate over layers and get any output, please have a look at [link](https://stackoverflow.com/questions/41711190/keras-how-to-get-the-output-of-each-layer).",need modify model access every layer model based functional iterate get output please look link,issue,negative,neutral,neutral,neutral,neutral,neutral
820507254,This looks similar as issue #647. There's no resolution as of now though. ,similar issue resolution though,issue,negative,neutral,neutral,neutral,neutral,neutral
820504468,"I am also getting exact same error. I am using docker with below versions of gcc and bazel:
`gcc version 9.3.0 (Ubuntu 9.3.0-11ubuntu0~18.04.1)`  and `bazel 3.4.1`

Not sure why the issue happens. Can someone help?
CC: @estebanre @crazydonkey200 ",also getting exact error docker version sure issue someone help,issue,negative,positive,positive,positive,positive,positive
820256199," > Sampling hard pseudo labels also works, and actually follows our formula. I managed to reproduce the results from the paper with this implementation. [Here](http://tensorboard/experiment/505870852301790830) is the log of the run. Note that this is only the meta pseudo labels training phase; fine-tuning from the last checkpoint on 4,000 labeled examples gave `96.06%`.

Hi, I just found the log has not been available. May I have another link for the copy? Thanks! @hyhieu 

",sampling hard pseudo also work actually formula reproduce paper implementation log run note meta pseudo training phase last gave hi found log available may another link copy thanks,issue,negative,positive,neutral,neutral,positive,positive
819199421,"I think I have used the latest version (as that's the version we work with internally) -- this `run_optimizer_variable_init` version seems like a flag used by DrQ and set to False (so I suppose you should use the mainline version).

Regarding CPU/GPU issue, it is likely that it is caused by the dataset being loaded on the CPU. Since the dataset is a `tf.data.TFRecordDataset`, I thought it should be automatically moved to the GPU but maybe we need an explicit with `with tf.device('/gpu:0')` call when running the contrastive part (also suggested by this [stackoverflow post](https://stackoverflow.com/questions/56008764/tensorflow-dataset-api-not-using-gpu/56026280)). Maybe something like [`prefetch_to_device`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/prefetch_to_device) might help here to fix the issue. Yes, the dataset path seems fine to me otherwise it would thrown error in the data loading part itself. ",think used latest version version work internally version like flag used set false suppose use version regarding issue likely loaded since thought automatically maybe need explicit call running contrastive part also post maybe something like might help fix issue yes path fine otherwise would thrown error data loading part,issue,positive,positive,positive,positive,positive,positive
819152111,"Another unrelated question. The latest and stable tf-agents version is 0.7.1. In this version, the Learner does not support `run_optimizer_variable_init`. This is only supported in the tf-agents mainline. Should we use the tf-agents mainline (or nightly) or tf-agents 0.7.1?",another unrelated question latest stable version version learner support use nightly,issue,positive,positive,positive,positive,positive,positive
819149481,"@agarwl Thanks for your lightning quick response! The machine I'm using is a 4-GPU lambda box. When I was reporting failure, I had `export CUDA_VISIBLE_DEVICES=0,1,2,3`. After I disabled all of them, i.e. `export CUDA_VISIBLE_DEVICES=`, non-zero `contrastive_loss_weight` training seems to be working now without code changes. I'll keep this going a bit.

The confusing part is when `contrastive_loss_weight` was set to zero, training with the use of GPU worked fine. Is it possible the episode data loading caused the CPU/GPU mismatch?

For the code to be able to load the GCP dataset, I added FLAGS.data_dir. This `data_dir` points to `some_path/cartpole-swingup/1/seed_0/`. I thought this was the correct path because the code looks for a file named `episodes2`.

BTW, I also tried `export CUDA_VISIBLE_DEVICES=0` but this also failed.",thanks lightning quick response machine lambda box failure export disabled export training working without code keep going bit part set zero training use worked fine possible episode data loading mismatch code able load added thought correct path code file also tried export also,issue,negative,positive,positive,positive,positive,positive
819128314,"From the error, it seems as if some summary op is complaining about some gpu/cpu placement issue (I am assuming you are using tf2). Some possible suggestions to investigate this issue more: 
 - Have you tried turning off the contrastive loss summary [here](https://github.com/google-research/google-research/blob/master/pse/dm_control/agents/pse_drq_agent.py#L348) (simply use a [OrderedDict](https://github.com/google-research/google-research/blob/8b51f2bcf6799d4885ce5997ed5a5111110c67db/pse/dm_control/agents/pse_drq_agent.py#L32) without contrastive loss)?
 - Try using a pdb at the [statement](https://github.com/google-research/google-research/blob/master/pse/dm_control/agents/pse_drq_agent.py#L328) when non-zero contrastive loss comes into play and see if an error is thrown there. 
- Try running the code on CPU (if that works, it's clear that it's some weird op placement issue). Let me know what you find.",error summary placement issue assuming possible investigate issue tried turning contrastive loss summary simply use without contrastive loss try statement contrastive loss come play see error thrown try running code work clear weird placement issue let know find,issue,negative,negative,neutral,neutral,negative,negative
817333188,"Hello! For your first suggestion to fill in with inf, couldn't you handle by adding inside cal_dtw_distance to ignore inf frames? 
I discussed with co-authors and you could also break this into 2 steps:
1. compute the full dtw distance matrix. (If computation speed is still an issue here, you could try something like https://pypi.org/project/fastdtw/ (we didn't use this approximation, just a suggestion))
2. the pre-computed distance matrix should not too big, even running non-parallel knn search on this matrix should be relatively quick, and this step can also be parallelized by each query. 
",hello first suggestion fill could handle inside ignore could also break compute full distance matrix computation speed still issue could try something like use approximation suggestion distance matrix big even running search matrix relatively quick step also query,issue,negative,positive,positive,positive,positive,positive
817251714,"Hi @Wjk666 , 

Thanks for reaching out! 

We are working on releasing the code. We will update the repo when it is ready. Thanks!

Best,
Long",hi thanks reaching working code update ready thanks best long,issue,positive,positive,positive,positive,positive,positive
816919576,"Do you know the good way to pass each layers output?
Do I have to modify layers modes script like [cnn.py](https://github.com/google-research/google-research/blob/0667f4c48f3920f0fa7ee85a54f5c3def92e4f97/kws_streaming/models/cnn.py)?",know good way pas output modify script like,issue,positive,positive,positive,positive,positive,positive
816886541,"Yes, you can add more log info in [line1](https://github.com/google-research/google-research/blob/0667f4c48f3920f0fa7ee85a54f5c3def92e4f97/kws_streaming/train/train.py#L149) ",yes add log line,issue,negative,neutral,neutral,neutral,neutral,neutral
816852559,"Hi! sorry for the late response, could you send us (Yang Li and me) an email listing your questions, so that we could track the discussion better. Looking forward to it!
Our emails are listing at the paper https://arxiv.org/abs/2005.03776",hi sorry late response could send u yang li listing could track discussion better looking forward listing paper,issue,negative,negative,negative,negative,negative,negative
816810482,"Thanks for your recommendation API.I use KNeighborsClassifier to train and predict.
knn = KNeighborsClassifier(n_neighbors=5,
                           algorithm='auto',
                           metric=cal_dtw_distance)
                          )
In the process of action recognition , each of my samples is a video. I reshape each sample (two dimension) into a one-dimensional matrix with the length of framenum * 16 (where 16 is embedding of frame). Because each video has different number of frames and the length of sample matrix is different, DTW algorithm is needed to align and calculate the distance of each sample.

But the train function(KNeighborsClassifier.fit(X_train, y_train))  needs samples of the same length. At present, I have two ideas to complement them:
1.Fill ∞ number, but the algorithm of kneighbors classifier （algorithm='auto'） will calculate the mean value of the ∞ number and the normal data of other samples when calculating the sample distance
2.Fill each sample into a one-dimensional index list, such as [videoID, frame_ start，frame_ end]。 When running the my distance function to calculate the distance, use the frame_ start，frame_ End find frames in embedding.csv File, get the matrix and calculate the distance. But it's going to be brute-force search again(algorithm='brute') ,which takes a long time..

How do you solve the problem that each training sample of kneighbors classifier model should be the same length (each sample is not the same length due to the number of frames)",thanks recommendation use train predict process action recognition video reshape sample two dimension matrix length frame video different number length sample matrix different algorithm align calculate distance sample train function need length present two complement number algorithm classifier calculate mean value number normal data calculating sample distance sample index list end running distance function calculate distance use end find file get matrix calculate distance going search long time solve problem training sample classifier model length sample length due number,issue,positive,negative,neutral,neutral,negative,negative
816440611,"Ah I haven't noticed [kws_experiments_paper_12_labels.md](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md)!
Thank you for this great environment and your kind responses!
I'm going to reproduce your paper at first.",ah thank great environment kind going reproduce paper first,issue,positive,positive,positive,positive,positive,positive
816364573,"Yes, none of the models presented in our [paper](https://arxiv.org/pdf/2005.06720.pdf) have default parameters of [Hello Edge paper](https://arxiv.org/pdf/1711.07128.pdf). In section ""4.2. Comparison with baseline"" we explicitly say ""We improved their accuracy on datasets V1 and V2, as shown on Table 1, by applying SpecAugment with hyper-parameters optimization of both neural net and speech feature extractor parameters"".

In Table 1 we labeled models from [Hello Edge paper](https://arxiv.org/pdf/1711.07128.pdf) by tag V1*. Star(*) means that it is baseline accuracy reported by [Hello Edge paper](https://arxiv.org/pdf/1711.07128.pdf). We compared their models with our re-implementations which have different hyperparameters. For example strided CNN (""CNN+strd"") parameters are show in [link](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#cnn_stride) are different with hyperparameters of (CNN-2) from [Hello Edge paper](https://arxiv.org/pdf/1711.07128.pdf) implemented in [link](https://github.com/ARM-software/ML-KWS-for-MCU/blob/8151349b110f4d1c194c085fcc5b3535bdf7ce4a/models.py#L643).

If you are interested in reproducing model accuracy from [Hello Edge paper](https://arxiv.org/pdf/1711.07128.pdf), I would suggest to use their [implementations](https://github.com/ARM-software/ML-KWS-for-MCU/blob/8151349b110f4d1c194c085fcc5b3535bdf7ce4a/models.py), so that all hyperparameters will be exactly the same as they are published in [Hello Edge paper](https://arxiv.org/pdf/1711.07128.pdf).

Please feel free to reproduce accuracy of models from [Hello Edge paper](https://arxiv.org/pdf/1711.07128.pdf) with [kws_streaming](https://github.com/google-research/google-research/tree/master/kws_streaming), but as you already noticed you will need to change hyperparameters.",yes none paper default hello edge paper section comparison explicitly say accuracy shown table optimization neural net speech feature extractor table hello edge paper tag star accuracy hello edge paper different example show link different hello edge paper link interested model accuracy hello edge paper would suggest use exactly hello edge paper please feel free reproduce accuracy hello edge paper already need change,issue,positive,positive,positive,positive,positive,positive
816310512,"Which model in the paper 1 does the default parameters of [cnn](https://github.com/google-research/google-research/issues/kws_streaming/model/cnn.py) mean?
It's not neither `cnn_trad_fpool3` (CNN-1) nor `cnn_one_fstride4` (CNN-2), right?",model paper default mean neither right,issue,negative,negative,neutral,neutral,negative,negative
816220949,"There is no need to combine DepthwiseConv2D with the paper you are trying to reproduce.
It looks like you already get similar number parameters in your fist model version.

In case of your second version, where you add DepthwiseConv2D:
your first conv layer
""conv2d (Conv2D)              (100, 1, 33, 54)          13878""
covers whole time span and reduces input time dim of 32 to 1
So after that there is no need to combine DepthwiseConv2D with Conv2D at all. Because they do not do any processing in time dimension:
```
depthwise_conv2d (DepthwiseC (100, 1, 1, 54)           1836      
_________________________________________________________________
conv2d_1 (Conv2D)            (100, 1, 1, 32)           1760    
```
In above you have (100, 1, 1, 54) = (batch=100, time=1, feature1=1, feature2=54)

I would suggest to visualize ds_tc_resnet and see how DepthwiseConv2D and Conv2D are combined.
You will see that time dim on data processed by DepthwiseConv2D will not be equal to 1 

Additional comments about paper citation:
Model [cnn](kws_streaming/model/cnn.py) is based on paper 1: ""Hello Edge: Keyword Spotting on Microcontrollers.""  because  paper 1 bench-marked their model on public data so we can do side by side comparison. I also referred paper 2: ""Convolutional Neural Networks for Small-footprint Keyword Spotting"" as a prior art of conv model applied on key word spotting (and [cnn](kws_streaming/model/cnn.py) is inspired by that), but I did no implement it because it was not benchmarked on public data, so I could not do side by side comparison and validate its performance.",need combine paper trying reproduce like already get similar number fist model version case second version add first layer whole time span input time dim need combine time dimension would suggest visualize see combined see time dim data equal additional paper citation model based paper hello edge spotting paper model public data side side comparison also paper convolutional neural spotting prior art model applied key word spotting inspired implement public data could side side comparison validate performance,issue,positive,positive,neutral,neutral,positive,positive
816124562,"@rybakov 
I think the 1st option should be match with `cnn-one-fpool3` in the [paper](https://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf).
I saw  [ds_tc_resnet.py](https://github.com/google-research/google-research/blob/master/kws_streaming/models/ds_tc_resnet.py) but I might not understand it enough.

Let me share this image from the paper:
<img width=""414"" alt=""Screen Shot 2021-04-08 at 12 20 15 PM"" src=""https://user-images.githubusercontent.com/62040028/114084282-da352680-9864-11eb-9b8c-fa58ce879b2f.png"">

Then I calculated the number of parameters by hand and I got the following results:

*input dimension is (`batch_size`, 32, 40, 1)
| layer   |      Output dim.      |  Params |
|----------|:-------------:|------:|
| conv       |  (`batch_size`, 1, 33, 54) | 13824 |
| maxpool |   (`batch_size`, 1, 11, 54)    |- | 
| linear      |  (`batch_size`, 1,   1, 32) |  19008* |
| dnn | (`batch_size`, 1,   1, 128) |    4096 |
| dnn | (`batch_size`, 1,   1, 128) |    16384 |
| softmax | (`batch_size`, 1,   1, 4) |    512 |

As you can see, the number of parameters except for the linear layer are same as those of the paper. So the calculation way should be ok basically. 
And the linear layer's output dimension looks good because the layers' params after it are correct.
I was not sure how to calculate the parameters of DepthwiseConv2D so I tried to build a model for 12 labels anyway.

the part of the linear layer:
```
  net = tf.keras.layers.DepthwiseConv2D(
          kernel_size=(1,33),
          padding='VALID')(net)
  net = tf.keras.layers.Conv2D(
          kernel_size=1,
          filters=32,
          padding='VALID')(net)
``` 
I saw [ds_tc_resnet.py](https://github.com/google-research/google-research/blob/master/kws_streaming/models/ds_tc_resnet.py) and found `kernel_size=(kernel_size, 1)` but in the case of `cnn_one_fpool3`, I guessed (1, 33) was good.

and the model summary:
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(100, 16000)]            0         
_________________________________________________________________
speech_features (SpeechFeatu (100, 32, 40)             0         
_________________________________________________________________
tf_op_layer_ExpandDims (Tens [(100, 32, 40, 1)]        0         
_________________________________________________________________
conv2d (Conv2D)              (100, 1, 33, 54)          13878     
_________________________________________________________________
max_pooling2d (MaxPooling2D) (100, 1, 33, 54)          0         
_________________________________________________________________
depthwise_conv2d (DepthwiseC (100, 1, 1, 54)           1836      
_________________________________________________________________
conv2d_1 (Conv2D)            (100, 1, 1, 32)           1760      
_________________________________________________________________
dropout (Dropout)            (100, 1, 1, 32)           0         
_________________________________________________________________
flatten (Flatten)            (100, 32)                 0         
_________________________________________________________________
dense (Dense)                (100, 128)                4224      
_________________________________________________________________
dense_1 (Dense)              (100, 128)                16512     
_________________________________________________________________
flatten_1 (Flatten)          (100, 128)                0         
_________________________________________________________________
dense_2 (Dense)              (100, 12)                 1548      
=================================================================
Total params: 39,758
Trainable params: 39,758
Non-trainable params: 0
_________________________________________________________________
* the last layer's output is different from the table above because this is the model for 12 labels.
```

`depthwise_conv2d` and `conv2d_1` should work as the linear layer but the number of parameters are far from the paper.

Could you tell me if you find wrong things in my code?",think st option match paper saw might understand enough let share image paper screen shot calculated number hand got following input dimension layer output dim linear see number except linear layer paper calculation way basically linear layer output dimension good correct sure calculate tried build model anyway part linear layer net net net net saw found case good model summary layer type output shape param dropout dropout flatten flatten dense dense dense flatten dense total trainable last layer output different table model work linear layer number far paper could tell find wrong code,issue,positive,positive,positive,positive,positive,positive
814964194,"@sML-90 I don't think this should be a CuBERT specific thing, and should be able to access the embeddings using the normal API. For example the [BertModel](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) and it's subclasses (like BertForSequenceClassification) takes a output_hidden_states argument, and it should give you the embeddings at every layer.",think specific thing able access normal example like argument give every layer,issue,negative,positive,positive,positive,positive,positive
814130257,"@PhilippSchmaelzle Thanks for your help! It is very kind of you to reply me in detail, because this problem has bug me for so long. I will try it right now!",thanks help kind reply detail problem bug long try right,issue,positive,positive,positive,positive,positive,positive
814111410,"Hey, I did some progress which seems to work fine. The following is work in progress and copy pasted from my current tests, so it is not nice code or a nice implementation, hard coded paths ... 
But it may help you.

### Relevant for inference
[My Fork of the Repo with changes for testing purposes](https://github.com/PhilippSchmaelzle/mono_depth/tree/inference_with_original_code_structure)

I did there training specific changes so the training would run with my image data. But I guess most of it is not necessary any more. But for inference, you should be able to train something first.

- `depth_and_motion_learning/infer_all_func_in_one.py`: contains the methods I used to get a *SavedModel* model for depth prediction only!
- `run_predict`: how I called it then. Quick 'n' dirty
- `depth_and_motion_learning/send_data_client.py`: helper to send images to the *TensorFlow Serving Docker Container* where the inference is running inside.

Guess that was all.
Below copy 'n' paste of my README how I now use the final trained and exported model.

### Unsorted Link collection which helped me
https://github.com/google-research/google-research/tree/master/depth_and_motion_learning
https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/17_Estimator_API.ipynb
https://towardsdatascience.com/an-advanced-example-of-tensorflow-estimators-part-1-3-c9ffba3bff03
https://stackoverflow.com/questions/53410469/tensorflow-estimator-servinginputreceiver-features-vs-receiver-tensors-when-and
https://branyang.gitbooks.io/tfdocs/content/get_started/export.html#specifying_the_outputs_of_a_custom_model
https://stackoverflow.com/questions/51996732/whats-the-difference-between-predictions-and-export-outputs-of-tensorflow-custo

#### tf serving
https://stackoverflow.com/questions/53356029/tensorflow-v1-10-serving-custom-estimator
https://www.tensorflow.org/tfx/serving/serving_basic
https://towardsdatascience.com/how-to-deploy-machine-learning-models-with-tensorflow-part-2-containerize-it-db0ad7ca35a7
https://github.com/Vetal1977/tf_serving_example/blob/master/HowTo.md
https://www.tensorflow.org/tfx/serving/docker

### How I used TF Serving:

## Set Up
### Docker
Refer to [docker installation guide.](/core/doc/docker.md#installation)
To enabe GPU usage with Docker, one has to install the 
[NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html ""NVIDIA Cnontiner Toolkit"") as well.

### TensorFlow Serving
After *Docker* is up and running, one can start to use the *TensorFlow Serving* framework.

Pull the *TensorFlow Serving* container. It is possible to define the supported version as label.
This is necessary to fit the *Serving* framework to the underlaying versions of TensorFlow, CUDA and cudnn.
**Version numbers and pull tags can be looked up here:
[Version Release Info](https://github.com/tensorflow/serving/releases ""Release Version Info""), 
[Docker Pull Tags](https://hub.docker.com/r/tensorflow/serving/tags/?page=1&ordering=last_updated ""Docker Version Pull Tags"")**

E.g.:
```
// Nuvo PC, Ubuntu 16.04.,TensorFlow 1.1.0, CUDA 10.0, cudnn 7
sudo docker pull tensorflow/serving:1.15.0-gpu

// Ubuntu 20.04., TensorFlow 2.2.o, NO GPU
sudo docker pull tensorflow/serving:2.2.0
```
Now a runnable Docker image for TensorFlow Serving is available.
The easiest but also inflexible way of usage (which is enough for this use case) will be described within [Example Usage](#example-usage ""Example Usage"")

## Example Usage
To do inference on raw images, the previously pulled Docker image needs to be run. 
Therefore, the following command can be used.

```
// CPU
sudo docker run -p 8500:8500 --mount type=bind,source=<PATH_TO_SAVED_MODEL_STRUCT__MODEL_NAME>,target=/models/<MODEL_NAME> -e MODEL_NAME=<MODEL_NAME> -t tensorflow/serving:2.2.0 &

// GPU
sudo docker run --runtime=nvidia -p 8500:8500 --mount type=bind,source=PATH_TO_SAVED_MODEL_STRUCT__MODEL_NAME>,target=/models/<MODEL_NAME> -e MODEL_NAME=<MODEL_NAME> -t  tensorflow/serving:2.2.0-gpu &
```

The mounted directory needs to fulfill the TensorFlow *SavedModel* structure.
- `ModelName`: Directories name defines the model name
- `Model Versioning`: Subdirectories names **have to** be integer number. (No need to start with 0 or increments of one). 
This gives the chance to have a model history. TensorFlow Serving will automatically start the latest/highest if no other version
  is defined.
- `saved_model.pb` and `variables`: output of [SavedModel](https://www.tensorflow.org/guide/saved_model ""SavedModel Guide"")
```
|-- ModelName
|    |
|    |-- 0 
|    |-- 1
|    |-- ...
|    |-- N
|    |    |-- saved_model.pb
|    |    |-- variables
|    |    |    |--variables.index
|    |    |    |-- variables.data-00000-of-00001
|
```

",hey progress work fine following work progress copy pasted current nice code nice implementation hard may help relevant inference fork testing training specific training would run image data guess necessary inference able train something first used get model depth prediction quick dirty helper send serving docker container inference running inside guess copy paste use final trained model unsorted link collection serving used serving set docker refer docker installation guide installation usage docker one install container well serving docker running one start use serving framework pull serving container possible define version label necessary fit serving framework underlaying version pull version release release version docker pull docker version pull docker pull docker pull runnable docker image serving available easiest also inflexible way usage enough use case within example usage example usage example usage inference raw previously docker image need run therefore following command used docker run mount docker run mount mounted directory need fulfill structure name model name model integer number need start one chance model history serving automatically start version defined output guide,issue,positive,positive,neutral,neutral,positive,positive
813931225,"Hi, did anyone find a way to do inference? I also did not know how to modify this code to get prediction.
Best Regards
Echo",hi anyone find way inference also know modify code get prediction best echo,issue,positive,positive,positive,positive,positive,positive
813046310,"Hi, did you find a solution to this? I also did not find any files about inference.",hi find solution also find inference,issue,negative,neutral,neutral,neutral,neutral,neutral
812799863,"Does your KNN implementation use parallel jobs? It seems that the sklearn implementation here (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) could be helpful. You could pre-compute the distance matrix and that would probably make it a lot faster. 
On our end it was ~25 minutes with parallelization. ",implementation use parallel implementation could helpful could distance matrix would probably make lot faster end parallelization,issue,negative,neutral,neutral,neutral,neutral,neutral
812285721,"Thank you for your suggestion. I have finished the code work of the evaluation. 
However, every test sample needs to go through KNN and DTW algorithm with all train samples , which takes a long time. 
How long did it take you to complete action recognition. Is there any way to accelerate it?",thank suggestion finished code work evaluation however every test sample need go algorithm train long time long take complete action recognition way accelerate,issue,negative,positive,neutral,neutral,positive,positive
809863039,"There are two options of doing low rank matrix decomposition:
1. In time and feature domains, well described in [END-TO-END STREAMING KEYWORD SPOTTING](https://arxiv.org/pdf/1812.02802.pdf) it is implemented in [svdf](https://github.com/google-research/google-research/blob/master/kws_streaming/models/svdf.py) and in [ds_tc_resnet](https://github.com/google-research/google-research/blob/master/kws_streaming/models/ds_tc_resnet.py). Here 2D matrix is decomposed into product of two low rank matrices implemented with depthwise convolution (applied in time dimension) and 1x1 convolution (applied in feature dimension).
2. In feature domain only: just represent fully connected layer (e.g 100x100) as two fully connected layers (e.g 100x40 and 40x100).",two low rank matrix decomposition time feature well streaming spotting matrix decomposed product two low rank matrix depthwise convolution applied time dimension convolution applied feature dimension feature domain represent fully connected layer two fully connected,issue,negative,negative,negative,negative,negative,negative
809493152,"Thank you @rybakov!

How can I make a linear low-rank layer?
I understood [this article on stackExchange](https://stats.stackexchange.com/questions/365179/what-is-low-rank-linear-layer-in-neural-networks),
but I'm not sure how to implement it with Tensorflow.
If I do Flatten() and Dense(), the parameters are too many.
",thank make linear layer understood article sure implement flatten dense many,issue,positive,positive,positive,positive,positive,positive
809299626,"Thank you, you must put a lot of effort on this paper^ ^",thank must put lot effort,issue,negative,neutral,neutral,neutral,neutral,neutral
809185991,"> Hi, when sampling the third pixel , after shifting right ,the i

Indeed, nice catch. I fixed it.

> The annotation should be rows and column respectively?

Yes. [1, num_columns] is row-attention. In this case, self-attention is applied to each row independent of the others. Similar for column attention. I'll fix the documentation

> The parameter num_encoder_layers is 1

We used 4 blocks in the papers. The configurations used for the experiments in the paper are here (https://github.com/google-research/google-research/blob/master/coltran/configs/colorizer.py#L60).
Yes, we apply row and column attention as you described. The code for the ""SelfAttentionND"" layer can be made simpler, but they are heavily optimized for TPU's",hi sampling third shifting right indeed nice catch fixed annotation column respectively yes case applied row independent similar column attention fix documentation parameter used used paper yes apply row column attention code layer made simpler heavily,issue,positive,positive,neutral,neutral,positive,positive
809165751,"Hello @DNGros 
Thank you for the above info on loading the pre-trained tf checkpoints with huggingface.  I tried as you said and I could convert the model to pytorch. Could you help me on how to use this model to generate embeddings of the inputs as we do with Huggingface LMs.

Thanks in advance!",hello thank loading tried said could convert model could help use model generate thanks advance,issue,positive,positive,positive,positive,positive,positive
809079565,"Thank you for evaluating kws_streaming lib!

For cnn model we referred two papers: [Convolutional Neural Networks for Small-footprint Keyword Spotting](http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf) and [Hello Edge: Keyword Spotting on Microcontrollers](https://arxiv.org/pdf/1711.07128.pdf), the first one as earliest cnn application and the second one is more similar to what we use in the code. So I would say that cnn model is inspired by [Convolutional Neural Networks for Small-footprint Keyword Spotting](http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf) and is based on [Hello Edge: Keyword Spotting on Microcontrollers](https://arxiv.org/pdf/1711.07128.pdf). If you need max pooling, I would suggest to try [ds_tc_resnet](https://github.com/google-research/google-research/blob/master/kws_streaming/models/ds_tc_resnet.py) option, here you can configure you model with and without max pooling.

net = stream.Stream(cell=tf.keras.layers.Flatten())(net) - streaming of flatten layer - it is not linear low-rank layer

net = tf.keras.layers.Dropout(rate=flags.dropout1)(net) is a dropout regularization.
",thank model two convolutional neural spotting hello edge spotting first one application second one similar use code would say model inspired convolutional neural spotting based hello edge spotting need would suggest try option configure model without net net streaming flatten layer linear layer net net dropout regularization,issue,positive,positive,neutral,neutral,positive,positive
808709303,"Thank you @afathi3. I opened a new issue asking you if you can provide a detailed example from scratch about the usage of tf3d. Anyway. Suppose I have a LiDAR dataset (x,y,z,r) where r is the reflectance, this comes from semantic KITTI (which is not a dataset considered on this repository), and I have labels associated to each of these points. What I would like to do is using your U-Net to perform semantic segmentation or in general to use the sparse convolution. So the first thing is voxelize the input point cloud and the corresponding labels, I show you my attempt:

```
    data_root = ""../data/sequences""
    data = glob.glob(os.path.join(data_root, ""00/velodyne/*.bin""))
    labels = glob.glob(os.path.join(data_root, ""00/labels/*.label""))

    ptc = load_velo_scan(data[0])
    lab = load_velo_label(labels[0])

    lab = tf.constant(lab, dtype=tf.int32)
    # lab = instance_segmentation_utils.map_labels_to_0_to_n(lab) ---> TOO SLOW
    #voxel_lab = preprocessor_utils.voxelize_semantic_labels(lab, None, (0.5, 0.5, 0.5)) --> DON'T KNOW HOW IT WORKS (Cannot convert 'object_class_points' to EagerTensor of dtype int32)

    # Encode 3D coordinates
    points = tf.expand_dims(tf.constant(ptc[:, 0:3], dtype=tf.float32), axis=0)
    # Encode reflectance
    features = tf.expand_dims(tf.constant(ptc[:, 3], dtype=tf.float32), axis=0)
    features = tf.expand_dims(features, axis=-1)
    num_valid_points = tf.constant([len(ptc)], dtype=tf.int32)
    grid_cell_size = (0.5, 0.5, 0.5)
    (voxel_features, voxel_indices, num_valid_voxels, segment_ids,
     voxel_start_locations) = voxel_utils.pointcloud_to_sparse_voxel_grid(
        points=points,
        features=features,
        num_valid_points=num_valid_points,
        grid_cell_size=grid_cell_size,
        voxels_pad_or_clip_size=None,
        segment_func=tf.math.unsorted_segment_max)
```",thank new issue provide detailed example scratch usage anyway suppose reflectance come semantic considered repository associated would like perform semantic segmentation general use sparse convolution first thing input point cloud corresponding show attempt data data lab lab lab lab lab slow lab none know work convert encode encode reflectance,issue,positive,positive,positive,positive,positive,positive
808603332,"FYI for anyone in the future, I also had to replace some

```
import tensorflow as tf
```

with

```
import tensorflow.compat.v1 as tf
```
in order to make everything work.",anyone future also replace import import order make everything work,issue,negative,neutral,neutral,neutral,neutral,neutral
808214714,"i haved singed


------------------&nbsp;原始邮件&nbsp;------------------
发件人:                                                                                                                        ""google-research/google-research""                                                                                    ***@***.***&gt;;
发送时间:&nbsp;2021年3月26日(星期五) 晚上9:19
***@***.***&gt;;
抄送:&nbsp;""な法--言说 ***@***.******@***.***&gt;;
主题:&nbsp;Re: [google-research/google-research] attentive learning table (#655)





 
Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).
 
📝 Please visit https://cla.developers.google.com/ to sign.
 
Once you've signed (or fixed any issues), please reply here with @googlebot I signed it! and we'll verify it.
 
 
What to do if you already signed the CLA
 
Individual signers
  
It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.
  
Corporate signers
  
Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).
 
The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.
 
The email used to register you as an authorized contributor must also be attached to your GitHub account.
  
ℹ️ Googlers: Go here for more info.
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.",singed attentive learning table thanks pull request like may first contribution open source project look help look pull request need sign contributor license agreement memo please visit sign fixed please reply verify already individual possible different address commit check data verify set git corporate company point contact authorized participate ask added group authorized know point contact direct project maintainer public version used register authorized contributor must used git commit check data verify set git used register authorized contributor must also attached account information go thread reply directly view,issue,positive,positive,neutral,neutral,positive,positive
808046307,"Hi [coltran/models/layers.py from L659 to L662](https://github.com/MechCoder/google-research/blob/d61d09ebb669542ca91113c22d7323ba34e338d4/coltran/models/layers.py#L659) The parameter **num_encoder_layers** is 1, so there are only one unmask_row and one unmask_col attentionBlock, so in unmask_row layer, your use attention for all rows separately and in unmask_col layer your use attention for all cols separately ?  

> It performs attention over axis k of the tensor x, mixing information along axis k while keeping information along other axes independent. It is straightforward to implement: axial attention over axis k can be implemented by transposing all axes except k to the batch axis, calling standard attention as a subroutine, then undoing the transpose (an alternative is to use the einsum operation available in most deep learning libraries).

and you perform attention over every row and col in this way  described above?

sorry , I should have understood this by looking at the code，but I have spent a whole day still not clearing my thoughts and thank you for your selfless help ",hi parameter one one layer use attention separately layer use attention separately attention axis tensor information along axis keeping information along ax independent straightforward implement axial attention axis ax except batch axis calling standard attention undoing transpose alternative use operation available deep learning perform attention every row col way sorry understood looking spent whole day still clearing thank selfless help,issue,positive,positive,neutral,neutral,positive,positive
808018096,"Hi @Wayne-wonderai and @michaellin99999,
Since 3090 is a relatively new card I suggest checking the minimum CUDA  and driver version (the 2.3.0 tf docker is cuda 10.1).
Besides, please try a few lines of code that use tf on gpu to test it out inside the docker first before retrying our sparse conv test.
Thanks",hi since relatively new card suggest minimum driver version docker besides please try code use test inside docker first sparse test thanks,issue,positive,positive,positive,positive,positive,positive
808015220,"Hi @Wayne-wonderai,
This below URL seems to work. Not sure why another one from mirror.bazel.build failed. 
Basically, the string is the git commit.  See [here](https://github.com/google-research/google-research/blob/master/tf3d/ops/repo.bzl#L29). Please try removing the first URL there and maybe the download will go through. Feel free to replace it with tf 2.3.0's git commit and their sha256 hash value [here](https://github.com/google-research/google-research/blob/master/tf3d/ops/WORKSPACE#L7).
Thanks.

https://github.com/tensorflow/tensorflow/archive/fcc4b966f1265f466e82617020af93670141b009.tar.gz",hi work sure another one basically string git commit see please try removing first maybe go feel free replace git commit sha hash value thanks,issue,positive,positive,positive,positive,positive,positive
808011772,"The pre-trained ResNet-18 model used by struct2depth / depth_from_video_in_the_wild / depth_and_motion_learning is the same as [this](https://github.com/dalgu90/resnet-18-tensorflow.git)

```bash
git clone https://github.com/dalgu90/resnet-18-tensorflow.git
cd resnet-18-tensorflow
wget https://d2j0dndfm35trm.cloudfront.net/resnet-18.t7

# Optional if necessary:
#   pip install torchfile
#
#   edit extract_torch_t7.py:
#     import cPickle as pickle --> import pickle  # https://github.com/JuliaPy/PyCallJLD.jl/issues/6#issuecomment-421732697

python extract_torch_t7.py

# The generated:
#    ./init/
#        model.ckpt.data-00000-of-00001
#        model.ckpt.index
#        model.ckpt.meta
# could be used for training depth_and_motion_learning.
```",model used bash git clone optional necessary pip install edit import pickle import pickle python could used training,issue,negative,neutral,neutral,neutral,neutral,neutral
808005555,"Hi, I am running into the same issue. any help would be greatly appreciated. ",hi running issue help would greatly,issue,negative,positive,positive,positive,positive,positive
807875777,"> > When training the input of the outer decoder is ground truth coarse color low resolution image. when testing ,what's the input of the Outer Decoder?
> 
> Ignore the encoder for now, since the grayscale image is present at both during training and evaluation.
> Let's say we want to colorize a 2x2 image. As you said, the ground truth is the input to the outer decoder. I use a, b, c and d to denote both the values and locations of the first four pixels.
> 
> a	b
> c	d
> # Training
> **Outputs: Outer Decoder**
> (shifted down)
> 
> 0	0
> a	b
> **Inputs: Inner Decoder**
> Ground truth is shifted right:
> 
> 0	a
> 0	c
> Net inputs to the inner decoder are (outer decoder output + ground truth shifted right), i.e
> 
> 0	a
> a	b + c
> # Sampling
> ## First pixel (Row 1, Column 1)
> Note that at training, the input location to the first pixel to both decoders are 0
> 
> Inputs are initialized to zero, i.e
> 
> 0	0
> 0	0
> **Outputs: Outer Decoder**
> (shifted down)
> 
> 0	0
> 0	0
> **Inputs: Inner Decoder**
> Ground truth shifted right + outputs of the outer decoder which is still zero, i.e
> 
> 0	0
> 0	0
> Now, we sample value **a** for the first pixel
> 
> ## Second pixel (Row 1, Column 2)
> Note that at training, the input location to the inner decoder for the second pixel is **a**
> 
> After sampling the first pixel, inputs are now:
> 
> a	0
> 0	0
> **Outputs: Outer Decoder**
> (shifted down)
> 
> 0	0
> a	0
> **Inputs: Inner Decoder**
> Ground truth shifted right:
> 
> 0	a
> 0	0
> Added to outputs of the outer decoder:
> 
> 0	a
> a	0
> The masking ensures that the second row is not attended to. Now, we sample value **b** for the second pixel
> 
> ## Third pixel (Row 2, Column 1)
> Note that at training, the input location to the inner decoder for the third pixel is **a**
> 
> After sampling, the second pixel, inputs are now:
> 
> a	b
> 0	0
> **Outputs: Outer Decoder**
> (shifted down)
> 
> 0	0
> a	b
> **Inputs: Inner Decoder**
> Ground truth shifted right:
> 
> 0	a
> 0	b
> Added to outputs of the outer decoder:
> 
> 0	a
> a	b
> Now, we sample value **c** for the third pixel
> 
> ## Fourth pixel (Row 2, Column 2)
> Note that at training, the input location to the inner decoder for the fourth pixel is **b+c**
> 
> After sampling, the third pixel, inputs are now:
> 
> a	b
> c	0
> **Outputs: Outer Decoder**
> (shifted down)
> 
> 0	0
> a	b
> **Inputs: Inner Decoder**
> Ground truth shifted right:
> 
> 0	a
> 0	c
> Added to outputs of the outer decoder:
> 
> 0	a
> a	b + c
> Now, we sample value **d** for the final pixel
> 
> The sampling strategy is not obvious, and unfortunately it is not easy to explain in a paper.


Hi, when sampling the third pixel , after shifting right  ,the image should be?  ^ ^
> 
> 0	a
> 0	0
",training input outer ground truth coarse color low resolution image testing input outer ignore since image present training evaluation let say want colorize image said ground truth input outer use denote first four training outer inner ground truth right net inner outer output ground truth right sampling first row column note training input location first zero outer inner ground truth right outer still zero sample value first second row column note training input location inner second sampling first outer inner ground truth right added outer second row sample value second third row column note training input location inner third sampling second outer inner ground truth right added outer sample value third fourth row column note training input location inner fourth sampling third outer inner ground truth right added outer sample value final sampling strategy obvious unfortunately easy explain paper hi sampling third shifting right image,issue,positive,positive,neutral,neutral,positive,positive
806855382,"OMG, you are really a friendly and patient person. I can no longer express my gratitude to you in words. Your help to me is really great. I wish you a happy life every day.^ ^",really friendly patient person longer express gratitude help really great wish happy life every,issue,positive,positive,positive,positive,positive,positive
806497848,"> The low resolution grayscale image enter the grayscale encoder , after n layer row/col attention layers , generated a context representation C , and the grayscale encoder also has an auxiliary parallel model as show in your paper figure2( Is it the linear and softmax layer? and as you answered before the parallel here is also means models of all the pixels in one forward pass? ) models a distribution pc hat , and the context C will as a condition to the outer decoder‘s attetion layers and bias the outer decoder's input . The geaysacale'encoders final ouput is a distribution that models 512 colors on one channel?

Yes, correct. It is a linear + softmax layer. See

https://github.com/google-research/google-research/blob/b6754f9b230ca5dabe7011d0d93ad21b09a8641d/coltran/models/colorizer.py#L89

and

https://github.com/google-research/google-research/blob/b6754f9b230ca5dabe7011d0d93ad21b09a8641d/coltran/models/colorizer.py#L151

>In your paper figure2 ,the ground truth low resolution coarse color image as input (add the C) enter the outer decoder ,then after n layer row and masked column attetion and shiftdown ,then the sum of the outer decoder's output and the C as condition to the inner decoder 's conditional attention layers, and they also add the embeddings that is the eq(4) in your paper as input to the inner decoder ,then after Linear and softmax layer , models the pc distribution,the distrubition after sampling generate a low resolution coarse color image to the color upsamping?

Yes.

> since you just supply the gray image and produces a single channel image, so the grayscale encoder just run once? I run your code and I found the output of the decoder's shape is (1, 64, 64, 1, 512). combined with your previous answer, it should be what the following sentence means in your paper , so you used one channel with 512 coarse color instead of 3bit per-channel with rgb channel

Yes, exactly. It predicts a distribution over 512 colors.

> all of your loss function is cross entropy loss in this paper?

Yes, correct.

> In the class ColorUpsampler, the inputs of the call function size is a (B, M, N, 3),but according to your description, shouldn’t it be [B, M, N, 1, 512]? If I read the wrong code line where can i find the convert in your code?

This is a bit subtle. We convert the coarse color back into a RGB image here (so that to visualize the image)

https://github.com/google-research/google-research/blob/b6754f9b230ca5dabe7011d0d93ad21b09a8641d/coltran/models/colorizer.py#L293

In the upsampler, it is converted to a 3-bit RGB image with 8 symbols per channel over here

https://github.com/google-research/google-research/blob/b6754f9b230ca5dabe7011d0d93ad21b09a8641d/coltran/models/upsampler.py#L64

It is then made into a one-hot representation (B, H, W, 3, 8).
Then, each channel (B, H, W, 8) is super-resolved independently into a distribution over 256 symbols, i.e (B, H, W, 256)
This is then concatenated across channels to give (B, H, W, 3, 256)

The coarse color already models dependencies between channels, so this approach worked.",low resolution image enter layer attention context representation also auxiliary parallel model show paper figure linear layer parallel also one forward pas distribution hat context condition outer bias outer input final distribution color one channel yes correct linear layer see paper figure ground truth low resolution coarse color image input add enter outer layer row masked column sum outer output condition inner conditional attention also add paper input inner linear layer distribution sampling generate low resolution coarse color image color yes since supply gray image single channel image run run code found output shape combined previous answer following sentence paper used one channel coarse color instead bit channel yes exactly distribution color loss function cross entropy loss paper yes correct class call function size according description read wrong code line find convert code bit subtle convert coarse color back image visualize image converted image per channel made representation channel independently distribution across give coarse color already approach worked,issue,negative,negative,neutral,neutral,negative,negative
806482314,"> When training the input of the outer decoder is ground truth coarse color low resolution image. when testing ,what's the input of the Outer Decoder?

Ignore the encoder for now, since the grayscale image is present at both during training and evaluation.
Let's say we want to colorize a 2x2 image. As you said, the ground truth is the input to the outer decoder. I use a, b, c and d to denote both the values and locations of the first four pixels.

|   |  |
| ---      | ---       |
| a |  b      |
| c    | d |


# Training

**Outputs: Outer Decoder** 
(shifted down)
|   |  |
| ---      | ---       |
| 0 |  0      |
| a    | b |

**Inputs: Inner Decoder**
Ground truth is shifted right:

|   |  |
| ---      | ---       |
| 0 |  a      |
| 0   | c |

Net inputs to the inner decoder are (outer decoder output + ground truth shifted right), i.e

|   |  |
| ---      | ---       |
| 0 |  a      |
| a   | b + c |

# Sampling

## First pixel (Row 1, Column 1)

Note that at training, the input location to the first pixel to both decoders are 0

Inputs are initialized to zero, i.e
|   |  |
| ---      | ---       |
| 0 |  0      |
| 0    | 0 |

**Outputs: Outer Decoder** 
(shifted down)
|   |  |
| ---      | ---       |
| 0 |  0      |
| 0    | 0 |

**Inputs: Inner Decoder**
Ground truth shifted right + outputs of the outer decoder which is still zero, i.e
|   |  |
| ---      | ---       |
| 0 |  0      |
| 0    | 0 |

Now, we sample value **a** for the first pixel

## Second pixel (Row 1,  Column 2)
Note that at training, the input location to the inner decoder for the second pixel is **a**

After sampling the first pixel, inputs are now:
|   |  |
| ---      | ---       |
| a |  0      |
| 0    | 0 |

**Outputs: Outer Decoder** 
(shifted down)
|   |  |
| ---      | ---       |
| 0 |  0      |
| a    | 0 |

**Inputs: Inner Decoder**
Ground truth shifted right:
|   |  |
| ---      | ---       |
| 0 |  a    |
| 0    | 0 |

Added to outputs of the outer decoder:
|   |  |
| ---      | ---       |
| 0 |  a  |
| a   | 0 |

The masking ensures that the second row is not attended to. Now, we sample value **b** for the second pixel

## Third pixel (Row 2, Column 1)
Note that at training, the input location to the inner decoder for the third pixel is **a**

After sampling, the second pixel, inputs are now:
|   |  |
| ---      | ---       |
| a |  b      |
| 0    | 0 |

**Outputs: Outer Decoder** 
(shifted down)
|   |  |
| ---      | ---       |
| 0 |  0      |
| a    | b |

**Inputs: Inner Decoder**
Ground truth shifted right:
|   |  |
| ---      | ---       |
| 0 |  a    |
| 0    | 0 |

Added to outputs of the outer decoder:
|   |  |
| ---      | ---       |
| 0 |  a  |
| a   | b |

Now, we sample value **c** for the third pixel

## Fourth pixel (Row 2, Column 2)
Note that at training, the input location to the inner decoder for the fourth pixel is **b+c**

After sampling, the third pixel, inputs are now:
|   |  |
| ---      | ---       |
| a |  b      |
| c    | 0 |

**Outputs: Outer Decoder** 
(shifted down)
|   |  |
| ---      | ---       |
| 0 |  0      |
| a    | b |

**Inputs: Inner Decoder**
Ground truth shifted right:
|   |  |
| ---      | ---       |
| 0 |  a    |
| 0    | c |

Added to outputs of the outer decoder:
|   |  |
| ---      | ---       |
| 0 |  a  |
| a   | b + c |

Now, we sample value **d** for the final pixel

The sampling strategy is not obvious, and unfortunately it is not easy to explain in a paper. ",training input outer ground truth coarse color low resolution image testing input outer ignore since image present training evaluation let say want colorize image said ground truth input outer use denote first four training outer inner ground truth right net inner outer output ground truth right sampling first row column note training input location first zero outer inner ground truth right outer still zero sample value first second row column note training input location inner second sampling first outer inner ground truth right added outer second row sample value second third row column note training input location inner third sampling second outer inner ground truth right added outer sample value third fourth row column note training input location inner fourth sampling third outer inner ground truth right added outer sample value final sampling strategy obvious unfortunately easy explain paper,issue,positive,positive,neutral,neutral,positive,positive
806449073,"Hi, did one of you find a useful solution to get the inference up and running?
Would be interested as well. 
Best Regards 
Philipp",hi one find useful solution get inference running would interested well best,issue,positive,positive,positive,positive,positive,positive
805973388,"Thanks for your patient explanation，I want to talk about my understanding of the network of your paper. I hope you can help me point out my mistakes ,maybe it will take you a lot of time to read this text because my English is not good. And since I am not familiar with tensorflow now, I cannot understand your paper through code for the time being.

When training：

1.The **low resolution grayscale image** enter the grayscale encoder , after n layer row/col attention layers ,  generated a context representation **C** , and the grayscale encoder also has an **auxiliary parallel model** as show in your paper figure2( Is it the linear and softmax layer? and as you answered before the **parallel** here is also means models of all the pixels in one forward pass? ) models a distribution pc hat , and the context  **C** will as a condition to the  outer decoder‘s attetion layers and bias the outer decoder's input . The geaysacale'encoders final ouput is a distribution that models 512 colors on one channel?

2.In your paper figure2 ,the ground truth low resolution coarse color image as input (add the **C**) enter the outer decoder ,then after n layer row and masked column attetion and shiftdown ,then the sum of the outer decoder's output  and the **C** as condition to the inner decoder 's conditional attention layers,  and they also add the embeddings that is the eq(4) in your paper as input to the inner decoder ,then after Linear and softmax layer , models the pc distribution,the distrubition after sampling generate a low resolution coarse color image to the color upsamping?

When testing:

3.When training the input of the outer decoder is ground truth coarse color low resolution image.  _**when testing ,what's the input of the Outer Decoder**_  

4.the previous answer you said  

> we just supply the gray image as input to the encoder and the decoder produces a single channel image.

since you just supply the gray image and produces a single channel image, so the grayscale encoder just run once? I run your code and I found the output of the decoder's shape is (1, 64, 64, 1, 512). combined with your previous answer, it should be what the following sentence means in your paper , so you used one channel with 512 coarse color instead of 3bit per-channel with rgb channel

> Thus, there are 8^3 = 512 coarse colors per pixel which are predicted directly as a single “color” channel

![image](https://user-images.githubusercontent.com/22975697/112343312-f762e480-8cfd-11eb-8489-fecf6d37e318.png)

In this picture,the labels is the coarse color low resolution obtained by ground truth?If so , then the same reason,the upsampler color also make cross entropy between the pc hat (generated from ColorUpsampler) and the pc from ground truth and the ps hat (generated from SpatialUpsampler) make  cross_entropy with ground truth? all of your loss function is cross entropy loss in this paper?

When Color upsampler

> We convert the coarse image x s↓c↓ ∈ RM×N×1 of 512 colors back into a 3 bit RGB image with 8 symbols per channel. The channels are embedded using separate embedding matrices to x s↓c↓ k ∈ RM×N×D, where k ∈ {R, G, B} indicates the channel.

In the class ColorUpsampler, the inputs of the call function size is a (B, M, N, 3),but according to your description, shouldn’t it be [B, M, N, 1, 512]?  If I read the wrong code line where can i find the convert in your code?


",thanks patient want talk understanding network paper hope help point maybe take lot time read text good since familiar understand paper code time low resolution image enter layer attention context representation also auxiliary parallel model show paper figure linear layer parallel also one forward pas distribution hat context condition outer bias outer input final distribution color one channel paper figure ground truth low resolution coarse color image input add enter outer layer row masked column sum outer output condition inner conditional attention also add paper input inner linear layer distribution sampling generate low resolution coarse color image color testing training input outer ground truth coarse color low resolution image testing input outer previous answer said supply gray image input single channel image since supply gray image single channel image run run code found output shape combined previous answer following sentence paper used one channel coarse color instead bit channel thus coarse color per directly single color channel image picture coarse color low resolution ground truth reason color also make cross entropy hat ground truth hat make ground truth loss function cross entropy loss paper color convert coarse image color back bit image per channel separate matrix channel class call function size according description read wrong code line find convert code,issue,positive,positive,neutral,neutral,positive,positive
805633149,"> How does it encodes each prior channel?

There is only one channel. Specifically, in our paper, we convert the 3 channel RGB image into a single channel image with 512 colors. We use a 3 bit RGB image, which has total of 8 symbols per channel. This leads to a total of 8**3=512 colors. See the start of Section 4 for more details. The main idea is that it saves 2x in terms of sampling speed.

Section 3.2 is meant to be as background for the Axial Transformer. For the original Axial Transformer, we indeed need to supply prior channels as input to the encoder, i.e (Gray, Red, Green).

For our paper, we just supply the gray image as input to the encoder and the decoder produces a single channel image.

>  it is one row attention layer follow one column attention layer or attention to all rows and then attention to all columns?

We follow one row attention layer with one column attention layer.
",prior channel one channel specifically paper convert channel image single channel image color use bit image total per channel total color see start section main idea sampling speed section meant background axial transformer original axial transformer indeed need supply prior input gray red green paper supply gray image input single channel image one row attention layer follow one column attention layer attention attention follow one row attention layer one column attention layer,issue,negative,positive,neutral,neutral,positive,positive
805413684,"In your 3.2 AXIAL TRANSFORMER Sampling section the last sentence is 

> The encoder, then recomputes context to generate the next channel.

So after a channel is generated, then the previous output of the grayscale encoder will enter the grayscale encoder again?If like this , as I ask before, how does it encodes each prior channel ?",axial transformer sampling section last sentence context generate next channel channel previous output enter like ask prior channel,issue,negative,negative,neutral,neutral,negative,negative
805391366,"In the row/column attention of your encoder, it is one row attention layer follow one column attention layer or attention to all rows and then attention to all columns? I wonder if my expression is clear?",attention one row attention layer follow one column attention layer attention attention wonder expression clear,issue,negative,positive,positive,positive,positive,positive
805389766,"`gray = tf.image.rgb_to_grayscale(inputs)
        z = self.encoder(gray)`
In your code ,  I see you pass a grayscale image to the enocder network, so how does it encodes each prior channel?",gray gray code see pas image network prior channel,issue,negative,neutral,neutral,neutral,neutral,neutral
805386129,"Hi in your paper,there is a description about 

> the grayscale encoder encodes each prior channel independently

 the encoder encodes each prior channel  ,but the grayscale image only have one channel,so the encoder encodes grayscale image or an image with three channel?",hi paper description prior channel independently prior channel image one channel image image three channel,issue,negative,neutral,neutral,neutral,neutral,neutral
804883224,"Hello everyone, I released a custom colorization script over here (https://github.com/google-research/google-research/blob/master/coltran/custom_colorize.py) to colorize or recolorize a directory of images.",hello everyone custom colorization script colorize directory,issue,negative,neutral,neutral,neutral,neutral,neutral
804704979,"Hi @ccpocker, If you colorize grayscale images directly, you have to remove the following lines in your script. Otherwise the input provided to the model is wrong. See the issue over here https://github.com/google-research/google-research/issues/619

https://github.com/google-research/google-research/blob/db508b0f9c444012d347b0407c4e398cbbc7705a/coltran/sample.py#L161",hi colorize directly remove following script otherwise input provided model wrong see issue,issue,negative,negative,negative,negative,negative,negative
804688117,"@Dhaskiru04 I am so sorry that i can't provide the script due to i am not finished this script.
But for my test, In the first step, colorizer can colorize some picture and some picture still gray after colorizer.",sorry ca provide script due finished script test first step colorize picture picture still gray,issue,negative,negative,negative,negative,negative,negative
804248170,"Hi @HRLTY, no problem thanks for your time.  Yes I'm running everything inside docker. Anyway I opened a new issue which I know is a huge request but I hope in your collaboration! ",hi problem thanks time yes running everything inside docker anyway new issue know huge request hope collaboration,issue,positive,positive,positive,positive,positive,positive
804244521,"Hi @FrancescoMandru,
sorry for the late reply. 
Yes Ubuntu / Linux and Mac OS are fairly different for shared library layout. I wouldn't expect them to be binary compatible. One easy workaround might be running it inside the docker too.",hi sorry late reply yes mac o fairly different library layout would expect binary compatible one easy might running inside docker,issue,positive,negative,neutral,neutral,negative,negative
804243902,"Hello, one important note is that you need to use the same keypoint detector on H3.6M to train Pr-VIPE as the keypoint detector on Penn Action to generate the 2D poses (which then can go into infer.py). If you followed the instructions on the Github, your Pr-VIPE model is trained using the Cascaded Pyramid Network (CPN) keypoints, so you would also need to run that detector on Penn Action. Alternatively, if you have your own 2D keypoint detector, you can run it on both H3.6M and Penn Action.

The other fields in the csv can be obtained by converting fields in the .mat file to .csv format, and this can be done using existing libraries in Python such as:
import scipy.io
mat = scipy.io.loadmat('file.mat')
Then you can write to csv once it is loaded. Hope your experiments go well!",hello one important note need use detector train detector action generate go model trained pyramid network would also need run detector action alternatively detector run action converting file format done python import mat write loaded hope go well,issue,positive,positive,positive,positive,positive,positive
804125842,"I have an example of some problems I'm encountering. Suppose I have a LiDAR dataset (x,y,z,r) where r is the reflectance, this comes from semantic KITTI (which is not a dataset considered on this repository), and I have labels associated to each of these points. What I would like to do is using your U-Net to perform semantic segmentation but it's really hard to understand the step by step procedure to follow. That's my attempt:

```
    data_root = ""../data/sequences""
    data = glob.glob(os.path.join(data_root, ""00/velodyne/*.bin""))
    labels = glob.glob(os.path.join(data_root, ""00/labels/*.label""))

    ptc = load_velo_scan(data[0])
    lab = load_velo_label(labels[0])

    lab = tf.constant(lab, dtype=tf.int32)
    # lab = instance_segmentation_utils.map_labels_to_0_to_n(lab) ---> TOO SLOW
    #voxel_lab = preprocessor_utils.voxelize_semantic_labels(lab, None, (0.5, 0.5, 0.5)) --> DON'T KNOW

    # Encode 3D coordinates
    points = tf.expand_dims(tf.constant(ptc[:, 0:3], dtype=tf.float32), axis=0)
    # Encode reflectance
    features = tf.expand_dims(tf.constant(ptc[:, 3], dtype=tf.float32), axis=0)
    features = tf.expand_dims(features, axis=-1)
    num_valid_points = tf.constant([len(ptc)], dtype=tf.int32)
    grid_cell_size = (0.5, 0.5, 0.5)
    (voxel_features, voxel_indices, num_valid_voxels, segment_ids,
     voxel_start_locations) = voxel_utils.pointcloud_to_sparse_voxel_grid(
        points=points,
        features=features,
        num_valid_points=num_valid_points,
        grid_cell_size=grid_cell_size,
        voxels_pad_or_clip_size=None,
        segment_func=tf.math.unsorted_segment_max)

    # Labels equal to 1 for every point (just for test) as I'm not able to extract voxelized ones
    voxel_lab = tf.expand_dims(tf.constant(np.ones(16586), dtype=tf.int32), axis=0)
    voxel_lab = tf.expand_dims(voxel_lab, axis=-1)

    inputs = {
        standard_fields.InputDataFields.voxel_features:
            voxel_features,
        standard_fields.InputDataFields.voxel_xyz_indices:
            voxel_indices,
        standard_fields.InputDataFields.num_valid_voxels:
            num_valid_voxels,
        standard_fields.InputDataFields.object_class_voxels:
            voxel_lab,
        standard_fields.InputDataFields.num_valid_points:
            num_valid_points,
        standard_fields.InputDataFields.points_to_voxel_mapping:
            segment_ids,
    }

    num_classes = 1
    segmentation_model = model.SemanticSegmentationModel(
        num_classes=num_classes)

    # TRAINING? what is the output? I expect a label for each voxel,
    # How can i Decide how many epochs? learning rate? loss, where the weights are stored?
    # How can i monitor the training with the tensorboard etc..
    outputs = segmentation_model(inputs, training=True)

    print(""OUT: "", outputs)

    eval_outputs = segmentation_model(inputs, training=False)
```

",example suppose reflectance come semantic considered repository associated would like perform semantic segmentation really hard understand step step procedure follow attempt data data lab lab lab lab lab slow lab none know encode encode reflectance equal every point test able extract training output expect label decide many learning rate loss monitor training print,issue,negative,positive,neutral,neutral,positive,positive
804084617,"@stopsilver In your snippet over here (https://github.com/google-research/google-research/issues/583#issuecomment-801976617), it is also necessary to restore the ema variables as done here (https://github.com/google-research/google-research/blob/master/coltran/sample.py#L143). It can be crucial to obtain better sample quality.

See Figure 6 over here (https://openreview.net/pdf?id=5NA1PinlGFu)",snippet also necessary restore done crucial obtain better sample quality see figure,issue,positive,positive,positive,positive,positive,positive
803870348,"> Thanks for the reply!
> I will try it.
> And I'm looking forward to the code being released.

@octpath do you have the repo to share with 2.0 implementation? ",thanks reply try looking forward code share implementation,issue,positive,positive,positive,positive,positive,positive
803864082,"@MechCoder Hai, I am trying the script in command prompt where data (single image) and model are in local folder. I didn't use any gpu for this. I got like this. I am new to this. Could you help me?
![Capture](https://user-images.githubusercontent.com/46004330/111960516-dfc90780-8b15-11eb-9cc9-ec0677094198.PNG)


",trying script command prompt data single image model local folder use got like new could help capture,issue,positive,positive,neutral,neutral,positive,positive
803859788,"> Thanks for your work and sharing.
> I tested your pre-trained models on my own image sequence.
> It seems the trajectory_inference.py only provided the relative position.
> The paper said your work can learn the camera intrinsic but I can't find any part about it in this project.


If I'm not mistaken, in order to retrieve intrinsics, you need to modify the model itself. Under modify I mean add intrinsic as a property to the model class, take the returned values from `motion_prediction_net.motion_field_net`, and assign it later as `self.intrinsics = 0.5 * (mat + inv_mat)`. 

You can later get this value by adding a method to the model class:

```
    def inference_intrinsics(self, image1, image2, sess):
        return sess.run([self.intrinsics],
                        feed_dict={
                            self._image1: image1,
                            self._image2: image2
                        })
```




",thanks work tested image sequence provided relative position paper said work learn camera intrinsic ca find part project mistaken order retrieve need modify model modify mean add intrinsic property model class take returned assign later mat later get value method model class self image image sess return image image,issue,negative,negative,neutral,neutral,negative,negative
803856023,"Thank you, your explanation is so easy to understand, thank you very much",thank explanation easy understand thank much,issue,positive,positive,positive,positive,positive,positive
803814524,">   this parallel that at this position in your paper is the inherent parallel feature of transformer?

In this case, ""parallel"" refers to the generation of all the pixels in one forward pass. Both the color and spatial upsampler generate all their outputs in one pass. The colorizer, in comparison is autoregressive; it needs a forward pass to generate a given color at a given row and column index.

>  is the main function of the context representation obtained from the grayscale image is to shift and scale the vector used for attention?

The main function is to provide information from the grayscale image to the colorizer. Otherwise, the colorizer becomes unconditional, i.e it will generate random images independent of the grayscale image. Shifting and scaling the attention vectors is one way to do that, there are other ways, for eg see Table 1 in our paper (https://openreview.net/pdf?id=5NA1PinlGFu)

",parallel position paper inherent parallel feature transformer case parallel generation one forward pas color spatial generate one pas comparison need forward pas generate given color given row column index main function context representation image shift scale vector used attention main function provide information image otherwise becomes unconditional generate random independent image shifting scaling attention one way way see table paper,issue,negative,negative,neutral,neutral,negative,negative
803809884,"Once you download the dataset in tfrecords, you can use pandas-tfrecords to convert it into csv format, even tsv.
It worked for me.

Also, have you tried training the model?",use convert format even worked also tried training model,issue,negative,neutral,neutral,neutral,neutral,neutral
803753335,@stopsilver Can you share your script? I just want to try this to my dataset?,share script want try,issue,negative,neutral,neutral,neutral,neutral,neutral
803703193,and more question is the main function of the context representation obtained from the grayscale image is to shift and scale the vector used for attention? Please forgive me for interrupting you again and again with these small problems ^ ^,question main function context representation image shift scale vector used attention please forgive interrupting small,issue,negative,negative,neutral,neutral,negative,negative
803702196,"Hi, in your paper , there are a description of network structure as below

>  we decompose the task into three simpler sequential subtasks: coarse low resolution autoregressive colorization parallel color and spatial super-resolution

so i want to ask is the parallel color and spatial super-resolution means **parallel** color and **parallel** spatial super-resolution separately?  in other words, this **parallel** that at this position in your paper is the inherent parallel feature of transformer?
",hi paper description network structure decompose task three simpler sequential coarse low resolution colorization parallel color spatial want ask parallel color spatial parallel color parallel spatial separately parallel position paper inherent parallel feature transformer,issue,negative,neutral,neutral,neutral,neutral,neutral
803624404,"Yes I would like to better understand the usage of this repository. For example, suppose that I have a dataset of point clouds with the related labels. (Eg Semantic Kitti dataset) and I just want to exploit the modules for 3D sparse convolutions, or the U-Net semantic segmentation model which are the core functions that are useful to try. How should I prepare my dataset? How can I build a simple model to train and test the semantic segmentation results? An `helloworld` example would be fantastic!",yes would like better understand usage repository example suppose point related semantic want exploit sparse semantic segmentation model core useful try prepare build simple model train test semantic segmentation example would fantastic,issue,positive,positive,positive,positive,positive,positive
803146496,Hi @HRLTY and @afathi3. I found the error with my workaround. the `_sparse_conv_ops.so` file works only for Ubuntu. I need the `dlopen`for MacOS or the source.c otherwise it will never work,hi found error file work need otherwise never work,issue,negative,neutral,neutral,neutral,neutral,neutral
802837781,"Thank you, because I do not fully understand your paper at present, I will read your paper repeatedly, I think your work is very meaningful, and your timely reply is also of great help to me, thank you again",thank fully understand paper present read paper repeatedly think work meaningful timely reply also great help thank,issue,positive,positive,positive,positive,positive,positive
802833710,Yes that is correct. We have code here that can help understand it better (https://github.com/google-research/google-research/blob/master/coltran/models/colorizer.py#L201),yes correct code help understand better,issue,positive,positive,positive,positive,positive,positive
802830785,"Thank you so much for your prompt reply, so when I generated the pixels in the second row and second column in colorizer, I used the ground truth color of all pixels in the first row and the ground truth color of the first pixel in the second column.",thank much prompt reply second row second column used ground truth color first row ground truth color first second column,issue,positive,positive,positive,positive,positive,positive
802825630,"Hi, thanks for your interest in the paper!

When the colorizer is trained, for a given pixel, the inputs are both:

- The grayscale image
- All the previous ground truth colors as per raster order.

During sampling, each color is generated one at a time, and has access to previously generated colors.
Does that answer your question?",hi thanks interest paper trained given image previous ground truth color per raster order sampling color one time access previously color answer question,issue,positive,negative,neutral,neutral,negative,negative
802823588,"and I'm so curious about how can a Xg(grayscale image) through N layers Row and Col attention can get X spatial down and color down image if there are not color image as input ,where is the color information come form? ",curious image row col attention get spatial color image color image input color information come form,issue,negative,negative,neutral,neutral,negative,negative
802732766,"In my test, there was a error with the update of string_int_lable_map.proto and label_map_util.py in utils folder.The following is my error message:
```
root@c31432fd4a88:/google-research-tf3d# bash tf3d/semantic_segmentation/scripts/rio/run_eval_locally.sh
EVAL_DIR at /google-research-tf3d/tf3d_experiment/seg_rio_001...
2021-03-19 09:07:41.415679: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/google-research-tf3d/tf3d/eval.py"", line 166, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/google-research-tf3d/tf3d/eval.py"", line 141, in main
    __import__(module_name)
  File ""/google-research-tf3d/tf3d/gin_imports.py"", line 34, in <module>
    from tf3d.semantic_segmentation import metric as semantic_segmentation_metric
  File ""/google-research-tf3d/tf3d/semantic_segmentation/metric.py"", line 24, in <module>
    from object_detection.utils import label_map_util
  File ""/google-research-tf3d/object_detection/utils/label_map_util.py"", line 29, in <module>
    from object_detection.protos import string_int_label_map_pb2
  File ""/google-research-tf3d/object_detection/protos/string_int_label_map_pb2.py"", line 22, in <module>
    serialized_pb=_b('\n\x1astring_int_label_map.proto\x12\x17object_detection.protos\""\xee\x01\n\x15StringIntLabelMapItem\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\n\n\x02id\x18\x02 \x01(\x05\x12\x14\n\x0c\x64isplay_name\x18\x03 \x01(\t\x12M\n\tkeypoints\x18\x04 \x03(\x0b\x32:.object_detection.protos.StringIntLabelMapItem.KeypointMap\x12\x14\n\x0c\x61ncestor_ids\x18\x05 \x03(\x05\x12\x16\n\x0e\x64\x65scendant_ids\x18\x06 \x03(\x05\x1a(\n\x0bKeypointMap\x12\n\n\x02id\x18\x01 \x01(\x05\x12\r\n\x05label\x18\x02 \x01(\t\""Q\n\x11StringIntLabelMap\x12<\n\x04item\x18\x01 \x03(\x0b\x32..object_detection.protos.StringIntLabelMapItem')
  File ""/usr/local/lib/python3.6/dist-packages/google/protobuf/descriptor.py"", line 942, in __new__
    return _message.default_pool.AddSerializedFile(serialized_pb)
TypeError: Couldn't build proto file into descriptor pool!
Invalid proto descriptor for file ""string_int_label_map.proto"":
  string_int_label_map.proto: A file with this name is already in the pool.
```
I downloaded the object_detection module of tensorflow, compiled the proto file separately in the protos folder, and copied the generated file to the utils/protos folder under the tf3d folder. This error was fixed.
The following are the two generated files(Because can not upload py files, I changed it to txt format). After comparison, it is found that many of them are different. I have not learned protoc, so I don’t know what caused this error.

[string_int_label_map_pb2(from_tf3d_utils_proto).txt](https://github.com/google-research/google-research/files/6170475/string_int_label_map_pb2.from_tf3d_utils_proto.txt)
[string_int_label_map_pb2(from_object_detection_module).txt](https://github.com/google-research/google-research/files/6170477/string_int_label_map_pb2.from_object_detection_module.txt)

PS: 
1. Under tf3d/datasets/labelmaps, I found that there is no pbtxt corresponding to the rio data set.
2. Under tf3d/object_detection, I also did not find the script and .gin file of the rio data set.

Thanks!",test error update following error message root bash successfully dynamic library recent call last file line file line code file line module main file line run main file line main file line main file line module import metric file line module import file line module import file line module file line return could build proto file pool invalid proto file file name already pool module proto file separately folder copied file folder folder error fixed following two format comparison found many different learned know error found corresponding rio data set also find script file rio data set thanks,issue,negative,positive,positive,positive,positive,positive
802673609,"Hi @hyhieu , I really love your paper. 
In appendix B, ""Compute the teacher’s feedback coefficient as in Equation 12"". I think that it should be ""student's feedback"" rather than ""teacher’s feedback"". Is that the typo error?
![image](https://user-images.githubusercontent.com/48278096/111756936-5aebad00-88cd-11eb-9bcc-8f8edc19717a.png)
",hi really love paper appendix compute teacher feedback coefficient equation think student feedback rather teacher feedback typo error image,issue,negative,positive,positive,positive,positive,positive
802658905,@Dhaskiru04 Thanks for your interest! I will try to release a script next week.,thanks interest try release script next week,issue,positive,positive,neutral,neutral,positive,positive
802654046,@ccpocker Can you provide the script for single image?,provide script single image,issue,negative,negative,neutral,neutral,negative,negative
802653809,@MechCoder Can you provide the script for single image? I collected some images from google and want to try it this transformer method for my dataset.,provide script single image collected want try transformer method,issue,negative,negative,neutral,neutral,negative,negative
802504398,"Hi @hewittzgh,
if eval_val_mesh folder's event file has meaningful non empty size, I suggest debugging into this [line](https://github.com/google-research/google-research/blob/07f7097a781a4a176a78fd84c30931ab8ebf48b2/tf3d/utils/callback_utils.py#L320)'s parameters, check if they are reasonable format.
On the other hand, Tensorboard will likely crash when so many meshes(10+) are to be displayed at the same time.
Thanks.",hi folder event file meaningful non empty size suggest line check reasonable format hand likely crash many displayed time thanks,issue,positive,positive,positive,positive,positive,positive
802418574,something something git branches something something,something something git something something,issue,negative,neutral,neutral,neutral,neutral,neutral
802184265,"@HRLTY The folder structure is the following:
```bash
root
|--- tensorflow
|--- custom-ops
|--- tf3d
|       |---ops
|              |---tensorflow_sparse_conv_ops
|                       |---files
|--- mytest.py
```

In each of these subfolder there is a `__init__.py`, also everytime I run my test in each subsolder a `.pyc` file is created, also in the folder where its not able to import the file properly. Moreover I'm able to navigate with import up the `tensorflow_sparse_conv_ops` directory without problems but inside that one there is something happening and my tester does not see those files. Can be something related with the python version or something else?

Also I tried a workaround copying the content of `sparse_conv_ops_test.py` file directly inside my testing file and also copying in the same location the `_sparse_conv_ops.so` file but I get the following:

```
Traceback (most recent call last):
  File ""hellosparse.py"", line 12, in <module>
    resource_loader.get_path_to_datafile('_sparse_conv_ops.so'))
  File ""/Users/francescomandru/Desktop/master-thesis/masterth-env/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(_sparse_conv_ops.so, 6): no suitable image found.  Did find:
	_sparse_conv_ops.so: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x03
	/Users/francescomandru/Desktop/master-thesis/sparseconvtf/_sparse_conv_ops.so: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x03
```

I'm running it inside a virtual environemnt",folder structure following bash root also run test file also folder able import file properly moreover able navigate import directory without inside one something happening tester see something related python version something else also tried content file directly inside testing file also location file get following recent call last file line module file line suitable image found find unknown file type first eight unknown file type first eight running inside virtual,issue,negative,positive,positive,positive,positive,positive
802172255,"Hi,

Thanks for trying it. This is a typical python import error.

I am not exactly sure your folder layout. But make sure you are in the *parent* folder of tf3d if you want to run ""import tf3d.ops.tensorflow_sparse_conv_ops as sparse_conv_ops”. Besides, each subfolder(if I am correct) should have a `__init__.py` file there, empty or not. 
Please check some tutorials for placing it correctly.

Thanks!

Sincerely,
Rui


> On Mar 18, 2021, at 10:42 AM, FraMan ***@***.***> wrote:
> 
> 
> Hi @HRLTY <https://github.com/HRLTY>, this morning I fixed everything doing again from scratch. Now there is an issue very strange to me. I tried writing a .py to test the library and I placed it in the main directory which contains the three directories (tf3d, tensorflow and custom_ops), the file is just a print with the import to test:
> 
> import tf3d.ops.tensorflow_sparse_conv_ops as sparse_conv_ops
> 
> print(""Ciao"")
> The problem is the following:
> 
> Traceback (most recent call last):
>   File ""hellosparse.py"", line 1, in <module>
>     import tf3d.ops.tensorflow_sparse_conv_ops as sparse_conv_ops
> ImportError: No module named tf3d.ops.tensorflow_sparse_conv_ops
> which is strange to me as the file is present in the following directory directory /Users/francescomandru/Desktop/master-thesis/sparseconvtf/tf3d/ops/tensorflow_sparse_conv_ops
> 
> The problem as you can see comes from the __init__.py :
> 
> # coding=utf-8
> # Copyright 2021 The Google Research Authors.
> #
> # Licensed under the Apache License, Version 2.0 (the ""License"");
> # you may not use this file except in compliance with the License.
> # You may obtain a copy of the License at
> #
> #     http://www.apache.org/licenses/LICENSE-2.0
> #
> # Unless required by applicable law or agreed to in writing, software
> # distributed under the License is distributed on an ""AS IS"" BASIS,
> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
> # See the License for the specific language governing permissions and
> # limitations under the License.
> 
> """"""Import the sparse conv 2d/3d op.""""""
> sys.path.append( os.abspath("".""))
> from __future__ import absolute_import
> 
> from tensorflow_sparse_conv_ops import sparse_conv_ops
> submanifold_sparse_conv3d = sparse_conv_ops.submanifold_sparse_conv3d
> submanifold_sparse_conv2d = sparse_conv_ops.submanifold_sparse_conv2d
> In the directory tensorflow_sparse_conv_ops there are the following files:
> 
> __init__.py , _sparse_conv_ops.so , sparse_conv_ops_test.py , sparse_conv_ops.py
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub <https://github.com/google-research/google-research/issues/630#issuecomment-802155559>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AB3WSIIL5M2DS32PD4ZGUV3TEI3Q3ANCNFSM4ZEAR73A>.
> 

",hi thanks trying typical python import error exactly sure folder layout make sure parent folder want run import besides correct file empty please check correctly thanks sincerely mar wrote hi morning fixed everything scratch issue strange tried writing test library main directory three file print import test import print problem following recent call last file line module import module strange file present following directory directory problem see come copyright research licensed apache license version license may use file except compliance license may obtain copy license unless applicable law agreed writing distributed license distributed basis without kind either express see license specific language governing license import sparse import import directory following reply directly view,issue,negative,positive,positive,positive,positive,positive
802155559,"Hi @HRLTY, this morning I fixed everything doing again from scratch. Now there is an issue very strange to me. I tried writing a .py to test the library and I placed it in the main directory which contains the three directories (tf3d, tensorflow and custom_ops), the file is just a print with the import to test:
```
import tf3d.ops.tensorflow_sparse_conv_ops as sparse_conv_ops

print(""Ciao"")
```
The problem is the following:
```
Traceback (most recent call last):
  File ""hellosparse.py"", line 1, in <module>
    import tf3d.ops.tensorflow_sparse_conv_ops as sparse_conv_ops
ImportError: No module named tf3d.ops.tensorflow_sparse_conv_ops
```
which is strange to me as the file is present in the following directory directory ` /Users/francescomandru/Desktop/master-thesis/sparseconvtf/tf3d/ops/tensorflow_sparse_conv_ops` 

The problem as you can see comes from the `__init__.py`  : 

```
# coding=utf-8
# Copyright 2021 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Import the sparse conv 2d/3d op.""""""

from __future__ import absolute_import

from tensorflow_sparse_conv_ops import sparse_conv_ops
submanifold_sparse_conv3d = sparse_conv_ops.submanifold_sparse_conv3d
submanifold_sparse_conv2d = sparse_conv_ops.submanifold_sparse_conv2d
```
In the directory tensorflow_sparse_conv_ops there are the following files:

`__init__.py` , `_sparse_conv_ops.so` , `sparse_conv_ops_test.py` , `sparse_conv_ops.py`
",hi morning fixed everything scratch issue strange tried writing test library main directory three file print import test import print problem following recent call last file line module import module strange file present following directory directory problem see come copyright research licensed apache license version license may use file except compliance license may obtain copy license unless applicable law agreed writing distributed license distributed basis without kind either express see license specific language governing license import sparse import import directory following,issue,negative,positive,neutral,neutral,positive,positive
801995374,"I tested with other image sets and it works! Thank you for helping.
Maybe there's some issue in my code when handling certain images. I will look into it and leave comment for others if I find out what was wrong.",tested image work thank helping maybe issue code handling certain look leave comment find wrong,issue,negative,negative,negative,negative,negative,negative
801987080,"Weird, is it gray for just one image or all images? Are they all of dtype int32?",weird gray one image,issue,negative,negative,negative,negative,negative,negative
801981792,Doesn't work neither. I can only get output of 3 channel but gray colored.,work neither get output channel gray colored,issue,negative,neutral,neutral,neutral,neutral,neutral
801976617,"For instance, this is my code for `colorizer` part. I deleted that part. Is this right?
```
next_data = next(data_iter)         # next_data.shape [1, 64, 64, 1]
colorizer_model = colorizer.ColTranCore(colorizer_config.model)
colorizer_model(zero, training=False)
checkpoints = train_utils.create_checkpoint(colorizer_model)
train_utils.restore(colorizer_model, checkpoints, log_dir)
a = colorizer_model.sample(gray_cond=next_data, mode='sample')
```",instance code part part right next zero,issue,negative,positive,positive,positive,positive,positive
801974195,"Right, so if you are feeding grayscale images directly, you have to remove this line from the code (https://github.com/google-research/google-research/blob/master/coltran/sample.py#L161).",right feeding directly remove line code,issue,negative,positive,positive,positive,positive,positive
801969992,"I'm confused. I made a custom dataloader and feeding gray image to colorizer model directly.

which is the line#186 in ([#583 (comment)](https://github.com/google-research/google-research/issues/583#issuecomment-784115297))?
```python
output = model.sample(gray_cond=curr_gray, mode=sample_mode) 
```
or
```python
# Provide low resolution ground truth image.
low_res = next_data['targets_%d_up_back' % downsample_res]
```

Sorry for poor understanding. I don't get the difference btw sample.py and your manual in ([#583 (comment)](https://github.com/google-research/google-research/issues/583#issuecomment-784115297))



",confused made custom feeding gray image model directly line comment python output python provide low resolution ground truth image sorry poor understanding get difference manual comment,issue,negative,negative,negative,negative,negative,negative
801956942,"Sorry for the confusion. The paper is indeed about bw to rgb conversion. But the codebase does not support it currently.

If you would like to provide bw images directly, you need to modify the sample.py code as mentioned here (https://github.com/google-research/google-research/issues/583#issuecomment-784115297). Does that help?

@ccpocker Are you still working on this? If not, I can write a script that does this.
",sorry confusion paper indeed conversion support currently would like provide directly need modify code help still working write script,issue,positive,negative,negative,negative,negative,negative
801952471,"@MechCoder 
Thank you for reply! I thought this paper is about bw to bgr colorization. My mistake.
",thank reply thought paper colorization mistake,issue,negative,neutral,neutral,neutral,neutral,neutral
801901008,"Is this issue relevant? https://github.com/google-research/google-research/issues/619

The input image folder should contain RGB images and not grayscale images. It is converted internally to gray images here (https://github.com/google-research/google-research/blob/master/coltran/sample.py#L161). See https://github.com/google-research/google-research/issues/619#issuecomment-797261367

Unfortunately right now, there is no direct way of providing grayscale images as input, but should not be very difficult to change that.",issue relevant input image folder contain converted internally gray see unfortunately right direct way providing input difficult change,issue,negative,positive,neutral,neutral,positive,positive
801898768,"@MechCoder 
Hello, I'm trying to colorize custom images using Coltran. I checked 'colorizer', 'color_upsampler', and 'spatial_upsampler' codes are all working without any error and also checked that each pretrained model is loaded. (checked by `Restoring from ~` and `Producing sample after ### training steps` messages)

But my problem is that colorization doesn't work. My output files(after each steep) only contain bw image with 3 channels. For example, output of coltran_core model has ['parallel', 'auto_sample', 'proba'] keys and 'parallel', 'auto_sample' are gray image in (64, 64, 3) shape. I used gray as input.

Any advice?",hello trying colorize custom checked working without error also checked model loaded checked sample training problem colorization work output steep contain image example output model gray image shape used gray input advice,issue,negative,neutral,neutral,neutral,neutral,neutral
801721090,"Hi Team,

I am using the pr-vipe model to do action recognition experiment described by the paper(View-Invariant Probabilistic Embedding forHuman Pose) .

In the second step of your previous reply,we compute Pr-VIPE embeddings on the detected pose from each video frame in Penn Action. I get Penn Action and want to use infer.py(https://github.com/google-research/google-research/blob/master/poem/infer.py) to compute Pr-VIPE embeddings.

But how can I convert Penn Action to csv file described by the infer.py .

",hi team model action recognition experiment paper probabilistic pose second step previous reply compute pose video frame action get action want use compute convert action file,issue,negative,positive,neutral,neutral,positive,positive
801601127,"The error https://github.com/google-research/google-research/issues/631#issuecomment-799867859 has been fixed. Thanks!
I still haven't been able to solve the problem with the 3D image display in the TensorBoard's Mesh TAB. Here is my screenshot:
![屏幕快照 2021-03-18 下午12 03 20](https://user-images.githubusercontent.com/60736416/111571637-06b7ce80-87e2-11eb-8296-9fe30167d1f8.png)

![屏幕快照 2021-03-18 上午11 45 48](https://user-images.githubusercontent.com/60736416/111570532-09b1bf80-87e0-11eb-82cd-d48a12da6c98.png)
You can see that there are empty Spaces where the image should be displayed.
This is my system environment:
> docker images: tensorflow/tensorflow:2.3.0-gpu
> nvidia-docker
> In docker container: tensorboard == 2.4 tensorflow = 2.3.0

Models can be trained and validated normally in my environment, but the tensorboard.mesh can not work.",error fixed thanks still able solve problem image display mesh tab see empty image displayed system environment docker docker container trained normally environment work,issue,negative,positive,positive,positive,positive,positive
801581464,"do you have any suggestion on how to troubleshoot this? as we are utilizing the docker inference, so is it a problem with our hardware/driver setup?




",suggestion docker inference problem setup,issue,negative,neutral,neutral,neutral,neutral,neutral
801575284,"Hi,
This line is before any op related code is run. Please check your tf /CUDA/ bazel environment. This is a Tensorflow error.
https://github.com/google-research/google-research/blob/master/tf3d/ops/tensorflow_sparse_conv_ops/sparse_conv_ops_test.py#L32 <https://github.com/google-research/google-research/blob/master/tf3d/ops/tensorflow_sparse_conv_ops/sparse_conv_ops_test.py#L32> 

Thanks!

Sincerely,
Rui


> On Mar 17, 2021, at 7:40 PM, Wayne-wonderai ***@***.***> wrote:
> 
> 
> Thanks for updating.
> I tried Op compilation instructions <https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md>, and now stop at step 6.
> 
> bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures
> Here's the logs.
> 
> 2021-03-18 02:02:29.285314: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
> Running tests under Python 3.6.12: /usr/bin/python3
> [ RUN      ] SparseConvOpTest.test_session
> [  SKIPPED ] SparseConvOpTest.test_session
> [ RUN      ] SparseConvOpTest.test_spar_conv_op
> 2021-03-18 02:02:30.022262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
> 2021-03-18 02:02:30.058706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.059657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
> pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
> coreClock: 1.755GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s
> 2021-03-18 02:02:30.059699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.060507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
> pciBusID: 0000:03:00.0 name: GeForce RTX 3090 computeCapability: 8.6
> coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
> 2021-03-18 02:02:30.060519: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
> 2021-03-18 02:02:30.061462: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
> 2021-03-18 02:02:30.062386: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
> 2021-03-18 02:02:30.062544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
> 2021-03-18 02:02:30.063485: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
> 2021-03-18 02:02:30.064012: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
> 2021-03-18 02:02:30.066111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
> 2021-03-18 02:02:30.066171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.067002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.067805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.068599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.069395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
> 2021-03-18 02:02:30.069622: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> 2021-03-18 02:02:30.074065: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3799900000 Hz
> 2021-03-18 02:02:30.074340: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5719590 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
> 2021-03-18 02:02:30.074347: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
> 2021-03-18 02:02:30.172593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.213582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.214498: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57858d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
> 2021-03-18 02:02:30.214506: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
> 2021-03-18 02:02:30.214508: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 3090, Compute Capability 8.6
> 2021-03-18 02:02:30.215138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.215928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
> pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
> coreClock: 1.755GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s
> 2021-03-18 02:02:30.215960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.216749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
> pciBusID: 0000:03:00.0 name: GeForce RTX 3090 computeCapability: 8.6
> coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
> 2021-03-18 02:02:30.216765: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
> 2021-03-18 02:02:30.216777: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
> 2021-03-18 02:02:30.216783: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
> 2021-03-18 02:02:30.216791: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
> 2021-03-18 02:02:30.216799: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
> 2021-03-18 02:02:30.216806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
> 2021-03-18 02:02:30.216814: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
> 2021-03-18 02:02:30.216844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.217628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.218417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.219196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2021-03-18 02:02:30.219963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
> 2021-03-18 02:02:30.219978: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
> INFO:tensorflow:time(__main__.SparseConvOpTest.test_spar_conv_op): 29.54s
> I0318 02:02:59.557888 140562656798464 test_util.py:1973] time(__main__.SparseConvOpTest.test_spar_conv_op): 29.54s
> [  FAILED  ] SparseConvOpTest.test_spar_conv_op
> ======================================================================
> ERROR: test_spar_conv_op (__main__.SparseConvOpTest)
> SparseConvOpTest.test_spar_conv_op
> ----------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/execroot/__main__/bazel-out/k8-opt/bin/sparse_conv_ops_py_test.runfiles/__main__/tensorflow_sparse_conv_ops/sparse_conv_ops_test.py"", line 32, in test_spar_conv_op
>     dtype=tf.float32)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
>     allow_broadcast=True)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 275, in _constant_impl
>     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 300, in _constant_eager_impl
>     t = convert_to_eager_tensor(value, ctx, dtype)
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 97, in convert_to_eager_tensor
>     ctx.ensure_initialized()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py"", line 539, in ensure_initialized
>     context_handle = pywrap_tfe.TFE_NewContext(opts)
> tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid
> 
> ----------------------------------------------------------------------
> Ran 2 tests in 29.538s
> 
> FAILED (errors=1, skipped=1)
> 
> Please if there's any suggestions to fix the problem, thanks!
> 
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub <https://github.com/google-research/google-research/issues/614#issuecomment-801573632>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AB3WSIPJZPEIPP2QHPZF3BDTEFRZ3ANCNFSM4Y3DZIWA>.
> 

",hi line related code run please check environment error thanks sincerely mar wrote thanks tried compilation stop step run successfully dynamic library running python run run successfully dynamic library successful node read negative value must least one node node zero found device name successful node read negative value must least one node node zero found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible binary deep neural network library use following enable rebuild appropriate compiler frequency service platform host guarantee used device host default version successful node read negative value must least one node node zero successful node read negative value must least one node node zero service platform guarantee used device compute capability device compute capability successful node read negative value must least one node node zero found device name successful node read negative value must least one node node zero found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible successfully dynamic library time time error recent call last file line file line constant file line return value shape file line value file line file line implicit status device kernel image invalid ran please fix problem thanks reply directly view,issue,positive,positive,neutral,neutral,positive,positive
801573632,"Thanks for updating.
I tried [Op compilation instructions](https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md), and now stop at step 6.
```
bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures
``` 
Here's the logs.

```
2021-03-18 02:02:29.285314: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Running tests under Python 3.6.12: /usr/bin/python3
[ RUN      ] SparseConvOpTest.test_session
[  SKIPPED ] SparseConvOpTest.test_session
[ RUN      ] SparseConvOpTest.test_spar_conv_op
2021-03-18 02:02:30.022262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-03-18 02:02:30.058706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.059657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.755GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s
2021-03-18 02:02:30.059699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.060507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:03:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2021-03-18 02:02:30.060519: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-03-18 02:02:30.061462: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-03-18 02:02:30.062386: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-03-18 02:02:30.062544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-03-18 02:02:30.063485: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-03-18 02:02:30.064012: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-03-18 02:02:30.066111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-03-18 02:02:30.066171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.067002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.067805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.068599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.069395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2021-03-18 02:02:30.069622: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-18 02:02:30.074065: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3799900000 Hz
2021-03-18 02:02:30.074340: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5719590 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-03-18 02:02:30.074347: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-03-18 02:02:30.172593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.213582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.214498: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57858d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-03-18 02:02:30.214506: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2021-03-18 02:02:30.214508: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 3090, Compute Capability 8.6
2021-03-18 02:02:30.215138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.215928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.755GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s
2021-03-18 02:02:30.215960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.216749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:03:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2021-03-18 02:02:30.216765: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-03-18 02:02:30.216777: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-03-18 02:02:30.216783: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-03-18 02:02:30.216791: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-03-18 02:02:30.216799: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-03-18 02:02:30.216806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-03-18 02:02:30.216814: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-03-18 02:02:30.216844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.217628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.218417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.219196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-03-18 02:02:30.219963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2021-03-18 02:02:30.219978: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
INFO:tensorflow:time(__main__.SparseConvOpTest.test_spar_conv_op): 29.54s
I0318 02:02:59.557888 140562656798464 test_util.py:1973] time(__main__.SparseConvOpTest.test_spar_conv_op): 29.54s
[  FAILED  ] SparseConvOpTest.test_spar_conv_op
======================================================================
ERROR: test_spar_conv_op (__main__.SparseConvOpTest)
SparseConvOpTest.test_spar_conv_op
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/execroot/__main__/bazel-out/k8-opt/bin/sparse_conv_ops_py_test.runfiles/__main__/tensorflow_sparse_conv_ops/sparse_conv_ops_test.py"", line 32, in test_spar_conv_op
    dtype=tf.float32)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
    allow_broadcast=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 275, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 300, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 97, in convert_to_eager_tensor
    ctx.ensure_initialized()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py"", line 539, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid

----------------------------------------------------------------------
Ran 2 tests in 29.538s

FAILED (errors=1, skipped=1)

```
Please if there's any suggestions to fix the problem, thanks!",thanks tried compilation stop step run successfully dynamic library running python run run successfully dynamic library successful node read negative value must least one node node zero found device name successful node read negative value must least one node node zero found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible binary deep neural network library use following enable rebuild appropriate compiler frequency service platform host guarantee used device host default version successful node read negative value must least one node node zero successful node read negative value must least one node node zero service platform guarantee used device compute capability device compute capability successful node read negative value must least one node node zero found device name successful node read negative value must least one node node zero found device name successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible successfully dynamic library time time error recent call last file line file line constant file line return value shape file line value file line file line implicit status device kernel image invalid ran please fix problem thanks,issue,positive,positive,neutral,neutral,positive,positive
801503084,"Actually, the tf3d/ops folder should be entered rather than newly
created... You should map the non empty tf3d folder into the docker. The copied files
from tf and custom op repo are added on top of it.
Your network setting might has some problem by look at ` GET returned 404
Not Found`
`error: cannot find python/_pywrap_tensorflow_internal.so` this might be
because your python env is not config correctly. please check tensorflow
pip package 2.3 is isntall inside docker, and configure.sh loads its
library path correctly.
Thanks!

Sincerely,
Rui

On Wed, Mar 17, 2021 at 4:09 PM FraMan ***@***.***> wrote:

> @HRLTY <https://github.com/HRLTY> Hi, I got the point on the error. When
> i created the tf3d/ops folder inside there were only the files copied with
> the cp commands and not the entire directory that you linked. I copied
> those file and the error now is the following:
>
> ***@***.***:/working_dir/tf3d/ops# bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures
> Extracting Bazel installation...
> Starting local Bazel server and connecting to it...
> WARNING: Download from https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/fcc4b966f1265f466e82617020af93670141b009.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
> WARNING: /root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/external/local_config_tf/BUILD:5395:1: target 'libtensorflow_framework.so' is both a rule and a file; please choose another name for the rule
> INFO: Analyzed target //:sparse_conv_ops_py_test (45 packages loaded, 1049 targets configured).
> INFO: Found 1 target...
> INFO: From Compiling submanifold_sparse_conv_utils.cc:
> In file included from submanifold_sparse_conv_utils.cc:15:0:
> submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> INFO: From Compiling submanifold_sparse_conv_grad_launcher.cc:
> In file included from submanifold_sparse_conv_grad_launcher.cc:17:0:
> submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> INFO: From Compiling submanifold_sparse_conv_ops.cc:
> In file included from submanifold_sparse_conv_ops.cc:21:0:
> submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> INFO: From Compiling submanifold_sparse_conv_grad.cc:
> In file included from submanifold_sparse_conv_grad.cc:22:0:
> submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> INFO: From Compiling submanifold_sparse_conv_launcher.cc:
> In file included from submanifold_sparse_conv_launcher.cc:17:0:
> submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
>  #pragma unroll
>  ^
> ERROR: /working_dir/tf3d/ops/BUILD:161:1: Linking of rule '//:tensorflow_sparse_conv_ops/_sparse_conv_ops.so' failed (Exit 1): gcc failed: error executing command
>   (cd /root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/execroot/__main__ && \
>   exec env - \
>     PATH=/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
>     PWD=/proc/self/cwd \
>     TF_HEADER_DIR=/usr/local/lib/python3.6/dist-packages/tensorflow/include \
>     TF_NEED_CUDA=0 \
>     TF_SHARED_LIBRARY_DIR=/usr/local/lib/python3.6/dist-packages/tensorflow \
>     TF_SHARED_LIBRARY_NAME=libtensorflow_framework.so.2 \
>   /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow_sparse_conv_ops/_sparse_conv_ops.so-2.params)
> Execution platform: @local_execution_config_platform//:platform
> /usr/bin/ld.gold: error: cannot find python/_pywrap_tensorflow_internal.so
> collect2: error: ld returned 1 exit status
> Target //:sparse_conv_ops_py_test failed to build
> INFO: Elapsed time: 273.785s, Critical Path: 61.35s
> INFO: 77 processes: 77 local.
> FAILED: Build did NOT complete successfully
> FAILED: Build did NOT complete successfully
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/630#issuecomment-801499779>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AB3WSIO5HKVMZES6STDM3MDTEEZDRANCNFSM4ZEAR73A>
> .
>
",actually folder rather newly map non empty folder docker copied custom added top network setting might problem look get returned found error find might python correctly please check pip package inside docker library path correctly thanks sincerely wed mar wrote hi got point error folder inside copied entire directory linked copied file error following run installation starting local server warning class get returned found warning target rule file please choose another name rule target loaded found target file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll error linking rule exit error command execution platform platform error find collect error returned exit status target build time critical path local build complete successfully build complete successfully reply directly view,issue,negative,positive,positive,positive,positive,positive
801499779,"@HRLTY Hi, I got the point on the error. When i created the tf3d/ops folder inside there were only the files copied with the cp commands and not the entire directory that you linked. I copied those file and the error now is the following:

```
root@218ddee70f12:/working_dir/tf3d/ops# bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: Download from https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/fcc4b966f1265f466e82617020af93670141b009.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
WARNING: /root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/external/local_config_tf/BUILD:5395:1: target 'libtensorflow_framework.so' is both a rule and a file; please choose another name for the rule
INFO: Analyzed target //:sparse_conv_ops_py_test (45 packages loaded, 1049 targets configured).
INFO: Found 1 target...
INFO: From Compiling submanifold_sparse_conv_utils.cc:
In file included from submanifold_sparse_conv_utils.cc:15:0:
submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
INFO: From Compiling submanifold_sparse_conv_grad_launcher.cc:
In file included from submanifold_sparse_conv_grad_launcher.cc:17:0:
submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
INFO: From Compiling submanifold_sparse_conv_ops.cc:
In file included from submanifold_sparse_conv_ops.cc:21:0:
submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
INFO: From Compiling submanifold_sparse_conv_grad.cc:
In file included from submanifold_sparse_conv_grad.cc:22:0:
submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
INFO: From Compiling submanifold_sparse_conv_launcher.cc:
In file included from submanifold_sparse_conv_launcher.cc:17:0:
submanifold_sparse_conv_utils.h:36:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:53:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:84:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:126:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:148:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:161:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:165:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:178:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
submanifold_sparse_conv_utils.h:190:0: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
 #pragma unroll
 ^
ERROR: /working_dir/tf3d/ops/BUILD:161:1: Linking of rule '//:tensorflow_sparse_conv_ops/_sparse_conv_ops.so' failed (Exit 1): gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/execroot/__main__ && \
  exec env - \
    PATH=/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF_HEADER_DIR=/usr/local/lib/python3.6/dist-packages/tensorflow/include \
    TF_NEED_CUDA=0 \
    TF_SHARED_LIBRARY_DIR=/usr/local/lib/python3.6/dist-packages/tensorflow \
    TF_SHARED_LIBRARY_NAME=libtensorflow_framework.so.2 \
  /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow_sparse_conv_ops/_sparse_conv_ops.so-2.params)
Execution platform: @local_execution_config_platform//:platform
/usr/bin/ld.gold: error: cannot find python/_pywrap_tensorflow_internal.so
collect2: error: ld returned 1 exit status
Target //:sparse_conv_ops_py_test failed to build
INFO: Elapsed time: 273.785s, Critical Path: 61.35s
INFO: 77 processes: 77 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```",hi got point error folder inside copied entire directory linked copied file error following root run installation starting local server warning class get returned found warning target rule file please choose another name rule target loaded found target file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll file included warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll warning unroll unroll error linking rule exit error command execution platform platform error find collect error returned exit status target build time critical path local build complete successfully build complete successfully,issue,negative,positive,positive,positive,positive,positive
801493129,"HI @FrancescoMandru ,
`ERROR: The 'run' command is only supported from within a workspace (below a directory having a WORKSPACE file).` indicates it might be in wrong folder, it wasn't able to find WORKSPACE file in the current folder.
Bazel should be run inside this folder https://github.com/google-research/google-research/tree/master/tf3d/ops.
Otherwise, your bazel config might be wrong. Please check some bazel tutorial and examples.
thanks.",hi error command within directory file might wrong folder able find file current folder run inside folder otherwise might wrong please check tutorial thanks,issue,negative,negative,neutral,neutral,negative,negative
801489777,"@afathi3 @HRLTY I tried from scratch. I'm inside the right folder. What do you mean by BUILD file with target sparse_conv_ops_py_test? I don't find it. Yes my Docker should map correctly the workspace. I attach the error with the new commands

```
root@df62a6af34ff:/working_dir/tf3d/ops# ./configure.sh
Do you want to build ops again TensorFlow CPU pip package? Y or enter for CPU (tensorflow-cpu), N for GPU (tensorflow). [Y/n] y
Build with CPU pip package.
Does the pip package have tag manylinux2010 (usually the case for nightly release after Aug 1, 2019, or official releases past 1.14.0)?. Y or enter for manylinux2010, N for manylinux1. [Y/n] n
Are you building against TensorFlow 2.1(including RCs) or newer?[Y/n] y
Build against TensorFlow 2.1 or newer.
Using installed tensorflow
2021-03-17 22:39:53.686456: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2021-03-17 22:39:53.686865: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-03-17 22:39:57.964222: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2021-03-17 22:39:57.965215: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
root@df62a6af34ff:/working_dir/tf3d/ops# bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
.bazelrc
Extracting Bazel installation...
ERROR: The 'run' command is only supported from within a workspace (below a directory having a WORKSPACE file).
See documentation at https://docs.bazel.build/versions/master/build-ref.html#workspace
root@df62a6af34ff:/working_dir/tf3d/ops# touch WORKSPACE
root@df62a6af34ff:/working_dir/tf3d/ops# bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures
Starting local Bazel server and connecting to it...
ERROR: Skipping 'sparse_conv_ops_py_test': couldn't determine target from filename 'sparse_conv_ops_py_test'
WARNING: Target pattern parsing failed.
ERROR: couldn't determine target from filename 'sparse_conv_ops_py_test'
INFO: Elapsed time: 3.852s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
FAILED: Build did NOT complete successfully (0 packages loaded)
```",tried scratch inside right folder mean build file target find yes docker map correctly attach error new root want build pip package enter build pip package pip package tag usually case nightly release official past enter building build could load dynamic library open object file file directory ignore set machine could load dynamic library open object file file directory ignore set machine root run warning following longer read please transfer content import path one standard installation error command within directory file see documentation root touch root run starting local server error skipping could determine target warning target pattern error could determine target time build complete successfully loaded build complete successfully loaded,issue,negative,positive,neutral,neutral,positive,positive
801483841,"Hi @FrancescoMandru , please make sure you are in tf3d/ops folder, which has a BUILD file there with target sparse_conv_ops_py_test. thanks. Also, make sure your docker's folder mapping makes you able to access tf3d folder inside.
FYI, without nvidia docker, please use CPU compilation since there might be errors with Mac in GPU compilation.",hi please make sure folder build file target thanks also make sure docker folder able access folder inside without docker please use compilation since might mac compilation,issue,positive,positive,positive,positive,positive,positive
801481879,"Sorry did you try everything from scratch with the new instructions?
",sorry try everything scratch new,issue,negative,negative,negative,negative,negative,negative
801476094,"> Hi @FrancescoMandru,
> 
> We have updated the [op compilation instructions](https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md).
> Could you please try again and let us know if you still have any issues?

I tried but I got the same error:

```
root@2409c5abf43b:/working_dir/tf3d/ops# bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
ERROR: Skipping 'sparse_conv_ops_py_test': couldn't determine target from filename 'sparse_conv_ops_py_test'
WARNING: Target pattern parsing failed.
ERROR: couldn't determine target from filename 'sparse_conv_ops_py_test'
INFO: Elapsed time: 10.829s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
FAILED: Build did NOT complete successfully (0 packages loaded)
```

I also tried with docker image without CPU `tensorflow:2.3.0-custom-op-ubuntu16`",hi compilation could please try let u know still tried got error root run installation starting local server error skipping could determine target warning target pattern error could determine target time build complete successfully loaded build complete successfully loaded also tried docker image without,issue,negative,positive,positive,positive,positive,positive
801338354,"Hi @Wayne-wonderai,
We have updated the [setup](https://github.com/google-research/google-research/blob/master/tf3d/doc/setup.md) and also [Op compilation instructions](https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md). Please take another look and let us know if you still have an issue.",hi setup also compilation please take another look let u know still issue,issue,negative,neutral,neutral,neutral,neutral,neutral
801323989,"Hi @FrancescoMandru,

We have updated the [op compilation instructions](https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md).
Could you please try again and let us know if you still have any issues?
",hi compilation could please try let u know still,issue,negative,neutral,neutral,neutral,neutral,neutral
801286229,"Thank you @hewittzgh.
We have updated the [setup](https://github.com/google-research/google-research/blob/master/tf3d/doc/setup.md) and also [Op compilation instructions](https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md). Please take another look and let us know if you still have an issue.

",thank setup also compilation please take another look let u know still issue,issue,positive,neutral,neutral,neutral,neutral,neutral
800680827,"Fixed with the following change on line 2805 of `tensorflow/python/ops/array_ops.py`:

```
-     if np.prod(shape) < 1000:
+     if tf.math.reduce_prod(shape) < 1000:
```",fixed following change line shape shape,issue,negative,positive,neutral,neutral,positive,positive
800634170,"I see my improper phrasing now. You have allowed me to reach the heart of my question that I could not think of. 

When provided with a suboptimal teacher, is the student be able to provide **more** reliable labels for the augmented data than the teacher?

This is in reference to the lines ""Despite the strong performance of Pseudo Labels methods, they have one main drawback: if the pseudo labels are inaccurate, the student will learn from inaccurate data. As a result, the student may not get significantly better than the teacher"" from the original paper.

Does the student achieve better performance than the teacher in this manner? With only the pseudo labels method, this would be more clear, but the student and the teacher both improve due to MPL. Was this confirmed?

Were there any tests or results showing the teacher's improvement with MPL or just the student?",see improper phrasing reach heart question could think provided suboptimal teacher student able provide reliable augmented data teacher reference despite strong performance pseudo one main drawback pseudo inaccurate student learn inaccurate data result student may get significantly better teacher original paper student achieve better performance teacher manner pseudo method would clear student teacher improve due confirmed showing teacher improvement student,issue,positive,positive,positive,positive,positive,positive
800595154,"Please correct me if I misunderstand your question.

If the student is trained on **augmented unlabeled data** and its **training** loss consistently goes down, then the student does produce reliable labels for these images, no?",please correct misunderstand question student trained augmented unlabeled data training loss consistently go student produce reliable,issue,negative,positive,positive,positive,positive,positive
800510440,"> First of all thanks for the paper and code. It inspired me for some own work.
> @hyhieu Can you help me in understanding the following two parts of your paper which are related to this thread?
> 
> 1. I do not fully understand your implementation of the dot product as elaborated above. I understand the Taylor expansion background but as written [here](https://github.com/kekmodel/MPL-pytorch/issues/6#issuecomment-800283613), I am not sure if it is correct. Can you clarify?
> 2. You write in the paper below (3) that with soft labels you can simply backpropagate to obtain the teacher gradient. This is not obvious to me. How can you go around the log-gradient-trick/REINFORCE when using soft labels?

Replied to you at [that issue](https://github.com/kekmodel/MPL-pytorch/issues/6).",first thanks paper code inspired work help understanding following two paper related thread fully understand implementation dot product understand expansion background written sure correct clarify write paper soft simply obtain teacher gradient obvious go around soft issue,issue,positive,positive,positive,positive,positive,positive
800328629,"First of all thanks for the paper and code. It inspired me for some own work.
@hyhieu Can you help me in understanding the following two parts of your paper which are related to this thread?

1. I do not fully understand your implementation of the dot product as elaborated above. I understand the Taylor expansion background but as written [here](https://github.com/kekmodel/MPL-pytorch/issues/6#issuecomment-800283613), I am not sure if it is correct. Can you clarify?
2. You write in the paper below (3) that with soft labels you can simply backpropagate to obtain the teacher gradient. This is not obvious to me. How can you go around the log-gradient-trick/REINFORCE when using soft labels?",first thanks paper code inspired work help understanding following two paper related thread fully understand implementation dot product understand expansion background written sure correct clarify write paper soft simply obtain teacher gradient obvious go around soft,issue,positive,positive,positive,positive,positive,positive
799867859,"@afathi3 Sorry, your update did not work properly.This is my error message:
```
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
I0316 01:00:52.480308 140413338064704 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
I0316 01:00:52.480569 140413338064704 train.py:149] writing path is /tmp/tf3d_experiment/seg_rio_001
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/google-research-tf3d/tf3d/train.py"", line 221, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/google-research-tf3d/tf3d/train.py"", line 154, in main
    __import__(module_name)
  File ""/google-research-tf3d/tf3d/gin_imports.py"", line 25, in <module>
    from tf3d.instance_segmentation import metric as instance_segmentation_metric
  File ""/google-research-tf3d/tf3d/instance_segmentation/metric.py"", line 20, in <module>
    from tf3d import base_ap_metric
  File ""/google-research-tf3d/tf3d/base_ap_metric.py"", line 23, in <module>
    from tf3d.utils import label_map_util
  File ""/google-research-tf3d/tf3d/utils/label_map_util.py"", line 19, in <module>
    from tf3d.utils.protos import string_int_label_map_pb2
ImportError: cannot import name 'string_int_label_map_pb2'
```
My fix: Run `protoc string_int_label_map.proto --python_out=.` under `utils/protos` folder and I can get a `string_int_label_map_pb2.py` file in the same path. The error was then fixed.

PS:  I also want to ask about the display problem of mesh in tensorboard: https://github.com/google-research/google-research/issues/631#issuecomment-799462735",sorry update work error message writing path recent call last file line file line code file line module main file line run main file line main file line main file line module import metric file line module import file line module import file line module import import name fix run folder get file path error fixed also want ask display problem mesh,issue,negative,positive,neutral,neutral,positive,positive
799711451,"I just added an update that should fix the string_int_label_map_pb2 issue. Let me know if it still is there.
We are working on the pip install issue (sorry for the inconvenience)
",added update fix issue let know still working pip install issue sorry inconvenience,issue,negative,negative,negative,negative,negative,negative
799462735,"When I was using tensroboard, I found that the 3D map under the mesh tag could not be displayed (The position where the image should be displayed is blank). Is the mesh plugin in the tf-nightly version? Do I  need to use the nightly version of tensorboard to display the mesh map normally?",found map mesh tag could displayed position image displayed blank mesh version need use nightly version display mesh map normally,issue,negative,positive,neutral,neutral,positive,positive
799356187,"Hi Rui,

thanks a lot again. Everything works now!

Great help :)

Best regards,

Max",hi thanks lot everything work great help best,issue,positive,positive,positive,positive,positive,positive
799236577,"I also found that you have modified the code about importing 'string_int_lable_map_pd2'. In label_map_util.py, line 19, 'from object_detection.protos import string_int_label_map_pb2', it does not work under my test. I still need to download object_detection and compile it with the protoc tool. Can be imported, it is recommended to compile by default and import directly by tf3d.utils.protos.",also found code line import work test still need compile tool compile default import directly,issue,negative,positive,neutral,neutral,positive,positive
798664821,"Oh, our format is different for sure. You can check the details here
https://github.com/google-research/google-research/blob/master/tf3d/datasets/specs/waymo_frames.py
Actually if you modify the script, you can download just a few chunks of
data to try out too.
Thanks

On Sat, Mar 13, 2021 at 1:35 AM Max Ronecker ***@***.***>
wrote:

> Hi Rui,
>
> thanks a lot for the help and response.
>
> Downloading the dataset using the script might to the trick. You're right
> about that there is something wrong with the data.
>
> I am currently downloading the dataset, probably takes a while but I keep
> you updated.
>
> Thanks a lot,
>
> Max
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/627#issuecomment-798030475>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AB3WSILTTNZCH3INFV5BOTLTDMWW3ANCNFSM4ZCSQDMA>
> .
>
-- 
Thanks!

Sincerely,
Rui
",oh format different sure check actually modify script data try thanks sat mar wrote hi thanks lot help response script might trick right something wrong data currently probably keep thanks lot reply directly view thanks sincerely,issue,positive,positive,positive,positive,positive,positive
798030475,"Hi Rui,

thanks a lot for the help and response.

Downloading the dataset using the script might to the trick. You're right about that there is something wrong with the data.

I am currently downloading the dataset, probably takes a while but I keep you updated.

Thanks a lot,

Max",hi thanks lot help response script might trick right something wrong data currently probably keep thanks lot,issue,positive,positive,neutral,neutral,positive,positive
797821392,"Experimenting with changing scann_builder.py causes the runtime of the residual quantitation to explode. ~50x slower.

3283 seconds with the value set to True vs 68 seconds with the value set to False

so I assume that it is correctly doing anisotropic hashing instead of plain k-means




-Assuming- changing this parameter gives the correct behaviour (I have not checked this, just observed runtime differences) you can fix this in a couple ways. One by editing 

scann_builder.py's section

@_factory_decorator(""score_ah"")


to set a variable

noise_shaped_training = not(math.isnan(anisotropic_quantization_threshold)) and (0 <= anisotropic_quantization_threshold)



Then add in the return section of @_factory_decorator(""score_ah"")

use_noise_shaped_training: {noise_shaped_training}





(pardon any syntax errors)
",residual explode value set true value set false assume correctly anisotropic instead plain parameter correct behaviour checked fix couple way one section set variable add return section pardon syntax,issue,positive,negative,neutral,neutral,negative,negative
797772700,"For 2) If they do not have it implemented you will have to calculate/save each heuristic weight value (eq 3 in the paper) and then edit the loss function so it is calculated correctly.

I would assume that ""use_noise_shaped_training"" is supposed to be set as the noise_shaping_threshold is in scann_builder.py.. however looking at the code that doesn't seem to be the case",heuristic weight value paper edit loss function calculated correctly would assume supposed set however looking code seem case,issue,negative,neutral,neutral,neutral,neutral,neutral
797685463,"Hi @MaxPRon ,
THanks for your question. 
I have seen some `OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input,` errors when I used wrong version of data, i.e. missing a field, or mismatch Tensorflow example specification.

In your case, I guess the data is incorrect or corrupted. Did you download using our [script](https://github.com/google-research/google-research/blob/master/tf3d/doc/waymo_open_dataset.md)?  Maybe re-download? 

Otherwise, I would guess somewhere the python decoding setup is incorrect.

As a sanity check, make sure it globs and reads the correct file in data loader.
Thanks.",hi thanks question seen invalid argument could parse example input used wrong version data missing field mismatch example specification case guess data incorrect corrupted script maybe otherwise would guess somewhere python setup incorrect sanity check make sure correct file data loader thanks,issue,negative,positive,neutral,neutral,positive,positive
797656303,"Hi @FrancescoMandru,

This is the function that is used for voxelization:
https://github.com/google-research/google-research/blob/master/tf3d/utils/voxel_utils.py#L305

This is the function for going back to point cloud from voxels:
https://github.com/google-research/google-research/blob/master/tf3d/utils/voxel_utils.py#L373

Let us know if you have more questions.
",hi function used function going back point cloud let u know,issue,negative,neutral,neutral,neutral,neutral,neutral
797261367,"With the help of the author, the problem is that the input image folder should contain RGB images rather than grayscale images. In this line (https://github.com/google-research/google-research/blob/master/coltran/sample.py#L161), the RGB images are converted to gray images. Since the problem is solved, I close this issue. Thanks for the kind help! @MechCoder",help author problem input image folder contain rather line converted gray since problem close issue thanks kind help,issue,positive,positive,positive,positive,positive,positive
797032443,"Hi @Wayne-wonderai,

Sorry about the inconvenience.
Based on your experience, it might be that changes in TensorFlow and object detection API have broken the op installation routine. Rui and I are investigating this and will get back to you soon (hopefully with a fix if we find an issue).
",hi sorry inconvenience based experience might object detection broken installation routine investigating get back soon hopefully fix find issue,issue,negative,negative,negative,negative,negative,negative
796981676,"Hi @Wayne-wonderai ,
Please try with the earlier version of those repos to fetch files for now. For example, [Tensorflow 2.3.0](https://github.com/tensorflow/tensorflow/tree/v2.3.0/third_party). 
Thanks!",hi please try version fetch example thanks,issue,positive,positive,positive,positive,positive,positive
796639432,"That's a great question. I think any form of residual or gated update should be much better and more stable than a direct replacement. So it is quite likely that the momentum update you suggested would work well, too.

As for more iterations: if you go far beyond 5 or so iterations you might benefit a lot from applying intermediate loss terms, like we do in one of the experiments in the appendix. This generally improves stability as well.",great question think form residual gated update much better stable direct replacement quite likely momentum update would work well go far beyond might benefit lot intermediate loss like one appendix generally stability well,issue,positive,positive,positive,positive,positive,positive
796531936,"I didn't find the inference script, but you can use the tensorboard to visualize the depth map during training.",find inference script use visualize depth map training,issue,negative,neutral,neutral,neutral,neutral,neutral
796517535,"Thanks for replying. As I tried to use `pip install tensorflow==2.3` in my virtual environment (I used `conda install tensorflow=2.3 `before), and it works well.
However, I got some other two problems when I am trying to compile the ops locally .
**First**, missing files in `tensorflow/third_party` folder
   * `com_google_absl_fix_mac_and_nvcc_build.patch`
   * `com_google_absl.BUILD`
   * `eigen.BUILD`<br>
    
**Second**, the     <a href='https://github.com/tensorflow/models/tree/master/research/object_detection#tensorflow-2x'>Tensorflow Object Detection API</a>. changes tensorflow version to 2.4.1...


 ",thanks tried use pip install virtual environment used install work well however got two trying compile locally first missing folder second object detection version,issue,negative,positive,neutral,neutral,positive,positive
796516744,@afathi3 Thank you for your contribution to this great project.Looking forward to the improvement of the project.,thank contribution great forward improvement project,issue,positive,positive,positive,positive,positive,positive
796485885,"You are right. I will add config files and script for Rio Dataset soon. I am going to also release trained checkpoints for different tasks and datasets soon.
",right add script rio soon going also release trained different soon,issue,negative,positive,positive,positive,positive,positive
796387501,"Hi, thanks for your interest! We don't support this at the moment - but training shouldn't take too long on a single GPU (about 1M steps to converge to something reasonable).",hi thanks interest support moment training take long single converge something reasonable,issue,positive,positive,neutral,neutral,positive,positive
795749806,"Yes, I use the latest checkpoints from here (https://console.cloud.google.com/storage/browser/gresearch/coltran/colorizer_v2). I send some example to your email. Besides, the code reference this issue #583.",yes use latest send example besides code reference issue,issue,negative,positive,positive,positive,positive,positive
795738843,Could you also try with our latest checkpoints at https://storage.cloud.google.com/gresearch/coltran/coltran.zip? They give slightly better results.,could also try latest give slightly better,issue,negative,positive,positive,positive,positive,positive
795730851,"Hi @xc-csc101 thanks for your interest in our paper.

Could you provide some examples?",hi thanks interest paper could provide,issue,positive,positive,positive,positive,positive,positive
794482673,"Sorry for inconvenience, these files are not available any more (there were several changes as a result pre-trained model became outdated), so they have to be re-trained.
I will update the readme file. 
Please use [description](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md) of model re-training.",sorry inconvenience available several result model outdated update file please use description model,issue,negative,negative,negative,negative,negative,negative
794266065,"@Himanshu-Embold you might find it useful to see my [comment on an issue on recreating fine-tuning](https://github.com/google-research/google-research/issues/582#issuecomment-794260254). I found it fairly easy to integrate CuBERT into Huggingface Transformers to finetune. Following a similar path would likely make it easy to load and use for your usecase.

Ideally there could be CuBERT integration into the huggingface model repo so can just download and use with a single line, but currently this doesn't exist.",might find useful see comment issue found fairly easy integrate following similar path would likely make easy load use ideally could integration model use single line currently exist,issue,positive,positive,positive,positive,positive,positive
794260254,"After some out-of-band discussions with @maniatis confirmed that the finetuning code had to be intentionally omitted due to ties with internal Google source.

However, Petros was thankfully able to confirm some details like that long functions were handled with truncating from the beginning and that eval was done equivalently.

That was enough to replicate finetuning of at least one task I tried. I found the easiest way to do this was using [Huggingface Transformers](https://github.com/huggingface/transformers). First I ran [conversion scripts](https://huggingface.co/transformers/converting_tensorflow_models.html#bert) and then was able to use this as input to `BertForSequenceClassification.from_pretrained()` and finetune with the huggingface [Train](https://huggingface.co/transformers/training.html#trainer) functionality. In [this gist](https://gist.github.com/DNGros/7c2fa0dcf566bd9f3732618669b591dd) I put the bert config used to convert, and the Huggingface API-complient tokenizer I used.

I'm currently in the middle of a messy research code version of playing with all the tasks, so not in a place to share whole pipelines. However, if you're seeing this in the future and run in to difficulties getting it working in your workflow, feel free to @ mention me here and I can give an update on if I had shared any more code for the tasks at that time.",confirmed code intentionally due internal source however thankfully able confirm like long handled beginning done equivalently enough replicate least one task tried found easiest way first ran conversion able use input train functionality gist put used convert used currently middle messy research code version place share whole however seeing future run getting working feel free mention give update code time,issue,positive,positive,positive,positive,positive,positive
794242585,"No prob - also for bullet point 3 I updated it to say ""the neg log of averaged matching probability"". Hope everything goes well!",prob also bullet point say log matching probability hope everything go well,issue,positive,neutral,neutral,neutral,neutral,neutral
794158819,"Hi, @Wayne-wonderai ,
Thanks for your question. We haven’t seen this symbol missing before. I guess it’s cuda version related. As you using the pip version of tensorflow 2.3, which is with cuda 10?

In any case, please try compiling the ops locally.

We will clarify in the instruction.",hi thanks question seen symbol missing guess version related pip version case please try locally clarify instruction,issue,negative,neutral,neutral,neutral,neutral,neutral
793905546,Thanks for your reply . I am trying to do the action recognition experiment in the paper!,thanks reply trying action recognition experiment paper,issue,negative,positive,positive,positive,positive,positive
793307644,"Hello, and thanks for your interest! This is described in Section 4.3 of our paper and here are the steps you can take to do action recognition:
1. There is no additional training of the embedding model (Pr-VIPE).
2. We compute Pr-VIPE embeddings on the detected pose from each video frame in Penn Action. To be more precise, we used detected poses given the person boxes provided by the Penn Action and for frames without any given box (box is missing), we smear a box from a closest frame.
3. The frame matching distance for a pair of frames i, j is the the neg log of the averaged matching probability of frames (i-6, i-3, i, i+3, i+6) and (j-6, j-3, j, j+3, j+6), of each corresponding frame pair
4. Use DTW to align two videos using frame matching distance, and the nearest neighbor is the video with the smallest averaged distance  after alignment
5. Do action classification of an unlabelled video by finding the nearest neighbor of a labelled video using the procedure above
There are additional implementation details in the Supplementary Materials (Section B), in particular, we use the standard protocol defined in https://openaccess.thecvf.com/content_cvpr_2015/papers/Nie_Joint_Action_Recognition_2015_CVPR_paper.pdf to split the dataset.",hello thanks interest section paper take action recognition additional training model compute pose video frame action precise used given person provided action without given box box missing smear box frame frame matching distance pair log matching probability corresponding frame pair use align two frame matching distance nearest neighbor video distance alignment action classification video finding nearest neighbor video procedure additional implementation supplementary section particular use standard protocol defined split,issue,negative,positive,positive,positive,positive,positive
793281352,"Apologies @expectopatronum for the lag in reply - this is a shared repo for research open source code so notifications can sometimes get lost in the noise. Great to hear of your interest using ROAR. Yes, you are correct in your interpretation of the rescale_image code, this rescales the range of values between [0,1]. Thus, the minimum possible attribution becomes 0 and the maximum is 1. The overall ranking remains the same.   

 I'll close this for now -- but if you have any other questions do reach out by tagging our github handles or by sending a message to the email address listed on the paper.",lag reply research open source code sometimes get lost noise great hear interest roar yes correct interpretation code range thus minimum possible attribution becomes maximum overall remains close reach sending message address listed paper,issue,positive,positive,positive,positive,positive,positive
792655392,"That's a great question! I just checked and we first filter by number of objects, which leaves about 50k examples, and then get the evaluation split (final 320 examples) from there and use the rest for training. Minor differences in this protocol should not matter much, however, as the variance between runs with different seeds on the same splits should be higher than what you get if you compared on different splits.",great question checked first filter number leaf get evaluation split final use rest training minor protocol matter much however variance different higher get different,issue,positive,positive,positive,positive,positive,positive
792629306,"Do 70K count before or after filtering by <= 6 objects?

It would indeed be better to have the actual splits for repro ^^ or at least a code snippet that does filtering / splits

FYI my pytorch reimplementation is at https://github.com/vadimkantorov/yet_another_pytorch_slot_attention - also there are some nuisance because of missing padding=""same"" mode in PyTorch, so I did some crude interpolate call instead of fighting the paddings. 

> I think your discrepancy comes from training on a different dataset

Could be! I'll do a rerun to check",count filtering would indeed better actual least code snippet filtering also nuisance missing mode crude interpolate call instead fighting think discrepancy come training different could rerun check,issue,negative,negative,negative,negative,negative,negative
792623888,"Hi Vadim, thanks a lot for your comment. We used 320 examples for evaluation, which IIRC were taken from the split right after the first 70k examples which we used for training. I verified that the results did not vary significantly based on the evaluation split (i.e. always stayed in the 98-99% range; this holds even when evaluating on samples from the training set). We chose 320 evaluation examples to stay in line with IODINE, but to be safe I'd recommend taking a larger evaluation split in future experiments.

I think your discrepancy comes from training on a different dataset (the non-DeepMind version of CLEVR) and then trying to transfer to CLEVR w/ masks (i.e. DeepMind's version). We remarked in the Readme that this is not advisable since dataset statistics appear to differ between the two datasets (e.g. object scale, camera position etc.). ",hi thanks lot comment used evaluation taken split right first used training vary significantly based evaluation split always stayed range even training set chose evaluation stay line iodine safe recommend taking evaluation split future think discrepancy come training different version trying transfer version advisable since statistic appear differ two object scale camera position,issue,positive,positive,positive,positive,positive,positive
792505570,"@soldierofhell Thanks for the details! It's interesting that a function *within* the package is not included in the compiled `so` library. I would guess the problem is more related to the differences between your Linux compiling environment v.s. the docker image from `custom-op` repo than Tensorflow version. There might also be a problem loading the compiled `so` in your environment somehow.
I suggest looking at Bazel's *log* for the actual gcc/clang command line and compare it with that from compiling inside the docker. 

Unfortunately, given the various dependencies on other packages and TensorFlow's Bazel build files and the Bazel helper functions in this package, it won't be easy to translate to makefile/CmakeList manually. With that said, we certainly welcome any contribution. 
",thanks interesting function within package included library would guess problem related environment docker image version might also problem loading environment somehow suggest looking log actual command line compare inside docker unfortunately given various build helper package wo easy translate manually said certainly welcome contribution,issue,positive,positive,positive,positive,positive,positive
792235563,"> Hello Jingyuan, co-author Ting Liu has added this inference script to the repo: https://github.com/google-research/google-research/blob/master/poem/infer.py. Both the input and output files are csvs. Hope this is helpful!

Hello! The script works fine now. Thank you for your efforts!
It has been a great help!

Best,
Jingyuan",hello ting added inference script input output hope helpful hello script work fine thank great help best,issue,positive,positive,positive,positive,positive,positive
792071377,"@HRLTY, I've checked that this not found symbol ValidateConvInputs is a functions defined in your utils https://github.com/google-research/google-research/blob/6ec0dfae4efe641d3c950ea82e0abe90c9813f28/tf3d/ops/submanifold_sparse_conv_utils.h#L206 I guess the problem is that there are many files in the op source dir and the build process with gcc is more complicated than one file/one line example in Tensorflow doc? Maybe you could provider a makefile/CMakeList or gcc commands list to build it in a correct way? ",checked found symbol defined guess problem many source build process complicated one line example doc maybe could provider list build correct way,issue,negative,neutral,neutral,neutral,neutral,neutral
792057069,Thank you for your great work. I would like to know if it is possible to evaluate on a custom dataset?  How can i get the motion vector for a given custom function ?,thank great work would like know possible evaluate custom get motion vector given custom function,issue,positive,positive,positive,positive,positive,positive
791615786,"Hi @soldierofhell ,
The error in your case is a different symbol. Generally, it should be able to manually compile in most environments. However, due to TF compiling environment differences, some symbols exist in other tf `so` libraries outisde of `libtensorflow_framework.so.2`. The general solution is to find that symbol's containing `so` file (check other tutorials for this), link it during the compilation too (i.e. _ZN10tensorflow4tf3d18ValidateConvInputsEbiPNS_15OpKernelContextE), and make sure run time can access the `so` library too.

To keep things standard, we are only testing the docker environment used for compiling pip TensorFlow versions(from https://github.com/tensorflow/custom-op). TF 2.4's support, based on our initial exploration, is also a similar `symbol not found` problem. We plan to look into that later.",hi error case different symbol generally able manually compile however due environment exist general solution find symbol file check link compilation make sure run time access library keep standard testing docker environment used pip support based initial exploration also similar symbol found problem plan look later,issue,negative,positive,positive,positive,positive,positive
791201138,"I find similar issues were reported before but no response.
Include the links of previously reported issues in order to raise the concern.

https://github.com/google-research/google-research/issues/435

https://github.com/google-research/google-research/issues/557",find similar response include link previously order raise concern,issue,negative,negative,neutral,neutral,negative,negative
791093517,"Thank you so much for bringing this to our attention. I am updating the README to describe the reason for this duplication, and verified that it doesn't affect the separation between fine-tuning and pre-training datasets. 
",thank much attention describe reason duplication affect separation,issue,negative,positive,positive,positive,positive,positive
790904737,"Hello Jingyuan, co-author Ting Liu has added this inference script to the repo: https://github.com/google-research/google-research/blob/master/poem/infer.py. Both the input and output files are csvs. Hope this is helpful! ",hello ting added inference script input output hope helpful,issue,positive,neutral,neutral,neutral,neutral,neutral
790663927,You should also probably change the title to include [MPL] at the beginning,also probably change title include beginning,issue,negative,neutral,neutral,neutral,neutral,neutral
790663548,"I agree

First order Taylor:
f(x) = f(a) + f'(x)(x-a)

let f(x) be the cross entropy function, x is the new parameters
let a be x+h, or the old parameters (h is the gradient on the unlabeled data for the old parameters)
f(x) = f(x+h) + f'(x)*(x-(x+h))
 f(x+h) -f(x) = f'(x) * h

old-new",agree first order let cross entropy function new let old gradient unlabeled data old,issue,negative,positive,positive,positive,positive,positive
790642143,"Ok, I've tried with 2.4.1. More or less the same error:
```
tensorflow.python.framework.errors_impl.NotFoundError: ./tf3d/ops/tensorflow_sparse_conv_ops/_sparse_conv_ops.so: undefined symbol: _ZN10tensorflow4tf3d18ValidateConvInputsEbiPNS_15OpKernelContextE
```",tried le error undefined symbol,issue,negative,neutral,neutral,neutral,neutral,neutral
790629908,"Hi @HRLTY,
I've compiled it manually, but failed to load the op at the end with error
```
tensorflow.python.framework.errors_impl.NotFoundError: ./tf3d/ops/tensorflow_sparse_conv_ops/_sparse_conv_ops.so: undefined symbol: _ZN10tensorflow4tf3d18ValidateConvInputsEbiPNS_15OpKernelContextE
```
I didn't use official pip3 install tensorflow==2.3.0, because I have CUDA 11.0, so I downloaded wheels from https://github.com/davidenunes/tensorflow-wheels, which in general works fine.
Then I followed the compilation process from https://www.tensorflow.org/guide/create_op#compile_the_op_using_your_system_compiler_tensorflow_binary_installation

It would be great if you could upgrade the op to tensorflow 2.4.0/CUDA11. In other case I guess this very desirable starting point to use 3D detection in Tensorflow will be obsolte very soon.

BTW Are there any chances that the op will run with 2.4.0/CUDA11 now? In readme you stated only that ""it has not been tested""

Regards,

",hi manually load end error undefined symbol use official pip install general work fine compilation process would great could upgrade case guess desirable starting point use detection soon run stated tested,issue,positive,positive,positive,positive,positive,positive
790477457,Thanks @ccpocker. Would you like to contribute a Pull Request with your changes to sample colorizations for single/custom grayscale images? It will be very helpful for others.,thanks would like contribute pull request sample helpful,issue,positive,positive,positive,positive,positive,positive
790474300,"Yes, it works! Thank you so much for quick reply and kind help.

After i finished all the inferecen pipeline, i'll cloesd the issue immediately.",yes work thank much quick reply kind help finished pipeline issue immediately,issue,positive,positive,positive,positive,positive,positive
790445881,"good morning! I can reproduce the issue. It seems on downloading the checkpoints from the cloud, it adds the prefix ''coltran-colorizer_v2-"". 

Can you rename the files, removing the prefixes and make sure that this is the directory information.

```
total 542905
drwxr-xr-x  2 root root      4096 Mar  4 15:35 ./
drwxr-xr-x 13 root root      4096 Mar  4 15:04 ../
-rw-r--r--  1 root root       997 Mar  3 16:41 checkpoint
-rw-r--r--  1 root root 555911401 Mar  3 16:43 model-302.data-00000-of-00001
-rw-r--r--  1 root root     13521 Mar  3 16:41 model-302.index
```

Let me know if it works and I will think of a permanent fix.",good morning reproduce issue cloud prefix rename removing make sure directory information total root root mar root root mar root root mar root root mar root root mar let know work think permanent fix,issue,positive,positive,positive,positive,positive,positive
790392659,"Another new day, @MechCoder Good morning!

I check my code and answer is:

1. The logdir is passed correctly to the infer function and this line: https://github.com/google-research/google-research/blob/cbdffab5084e4d823f0c917be01d07969e6460a9/coltran/sample.py#L144
2. I have try your code: and the output is None.  

Firstly, i think i downloaded checkpoint incorrectly, so i have redownload the checkpoint three times from this link you provided, but the output is still None.

The checkpoint dir information in my machine is：
```
total 542905
drwxr-xr-x  2 root root      4096 Mar  4 15:35 ./
drwxr-xr-x 13 root root      4096 Mar  4 15:04 ../
-rw-r--r--  1 root root       997 Mar  3 16:41 coltran-colorizer_v2-checkpoint
-rw-r--r--  1 root root 555911401 Mar  3 16:43 coltran-colorizer_v2-model-302.data-00000-of-00001
-rw-r--r--  1 root root     13521 Mar  3 16:41 coltran-colorizer_v2-model-302.index
```

could you help me check the checkpoint information is correctly?

",another new day good morning check code answer correctly infer function line try code output none firstly think incorrectly three time link provided output still none information machine total root root mar root root mar root root mar root root mar root root mar could help check information correctly,issue,positive,positive,positive,positive,positive,positive
790242494,"> Hello Jingyuan, thanks your interest in our work! Hmm I'm not really sure what you mean by the feature changes with each run - do you mean that the feature size changes? It is possible that you might have dropout enabled for evaluation? (Note that for inference, dropout should be disabled.)

Hello Jennifer, thanks for your reply! 

I was trying to draft a test_base.py based on your repository. What I did was modify train_base.py into an evaluation script. I first set `is_training` to be false in Line 401 in the configuration of embedder_fn, and then replace `def run()` with what I have pasted in the previous post. I tested the script with a pre-defined input which is an all-one tensor, which is supposed to give the same feature tensor every time I run the script. But it turns out that the values of the feature change, so I guess the pre-trained model parameters were not properly loaded.

Would you indicate what I got wrong to implement such an inference API? All we need is to input some pre-processed 2D pose tensors and get the corresponding feature tensor. I suppose such an API is also needed for other people under this issue. Thank you for your consideration!

Best,
Jingyuan
",hello thanks interest work really sure mean feature run mean feature size possible might dropout evaluation note inference dropout disabled hello thanks reply trying draft based repository modify evaluation script first set false line configuration replace run pasted previous post tested script input tensor supposed give feature tensor every time run script turn feature change guess model properly loaded would indicate got wrong implement inference need input pose get corresponding feature tensor suppose also people issue thank consideration best,issue,positive,positive,neutral,neutral,positive,positive
790215814,"Hello Jingyuan, thanks your your interest in our work! Hmm I'm not really sure what you mean by the feature changes with each run - do you mean that the feature size changes? It is possible that you might have dropout enabled for evaluation? (Note that for inference, dropout should be disabled.)

To better understand what is going on, if you could share your embedder_fn, then we can help more.",hello thanks interest work really sure mean feature run mean feature size possible might dropout evaluation note inference dropout disabled better understand going could share help,issue,positive,positive,neutral,neutral,positive,positive
789691965,"I trained Object Discovery task, on regular full CLEVRtrain dataset for 500k iterations. Evaluating ARI on my own sampled CLEVR6-like dataset from CLEVRwithmasks - getting ARI of only 92% (contrary to 98-99% reported in the paper).

Would it be possible to provide file lists for the train/test splits for the Object Discovery experiment?

cc @dosovits @tkipf ",trained object discovery task regular full getting contrary paper would possible provide file object discovery experiment,issue,negative,positive,positive,positive,positive,positive
789691874,"1. Can you verify that the logdir is passed correctly to the script by printing it inside the infer function?
2. Can you also just try
```
ckpt_dir = '/data/glusterfs_hz_cv/11119980/google-research-master/coltran/colorizer_v2'
out = tf.train.latest_checkpoint(ckpt_dir)
print(out)
```

This should print `/data/glusterfs_hz_cv/11119980/google-research-master/coltran/colorizer_v2/model-302` if the checkpoint is downloaded correctly",verify correctly script printing inside infer function also try print print correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
789679484,"The pwd dir is /data/glusterfs_hz_cv/11119980/google-research-master/
The command-line code is:
```bash
python -m coltran.infer --config=coltran/configs/colorizer.py --mode=infer --logdir=/data/glusterfs_hz_cv/11119980/google-research-master/coltran/colorizer_v2
```

and the infer.py some code is 

```python
def infer(logdir):
    ""Executes infering for single gray-scale image""
    logging.info('begingning the infer single image')
    config = FLAGS.config
    batch_size = config.sample.get('batch_size', 1)
    # used to parallelize sampling jobs.
    skip_batches = config.sample.get('skip_batches', 0)
    gen_data_dir = config.sample.get('gen_data_dir', None)
    is_gen = gen_data_dir is not None

    model_name = config.model.get('name')

    dir = config.get('dir', input_dir)
    total_number = len(os.listdir(dir))
    config.sample.num_outputs = total_number

    if not is_gen and 'upsampler' in model_name:
        logging.info('Generated low resolution not provided, using ground '
                     'truth input.')

    # input image to ten
    img_ds = get_dataset_from_dir(config)
    data_iter = iter(img_ds)

    gen_iter = None
    if is_gen:
        pass
    store_samples(data_iter, config, logdir, gen_iter)

def main(_):
    logging.info('Logging to %s.', FLAGS.logdir)
    if FLAGS.mode == 'sample':
        logging.info('[main] I am the sampler.')
        sample(FLAGS.logdir, subset='valid')
    if FLAGS.mode == 'sample_test':
        logging.info('[main] I am the sampler test.')
        sample(FLAGS.logdir, subset='test')
    if FLAGS.mode == 'infer':
        logging.info('[main] i am the inferecen')
        infer(FLAGS.logdir)
    else:
        raise ValueError(
            'Unknown mode {}. '
            'Must be one of [sample, sample_test]'.format(FLAGS.mode))
```",code bash python code python infer single image infer single image used parallelize sampling none none low resolution provided ground input input image ten iter none pas main main sampler sample main sampler test sample main infer else raise mode one sample,issue,negative,positive,neutral,neutral,positive,positive
789668316,"Nice work.

On Wed, Mar 3, 2021, 7:01 AM S. Arash Hosseini <notifications@github.com>
wrote:

> Well done @frankplus <https://github.com/frankplus>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/187#issuecomment-789664582>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AKLIUXDCETBQY3N7IMT4U3DTBYQJNANCNFSM4KNFMTKA>
> .
>
",nice work wed mar wrote well done reply directly view,issue,positive,positive,positive,positive,positive,positive
789616647,"I developed my own implementation, following the details on the paper. It works quite well in my own language even with a smaller model and small subset of the Opensubtitles dataset. It would be interesting to train it on the reddit dataset with a bigger model. However I don't have enough resources to do that, so I would be happy if someone would like to contribute. https://github.com/frankplus/meena-chatbot",implementation following paper work quite well language even smaller model small subset would interesting train bigger model however enough would happy someone would like contribute,issue,positive,positive,positive,positive,positive,positive
789605613,Could you provide the command-line that you are using to run the code?,could provide run code,issue,negative,neutral,neutral,neutral,neutral,neutral
789587199,"I debug the code and found the problem is exactly the model don't load the checkpoint correctly.

My log infomation  is ""Producing sample after 0 training steps"".

The problem is this line:
https://github.com/google-research/google-research/blob/2d5a316a758d2b27b745bc5857503fd822592fa5/coltran/utils/train_utils.py#L186

tf.train.latest_checkpoint(ckpt_dir_) return None

while ckpt_dir_  is the abosolute dir of the colorizer_v2 dir, just like this:

ckpt_dir_  =”/data/glusterfs_hz_cv/11119980/google-research-master/coltran/colorizer_v2“

Any suggestion to fix this problem？
",code found problem exactly model load correctly log sample training problem line return none like suggestion fix,issue,negative,positive,positive,positive,positive,positive
789525006,"Quick questions before I look into detail.

1. Can you verify that the input gray-image has int32 dtype.

2. Is the checkpoint being restored correctly?You can use our latest checkpoints from here (https://console.cloud.google.com/storage/browser/gresearch/coltran/colorizer_v2) which have better results. You should be able to see logs such as ""Producing sample after 601000 training steps."" (https://github.com/google-research/google-research/blob/master/coltran/sample.py#L146)
",quick look detail verify input correctly use latest better able see sample training,issue,negative,positive,positive,positive,positive,positive
789511021,"hi @MechCoder, i follow your instruction and write a script：

I adopt the sample.py，just change depending on the datasets.get_datasets.

Code are below here:

```python
def load_and_preprocess_image(path):
    image_str = tf.io.read_file(path)
    image = tf.image.decode_image(image_str)
    image = tf.image.grayscale_to_rgb(image)
    example = {}
    example['image'] = image
    return example


def get_dataset_from_dir(config):
    """"""

    """"""
    downsample = config.get('downsample', False)
    downsample_res = config.get('downsample_res', 64)
    downsample_method = config.get('downsample_method', 'area')
    dir = config.get('dir', input_dir)
    dir = Path(dir)
    auto = tf.data.experimental.AUTOTUNE

    fileList = sorted([str(file) for file in dir.rglob('*.*')])

    path_ds = tf.data.Dataset.from_tensor_slices(fileList)
    image_ds = path_ds.map(load_and_preprocess_image)
    ds = image_ds.map(lambda x: datasets.preprocess(x, train=False))
    if downsample:
        downsample_part = functools.partial(
            datasets_utils.downsample_and_upsample,
            train=False,
            downsample_res=downsample_res,
            upsample_res=256,
            method=downsample_method)
        ds = ds.map(downsample_part, num_parallel_calls=100)
    ds = ds.batch(1, drop_remainder=True)
    ds = ds.prefetch(auto)
    return ds
```

The output for this line is dict:
https://github.com/google-research/google-research/blob/f2b680b0a8422ba852fd268829a127a41ebe4414/coltran/sample.py#L186 

output=
{'parallel': <tf.Tensor: shape=(1, 64, 64, 3), dtype=uint8，
'auto_sample': <tf.Tensor: shape=(1, 64, 64, 3),dtype=uint8,
'proba': <tf.Tensor: shape=(1, 64, 64, 512), dtype=float32}

when i save the output['parallel'] or output['auto_sample'] are colorful noisy picture( just meanless colorful noisy picture).

It is not the coarsely colorization for the input gray-scale image.

Is there any misunderstanding or wrong for my code? 




",hi follow instruction write adopt change depending code python path path image image image example example image return example false path auto sorted file file lambda auto return output line save output output colorful noisy picture colorful noisy picture coarsely colorization input image misunderstanding wrong code,issue,negative,negative,neutral,neutral,negative,negative
787098833,"You can find `mel_features.py` in `tensorflow_models/audioset/vggish/`. To fix this and oother import problems just create `tensorflow_models/audioset/__init__.py` and add the line: `from .vggish import mel_features, vggish_params, vggish_slim`
Also, in file `tensorflow_models/audioset/vggish/vggish_slim.py` you should change the import to `from . import vggish_params as params`.

You will also have problems with Tensorflow version: for me it worked r1.15 and changing in file `frechet_audio_distance/fad_utils.py` line 37 to `tf_record = tf.python_io.tf_record_iterator(filename).__next__()`",find fix import create add line import also file change import import also version worked file line,issue,negative,neutral,neutral,neutral,neutral,neutral
786843549,"Hi @soldierofhell,
Thanks for your question!
The python 3.8 package wasn't provided because we had some issues setting up python3.8 in the `2.3.0-custom-op-gpu-ubuntu16` docker image.
For now, please feel free to try the manual compile & install steps in your `Ubuntu 20.04 LTS` environment, and possibly compile the `wheel` package there too. The `2.3.0-custom-op-gpu-ubuntu16` is used to ensure the compatibility with `pip` version `Tensorflow`, but other environments might too.

Thanks!
Rui",hi thanks question python package provided setting python docker image please feel free try manual compile install environment possibly compile wheel package used ensure compatibility pip version might thanks,issue,positive,positive,positive,positive,positive,positive
786518705,"> Hi! For implementing the KNN evaluation, you need to:
> (1) Do inference using the model to get the mean and variance for each 2D pose
> (2) Draw 20 samples from each distribution
> (3) Compute the averaged L2 distance between pairs of samples
> (4) Do knn with this distance
> (5) Evaluate the correctness of the retrieved pose using the corresponding 3D poses
> Hope it goes well!

Hello @jenjsun! I was also wondering how to apply the pertained model in other scenarios. I could obtain the checkpoint files using the release training and the data pre-processing script, but since the project is highly integrated for better running performance, it's pretty hard to directly form an inference API. I tried something like:


```
def run(master, input_dataset_class, common_module, keypoint_profiles_module,
        models_module, input_example_parser_creator, keypoint_preprocessor_3d,
        create_model_input_fn, keypoint_distance_config_override,
        embedder_fn_kwargs):

  configs = _validate_and_setup(
      common_module=common_module,
      keypoint_profiles_module=keypoint_profiles_module,
      models_module=models_module,
      keypoint_distance_config_override=keypoint_distance_config_override,
      embedder_fn_kwargs=embedder_fn_kwargs)

  g = tf.Graph()
  with g.as_default():
    with tf.device(tf.train.replica_device_setter(FLAGS.num_ps_tasks)):

      with tf.Session() as sess:

        embedder_fn = configs['embedder_fn']

        loader = tf.train.import_meta_graph('checkpoints/model.ckpt-00001393' + '.meta')
        loader.restore(sess, 'checkpoints/model.ckpt-00001393')

        inputs = np.ones([512, 1, 26])
        inputs = tf.cast(tf.convert_to_tensor(inputs), tf.float32)
        outputs, activations = embedder_fn(inputs)
        sess.run(tf.initializers.global_variables())
        outputs_result, activations_result = sess.run([outputs, activations])
        feature = outputs_result['unnormalized_embeddings']
```

but it seems the checkpoints are not successfully loaded since the feature changes with each run.

Would you please provide some instructions on how we could tune-in the trained models for a test script?
Thank you very much for consideration!

Sincerely,
Jingyuan",hi evaluation need inference model get mean variance pose draw distribution compute distance distance evaluate correctness pose corresponding hope go well hello also wondering apply model could obtain release training data script since project highly better running performance pretty hard directly form inference tried something like run master sess loader sess feature successfully loaded since feature run would please provide could trained test script thank much consideration sincerely,issue,positive,positive,positive,positive,positive,positive
785864893,"Thanks for the reply. I'm looking forward to the next version of your paper 😄
Coincidentally, I've actually learned the formula from the blog just a few weeks ago in the context of Fisher information, but the inverted order and probability hidden in the expectation makes it not that obvious. The entire blog looks like a good read and I'm looking forward to dive into some of his articles. Thanks for the suggestion.",thanks reply looking forward next version paper coincidentally actually learned formula ago context fisher information inverted order probability hidden expectation obvious entire like good read looking forward dive thanks suggestion,issue,positive,positive,positive,positive,positive,positive
785212603,"> @hyhieu Thanks for the great paper!

Thank you for your interest in the paper ^_^

> couldn't we just treat the student parameters as fixed and perform back-propagation on equation (3) like you implied to avoid the costly approximation of appendix A? Or did you start with hard pseudo labels, for which this is impossible, and later discovered that it works with soft pseudo labels as well and never tried the above?

We can, and it works. We will revise the paper and provide more complete insights about soft/hard pseudo labels. We also got this questions from our reviewers 😄

Hard pseudo labels are needed for a completely unrelated reason. The funny (and quite counterintuitive) thing is that the ""costly approximation"" you mentioned is much cheaper in terms of compiler graphs. While soft pseudo labels plus some JVP tricks will allow you to back-prop through everything, the resulting XLA graph has a terrible size. When we tried that with `xla_sharding` for model parallelism, which is required for the large-scale ImageNet experiments, the graph size exceeded the limit of protobuf, and then the program failed to compile to run on TPUs.

> could you tell me which formula from Williams (1992) is used to apply REINFORCE?

It is from Theorem 1 in this [link](file:///Users/hyhieu/Desktop/AssistProf_DossierPreperation/2.1%20Referees/Williams1992_Article_SimpleStatisticalGradient-foll.pdf). I recently learned that this is also called the log-gradient trick. You can find a more modern description [here](http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/).
",thanks great paper thank interest paper could treat student fixed perform equation like avoid costly approximation appendix start hard pseudo impossible later discovered work soft pseudo well never tried work revise paper provide complete pseudo also got hard pseudo completely unrelated reason funny quite thing costly approximation much compiler soft pseudo plus allow everything resulting graph terrible size tried model parallelism graph size limit program compile run could tell formula used apply reinforce theorem link file recently learned also trick find modern description,issue,positive,negative,neutral,neutral,negative,negative
785200985,"> Hello @hyhieu, thank you for your great work! Could you please share the logs of the run on CIFAR10-4k? I have some problems reproducing the results from the paper, so the logs would be of great help. Unfortunately, the link specified [here](https://github.com/google-research/google-research/issues/534#issuecomment-768527718) is unavailable, there might be a misprint in the link.

How about these links? I also included the run for ImageNet-10%. Note that for both runs, we still had to finetune the resulting checkpoints on labeled data.
- CIFAR-10-4K: [link](https://tensorboard.dev/experiment/i6sjrvdVQcanGLca8FV3LQ/#scalars)
- ImageNet-10%: [link](https://tensorboard.dev/experiment/rRmdsRbYQ5qKksm0Pv4KPw/#scalars)",hello thank great work could please share run paper would great help unfortunately link unavailable might misprint link link also included run note still resulting data link link,issue,positive,positive,positive,positive,positive,positive
785012447,"Hello @hyhieu, thank you for your great work! Could you please share the logs of the run on CIFAR10-4k? I have some problems reproducing the results from the paper, so the logs would be of great help. Unfortunately, the link specified [here](https://github.com/google-research/google-research/issues/534#issuecomment-768527718) is unavailable, there might be a misprint in the link.",hello thank great work could please share run paper would great help unfortunately link unavailable might misprint link,issue,positive,positive,positive,positive,positive,positive
784710463,"Hi @MechCoder, thank you so much for such quick reply. 

Yes i will have a try to write a script for single gray-scale image inference  following your instructions. Due to unfamiliar with TF2.0 and limitation of time, i am not sure that i can achieve it.



",hi thank much quick reply yes try write script single image inference following due unfamiliar limitation time sure achieve,issue,positive,positive,positive,positive,positive,positive
784524396,"@hyhieu Thanks for the great paper!
Most of my questions from #584 found an answer, when I discovered this issue. However, I still have one question:
It seems like using soft pseudo labels works on most datasets, especially if they are smaller. Could you explain the section right underneath equation (3) in version 3 of your paper? Since the method works with soft labels on some datasets, couldn't we just treat the student parameters as fixed and perform back-propagation on equation (3) like you implied to avoid the costly approximation of appendix A? Or did you start with hard pseudo labels, for which this is impossible, and later discovered that it works with soft pseudo labels as well and never tried the above?

If you don't have to look it up again, could you tell me which formula from Williams (1992) is used to apply REINFORCE? I guess it's something from section 7 ""Compatibility with backpropagation"", but I can't find anything that fits on the first glance.

Thanks for your help",thanks great paper found answer discovered issue however still one question like soft pseudo work especially smaller could explain section right underneath equation version paper since method work soft could treat student fixed perform equation like avoid costly approximation appendix start hard pseudo impossible later discovered work soft pseudo well never tried look could tell formula used apply reinforce guess something section compatibility ca find anything first glance thanks help,issue,positive,positive,neutral,neutral,positive,positive
784518520,"I think I found the questions to most of my answers in #534 , sorry for the inconvenience. ",think found sorry inconvenience,issue,negative,negative,negative,negative,negative,negative
784115297,"Hi @ccpocker, thanks for your interest in our paper. Yes it is true, I did not open source any script for a single gray-scale image inference. However, it should not be too difficult to achieve that.

You can adapt sample.py in the following way.

-  Remove all the code depending on datasets.get_datasets (https://github.com/google-research/google-research/blob/master/coltran/sample.py#L223). This will prevent downloading ImageNet.
-  For the **colorizer**:
    - **input** - 64x64 grayscale image. You need to provide inputs here https://github.com/google-research/google-research/blob/master/coltran/sample.py#L186.
    -  **output['sample']** will output a 64x64 colorized sample. - 
- For the **color upsampler**
    - You need to provide inputs here (https://github.com/google-research/google-research/blob/master/coltran/sample.py#L172)
    - **input: gray_cond** - 64x64 grayscale image.
    - **input: bit_cond** - 64x64 sample from the colorizer.
    -  **output['argmax']** will output a 64x64 RGB image.
 - For the **spatial upsampler**
     - You need to provide inputs here (https://github.com/google-research/google-research/blob/master/coltran/sample.py#L183)
     - **input: gray_cond** - 256x256 grayscale image.
     - **input: low_res**
         - Let the 64x64 output from the color upsampler be `cup_out`
         - Use `low_res=datasets_utils.change_resolution(cup_out, 256)`  to convert it to a blurry 256x256 colorized image.
     - **output['argmax']** - 256x256 final image.

You can save the intermediate images at each stage to disk and then reload them to avoid issues with memory. Let me know if you want to take a stab at it and if you wish to contribute a script. If not, I will try to write one after a couple of weeks.

Re: **inference** vs **sampling**, I think the word inference is used broadly these days. It was used in a probabilistic sense to infer unobserved latent variables but now it can just mean to use the trained model for predictions (incl sampling)",hi thanks interest paper yes true open source script single image inference however difficult achieve adapt following way remove code depending prevent input image need provide output output sample color need provide input image input sample output output image spatial need provide input image input let output color use convert blurry image output final image save intermediate stage disk reload avoid memory let know want take stab wish contribute script try write one couple inference sampling think word inference used broadly day used probabilistic sense infer unobserved latent mean use trained model sampling,issue,positive,negative,neutral,neutral,negative,negative
783435866,"IIRC, it was just that `serialize` expected folder name, not file name. Doing what @DavidWiesner mentioned in the last post worked for me.",serialize folder name file name last post worked,issue,negative,neutral,neutral,neutral,neutral,neutral
783416053,"@MFajcik Have you managed to get it working? Having the same issue, and the suggested by @DavidWiesner brings the same error.",get working issue error,issue,negative,neutral,neutral,neutral,neutral,neutral
782946625,"kws_streaming was designed as a prototype tool for keras streaming aware models.
In the paper we use accuracy metrics and data sets because of:
* accuracy metrics were selected only for the purpose of comparison with previously published work. for real life you should extend it with False accept rate and miss rate, also measure some of these metrics in time: false accept rate per hour or something like this.
* we selected speech commands data for benchmarking and reporting purpose and comparison with previously published papers. But in real applications you should have more data with different accent, gender, age categories plus different kind of noise environments (indoor, outdoor, car, etc).

So it is hard to say how much data you need without considering the context of the application.",designed prototype tool streaming aware paper use accuracy metric data accuracy metric selected purpose comparison previously work real life extend false accept rate miss rate also measure metric time false accept rate per hour something like selected speech data purpose comparison previously real data different accent gender age plus different kind noise indoor outdoor car hard say much data need without considering context application,issue,positive,positive,neutral,neutral,positive,positive
782814872,"@afathi3 
Thank you so much for your great advice.",thank much great advice,issue,positive,positive,positive,positive,positive,positive
782749687,"@TokyoYoshida 
Thank you for finding the typo. I am submitting a fix.
",thank finding typo fix,issue,negative,neutral,neutral,neutral,neutral,neutral
782713403,"@Manueljohnson063 

Thank you so much. Here is mail ID mariya@rhrk.uni-kl.de
",thank much mail id,issue,negative,positive,positive,positive,positive,positive
782713160,"@mariya12290 bro one of my friend send me a paper regarding this ,could you please share your mail id i will sent it to u!!",one friend send paper regarding could please share mail id sent,issue,positive,neutral,neutral,neutral,neutral,neutral
782712274,@mariya12290 Dont worry bro let me ask to my friends....i am also a student in bits pilani,dont worry let ask also student,issue,negative,neutral,neutral,neutral,neutral,neutral
782712174,"@afathi3 

Hey

Could you please explain what is the reason for doing the NMS in 2d instead of 3D directly?. And also I have been searching for papers regarding 3D NMS, I have not found one. 

Is there any specific reason for the 2D NMS like computation time or memory consumption, any others ?",hey could please explain reason instead directly also searching regarding found one specific reason like computation time memory consumption,issue,positive,positive,neutral,neutral,positive,positive
782710661,"@Manueljohnson063 

Thank you so much for your generosity.  My task is to detect pedestrain and cyclist. So I trained the model and I got around 48% AP for pedestrian and cyclist. Now I need to get the model as a graph to use on hardware. But my supervisor told me to implement 3D NMS instead of 2D. 
So I have been looking for papers on 3D NMS or Code in github. But I could not able to find a one. 

That""s why I thought of asking @afathi3 . 

When I look into TF3D NMS, they also used 2D NMS in the backend. 

Do you have any Idea regarding 3D NSM or atleast papers to study and implement?",thank much generosity task detect cyclist trained model got around pedestrian cyclist need get model graph use hardware supervisor told implement instead looking code could able find one thought look also used idea regarding study implement,issue,positive,positive,positive,positive,positive,positive
782709384,"Haha,I had done in detecting single object using voxelnet in my google colab if you want i will share it.",done single object want share,issue,negative,negative,neutral,neutral,negative,negative
782708952,"@Manueljohnson063  Dude you can laugh, I am running out of my time. ",dude laugh running time,issue,positive,positive,positive,positive,positive,positive
782694634,"@afathi3  Hey,

I checked that function. Since I need to build the graph to use in C++ at the end of my thesis. I can not use [this function](https://github.com/google-research/google-research/blob/master/tf3d/object_detection/box_utils/box_ops.py#L122l). It is too big implementation and mostly written using numpy. If I start implementing it now in tensorflow, It takes atleast 2 months for me to get the perfect 3D NMS after doing lots of debugging . 

My current situation:

I have implemented preprocessing in tensorflow and have graphs of pre-processing, model, processing on predictions. Now I need to implement [3D NMS](https://github.com/qianguih/voxelnet/blob/b74823daa328fc2fa99452bf79793e1f3c32c72a/model/model.py#L294) to get the max of one bounding box per object in a very short time at the end [till here ](https://github.com/qianguih/voxelnet/blob/b74823daa328fc2fa99452bf79793e1f3c32c72a/model/model.py#L312) to get the graph as well. 

Could you please help me with a simple solution? since you have been studying for a long time, you will have better idea.

If get any hints or suggestions are also fine.  I will implement code my self. 

Thanks in advance",hey checked function since need build graph use end thesis use function big implementation mostly written start get perfect lot current situation model need implement get one bounding box per object short time end till get graph well could please help simple solution since long time better idea get also fine implement code self thanks advance,issue,positive,positive,positive,positive,positive,positive
782573403,"@afathi3  Thanks for your replay.Some of the repo when i look they are trying to fuse the image data with cloud at some point ,any way thankz for your support once again :)",thanks look trying fuse image data cloud point way support,issue,positive,positive,positive,positive,positive,positive
782573114,"@Manueljohnson063 

The pipeline needs point clouds and labels. Currently it doesn't use image data (that is an interesting feature that could be added in the future).
",pipeline need point currently use image data interesting feature could added future,issue,negative,positive,positive,positive,positive,positive
782572882,"@mariya12290 
I think [this function](https://github.com/google-research/google-research/blob/master/tf3d/object_detection/box_utils/box_ops.py#L122) is closest to what you are looking for.
The only thing is that you need to turn your Z_raw into a rotation matrix.
You can do sth like:
c = np.cos(angle)
s = np.sin(angle)
rot_z = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])

But also, you need to make sure you move everything into world coordinate (in Kitti everything is in camera coordinate)

",think function looking thing need turn rotation matrix like angle angle also need make sure move everything world everything camera,issue,positive,positive,positive,positive,positive,positive
782571612,"@KangchengLiu 
I hope to release checkpoints for some of the datasets, together with code that runs those checkpoints and visualizes them soon (in 3-4 weeks from now).
",hope release together code soon,issue,negative,neutral,neutral,neutral,neutral,neutral
782571425,"@xuzhang5788 
If you have voxels that are empty, then this model should be appropriate for it.
However, you have data in all voxels, but some of the feature dimensions are 0 then this is not a good solution for you.
If you can fit everything in memory, dense model could be better, since the information can propagate easier across the grid.
[This](https://github.com/google-research/google-research/blob/master/tf3d/utils/voxel_utils.py#L305) is the function for turning the point cloud into a sparse voxel grid.
",empty model appropriate however data feature good solution fit everything memory dense model could better since information propagate easier across grid function turning point cloud sparse grid,issue,positive,positive,positive,positive,positive,positive
782553631,@mariya12290 No i didnt got your point >>>Could you please explain what is the meaning of respect!LOL,didnt got point could please explain meaning respect,issue,positive,neutral,neutral,neutral,neutral,neutral
782519018,"Thanks for your great work of tf3D, could you please provide the visualization scripts of detection and semantic/instance segmentation for the datasets you tested (Waymo Open Dataset, Scannet Dataset, Rio Dataset)? Many thanks!",thanks great work could please provide visualization detection segmentation tested open rio many thanks,issue,positive,positive,positive,positive,positive,positive
782067047,"@Manueljohnson063  

Hi,

Please give some respect to @afathi3 He is the author of this repo.   

To know what it needs just clone the repo and run it with the data and also check the documentation to know how to use this repo. 

Hope you got my point.",hi please give respect author know need clone run data also check documentation know use hope got point,issue,positive,neutral,neutral,neutral,neutral,neutral
781905109,"@afathi3 @mariya12290 Guys i am new to this ,does it require only point clouds,label,caliberation as data or it is incorporated with images? please help",new require point label data incorporated please help,issue,positive,positive,positive,positive,positive,positive
781560105,"@afathi3

Hi,

I have few questions in using your post processing work in my master thesis. I am using voxelnet for object detection and I wrote the code for pre-processing(voxelization) in tensorflow 2.4. Now I need to do post processing 3D NMS instead of using the 2D NMS used in the original network.  

Currenlty I have output of model(Faster RPN )as below:

Bounding Box (None,7)   #using for binary classification
Predicted scores (None,)   #object score       

Now I need to apply 3D NMS for the output of the Faster RPN. How can I utilize your work? I am getting the output in kitti format(X,Y,Z,L,W,H,raw_Z) lidar co-ordinate system. 

Could you please guide me with this probelm?",hi post work master thesis object detection wrote code need post instead used original network output model faster bounding box none binary classification none object score need apply output faster utilize work getting output format system could please guide,issue,positive,positive,positive,positive,positive,positive
781124185,"Thanks for your response...
Best Regards,

Chinnam Harish Naidu
Mobile:01753618706


On Thu, Feb 18, 2021 at 4:52 AM yuan0101213 <notifications@github.com>
wrote:

> Thanks for your reply.It helps a lot!
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/457#issuecomment-781029653>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AJDDEMFQHXOXOU3AMTJQQDDS7SFG7ANCNFSM4TQQP2EQ>
> .
>
",thanks response best chinnam harish mobile yuan wrote thanks lot thread reply directly view,issue,positive,positive,positive,positive,positive,positive
781008721,"Hi! For implementing the KNN evaluation, you need to:
(1) Do inference using the model to get the mean and variance for each 2D pose
(2) Draw 20 samples from each distribution
(3) Compute the averaged L2 distance between pairs of samples
(4) Do knn with this distance
(5) Evaluate the correctness of the retrieved pose using the corresponding 3D poses 
Hope it goes well!",hi evaluation need inference model get mean variance pose draw distribution compute distance distance evaluate correctness pose corresponding hope go well,issue,positive,negative,negative,negative,negative,negative
780756636,"@afathi3 
Thanks you so much for the quick response. 

I think I got it partially. I will check it once again. If I have doubt, I will ask, or else I will close the issue in a day.
",thanks much quick response think got partially check doubt ask else close issue day,issue,negative,positive,positive,positive,positive,positive
780733762,"@mariya12290 

Hi,

I will make sure that we will expand the function docstr to make this more clear.
The first argument that the post [postprocess function](https://github.com/google-research/google-research/blob/master/tf3d/object_detection/postprocessor.py#L92) receives is a dictionary. This dictionary contains standard keys (the key names are defined in [standard_fields.py](https://github.com/google-research/google-research/blob/master/tf3d/standard_fields.py#L260-L276)) for object length, height, width, center, rotation matrix, and score.
length, height, width are [N, 1] tensors
center is a [N, 3] tensor
rotation matrix is a [N, 3, 3] tensor
and score is a [N, C] tensor where C is the number of classes
",hi make sure expand function make clear first argument post function dictionary dictionary standard key defined object length height width center rotation matrix score length height width center tensor rotation matrix tensor score tensor number class,issue,positive,positive,neutral,neutral,positive,positive
780400891,"@afathi3 Thank you so much for your response.
Do you mean that a dense 3D network is normally a better choice if I can fit my model in memory? Although I only have 24x24x24 grid, I have 31 channels. I want a bigger cubic size if possible,  but I worried about the limitation of memory. Maybe I can fit the model in memory, but I have to use a small batch size to train my model. I am dealing with chemical molecules, so most of the values on the grid are zero. So, I am interested in your work.

I found your work from the blog ""https://www.analyticsvidhya.com/blog/2021/02/introduction-to-tensorflow-3d-for-3d-scene-understanding-by-google-ai/"". It mentioned the paper ""Submanifold Sparse Convolutional Networks"". I mistakenly think that is your paper. Sorry about that. ",thank much response mean dense network normally better choice fit model memory although grid want bigger cubic size possible worried limitation memory maybe fit model memory use small batch size train model dealing chemical grid zero interested work found work paper sparse convolutional mistakenly think paper sorry,issue,positive,positive,neutral,neutral,positive,positive
780375116,"@xuzhang5788 
I am sorry but I am not sure if we mean the same thing by sparse.
What I mean is that if most of the voxels in your 3D grid are somehow empty and you can afford to only keep a small set of voxels. Given that your grid is only 24x24x24, you can easily fit a dense 3D network like S3D or I3D in memory.
I am not sure which paper is 2017, but this codebase contains the code that we have used in our recent papers. Please refer to [DOPS: Learning to detect 3D objects and predict their 3D shapes](https://arxiv.org/abs/2004.01170)",sorry sure mean thing sparse mean grid somehow empty afford keep small set given grid easily fit dense network like id memory sure paper code used recent please refer learning detect predict,issue,positive,negative,neutral,neutral,negative,negative
780355211,"@afathi3 Thank you for your fast response. My 3D data is very sparse, (24x24x24) with 31 channels. but I don't know how to turn data into a sparse representation. Do you have some examples? Where can I find the release code that can reproduce the results of your experiments in the paper, such as the ModelNet-40 dataset?
In addition, the paper is in 2017, I am curious why you release the code until now?
Many thanks",thank fast response data sparse know turn data sparse representation find release code reproduce paper addition paper curious release code many thanks,issue,positive,positive,positive,positive,positive,positive
780316868,"@xuzhang5788 this is an interesting question.
We have never tried it on 3D medical images, but in theory why not if you can turn such data into a sparse representation. Otherwise, dense 3d convolutions might be more appropriate.
",interesting question never tried medical theory turn data sparse representation otherwise dense might appropriate,issue,negative,positive,positive,positive,positive,positive
780236257,"Can I use it for 3D image classification? Such as keras example '3D Image Classification from CT Scans' (https://keras.io/examples/vision/3D_image_classification/). Hopefully, you could give more examples to show how to use it. ",use image classification example image classification hopefully could give show use,issue,negative,neutral,neutral,neutral,neutral,neutral
779557914,"I'm very interested in your work. I'm trying to write KNN code for evaluation. However, model generation uses library functions（tf_slim.learning.train）, and I see garbled code when I open the model by txt. So I wonder how to write KNN code for evaluation",interested work trying write code evaluation however model generation library see code open model wonder write code evaluation,issue,negative,positive,positive,positive,positive,positive
779270787,"@kchoro I actually tested our proposal in the unit test.
So I created V as an identity matrix in order to be able to see the attention scores in the performer.
Here is the code:
```python
def test_softmax_noncausal_attention_block_output(self):
        batch_size = 1
        length = 32
        num_heads = 1
        dim = 32
        num_random_features = 2000
        tf.random.set_random_seed(5)
        query = tf.random.normal([batch_size, length, num_heads, dim])
        key = tf.random.normal([batch_size, length, num_heads, dim])
        value = tf.eye(dim, batch_shape=[batch_size, num_heads])
        value = tf.transpose(value, perm=[0, 2, 1, 3])
        kernel_transformation = softmax_kernel_transformation
        projection_matrix = create_projection_matrix(
            num_random_features, dim)
        attention_block_output = favor_attention(
            query, key, value, kernel_transformation, False, projection_matrix)


        ## Check that attention weights for performer sum to one
        sum_of_weights = tf.math.reduce_sum(attention_block_output,axis=-1)
        self.assertLess(tf.reduce_max(tf.abs(sum_of_weights - 1)), 10e-4)

        ## Check difference to exact attention
        query = tf.multiply(query, 1.0 / math.sqrt(float(dim)))
        attention_scores = tf.einsum(""BXHD,BYHD->BXYH"", query, key)
        attention_scores = tf.nn.softmax(attention_scores, axis=2)
        exact_attention_block_output = tf.einsum(""BXYH,BYHD->BXHD"",
                                                 attention_scores, value)

        ## Visualize attention scores
        from matplotlib import pyplot as plt
        fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, constrained_layout=True)
        im0 = ax0.imshow(attention_scores[0,:,:,0])
        ax0.set_title(""attention_scores"")

        im1 = ax1.imshow(exact_attention_block_output[0, :, 0, :])
        ax1.set_title(""exact_attention_block_output"")
        im2 = ax2.imshow(attention_block_output[0, :, 0, :])
        ax2.set_title(""attention_block_output"")
        fig.colorbar(im0, ax=ax0)
        fig.colorbar(im1, ax=ax1)
        fig.colorbar(im2, ax=ax2)
        plt.show()
```
While the fast attention scores are normalized for a certain query, the visual comparison of the attention scores between fast_attention and the exact attention shows quite some differences.

![FastAttentionScoreCompare](https://user-images.githubusercontent.com/3592251/107960707-3be3bd80-6fa5-11eb-8882-7a015cd4ac0b.png)
Top: Exact attention scores; Middle: Exact attention block output; Bottom: FastAttention block output

Any idea, what could cause that discrepancy?
",actually tested proposal unit test identity matrix order able see attention performer code python self length dim query length dim key length dim value dim value value dim query key value false check attention performer sum one check difference exact attention query query float dim query key value visualize attention import fig ax ax ax fast attention certain query visual comparison attention exact attention quite top exact attention middle exact attention block output bottom block output idea could cause discrepancy,issue,positive,positive,positive,positive,positive,positive
778944141,"Hi @soldierofhell,

Thank you for your interest.
* In the first released version we tried to have the models be as simple as possible. Graph layer usually improves the results slightly (less than 1%), but on the other hand since at this point computing the adjacencies for building the graph happens on CPU, it could slow down training and eval.
* Currently LSTM implementation is not released. One obstacle to releasing that is that the functionality in TFDS that lets us read a sequence of frames is not open sourced yet. Once that becomes open source, we can revisit open sourcing that paper again.
* ",hi thank interest first version tried simple possible graph layer usually slightly le hand since point building graph could slow training currently implementation one obstacle functionality u read sequence open yet becomes open source revisit open paper,issue,positive,negative,neutral,neutral,negative,negative
778942668,We still plan to add pertained models soon. It should be hopefully within a month. Sorry about the inconvenience.,still plan add soon hopefully within month sorry inconvenience,issue,negative,negative,negative,negative,negative,negative
777365189,"ok, I think I got it now, when stepping through the code I find [rescale_image](https://github.com/google-research/google-research/blob/0816de24d69a8db2f59ed46bbef3261c687ace04/pruning_identified_exemplars/utils/preprocessing_helper.py#L129) which is used by `compute_feature_ranking()`. this rescales the input image (saliency map) in a way that the ""most negative"" attributions will become 0, and contributions are sorted from largest positive to largest negative value.

if this is correct, this issue can be closed, otherwise please help me figure out what I misunderstood :)",think got stepping code find used input image map way negative become sorted positive negative value correct issue closed otherwise please help figure misunderstood,issue,negative,negative,negative,negative,negative,negative
776722451,"Thank you for your work, docker image will be sufficient for our needs!",thank work docker image sufficient need,issue,negative,neutral,neutral,neutral,neutral,neutral
776263081,Thanks for your interest! Our dataset has been released. There is a small fraction of the original dataset that can't be released due to screen copyright. But the majority of the dataset is there. Please check it out and ping us if you have any questions.,thanks interest small fraction original ca due screen copyright majority please check ping u,issue,positive,positive,neutral,neutral,positive,positive
776249274,"Issue was with protoc version. It was defaulting to version2. However, upgrading to version 3.12 fixed it.",issue version version however version fixed,issue,negative,positive,neutral,neutral,positive,positive
775941027,"@sammymax  thanks for the explanation, I followed the code and have some few questions. 
1)  I found that `anisotropic_quantization_threshold=0.2` is being used when training pq codebook at hashes/internal/asymmetric_hashing_impl.cc:189. 
`if (!std::isnan(opts.config().noise_shaping_threshold()) &&
      opts.config().use_noise_shaped_training())`
Specifically at this line, it checks whether the threshold was given as parameters and if `use_noise_shaped_training` is set to True. But I checked that use_noise_shaped_trainig's default value is false (refering ./scann/proto/hash.proto) and its value does not change. In summary, above if clause is never true, so it never enters to function ComputeParallelCostMultiplier. So whatever value I give to `anisotropic_quantization_threshold`, parallel_cost_multiplier value is always 1. 
Refering to paper, it this value is 1 _"" the update rule becomes finding the average of datapoints in the partition, same as standard k-means update rule.""_ It is not ScaNN, it's just k-means clustering. Could you tell me where ScaNN uses the loss function from the paper?
2) Assuming 1) problem is resolved, it still calls 
`ComputeParallelCostMultiplier(opts.config().noise_shaping_threshold(), 1.0, dataset.dimensionality())` at hashes/internal/asymmetric_hashing_impl.cc:191. Refering to the paper, 1.0 should be the norm of the database. It will specially work with glove since we normalized its norms, but for general MIPS problems, norms of the database are not constant. How should we fix it to support dataset with various norms?   
3) How do we set use_noise_shaped_training to True? I can just fix ./scann/proto/hash.proto, but is there any way to set this value via paremter?

I will be very helpful if you answer above questions!",thanks explanation code found used training specifically line whether threshold given set true checked default value false value change summary clause never true never function whatever value give value always paper value update rule becomes finding average partition standard update rule clustering could tell loss function paper assuming problem resolved still paper norm specially work glove since general constant fix support various set true fix way set value via helpful answer,issue,positive,positive,neutral,neutral,positive,positive
775757062,"This is unfortunate, but answers my question.

Thanks for the response!",unfortunate question thanks response,issue,negative,negative,negative,negative,negative,negative
775607087,"> I'm not associated with this repo or google. But the updates of both the student and teacher happen per batch at the same batch. See the pseudo algorithm in the appendix of the paper. UDA, Supervised, and MPL loss are all added together for the teacher loss.
> see here:
> https://github.com/google-research/google-research/blob/master/meta_pseudo_labels/training_utils.py#L494

thanks！I will try it！",associated student teacher happen per batch batch see pseudo algorithm appendix paper loss added together teacher loss see try,issue,negative,neutral,neutral,neutral,neutral,neutral
775570828,ScaNN doesn't support sparse vector search.,support sparse vector search,issue,negative,neutral,neutral,neutral,neutral,neutral
775570374,The `anisotropic_quantization_threshold=0.2` portion results in the loss function from the paper being used. Removing that value or setting it to NaN will change ScaNN to use standard reconstruction loss.,portion loss function paper used removing value setting nan change use standard reconstruction loss,issue,negative,neutral,neutral,neutral,neutral,neutral
775569807,"Either version should be ok. As far as I know, TensorFlow doesn't have separate CPU and GPU version anymore though:

> `tensorflow` —Latest stable release with CPU and GPU support (Ubuntu and Windows)

[source](https://www.tensorflow.org/install/pip#tensorflow-2-packages-are-available)",either version far know separate version though stable release support source,issue,positive,positive,neutral,neutral,positive,positive
775569102,"I just released Dockerfiles for TF Serving with ScaNN (also see [this bug](https://github.com/tensorflow/recommenders/issues/216)), but if you would still like to compile it yourself, the errors are most likely due to ScaNN using C++17 features or referencing AVX2 intrinsics, so this should work once the compiler is configured correctly. Just note that the TF Serving version should be [this commit](https://github.com/tensorflow/serving/commit/a98d1649061133c589c1a3872aee9ecd774bc52a) or later because previous ones had problems with C++17.",serving also see bug would still like compile likely due work compiler correctly note serving version commit later previous,issue,positive,negative,neutral,neutral,negative,negative
774742539,"> @hyhieu Thanks for your great work! I really admire you!
> Could you please explain more about the Equation (10), how can I derive it from REINFORCE equation?
> Many thanks!
> ![Untitled](https://user-images.githubusercontent.com/71539555/106589173-4dc56980-657e-11eb-9eff-219f21e8d547.png)

The REINFORCE equation is applied to go from the first line to the second line, moving the $\frac{\partial}{\partial \theta_T}$ from outside the expectation to inside the expectation with the extra $\log$ term.",thanks great work really admire could please explain equation derive reinforce equation many thanks untitled reinforce equation applied go first line second line moving outside expectation inside expectation extra term,issue,positive,positive,positive,positive,positive,positive
774741996,"> Hi, a couple questions related to this. In the paper I believe there is a small typo where me multiply by twice `\eta_s` in dot product scaling? Second in the code, by using the Taylor approximation in the code, we forgo cosine similarity for the raw dot product again, correct?

Both points of yours are correct. We will update the paper and release an update to the open-sourced code with more detailed comments. I apologize for the confusion.",hi couple related paper believe small typo multiply twice dot product scaling second code approximation code forgo cosine similarity raw dot product correct correct update paper release update code detailed apologize confusion,issue,negative,negative,neutral,neutral,negative,negative
774710886,"Hi, a couple questions related to this. In the paper I believe there is a small typo where we multiply twice by `\eta_s` in dot product scaling? Second in the code, by using the Taylor approximation in the code, we forgo cosine similarity for the raw dot product again, correct? ",hi couple related paper believe small typo multiply twice dot product scaling second code approximation code forgo cosine similarity raw dot product correct,issue,negative,negative,neutral,neutral,negative,negative
774706678,"I'm not associated with this repo or google. But the updates of both the student and teacher happen per batch at the same batch. See the pseudo algorithm in the appendix of the paper. UDA, Supervised, and MPL loss are all added together for the teacher loss. 
see here:
https://github.com/google-research/google-research/blob/master/meta_pseudo_labels/training_utils.py#L494",associated student teacher happen per batch batch see pseudo algorithm appendix paper loss added together teacher loss see,issue,negative,neutral,neutral,neutral,neutral,neutral
773777571,"@Feulo Thank you so much, your solution really helped me a lot!! This works perfectly. ",thank much solution really lot work perfectly,issue,positive,positive,positive,positive,positive,positive
773769081,"Hello and thanks for your interest! You can run training like this on the full data:

python3 -m poem.train \
  --alsologtostderr \
  --input_table=""${TRAIN_INPUT}"" \
  --train_log_dir=""${TRAIN_DIR}"" \
  --batch_size=256 \
  --num_steps=5000000 \
  --input_shuffle_buffer_size=1000000 \
  --input_keypoint_profile_name_2d=""LEGACY_2DH36M13"" \
  --summarize_percentiles=false \
  --log_every_n_steps=1000  
  
Feel free to change the number of steps, batch size, and learning rate as well. (5 million steps may take a while, so can try 1 million steps first.)",hello thanks interest run training like full data python feel free change number batch size learning rate well million may take try million first,issue,positive,positive,positive,positive,positive,positive
773582230,"Great question! We indeed use no strides in the encoder, so the encoded feature map is still 128x128 in size. Using some strides in the encoder generally works well, but downsampling too harshly tends to reduce performance (i.e. result quality) in my experience (7x7 might potentially be too coarse), at least for a very shallow/naive CNN. I suspect that a ResNet would still work well, though, despite the larger strides.

The illustration in the paper is just meant as an illustration of the idea, hence the 8x8 grid is only meant to show that these are encoded feature maps.

And you are right in that the decoder starts with spatial broadcasting to a 8x8 grid, followed by several (de-)conv layers with strides > 1 to get back to 128x128 resolution.",great question indeed use feature map still size generally work well harshly reduce performance result quality experience might potentially coarse least suspect would still work well though despite illustration paper meant illustration idea hence grid meant show feature right spatial grid several get back resolution,issue,positive,positive,neutral,neutral,positive,positive
773401982,"@YJYJLee I solved for me by changing some `int` type variables for `long long int` (thought it will be better with size_t)
and recompile it by hand. The only version of scann that needs the change is the one you will serialize the searcher, to load the searcher you can use the package version installed with pip

heres the diff:

```
--- a/scann/scann/tree_x_hybrid/internal/utils.h
+++ b/scann/scann/tree_x_hybrid/internal/utils.h
@@ -34,11 +34,11 @@ template <template <class> class V, typename T>
 StatusOr<SingleMachineFactoryOptions> MergeAHLeafOptions(
     const vector<unique_ptr<V<T>>>& leaf_searchers,
     ConstSpan<std::vector<DatapointIndex>> datapoints_by_token,
-    const int expected_size) {
+    const long long int expected_size) {
   const int n_leaves = leaf_searchers.size();
   auto leaf_opts = std::vector<SingleMachineFactoryOptions>(n_leaves);
 
-  int hash_ct = 0, codebook_ct = 0, total_hashed = 0, hash_dim = -1;
+  long long int hash_ct = 0, codebook_ct = 0, total_hashed = 0, hash_dim = -1;
 
   ssize_t int8_ct = 0, total_int8 = 0, int8_dim = -1;
   bool int8_has_norms = false;
@@ -60,6 +60,7 @@ StatusOr<SingleMachineFactoryOptions> MergeAHLeafOptions(
               hash_dim, cur_dims));
       }
     }
+
     if (leaf_opts[i].ah_codebook != nullptr) codebook_ct++;
     auto int8_t = leaf_opts[i].pre_quantized_fixed_point;
     if (int8_t != nullptr) {
@@ -108,7 +109,9 @@ StatusOr<SingleMachineFactoryOptions> MergeAHLeafOptions(
         return FailedPreconditionError(""Inconsistent codebooks among leaves"");
     }
 
-    vector<uint8_t> storage(hash_dim * expected_size);
+    long long int s = hash_dim * expected_size;
+    vector<uint8_t> storage(s);
+    //vector<uint8_t> storage(hash_dim * expected_size);
     for (int i = 0; i < n_leaves; i++) {
       int inner_idx = 0;
       for (const auto dptr : *leaf_opts[i].hashed_dataset) {

```",type long long thought better recompile hand version need change one serialize searcher load searcher use package version pip template template class class vector long long auto long long bool false auto return inconsistent among leaf vector storage long long vector storage storage auto,issue,positive,negative,neutral,neutral,negative,negative
773105020,"Your inference output seems stuttering in the early stage. Then it must be related to the inference cycle. Say the proper use of the 'cache' parameter in the function call() because, unlike ordinary RNN with self-attention, fast attention relies on manual relaying of internal states. If it's still not working, then there maybe programming bugs in the inference logic.",inference output stuttering early stage must related inference cycle say proper use parameter function call unlike ordinary fast attention manual internal still working maybe inference logic,issue,negative,positive,neutral,neutral,positive,positive
772942109,"I guess: this error is due to running training with python2. Please try python3.
Also I updated [readme](https://github.com/google-research/google-research/blob/master/kws_streaming/README.md#training-on-custom-data) with more detailed example: please try [kws_experiments_35_labels](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_35_labels.md) - it is an example of model training on custom data.
",guess error due running training python please try python also detailed example please try example model training custom data,issue,negative,positive,positive,positive,positive,positive
771973852,"I am getting the same error. I replaced tf v2 to v1 and while running ""extract_features.py"", I am getting the same error.

:(",getting error running getting error,issue,negative,neutral,neutral,neutral,neutral,neutral
771750125,"@AustinCStone hi, I am interested in pre-trained weights as well for my diploma thesis. Thank you.",hi interested well diploma thesis thank,issue,positive,positive,positive,positive,positive,positive
771551811,"@hyhieu Thanks for your great work! I really admire you!
Could you please explain more about the Equation (10), how can I derive it from REINFORCE equation?
Many thanks!
![Untitled](https://user-images.githubusercontent.com/71539555/106589173-4dc56980-657e-11eb-9eff-219f21e8d547.png)
",thanks great work really admire could please explain equation derive reinforce equation many thanks untitled,issue,positive,positive,positive,positive,positive,positive
771062897,"It seems that in reality, nn.LayerNorm defaults and LayerNormalization defaults to:
![image](https://user-images.githubusercontent.com/1041752/106501534-94a25900-64c3-11eb-865e-8aca5d494162.png)
i.e. stats are calculated PER temporal location, per batch index OVER channels dimension, i.e. every embedding vector is standardized",reality image calculated per temporal location per batch index dimension every vector standardized,issue,negative,neutral,neutral,neutral,neutral,neutral
771061740,"Yes, and that's what I'm doing in that ported code. I was just trying to understand the docs of nn.LayerNorm wrt expected layout (I was worried that TF has default BTC layout, and PyTorch BCT) and the GroupNorm paper visualization confused me (since it also normalizes over the spatial/temporal dimension and TF / PyTorch don't do that by default). The docs phrasing is not very clear  about what's averaged ""over"" versus ""per"" what the stats are calculated. The TF phrasing is a little better, but also not 100% great.

The original LayerNorm paper is also hard to grasp wrt what exactly is averaged over :(",yes ported code trying understand layout worried default layout paper visualization confused since also dimension default phrasing clear versus per calculated phrasing little better also great original paper also hard grasp exactly,issue,positive,positive,positive,positive,positive,positive
771058311,Your implementation looks correct to me on a quick skim. But couldn't you achieve the same by only passing an integer `slot_size` to `torch.nn.LayerNorm(normalized_shape=slot_size)`?,implementation correct quick skim could achieve passing integer,issue,negative,positive,positive,positive,positive,positive
771022240,"So far my conclusion is that PyTorch's nn.LayerNorm does not perform the LayerNorm (as pictured in GroupNorm paper), and is quite hard to coerce to do so, since it requires static `normalized_shape` and blocks dynamic spatial/temporal batch sizes. TensorFlow is much more flexible and just asks the user to supply the list of dimensions to normalize over.

However, by default TF also does not do LayerNorm. In simples case of BTC layout, TF's default LayerNormalization and PyTorch nn.LayerNorm do the same thing and it's not LayerNorm, but something else :(  ",far conclusion perform pictured paper quite hard coerce since static dynamic batch size much flexible user supply list normalize however default also case layout default thing something else,issue,negative,positive,positive,positive,positive,positive
770613342,"@dongzhen123 could you share the answers? It would be helpful a lot. Thanks!
",could share would helpful lot thanks,issue,positive,positive,positive,positive,positive,positive
770334960,"@Feulo Have you tried any of the workarounds you suggested? I'm also trying to run billion-scale datasets, but Scann crashes even with 768GB memory. Or can you give more specific guidelines on how to ""Make a different instance os searcher for each 16GB of the dataset and combine the results""?",tried also trying run even memory give specific make different instance o searcher combine,issue,negative,neutral,neutral,neutral,neutral,neutral
770168282,"My mistake, i was looking at wrong place.",mistake looking wrong place,issue,negative,negative,negative,negative,negative,negative
769634036,"I guess you run only inference colab and test models from older example: these models are not compatible any more and have to be re-trained.
Please update all colabs and run them in order:
[00_check_data.ipynb](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/00_check_data.ipynb)
[01_train.ipynb](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/01_train.ipynb)
[02_inference.ipynb](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/02_inference.ipynb)
",guess run inference test older example compatible please update run order,issue,negative,positive,positive,positive,positive,positive
769610455,"@rybakov I tried it again and along now the flag for `wav` is missing. 

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-27-28f17008ce2c> in <module>()
      1 # prepare mapping of index to word
----> 2 audio_processor = data.input_data.AudioProcessor(flags)
      3 index_to_label = {}
      4 # labels used for training
      5 for word in audio_processor.word_to_index.keys():

/content/google-research/kws_streaming/data/input_data.py in __init__(self, flags)
    186   def __init__(self, flags):
    187     wanted_words = flags.wanted_words.split(',')
--> 188     if flags.wav:
    189       file_ext = '*.wav'
    190     else:

AttributeError: 'Namespace' object has no attribute 'wav'
```

In fact, there are more flags that are missing like this `AttributeError: 'Namespace' object has no attribute 'sp_resample'`",tried along flag missing recent call last module prepare index word used training word self self else object attribute fact missing like object attribute,issue,negative,negative,neutral,neutral,negative,negative
769582878,Sure. Thanks. Let me try to run it again,sure thanks let try run,issue,positive,positive,positive,positive,positive,positive
769559165,"It's `u_aug`. UDA is a separate objective to train the teacher. You could remove UDA from the teacher, or replace UDA with something else like an unlabeled objective from [VAT](https://arxiv.org/abs/1704.03976) or [FixMatch](https://arxiv.org/abs/2001.07685).

Also, that formula for `g_{T, \text{UDA}}^{(t)}` should be all about `x_u`. Thanks for spotting this typo. I'll fix it in the paper too.",separate objective train teacher could remove teacher replace something else like unlabeled objective vat also formula thanks spotting typo fix paper,issue,positive,positive,neutral,neutral,positive,positive
769527910,"It's because the student is trained with `u_aug`:
https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L404

This corresponds to Equation (12) in the paper, where the `x_u` should be the same `x_u` that the student is trained on.
![image](https://user-images.githubusercontent.com/5642307/106222678-ea5dc500-6194-11eb-980d-944acddea6cd.png)
",student trained equation paper student trained image,issue,negative,neutral,neutral,neutral,neutral,neutral
769526361,"https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L409-L413
https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L484-L487
What is the reason for using `u_aug` here? As far as I understand, `u_ori` is used in your paper.",reason far understand used paper,issue,negative,positive,neutral,neutral,positive,positive
769497736,@fbleibel-g Please squash and rebase rather than merge.,please squash rebase rather merge,issue,negative,neutral,neutral,neutral,neutral,neutral
769162269,"> Thanks for your reply, after another trying, it fixed.
> But i wonder that when writting the test outputs in `run_classifier.py` Line 269, the distill_input perhaps should be loaded from the corresponding files like: `{$BAM_DIR}/models/{teachers[task]}/output/{task}_test_predictions_1.pkl`? Due to the testing examples are pickling from the `test.tsv`
> Is there any mismatch between the testing examples and distill inputs during the prediction?

May I know how you fix this? I am running BAM, and on one of my datasets, I am getting key_error and I am not sure why. ",thanks reply another trying fixed wonder test line perhaps loaded corresponding like task task due testing mismatch testing distill prediction may know fix running bam one getting sure,issue,positive,positive,positive,positive,positive,positive
768772252,"> > Not using `stop_gradient` works, but it does not seem to correspond to the feedback formula that we derived for the teacher. This could be an entirely different method.
> 
> If you keep using `stop_gradient`, it won't work as ""Entropy Minimization"" as you said?

Right. Without `stop_gradient`, it works like Entropy Minimization.",work seem correspond feedback formula derived teacher could entirely different method keep wo work entropy minimization said right without work like entropy minimization,issue,negative,positive,positive,positive,positive,positive
768771229,"> Not using `stop_gradient` works, but it does not seem to correspond to the feedback formula that we derived for the teacher. This could be an entirely different method.
> 

If you keep using `stop_gradient`, it won't work as ""Entropy Minimization"" as you said?
",work seem correspond feedback formula derived teacher could entirely different method keep wo work entropy minimization said,issue,negative,neutral,neutral,neutral,neutral,neutral
768527718,"Not using `stop_gradient` works, but it does not seem to correspond to the feedback formula that we derived for the teacher. This could be an entirely different method.

Sampling hard pseudo labels also works, and actually follows our formula. I managed to reproduce the results from the paper with this implementation. [Here](http://tensorboard/experiment/505870852301790830) is the log of the run. Note that this is only the meta pseudo labels training phase; fine-tuning from the last checkpoint on 4,000 labeled examples gave `96.06%`.

I'll update the code repo in a few days with options to sample hard pseudo labels, as well as another option to apply a temperature on the soft pseudo labels (it needs to go through reviews). I will also update the paper to clarify the details on hard vs. soft pseudo labels.",work seem correspond feedback formula derived teacher could entirely different method sampling hard pseudo also work actually formula reproduce paper implementation log run note meta pseudo training phase last gave update code day sample hard pseudo well another option apply temperature soft pseudo need go also update paper clarify hard soft pseudo,issue,negative,negative,neutral,neutral,negative,negative
768234175,"I have the same question.
And is it possible to delete some  data points once the tree is built？
",question possible delete data tree,issue,negative,neutral,neutral,neutral,neutral,neutral
768122655,"@pyyush Hi,
The amazon 3M data on this website http://manikvarma.org/downloads/XC/XMLRepository.html doesn't include metadata.json. It's not in both ""BoW Features"" and ""Raw text""
Do you have a copy？
Thank you!",hi data include bow raw text thank,issue,negative,negative,negative,negative,negative,negative
767974729,"I train the transformer model with the built-in attention. The predictions are good with results like ""A B C D E F G..."".

Then I almost directly replace the built-in attention with the fast one(TF performer). Without any re-training, the predictions become bad with results containing repetive words like ""A B B B...""

Maybe the TF-version performer has a bug. I'll keep going through the code.",train transformer model attention good like almost directly replace attention fast one performer without become bad like maybe performer bug keep going code,issue,positive,positive,neutral,neutral,positive,positive
767376456,"> Just another fast attention user. Haven't studied your model.
> From my observation, the attention_dropout and the training are not yet supported in the build and call function correspondingly.
> In my model, the validation loss is higher than the training loss. Hence, overfitting.
> Is this the reason behind your 'train well predict bad' scenario? If yes, place a separate Dropout layer after this fast Attention layer.
> ps: tensorflow supports embedded dropout badly. e.g. LSTM(dropout=...) memory leakage since 2.0 not fixed until recently >2.3?

I check the code and it's true for ""the attention_dropout and the training are not yet supported in the build"".

For ""If yes, place a separate Dropout layer after this fast Attention layer."", each layer has been wrapped by the PrePostProcessingWrapper layer, which include the layer norm and dropout:
```
# code in DecodeStack/build()
...
        for _ in range(params[""num_hidden_layers""]):
            if not params[""use_fast_attn""]:
                self_attention_layer = attention_layer.SelfAttention(
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""])
                enc_dec_attention_layer = attention_layer.Attention(
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""])
            else:
                self_attention_layer = fast_attention.SelfAttention(
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""],
                    kernel_transformation=fast_attention.softmax_kernel_transformation,
                    causal=True,
                    projection_matrix_type=1,
                    nb_random_features=nb_random_features,
                )
                enc_dec_attention_layer = fast_attention.Attention(
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""],
                    kernel_transformation=fast_attention.softmax_kernel_transformation,
                    causal=False,
                    projection_matrix_type=1,
                    nb_random_features=nb_random_features,
                )
            feed_forward_network = ffn_layer.FeedForwardNetwork(
                params[""hidden_size""], params[""filter_size""], params[""relu_dropout""])
            # ==================wrap each layer===================
            self.layers.append([
                PrePostProcessingWrapper(self_attention_layer, params),  # x->LNorm(x)->{layer(x)=y}->Dropout(y)->y+x
                PrePostProcessingWrapper(enc_dec_attention_layer, params),
                PrePostProcessingWrapper(feed_forward_network, params)
            ])
...
# And the PrePostProcessingWrapper
class PrePostProcessingWrapper(tf.keras.layers.Layer):
    """"""Wrapper class that applies layer pre-processing and post-processing.""""""
    ...
    def call(self, x, *args, **kwargs):
        """"""Calls wrapped layer with same parameters.""""""
        # Preprocessing: apply layer normalization
        training = kwargs[""training""]

        # axis=-1
        y = self.layer_norm(x)  #  ============using pre layer norm==============

        # Get layer output
        y = self.layer(y, *args, **kwargs)

        # Postprocessing: apply dropout and residual connection
        if training:
            y = tf.nn.dropout(y, rate=self.postprocess_dropout)  # ========using dropout when training=========
        return x + y  #
```

I will try to add dropout in the favor_attention.",another fast attention user studied model observation training yet build call function correspondingly model validation loss higher training loss hence reason behind well predict bad scenario yes place separate dropout layer fast attention layer dropout badly memory leakage since fixed recently check code true training yet build yes place separate dropout layer fast attention layer layer wrapped layer include layer norm dropout code range else layer dropout class wrapper class layer call self wrapped layer apply layer normalization training training layer get layer output apply dropout residual connection training dropout return try add dropout,issue,negative,negative,neutral,neutral,negative,negative
767360685,"> Just another fast attention user. Haven't studied your model.
> From my observation, the attention_dropout and the training are not yet supported in the build and call function correspondingly.
> In my model, the validation loss is higher than the training loss. Hence, overfitting.
> Is this the reason behind your 'train well predict bad' scenario? If yes, place a separate Dropout layer after this fast Attention layer.
> ps: tensorflow supports embedded dropout badly. e.g. LSTM(dropout=...) memory leakage since 2.0 not fixed until recently >2.3?

Thanks, I just tried it and indeed didn't delve into the implementation of fast-attention, sorry. 

Out of confidence in the author, I didn't think it was an simple overfitting problem due to the invalid dropout, but a complex one.

I will read the code in detail. 

Thanks again.",another fast attention user studied model observation training yet build call function correspondingly model validation loss higher training loss hence reason behind well predict bad scenario yes place separate dropout layer fast attention layer dropout badly memory leakage since fixed recently thanks tried indeed delve implementation sorry confidence author think simple problem due invalid dropout complex one read code detail thanks,issue,negative,negative,negative,negative,negative,negative
767346379,"> I see your point now, @HansPinckaers and @kekmodel. Thanks for bringing them up. Yes the code seems weird. I will investigate and get back to you soon.

https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L484-L487
If you don't use `stop_gradient`, it will work as intended. (like Entropy Minimization)",see point thanks yes code weird investigate get back soon use work intended like entropy minimization,issue,positive,negative,neutral,neutral,negative,negative
767297019,"I see your point now, @HansPinckaers and @kekmodel. Thanks for bringing them up. Yes the code seems weird. I will investigate and get back to you soon.",see point thanks yes code weird investigate get back soon,issue,positive,negative,neutral,neutral,negative,negative
767248607,"> > https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L484-L487
> > 
> > In the case of soft pseudo labels, the gradient of this formula seems to be 0, right?
> 
> No, the gradient of that formula with respect to the teacher's parameters is not `0`. The function `tf.losses.softmax_cross_entropy` between a `onehot_target` and a `logits` computes `sum([o*l for o, l in zip(onehot_target, tf.nn.log_softmax(logits))])`.
> 
> When `onehot_targets = softmax(logits)`, the result of that function is the entropy of `softmax(logits)`, which is a positive number that leads to a non-zero gradient. This non-zero gradient has been used in [Entropy Minimization](https://papers.nips.cc/paper/2004/file/96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf) for semi-supervised learning. The formula also appears a lot in policy gradient methods from the literature of reinforcement learning.
> 
> > The gradient of CE loss is multiplied by `target - softmax(logit)`.
> 
> I am not sure what you meant by this line?

![gradient of CE loss](https://miro.medium.com/max/210/1*oywcdZSuj7S3YOD2o4_CZQ.png)
So, MPL loss doesn't provide a gradient for the teacher's update, just scales UDA loss, isn't it?
",case soft pseudo gradient formula right gradient formula respect teacher function sum zip result function entropy positive number gradient gradient used entropy minimization learning formula also lot policy gradient literature reinforcement learning gradient ce loss target sure meant line gradient ce loss loss provide gradient teacher update scale loss,issue,negative,positive,positive,positive,positive,positive
767225834,"I fixed colab examples, please update it and let me know if you have any issues.",fixed please update let know,issue,negative,positive,neutral,neutral,positive,positive
766949033,The code is more up-to-date. The difference in hparams is due to the difference between the environments between our internal experiments and the Cloud machines that we tested the open-sourced code on.,code difference due difference internal cloud tested code,issue,negative,negative,neutral,neutral,negative,negative
766945999,"> https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L484-L487
> 
> 
> In the case of soft pseudo labels, the gradient of this formula seems to be 0, right?

No, the gradient of that formula with respect to the teacher's parameters is not `0`. The function `tf.losses.softmax_cross_entropy` between a `onehot_target` and a `logits` computes `sum([o*l for o, l in zip(onehot_target, tf.nn.log_softmax(logits))])`.

When `onehot_targets = softmax(logits)`, the result of that function is the entropy of `softmax(logits)`, which is a positive number that leads to a non-zero gradient. This non-zero gradient has been used in [Entropy Minimization](https://papers.nips.cc/paper/2004/file/96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf) for semi-supervised learning. The formula also appears a lot in policy gradient methods from the literature of reinforcement learning.

> The gradient of CE loss is multiplied by `target - softmax(logit)`.

I am not sure what you meant by this line?
",case soft pseudo gradient formula right gradient formula respect teacher function sum zip result function entropy positive number gradient gradient used entropy minimization learning formula also lot policy gradient literature reinforcement learning gradient ce loss target sure meant line,issue,positive,positive,positive,positive,positive,positive
766787343,"Hi @kekmodel, the root of my question was indeed about the same thing. It is essentially (in pseudocode) a `cross_entropy(preds=softmax(logits), targets=softmax(logits))` as such minimizing this would not make a lot of sense I think. Also, I think you are correct, even if you want to maximize it (when `dot_product` is negative) the computation of the gradients seem still to result in zeroes. However, I didn't have time to investigate the REINFORCE method mentioned by @hyhieu.",hi root question indeed thing essentially would make lot sense think also think correct even want maximize negative computation seem still result however time investigate reinforce method,issue,negative,negative,negative,negative,negative,negative
766775078,"https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L484-L487
In the case of soft pseudo labels, the gradient of this formula seems to be 0, right?
The gradient of CE loss is multiplied by `target - softmax(logit)`.",case soft pseudo gradient formula right gradient ce loss target,issue,negative,positive,positive,positive,positive,positive
766619011,"Thanks Oleg! It seems to be working now. Just a note to whoever runs into this issue, I've uncommented those two lines:

flags = model_params.HOTWORD_MODEL_PARAMS['gru']
flags = model_flags.update_flags(flags)",thanks working note whoever issue uncommented two,issue,negative,positive,positive,positive,positive,positive
766405298,"Thank you for your interest in the paper.

First, the `S(x_u, \theta_T)` should be `T(x_u, \theta_T)`. This is a typo, and we will fix it in our next revision. Thank you for spotting the bug.

Second, the log-gradient trick in the REINFORCE algorithm is applied to go from the first line in your screenshot to the second line. Applying the log-gradient trick moves the `\partial / \partial_T` from outside the expectation to inside the expectation, and creates the new term `\partial / partial \theta_T log{...}`",thank interest paper first typo fix next revision thank spotting bug second trick reinforce algorithm applied go first line second line trick outside expectation inside expectation new term partial log,issue,positive,positive,neutral,neutral,positive,positive
766352496,"Just another fast attention user. Haven't studied your model.
From my observation, the attention_dropout and the training are not yet supported in the build and call function correspondingly.
In my model, the validation loss is higher than the training loss. Hence, overfitting.
Is this the reason behind your 'train well predict bad' scenario? If yes, place a separate Dropout layer after this fast Attention layer.
ps: tensorflow supports embedded dropout badly. e.g. LSTM(dropout=...) memory leakage since 2.0 not fixed until recently >2.3?",another fast attention user studied model observation training yet build call function correspondingly model validation loss higher training loss hence reason behind well predict bad scenario yes place separate dropout layer fast attention layer dropout badly memory leakage since fixed recently,issue,negative,negative,negative,negative,negative,negative
765598607,thank you for reporting it! I will update colab example in couple of days,thank update example couple day,issue,negative,neutral,neutral,neutral,neutral,neutral
765438127,I'm interested on this too. Please let me know if you got the pretrained weights. Thank you,interested please let know got thank,issue,positive,positive,positive,positive,positive,positive
765299357,"Thank you for your response.

I've managed to train on a custom dataset and save checkpoints. Now I'm wondering if there is a way to evaluate on a custom dataset (without labels) using only photometric loss as metric.",thank response train custom save wondering way evaluate custom without photometric loss metric,issue,positive,neutral,neutral,neutral,neutral,neutral
765105271,Thank you for your reply. It will be helpful if the content of the paper is updated as well.,thank reply helpful content paper well,issue,positive,neutral,neutral,neutral,neutral,neutral
764804773,"### Computation of `h`

`h` is computed using Taylor expansion **to approximate the dot product formula described in the paper**.

To see why this is the case, please note that `cross_entropy['s_on_l_old']` is `S(x_l; \theta_S^{(t-1)})` in the paper and `cross_entropy['s_on_l_new']` is `S(x_l; \theta_S^{(t)})` in the paper. The difference between these two `theta`s is precisely `grad_S(x_u; \theta_S^{(t-1)})` (or a centralized version derived from moving averages of past gradients).

Now, the first-order Taylor approximation computes `f(x+h) - f(x) = h^T * grad_f(x)`. If you substitute all the components described above, you will see that the finite difference here approximates the dot product quantity in the paper.

### Using _hard_ or _soft_ labels

Both work equally well. The only place where we actually need the soft labels is the large-scale ImageNet experiments with model parallelism and EfficientNet, but that's for a compiler reason. This repo provides the code for CIFAR-10-4000 and ImageNet-10%, for which both soft and hard labels will do well.",computation expansion approximate dot product formula paper see case please note paper paper difference two theta precisely version derived moving past approximation substitute see finite difference dot product quantity paper work equally well place actually need soft model parallelism compiler reason code soft hard well,issue,positive,negative,neutral,neutral,negative,negative
764489107,"Hi, I switched to 2.4 and most of the code is working but there are some flags that are missing in the inference notebook. When I am loading the weights in these lines:

```python
# create model with flag's parameters
model_non_stream_batch = models.MODELS[flags.model_name](flags)

# load model's weights
weights_name = 'best_weights'
model_non_stream_batch.load_weights(os.path.join(train_dir, weights_name))
```

I am getting this error:

```---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-28-a170e05f648d> in <module>()
      1 # create model with flag's parameters
----> 2 model_non_stream_batch = models.MODELS[flags.model_name](flags)
      3 
      4 # load model's weights
      5 weights_name = 'best_weights'

1 frames
/content/google-research/kws_streaming/models/svdf.py in model(flags)
    104     # it is a self contained model, user need to feed raw audio only
    105     net = speech_features.SpeechFeatures(
--> 106         speech_features.SpeechFeatures.get_params(flags))(
    107             net)
    108 

/content/google-research/kws_streaming/layers/speech_features.py in get_params(flags)
    274     """"""
    275 
--> 276     if flags.time_shift_ms != 0.0 and flags.sp_time_shift_ms != 0.0:
    277       raise ValueError('both time_shift_ms and sp_time_shift_ms are set '
    278                        'only one parameter should be used: '

AttributeError: 'Namespace' object has no attribute 'sp_time_shift_ms'

```",hi switched code working missing inference notebook loading python create model flag load model getting error recent call last module create model flag load model model self model user need feed raw audio net net raise set one parameter used object attribute,issue,negative,negative,neutral,neutral,negative,negative
764482510,"@humandream1  Hi, I have the same question about the scalar h.
Besides the computation of h, I also find that the code uses the soft label, while the paper claims that MPL samples hard pseudo labels from the teacher's output distribution.
https://github.com/google-research/google-research/blob/6a6c881cbce5532bc9661a56476b4f0b97c529ad/meta_pseudo_labels/training_utils.py#L409-L413
![image](https://user-images.githubusercontent.com/7700200/105326379-a6087d00-5c08-11eb-86a6-54ffa650a062.png)

Could @hyhieu help make some explanations? ",hi question scalar besides computation also find code soft label paper hard pseudo teacher output distribution image could help make,issue,negative,negative,neutral,neutral,negative,negative
764244855,"Thank you for reporting it! @ybNo1, the replace you suggest will fix the issues.
There was a refactor for internal use case in testing utils and it was not reflected here.
@wangtz the fix is submitted, please pull the latest version and let me know if you have any issues with it.",thank replace suggest fix internal use case testing reflected fix please pull latest version let know,issue,positive,positive,positive,positive,positive,positive
764219332,"I just replace the bug line and it works :
``` 
# stream_step_size = flags.data_shape[0]
stream_step_size = flags.window_stride_samples * flags.data_stride
``` 
@wangtz  ",replace bug line work,issue,negative,neutral,neutral,neutral,neutral,neutral
763805052,"Hello Hans,

Thank you for your interest in the work. Please find the answers to your
questions inline below, and please let me know if you have any further
questions.

*--Hieu*


On Wed, Jan 20, 2021 at 12:00 AM Hans Pinckaers <notifications@github.com>
wrote:

> Hi there,
>
> I really enjoyed reading the paper by @hyhieu <https://github.com/hyhieu>
> et al.! I don't have the resources to train the models, or even run the
> code, but I do have some questions about the code. Especially about the
> loss of the teacher model. If I'm understanding correctly, the final loss
> is defined here:
>
>
> https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L494-L496
>
> I have a question about the MPL term. cross_entropy['mpl'] * dot_product.
>
> dot_product seems to be a scalar (like mentioned in the paper), without
> gradient computation:
>
>
> https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L483
>
Yes. dot_product is intended to be cross_entropy['s_on_l_new'] -
cross_entropy['s_on_l_old']. I use the Taylor approximation that for any
function f and a small perturbation vector h, we have: f(x+h) - f(x) = h^T
* grad_f(x).
Note that the difference between the weights in the student models
at 's_on_l_new' and 's_on_l_old' is the gradient computed on the data that
the teacher provides, so the resulting difference approximates the dot
product of that gradient and student_grad['s_on_l_old']. For better
understanding, I suggest that you refer to the pseudo code and the
derivation in the Appendix of our paper and figure out the reference which
terms correspond to what parts in the code.

> cross_entropy['mpl'] seems to be defined a few lines above:
>
> https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L484-L487
>
> This seems to be a cross-entropy where the logits and targets are the same
> (since softmax_cross_entropy also applies a softmax to the logits). This
> loss can be non-zero when the targets are not 'hard' (soft targets),
> however, I don't think there is any signal in that case? Since the logits
> and targets seem to be equal.
>
The targets are not equal to the logits. The targets are the softmax of the
logits, but its gradients must be stopped (as done in the code). This loss
will always be positive. It could be scaled into a negative number by the
dot_product, but the cross_entropy loss by itself is always positive.

To understand why the cross_entropy is computed like that, you would need
to refer to our Appendix in the paper. It's a variation of the log-gradient
trick in the REINFORCE algorithm in reinforcement learning. Both soft
targets and hard targets conform to this derivation. In the paper, we
intended to say that ""we *support sampling hard labels*"". In detailed
implementations, we do this for the large-scale tasks on ImageNet+extra
data but we use soft labels for CIFAR-10-4000 and ImageNet-10% as it's
faster. Both works, but for some reasons that I don't know, sampling hard
labels work better for large-scale ImageNet models.

> My question is — if I'm understanding the code correctly — if there is no
> signal and it's scaled by the dot_product, I do not know what the value
> is of this term in the optimization?
>
Maybe this question is related to the one above? As I mentioned, the signal
will always be positive, but could be scaled negatively by dot_product.
",hello thank interest work please find please let know wed wrote hi really reading paper train even run code code especially loss teacher model understanding correctly final loss defined question term scalar like paper without gradient computation yes intended use approximation function small perturbation vector note difference student gradient data teacher resulting difference dot product gradient better understanding suggest refer pseudo code derivation appendix paper figure reference correspond code defined since also loss soft however think signal case since seem equal equal must stopped done code loss always positive could scaled negative number loss always positive understand like would need refer appendix paper variation trick reinforce algorithm reinforcement learning soft hard conform derivation paper intended say support sampling hard detailed data use soft faster work know sampling hard work better question understanding code correctly signal scaled know value term optimization maybe question related one signal always positive could scaled negatively,issue,positive,positive,neutral,neutral,positive,positive
763435297,"I think if you are using tensorflow, you should call tf.saver.save() to save the session.

> I can't find how to save trained model.
> I've seen TensorFlow recommenders have scann integrated in their models, and as such have similar saving process like:
> 
> ```
> scann = tfrs.layers.factorized_top_k.ScaNN(model.user_model, num_reordering_candidates=1000)
> 
> ...
>   scann.save(
>     path,
>     options=tf.saved_model.SaveOptions(namespace_whitelist=[""Scann""])
>   )
> ```
> 
> Any help will be appreciated.
",think call save session ca find save trained model seen similar saving process like path help,issue,positive,neutral,neutral,neutral,neutral,neutral
762526838,"hi @AustinCStone 
I haven't resovled this issue. I found that if I using the dataset converted from Kitti, the data loader can't load data and the iterator train_it is empty. so the training wont start. 
I have no problem when using Sintel dataset. 
 ",hi issue found converted data loader ca load data empty training wont start problem,issue,negative,negative,neutral,neutral,negative,negative
761690903,Thank you for your interests on the SMITH model and Gwikimatch data! I have pushed the data to https://github.com/google-research/google-research/tree/master/gwikimatch. Please check this url for the latest version of data sets.,thank smith model data data please check latest version data,issue,positive,positive,positive,positive,positive,positive
761298143,"Thank you for reporting it! It looks like a bug, which I introduced two weeks ago. 
Please replace ""framing_stride = max(flags.window_stride_samples, window_frame_overlap)"" by ""framing_stride = flags.window_stride_samples"".
Above fix with unit test will be merged in kws_streaming on Monday.

After fix is applied, please retrain your model with additional flag: 
--ds_padding ""'causal', 'causal', 'causal', 'causal', 'causal', 'causal'""
Causal model can be converted to streaming mode out of the box in kws lib.

If a model uses 'same' padding - it is non causal padding (by default ds_tc_resnet uses ds_padding = 'same'). Such a mode can be streamed, but it has to be modified: delay layer has to be inserted in all residual connections and conv layers. Core layers to support this use case were implemented (an example of a model supporting both 'causal' and 'same' paddings is shown [here](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/delay_test.py#L56)), but all models re-implementation was out of scope of the paper.

Please let me know if it works for you.",thank like bug two ago please replace fix unit test fix applied please retrain model additional flag causal model converted streaming mode box model padding non causal padding default mode delay layer inserted residual core support use case example model supporting shown scope paper please let know work,issue,positive,positive,positive,positive,positive,positive
761199330,I've just noticed your comment and am looking into the issue. Would you mind sharing your fix in a pull request and sharing the command you used to train? ,comment looking issue would mind fix pull request command used train,issue,negative,neutral,neutral,neutral,neutral,neutral
761198056,"The Sintel test data does not include labels so you cannot evaluate on it (it is failing to find the flow_uv feature because it is not there). In our paper, in order to perform ablations and test the impact of our changes, we would train on the test data (which does not include labels) and then evaluate on the train data (which does include labels). The test data is what is actually uploaded to the evaluation server, so they keep the labels private. ",test data include evaluate failing find feature paper order perform test impact would train test data include evaluate train data include test data actually evaluation server keep private,issue,negative,neutral,neutral,neutral,neutral,neutral
761196800,"Hi Noam! I've just now noticed your comment and am looking into providing the weights. Sorry for the delay, but I'll get back to you soon. ",hi comment looking providing sorry delay get back soon,issue,negative,negative,negative,negative,negative,negative
761195963,"Yes, what you said is correct. There is also a premade script to convert any video to a trainable dataset: https://github.com/google-research/google-research/blob/master/uflow/misc/convert_video_to_dataset.py",yes said correct also script convert video trainable,issue,negative,neutral,neutral,neutral,neutral,neutral
760608437,"> In our evaluation, we used the samples (20 samples based on the embedding mean and variance). You can use the embedding mean as well (which is less expensive than using 20 samples, but do not contain variance information).

Thx for your reply. Great job.",evaluation used based mean variance use mean well le expensive contain variance information reply great job,issue,positive,negative,neutral,neutral,negative,negative
760593695,"In our evaluation, we used the samples (20 samples based on the embedding mean and variance). You can use the embedding mean as well (which is less expensive than using 20 samples, but do not contain variance information).",evaluation used based mean variance use mean well le expensive contain variance information,issue,negative,negative,negative,negative,negative,negative
759298131,"> > > Hi,
> > > Could you please explain how did you managed to train your network. I'm trying the following command ""python -m uflow.uflow_main --train_on=""chairs:uflow/video_data"" --from_scratch=True --batch_size=1 --checkpoint_dir=""uflow/checkpoints"" --plot_dir=""uflow/plots"", but no checkpoints or plots are saved.
> > > Thnak you.
> > 
> > 
> > Hi, I used this command `#python3 -m uflow.uflow_main --train_on=""chairs:uflow/data_tfrecords/train"" --plot_dir=uflow/plot --init_checkpoint_dir=uflow/checkpoints/chairs_batch1 - -checkpoint_dir=uflow/checkpoints/chairs_test --batch_size=1`, and I do have checkpoints generated.
> > Have you generate tf_record data?
> 
> Hi, did you attempt to train on multiple GPUs?

No
",hi could please explain train network trying following command python saved hi used command python generate data hi attempt train multiple,issue,positive,neutral,neutral,neutral,neutral,neutral
759175149,"hi, I am glad to receive your reply soon. Thank you very much for explaining this problem to me, I have understood it.(it seems no problem in your training and package code).
I have another question:
The output of the code seems to contain three parts: ""embedding mean"", ""embedding variance"", ""embedding samples"" (20 by default in the code). So in the test application stage, If I input 2D keypoints, which is the output really used for evaluation? Is it just to use ""embedding mean"" as the real output of the evaluation(like quantitative evaluation in other datasets, retrival, video alignment in paper)? I don't find an explanation in the paper.
Looking forward to your reply.",hi glad receive reply soon thank much explaining problem understood problem training package code another question output code contain three mean variance default code test application stage input output really used evaluation use mean real output evaluation like quantitative evaluation video alignment paper find explanation paper looking forward reply,issue,negative,positive,neutral,neutral,positive,positive
758889445,"Hi! Thanks for your interest. You can look at compute_keypoint_triplet_loss for the code which does online negative mining (which means that the negatives are found in the batch): https://github.com/google-research/google-research/blob/da2852b846559efc3f7cff87931268b7edb9730f/poem/core/loss_utils.py#L338. The negatives are the 3D poses in the batch that are different from the anchor, and is computed by the function ""compute_negative_indicator_matrix"".

We also added a data processing script to our website here: https://sites.google.com/view/pr-vipe and click on ""data processing script""",hi thanks interest look code negative mining found batch batch different anchor function also added data script click data script,issue,negative,negative,neutral,neutral,negative,negative
758868903,We have provided a data processing script on our website here: https://sites.google.com/view/pr-vipe. You can start from the files provided in the links in the script ,provided data script start provided link script,issue,negative,neutral,neutral,neutral,neutral,neutral
758868341,Hello! We have provided a data processing script on our website here: https://sites.google.com/view/pr-vipe,hello provided data script,issue,negative,neutral,neutral,neutral,neutral,neutral
758272561,"FYI: tflite conversion of CompressedDense is still untested.

CompressedDense contains two kernels: The original weight matrix (_matrix_a_) and the compressed kernel (_matrix_b_ * _matrix_c_). _matrix_a_ will need to be manually removed from the layer before tflite conversion.",conversion still untested two original weight matrix compressed kernel need manually removed layer conversion,issue,negative,positive,positive,positive,positive,positive
757941802,"I tried your command but still no checkpoints saved.

As a training dataset I'm using the script ""convert_video_to_dataset.py"" to convert a video stream to a dataset and I think its systematicly transformed to tfRecords format.

Also, I'm training Uflow for just one epoch using flag ""from_scratch=True"".

This is my full command !python -m uflow.uflow_main --train_on=""custom:uflow/video_data"" --from_scratch=True  --batch_size=1 --checkpoint_dir=uflow/checkpoints --tensorboard_logdir=uflow/plots --plot_dir=uflow/plot

I'm curious about your checkpoints size, since I'm using google colab and the memory is limited.
",tried command still saved training script convert video stream think format also training one epoch flag full command python custom curious size since memory limited,issue,negative,positive,neutral,neutral,positive,positive
757826323,"https://github.com/google-research/google-research/blob/master/poem/tools/gen_train_tfrecords.py#L36
""d. In practice, we include all possible pairs in our training tables. For
   example, for one pose in the Human3.6M dataset (which has 4 views C{0..3}),
   we include all 12 pairs in the CSV, i.e., (C0, C1), (C0, C2), (C0, C3), (C1,
   C0), (C1, C2), (C1, C3), (C2, C0), (C2, C1), (C2, C3), (C3, C0), (C3, C1),
   and (C3, C2).""
If this represents the pose of a single frame, the 3D joint points corresponding to all 12 situations in the current frame are exactly the same. (My current train situation)
If it represents a pose sequence, then there will be very complicated possibilities. I want to know your data package method...",practice include possible training table example one pose include pose single frame joint corresponding current frame exactly current train situation pose sequence complicated want know data package method,issue,negative,negative,negative,negative,negative,negative
757772486,"> Hi,
> 
> Could you please explain how did you managed to train your network. I'm trying the following command ""python -m uflow.uflow_main --train_on=""chairs:uflow/video_data"" --from_scratch=True --batch_size=1 --checkpoint_dir=""uflow/checkpoints"" --plot_dir=""uflow/plots"", but no checkpoints or plots are saved.
> 
> Thnak you.

Hi, I used this command `#python3 -m uflow.uflow_main --train_on=""chairs:uflow/data_tfrecords/train"" --plot_dir=uflow/plot --init_checkpoint_dir=uflow/checkpoints/chairs_batch1 -    -checkpoint_dir=uflow/checkpoints/chairs_test --batch_size=1`, and I do have checkpoints generated. 
Have you generate tf_record data?",hi could please explain train network trying following command python saved hi used command python generate data,issue,positive,neutral,neutral,neutral,neutral,neutral
757020561,"bump, anyone found how to do inference? I've tried using the infer_depth function but not clear how to load the correct checkpoint before calling that function.",bump anyone found inference tried function clear load correct calling function,issue,negative,positive,positive,positive,positive,positive
756798554,"Hi,

Could you please explain how did you managed to train your network. I'm trying the following command ""python -m uflow.uflow_main --train_on=""chairs:uflow/video_data"" --from_scratch=True  --batch_size=1 --checkpoint_dir=""uflow/checkpoints"" --plot_dir=""uflow/plots"", but no checkpoints or plots are saved.

Thnak you.",hi could please explain train network trying following command python saved,issue,positive,neutral,neutral,neutral,neutral,neutral
756539266,"Update !!

To train on a custom dataset you need to change the format from 'custom' to 'chairs' on the flag train_on, this way the training program will call the 'make_dataset' function from generic_flow_dataset.py as flow_dataset.

Please let me know if my remark is valid or not.",update train custom need change format flag way training program call function please let know remark valid,issue,negative,neutral,neutral,neutral,neutral,neutral
756011836,"Hi György , I'm running into the same issue ; wondering if you have found a way around it ? ",hi running issue wondering found way around,issue,negative,neutral,neutral,neutral,neutral,neutral
755411142,"> Facebook's new BlenderBot is claimed to be better than Meena. I think people can alternatively think about testing BlenderBot if they want. All checkpoints of BlenderBot are available here: https://parl.ai/projects/recipes/

Either my implementation of BlenderBot is a big fail, or the bot itself isn't that good. BlenderBot is too predictable, and setting different random seeds did not affect its output. I think finetuning GPT-2 for conversation would be the best way to achieve results, because dialog datasets are (sort of :sweat_smile: where did they find 314GB of conversations) scarse and don't contain enough information about the world. I feel that conversations between people are not enough to teach a model language, common sense, and the human nature.",new better think people alternatively think testing want available either implementation big fail bot good predictable setting different random affect output think conversation would best way achieve sort find contain enough information world feel people enough teach model language common sense human nature,issue,positive,positive,neutral,neutral,positive,positive
753634177,Have any code authors been able to answer this question? This is an important issue for me. Maybe some sort of VAD preprocessing is used to remove non-speech portions of the input audio? @joel-shor @yunjie-zhang ,code able answer question important issue maybe sort used remove input audio,issue,negative,positive,positive,positive,positive,positive
753499936,Thanks! Looks good to me! Including @dattias who can review the change!,thanks good review change,issue,positive,positive,positive,positive,positive,positive
752321029,"Currently the problem is:

While inferring, the first token can be generated correctly, but it starts generating repeatedly from the second step. So the output looks like ""PAD A A A A A A A..."". (A is expected; PAD is the start token of sentence of decoder.)

After the first token has been generated, I remove the PAD-token, then the second token can be correct, but afterwards it repeats again. The output looks like ""A B C B C B C B C..."" or ""A B B B B B B B B..."" (A B C are all expected)",currently problem first token correctly generating repeatedly second step output like pad pad start token sentence first token remove second token correct afterwards output like,issue,negative,positive,neutral,neutral,positive,positive
751827304,@ddemszky Please suggest edits wherever necessary,please suggest wherever necessary,issue,negative,neutral,neutral,neutral,neutral,neutral
751719354,"Hi, when I try to train the network (in depth_and_motion_learning project), I get NaN in loss function even when I train on CPU. 
The error is:
""tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.""
does anybody now why it occurs? ",hi try train network project get nan loss function even train error nan loss training anybody,issue,negative,neutral,neutral,neutral,neutral,neutral
751327073,"Yes, it's better to have rather suboptimal rollouts with a larger coverage for the replay buffer.",yes better rather suboptimal coverage replay buffer,issue,positive,positive,positive,positive,positive,positive
751324793,"I see! so is it fair to say that we would actually want the policy roll-outs to be somewhat sub-optimal? Because really good policy roll-outs (that almost look like the expert) are probably sending a contradictory learning signal to the critic, yes?
Thanks for your responses, they're really helpful!",see fair say would actually want policy somewhat really good policy almost look like expert probably sending contradictory learning signal critic yes thanks really helpful,issue,positive,positive,positive,positive,positive,positive
751167228,"@gunshi I think it might be easier to explain for the toy MDP that we used in the paper. When the regularization is not used, the values are not defined for certain states and actions. Also it's similar to soft targets used in Improved Techniques for Training GANs.",think might easier explain toy used paper regularization used defined certain also similar soft used training,issue,positive,positive,positive,positive,positive,positive
750427400,"Yes, exactly (see my comment above, I guess we posted our comments at the same time :))",yes exactly see comment guess posted time,issue,negative,positive,positive,positive,positive,positive
750426983,"I guess to mimic this, the value matrix would need to be identity and `d_value == length`.
So basically a quadratic matrix with main diagonal set to 1",guess mimic value matrix would need identity length basically quadratic matrix main diagonal set,issue,negative,positive,positive,positive,positive,positive
750426282,"Hi @mschoeler, very good point. Yes, we will definitely add this test to the open-sourced version. We have it in our internal codebase and as you noticed, indeed the error is even smaller if we focus only on the attention matrix. Of course the test can be implemented in particular by taking the value matrix to be an identity matrix. And by the way, we have just updated open-sourced version to measure absolute rather than relative error in the unit test. (https://github.com/google-research/google-research/commit/110d0d49e4043c767eb57085d270c47bf6e254be). ",hi good point yes definitely add test version internal indeed error even smaller focus attention matrix course test particular taking value matrix identity matrix way version measure absolute rather relative error unit test,issue,positive,positive,positive,positive,positive,positive
750424103,"Thanks for your quick reply. 
The error could also be heavily influenced by the dimensionality of the values. Since the weighted sum becomes more similar, if values are sampled from a lower dimensional space I assume.
Wouldn't it make sense to add a unit test for the kernel approximation of the softmax directly?",thanks quick reply error could also heavily dimensionality since weighted sum becomes similar lower dimensional space assume would make sense add unit test kernel approximation directly,issue,negative,positive,neutral,neutral,positive,positive
750405412,"Hi @mschoeler, thank you for an update. What I see is that the mean absolute error is pretty small which is great. My guess is that the small fraction of larger values of absolute error (most of the values are concentrated around 0) correspond also to larger values that are being approximated. Also take into account that the computational block includes renormalization so it is not purely an approximation of the linear transformation of the attention matrix.",hi thank update see mean absolute error pretty small great guess small fraction absolute error concentrated around correspond also also take account computational block purely approximation linear transformation attention matrix,issue,negative,positive,positive,positive,positive,positive
750398687,"Hi @kchoro,
the commit b09ac83 is a predecessor of 6f29c80, so I should have those changes included.
Here is the configuration, which is the same as the standard, except varying in `length`
```
batch_size = 1
length = VAR
num_heads = 1
dim = 8
num_random_features = 30000
```

Your argument makes definetely sense. I updated the table with the max absolute and mean absolute errors.


I used the following code to generate the errors (mostly the unit test code):
```python
def test_softmax_noncausal_attention_block_output(self):
    batch_size = 1
    length = 50 # <-- this varibale is changed for the table
    num_heads = 1
    dim = 8
    num_random_features = 30000
    query = tf.random.normal([batch_size, length, num_heads, dim])
    key = tf.random.normal([batch_size, length, num_heads, dim])
    value = tf.random.normal([batch_size, length, num_heads, dim])
    kernel_transformation = fast_attention.softmax_kernel_transformation
    projection_matrix = fast_attention.create_projection_matrix(
        num_random_features, dim)
    attention_block_output = fast_attention.favor_attention(
        query, key, value, kernel_transformation, False, projection_matrix)
  
    query = tf.multiply(query, 1.0 / math.sqrt(float(dim)))
    attention_scores = tf.einsum(""BXHD,BYHD->BXYH"", query, key)
    attention_scores = tf.nn.softmax(attention_scores, axis=2)
    exact_attention_block_output = tf.einsum(""BXYH,BYHD->BXHD"",
                                             attention_scores, value)
    error = tf.math.abs(exact_attention_block_output - attention_block_output)
    print(""Max: {}"".format(tf.math.reduce_max(error)))
    print(""Mean: {}"".format(tf.math.reduce_mean(error)))
```

length | max abs_error | mean abs_error
---- | ----- | ----
2 | 0.081 | 0.023
4 | 0.19 | 0.047
10 | 0.20 | 0.064
50 | 0.67 | 0.045
500 | 0.67 | 0.022

This is a scatter plot showing abs_error `error` over `exact_attention_block_output`. The error sometimes seems to be quite big
![Figure_1](https://user-images.githubusercontent.com/3592251/103022362-1e1d4d80-454c-11eb-811d-9e2d83cc52f3.png)


",hi commit bac predecessor included configuration standard except length length dim argument sense table absolute mean absolute used following code generate mostly unit test code python self length table dim query length dim key length dim value length dim dim query key value false query query float dim query key value error print error print mean error length mean scatter plot showing error error sometimes quite big,issue,negative,positive,neutral,neutral,positive,positive
750372748,"Also, I think what you observe (for some values of L) is simply much larger *relative* error. The way we compute it in unit tests is: |our_value - exact_value|/exact_value per entry of a matrix (and then take max over all entries). So if exact value is very small, even with very good estimator (with variance going to 0 as approximated values go to 0) you might have large relative error and still very small absolute error which is really what matters at the end. We will update the unit tests to measure absolute error since it is more meaningful and and informative than the relative error.",also think observe simply much relative error way compute unit per entry matrix take exact value small even good estimator variance going go might large relative error still small absolute error really end update unit measure absolute error since meaningful informative relative error,issue,negative,positive,positive,positive,positive,positive
750365024,"How many random features do you use ? Also, we recently updated our TF implementation (see: https://github.com/google-research/google-research/commit/b09ac837cd5720bc60f1c16b472a7ab462b0ddb8) (previously there was a bug in the code).  Did you run the most updated version ?",many random use also recently implementation see previously bug code run version,issue,negative,negative,neutral,neutral,negative,negative
750299556,"For the `urdf_root=FLAGS.urdf_data_path`, you can eliminate that FLAGS; sorry it was used internally for robotics applications and I forgot to delete. I'll update with a followup push later on.

For the `loas2` and `ports=...` parts, I'm importing the `grpc` from an internal library, but this library has been pushed to open-source here: https://github.com/grpc/grpc. I think some of the functions though like `loas2` are google specific (which simply returns my Google user-name for credentialing for secure connections). For `ports...`, it tells the locations that the aggregator client should connect to. 

I'll also edit the followup CL with hopefully the correct way to use GRPC outside of Google. I'm following the examples here: https://grpc.io/docs/languages/python/basics/

In the meantime, I believe what should be done are:
- Delete `ports=` arguments from `futures.ThreadPoolExecutor`
-  Also use `server.add_insecure_port` and `grpc.insecure_channel` which don't need credentialing (and thus removes `loas2`)

Anyways, these implementations are optimized for Google-specific networking; if there's a better method to do client-server connecting on e.g. Cloud instances, you should use that framework instead.

EDIT: here's the new update: https://github.com/google-research/google-research/commit/1142ddee3db18c190d3e628f1dc443225665ff26",eliminate sorry used internally forgot delete update push later internal library library think though like specific simply secure aggregator client connect also edit hopefully correct way use outside following believe done delete also use need thus anyways better method cloud use framework instead edit new update,issue,positive,positive,neutral,neutral,positive,positive
750104861,"@xingyousong 

Sorry for the interrupt again, but I got 2 more questions to ask...

[1]
`es_maml_server.py` and `es_maml_client.py` use a flag for `urdf_data_path`

```
def main(unused_argv):
  base_config = config_util.get_config(urdf_root=FLAGS.urdf_data_path)
  config = config_util.generate_config(
      base_config, current_time_string=FLAGS.current_time_string)
```

However, the flag is not declared at all like below.

```
flags.DEFINE_integer(""server_id"", 0, ""The id of the server."")
flags.DEFINE_integer(""port"", 20000, ""Port number."")
flags.DEFINE_string(""current_time_string"", ""NA"", ""current time string"")
```

As a result, an AttributeError is raised.

How could I solve it? Is it OK to eliminate `urdf_root` parameter?

[2]
And there is a dependency issue too.

First, grpc doesn't have loas2 for now.

Maybe the configuration of grpc has been changed.

Vikram Agarwal said same issue on [this link](https://gitter.im/tensor2tensor/Lobby?at=5d5c6a736ddc404fbce2b346)

Anyway import below doesn't work at all.

```
import grpc
from grpc import loas2 #ImportError
```

Also, `grpc.server()` function doesn't have `ports` argument no more.

```
  server = grpc.server(
      futures.ThreadPoolExecutor(max_workers=100), ports=(port,)) #TypeError
```

Could you give any information of requisite library that you use?

Then It would help me and other users a lot.

In parallel, I will try to find and share any solution for this.

Thank you.",sorry interrupt got ask use flag main however flag declared like id server port port number na current time string result raised could solve eliminate parameter dependency issue first maybe configuration agarwal said issue link anyway import work import import also function argument server port could give information requisite library use would help lot parallel try find share solution thank,issue,positive,negative,neutral,neutral,negative,negative
749475122,I don't think it allows multiple GPUs because it is just a modified version of the original code of BERT. ,think multiple version original code,issue,negative,positive,positive,positive,positive,positive
749473347,"I see. From what I recall, Bert pretraining code did not allow the training on multiple gpus. I wonder if MobileBert does the same. I don't have tpus, so can only use gpus.",see recall pretraining code allow training multiple wonder use,issue,negative,neutral,neutral,neutral,neutral,neutral
749468568,You can use either without distillation parameters but note the authors used larger batch sizes with lamb optimizer and higher learning rate.,use either without distillation note used batch size lamb higher learning rate,issue,negative,positive,positive,positive,positive,positive
749410419,"Hey @ikostrikov, can I also ask for help with the derivation of, or even the intuition behind the replay buffer regularisation version of the objective? It looks to me like in eqn 18 of the arxiv version, we're maximising and minimising the same quantity in the log and linear parts of the expression (for the replay buffer samples). I'm trying to make sense in the replay case becasue my own implementation seems to degrade with policy samples in the replay buffer and I'm trying to figure out why..
Thanks!",hey also ask help derivation even intuition behind replay buffer version objective like version quantity log linear expression replay buffer trying make sense replay case implementation degrade policy replay buffer trying figure thanks,issue,positive,negative,neutral,neutral,negative,negative
749392337,"Maybe I understood your advise properly and finally generated pb2.py and pb2_grpc.py

[The link](https://github.com/tensorflow/serving/issues/42) that you give provides useful information but it is a little bit out-of-date.

So the command below doesn't work.
`$ protoc -I=./ --python_out=./ --grpc_out=./ --plugin=protoc-gen-grpc=``which grpc_python_plugin`` ./first_order.proto`

Alternatively, I refered [this link](https://stackoverflow.com/questions/34713861/python-grpc-protobuf-stubs-generation-issue-grpc-out-protoc-gen-grpc-plugin).

In conclusion, commands below work.
```
$ pip install protobuf
$ pip install grpcio-tools==1.32
$ pip install googleapis-common-protos

$ python -m grpc_tools.protoc --proto_path=. --python_out=. --grpc_python_out=. first_order.proto
```

Thank you for the reply, @xingyousong! ",maybe understood advise properly finally link give useful information little bit command work alternatively link conclusion work pip install pip install pip install python thank reply,issue,positive,positive,neutral,neutral,positive,positive
748990710,"Hi there, thanks for your question. Sorry about that - I didn't realize that the generated `pb2`, `py_pb2`, and `py_pb2_grpc` files wouldn't automatically get sent to this public repository. In Google's internal repository, I have a Bazel ""BUILD"" file defining how the system should build extra libraries from the first_order.proto:
```
proto_library(
    name = ""first_order_proto"",
    srcs = [""first_order.proto""],
    has_services = 1,
    cc_api_version = 2,
)

py_proto_library(
    name = ""first_order_py_pb2"",
    has_services = 1,
    api_version = 2,
    deps = ["":first_order_proto""],
)

google3_py_grpc_library(
    name = ""first_order_py_pb2_grpc"",
    srcs = ["":first_order_proto""],
    deps = ["":first_order_py_pb2""],
)
```

To use the exact same setup with protos I have, you should generate the libraries using GRPC library, e.g. see [an example here](https://github.com/tensorflow/serving/issues/42).

However, these proto definitions are only really useful if you're using GRPC as a way of communication - If you're using a different API to allow the client to communicate with the servers, then you probably don't need any of this and maybe can modify the code to directly use e.g. dictionaries instead of protos as messages.

Hope that helps!

",hi thanks question sorry realize would automatically get sent public repository internal repository build file system build extra name name name use exact setup generate library see example however proto really useful way communication different allow client communicate probably need maybe modify code directly use instead hope,issue,positive,positive,neutral,neutral,positive,positive
748575213,"Hi all, 
the quantized int8 SavedModel can be converted in tensorflow v2.3.0.
I got the result as 
{""exact_match"": 81.06906338694418, ""f1"": 88.54016833795568}.
Thank you.
",hi converted got result thank,issue,negative,neutral,neutral,neutral,neutral,neutral
748444890,i am also interested in knowing if it supports sparse matrices,also interested knowing sparse matrix,issue,negative,positive,positive,positive,positive,positive
747733498,"I have this same question. Resnet-ish has non overlapping 960ms frames, but for a 30 second file I am retrieving 173 feature vectors so that's definitely not 960ms each. How long is each window and what is the overlap? It seems nondeterministic (as 
@yunjie-zhang shows above). @joel-shor ",question non second file feature definitely long window overlap,issue,negative,negative,neutral,neutral,negative,negative
746386301,I actually never tried beam search. We had found greedy worked best in our past experiments.,actually never tried beam search found greedy worked best past,issue,negative,positive,positive,positive,positive,positive
745502425,"Hi there,

We have updated the README and it has more clear instructions now. We will add pretrained models soon.
@HRLTY ",hi clear add soon,issue,negative,positive,positive,positive,positive,positive
745142419,"@VladimirYugay @adizhol @mathmax12 

Run tf.reset_default_graph() before restoring the checkpoint to cope with the error below

> NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
> 
> Key MotionFieldNet/CameraIntrinsics/foci/biases not found in checkpoint
> 	 [[node save_3/RestoreV2 (defined at <ipython-input-13-a5df47d040a7>:2) ]]",run cope error see likely due variable name graph key missing please ensure graph based original error key found node defined,issue,negative,positive,neutral,neutral,positive,positive
744664190,"Not really. I managed to load them, but the output is different

On Sun, Dec 13, 2020 at 12:01 AM Surabhi Verma <notifications@github.com>
wrote:

> @adizhol <https://github.com/adizhol> did you finally manage to convert a
> torch model to a tensorflow model?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/130#issuecomment-743911319>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADD255YEYTQQEBL6VPFJ22DSUPR3LANCNFSM4JNV7GXQ>
> .
>
",really load output different sun wrote finally manage convert torch model model reply directly view,issue,negative,positive,neutral,neutral,positive,positive
743911319,@adizhol did you finally manage to convert a torch model to a tensorflow model?,finally manage convert torch model model,issue,negative,neutral,neutral,neutral,neutral,neutral
742876397,"Hey there, I'm still facing this:

NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key MotionFieldNet/CameraIntrinsics/foci/biases not found in checkpoint
	 [[node save_3/RestoreV2 (defined at <ipython-input-13-a5df47d040a7>:2) ]]



Currently, I'm using the latest code version:

``
        vars_to_restore = [
            v for v in tf.trainable_variables()
            if v.op.name.startswith(DEPTH_SCOPE + '/conv')
        ]
        vars_to_restore = {
            v.op.name[len(DEPTH_SCOPE) + 1:]: v for v in vars_to_restore
        }
``
@adizhol ",hey still facing see likely due variable name graph key missing please ensure graph based original error key found node defined currently latest code version,issue,negative,positive,neutral,neutral,positive,positive
742260754,"> Yes, it is tested with tf nightly builds to guarantee compatibility with the latest tf version.

Ahh okay. Got it. That makes sense. Thank you. I will check again with 2.4.rcx and will close this issue if everything goes fine.",yes tested nightly guarantee compatibility latest version got sense thank check close issue everything go fine,issue,positive,positive,positive,positive,positive,positive
742009218,"Hi Tong, we've just submitted the Tensorflow version of FAVOR+: https://github.com/google-research/google-research/tree/master/performer/fast_attention/tensorflow

Enjoy!",hi tong version enjoy,issue,negative,positive,positive,positive,positive,positive
741999034,"Thank you for evaluating kws_streaming lib! 
Yes, it is tested with tf nightly builds to guarantee compatibility with the latest tf version. Recently there was a refactor in tf.keras, in which 'tf_utils' was replaced by 'control_flow_util'. That is why kws_streaming was evaluated on tf version 2.4.0-dev20200917 as described in [experiments](https://github.com/google-research/google-research/tree/master/kws_streaming/experiments). I would suggest to follow instructions in [experiments](https://github.com/google-research/google-research/tree/master/kws_streaming/experiments), or install tf_nightly.

If you prefer to use tf 2.3, then I would suggest to replace ""from tensorflow.python.keras.utils import control_flow_util"" by ""from tensorflow.python.keras.utils import tf_utils"" it should work with older version of tf.",thank yes tested nightly guarantee compatibility latest version recently version would suggest follow install prefer use would suggest replace import import work older version,issue,positive,positive,positive,positive,positive,positive
740144133,"Hey there, I did some testing and find out that if the dataset has more than 16777220 bytes (2^24 +4) it crashes with the `bad_alloc` error. I don't know why it behaves like that or how to fix it, but I will test some workarounds:

- Make a diferente instace os searcher for each 16GB of the dataset and combine the results
- Comment out the code responsible for save de dataset in the `.npy` format and manually export the dataset and put the file in exported searcher folder
- downgrade to the previous version of SCANN which did not serialized the dataset, and export the searcher to import in the recent version.",hey testing find error know like fix test make o searcher combine comment code responsible save de format manually export put file searcher folder downgrade previous version export searcher import recent version,issue,positive,positive,neutral,neutral,positive,positive
739295596,"Same problem here, I have at leat 500GB of free RAM, but if I try to serialize my searcher (that has 40M rows of 512 float32) it crashes with `MemoryError: std::bad_alloc`.",problem leat free ram try serialize searcher float,issue,negative,positive,positive,positive,positive,positive
738647068,"@zaidalyafeai I see. So essentially I need to train IB-BERT by myself first. And then use this pre-trained model to train MobileBERT. In terms of IB-BERT architecture, it seems the same as MobileBert, just with different configuration. ",see essentially need train first use model train architecture different configuration,issue,negative,positive,neutral,neutral,positive,positive
738277255,"Thank you for your feedback!
I updated [description](https://github.com/google-research/google-research/tree/master/kws_streaming) and added an example of [model streaming with stride/pool](https://github.com/google-research/google-research/blob/master/kws_streaming/models/models_test.py). Please let me know if you have any questions.",thank feedback description added example model streaming please let know,issue,positive,neutral,neutral,neutral,neutral,neutral
738191301,"@UmarSpa This is the configuration for IB-BERT teacher model in the paper. Unfortunately, the authors didn't publish these models. ",configuration teacher model paper unfortunately publish,issue,negative,negative,negative,negative,negative,negative
738182824,"@zaidalyafeai thanks for the reply.

Data split and tf record files are are all good now.

One more question. For the pre-training of the mobilenet,  the teacher checkpoint (`TEACHER_CHECKPOINT`) refers to the pre-trained BERT model. Right ? 
If so, then I suppose the pre-trained model should have the configuration in `uncased_L-24_H-1024_B-512_A-4.json` file. But none of the provided pre-trained models on BERT [repo](https://github.com/google-research/bert) has that configuration. Do you know, by any chance, if there is any other place where I can find a pre-trained BERT model with that configuration ?",thanks reply data split record good one question teacher model right suppose model configuration file none provided configuration know chance place find model configuration,issue,positive,positive,positive,positive,positive,positive
737038419,"Ah..., the hashed_dataset is different without the parameter anisotropic_quantization_threshold = 0.2. ",ah different without parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
736983312,I also want to get records of the above mention...,also want get mention,issue,negative,neutral,neutral,neutral,neutral,neutral
736198572,"Hi @phi-line,

As I mentioned, each subdirectory in the google-research/google-research belongs to a different research group. I'm only involved in the /cubert subdirectory currently, so I am not able to review your pull request.

Apologies.

Petros",hi different research group involved currently able review pull request,issue,negative,positive,positive,positive,positive,positive
736005291,What is this tf2.x-gpu package you're referring to? The TensorFlow 2.x package already includes GPU support: https://www.tensorflow.org/install/gpu#pip_package,package package already support,issue,negative,neutral,neutral,neutral,neutral,neutral
735988248,"I think you should change pip `requirements.txt`.  
When pip install scann , it will uninstall tf2.x-gpu,  then install tf-2.x .  QAQ. 
@sammymax ",think change pip pip install install,issue,negative,neutral,neutral,neutral,neutral,neutral
735950793,"Because ScaNN interacts with TF at the C++ interface level, maintaining compatibility between TF versions is pretty difficult. The `tf-addons` package has the same issue and [here's what they said about it](https://github.com/tensorflow/addons#c-custom-op-compatibility). We would be more than happy to build ScaNN wheels for TF 2.4 (once it's released, it's still in RC status) but there wouldn't be a good way to distribute the wheels--I don't think pip has a mechanism for ""install version A of this package if you have version X of TensorFlow, and install version B of this package if you have version Y of TensorFlow.""

Can you clarify more about compatibility with the GPU versions of TF? From what I understand, the GPU version of TF is no longer a separate package, so there really isn't a distinction between GPU and non-GPU TF anymore.",interface level compatibility pretty difficult package issue said would happy build still status would good way distribute think pip mechanism install version package version install version package version clarify compatibility understand version longer separate package really distinction,issue,positive,positive,positive,positive,positive,positive
734820154,"Hi @UmarSpa 

1. The script doesn't deal with large text files. For my case, I had to split the file into multiple files each with size around ~ 500MB `split -l 100000 files` then you can use a for loop like this one 

```
for file in glob.glob('../train_files/**.txt'):
  !python3 create_pretraining_data.py \
    --input_file={file}.txt \
    --output_file=path/to/output/{file}.tfrecord \
    --vocab_file=vocab.txt \
    --max_seq_length=512 \
    --max_predictions_per_seq=20 \
    --masked_lm_prob=0.15 \
    --random_seed=12345 \
    --dupe_factor=5
```

2. To my understanding mobilebert didn't use two stages of training since `first_num_train_steps=0`. The two stages training are used to speed up the training, in either cases you don't need to create two sets of pretraining data because the code automatically truncate the sequences to `128`.  ",hi script deal large text case split file multiple size around split use loop like one file python file file understanding use two training since two training used speed training either need create two pretraining data code automatically truncate,issue,positive,positive,positive,positive,positive,positive
734777742,"Hi @zaidalyafeai @tromedlov22 
Sorry to bother you. Since you are already working on this, I was wondering if you can help me with my following queries:

- For the data preparation (python create_pretraining_data.py) I am using the [code](https://github.com/google-research/bert/blob/master/create_pretraining_data.py) in BERT repo. I have wiki dataset (13G) and I have sharded it and created 130 small txt files. But when I run the code, it takes almost 1 day for each sharded file. Is there anything I am doing wrong ? I am using max_seq_length=512, as this is needed to replicate the results.

- For mobilebert, do you generate two separate sets of tf.records using the above code. One with max_seq_length=128 and other with max_seq_length=512 ?

",hi sorry bother since already working wondering help following data preparation python code sharded small run code almost day sharded file anything wrong replicate generate two separate code one,issue,negative,negative,negative,negative,negative,negative
733468520,"@maniatis Thanks for the transparency, I'll tag you in my current PR.",thanks transparency tag current,issue,negative,positive,neutral,neutral,positive,positive
733234427,"@phi-line 
> I see, yeah. There are a few bug fixes though, posted by the community and including myself. Would be nice to see these get some attention since there are community projects that reference Google Research projects as a dependencies. I understand that it's not the organization's goal, but it is an Open Source repo after-all with lots of helpful code inside!

Understood. Every subdirectory within the repo is handled by a different team. Certainly if you have any suggestions/PR for google-research/google-research/cubert, bring it on. I'm happy to take a look.",see yeah bug though posted community would nice see get attention since community reference research understand organization goal open source lot helpful code inside understood every within handled different team certainly bring happy take look,issue,positive,positive,positive,positive,positive,positive
733226407,"> @maniatis The way this repository is managed makes it difficult to accept contributions and handle them. It's mostly a write-only medium [...]

I see, yeah. There are a few bug fixes though, posted by the community and including myself. Would be nice to see these get some attention since there are community projects that reference Google Research projects as a dependencies. I understand that it's not the organization's goal, but it is an Open Source repo after-all with lots of helpful code inside!",way repository difficult accept handle mostly medium see yeah bug though posted community would nice see get attention since community reference research understand organization goal open source lot helpful code inside,issue,positive,positive,positive,positive,positive,positive
733206374,"@adityakanade please revert this PR. I've applied its fix so it's no longer required.

Thanks!",please revert applied fix longer thanks,issue,positive,positive,positive,positive,positive,positive
733180771,"> @maniatis I'm not part of the org so I have to defer to you. But, I noticed that google research has a lot of stale PRs left hanging. Would be helpful to have these cleaned so the community can assist better!

@phi-line That's a good point. The way this repository is managed makes it difficult to accept contributions and handle them. It's mostly a write-only medium, compared to other repositories within Google's organizations in GitHub. Apologies for the confusion.",part defer research lot stale left hanging would helpful community assist better good point way repository difficult accept handle mostly medium within confusion,issue,positive,positive,positive,positive,positive,positive
732479289,"@maniatis I'm not part of the org so I have to defer to you. But, I noticed that google research has a lot of stale PRs left hanging. Would be helpful to have these cleaned so the community can assist better!",part defer research lot stale left hanging would helpful community assist better,issue,positive,neutral,neutral,neutral,neutral,neutral
732448188,"Yes please!

Petros

On Mon, Nov 23, 2020 at 12:17 PM kishan <notifications@github.com> wrote:

> Can this ticket be closed then @maniatis <https://github.com/maniatis>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/pull/395#issuecomment-732402355>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF7SXGIQNRBGK2QCI3RD5MDSRK7NBANCNFSM4RQRJP7Q>
> .
>


-- 
Petros Maniatis
Research Scientist
Google Brain
",yes please mon wrote ticket closed reply directly view research scientist brain,issue,positive,neutral,neutral,neutral,neutral,neutral
731091114,"@uniqueness Thank you.  I followed your instructions and found the supplementary material, as well as some videos.",uniqueness thank found supplementary material well,issue,positive,neutral,neutral,neutral,neutral,neutral
730668885,"Excellent! Thanks, Sachin and Jacob.

On Fri, Nov 20, 2020, 3:24 AM Sachin Joglekar <notifications@github.com>
wrote:

> I submitted Jacob's code with lint changes in this commit
> <https://github.com/google-research/google-research/commit/b45380774d8258bceeb5175291eb1e80a3664c3d>
> .
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/pull/452#issuecomment-730554967>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABN567F5OOWYEL6I3MSTHT3SQVPHFANCNFSM4TKOQ5CA>
> .
>
",excellent thanks wrote code lint commit reply directly view,issue,positive,positive,positive,positive,positive,positive
730580263,"> > @studennis911988 Hello, I'm also interested in real depth value, do you how to get the real depth value from the output depth image?
> 
> Did you have any luck finding answer to this?

I have been using something equivalent to equation (4) from this paper to find the scale factor for a particular frame: https://arxiv.org/pdf/2009.07714.pdf. This method uses a scale factor equal to the ratio of median depths between ground truth and prediction. I believe this is a common way to compute the scale factor. The method outlined in the paper I referenced looks interesting, I would be very interested if anyone has tried this and/or found a better approach.",hello also interested real depth value get real depth value output depth image luck finding answer something equivalent equation paper find scale factor particular frame method scale factor equal ratio median ground truth prediction believe common way compute scale factor method outlined paper interesting would interested anyone tried found better approach,issue,positive,positive,positive,positive,positive,positive
729896808,And in my first comment about d (d x d orthogonal blocks) I meant the dimensionality of the embedding after doing the split. ,first comment orthogonal meant dimensionality split,issue,negative,positive,positive,positive,positive,positive
729896177,"So d is the dimensionality of the input between splitting into heads. So for instance, in that setting after splitting into heads, the dimensionality of each embedding is 512 / 8 = 64.",dimensionality input splitting instance setting splitting dimensionality,issue,negative,neutral,neutral,neutral,neutral,neutral
729572655,"@kchoro Thanks a lot.
By the way, what is the true meaning of d in the paper? For example, in fig.6 you said (n_heads, n_layers, d_ff, d) = (8, 36, 1024, 512), so is d=512 the head_size or hidden_size (hedden_size=head_size*n_heads).",thanks lot way true meaning paper example fig said,issue,positive,positive,positive,positive,positive,positive
729296467,"Hey Sachin, two things:

1. I added a few extra flags to control the learning rate schedule. This is unrelated to the linting issues above.

2. I seemed to have fixed all of the problematic formatting issues. My linter picked up a lot of things that were already committed to the codebase, so I'm assuming those particular issues aren't blocking. To that end, I fixed every _new_ formatting issue my code introduced, with the exception of `Access to a protected member _inverted_res_block`, where I opted to disable the check (using `pylint:disable=protected-access`). This saves me from duplicating a lot of mobilenet code. 

Let me know if you run into any issues.",hey two added extra control learning rate schedule unrelated fixed problematic linter picked lot already assuming particular blocking end fixed every issue code exception access member disable check lot code let know run,issue,negative,positive,neutral,neutral,positive,positive
729161858,"So in the ORF setting we use orthogonal blocks. In other words, within each square block samples are orthogonal, but different blocks are chosen independently. That enables us to apply the mechanism also for m > d.",orf setting use orthogonal within square block orthogonal different chosen independently u apply mechanism also,issue,negative,neutral,neutral,neutral,neutral,neutral
728240179,"Hey Jacob, I see multiple PyLint errors internally, which is blocking submission (for example, 'Access to a protected member _inverted_res_block of a client class' & `lambda x: _parse_examples(x)` -> `_parse_examples`). Can you resolve these (I think yapf might be useful)?",hey see multiple internally blocking submission example member client class lambda resolve think might useful,issue,negative,positive,neutral,neutral,positive,positive
726280074,"There is a supplementary materials section of this version of the paper: 
https://storage.googleapis.com/pub-tools-public-publication-data/pdf/662284996d9111bef7ba49285e71ce62a5f2ecbd.pdf
Although I'm not sure it contains everything they reference in the arxiv version.",supplementary section version paper although sure everything reference version,issue,negative,positive,positive,positive,positive,positive
726223934,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
725195944,"Hello, thanks for reaching out! We are working on the evaluation code for release. We will update the repo when it is ready.",hello thanks reaching working evaluation code release update ready,issue,positive,positive,positive,positive,positive,positive
723790712,Travis errors are build issues outside of our project. Seems unrelated to these changes. ,travis build outside project unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
722111730,"> > if I use clang-8,it works,but I have to use gcc10 to build scann and there is a error. Anyone knows how to solve. it likes the different rule between clang and gcc.
> > command：
> > CC=gcc bazel build -c opt --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg
> > --->
> > ERROR: /home/xuyao/src/google-research/scann/scann/distance_measures/one_to_one/BUILD.bazel:146:11: C++ compilation of rule '//scann/distance_measures/one_to_one:dot_product_sse4' failed (Exit 1): gcc failed: error executing command /home/xuyao/gcc9.3.0/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 58 argument(s) skipped)
> > scann/distance_measures/one_to_one/dot_product_sse4.cc: In lambda function:
> > scann/distance_measures/one_to_one/dot_product_sse4.cc:30:62: error: expected '{' before '->' token
> > 30 | auto as_m128i = [](const Byte* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
> > | ^~
> > scann/distance_measures/one_to_one/dot_product_sse4.cc: In function 'double tensorflow::scann_ops::dp_internal::DenseDotProductByteImpl(const Byte*, const Byte*, size_t)':
> > scann/distance_measures/one_to_one/dot_product_sse4.cc:30:74: error: expected primary-expression before '{' token
> > 30 | auto as_m128i = [](const Byte* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
> > | ^
> > scann/distance_measures/one_to_one/dot_product_sse4.cc:30:74: error: expected ',' or ';' before '{' token
> > scann/distance_measures/one_to_one/dot_product_sse4.cc: In lambda function:
> > scann/distance_measures/one_to_one/dot_product_sse4.cc:169:64: error: expected '{' before '->' token
> > 169 | auto as_m128i = [](const int8_t* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
> > | ^~
> > scann/distance_measures/one_to_one/dot_product_sse4.cc: In function 'double tensorflow::scann_ops::dp_internal::DenseDotProductSse4(const tensorflow::scann_ops::DatapointPtr&, const tensorflow::scann_ops::DatapointPtr&)':
> 
> scann/utils/common.h
> line 130,132
> change to
> 
> ``
> #define SCANN_INLINE_LAMBDA
> 
> #define SCANN_OUTLINE
> 
> ```
> 
> when disable these marco, it will solve this error.
> ```

Thank you. About lamda issue, it has been solved. But there is a new issue, can you help me have a look.


./scann/distance_measures/many_to_many/many_to_many_impl.inc:158:59: error: cannot convert '__m128i' to '__m128'
  158 |             __m128i int8s = _mm_loadl_pi(_mm_setzero_si128(),
      |                                          ~~~~~~~~~~~~~~~~~^~
      |                                                           |
      |                                                           __m128i
In file included from /home/intel/gcc10/bin/../lib/gcc/x86_64-pc-linux-gnu/10.1.0/include/emmintrin.h:31,
                 from external/com_google_absl/absl/container/internal/have_sse.h:42,
                 from external/com_google_absl/absl/container/internal/hashtablez_sampler.h:49,
                 from external/com_google_absl/absl/container/internal/raw_hash_set.h:113,
                 from external/com_google_absl/absl/container/internal/raw_hash_map.h:24,
                 from external/com_google_absl/absl/container/flat_hash_map.h:41,
                 from ./scann/utils/common.h:27,
                 from ./scann/utils/types.h:24,
                 from ./scann/data_format/gfv_properties.h:21,
                 from ./scann/data_format/gfv_conversion.h:24,
                 from ./scann/data_format/datapoint.h:21,
                 from ./scann/distance_measures/many_to_many/many_to_many.h:22,
                 from scann/distance_measures/many_to_many/many_to_many_double_results.cc:17:
/home/intel/gcc10/bin/../lib/gcc/x86_64-pc-linux-gnu/10.1.0/include/xmmintrin.h:800:22: note:   initializing argument 1 of '__m128 _mm_loadl_pi(__m128, const __m64*)'
  800 | _mm_loadl_pi (__m128 __A, __m64 const *__P)
      |               ~~~~~~~^~~
",use work use build error anyone solve different rule clang build opt error compilation rule exit error command argument lambda function error token auto function error token auto error token lambda function error token auto function line change define define disable marco solve error thank issue new issue help look error convert file included note argument,issue,negative,positive,neutral,neutral,positive,positive
722098762,"> if I use clang-8,it works,but I have to use gcc10 to build scann and there is a error. Anyone knows how to solve. it likes the different rule between clang and gcc.
> 
> command：
> CC=gcc bazel build -c opt --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg
> 
> --->
> ERROR: /home/xuyao/src/google-research/scann/scann/distance_measures/one_to_one/BUILD.bazel:146:11: C++ compilation of rule '//scann/distance_measures/one_to_one:dot_product_sse4' failed (Exit 1): gcc failed: error executing command /home/xuyao/gcc9.3.0/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 58 argument(s) skipped)
> scann/distance_measures/one_to_one/dot_product_sse4.cc: In lambda function:
> scann/distance_measures/one_to_one/dot_product_sse4.cc:30:62: error: expected '{' before '->' token
> 30 | auto as_m128i = [](const Byte* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
> | ^~
> scann/distance_measures/one_to_one/dot_product_sse4.cc: In function 'double tensorflow::scann_ops::dp_internal::DenseDotProductByteImpl(const Byte*, const Byte*, size_t)':
> scann/distance_measures/one_to_one/dot_product_sse4.cc:30:74: error: expected primary-expression before '{' token
> 30 | auto as_m128i = [](const Byte* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
> | ^
> scann/distance_measures/one_to_one/dot_product_sse4.cc:30:74: error: expected ',' or ';' before '{' token
> scann/distance_measures/one_to_one/dot_product_sse4.cc: In lambda function:
> scann/distance_measures/one_to_one/dot_product_sse4.cc:169:64: error: expected '{' before '->' token
> 169 | auto as_m128i = [](const int8_t* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
> | ^~
> scann/distance_measures/one_to_one/dot_product_sse4.cc: In function 'double tensorflow::scann_ops::dp_internal::DenseDotProductSse4(const tensorflow::scann_ops::DatapointPtr&, const tensorflow::scann_ops::DatapointPtr&)':

scann/utils/common.h
line 130,132
change to 

``
#define SCANN_INLINE_LAMBDA

#define SCANN_OUTLINE
```

when disable these marco, it will solve this error.
",use work use build error anyone solve different rule clang build opt error compilation rule exit error command argument lambda function error token auto function error token auto error token lambda function error token auto function line change define define disable marco solve error,issue,negative,neutral,neutral,neutral,neutral,neutral
721897270,Facebook's new BlenderBot is claimed to be better than Meena. I think people can alternatively think about testing BlenderBot if they want. All checkpoints of BlenderBot are available here: https://parl.ai/projects/recipes/ ,new better think people alternatively think testing want available,issue,negative,positive,positive,positive,positive,positive
721479276,"No prob, glad it worked out! Feel free to close this issue if everything is good. :) ",prob glad worked feel free close issue everything good,issue,positive,positive,positive,positive,positive,positive
721463327,"Very very thank you!

-now, I change my num_steps and get a good result!

-I have checked /tmp/e3d/train, the checkpoint files is existing.  And  I find the cause of ""script.sh: 37: script.sh: [[: not found"" is vash syntax, you are right!

Thank you again!

",thank change get good result checked find cause found syntax right thank,issue,positive,positive,positive,positive,positive,positive
721370976,"Hello, if this issue is resolved, could you close the issue? Thanks!",hello issue resolved could close issue thanks,issue,positive,positive,positive,positive,positive,positive
721370713,"Hello, thanks for your interest!
A couple of points:
- The training finishes really fast because run.sh is only for testing the package requirements and checking that the training runs, and not for actual model training. Here train is called with num_steps=5 and so your training finishes quickly. For the paper, we trained with num_steps=5000000 but the model usually converges much earlier, so you could use num_steps=1000000.
- You seem to have a bash syntax error based on the line ""script.sh: 37: script.sh: [[: not found"". This seem to occur when run.sh is checking for the existence of the checkpoint files. (You can also check this manually by seeing if the files are in /tmp/e3d/train)",hello thanks interest couple training really fast testing package training actual model training train training quickly paper trained model usually much could use seem bash syntax error based line found seem occur existence also check manually seeing,issue,negative,positive,positive,positive,positive,positive
720289312,"I haven't figured it out yet.
I instead used the experimental results that they uploaded on the Google Cloud, [[uq-benchmark-2019](https://console.cloud.google.com/storage/browser/uq-benchmark-2019;tab=objects?pli=1&forceOnObjectsSortingFiltering=false)].

In cifar_model_prediction.hdf5 or mnist_model_prediction.hdf5, you can get the prediction results of the ensemble under distributional shifts.",figured yet instead used experimental cloud get prediction ensemble distributional,issue,negative,positive,neutral,neutral,positive,positive
719995675,"Hi patrickvonplaten, thanks for the comment! We may release the weights in the future, but not as of right now. Thanks for your patience!",hi thanks comment may release future right thanks patience,issue,positive,positive,positive,positive,positive,positive
719991778,"Thanks for the response. 

I agree with you that in higher dimensions this probably doesnt affect the MSE as much.

From what I've observed it does increase variance in all cases, but that doesn't necessarily lead to better performance on downstream tasks.

I'll close this issue for now :)",thanks response agree higher probably doesnt affect much increase variance necessarily lead better performance downstream close issue,issue,positive,positive,positive,positive,positive,positive
719986729,"Thank you for the comment ! So accuracy gains can come from other (than softmax) kernels used in the attention mechanism (FAVOR provides a framework for generalized attention with regular softmax-based one only as a special instantiation and gives linear space & time complexity also for the generalized model). For instance, we observed that in practice Performer-ReLU leveraging kernels with ReLU nonlinearities works very well (see: https://arxiv.org/abs/2009.14794).",thank comment accuracy gain come used attention mechanism favor framework generalized attention regular one special linear space time complexity also generalized model instance practice work well see,issue,positive,positive,positive,positive,positive,positive
719972238,"Hi Parskatt,

My point is that you can get low MSE kernel estimation (*strictly more accurate than iid*) unless your d is small without having exactly Haar measure in the implementation. This is what is done in many papers on ORFs (for reference: https://papers.nips.cc/paper/6246-orthogonal-random-features). Lets conclude this thread with the statement that if you want to have exact Haar measure, you should definitely flip according to R and it is definitely not a bad idea (thank you for catching this !). And of course in theoretical results Haar measure is assumed. I would encourage you to run Performers with R-based flipping and without on a couple of downstream tasks and if you see additional gains in *nontrivial downstream tasks* from flipping, please let us know right away :)",hi point get low kernel estimation strictly accurate unless small without exactly measure implementation done many reference conclude thread statement want exact measure definitely flip according definitely bad idea thank catching course theoretical measure assumed would encourage run without couple downstream see additional gain downstream please let u know right away,issue,positive,positive,positive,positive,positive,positive
719908341,"The simplest case (which is slightly ridicoulus), is `q,r = torch.qr(torch.ones(1,1))` and `q,r = torch.qr(-torch.ones(1,1))`. These always produce the same q.",case slightly always produce,issue,negative,negative,negative,negative,negative,negative
719907463,"Hi, thanks for the response.

I'm not quite sure I follow your reasoning.

I think the distributions are not actually still gaussian after renormalization if you use the biased estimation.

I'm sorry if I'm completely misunderstanding your argument.  I provide a notebook below to show what I mean with biased estimation:

https://colab.research.google.com/drive/1xMSh8c8E8cDK9GqBpVEcGPKHDV6W44C-?usp=sharing

What I get in the special case that d=2 is that literally no rows have an angle between [-pi/2, 0]. I don't see how renormalization would solve this problem. It also means that estimates of any k,q lying in that range would be an underestimate, which increases the variance of the estimate.


",hi thanks response quite sure follow reasoning think actually still use estimation sorry completely misunderstanding argument provide notebook show mean estimation get special case literally angle see would solve problem also lying range would underestimate variance estimate,issue,negative,positive,neutral,neutral,positive,positive
719718871,"Hi Parskatt, thank you for the comment. So you are right that for unbiased sampling from the Haar measure, flipping of the columns via R should be done, however here ultimately we want unbiased low-variance estimation of the kernel value, not of the matrix. If flipping is not done, the marginal distributions  of rows (after chi(d)-renormalization) are still Gaussian so kernel estimator remains unbiased. Furthermore, since all the proofs effectively rely only on the fact that marginal distributions of rows match gaussian and different rows are exactly orthogonal, all the results regarding lower variance for the orthogonal case remain valid regardless of whether you use R or not. So ultimately whether you use R or not should not matter when we talk about the accuracy of kernel estimation. ",hi thank comment right unbiased sampling measure via done however ultimately want unbiased estimation kernel value matrix done marginal chi still kernel estimator remains unbiased furthermore since effectively rely fact marginal match different exactly orthogonal regarding lower variance orthogonal case remain valid regardless whether use ultimately whether use matter talk accuracy kernel estimation,issue,positive,positive,positive,positive,positive,positive
718349370,"Hello! Thanks for your interest. The training data our model takes in is in pairs, which is why the flag read_csv_pairs is enabled by default. You can create a pair by choosing the same pose from different views, and so the header will be double the size of that from a single pose.

You can also refer to this docstring here: https://github.com/google-research/google-research/blob/master/poem/tools/gen_train_tfrecords.py#L18-L40",hello thanks interest training data model flag default create pair choosing pose different header double size single pose also refer,issue,positive,positive,neutral,neutral,positive,positive
717672479,"Yes, I was able to install it successfully and wrote about it [here](https://eugeneyan.com/writing/how-to-install-scann-on-mac/).",yes able install successfully wrote,issue,positive,positive,positive,positive,positive,positive
716044105,Such an astonishing report. I'll reimplement and try this in my own code because I minimize the code as much as possible now. Sorry that I don't have infinite time. Such a shame tho.,astonishing report try code minimize code much possible sorry infinite time shame tho,issue,negative,neutral,neutral,neutral,neutral,neutral
715642402,"- 35 labels speech commands v2:
  * [ds_tc_resnet](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_35_labels.md) model accuracy 96.9%

- 12 labels speech commands v2:
  * [ds_tc_resnet](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_12_labels.md) model accuracy 98.0%
  * [att_mh_rnn](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_12_labels.md) model accuracy 98.3%
",speech model accuracy speech model accuracy model accuracy,issue,negative,neutral,neutral,neutral,neutral,neutral
714842635,"Sorry for the delay, but I will make sure to do it early next week. @craffel will be your best resource for T5.1.1 over in https://github.com/huggingface/transformers/issues/6285",sorry delay make sure early next week best resource,issue,positive,positive,positive,positive,positive,positive
714839117,"Thanks Adam @adarob !

Aside from that, I would also love to try loading these T5.1.1 checkpoints into huggingface library, but it's not possible without knowing exact difference between original T5 and T5.1.1 .  I could not find any source code of T5.1.1 , is it possible to provide the link to the source?",thanks aside would also love try loading library possible without knowing exact difference original could find source code possible provide link source,issue,positive,positive,positive,positive,positive,positive
714599647,"> Hi there, a problem occured when running the bam/run_classifier.py.
> 
> Tensorflow seems to lock the `events.out.tfevents` file until the whole program end, when execute the command `utils.rmkdir(config.checkpoints_dir)` at run_classfier.py line 271, `tf.gfile.DeleteRecursively` appears to can't be done and raise the error given below
> 
> ```
> Traceback (most recent call last):
>   File ""D:/bam/run_classifier.py"", line 281, in <module>
>     tf.app.run()
>   File ""C:\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""D:/bam/run_classifier.py"", line 276, in main
>     utils.rmkdir(config.checkpoints_dir)
>   File ""D:\bam\helpers\utils.py"", line 71, in rmkdir
>     rmrf(path)
>   File ""D:\bam\helpers\utils.py"", line 60, in rmrf
>     tf.gfile.DeleteRecursively(path)
>   File ""C:\Python36\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 563, in delete_recursively
>     delete_recursively_v2(dirname)
>   File ""C:\Python36\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 577, in delete_recursively_v2
>     pywrap_tensorflow.DeleteRecursively(compat.as_bytes(path), status)
>   File ""C:\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 528, in __exit__
>     c_api.TF_GetCode(self.status.status))
> tensorflow.python.framework.errors_impl.FailedPreconditionError: Failed to remove a directory: bam_dir\models\debug-model\checkpoints; Directory not empty
> ```
> 
> The enviroment currently use : tensorflow-gpu 1.13.1 windows10.
> However, when run this code on centos, the problem doesn't exist.
> Really hope to give me some advice

Following the steps in the repo, I am trying to run command` python -m bam.run_classifier debug-model $BAM_DIR '{""debug"": true}'` as a minimal test. However, below line of code, raise `Extra data: line 1 column 5 (char 4) error`. Do you have any idea what should be the root cause? I tried to run other commands, still, this line raises an issue with the JSON.

Line raining the error:
`\bam\run_classifier.py"", line 258, in main config = configure.Config(topdir, model_name, **json.loads(hparams))`

Error:
`raise JSONDecodeError(""Extra data"", s, end) json.decoder.JSONDecodeError: Extra data: line 1 column 5 (char 4)`",hi problem running lock file whole program end execute command line ca done raise error given recent call last file line module file line run main file line main file line path file line path file line file line path status file line remove directory directory empty currently use however run code problem exist really hope give advice following trying run command python true minimal test however line code raise extra data line column char error idea root cause tried run still line issue line error line main error raise extra data end extra data line column char,issue,negative,positive,neutral,neutral,positive,positive
714166982,"yes, these are good points! I also observed that novograd did not help much (although one note here: I use novograd from tensorflow_addons it can be different from original implementation).
I agree that with adam accuracy is slightly better (also it converges faster than sgdm): you can get 96.9% accuracy on data set v2 with 35 labels set up. It is the same accuracy reported in Matchboxnet paper. Here is a [link](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_75k_35_labels.md) with instructions how to reproduce it. 
pick_deterministically option is added.

I will also add a experiment link with 12 labels set up tomorrow.",yes good also help much although one note use different original implementation agree accuracy slightly better also faster get accuracy data set set accuracy paper link reproduce option added also add experiment link set tomorrow,issue,positive,positive,positive,positive,positive,positive
713906211,It seems some recent automatic commit caused this. It should be fixed now by [this commit](https://github.com/google-research/google-research/commit/63ee52c63a7e64daf5f9a92fa25e100a64bdadda). ,recent automatic commit fixed commit,issue,positive,positive,neutral,neutral,positive,positive
713825081,"Should be resolved now. This data was indeed missing. As a solution, I simply deleted the task that had the partial data.
",resolved data indeed missing solution simply task partial data,issue,negative,negative,negative,negative,negative,negative
713158658,"Built successfully following this guide: https://eugeneyan.com//writing/how-to-install-scann-on-mac/

Thanks for the writeup! @eugeneyan",built successfully following guide thanks,issue,positive,positive,neutral,neutral,positive,positive
713106292,"I am running into the exact same problem when running `./run_demo.sh`, despite the fix.
`experiment_util.cc:25:10: fatal error: 'absl/container/node_hash_set.h' file not found`
I am on macOS Catalina version 10.15.7. 
I am not sure what is my gcc version (looks like I am using clang instead). 
```
$ gcc --version
Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/4.2.1
Apple clang version 12.0.0 (clang-1200.0.32.2)
Target: x86_64-apple-darwin19.6.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
```
bazel version: 3.6.0
Was there a later commit that could have brought back the issue?",running exact problem running despite fix fatal error file found catalina version sure version like clang instead version apple clang version target thread model version later commit could brought back issue,issue,positive,positive,positive,positive,positive,positive
712624538,"Give me money please 2000 rupees

On Tue, Oct 20, 2020, 8:51 AM Gary Liu <notifications@github.com> wrote:

> # searcher = scann_builder.create_pybind()
> # searcher.serialize(""./docs/trained_model"")
> searcher = scann_ops_pybind.load_searcher(normalized_dataset, ""./docs/trained_model"")
>
>
> Best Regards!
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/376#issuecomment-712563013>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AQATMZTHESPKO4MJWHXLXXLSLT63RANCNFSM4QNTOSPA>
> .
>
",give money please tue wrote searcher searcher best thread reply directly view,issue,positive,positive,positive,positive,positive,positive
712305026,"Oh I see the problem. It is only decoding the sequence label here not the per-frame labels.

You can change decode function to this:


```python
def decode(serialized_example):
  """"""Decode a serialized a SequenceExample string.
  Args:
    serialized_example: SequenceExample, A SequenceExample from a TFRecord.
  Returns:
    frames: list, A list of frames in the video in SequenceExample.
    name: string, Name of the sequence.
  """"""

  context_features = {
      'label': tf.io.FixedLenFeature([], dtype=tf.int64),
      'len': tf.io.FixedLenFeature([], dtype=tf.int64)
  }
  sequence_features = {
      'video': tf.io.FixedLenSequenceFeature([], dtype=tf.string),
      'frame_labels' = tf.io.FixedLenSequenceFeature([], dtype=tf.int64)
  }

  # Extract features from serialized data.
  context_data, sequence_data = tf.io.parse_single_sequence_example(
      serialized=serialized_example,
      context_features=context_features,
      sequence_features=sequence_features)

  # Decode the encoded JPG images.
  frames = tf.map_fn(tf.image.decode_jpeg, sequence_data['video'],
                     dtype=tf.uint8, parallel_iterations=12)
  label = context_data['label']
  frame_labels = sequence_data.get('frame_labels', [])
  return frames, label, frame_labels
```

And then replace:

```python
for batch_videos, batch_labels in dataset:
    batch_videos = batch_videos.numpy()
    batch_labels = batch_labels.numpy()
```
with 
```python
for batch_videos, batch_labels, batch_frame_labels in dataset:
    batch_videos = batch_videos.numpy()
    batch_labels = batch_labels.numpy()
    batch_frame_labels = batch_frame_labels.numpy()
```

The batch_frame_labels have the per-frame labels you want. ",oh see problem sequence label change decode function python decode decode string list list video name string name extract data decode label return label replace python python want,issue,negative,neutral,neutral,neutral,neutral,neutral
712204622,"Thanks a lot! But I have no idea how to extract frames and per-frame labels from the provided TFRecords using that code. If I print label like this 
<img width=""477"" alt=""print_label"" src=""https://user-images.githubusercontent.com/42955128/96463551-72265180-11ec-11eb-9947-9e721cfe9504.png"">
The labels it prints are always [4]
<img width=""650"" alt=""label"" src=""https://user-images.githubusercontent.com/42955128/96463823-bfa2be80-11ec-11eb-8c5f-9aa72b9bf2e0.png"">
Do you have per-frame annotated label files may publish? Or are there any other ways I can extract per-frame labels from the TFRecords provided? Thanks for your help.",thanks lot idea extract provided code print label like always label label may publish way extract provided thanks help,issue,positive,positive,positive,positive,positive,positive
712082288,"I also had to do this myself. As google is not answering, i guess you can create your own version of the FAD. Will save us all lots of work!",also guess create version fad save u lot work,issue,positive,neutral,neutral,neutral,neutral,neutral
711631350,"Any update about the code link of the  ""Stand-Alone Self-Attention in Vision Models""?",update code link vision,issue,negative,neutral,neutral,neutral,neutral,neutral
710236315,No problem! Ting (co-author) added the script. Feel free to close the issue if this is good for now - and feel free to contact us with any other questions!,problem ting added script feel free close issue good feel free contact u,issue,positive,positive,positive,positive,positive,positive
710066123,Thanks for the fast answer :) Worked!,thanks fast answer worked,issue,negative,positive,positive,positive,positive,positive
709788328,"Hello and thanks for your interest! 
1. There are no pre-trained models shared at the moment.
2. We added code to generate the tfrecords file from a csv file to /tools/gen_train_tfrecords.py. We are still working on releasing the data csv but feel free to use your own csv as input!

Let us know if there is anything else!",hello thanks interest moment added code generate file file still working data feel free use input let u know anything else,issue,positive,positive,positive,positive,positive,positive
709043866,"I tried matchboxnet on my own code and the result is similar to my own mobile tcn (the topology is mostly the same though) I tried before. NovoGrad didn't help in my case. Actually, adam performs slightly better than sgdm in my case. About the input_data I mentioned before
```
# each epoch
train_fingerprints, train_ground_truth = audio_processor.get_data(
        -1, 0, flags, flags.background_frequency,
        flags.background_volume, flags.time_shift_samples, 'training', flags.resample,
        flags.volume_resample, sess)

model.fit(x=train_fingerprints, y=train_ground_truth, batch_size=flags.batch_size, verbose=0)
```
or 
```
for i in range(int(audio_processor.set_size('training')/flags.batch_size)):
      train_fingerprints, train_ground_truth = audio_processor.get_data(
          flags.batch_size, (i*flags.batch_size), flags, flags.background_frequency,
          flags.background_volume, flags.time_shift_samples, 'training', flags.resample,
          flags.volume_resample, sess)
      model.train_on_batch(x=train_fingerprints, y=train_ground_truth)
```

And remember to turn on `pick_deterministically` in get_data function",tried code result similar mobile topology mostly though tried help case actually slightly better case epoch sess range sess remember turn function,issue,positive,positive,positive,positive,positive,positive
708603400,"Another workaround for this problem without upgrading CUDA is to increase the precision of the tensors from float32 to float64, then cast back to float32 after matmul:

```
  rot_mat1 = tf.cast(rot_mat1, tf.float64)
  trans_vec1 = tf.cast(trans_vec1, tf.float64)
  rot_mat2 = tf.cast(rot_mat2, tf.float64)

  r2r1 = tf.matmul(rot_mat2, rot_mat1)
  r2t1 = tf.matmul(rot_mat2, tf.expand_dims(trans_vec1, -1))
  r2t1 = tf.squeeze(r2t1, axis=-1)

  r2r1 = tf.cast(r2r1, tf.float32)
  r2t1 = tf.cast(r2t1, tf.float32)

  return r2r1, r2t1 + trans_vec2
```",another problem without increase precision float float cast back float return,issue,negative,neutral,neutral,neutral,neutral,neutral
708147384,"We updated the filter.txt, similar change to what you suggested. 
Seems that we were using an old version of RICO dataset but now I can hardly find it on their official website. Thanks!",similar change old version hardly find official thanks,issue,negative,positive,neutral,neutral,positive,positive
708062699,"If you plan to run your model on mobile phone, I would suggest to use:
 --feature_type 'mfcc_op'
as it is shown in [kws_experiments_q](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_q.md)
It will reduce model size significantly and will allow you to quantize the model and get 2x speed up.",plan run model mobile phone would suggest use shown reduce model size significantly allow quantize model get speed,issue,negative,positive,positive,positive,positive,positive
707555535,"You are right. Since the images reconstructed from `predicted_keypoints` are not used during training, they are not computed in this code. But it is very easy to get them. Simply add
```
  reconstructed_images_pred = keypoints_to_images_net([
      predicted_keypoints,
      input_images[:, 0, Ellipsis],
      observed_keypoints[:, 0, Ellipsis]])
```
below [this line](https://github.com/google-research/google-research/blob/master/video_structure/train.py#L79), and add `reconstructed_images_pred` as one of the model outputs.

And if you already have a trained model, you can dig through the sub-modules of the Keras model object to find the ""vision"" part of the model (defined [here](https://github.com/google-research/google-research/blob/master/video_structure/vision.py#L105)) and feed it the `predicted_keypoints` to reconstruct the corresponding images.",right since reconstructed used training code easy get simply add ellipsis ellipsis line add one model already trained model dig model object find vision part model defined feed reconstruct corresponding,issue,positive,positive,positive,positive,positive,positive
707509593,"Summary: there is no any streaming during training: we train a model as it is. 
Batch normalization during training compuets moving average of features in time and is updated at every mini batch.

During inference of batch normalization layer, we use moving average value, which was pre-computedd during training (but now it is frozen - it is not updated any more, as shown in keras [normalization.py](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/layers/normalization.py#L765)). It is applied on features and there is no averaging in time becasue we are in inference mode now. So this layer is streamable in inference mode by default.
",summary streaming training train model batch normalization training moving average time every batch inference batch normalization layer use moving average value training frozen shown applied time inference mode layer inference mode default,issue,negative,negative,negative,negative,negative,negative
707461315,@rybakov Thank you so much. have a nice day :),thank much nice day,issue,positive,positive,positive,positive,positive,positive
707423406,"Yes, you should read the audio frames from recording device, stack them in a recording buffer. 
Then read 256 samples (with no overlaps) from recording buffer and feed it into model, then repeat it until recording buffer has no data left.

Please have a look at [colab example](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/02_inference.ipynb) in this example _input_data_ has the whole sequence of audio samples and during streaming we feed _stream_update_ into the model with no overlap. You can think about _input_data_  as your recording buffer.
",yes read audio recording device stack recording buffer read recording buffer feed model repeat recording buffer data left please look example example whole sequence audio streaming feed model overlap think recording buffer,issue,positive,positive,neutral,neutral,positive,positive
707417149,"@rybakov Thanks for your kindly answer.
Let assume that i got the audio data as `512 length 1 frame` from recording device, and then i stack the audio data in recording buffer and feed every `256 length data` to KWS model without any overlap data?
",thanks kindly answer let assume got audio data length frame recording device stack audio data recording buffer feed every length data model without overlap data,issue,positive,positive,positive,positive,positive,positive
707399624,"> Update: I managed to resolve the issue and install it successfully. Documented it [here](https://eugeneyan.com/writing/how-to-install-scann-on-mac/).

Were you able to import and use scann sucessfully? After applying you fixes, I still faced the issues below both Python 3.7.9 and Python 3.8.6.

```>>> import scann
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.7/site-packages/scann/__init__.py"", line 3, in <module>
    from scann.scann_ops.py import scann_ops_pybind
  File ""/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.7/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 30, in <module>
    import scann_pybind
ImportError: dlopen(/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so, 2): Symbol not found: __PyThreadState_Current
  Referenced from: /Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so
  Expected in: flat namespace
 in /Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so
```

With Python 3.6.12 I get this issue:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.6/site-packages/scann/__init__.py"", line 3, in <module>
    from scann.scann_ops.py import scann_ops_pybind
  File ""/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.6/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 30, in <module>
    import scann_pybind
ImportError: dlopen(/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.6/site-packages/scann/scann_ops/cc/python/scann_pybind.so, 2): Symbol not found: __Py_ZeroStruct
  Referenced from: /Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.6/site-packages/scann/scann_ops/cc/python/scann_pybind.so
  Expected in: flat namespace
 in /Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.6/site-packages/scann/scann_ops/cc/python/scann_pybind.so```",update resolve issue install successfully able import use still faced python python import recent call last file line module file line module import file line module import symbol found flat python get issue recent call last file line module file line module import file line module import symbol found flat,issue,positive,positive,neutral,neutral,positive,positive
707389048,"> Update: I managed to resolve the issue and install it successfully. Documented it [here](https://eugeneyan.com/writing/how-to-install-scann-on-mac/).

I have a similar but different error.

```>>> import scann
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.7/site-packages/scann/__init__.py"", line 3, in <module>
    from scann.scann_ops.py import scann_ops_pybind
  File ""/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.7/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 30, in <module>
    import scann_pybind
ImportError: dlopen(/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so, 2): Symbol not found: __PyThreadState_Current
  Referenced from: /Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so
  Expected in: flat namespace
 in /Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so
```

Were you able to resolve this eventually? I've tried on both Python 3.7.9 and Python 3.8.6 and both lead to the same error.

Update: With Python 3.6.12 I get this issue

```
>>> import scann
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.6/site-packages/scann/__init__.py"", line 3, in <module>
    from scann.scann_ops.py import scann_ops_pybind
  File ""/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.6/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 30, in <module>
    import scann_pybind
ImportError: dlopen(/Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.6/site-packages/scann/scann_ops/cc/python/scann_pybind.so, 2): Symbol not found: __Py_ZeroStruct
  Referenced from: /Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.6/site-packages/scann/scann_ops/cc/python/scann_pybind.so
  Expected in: flat namespace
 in /Users/eugeneya/projects/Cobar/src/Cobar/google-research/scann/.venv/lib/python3.6/site-packages/scann/scann_ops/cc/python/scann_pybind.so```",update resolve issue install successfully similar different error import recent call last file line module file line module import file line module import symbol found flat able resolve eventually tried python python lead error update python get issue import recent call last file line module file line module import file line module import symbol found flat,issue,negative,positive,neutral,neutral,positive,positive
707274960,"@LeeYongHyeok the shape of input data in streaming mode depends on ""preprocess"" flag.
By default preprocess=raw, I guess that is what you use in your example.

**For the case when preprocess=raw**
In this case model has [speech feature extractor](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/speech_features.py) with [data framing](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/data_frame.py) and neural network parts - so it is end to end model.
Speech feature extractor layer has several steps: 
1 data framing which will automatically create audio frames.
2 convert audio frames to spectrum features mfcc.

With your parameters
window_size_ms  = 32.0ms
window_stride_ms  = 16.0ms
audio frames will looks like this:
```
        32.ms(frm_0)
<--------------------->
                   32ms (frm_1)
            <--------------------->
                           32ms (frm_2)
                          <--------------------->
```
Let assume that model already processed frm_0, now we need to form frm_1 in [data_frame](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/data_frame.py). To generate frm_1 we need prev_0 and next_1 shown below:
```
        32.ms(frm_0)      
<--------------------->
              prev_0    next_1
           <---------><--------->
                   32ms (frm_1)
           <--------------------->
```
Length of next_1 = 16ms is 256 samples - this is the input shape in streaming mode.
We get prev_0 from frm_0, by taking last 256 samples and then concatenate is with next_1 and get frm_1 = concat(prev_0, next_1).
Now we can repeat this process for next iteration.
[data_frame](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/data_frame.py) keeps the state of the previous frame and manages framing automatically for you, so you just need to feed the new update data with size 256 samples.


**For the case when preprocess=mfcc or preprocess=micro**
Model will receive mfcc features as input, so the input shape in streaming mode will be dct_num_features as shown in [link] (https://github.com/google-research/google-research/blob/master/kws_streaming/layers/modes.py#L69)
In this case framing and speech feature extraction will happen outside of the model and you will need to manage it by yourself.

Thank you for evaluating kws lib! please let me know if you have more questions.",shape input data streaming mode flag default guess use example case case model speech feature extractor data framing neural network end end model speech feature extractor layer several data framing automatically create audio convert audio spectrum audio like let assume model already need form generate need shown length input shape streaming mode get taking last concatenate get repeat process next iteration state previous frame framing automatically need feed new update data size case model receive input input shape streaming mode shown link case framing speech feature extraction happen outside model need manage thank please let know,issue,positive,negative,neutral,neutral,negative,negative
707240762,"@tu1258 Sounds good! I updated [kws_experiments_75k_35_labels.md](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_75k_35_labels.md). Note that by default it uses ""same"" padding as in Matchbox paper (it is not streamable in current config: needs additional delays layers), for streaming the padding should be 'causal'. I also added [tc_resnet_v2](https://github.com/google-research/google-research/blob/master/kws_streaming/models/tc_resnet_v2.py) it is similar model with tc_resnet, but with cleaner parameters settings. 

Yes, spec_aug does not help on small models and it is better to turn it off on such cases: there is no much to regularize there. Please let me know what you changed in input_data.py, so I can validate it.

Hardware chip project sounds challenging! I guess you will need to quantize your model.",tu good note default padding matchbox paper current need additional streaming padding also added similar model cleaner yes help small better turn much regularize please let know validate hardware chip project guess need quantize model,issue,positive,positive,positive,positive,positive,positive
706386164,"@rybakov Dude why do you do this to me lol. Last week I was planning to stop tuning and ready to implement it on hardware chip(this is my major research) but you just keep reporting some awesome models. Don't get me wrong. I am not mad at all and am glad to try it! Thanks! 
Btw, I have two question here. First, spec_aug seems don't help for lightweight model. It may solve overfitting but overfitting rarely occur in lightweight model. How about your case? Second, I found that the `get_data` function in `input_data.py` actually fetch random data in training set. I wonder if you have tried training the full training set. I think in the end it doesn't affect much but a 0~0.5% improvement may be reasonable? Or maybe converge more efficiently? I've tried but the results are slightly better so that I can't tell if it's just luck.",dude last week stop tuning ready implement hardware chip major research keep awesome get wrong mad glad try thanks two question first help lightweight model may solve rarely occur lightweight model case second found function actually fetch random data training set wonder tried training full training set think end affect much improvement may reasonable maybe converge efficiently tried slightly better ca tell luck,issue,positive,positive,positive,positive,positive,positive
706309742,"I'm glad to hear that. How were you able to find the corrupted kepids?

On Fri, Oct 9, 2020 at 7:37 AM stefanofisc <notifications@github.com> wrote:

> Yes, it was one of the first thing I tried. But with this approach I've
> found too many ""corrupted"" kepid only in the first 10k rows. Thus, the
> problem could be in their interpolation method.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/309#issuecomment-706130250>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AQ7FSKFNW7V64FVTAAQ6453SJ3YWPANCNFSM4OGEBXEQ>
> .
>
",glad hear able find corrupted wrote yes one first thing tried approach found many corrupted first thus problem could interpolation method reply directly view,issue,negative,positive,positive,positive,positive,positive
706130250,"Yes, it was one of the first thing I tried. But with this approach I've found too many ""corrupted"" kepid only in the first 10k rows. Thus, the problem could be in their interpolation method.",yes one first thing tried approach found many corrupted first thus problem could interpolation method,issue,negative,positive,positive,positive,positive,positive
703382594,"I might suggest that you write in some code that prints into the Terminal what kepid or rowid the code fails at using the tf.logging.info command. If this works out, let me know.",might suggest write code terminal code command work let know,issue,negative,neutral,neutral,neutral,neutral,neutral
703042674,"@tu1258 here is an [example of training Matchbox model](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_75k_35_labels.md) on data sets v2 with 35 labels. I get around 96.6  on 35 labels (it needs more hyper parameters optimization - please feel free to explore optimizer parameters (novograd) with model settings), and I expect that accuracy on 12 labels will be > 97.4",tu example training matchbox model data get around need hyper optimization please feel free explore model expect accuracy,issue,positive,positive,positive,positive,positive,positive
702883561,"Hi,
I have not resolve the problem yet. I know that the problem is here 
`third_party/kepler_spline/kepler_spline.py"", line 104, in kepler_spline
curve = bspline.iterfit(time[mask], flux[mask], bkspace=bkspace)[0]`
in the interpolation method. In fact, I have tried to generate tfrecord files whitout the interpolation module and it works. But you obviously obtain a noisy light curves.
I was thinking about to change interpolation method.
Any suggestion? 

",hi resolve problem yet know problem line curve time mask flux mask interpolation method fact tried generate interpolation module work obviously obtain noisy light thinking change interpolation method suggestion,issue,negative,positive,positive,positive,positive,positive
702151458,"@MFajcik I have used scann in version 1.0. But the problem is you will store your results in `.scann_artefacts/serialize_test` but this directory was not created yet.
```python
os.makedirs('.scann_artefacts/serialize_test', exist_ok=True)
searcher.serialize(f'.scann_artefacts/serialize_test')
```
will do the job
",used version problem store directory yet python job,issue,negative,neutral,neutral,neutral,neutral,neutral
702026845,"@DavidWiesner which version are you using? In version 1.1, `serialize` method crashes for me even in following dummy scenario:

```
# coding: utf-8
import numpy as np
import os

import scann
import time

start = time.time()
train = np.random.rand(1_000_000, 768)

searcher = scann.scann_ops_pybind.builder(
    db=train,
    num_neighbors=10,
    distance_measure=""dot_product"") \
    .tree(
    num_leaves=2000,
    num_leaves_to_search=100,
    training_sample_size=250_000) \
    .score_ah(
    dimensions_per_block=2,
    anisotropic_quantization_threshold=0.2) \
    .reorder(reordering_num_neighbors=100).build()
end = time.time()
print(f""Indexed in {(end - start) / 60} minutes!"")

os.makedirs('.scann_artefacts/', exist_ok=True)
searcher.serialize(f'.scann_artefacts/serialize_test')
```

produces the output:

```
2020-10-01 11:52:06.382858: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-10-01 11:52:06.382881: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2020-10-01 11:52:18.600410: I scann/partitioning/partitioner_factory_base.cc:58] Size of sampled dataset for training partition: 249544
2020-10-01 11:55:38.117967: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:89] PartitionerFactory ran in 3m19.517490462s.
Indexed in 5.038116590181986 minutes!
Traceback (most recent call last):
  File ""/home/ifajcik/PycharmProjects/QA/fit_qa/scripts/pruning/scann_pruning.py"", line 27, in <module>
    searcher.serialize(f'.scann_artefacts/serialize_test')
  File ""/opt/anaconda3/envs/qaenv/lib/python3.6/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 56, in serialize
    self.searcher.serialize(artifacts_dir)
RuntimeError: Failed to extract SingleMachineFactoryOptions: 
```



",version version serialize method even following dummy scenario import import o import import time start train searcher end print indexed end start output could load dynamic library open object file file directory ignore set machine size training partition ran indexed recent call last file line module file line serialize extract,issue,negative,neutral,neutral,neutral,neutral,neutral
701441403,"![depth](https://user-images.githubusercontent.com/45387325/94702910-9229ca00-033e-11eb-91bb-0ec8724bd877.png)
> Hello @ikramabdel ,Its been a while since I did the inference,but I hope the following snippet works
> 
> `import` numpy as np
> import tensorflow as tf
> from depth_from_video_in_the_wild import model
> import cv2
> import matplotlib.pyplot as plt
> 
> train_model = model.Model(img_height=None, img_width=None, batch_size=None, is_training=False)
> sess = tf.Session()
> saver = tf.train.Saver()
> saver.restore(sess, './depth_from_video_in_the_wild/cityscapes_learned_intrinsics/model.ckpt')
> image = cv2.imread('./depth_from_video_in_the_wild/rgb_image_93.jpgl.png')
> inputs = cv2.resize(image, (416, 128))[np.newaxis,:]
> depth = train_model.inference_depth(inputs, sess)
> 
> dd=(depth[0]*256)
> cv2.imwrite('./depth_from_video_in_the_wild/depth.png',dd)
> 
> plt.figure(figsize=(20, 10))
> plt.imshow(image)
> plt.figure(figsize=(20, 10))
> plt.imshow(1/depth[0,:,:,0], cmap='plasma')

Thank for replying @poornimajd, how do you get a model.ckpt? I am loading the model and the graph. and an example of my prediction above.",depth hello since inference hope following snippet work import import import model import import sess saver sess image image depth sess depth image thank get loading model graph example prediction,issue,positive,neutral,neutral,neutral,neutral,neutral
701392304,"Thanks for your reply! It turns out that I shouldn't use flags in `net = Lambda(lambda x: tf.math.log(tf.math.maximum(x, flags.log_epsilon)))(net)` in my case. I'll close this issue. Thanks for the help, you are always so kind! ❤️ ",thanks reply turn use net lambda lambda net case close issue thanks help always kind,issue,positive,positive,positive,positive,positive,positive
701260354,"I used the same Hendrycks 20news data; yet I used a separate script to read
the data.

On Wed, 30 Sep 2020 at 00:27, Jessie Jie Ren <notifications@github.com>
wrote:

> Hi Jordy-VL,
>
> May I ask how you processed the raw dataset? Did you use the
> data_generate.py script?
>
> Thanks.
>
> Jie
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/393#issuecomment-701025645>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AD2KRWMTFYBW7VXTOARZYIDSIJNOJANCNFSM4ROTMI7A>
> .
>
",used news data yet used separate script read data wed wrote hi may ask raw use script thanks thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
701151450,"This error says that you can not save flags (all parametrs there are belong to global flag). It is not clear why you are getting this error. I created a colab with such a model and it works. I also would suggest to save just model weights: model.save_weights(""/tmp/01/""), please find attached example 
[test.zip](https://github.com/google-research/google-research/files/5303135/test.zip)

",error save belong global flag clear getting error model work also would suggest save model please find attached example,issue,positive,positive,neutral,neutral,positive,positive
701025645,"Hi Jordy-VL,

May I ask how you processed the raw dataset? Did you use the data_generate.py script?

Thanks.

Jie",hi may ask raw use script thanks,issue,negative,negative,neutral,neutral,negative,negative
700808463,"Hello @ikramabdel ,Its been a while since I did the inference,but I hope the following snippet works

`import` numpy as np
import tensorflow as tf
from depth_from_video_in_the_wild import model
import cv2
import matplotlib.pyplot as plt


train_model = model.Model(img_height=None, img_width=None, batch_size=None, is_training=False)
sess = tf.Session()
saver = tf.train.Saver()
saver.restore(sess, './depth_from_video_in_the_wild/cityscapes_learned_intrinsics/model.ckpt')
image = cv2.imread('./depth_from_video_in_the_wild/rgb_image_93.jpgl.png')
inputs = cv2.resize(image, (416, 128))[np.newaxis,:]
depth = train_model.inference_depth(inputs, sess)

dd=(depth[0]*256)
cv2.imwrite('./depth_from_video_in_the_wild/depth.png',dd)

plt.figure(figsize=(20, 10))
plt.imshow(image)
plt.figure(figsize=(20, 10))
plt.imshow(1/depth[0,:,:,0], cmap='plasma')",hello since inference hope following snippet work import import import model import import sess saver sess image image depth sess depth image,issue,negative,neutral,neutral,neutral,neutral,neutral
700730655,@poornimajd hi can you please share how you replaced the model in the inference code Struct2depth to get those results ? Thanks !,hi please share model inference code get thanks,issue,positive,positive,positive,positive,positive,positive
698626270,"Great report, thank you for running it! I will add matchbox experiment. Also will add an example of model training on all labels v2 data sets (similar set up with matchbox paper).",great report thank running add matchbox experiment also add example model training data similar set matchbox paper,issue,positive,positive,positive,positive,positive,positive
698598354,"Can someone please clarify : 
To reproduce the quantized int8 savedmodel as provided here (https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/mobilebert_squad_savedmodels.tar.gz), 
1. Distillation on Pre-trained mobilebert checkpoint using Pre-trained data (https://github.com/google-research/google-research/tree/master/mobilebert#distillation)
2. Running Quantization-aware-training with Squad (https://github.com/google-research/google-research/tree/master/mobilebert#run-quantization-aware-training-with-squad)

Are both the above steps required to get the quantized int8 savedmodel or just the step 2 with pre-trained model will give the quantized int8 model?

@saberkun @renjie-liu @liufengdb @nadongguri 
",someone please clarify reproduce provided distillation data running squad get step model give model,issue,negative,neutral,neutral,neutral,neutral,neutral
697966642,"Even though there are 3 TFRecord files, there are multiple examples in each of them. Those files contain all the videos used for training.

Sorry you'd have to contact the dataset owners about why the pouring dataset is unavailable. ",even though multiple contain used training sorry contact pouring unavailable,issue,negative,negative,negative,negative,negative,negative
697682517,"Thanks! But, it seems like there are only three annotated videos in the tfrecords provided? Do you have the whole pouring dataset available? I need it to have a try on training please! The pouring dataset is unavailable in this website now https://sites.google.com/site/brainrobotdata/home/multiview-pouring.",thanks like three provided whole pouring available need try training please pouring unavailable,issue,positive,positive,positive,positive,positive,positive
696550154,I have found the method to serialize also the tensorflow version. If anybody interested see here https://github.com/google-research/google-research/issues/355#issuecomment-694737822 ,found method serialize also version anybody interested see,issue,negative,positive,positive,positive,positive,positive
696509244,"Due to the results you post, I didn't really try the MatchboxNet. Instead, I experiment separable conv with TC Resnet and group conv with TC Resnet. Here are some results.

  | Data1 | Data2 | Param | Parents | Description
-- | -- | -- | -- | -- | --
V2.0 | 96.03 | 96.05 | 27K | V1.1 | 7x36   layer
V2.1 | 94.5 | 95.02 | 11K | V2.0 | 7x36   layer, 3 group
V2.2 | 95.44 | 95.44 | 12K | V2.0 | Mobile TCN
V2.3 | 94.95 | 95.09 | 12K | V2.2 | Mobile   TCN+Resnet
V2.4 | 93.5 | 93.6 | 6K | V2.0 | 5x20   layer
V2.5 | 93.88 | 94.18 | 6K | V2.0 | 7x36   layer, 6 group
V2.6 | 94.3 | 94.22 | 4.5K | V2.2 | Kernel 5x1, 5x24
V2.7 | 93.68 | 94.43 | 4.5K | V2.6 | Kernel   5x1, 5x24 + Resnet

To make it clear, the network shapes are at below. The resnet didn't use pointwise convolution because of abundant computation and no accuracy improvement in my own experiment. I just add the previous layer(previous two like the original resnet paper) with same shape and accuracy is worse unfortunately. The group convolution is just setting the group in `tf.keras.layer.conv2D`. In addition, since I would like to try ""group pointwise convolution + depthwise convolution"", I didn't just use ""separable convolution"" to implement mobilenet version of TCN.


> **V2.0**
> _________________________________________________________________
> Layer (type)                 Output Shape              Param #   
> =================================================================
> input_1 (InputLayer)         [(128, 16000)]            0         
> _________________________________________________________________
> speech_features (SpeechFeatu (128, 49, 30)             0         
> _________________________________________________________________
> tf_op_layer_ExpandDims (Tens [(128, 49, 30, 1)]        0         
> _________________________________________________________________
> tf_op_layer_Reshape (TensorF [(128, 49, 1, 30)]        0         
> _________________________________________________________________
> conv2d (Conv2D)              (128, 49, 1, 36)          3276      
> _________________________________________________________________
> batch_normalization (BatchNo (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation (Activation)      (128, 49, 1, 36)          0         
> _________________________________________________________________
> conv2d_1 (Conv2D)            (128, 49, 1, 36)          3924      
> _________________________________________________________________
> batch_normalization_1 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_1 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> conv2d_2 (Conv2D)            (128, 49, 1, 36)          3924      
> _________________________________________________________________
> batch_normalization_2 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_2 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> conv2d_3 (Conv2D)            (128, 49, 1, 36)          3924      
> _________________________________________________________________
> batch_normalization_3 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_3 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> conv2d_4 (Conv2D)            (128, 49, 1, 36)          3924      
> _________________________________________________________________
> batch_normalization_4 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_4 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> conv2d_5 (Conv2D)            (128, 49, 1, 36)          3924      
> _________________________________________________________________
> batch_normalization_5 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_5 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> conv2d_6 (Conv2D)            (128, 49, 1, 36)          3924      
> _________________________________________________________________
> batch_normalization_6 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_6 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> average_pooling2d (AveragePo (128, 1, 1, 36)           0         
> _________________________________________________________________
> dropout (Dropout)            (128, 1, 1, 36)           0         
> _________________________________________________________________
> conv2d_7 (Conv2D)            (128, 1, 1, 12)           444       
> _________________________________________________________________
> tf_op_layer_Reshape_1 (Tenso [(128, 12)]               0         
> _________________________________________________________________
> =================================================================
> Total params: 28,272
> Trainable params: 27,768
> Non-trainable params: 504
> _________________________________________________________________

> **V2.2**
> _________________________________________________________________
> Layer (type)                 Output Shape              Param #   
> =================================================================
> input_1 (InputLayer)         [(128, 16000)]            0         
> _________________________________________________________________
> speech_features (SpeechFeatu (128, 49, 30)             0         
> _________________________________________________________________
> tf_op_layer_ExpandDims (Tens [(128, 49, 30, 1)]        0         
> _________________________________________________________________
> tf_op_layer_Reshape (TensorF [(128, 49, 1, 30)]        0         
> _________________________________________________________________
> conv2d (Conv2D)              (128, 49, 1, 36)          3276      
> _________________________________________________________________
> batch_normalization (BatchNo (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation (Activation)      (128, 49, 1, 36)          0         
> _________________________________________________________________
> depthwise_conv2d (DepthwiseC (128, 49, 1, 36)          144       
> _________________________________________________________________
> conv2d_1 (Conv2D)            (128, 49, 1, 36)          1332      
> _________________________________________________________________
> batch_normalization_1 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_1 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> depthwise_conv2d_1 (Depthwis (128, 49, 1, 36)          144       
> _________________________________________________________________
> conv2d_2 (Conv2D)            (128, 49, 1, 36)          1332      
> _________________________________________________________________
> batch_normalization_2 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_2 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> depthwise_conv2d_2 (Depthwis (128, 49, 1, 36)          144       
> _________________________________________________________________
> conv2d_3 (Conv2D)            (128, 49, 1, 36)          1332      
> _________________________________________________________________
> batch_normalization_3 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_3 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> depthwise_conv2d_3 (Depthwis (128, 49, 1, 36)          144       
> _________________________________________________________________
> conv2d_4 (Conv2D)            (128, 49, 1, 36)          1332      
> _________________________________________________________________
> batch_normalization_4 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_4 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> depthwise_conv2d_4 (Depthwis (128, 49, 1, 36)          144       
> _________________________________________________________________
> conv2d_5 (Conv2D)            (128, 49, 1, 36)          1332      
> _________________________________________________________________
> batch_normalization_5 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_5 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> depthwise_conv2d_5 (Depthwis (128, 49, 1, 36)          144       
> _________________________________________________________________
> conv2d_6 (Conv2D)            (128, 49, 1, 36)          1332      
> _________________________________________________________________
> batch_normalization_6 (Batch (128, 49, 1, 36)          144       
> _________________________________________________________________
> activation_6 (Activation)    (128, 49, 1, 36)          0         
> _________________________________________________________________
> average_pooling2d (AveragePo (128, 1, 1, 36)           0         
> _________________________________________________________________
> dropout (Dropout)            (128, 1, 1, 36)           0         
> _________________________________________________________________
> conv2d_7 (Conv2D)            (128, 1, 1, 12)           444       
> _________________________________________________________________
> tf_op_layer_Reshape_1 (Tenso [(128, 12)]               0         
> _________________________________________________________________
> =================================================================
> Total params: 13,584
> Trainable params: 13,080
> Non-trainable params: 504
> _________________________________________________________________

> **V2.6**
> _________________________________________________________________
> Layer (type)                 Output Shape              Param #   
> =================================================================
> input_1 (InputLayer)         [(128, 16000)]            0         
> _________________________________________________________________
> feature_layer (SpeechFeature (128, 49, 30)             0         
> _________________________________________________________________
> tf_op_layer_ExpandDims (Tens [(128, 49, 30, 1)]        0         
> _________________________________________________________________
> tf_op_layer_Reshape (TensorF [(128, 49, 1, 30)]        0         
> _________________________________________________________________
> conv0 (Conv2D)               (128, 49, 1, 24)          744       
> _________________________________________________________________
> bn0 (BatchNormalization)     (128, 49, 1, 24)          96        
> _________________________________________________________________
> act0 (Activation)            (128, 49, 1, 24)          0         
> _________________________________________________________________
> dw1 (DepthwiseConv2D)        (128, 49, 1, 24)          144       
> _________________________________________________________________
> pw1 (Conv2D)                 (128, 49, 1, 24)          600       
> _________________________________________________________________
> bn1 (BatchNormalization)     (128, 49, 1, 24)          96        
> _________________________________________________________________
> act2 (Activation)            (128, 49, 1, 24)          0         
> _________________________________________________________________
> dw2 (DepthwiseConv2D)        (128, 49, 1, 24)          144       
> _________________________________________________________________
> pw2 (Conv2D)                 (128, 49, 1, 24)          600       
> _________________________________________________________________
> bn2 (BatchNormalization)     (128, 49, 1, 24)          96        
> _________________________________________________________________
> act3 (Activation)            (128, 49, 1, 24)          0         
> _________________________________________________________________
> dw3 (DepthwiseConv2D)        (128, 49, 1, 24)          144       
> _________________________________________________________________
> pw3 (Conv2D)                 (128, 49, 1, 24)          600       
> _________________________________________________________________
> bn3 (BatchNormalization)     (128, 49, 1, 24)          96        
> _________________________________________________________________
> act4 (Activation)            (128, 49, 1, 24)          0         
> _________________________________________________________________
> dw4 (DepthwiseConv2D)        (128, 49, 1, 24)          144       
> _________________________________________________________________
> pw4 (Conv2D)                 (128, 49, 1, 24)          600       
> _________________________________________________________________
> bn4 (BatchNormalization)     (128, 49, 1, 24)          96        
> _________________________________________________________________
> act5 (Activation)            (128, 49, 1, 24)          0         
> _________________________________________________________________
> pool (AveragePooling2D)      (128, 1, 1, 24)           0         
> _________________________________________________________________
> dropout (Dropout)            (128, 1, 1, 24)           0         
> _________________________________________________________________
> last_conv (Conv2D)           (128, 1, 1, 12)           300       
> _________________________________________________________________
> tf_op_layer_reshape_1 (Tenso [(128, 12)]               0      
> _________________________________________________________________   
> =================================================================
> Total params: 4,500
> Trainable params: 4,260
> Non-trainable params: 240
> _________________________________________________________________",due post really try instead experiment separable group data data param description layer layer group mobile mobile layer layer group kernel kernel make clear network use pointwise convolution abundant computation accuracy improvement experiment add previous layer previous two like original paper shape accuracy worse unfortunately group convolution setting group addition since would like try group pointwise convolution depthwise convolution use separable convolution implement version layer type output shape param activation activation batch activation batch activation batch activation batch activation batch activation batch activation dropout dropout total trainable layer type output shape param activation activation batch activation batch activation batch activation batch activation batch activation batch activation dropout dropout total trainable layer type output shape param act activation act activation act activation act activation act activation pool dropout dropout total trainable,issue,positive,negative,neutral,neutral,negative,negative
695602122,"Thank you for the correction. I have updated it directly in the repository, because merging in a pull request is too involved.",thank correction directly repository pull request involved,issue,negative,positive,neutral,neutral,positive,positive
695093630,"I don't have all the files listed in the `filter.txt`.
I've attached a txt ([missing_json_files.txt](https://github.com/google-research/google-research/files/5248258/missing_json_files.txt)) with all .json files that are missing in the `unique_uis.tar.gz` (RicoSCA dataset - UI Screenshots and View Hierarchies: https://storage.googleapis.com/crowdstf-rico-uiuc-4540/rico_dataset_v0.1/unique_uis.tar.gz) according to the `filter.txt` file that you mentioned.


",listed attached missing view according file,issue,negative,negative,negative,negative,negative,negative
695075424,Thanks so much! The results about v1-12 and v1-30 are surprising tho. I would post some results in few weeks if there is something I can contribute.,thanks much surprising tho would post something contribute,issue,positive,positive,positive,positive,positive,positive
694737822,"Yes you can with the `serialize` method on the `ScannSearcher`.  ~~Regarding to the issue #397 this is only possible with the non-tensorflow version of the searcher.~~ (**Update**: I found a method to also serialize and load the tensorflow version)

## Serialize numpy `Searcher` (created with `create_pybind()`)
```python
# create serialize target dir
os.makedirs('.scann_artefacts/glove/', exist_ok=True)

# serialize the searcher
searcher.serialize('.scann_artefacts/glove/')

# later... restore the searcher
```

### Load numpy searcher for ScaNN 1.0
```python
searcher = scann.scann_ops_pybind.load_searcher(
                normalized_dataset, 
                '.scann_artefacts/glove/')

```
### Load numpy searcher for ScaNN 1.1
In ScaNN 1.1. loading the searcher will not require the dataset as argument
```python
searcher = scann.scann_ops_pybind.load_searcher('.scann_artefacts/glove/')
```

## Serialize tensorflow `Searcher` (created with `create_tf()`)
```python
# create a tf.Module from the Searcher
scann_module = searcher.serialize_to_module()
# store the scann_module
tf.saved_model.save(scann_module, '.scann_artefacts/glove_tf/')

# later... restore the searcher
```

### Load tensorflow searcher for ScaNN 1.0
```python
scann_module = tf.saved_model.load('.scann_artefacts/glove_tf/')
# create the searcher with the normalized dataset
searcher = scann.scann_ops.searcher_from_module(scann_module, normalized_dataset)
```
### Load tensorflow searcher for ScaNN 1.1
In ScaNN 1.1. loading the searcher will not require the dataset as argument
```python
scann_module = tf.saved_model.load('.scann_artefacts/glove_tf/')
# create the searcher without the normalized dataset
searcher = scann.scann_ops.searcher_from_module(scann_module)
```
",yes serialize method issue possible version update found method also serialize load version serialize searcher python create serialize target serialize searcher later restore searcher load searcher python searcher load searcher loading searcher require argument python searcher serialize searcher python create searcher store later restore searcher load searcher python create searcher searcher load searcher loading searcher require argument python create searcher without searcher,issue,positive,neutral,neutral,neutral,neutral,neutral
694729749,"Thats the same api like the numpy version, excepts the searcher than will work with tensorflow Tensors instead of numpy arrays for input and output. The only thing you need to change is the last call on the `ScannBuilder`. So instead of call `create_pybind()` you need to call `create_tf()`. 

Than the last cell in the `google-research/scann/docs/example.ipynb` will output:

```
tf.Tensor([ 97478 262700 846101 671078 232287], shape=(5,), dtype=int32)
tf.Tensor([2.5518737 2.542952  2.539792  2.5383418 2.519638 ], shape=(5,), dtype=float32)
Latency (ms): 0.7724761962890625
```",thats like version searcher work instead input output thing need change last call instead call need call last cell output latency,issue,negative,neutral,neutral,neutral,neutral,neutral
694673139,"@tu1258 related to your comment about ""higher accuracy with 12 class"" in comparison to 30/35 classes. 
Please have a look at Nvidia report of [jasper model](https://nvidia.github.io/OpenSeq2Seq/html/speech-commands.html#jasper-10x3) on speech commands v1 with 12 and 30 labels. Matchbox model is based on [jasper model](https://arxiv.org/pdf/1904.03288.pdf). In the [link](https://nvidia.github.io/OpenSeq2Seq/html/speech-commands.html#jasper-10x3) they showed that for data sets v1, test accuracy on 12 labels (v1-12) is 96.6% and on 30 labels (v1-30) is 97.3%. So in this case test accuracy on 30 labels is higher than on 12 labels (in our paper we use v1-12 and v2-12 configs), I guess on other model it can be opposite.

To validate Matchbox, you can to re-train it with v1-12 config: 
* set labels parameters in model config matchboxnet_3x1x64_v2.yaml as:   
  labels: ${model.labels_subset}
* specify ""class_split=""sub"" "" when you run process_speech_commands_data.py in  [nemo](https://github.com/NVIDIA/NeMo/blob/main/tutorials/asr/03_Speech_Commands.ipynb), set 200 epochs.

Comments about v1-12 and v1-30 data configs: 
* v1-12 config has ""unknown"" label which contains mixture of different labels from remaining 20 labels - it can make it harder to learn; (v1-30 config does not have ""unknown"" label);
* you should also check the amount of training samples per label in v1-12 and v1-30 configs (it can make a difference in accuracy too).",tu related comment higher accuracy class comparison class please look report jasper model speech matchbox model based jasper model link data test accuracy case test accuracy higher paper use guess model opposite validate matchbox set model specify sub run set data unknown label mixture different make harder learn unknown label also check amount training per label make difference accuracy,issue,positive,positive,neutral,neutral,positive,positive
694649153,"> Hello,
> This is a question for the repo about seq2act paper.
> i follow the steps in the README file under seq2act/data_generation and try to Generate RicoSCA Datasets. I can successfully run seq2act/data_generation/create_rico_sca.sh and geneate tfrecord files，but those generated tfrecord files are empty. Do i need extra step except what listed in the README file? please give me your suggestion when you are free, thank you.

We have fixed the pipeline weeks ago and you may have a try now if have not yet, thanks!",hello question paper follow file try generate successfully run empty need extra step except listed file please give suggestion free thank fixed pipeline ago may try yet thanks,issue,positive,positive,positive,positive,positive,positive
694648651,"> I'm facing the following problem when trying to generate the RicoSCA tfrecord (sh seq2act/data_generation/create_rico_sca.sh)
> 
> ```
> File ""/home/dfalcao/Projects/git/seq2act/data_generation/create_android_synthetic_dataset.py"", line 538, in _write_dataset
>     assert len(all_file_path) == 26210
> AssertionError
> ```
> 
> The current len(all_file_path) is equal to 24598. Am I missing anything? Is the assert line or the RicoSCA dataset (UI Screenshots and View Hierarchies) wrong?

The RICO dataset is very large (more than 26210 json files) and you need download it and put all json files in a folder and use that path for `--input_dir` (see line 27 of `create_rico_sca.sh`). We also have a `filter.txt` (which contains 26210 lines of json file names as a whilelist). 
Could you check your input_dir to confirm that you have all the 26210 files listed in filter.txt? ",facing following problem trying generate sh file line assert current equal missing anything assert line view wrong large need put folder use path see line also file could check confirm listed,issue,negative,negative,neutral,neutral,negative,negative
694619270,"Sorry, didn't notice the notification few days ago. Thanks for your contribution again. Will definitely try it!",sorry notice notification day ago thanks contribution definitely try,issue,positive,negative,neutral,neutral,negative,negative
694614098,It should be possible to extract the frames and labels from the provided TFRecords using this [script](https://github.com/google-research/google-research/blob/master/tcc/dataset_preparation/visualize_tfrecords.py)? Let me know if this doesn't work.,possible extract provided script let know work,issue,negative,neutral,neutral,neutral,neutral,neutral
694513536,"@tu1258, I implemented [Matchbox](https://github.com/google-research/google-research/blob/master/kws_streaming/models/ds_tc_resnet.py) model, but have not trained it yet, so please feel free to run hyper parameters optimization.",tu matchbox model trained yet please feel free run hyper optimization,issue,positive,positive,positive,positive,positive,positive
694472239,"Okay seems like to v2, you need to pass ""file prefix"" of the checkpoint.

```bash
python checkpoint_sparsity.py --checkpoint /home/data/magnitude_pruning/model.ckpt-500000
```

And then use: https://stackoverflow.com/questions/40118062/how-to-read-weights-saved-in-tensorflow-checkpoint-file",like need pas file prefix bash python use,issue,negative,neutral,neutral,neutral,neutral,neutral
694285252,"CLAs look good, thanks!

ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle-research%2Fgoogle-research%2Fpull%2F395) for more info**.

<!-- ok -->",look good thanks information go,issue,positive,positive,positive,positive,positive,positive
693356063,"> @MarziehLenjani, how were you able to resolve this issue?

I did not resolve it. But I guess we should change the graph and add a node that prints values. I 

",able resolve issue resolve guess change graph add node,issue,positive,positive,positive,positive,positive,positive
692371233,"@tu1258 Matchbox and svdf_resnet are similar in terms of model topology.
I can look into Matchbox in a week or so: I will plug the model, and hope you can run hyper parameters optimization with model tuning.

At first glance there are several differences:
1 optimizer Novograd
2 they run model with stride 10ms vs 20ms (my case), so they will invoke a model two times more.

",tu matchbox similar model topology look matchbox week plug model hope run hyper optimization model tuning first glance several run model stride case invoke model two time,issue,positive,positive,neutral,neutral,positive,positive
691729870,"Sorry, have you found the dataset yet? I didn't find it.",sorry found yet find,issue,negative,negative,negative,negative,negative,negative
691653770,"Hi, @rybakov  Sorry, that must be a instant dumb thought. But now there may be one thing worth noted, [Matchboxnet (when the paper was out, there was only `labels_full`)](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/matchboxnet_3x1x64_v1.yaml) seems that they achieved 97+ accuracy with 30/35 class(and the model size is below 100k!)? I doubt if accuracy would be better with more class or more complex words. If not, then it could achieve higher accuracy with 12 class. I think it's worth trying and am now trying to implement Matchboxnet with kws_streaming framework, but fail so far unfortunately. Maybe you could try it?

Will let you know if have any problem with kws. Thanks for the kind help.
",hi sorry must instant dumb thought may one thing worth noted paper accuracy class model size doubt accuracy would better class complex could achieve higher accuracy class think worth trying trying implement framework fail far unfortunately maybe could try let know problem thanks kind help,issue,negative,positive,neutral,neutral,positive,positive
689989251,"Agree, would love a multi-lingual version.",agree would love version,issue,positive,positive,positive,positive,positive,positive
689146697,"I'm facing the following problem when trying to generate the RicoSCA tfrecord (sh seq2act/data_generation/create_rico_sca.sh)
  
```
File ""/home/dfalcao/Projects/git/seq2act/data_generation/create_android_synthetic_dataset.py"", line 538, in _write_dataset
    assert len(all_file_path) == 26210
AssertionError
```

The current len(all_file_path) is equal to 24598. Am I missing anything? Is the assert line or the RicoSCA dataset (UI Screenshots and View Hierarchies) wrong?",facing following problem trying generate sh file line assert current equal missing anything assert line view wrong,issue,negative,negative,negative,negative,negative,negative
687559290,"lippman1125@ thank you for reporting your results - it is great that all your numbers are higher than what I reported in [paper](https://arxiv.org/pdf/2005.06720.pdf). In my case, I trained every model 4 times and reported averaged accuracy, that is why my results are more reserved.",thank great higher paper case trained every model time accuracy reserved,issue,positive,positive,positive,positive,positive,positive
687533974,@MortenHannemose  can you tell me where is the Inference code for other datasets and how can I use it on different datasets because I don't understand from readme?,tell inference code use different understand,issue,negative,neutral,neutral,neutral,neutral,neutral
686956821,"@rybakov 
Thanks for your reply, I have reproduced the results as follow:

| gscd v1 dataset| gscd v1 dataset | gscd v2 dataset|gscd v2 dataset|
|--|--|--|--|
|tc_resnet_305k | tc_resnet_365k | tc_resnet_305k | tc_resnet_365k|
|96.82% | 97.21% | 97.18% | 97.63%|

Each model I trained 15 times and picked the best。

",thanks reply follow model trained time picked,issue,negative,positive,positive,positive,positive,positive
686933604,"The project is everything ok in my task.
Previous error is due to the different chunk head of my data.
I change the chunk head and it's running now.
Thank you
",project everything task previous error due different chunk head data change chunk head running thank,issue,negative,negative,neutral,neutral,negative,negative
686907825,"Hi xcrpkuss, thank you for evaluating kws lib! I did not test kws lib on mobvoi_hotword_dataset and curios to see how it goes, please keep me updated and let me know if you need any help with kws. /Oleg",hi thank test see go please keep let know need help,issue,positive,neutral,neutral,neutral,neutral,neutral
686855886,"Oh it's because of the syntax of my wav file, I solve by changing them.
I'm sorry disturbed you.",oh syntax file solve sorry disturbed,issue,negative,negative,negative,negative,negative,negative
686368646,"create a checkpoint file manually as workaround:
```shell
CKPT_DIR=depth_from_video_in_the_wild/checkpoints/cityscapes_kitti_learned_intrinsics
touch $CKPT_DIR/checkpoint
CKPT_STEP=1000977 # replace with you own latest checkpoint
echo ""model_checkpoint_path: \""model-$CKPT_STEP\""
all_model_checkpoint_paths: \""model-$CKPT_STEP\"""" > $CKPT_DIR/checkpoint
```",create file manually shell touch replace latest echo,issue,negative,positive,positive,positive,positive,positive
686228589,"![image](https://user-images.githubusercontent.com/56855140/92068077-84813300-edd8-11ea-9c40-34ff0b0e1899.png)
Oh,maybe because of the size of wav file?
Thank you if you can give me some  suggestions!!
",image oh maybe size file thank give,issue,negative,neutral,neutral,neutral,neutral,neutral
685236753,"> > > Have you solved it? I have trouble in build from source too.
> > > I have solved this problem. I uninstall bazel 0.21.0 and install bazel with the version of 3.4.1. And if it prompts a error in python path, you can try add ""--action_env=PYTHON_BIN_PATH=/usr/bin/python3"" at the end of step 2.
> 
> 可以加个微信交流下关于scann的吗？
> My wechat: worddz

",trouble build source problem install version error python path try add end step,issue,negative,negative,negative,negative,negative,negative
685061259,"For 1:
Generally, when we have imbalanced data, we would like to down-weight the samples from the majority class.
For the minority class, we can assign w=1.0 to every sample. If w is bigger than 1, it is similar to class weighting in the loss function which is useful for highly imbalanced dataset.

For the noisy label part:
we can consider MentorMix, where we use mixup to average a pair of noisy examples and their labels. 

",generally data would like majority class minority class assign every sample bigger similar class weighting loss function useful highly noisy label part consider use average pair noisy,issue,negative,positive,neutral,neutral,positive,positive
685050409,"Thanks for your quick response!

2. seems a promising approach.

For 1. , ""adding a if statement and uses MentorNet to only computes the weight for negative""? How about the weight of positive samples? Use a weight of 1 for positives? the label(positive/negative) is noisy though. ",thanks quick response promising approach statement weight negative weight positive use weight label noisy though,issue,positive,positive,positive,positive,positive,positive
685034197,"Thanks for the question. In this case, we should consider the label either by:
1. adding a if statement and uses MentorNet to only computes the weight for negative or
2. training a MentorNet that considers the label as input as well. The provided MentorNet uses 0 to represent all labels but notice MentorNet can take label as input:
https://github.com/google/mentornet/blob/76d6be2db1be39714dec6db6bb3bcbb77855ce6e/code/utils.py#L163

For MentorNet training, see https://github.com/google/mentornet/blob/master/TRAINING.md",thanks question case consider label either statement weight negative training label input well provided represent notice take label input training see,issue,negative,negative,neutral,neutral,negative,negative
684611726,"> > Have you solved it? I have trouble in build from source too.
> > I have solved this problem. I uninstall bazel 0.21.0 and install bazel with the version of 3.4.1. And if it prompts a error in python path, you can try add ""--action_env=PYTHON_BIN_PATH=/usr/bin/python3"" at the end of step 2.

Do you know
which message in the proto should we used to parse  ""serialized-partitioner.pb""?  and 
What information  we get from this file?",trouble build source problem install version error python path try add end step know message proto used parse information get file,issue,negative,negative,negative,negative,negative,negative
684141697,"Added couple of examples:

Example of folding/fusing convolution layer with batch normalization [conv_batch_norm_folding](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/conv_batch_norm_folding.ipynb)

Here is end to end test of running convolution with batch normalization with TF and TFLite [conv_batch_norm_on_generated_data](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/conv_batch_norm_on_generated_data.ipynb)
",added couple example convolution layer batch normalization end end test running convolution batch normalization,issue,negative,neutral,neutral,neutral,neutral,neutral
683934919,"Yes, you already found the answer.

In more details:
[input_data.py](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py) in kws_streaming was branched out of [input_data.py](https://github.com/tensorflow/tensorflow/blob/3c75664e72c40fc202fd986903cea39bd526f63d/tensorflow/examples/speech_commands/input_data.py#L318) so that test train data splits are the same and accuracy can be compared with previously published work.

[input_data.py](https://github.com/tensorflow/tensorflow/blob/3c75664e72c40fc202fd986903cea39bd526f63d/tensorflow/examples/speech_commands/input_data.py#L318)  is used to generate testing data list testing_list.txt which is part of archive [speech_commands_v0.02](https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz)
By default 10% of the data are used for testing (listed in testing_list.txt).

Please feel free to look at [colab](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/00_check_data.ipynb). There you can check data properties and validate that list of the data generated for testing by [input_data.py](https://github.com/google-research/google-research/blob/master/kws_streaming/data/input_data.py) is the same with the list of files in testing_list.txt.
Please let me know if you have more questions.
",yes already found answer branched test train data accuracy previously work used generate testing data list part archive default data used testing listed please feel free look check data validate list data testing list please let know,issue,positive,positive,positive,positive,positive,positive
683811177,"Ah, I see it is decided here to have 10% unknown samples out of total set:

https://github.com/tensorflow/tensorflow/blob/3c75664e72c40fc202fd986903cea39bd526f63d/tensorflow/examples/speech_commands/input_data.py#L318",ah see decided unknown total set,issue,negative,negative,neutral,neutral,negative,negative
682990902,"The datasets are updated as well as the README.md file, please have a try, thx!",well file please try,issue,positive,neutral,neutral,neutral,neutral,neutral
681045918,Thanks @shashiongithub! I am desperately waiting for sentence splitting models to generate simpler sentence since your Split and Rephrase paper.,thanks desperately waiting sentence splitting generate simpler sentence since split rephrase paper,issue,negative,negative,negative,negative,negative,negative
680904217,"Thanks @shashiongithub 
Does this mean there will be an SP model in the `assets` (or separately provided until the next version of `roberta24_cnndm` released)? ",thanks mean model asset separately provided next version,issue,positive,negative,neutral,neutral,negative,negative
680854436,"Just to update our email conversation here: We are trying to release our tokenizers, they should be available in a couple of days.",update conversation trying release available couple day,issue,negative,positive,positive,positive,positive,positive
679678936,"About weight decay - I did not see much accuracy improvements with it, but on the other hand I did not run exhaustive hyper parameter optimization, so please feel free to explore it.
Please let me know if you have more questions, thank you for evaluating this lib!",weight decay see much accuracy hand run exhaustive hyper parameter optimization please feel free explore please let know thank,issue,positive,positive,positive,positive,positive,positive
679430089,"lippman1125@ I reverted a code refactor (which had a bug - it was the reason why accuracy went down) please update your code with the latest version. I trained tc_resent model on data set v1 and get 97.1%

I use 365K model:
$CMD_TRAIN \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/tc_resnet/ \
--mel_upper_edge_hertz 7000 \
--how_many_training_steps 20000,20000,20000,20000 \
--learning_rate 0.1,0.05,0.02,0.01 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 40 \
--dct_num_features 40 \
--resample 0.15 \
--alsologtostderr \
--train 1 \
--optimizer 'momentum' \
--lr_schedule 'exp' \
--use_spec_augment 1 \
--time_masks_number 2 \
--time_mask_max_size 10 \
--frequency_masks_number 2 \
--frequency_mask_max_size 5 \
tc_resnet \
--channels '50, 50, 50, 50, 50, 72, 72' \
--debug_2d 0 \
--pool_size '' \
--pool_stride 0 \
--bn_momentum 0.997 \
--bn_center 1 \
--bn_scale 1 \
--bn_renorm 0 \
--dropout 0.2  

About reproducing accuracy 96.6% of model TC-ResNet14-1.5, 305K on data set v1. I re-implemented tc resnet in keras in kws_streaming lib: the model topology is correct, but there can be some differences in speech feature extractor and model weights initialization, because the original TC-ResNet14-1.5 was implemented with slim library (my implementation is based on it but it is not absolutely the same). That is why some hyper-parameters are different with paper(TC-ResNet14-1.5).
With below command I get 96.69% on model topology TC-ResNet14-1.5:

305K model:
$CMD_TRAIN \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/tc_resnet_305/ \
--mel_upper_edge_hertz 7000 \
--how_many_training_steps 20000,20000,20000,20000 \
--learning_rate 0.1,0.05,0.02,0.01 \
--window_size_ms 30.0 \
--window_stride_ms 10.0 \
--mel_num_bins 40 \
--dct_num_features 40 \
--resample 0.15 \
--alsologtostderr \
--train 1 \
--optimizer 'momentum' \
--lr_schedule 'linear' \
--use_spec_augment 1 \
--time_masks_number 2 \
--time_mask_max_size 10 \
--frequency_masks_number 2 \
--frequency_mask_max_size 5 \
tc_resnet \
--kernel_size '(9,1)' \
--channels '24, 36, 36, 48, 48, 72, 72' \
--debug_2d 0 \
--pool_size '' \
--pool_stride 0 \
--bn_momentum 0.997 \
--bn_center 1 \
--bn_scale 1 \
--bn_renorm 0 \
--dropout 0.5 ",code bug reason accuracy went please update code latest version trained model data set get use model resample train dropout accuracy model data set model topology correct speech feature extractor model original slim library implementation based absolutely different paper command get model topology model resample train dropout,issue,positive,positive,positive,positive,positive,positive
679272954,lippman1125@ I reverted a code refactor (which was compatible with eager mode only) please update your code with the latest [version](https://github.com/google-research/google-research/tree/master/kws_streaming). I trained att_mh_rnn model several time on data set v2 and get 98%,code compatible eager mode please update code latest version trained model several time data set get,issue,positive,positive,positive,positive,positive,positive
678826355,"Thanks for reporting your numbers! It looks like I added a bug in my recent code refactor, that is why your accuracy is a little off. I am validating it now and will follow up on Monday.",thanks like added bug recent code accuracy little follow,issue,positive,positive,neutral,neutral,positive,positive
678601456,"@rybakov 
Thanks for your rely,  I got it. 

The logits in the default [script](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper.md#att_mh_rnn) is [batch, 32], the accuracy is 97.1%.
I change the logits last dim from 32 to 12, the accuracy is 97.2%.",thanks rely got default script batch accuracy change last dim accuracy,issue,negative,positive,positive,positive,positive,positive
677979133,"Hello, thank you for evaluating kws lib! 

In this lib we use [SparseCategoricalCrossentropy(from_logits=True)](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L82) loss, so with from_logits=True, return_softmax has to be equal zero. With this set up loss function computation may be more numerically stable as mentioned in [SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)

return_softmax was introduced for special cases where loss function require probability distribution. For example if you change parameters of your loss function SparseCategoricalCrossentropy(from_logits=False) and set from_logits=False then you will need to enable return_softmax=1.

There is no need to modify return_softmax in this lib. If you plan to modify your loss function, only then you should consider to use return_softmax (depending on loss function parameters/config).
Please let me know if you have more questions.",hello thank use loss equal zero set loss function computation may numerically stable special loss function require probability distribution example change loss function set need enable need modify plan modify loss function consider use depending loss function please let know,issue,negative,positive,positive,positive,positive,positive
677929843,"hello, i managed to get it working using tensorflow 1.15, but the results folder is empty.
is there a way to plot/see anything more than the loss? 
this is my result for traffic 

> 324531/324531 [==============================] - 156s 482us/sample - loss: 1.3585
> Computing test loss
> Training completed @ 2020-08-21 19:08:08.512450
> Best validation loss = 1.3585465860985548

@SachitNayak @Ubikas",hello get working folder empty way anything loss result traffic loss test loss training best validation loss,issue,negative,positive,positive,positive,positive,positive
676326697,"Never mind, the problem was during installation.

For MacOs (Catalina 10.15.6)  I needed to change all `#include <hash_set>`


```
 #if defined __GNUC__ || defined __APPLE__
#include <ext/hash_set>
#else
#include <hash_set>
#endif
```",never mind problem installation catalina change include defined defined include else include,issue,negative,neutral,neutral,neutral,neutral,neutral
675112335,"Hi,

There are a few missing files needed for the RicoSCA pipeline to work properly. We will fix the pipeline soon!

Yang",hi missing pipeline work properly fix pipeline soon yang,issue,negative,negative,neutral,neutral,negative,negative
674994678,"Hi all, very sorry for the inconveniences and late response. We encountered some non-technical issues when releasing some of the data. We are actively working on that and there will be an update soon.
Thanks!",hi sorry late response data actively working update soon thanks,issue,positive,negative,negative,negative,negative,negative
674268334,"You are saying that: ""_However, batch normalization is computed by moving average when inference thus I am confused now._""
It looks like you a mixing batch normalization behavior during training and inference. 

Yes, batch normalization computes moving average of the input features, but only during training.
But in the inference mode all weights of batch normalization layer are frozen. So, during inference batch normalization can be represented as a simple 1x1 convolution as described in [link](https://tehnokv.com/posts/fusing-batchnorm-and-conv/), 1x1 convolution in this case is just elementwise multiplication with addition of bias and it is streamable by default, also can be folded/fused with predecessing conv layer.

Please find attached example colab with conv and batch norm streaming (it is an implementation of the model on your drawing): 
[conv_batch_norm.zip](https://github.com/google-research/google-research/files/5077058/conv_batch_norm.zip)
. It demonstrates how to train a model a non streaming model, run inference in non streaming mode with TF/TFLite and run inference in streaming mode with TF/TFLite (data sets in this example are artificially generated and used only to validate model correctness).
In this colab you will see that during training conv layer is producing output with dim: [16/_batch_, 47/_time_, 18/_feature1_, 5/_feature2_] and batch norm layer is processing  [16/_batch_, 47/_time_, 18/_feature1_, 5/_feature2_] (it applies exponential averaging only during training).
During streaming inference _batch_ and and _time_ dims are equal to one, so conv layer produces output [1/_batch_, 1/_time_, 18/_feature1_, 5/_feature2_] and batch norm layer is processing  [1/_batch_, 1/_time_, 18/_feature1_, 5/_feature2_]. The weights of batch norm layer are frozen during inference and applied only on feature dimensions (so there is no exponential averaging during inference).

Thank you for looking into kws_streaming lib! and let me know if you have more questions.
/Oleg.",saying batch normalization moving average inference thus confused like batch normalization behavior training inference yes batch normalization moving average input training inference mode batch normalization layer frozen inference batch normalization simple convolution link convolution case multiplication addition bias default also layer please find attached example batch norm streaming implementation model drawing train model non streaming model run inference non streaming mode run inference streaming mode data example artificially used validate model correctness see training layer output dim batch norm layer exponential training streaming inference equal one layer output batch norm layer batch norm layer frozen inference applied feature exponential inference thank looking let know,issue,positive,negative,negative,negative,negative,negative
674050522,"Hi, @rybakov  Thanks for your reply! I am trying to understand it theoretically so the usage issue may not be my concern😄.  From my point of view, what you are saying is batch normalization only affects the latest column of feature map.(the gray area in the [paper](https://arxiv.org/pdf/2005.06720.pdf)) It is applied on feature domain so it updates the feature map column by column. However, batch normalization is computed by moving average when inference thus I am confused now. Would you mind giving an example based on this below pic? 

let's say blue is column0 and gray is column6 which is the latest input frame, and the model is

> input
> conv2D
> BN
> ---------------------below will compute entire feature map regardlessly
> Flatten
> Dense
> Softmax

From what I know, Conv2D only computes column4,5,6 and the new feature map before BN is column1 ~ column6 now. Then which column and row will be changed by BN and how does that BN compute average and variance.
![image](https://user-images.githubusercontent.com/59950011/90251697-c6b9f300-de70-11ea-8ec3-9edb0ccd6538.png)
",hi thanks reply trying understand theoretically usage issue may concern point view saying batch normalization latest column feature map gray area paper applied feature domain feature map column column however batch normalization moving average inference thus confused would mind giving example based pic let say blue column gray column latest input frame model input compute entire feature map regardlessly flatten dense know column new feature map column column column row compute average variance image,issue,negative,positive,neutral,neutral,positive,positive
673885755,"Batch normalization is applied on feature domain, so it is streamable by default. We apply streaming conversion only on layers, which do data processing in time domain (such as conv, rnn,...). For example during conversion of conv layer to streaming mode we insert time buffer before conv layer, so it can run convolution incrementally on new input samples. All other layers which do processing in feature domain only are kept as it is.

If you do not need to do folding of conv layer with batch normalization then it should work out of the box. But if you need to fold conv layer with batch norm, then there will be several steps: 1) convert non streaming model to streaming one, 2) fold conv and batch norm layers of streaming model. For second step I would suggest to use keras tool: [model_transformer](https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/graph_transformations/model_transformer_test.py). Please let me know if you have more questions, feel free to create an example, so it will be clear what kind of problem you have with kws_streaming lib.",batch normalization applied feature domain default apply streaming conversion data time domain example conversion layer streaming mode insert time buffer layer run convolution new input feature domain kept need folding layer batch normalization work box need fold layer batch norm several convert non streaming model streaming one fold batch norm streaming model second step would suggest use tool please let know feel free create example clear kind problem,issue,positive,positive,positive,positive,positive,positive
673643636,"Hello. I am facing the same problem. There are some missing files in the repo.
Has anyone managed to run the examples?",hello facing problem missing anyone run,issue,negative,negative,negative,negative,negative,negative
672705320,"Hi @harrydrippin , for me it wasn't generating the `.bazelrc` so for the moment I hardcoded ""libtensorflow_framework.2.dylib"" as the return value in `generate_shared_lib_name`. Let's hope they answer to this soon.

Thanks for the hint, it helped a lot!",hi generating moment return value let hope answer soon thanks hint lot,issue,positive,positive,positive,positive,positive,positive
672598519,"> @studennis911988 Hello, I'm also interested in real depth value, do you how to get the real depth value from the output depth image?

Did you have any luck finding answer to this?",hello also interested real depth value get real depth value output depth image luck finding answer,issue,positive,positive,positive,positive,positive,positive
671917986,"Hi @theUnkownName,

You can bypass this problem temporarily by modifying `.bazelrc` directly like below:

```diff
- build --action_env TF_SHARED_LIBRARY_NAME=""ensorflow_framework.2""
+ build --action_env TF_SHARED_LIBRARY_NAME=""libtensorflow_framework.2.dylib""
```

I have a simple fix for this issue but not sent a PR, just waiting Googlers for the guideline.",hi bypass problem temporarily directly like build build simple fix issue sent waiting guideline,issue,negative,positive,neutral,neutral,positive,positive
671828021,Did you manage to solve the problem? I'm having the exact same issue,manage solve problem exact issue,issue,negative,positive,positive,positive,positive,positive
671668692,"OK. Got it. I'll try it.
Thank you for your response.",got try thank response,issue,negative,neutral,neutral,neutral,neutral,neutral
671603318,"Hi hzxie,

We had used TPUs to train with batch size 32. We recommend training with batch size 1 on GPU. Although it takes longer to train from scratch (approximately 2.5 days on a V100), we found it converges to the same results as batch size 32 on TPUs.

Thanks.
",hi used train batch size recommend training batch size although longer train scratch approximately day found batch size thanks,issue,positive,negative,neutral,neutral,negative,negative
671091638,"After training on custom data, I'm getting different depth when training and when doing inference (on the same images).
When training, the depth is smooth and when inferencing it's very rough and inaccurate.
I'm using the inference code from struct2depth inference.py.
Has anyone experienced something like this?

**update**
I just saw that I had messages like
`util.py:198] Did not find var depth_prediction/conv2_2/bn_1/moving_mean in checkpoint
`
```
The following variables in the checkpoint were not loaded:
util.py:210] MotionFieldNet/compute_loss/MotionFieldNet_2/Conv1/Relu/MotionBottleneck/weights
```
**update**
I changed 
`vars_to_restore = util.get_vars_to_save_and_restore(model_ckpt)`
to:
```
  vars_to_restore = [v for v in tf.trainable_variables()]
```
And the error\warning is gone, but the problem still exists

**update**
Using Batch Normalization or Randomized Layer Normalization in ""train"" mode during inference yields results like during training.
I don't understand why do you add the Gaussian noise only when is_train=True. 
In the code you ramp-up the stddev of the noise for the Randomized Layer Normalization, but you didn't mention anything about it in the paper. 

Also, during inference you're inferring on a flipped image, and taking the minimum with the no-flipped image. 
This should be used only if trained with flip_mode != none",training custom data getting different depth training inference training depth smooth rough inaccurate inference code anyone experienced something like update saw like find following loaded update gone problem still update batch normalization layer normalization train mode inference like training understand add noise code noise layer normalization mention anything paper also inference image taking minimum image used trained none,issue,positive,positive,positive,positive,positive,positive
670451848,"The issue is resolved. 
It turns out, searcher expects lots of training(dataset) examples. 
In my case I had only 50 in dataset var.",issue resolved turn searcher lot training case,issue,negative,neutral,neutral,neutral,neutral,neutral
669386641,"Hi, I was also looking for the metadata.json and the file formats specified in the given code. Was anyone able to run the gcn for amazon-2m",hi also looking file given code anyone able run,issue,negative,positive,positive,positive,positive,positive
669132290,"Hi, 
Thanks for the quick response.
Can you please explain where does the term `min_distortion = -4.0 / 27.0/ maxrr` come from?",hi thanks quick response please explain term come,issue,positive,positive,positive,positive,positive,positive
668857601,"Thanks for pointing this out. This seems to be a bug on our side, then. I will look into it, but it may take some time till I can get to it and debug. Sorry about that :-)",thanks pointing bug side look may take time till get sorry,issue,negative,negative,negative,negative,negative,negative
668856249,"@gariel-google @adizhol  I am facing the same issue, i.e., the checkpoints of "" cityscape and kitti"" works well with the model.py.
But the euro one complains.
```
sess = tf.Session()
saver = tf.train.Saver()
saver.restore(sess, 'euroc_ckpt_ViconRoom1-01/model-2091000')
```

```
NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key MotionFieldNet/CameraIntrinsics/foci/biases not found in checkpoint
	 [[node save_3/RestoreV2 (defined at <ipython-input-13-a5df47d040a7>:2) ]]
```
Are there any updates about this?

Thanks

",facing issue cityscape work well one sess saver sess see likely due variable name graph key missing please ensure graph based original error key found node defined thanks,issue,positive,positive,neutral,neutral,positive,positive
668810493,"Hi @adizhol 

You're right, the code doesn't have that yet. 

We followed two different approaches: 
1. Having the distortion predicted by the network. In this case, we only predicted quadratic distortion and ignored quartic distortion. You have to make sure that the distortion curve is monotonically increasing, and if we let the network predict arbitrary distortions, that won't be the case and the training will crash. We way we did it is:

```python
      maxx = tf.maximum(intrinsic_mat[:, 0, 2], tf.to_float(image_width) - intrinsic_mat[:, 0, 2])
      maxy = tf.maximum(intrinsic_mat[:, 1, 2], tf.to_float(image_height) - intrinsic_mat[:, 1, 2])
      maxx /= intrinsic_mat[:, 0, 0]
      maxy /= intrinsic_mat[:, 1, 1]
      maxrr = maxx * maxx + maxy * maxy
      min_distortion = -4.0 / 27.0/ maxrr

      min_distortion = tf.expand_dims(min_distortion, -1)
      min_distortion = tf.expand_dims(min_distortion, -1)

      distortion_coeff = min_distortion + tf.squeeze(layers.conv2d(
          bottleneck,
          1, [1, 1],
          stride=1,
          activation_fn=tf.nn.softplus,
          weights_regularizer=None,
          scope='distortion'), axis=(3))
```
Now the distortion curve is monotonically increasing through the entire image.

2. You can have the intrinsics, including distortion, predicted directly by variables rather than by a network, if we know that the entire dataset was captured with the same camera (such as in EuRoC). Then you just initialize the distortions to zero, and they converge to the right values over time

Hope that helps...",hi right code yet two different distortion network case quadratic distortion quartic distortion make sure distortion curve monotonically increasing let network predict arbitrary wo case training crash way python bottleneck distortion curve monotonically increasing entire image distortion directly rather network know entire camera initialize zero converge right time hope,issue,negative,positive,positive,positive,positive,positive
668128417,"No modifications.
The cityscape and kitti snapshots load normally.

Thank you

On Mon, Aug 3, 2020, 19:46 Ariel Gordon <notifications@github.com> wrote:

> Did you run the code as is or with modifications? I am asking because a few
> months ago, when I uploaded the checkpoint, I did verify that it loads, so
> I am trying to track down the reason for the change in behavior.
>
> On Mon, Aug 3, 2020 at 1:29 AM adizhol <notifications@github.com> wrote:
>
> > Hi,
> >
> > I'm getting an error when loading EuroC MAV checkpoint for training.
> > [depth_from_video_in_the_wild_euroc_ckpt_MachineHallAll]
> >
> > Key
> >
> MotionFieldNet/compute_loss/MotionFieldNet_2/Conv1/Relu/MotionBottleneck/weights
> > not found in checkpoint
> > [[node save/RestoreV2 (defined at
> > /depth_from_video_in_the_wild/model.py:117) ]]
> >
> > @gariel-google <https://github.com/gariel-google>
> >
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <
> https://github.com/google-research/google-research/issues/46#issuecomment-667886281
> >,
> > or unsubscribe
> > <
> https://github.com/notifications/unsubscribe-auth/ADXKUNEGVWFILC3KXWQRVF3R6ZYOZANCNFSM4IMPQMMA
> >
> > .
> >
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46#issuecomment-668125620>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADD2556GIDE6PEKMOL3MX2LR63SVBANCNFSM4IMPQMMA>
> .
>
",cityscape load normally thank mon ariel wrote run code ago verify trying track reason change behavior mon wrote hi getting error loading training key found node defined reply directly view reply directly view,issue,negative,positive,neutral,neutral,positive,positive
668125620,"Did you run the code as is or with modifications? I am asking because a few
months ago, when I uploaded the checkpoint, I did verify that it loads, so
I am trying to track down the reason for the change in behavior.

On Mon, Aug 3, 2020 at 1:29 AM adizhol <notifications@github.com> wrote:

> Hi,
>
> I'm getting an error when loading EuroC MAV checkpoint for training.
> [depth_from_video_in_the_wild_euroc_ckpt_MachineHallAll]
>
> Key
> MotionFieldNet/compute_loss/MotionFieldNet_2/Conv1/Relu/MotionBottleneck/weights
> not found in checkpoint
> [[node save/RestoreV2 (defined at
> /depth_from_video_in_the_wild/model.py:117) ]]
>
> @gariel-google <https://github.com/gariel-google>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46#issuecomment-667886281>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNEGVWFILC3KXWQRVF3R6ZYOZANCNFSM4IMPQMMA>
> .
>
",run code ago verify trying track reason change behavior mon wrote hi getting error loading training key found node defined reply directly view,issue,negative,positive,neutral,neutral,positive,positive
668099554,"@poornimajd I tried to use ONNX, but that didn't work.
Maybe this will work for you: https://github.com/dalgu90/resnet-18-tensorflow",tried use work maybe work,issue,negative,neutral,neutral,neutral,neutral,neutral
667886281,"Hi,

I'm getting an error when loading EuroC MAV checkpoint for training. [depth_from_video_in_the_wild_euroc_ckpt_MachineHallAll]

```
  saver = train_model.saver
  with sv.managed_session(config=config) as sess:
      saver.restore(sess, 'depth_from_video_in_the_wild_euroc_ckpt_MachineHallAll/model-1797000')
```

When using the same code as above with checkpoint saved after training from scratch - no errors.

Key MotionFieldNet/compute_loss/MotionFieldNet_2/Conv1/Relu/MotionBottleneck/weights not found in checkpoint
	 [[node save/RestoreV2 (defined at /depth_from_video_in_the_wild/model.py:117) ]]

@gariel-google 
",hi getting error loading training saver sess sess code saved training scratch key found node defined,issue,negative,neutral,neutral,neutral,neutral,neutral
667671824,"@gunshi see equation 12 in https://arxiv.org/pdf/1801.04062.pdf

In this case the softmax part corresponds to analytically computed gradients of log sum exp.",see equation case part analytically log sum,issue,negative,neutral,neutral,neutral,neutral,neutral
666599716,We found the pooler head is harmful to GLUE tasks sometime. You may try to do not initialize from the pooler dense or just do not have that inside the finetuning model.,found pooler head harmful glue sometime may try initialize pooler dense inside model,issue,negative,neutral,neutral,neutral,neutral,neutral
665684800,"&nbsp;@googlebot I signed it!




------------------&nbsp;原始邮件&nbsp;------------------
发件人: ""googlebot""<notifications@github.com&gt;; 
发送时间: 2020年7月29日(星期三) 晚上10:02
收件人: ""google-research/google-research""<google-research@noreply.github.com&gt;; 
抄送: ""Wendi Li""<wendili.academic@qq.com&gt;; ""Author""<author@noreply.github.com&gt;; 
主题: Re: [google-research/google-research] Update tft_model.py (#337)





 
Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).
 
 Please visit https://cla.developers.google.com/ to sign.
 
Once you've signed (or fixed any issues), please reply here with @googlebot I signed it! and we'll verify it.
 
 
What to do if you already signed the CLA
 
Individual signers
  
It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.
  
Corporate signers
  
Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).
 
The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.
 
The email used to register you as an authorized contributor must also be attached to your GitHub account.
  
️ Googlers: Go here for more info.
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.",li author author update thanks pull request like may first contribution open source project look help look pull request need sign contributor license agreement please visit sign fixed please reply verify already individual possible different address commit check data verify set git corporate company point contact authorized participate ask added group authorized know point contact direct project maintainer public version used register authorized contributor must used git commit check data verify set git used register authorized contributor must also attached account go thread reply directly view,issue,positive,positive,neutral,neutral,positive,positive
664758359,"Hi, i am running in the same file and trying to generate Rico SCA tfrecord. But the generated tfrecord files are empty. did you slove your problem annd generate the tfrecord data?",hi running file trying generate empty problem generate data,issue,negative,negative,neutral,neutral,negative,negative
664149865,"Hello,which version is your TensorFlow? I keep getting an error when I load the checkpoint.",hello version keep getting error load,issue,negative,neutral,neutral,neutral,neutral,neutral
662599254,"I modified the run_squad.py script to remove the contrib module in order to export a quantized tflite model file in tf 2.4.0-dev20200712 version.
I used two saved_model; one is provided (https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/mobilebert_squad_savedmodels.tar.gz), the other one is trained as guided.
I got the same error during convert.

>loc(""bert/encoder/layer_2/attention/self/clip_by_value/Minimum""): error: 'tfl.minimum' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.32020080089569092:-67> vs. !quant.uniform<i8:f32, 0.18242761492729187:-77>
Traceback (most recent call last):
  File ""export_tflite.py"", line 642, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""export_tflite.py"", line 632, in main
    tflite_model = converter.convert()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 1972, in convert
    return super(TFLiteConverter, self).convert()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 1341, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py"", line 444, in _calibrate_quantize_model
    return _mlir_quantize(calibrated)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py"", line 147, in mlir_quantize
    inference_type)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/wrap_toco.py"", line 52, in wrapped_experimental_mlir_quantize
    inference_type)
RuntimeError: Failed to quantize: <unknown>:0: error: loc(""bert/encoder/layer_2/attention/self/clip_by_value/Minimum""): 'tfl.minimum' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.32020080089569092:-67> vs. !quant.uniform<i8:f32, 0.18242761492729187:-77>
<unknown>:0: note: loc(""bert/encoder/layer_2/attention/self/clip_by_value/Minimum""): see current operation: %3377 = ""tfl.minimum""(%3373, %36) : (tensor<1x4x384x384x!quant.uniform<i8:f32, 0.32020080089569092:-67>>, tensor<!quant.uniform<i8:f32, 0.18242761492729187:-77>>) -> tensor<1x4x384x384x!quant.uniform<i8:f32, 0.32020080089569092:-67>>
",script remove module order export model file version used two one provided one trained got error convert error quantization violate scale constraint recent call last file line module file line run file line run main file line main file line main file line convert return super self file line convert result result file line return file line file line quantize unknown error quantization violate scale constraint unknown note see current operation tensor tensor tensor,issue,negative,positive,neutral,neutral,positive,positive
662485549,"Hi @saberkun,
I tried to run a command ""python run_squad.py ..."" in tf 2.4.0-dev20200712 version with tf.compat.v1.disable_v2_behavior() in main and also in modeling.py. 
The error is as follows.

> WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Traceback (most recent call last):
  File ""run_squad.py"", line 32, in <module>
    from mobilebert import modeling
  File ""/home/google-research/mobilebert/modeling.py"", line 32, in <module>
    from tensorflow.contrib import layers as contrib_layers
ModuleNotFoundError: No module named 'tensorflow.contrib'

Do I remove code that uses the contrib package? (contrib_layers and quantize method only)

Also, you provide saved model files for float and quantized type so I tested quantized saved model file with toco in tf 1.15 but I got an error message during conversion.

> 2020-07-22 13:05:32.200417: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Fatal Python error: Aborted
Current thread 0x00007f06cdae8740 (most recent call first):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250 in _run_main
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299 in run
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""/usr/local/bin/toco_from_protos"", line 8 in <module>
Aborted (core dumped)

Regards,
Dongjin.",hi tried run command python version main also error warning removed future version long term recent call last file line module import modeling file line module import module remove code package quantize method also provide saved model float type tested saved model file toco got error message conversion check graph operator type cast form yet sorry welcome relatively fun patch write mostly providing actual arithmetic code fatal python error aborted current thread recent call first file line execute file line file line run file line run file line main file line module aborted core,issue,negative,positive,positive,positive,positive,positive
661746548,"DeepMind is so stupid.
Risk=0 , it can't literally do anything in the world, ML basically doesn't work except for extremely narrow tasks.
Benefits=0 , especially for the DeepPockets of DeepMind (phun intended).
""AI"" is frustrating and depressing, I can't take it anymore.

Funny that they overloaded the term ""SSA"" too, as if Schmidhuber wouldn't be happy about it. Why not call everything the same way, someone will figure out what is meant in which instance, eventually. Maybe we need a stupid chatbot to figure out what was meant in a paper.",stupid ca literally anything world basically work except extremely narrow especially intended ai depressing ca take funny term would happy call everything way someone figure meant instance eventually maybe need stupid figure meant paper,issue,negative,negative,negative,negative,negative,negative
661689874,I am writing a thesis about chatbots and I would like to test Meena. Will it be released soon? ,writing thesis would like test soon,issue,negative,neutral,neutral,neutral,neutral,neutral
660780089,"I think we already removed all tf.contrib usage. The code could probably run with TF1 compatible mode with tf.comat.v1.disable_v2_behavior. https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_v2_behavior
We just did not get a change to test in open source.

@nadongguri Would you try tf 2.x and adding tf.comat.v1.disable_v2_behavior() in main()?",think already removed usage code could probably run compatible mode get change test open source would try main,issue,negative,positive,neutral,neutral,positive,positive
660778270,"I think the real issue is the model trained in 1.x world but the quantization needs 2.x. (it's easier for us to do internally)

@saberkun we probably need to migrate mobilebert to 2.x asap.

wdyt?",think real issue model trained world quantization need easier u internally probably need migrate,issue,negative,positive,neutral,neutral,positive,positive
660659184,"I am seeing this and other issues on trying to run the `non_semantic_speech_benchmark` : it seems that one has not been upgraded to recent TF versions , I am creating a new issue.
",seeing trying run one recent new issue,issue,negative,positive,neutral,neutral,positive,positive
660221658,"I didn't look into svdf too much, I apologize. Thanks for your reply. This would help me a lot. :) ",look much apologize thanks reply would help lot,issue,positive,positive,positive,positive,positive,positive
660206224,"[depthwiseconv1d.py](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/depthwiseconv1d.py) is a core component of [svdf layer](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/svdf.py). [Svdf layer](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/svdf.py) is a composition of 1d depthwise convolution and 1x1 convolution (fully connected layer). [Svdf layer](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/svdf.py) is used by two models: [svdf](https://github.com/google-research/google-research/blob/master/kws_streaming/models/svdf.py) and [svdf_resnet](https://github.com/google-research/google-research/blob/master/kws_streaming/models/svdf_resnet.py). [svdf_resnet](https://github.com/google-research/google-research/blob/master/kws_streaming/models/svdf_resnet.py) is similar with DSC+Resnet and MatchboxNet models.
Please feel free to compose your own model and use [depthwiseconv1d.py](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/depthwiseconv1d.py) directly, so it will be easier to experiment with it, I can help to review it, and let me know if you have any questions.",core component layer layer composition depthwise convolution convolution fully connected layer layer used two similar please feel free compose model use directly easier experiment help review let know,issue,positive,positive,positive,positive,positive,positive
659896977,"Thanks, Clayton! Those are indeed the fixes needed. The problem should be fixed now by [this commit](https://github.com/google-research/google-research/commit/270a6030921ddea727bdccae556ff42f4a0231bb). ",thanks indeed problem fixed commit,issue,negative,positive,positive,positive,positive,positive
659876632,"@liufengdb 

Can you help take a look?

Thanks",help take look thanks,issue,positive,positive,positive,positive,positive,positive
659860515,"Hi @renjie-liu,
thank you for your response.
I've already tried to export using tf-nightly version on docker image. (tensorflow/tensorflow:nightly-gpu)
As far as I know, mobilebert uses tensorflow 1.15 version right? so It's not working.

> Traceback (most recent call last):
  File ""run_squad.py"", line 31, in <module>
    from mobilebert import modeling
  File ""/home/google-research/mobilebert/modeling.py"", line 32, in <module>
    from tensorflow.contrib import layers as contrib_layers
ModuleNotFoundError: No module named 'tensorflow.contrib'

BTW, does it mean ""counting on the new quantizer"" to use tensorflow 2.x converter?

Regards,
Dongjin.",hi thank response already tried export version docker image far know version right working recent call last file line module import modeling file line module import module mean counting new use converter,issue,negative,positive,neutral,neutral,positive,positive
659829846,"That's interesting, can you update to the latest tf-nightly and try again?

We're counting on the new quantizer.

thanks",interesting update latest try counting new thanks,issue,positive,positive,positive,positive,positive,positive
659663486,"Google staff, could you please take a look on my changes?",staff could please take look,issue,negative,neutral,neutral,neutral,neutral,neutral
658513101,"Looks like I was able to hack around this,

I added 

`""@com_google_absl//absl/container:node_hash_map""`

to the BUILD file (not really sure if this makes sense or not, not that familiar with non-windows build systems..).

So that the experiment_util component looks like 

```
cc_library(
    name = ""experiment_util"",
    srcs = [""experiment_util.cc""],
    hdrs = [""experiment_util.h""],
    deps = [
        "":datasets_cc_proto"",
        "":definitions"",
        "":instruction_cc_proto"",
        ""@com_google_absl//absl/algorithm:container"",
	""@com_google_absl//absl/container:node_hash_map"",
        ""@com_google_absl//absl/memory"",
        ""@com_google_protobuf//:protobuf"",
    ],
)
```

This took me to the following error

`
task_util.h:196:34: error: 'StrCat' is not a member of 'absl'; did you mean 'google::protobuf::StrCat'?
`

I hacked around this by adding 

`#include ""absl/strings/str_cat.h""`

to task_util.h.

And now it seems to work (compiles, at least)!

",like able hack around added build file really sure sense familiar build component like name container took following error error member mean hacked around include work least,issue,negative,positive,positive,positive,positive,positive
658462709,"Afraid I'm seeing this exact error as well, on a docker container install I'm attempting:

Linux version 4.15.0-107-generic (buildd@lcy01-amd64-010) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1\~16.04.12)) #108\~16.04.1-Ubuntu SMP Fri Jun 12 02:57:13 UTC 2020

gcc (Ubuntu 9.3.0-10ubuntu2) 9.3.0
g++ (Ubuntu 9.3.0-10ubuntu2) 9.3.0
bazel 3.4.1

I'm guessing this has something to do with https://github.com/abseil/abseil-cpp ?  Looks like the file ./container/node_hash_map.h exists in that repository. In my case, I don't seem to have the external/ directory in bazel-automl_zero  at all.",afraid seeing exact error well docker container install version generic version guessing something like file repository case seem directory,issue,negative,negative,negative,negative,negative,negative
657333673,"When will be the code used in the paper ""Stand-Alone Self-Attention in Vision Models"" be available? The code link in the paper leads to this page which just has a readme updated 9 months ago.",code used paper vision available code link paper page ago,issue,negative,positive,positive,positive,positive,positive
657201979,"> @ikramabdel instead of [this](https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/motion_prediction_net.py#L176) line.

Thank you ! @gariel-google , but without the 'add_intrinsics_head' how can we have the same shape as required (B,3,3) ?

Edit : I tried to make it fit by adding the dimension (not sure if it is the correct way): but a new issue is that the returned intrinsic_mat by the moton_prediction_net seems to be  'InvalidArgumentError (see above for traceback): Input is not invertible.'

Thanks again!",instead line thank without shape edit tried make fit dimension sure correct way new issue returned see input invertible thanks,issue,positive,positive,positive,positive,positive,positive
656984169,"@gariel-google Thank you for your replying. However your released trajectory file only includes XYZ position. I additionally need roll pitch yaw angles to reproduce the values on your paper.

Or can you provide the evaluation file to reproduce the value about ego motion on your paper? That can help me to find differences!!",thank however trajectory file position additionally need roll pitch yaw reproduce paper provide evaluation file reproduce value ego motion paper help find,issue,positive,neutral,neutral,neutral,neutral,neutral
656947146,"Sorry for the delayed response.

@NHirose we released the trajectories [here](https://github.com/google-research/google-research/tree/master/depth_from_video_in_the_wild#pretrained-checkpoints-and-respective-odometry-results) - please see the table below the title, under the links ""trajectory"".

@player1321 We didn't evaluate quantitatively the prediction of residual motion. Qualitatively it looks good in most cases - I know it sounds hand-wavy, but unfortunately there is no number that I can quote to support this quantitatively.",sorry response please see table title link trajectory player evaluate quantitatively prediction residual motion qualitatively good know unfortunately number quote support quantitatively,issue,positive,negative,negative,negative,negative,negative
656946027,@PriyaThakur Unfortunately we were unable to get the approval from our legal department to release these checkpoint. I am sorry.  ,unfortunately unable get approval legal department release sorry,issue,negative,negative,negative,negative,negative,negative
656940288,"@poornimajd AFAIK, the imagenet imagenet checkpoint is from [here] (https://pytorch.org/docs/stable/torchvision/models.html) (resnet18), but honestly I'm not 100% sure. 

Sure, you can initialize from a cityscapes model. The existing code doesn't expose this option as a flag, but an easy hack would be top copy the cityscapes checkpoint into the training directory and then start the training. ",honestly sure sure initialize model code expose option flag easy hack would top copy training directory start training,issue,positive,positive,positive,positive,positive,positive
656770587,Actually 6x sounds right since there are 60 ood and 10 in distribution classes. So the whole data is indeed used in the experiments.,actually right since distribution class whole data indeed used,issue,negative,positive,positive,positive,positive,positive
656693287,"> Hi octpath@, we will do our best to release that code too, but meanwhile, here's a snippet of how to do that. If you know that all your frames come from the same camera (which is true for EuRoC but not for KITTI / cityscapes), you'd do something like this
> 
> ```python
> fx = tf.get_variable('fx', initializer=image_width)
> fy = tf.get_variable('fy', initializer=image_height)
> x0 = tf.get_variable('x0', initializer=image_width * 0.5)
> y0 = tf.get_variable('y0', initializer=image_height * 0.5)
> 
> intrinsics_mat = tf.convert_to_tenosr([[fx, 0.0, x0], [0.0, fy, y0], [0.0, 0.0, 1.0]])
> ```
> 
> If you have multiple videos, assuming that for each frame / triplet you have a key indicating what video they came from, you can use a tensorflow hash table to keep a separate intrinsic matrix per key. Hope that helps!

Hi @gariel-google 

I'm attempting the same thing , but i'd like to know where to input your suggestion exactly ? Thanks ",hi best release code meanwhile snippet know come camera true something like python multiple assuming frame triplet key video came use hash table keep separate intrinsic matrix per key hope hi thing like know input suggestion exactly thanks,issue,positive,positive,positive,positive,positive,positive
652890025,"As @fxtentacle said, it's a bug in CUDA10.0 when using tf.matmul. I did some test and found that this error only happens when the matrix is too big, so we can separate the large matrix to small matrixs and then merge the result.
Here is a solution without using CUDA10.1
```
def tf_matmul_separate(A,B):
    batch,a_h,a_w,_,_ = A.get_shape()
    if batch*a_h*a_w < 80000:
        return tf.matmul(A,B)
    w_half = tf.cast(tf.div(a_w,2),tf.int32)
    h_half = tf.cast(tf.div(a_h,2),tf.int32)
    A_1 = A[:,:h_half,:w_half,:,:]
    A_2 = A[:,:h_half,w_half:,:,:]
    A_3 = A[:,h_half:,:w_half,:,:]
    A_4 = A[:,h_half:,w_half:,:,:]
    
    B_1 = B[:,:h_half,:w_half,:,:]
    B_2 = B[:,:h_half,w_half:,:,:]
    B_3 = B[:,h_half:,:w_half,:,:]
    B_4 = B[:,h_half:,w_half:,:,:]
    
    res_1 = tf_matmul_separate(A_1,B_1)
    res_2 = tf_matmul_separate(A_2,B_2)
    res_3 = tf_matmul_separate(A_3,B_3)
    res_4 = tf_matmul_separate(A_4,B_4)
    tmp_up = tf.concat([res_1,res_2],2)
    tmp_down = tf.concat([res_3,res_4],2)
    final_res = tf.concat([tmp_up,tmp_down],1)
    return final_res
```
using this function replace the tf.matmul function in:
https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/transform_utils.py#L110
https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/transform_utils.py#L111",said bug test found error matrix big separate large matrix small merge result solution without batch batch return return function replace function,issue,negative,negative,neutral,neutral,negative,negative
652397569,"> > @cognitiveRobot thanks for the reply.
> > What kind of error messages did you get?
> 
> I meant to say loss is too much.
Hi !! please tell me how you made it work without input of intrinsics ?? i put  cam.txt = 9 zeros like @pigletsc said but i keep getting error 

Mostly  : **tensorflow.python.framework.errors_impl.InvalidArgumentError: Input is not invertible.
	 [[{{node data_loading/multi_scale_intrinsics/MatrixInverse}}]]**


thanks !
",thanks reply kind error get meant say loss much hi please tell made work without input put like said keep getting error mostly input invertible node thanks,issue,negative,positive,positive,positive,positive,positive
652262191,"Hi there!
I don't know what the people from the Bart paper are reporting but as to low, mid and high, you can see that they are just percentiles:
https://github.com/google-research/google-research/blob/478efa18a3a1fef144fac03fcd034829f348794e/rouge/scoring.py#L127
```
      intervals = tuple(
          (scores[0].__class__(*percentiles[j, :]) for j in range(3)))
      result[score_type] = AggregateScore(
          low=intervals[0], mid=intervals[1], high=intervals[2])
```
```
Mid is always the mean, while low and high
bounds are specified by self._confidence_interval (which defaults to 0.95
meaning it will return the 2.5th and 97.5th percentiles for a 95%
confidence interval on the mean).
```

so basically in `q = 100 * np.array([percentile_delta, 0.5, 1 - percentile_delta])`
* low is the percentile_delta percentile
* mid is the 0.5 percentile
* high is the 1 - percentile_delta percentile

and this is based on all of the scores you have added.

I think that's it from a quick glance but I could be wrong.",hi know people paper low mid high see range result mid always mean low high meaning return confidence interval mean basically low percentile mid percentile high percentile based added think quick glance could wrong,issue,negative,negative,neutral,neutral,negative,negative
652237499,"1 Please update both colab and kws code, so there will be no issues with data_url in colab (setting it to empty string in colab works too).

2 Speech commands data with 10 + 2 categories is a toy data set.
It has: 
training_set_size 22,246
testing_set_size 3,081
validation_set_size 3,093
To be able to classify all other unknown sounds with better accuracy you will need to prepare more negative/""unknown"" examples and use larger data sets.
Also [models2_30k.zip](https://storage.googleapis.com/kws_models/models2_30k.zip) which are used in [02_inference.ipynb](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/02_inference.ipynb) are realy toy models with 30K parameters.
These kind of models can be good for microcontrollers with limited cpu and memory.

3.1 If you do not have limitations on model size, I would suggest to look at [models2](https://storage.googleapis.com/kws_models/models2.zip) published in our [paper](https://arxiv.org/abs/2005.06720) and its quantizerd version [models2_q](https://storage.googleapis.com/kws_models/models2_q.zip). These models are well suited for mobile phones: have more parameters with better accuracy. You can also use more accurate non streaming models and run them every 50ms (for example). Some of non streaming models (with stride and pooling) can be converted to streaming mode (it just not implemented in kws, please feel free to convert them and contribute)
3.2 When you run your model in streaming mode you should analyze not only predicted word, but its confidence level too
In the this collab example: [02_inference.zip](https://github.com/google-research/google-research/files/4856123/02_inference.zip), I showed both predicted word and confidence level of predicted word in streaming mode. You can see that with more data in time, confidence level grows and it has highest value in the end of the word. So in addition to checking what is the current word you should also compare its confidence level with a threshold (and check that it is a local maximum). Also detection events should be smoothed: for example trigger word detection if it was detected 5 times in the last 100ms of audio.
I would also suggest to compute more metrics for example streaming false alarm and false reject per hour (in our paper we use accuracy metric only for academic purpose and comparison with other previously published work).
3.3 There can be false positives on short words such as ""on"". For example ""on"" can be detected in words ""common"", ""lemon"", etc
3.3 Use more data

",please update code setting empty string work speech data toy data set able unknown better accuracy need prepare unknown use data also used toy kind good limited memory model size would suggest look paper version well mobile better accuracy also use accurate non streaming run every example non streaming stride converted streaming mode please feel free convert contribute run model streaming mode analyze word confidence level example word confidence level word streaming mode see data time confidence level highest value end word addition current word also compare confidence level threshold check local maximum also detection example trigger word detection time last audio would also suggest compute metric example streaming false alarm false reject per hour paper use accuracy metric academic purpose comparison previously work false short example common lemon use data,issue,positive,positive,neutral,neutral,positive,positive
652161052,"Thank you for updating the files.

The issues are following:
1. In 01_train, ""FLAGS.data_url"" should be set to empty string to avoid error.
2. I printed ""index_to_label[stream_output_arg]"" in streaming mode with external state. And I found that ""left"" appears in the beginning of inference whatever audio files were used. Why it would not be ""unknown""?
= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
The ouput example of infering ""/kws_streaming/data2/on/fb7c9b3b_nohash_0.wav""
![on_example](https://user-images.githubusercontent.com/44760575/86198151-87b63380-bb89-11ea-8336-a24477109766.png)
= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
3. I am interested in detecting multiple keywords in streaming mode. Do you have any advice?  

Thanks again!",thank following set empty string avoid error printed streaming mode external state found left beginning inference whatever audio used would unknown example interested multiple streaming mode advice thanks,issue,negative,positive,neutral,neutral,positive,positive
652016146,"I have updated colabs, please run them one by one:

1. [00_check_data](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/00_check_data.ipynb) it shows index_to_label and will prepare data set v2

2. [01_train](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/01_train.ipynb) it will train a model (with small number of iterations, so accuracy will not be high) this step is optional.

3. [02_inference](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/02_inference.ipynb) it will run inference on pre-trained models (I added a remapping from id to word)

Please let me know if you have any questions or issues.",please run one one prepare data set train model small number accuracy high step optional run inference added id word please let know,issue,positive,negative,neutral,neutral,negative,negative
651496571,"Thank you for evaluating kws_streaming!

_""1.When I run inference.ipynb, the error occurs.""_
Thank you for reporting this issue. ipynb is outdated. The fix will be in couple of days.

_""2.How can I know the predicted label after inference?""_
Please have a look at [check-data](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/check-data.ipynb). It has an example of mapping index_to_label:
{1: '_unknown_',
 5: 'down',
 11: 'go',
 3: 'no',
 7: 'right',
 8: 'on',
 6: 'left',
 2: 'yes',
 9: 'off',
 4: 'up',
 10: 'stop',
 0: '_silence_'}

unknown=1 and silence=0 always.
I will update notebooks with more details and then post another comment here.
",thank run error thank issue outdated fix couple day know label inference please look example always update post another comment,issue,positive,negative,negative,negative,negative,negative
651209898,Did you run the code successfully? did you use tf1.14 and cuda10?   I look forward to hearing from you very much.,run code successfully use look forward hearing much,issue,negative,positive,positive,positive,positive,positive
651045594,They concatenate the input features of the GCN layer and features obtained after aggregation,concatenate input layer aggregation,issue,negative,neutral,neutral,neutral,neutral,neutral
651044214,I have a similar question: Why the parameters used here are different from the parameters in your paper?,similar question used different paper,issue,negative,neutral,neutral,neutral,neutral,neutral
650794939,"Per suggestion from similar [issue](https://github.com/tensorflow/tensorflow/issues/37756)
please try ""pip list"" and uninstall tf v1 related packages, then install tf v2
If it does not work for you please create issue at [tensorflow](https://github.com/tensorflow/tensorflow)",per suggestion similar issue please try pip list related install work please create issue,issue,positive,neutral,neutral,neutral,neutral,neutral
650566589,"Hi @CalogeroZarbo , I too share a great interest in this model and I can corroborate your findings. I've been using it to forecast multivariate timeseries 5 time-steps in the future using 252 time-steps into the past as look-back (this is the _num_encoder_steps_) , much like their volatility dataset example and I got stuck into how to actually use it in production with out-of-sample data to forecast into the future. Bryan helped me on this regard. He said ""_The TFT does filter out any observed inputs and targets in the forecast horizon by default, so you can debug by ensuring that any value used does not modify the predictions._"" and what I did to verify this was to have a date on my test dataset set as the ""present day"" and, per identifier, have ""present day"" - _num_encoder_steps_ (252 in my case) as the past and ""present day"" + 5 as my ""future"" horizon. Note that it has to be done **per identifier.**
Next, I've set to ZERO all my observed inputs and the targets where datetime > ""present day"" (i.e.) info that we couldn't have known in advance. Note that known and static inputs were kept unchanged.

The TFT outuput (model.predict(test, return_targets=True)) when compared with the one produced with the unchanged test dataset was pretty much the same! That confirms what he and you said.

What still bothers me is the fact that the TFT predict() absolutely needs the total_time_steps (num_encoder_steps + horizon).  That doesn't make much sense to me since one are interested in predicting only the horizon, why would one wants to predict data that is in the past?!? But I guess the LSTMs are inherently built this way... I've found some solace when I took a closer look into the function that computes the normalised quantile loss, called numpy_normalised_quantile_loss(). I've noticed that the quantile_loss is roughly the same for all the time-steps (t+0, t+1... t+n). Additionally, I've checked the quantile loss per row and it kept roughly the same along the results. Amazing! My intuition was that as the days go by, the loss would be greater, but it doesn't seem to be the case and I can't say why... 

Hope that helps.",hi share great interest model corroborate forecast future past much like volatility example got stuck actually use production data forecast future regard said filter forecast horizon default value used modify verify date test set present day per identifier present day case past present day future horizon note done per identifier next set zero present day could known advance note known static kept unchanged test one produced unchanged test pretty much said still fact predict absolutely need horizon make much sense since one interested horizon would one predict data past guess inherently built way found solace took closer look function loss roughly additionally checked loss per row kept roughly along amazing intuition day go loss would greater seem case ca say hope,issue,positive,positive,positive,positive,positive,positive
650474280,"> The source code is on it’s way

Is it released somewhere here?",source code way somewhere,issue,negative,neutral,neutral,neutral,neutral,neutral
648923526,Quick note: mfcc_tf can be slower than mfcc_op because one uses DFT and another FFT. Also mfcc_op is preferable in case if you plan to quantize your model.,quick note one another also preferable case plan quantize model,issue,negative,positive,positive,positive,positive,positive
648884809,"Comments about conversion steps:
1 TensoFlow(TF) model can be converted to TF streaming model with external or internal state.
2 TF streaming models with external state can be converted to TFLite and run on mobile devices (benchmarks in streaming and non streaming modes were reported in our [paper](https://arxiv.org/abs/2005.06720)).
3 TF streaming models with internal state can not be converted to TFLite now - this work is still in progress.

Comments about speech feature extractor:
4 mfcc_tf should work everywhere. It is implemented based on DFT and uses only basic ops such as matrix matrix multiplication.
5 mfcc_op is implemented based on FFT. It is a special op in TFLite. This feature extractor can be part of the model (set by preprocess 'raw'; feature_type 'mfcc_op') or it can be outside of the model (set by preprocess 'mfcc';). In both cases it is the same TFLite op and if it works for one case it will work for another too. I never tested it on CoreML/iOS. Please feel free to try it and let me know if you have any problems. If it does not work we can create an issue for TFLite team.
",conversion model converted streaming model external internal state streaming external state converted run mobile streaming non streaming paper streaming internal state converted work still progress speech feature extractor work everywhere based basic matrix matrix multiplication based special feature extractor part model set outside model set work one case work another never tested please feel free try let know work create issue team,issue,positive,positive,neutral,neutral,positive,positive
648438357,"Great work @gariel-google and team! 
As mentioned below,I need to convert the torch model to tensorflow.



> Assuming you converted the torch model to tensorflow model and are having the following files for imagenet checkpoint:
> 
> init/
> checkpoint
> model.ckpt.data-00000-of-00001
> model.ckpt.index
> model.ckpt.meta
> 
> in this case you have to specify --imagenet_ckpt=.../init/model.ckpt
> So not even an existing filename (and not the .index file, or the init dir)
> That works for me

In regard to this , I am not sure as to where exactly is the torch model which is to be used as imagenet pretrained weights(after conversion.)
I also wanted to know if it is possible to use the checkpoints trained on Cityscapes(as provided in the repo) as the pretrained weights in place of the imagenet checkpoint?
Any suggestion is greatly appreciated!
Thank you
",great work team need convert torch model assuming converted torch model model following case specify even file work regard sure exactly torch model used conversion also know possible use trained provided place suggestion greatly thank,issue,positive,positive,positive,positive,positive,positive
647422033,"Hi @cometta, I extensively worked on the TFT code recently, and I tested all sorts of biases that came up in my mind. In my experiment I have a history of 1 sample that predicts the next 4 time steps in the future, having in total 5 timesteps. To see whether or not it used information from the future I obscured in prediction the 4future time steps feeding vector with all 0s. I had the same results as the normal evaluation code. Amazing.

You have to do this in the function _batch_data() of the tft_model.py. There the model is gathering the samples from the dataset grouping them by the ID and ordering them by the TIME column. At some point, it calls the function to _batch_single_entity(), if that particular time series has enough time point it will return them otherwise will return None.
If you substitute the in the output vector from _batch_single_entity() function the original values with all 0 from the Encoder Step index till the end, you are feeding 0s to the model for the future. 
Remember to do this only in Evaluation / Prediction otherwise, you will obscuring also training batches since that function is called many times. 

Let me know if you need further details.

Thank you @sjblim for the amazing work.

Cheers,
Cal",hi extensively worked code recently tested came mind experiment history sample next time future total see whether used information future prediction future time feeding vector normal evaluation code amazing function model gathering grouping id time column point function particular time series enough time point return otherwise return none substitute output vector function original step index till end feeding model future remember evaluation prediction otherwise also training since function many time let know need thank amazing work cal,issue,positive,positive,positive,positive,positive,positive
647174433,Thank you! I also updated final files tree with description.,thank also final tree description,issue,negative,neutral,neutral,neutral,neutral,neutral
646881007,"thanks @saberkun can you share two example instructions on CPU, one for finetune with squad, one for exporting to pd file from the previous step? ",thanks share two example one squad one file previous step,issue,positive,positive,neutral,neutral,positive,positive
646839039,"Apologies, we're in the process of updating the models with the new checkpoints supported by the TFLite XNNPACK delegate and I removed the old models (which would not have worked) before we were ready to upload the new ones. This should be fixed shortly!",process new delegate removed old would worked ready new fixed shortly,issue,negative,positive,positive,positive,positive,positive
646788124,"Do you have some more specifics for the hyperparameters for the GLUE tasks? Also it seems like the results for CoLA, MRPC, and RTE vary quite a bit (~3%) between runs. Is there a reason for that and is there a way to get more stable results? ",glue also like cola vary quite bit reason way get stable,issue,positive,neutral,neutral,neutral,neutral,neutral
646761899,"Hi @luochonghai,

Thanks for bringing up the question and detailed explanation. I think your derivation is correct. 

`raw_weight = target - grad_target - init_eps_val := - grad_target. 
`

Previously we were treating 
`raw_weight = raw_weight - init_eps_val` and the following [3 lines](https://github.com/google-research/google-research/blob/master/ieg/models/model.py#L311-L315) as the normalization step as only briefly mentioned in the paper.

The conclusion here is that the implementation is correct and the formula is less precise. Both leads to the same outputs. We will update the paper to reduce confusion.
",hi thanks question detailed explanation think derivation correct target previously treating following normalization step briefly paper conclusion implementation correct formula le precise update paper reduce confusion,issue,negative,positive,positive,positive,positive,positive
646479900,"@gariel-google Thanks for sharing the code and helping us. 
Even though I am using your pretrained model(learned intrinsic), I can not reproduce the results of egomotion. 
I got 0.0259@seq-09  and  0.0210@seq-10, which are much worse than values on your paper.

Can you provide trajectories with poses and/or code to reproduce the same value?
Shared trajecotries only include XYZ positions. If you can add poses, it is very helpful to reproduce the results.",thanks code helping u even though model learned intrinsic reproduce got much worse paper provide code reproduce value include add helpful reproduce,issue,positive,negative,neutral,neutral,negative,negative
646453151,"Usually the good parameters for squad v1.1 is around 5 epochs, learning rate 4e-5, batch size 32.
The search range for squad is small. +-2 for epochs and LR.
But for GLUE, some tasks need to run longer.",usually good squad around learning rate batch size search range squad small glue need run longer,issue,negative,positive,positive,positive,positive,positive
646452565,"Hi, it is because of this line:
https://github.com/google-research/google-research/blob/master/mobilebert/run_squad.py#L652
export should be run as a separated pass from training.",hi line export run pas training,issue,negative,neutral,neutral,neutral,neutral,neutral
646412891,@tgale96 Please take a look. Thank you very much!,please take look thank much,issue,positive,positive,positive,positive,positive,positive
645460085,"I met the same problem and I solved it by slightly modifying the original metis wrapper to support the lastest networkx.
You can check my repo:
https://github.com/james77777778/metis_python",met problem slightly original metis wrapper support check,issue,negative,positive,positive,positive,positive,positive
645102657,"After `pip3 install tf_nightly==2.3.0.dev20200515`, it seems Okay now. It could convert non-stream to stream model automaticly without any errors.

Thanks  for updating requirements.txt. It's great.

The final files tree as follow:
```shell
⇒  tree ./train_model/cnn
./train_model/cnn
├── accuracy_last.txt
├── best_weights.data-00000-of-00002
├── best_weights.data-00001-of-00002
├── best_weights.index
├── flags.json
├── flags.txt
├── graph.pbtxt
├── labels.txt
├── last_weights.data-00000-of-00002
├── last_weights.data-00001-of-00002
├── last_weights.index
├── logs
│   ├── train
│   │   ├── events.out.tfevents.1591857342.ubuntu
│   │   ├── events.out.tfevents.1591857494.ubuntu
│   │   ├── events.out.tfevents.1591870751.RT-AI
│   │   └── events.out.tfevents.1591875621.ubuntu
│   └── validation
│       ├── events.out.tfevents.1591857855.ubuntu
│       └── events.out.tfevents.1591870970.RT-AI
├── model_summary.txt
├── non_stream
│   ├── assets
│   ├── model_summary.txt
│   ├── saved_model.pb
│   └── variables
│       ├── variables.data-00000-of-00002
│       ├── variables.data-00001-of-00002
│       └── variables.index
├── quantize_opt_for_size_tflite_non_stream
│   ├── model_summary.txt
│   ├── non_stream.tflite
│   └── tflite_non_stream_model_accuracy.txt
├── quantize_opt_for_size_tflite_stream_state_external
│   ├── model_summary.txt
│   ├── stream_state_external.tflite
│   ├── tflite_stream_state_external_model_accuracy_reset0.txt
│   └── tflite_stream_state_external_model_accuracy_reset1.txt
├── stream_state_internal
│   ├── assets
│   ├── model_summary.txt
│   ├── saved_model.pb
│   └── variables
│       ├── variables.data-00000-of-00002
│       ├── variables.data-00001-of-00002
│       └── variables.index
├── tf
│   ├── model_summary_non_stream.png
│   ├── model_summary_non_stream.txt
│   ├── model_summary_stream_state_external.png
│   ├── model_summary_stream_state_external.txt
│   ├── model_summary_stream_state_internal.png
│   ├── model_summary_stream_state_internal.txt
│   ├── stream_state_external_model_accuracy_sub_set_reset0.txt
│   ├── stream_state_external_model_accuracy_sub_set_reset1.txt
│   ├── tf_non_stream_model_accuracy.txt
│   ├── tf_non_stream_model_sampling_stream_accuracy.txt
│   └── tf_stream_state_internal_model_accuracy_sub_set.txt
├── tflite_non_stream
│   ├── model_summary.txt
│   ├── non_stream.tflite
│   └── tflite_non_stream_model_accuracy.txt
├── tflite_stream_state_external
│   ├── model_summary.txt
│   ├── stream_state_external.tflite
│   ├── tflite_stream_state_external_model_accuracy_reset0.txt
│   └── tflite_stream_state_external_model_accuracy_reset1.txt
└── train
    ├── 0weights_400.data-00000-of-00002
    ├── 0weights_400.data-00001-of-00002
    ├── 0weights_400.index
    ├── ...
    ├── 9027weights_15000.data-00000-of-00002
    ├── 9027weights_15000.data-00001-of-00002
    ├── 9027weights_15000.index
    └── checkpoint
```",pip install dev could convert stream model without thanks great final tree follow shell tree train validation asset asset train,issue,negative,positive,positive,positive,positive,positive
644904804,"Hi Lebhoryi,

There was a refactor in Keras which impacting ""clone"" api. I updated kws lib with new api and bumped up tf version requirements in the [experiments folder](https://github.com/google-research/google-research/tree/master/kws_streaming/experiments), but did not update requirements.txt. Thank you for reporting it! Fix with updated requirements.txt is submitted. 
Please try ""pip install tf_nightly==2.3.0.dev20200515"" - kws was validated on this version, and let me know if it works for you (tf_nightly should work too).

Step with utils.model_to_saved(model, flags, path_model, mode) is optional, that is why even it fails, only warning is printed.

Best,
Oleg.",hi clone new version folder update thank fix please try pip install dev version let know work work step model mode optional even warning printed best,issue,positive,positive,positive,positive,positive,positive
644667172,"other question is what the version about tensorflow?

there are some missing files in tf 2.1 or 2.2, but have found in `github tf master`

I have found the reason about `WARNING: failed to convert to SavedModel: 'Node' object has no attribute 'outputs'`. In file `~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/node.py`, there is not `output` in `tf 2.1` or `tf 2.2` but in `tf master`. I would replace this file.

But it causes other problem, maybe my question is the tf version doesn't match the KWS project. I would try it",question version missing found master found reason warning convert object attribute file output master would replace file problem maybe question version match project would try,issue,negative,negative,negative,negative,negative,negative
644539942,"Thanks for the quick reply and tips!!
I'm gonna use your snippet : )",thanks quick reply gon na use snippet,issue,negative,positive,positive,positive,positive,positive
644218193,"There is no issue with using future frames for representation learning. It should help.

In some downstream applications, it is not possible to use future  frames (like predicting next action in robotics). To be compatible with both kinds of applications we did not use future frames. 

You can use the following snippet for using information from the future:

```python
steps = tf.range(step - (num_steps - num_steps//2 -1) * stride,
                          step + (num_steps//2 + 1) * stride,
                          stride)
```",issue future representation learning help downstream possible use future like next action compatible use future use following snippet information future python step stride step stride stride,issue,positive,positive,neutral,neutral,positive,positive
642163011,"Hi! 
I ran into the same issue...
Have you been able to create your own vocabulary and candidate words files?",hi ran issue able create vocabulary candidate,issue,negative,positive,positive,positive,positive,positive
641396737,"Hi @gariel-google,
I wanted to know if you got clearance for releasing the pre-trained checkpoints for the model trained on YouTube8M data?",hi know got clearance model trained data,issue,negative,neutral,neutral,neutral,neutral,neutral
640306548,"Looking forward to the code, also if anyone's interested in write the code with me let me know.",looking forward code also anyone interested write code let know,issue,negative,positive,positive,positive,positive,positive
638438676,I'd also be very interested in seeing more of the source notebooks replicating experiments from the paper.,also interested seeing source paper,issue,negative,positive,positive,positive,positive,positive
638275463,"> The source code is on it’s way

how long are we talking about?",source code way long talking,issue,negative,negative,neutral,neutral,negative,negative
638238185,"ye, somehow I missed it, never the less it is like:

> 1) HyperparamOptManager used for optimisation on a single machine/GPU.
> 2) DistributedHyperparamOptManager for multiple GPUs on different machines.

while I need multi gpu on one machine. Anyway, @SachitNayak,  thanks for pointing out.",ye somehow never le like used single multiple different need one machine anyway thanks pointing,issue,negative,positive,neutral,neutral,positive,positive
637855244,"@Ubikas I think there is a multi (distributed) GPU version of the hyperamater tuning code available in the libs/hyperparam_opt.py file. I have only 1 GPU, so I train on that. If you are asking me if I could make the training code to run on multiple GPUs, to be frank, I really don't know how to do that.",think distributed version tuning code available file train could make training code run multiple frank really know,issue,negative,positive,positive,positive,positive,positive
637386865,"@SachitNayak, it's out of topic, but maybe do you know how to make it multi gpu by chance?",topic maybe know make chance,issue,negative,neutral,neutral,neutral,neutral,neutral
637045227,"Hi @liyingliu,

Indeed the translation vector is scaled by a learned factor. The code is [here](https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/motion_prediction_net.py#L171). If you use [motion_field_net](https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/motion_prediction_net.py#L80) directly, and load form the checkpoints, all this should be taken care of for you. 
Hope that helps!",hi indeed translation vector scaled learned factor code use directly load form taken care hope,issue,positive,positive,neutral,neutral,positive,positive
636922075,"Yeah I noticed that Pull Request merge too! only 2 lines seemed to have been changed in that merge.
However, for now I'm using TF 1.14 and things run fine. ",yeah pull request merge merge however run fine,issue,negative,positive,positive,positive,positive,positive
636768179,"Hi, yes you are right. Don't know why, but 10 days ago tensorflow.compat.v1 was changed to tensorflow. You have to change it back in utils.py and tft_model.py",hi yes right know day ago change back,issue,negative,positive,positive,positive,positive,positive
636455113,"Actually now I do.
I don't know what I did wrong last time.
Thanks for your help @debidatta!",actually know wrong last time thanks help,issue,negative,negative,neutral,neutral,negative,negative
634781493,"Thanks for sharing that. Everything seems in order. 

So the batch size from the TF dataset on this [line](https://github.com/google-research/google-research/blob/master/tcc/train.py#L114) should be twice of the batch size in the config. Is that happening? 

If you run `watch -n0.1 nvidia-smi` on another console do you see any activity on the 2nd GPU?",thanks everything order batch size line twice batch size happening run watch another console see activity,issue,negative,positive,positive,positive,positive,positive
634601573,"I have printed the strategy.num_replicas_in_sync and here are the logs up to this, we can also see both GPUs are available.

0527 11:13:30.057750 140542352119552 utils.py:249] Using config from config.py as no config.yml file exists in /tmp/alignment_logs
2020-05-27 11:13:30.159646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-27 11:13:30.218652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: c323:00:00.0
2020-05-27 11:13:30.219569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: e230:00:00.0
2020-05-27 11:13:30.300098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-05-27 11:13:31.829912: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-05-27 11:13:32.861495: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-05-27 11:13:33.216085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-05-27 11:13:34.537053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-05-27 11:13:35.865859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-05-27 11:13:37.633048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-27 11:13:37.636800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2020-05-27 11:13:37.644554: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-27 11:13:37.893260: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593990000 Hz
2020-05-27 11:13:38.010210: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561619937500 executing computations on platform Host. Devices:
2020-05-27 11:13:38.010258: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2020-05-27 11:13:38.189159: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56161996ab20 executing computations on platform CUDA. Devices:
2020-05-27 11:13:38.189196: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-05-27 11:13:38.189212: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-05-27 11:13:38.190366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: c323:00:00.0
2020-05-27 11:13:38.191252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: e230:00:00.0
2020-05-27 11:13:38.191304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-05-27 11:13:38.191330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-05-27 11:13:38.191345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-05-27 11:13:38.191359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-05-27 11:13:38.191373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-05-27 11:13:38.191394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-05-27 11:13:38.191410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-27 11:13:38.194701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2020-05-27 11:13:38.234960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-05-27 11:13:38.260200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-27 11:13:38.260231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 
2020-05-27 11:13:38.260244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N 
2020-05-27 11:13:38.260253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N 
2020-05-27 11:13:38.270604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: c323:00:00.0, compute capability: 6.0)
2020-05-27 11:13:38.272442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15216 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: e230:00:00.0, compute capability: 6.0)
2020-05-27 11:13:38.282760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: c323:00:00.0
2020-05-27 11:13:38.283670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: e230:00:00.0
2020-05-27 11:13:38.283719: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-05-27 11:13:38.283744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-05-27 11:13:38.283766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-05-27 11:13:38.283787: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-05-27 11:13:38.283808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-05-27 11:13:38.283828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-05-27 11:13:38.283849: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-27 11:13:38.287317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2020-05-27 11:13:38.287378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-27 11:13:38.287396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 
2020-05-27 11:13:38.287408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N 
2020-05-27 11:13:38.287418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N 
2020-05-27 11:13:38.289827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: c323:00:00.0, compute capability: 6.0)
2020-05-27 11:13:38.290742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:1 with 15216 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: e230:00:00.0, compute capability: 6.0)
I0527 11:14:18.026844 140542352119552 cross_device_ops.py:427] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0527 11:14:18.039508 140542352119552 cross_device_ops.py:427] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0527 11:14:18.333121 140542352119552 cross_device_ops.py:427] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0527 11:14:18.344817 140542352119552 cross_device_ops.py:427] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0527 11:14:18.490315 140542352119552 cross_device_ops.py:427] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0527 11:14:18.501933 140542352119552 cross_device_ops.py:427] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0527 11:14:18.663970 140542352119552 cross_device_ops.py:427] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0527 11:14:18.675677 140542352119552 cross_device_ops.py:427] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0527 11:14:18.815266 140542352119552 cross_device_ops.py:427] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0527 11:14:18.826851 140542352119552 cross_device_ops.py:427] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
strategy.num_replicas_in_sync: 2
I0527 11:14:27.238750 140542352119552 datasets.py:286] Loading train data from: /tmp/pouring_tfrecords/pouring_train*
",printed also see available file successfully dynamic library found device name major minor found device name major minor successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible binary use frequency service platform host device host default version service platform device compute capability device compute capability found device name major minor found device name major minor successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible successfully dynamic library device interconnect strength edge matrix device memory physical device name bus id compute capability device memory physical device name bus id compute capability found device name major minor found device name major minor successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible device interconnect strength edge matrix device memory physical device name bus id compute capability device memory physical device name bus id compute capability reduce broadcast reduce broadcast reduce broadcast reduce broadcast reduce broadcast reduce broadcast reduce broadcast reduce broadcast reduce broadcast reduce broadcast loading train data,issue,positive,positive,neutral,neutral,positive,positive
634167874,"Hi, 

Thanks for checking out TCC.

Can you print the value of strategy.num_replicas_in_sync on this [line](https://github.com/google-research/google-research/blob/master/tcc/train.py#L85)?

Have you set CUDA_VISIBLE_DEVICES in the environment?",hi thanks print value line set environment,issue,positive,positive,positive,positive,positive,positive
631596329,"Hi. the issue is already fixed. It appears the issue is on the script file ***.sh** used to run the experiment. Copying of text in Visual Studio Code appears to have issues. 

Closing the issue.",hi issue already fixed issue script file used run experiment text visual studio code issue,issue,negative,positive,neutral,neutral,positive,positive
628699905,"@gariel-google Thanks for your guidance! It's very helpful!
You are right, I'm using checkpoints with learned intrinsics.
I have a new question now:
![image](https://user-images.githubusercontent.com/38416786/81951377-ac853480-9637-11ea-8f15-c631cfe7e027.png)
In your paper, you show that the network can carve the silhouette of the people out of a rough mask, but in my test, it only happens in a few cases, and in most of the cases the output is like the region of the car, that is, the network cannot catch the residual translation correctly.
Does this happen in your test?",thanks guidance helpful right learned new question image paper show network carve silhouette people rough mask test output like region car network catch residual translation correctly happen test,issue,positive,positive,positive,positive,positive,positive
628445411,@gariel-google Thanks for helping with this! Any news about releasing the weights for the given intrinsics odometry model? ,thanks helping news given odometry model,issue,positive,positive,positive,positive,positive,positive
626050243,"@saberkun thanks for answer. We are trying to use dql_grasping repo in our research team for possiblity of project, thin slice. That would be my suggestion since there are more and more papers going on about robotics task with RL
I saw tensor2tensor group moved to JAX i believe",thanks answer trying use research team project thin slice would suggestion since going task saw group believe,issue,negative,negative,neutral,neutral,negative,negative
626043529,"Tensorflow team is updating the TF model garden with more TF2 implementations. https://github.com/tensorflow/models/tree/master/official
If you have a wishlist, please let us know. We are ramping up community contribution as well.",team model garden please let u know ramping community contribution well,issue,positive,neutral,neutral,neutral,neutral,neutral
625664924,Would echo this sentiment.  This is a great step in the evolution of usage of neural networks in tabular settings.  Currently I use a PyTorch implementation but it would be great if this code base could be updated.,would echo sentiment great step evolution usage neural tabular currently use implementation would great code base could,issue,positive,positive,positive,positive,positive,positive
624807823,"@frobinet I'll have a look at the model links and get back to you, thanks for pointing this out.",look model link get back thanks pointing,issue,negative,positive,neutral,neutral,positive,positive
624807362,"@player1321 The definition of ATE we used follows Zhou et al. Our ATE eval is based on theirs, which is given here: https://github.com/tinghuiz/SfMLearner/tree/master/kitti_eval. The numbers typically are in the 10^-2-s. Your's are in meters, and are large-ish, so indeed it's probably the same definition. 

Regarding the translation error, we didn't check if tor the city+KITTI checkpoint, and while your numbers are different, it seems that your numbers are not far from ours, assuming you tested checkpoints with learned intrinsics (right?).

@frobinet The odometry predictions are scale-less, just like the depth predictions. We normalized the entire trajectory by its length. That is, we scaled the predicted trajectory uniformly until its total length was identical to the GT length.
",player definition ate used al ate based given typically indeed probably definition regarding translation error check tor different far assuming tested learned right odometry like depth entire trajectory length scaled trajectory uniformly total length identical length,issue,negative,positive,neutral,neutral,positive,positive
624223159,"> The source code is on it’s way

I am looking forward to experiment with it!",source code way looking forward experiment,issue,negative,neutral,neutral,neutral,neutral,neutral
624047346,"Also, I realized that the given intrinsics weights link with kitti odometry is wrong: it references references the cityscape model from the depth table right above: https://www.googleapis.com/download/storage/v1/b/gresearch/o/depth_from_video_in_the_wild%2Fcheckpoints%2Fcityscapes_learned_intrinsics.zip?generation=1566493765410932&alt=media ",also given link odometry wrong cityscape model depth table right,issue,negative,negative,negative,negative,negative,negative
623987857,"@gariel-google Thanks for sharing the code and helping us reproduce the results. I'm able to reproduce similar figures to the paper using the odometry checkpoints, but the scale seems to be wrong. Is the egomotion network supposed to output positions with the real-world scale immediately, or is it assumed that we're performing a scaling as postprocessing? If yes, which type of scaling is used in the paper? 

EDIT: From the looks of it, I think the scale-7dof scaling technique is used (see https://github.com/Huangying-Zhan/kitti-odom-eval)",thanks code helping u reproduce able reproduce similar paper odometry scale wrong network supposed output scale immediately assumed scaling yes type scaling used paper edit think scaling technique used see,issue,positive,positive,neutral,neutral,positive,positive
623045474,"Giving more logs plus verifying it won't even go 1 step on a v3-8 with 2x2 topology config.

If I kill the process, it prints this:

(edit, if there's a better way to format this please let me know)
# Termination print
`INFO:tensorflow:training_loop marked as finished
I0503 02:43:25.864410 140402965063488 error_handling.py:108] training_loop marked as finished
Traceback (most recent call last):
  File ""/home/eligray2/.local/bin/t5_mesh_transformer"", line 8, in <module>
    sys.exit(console_entry_point())
  File ""/home/eligray2/.local/lib/python3.7/site-packages/t5/models/mesh_transformer_main.py"", line 222, in console_entry_point
    app.run(main)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/eligray2/.local/lib/python3.7/site-packages/t5/models/mesh_transformer_main.py"", line 216, in main
    model_dir=FLAGS.model_dir)
  File ""/usr/local/lib/python3.7/dist-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/mesh_tensorflow/transformer/utils.py"", line 1738, in run
    train_dataset_fn, train_steps, ensemble_inputs)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/mesh_tensorflow/transformer/utils.py"", line 1132, in train_model
    estimator.train(input_fn=input_fn, max_steps=train_steps)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3030, in train
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 374, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1164, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1194, in _train_model_default
    features, labels, ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2857, in _call_model_fn
    config)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1152, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3148, in _model_fn
    input_holders.generate_infeed_enqueue_ops_and_dequeue_fn())
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1428, in generate_infeed_enqueue_ops_and_dequeue_f
n
    self._invoke_input_fn_and_record_structure())
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1483, in _invoke_input_fn_and_record_structure
    num_hosts))
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1090, in generate_broadcast_enqueue_ops_fn
    inputs = _Inputs.from_input_fn(input_fn(user_context))
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3001, in _input_fn
    return input_fn(**kwargs)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/mesh_tensorflow/transformer/utils.py"", line 1126, in input_fn
    dataset_split=dataset_split)
  File ""/usr/local/lib/python3.7/dist-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/t5/models/mesh_transformer.py"", line 66, in mesh_train_dataset_fn
    sequence_length, split=dataset_split, use_cached=use_cached, shuffle=True)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/t5/data/utils.py"", line 637, in get_dataset
    ds = self._dataset_fn(split=split, shuffle_files=shuffle)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/t5/data/utils.py"", line 739, in dataset_fn
    return self._tfds_dataset.load(split, shuffle_files)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/t5/data/utils.py"", line 217, in load
    try_gcs=True)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py"", line 69, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/core/registered.py"", line 369, in load
    dbuilder.download_and_prepare(**download_and_prepare_kwargs)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py"", line 69, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py"", line 363, in download_and_prepare
    download_config=download_config)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py"", line 1004, in _download_and_prepare
    max_examples_per_split=download_config.max_examples_per_split,
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py"", line 924, in _download_and_prepare
    dl_manager, **split_generators_kwargs):
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/text/movie_rationales.py"", line 71, in _split_generators
    dl_dir = dl_manager.download_and_extract(_DOWNLOAD_URL)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/core/download/download_manager.py"", line 419, in download_and_extract
    return _map_promise(self._download_extract, url_or_urls)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/core/download/download_manager.py"", line 462, in _map_promise
    res = utils.map_nested(_wait_on_promise, all_promises)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py"", line 169, in map_nested
    return function(data_struct)
  File ""/home/eligray2/.local/lib/python3.7/site-packages/tensorflow_datasets/core/download/download_manager.py"", line 446, in _wait_on_promise
    return p.get()
  File ""/usr/local/lib/python3.7/dist-packages/promise/promise.py"", line 511, in get
    self._wait(timeout or DEFAULT_TIMEOUT)
  File ""/usr/local/lib/python3.7/dist-packages/promise/promise.py"", line 506, in _wait
    self.wait(self, timeout)
  File ""/usr/local/lib/python3.7/dist-packages/promise/promise.py"", line 502, in wait
    async_instance.wait(promise, timeout)
  File ""/usr/local/lib/python3.7/dist-packages/promise/async_.py"", line 117, in wait
    target.scheduler.wait(target, timeout)
  File ""/usr/local/lib/python3.7/dist-packages/promise/schedulers/immediate.py"", line 25, in wait
    waited = e.wait(timeout)
  File ""/usr/lib/python3.7/threading.py"", line 552, in wait
    signaled = self._cond.wait(timeout)
  File ""/usr/lib/python3.7/threading.py"", line 296, in wait
    waiter.acquire()
KeyboardInterrupt
`",giving plus wo even go step topology kill process edit better way format please let know termination print marked finished marked finished recent call last file line module file line main file line run main file line main file line main file line return file line run file line file line train file line train loss file line return file line file line file line file line file line file line file line file line return file line file line return file line file line file line return split file line load file line return file line load file line return file line file line file line file line file line return file line file line return function file line return file line get file line self file line wait promise file line wait target file line wait file line wait file line wait,issue,negative,positive,positive,positive,positive,positive
621302407,"@gariel-google Thanks for your reply.
Actually I have trouble generating the numbers. I'm using [this evaluation tool](https://github.com/Huangying-Zhan/kitti-odom-eval), and the visualization looks fine but the numbers are very different from yours.
Here is the visualization result:
Seq-09
![image](https://user-images.githubusercontent.com/38416786/80617762-14068600-8a75-11ea-84a8-40c1cf58ae88.png)
Seq-10
![image](https://user-images.githubusercontent.com/38416786/80617817-25e82900-8a75-11ea-9113-15952a5cc6be.png)

And here are the numbers:
- KITTI
Sequence: 	 9 
Trans. err. (%): 	 6.572 
Rot. err. (deg/100m): 	 2.254 
ATE (m): 	 40.574 
RPE (m): 	 0.068 
RPE (deg): 	 0.101 
Sequence: 	 10 
Trans. err. (%): 	 13.659 
Rot. err. (deg/100m): 	 3.904 
ATE (m): 	 145.783 
RPE (m): 	 0.100 
RPE (deg): 	 0.097

- KITTI + cityscapes
Sequence: 	 9 
Trans. err. (%): 	 7.727 
Rot. err. (deg/100m): 	 2.220 
ATE (m): 	 45.602 
RPE (m): 	 0.088 
RPE (deg): 	 0.093 
Sequence: 	 10 
Trans. err. (%): 	 13.032 
Rot. err. (deg/100m): 	 2.529 
ATE (m): 	 88.142 
RPE (m): 	 0.104 
RPE (deg): 	 0.082

It seems that the definition of the ATE is not the same as yours. Would you share some evaluation tools? Or any reference about the definitions can be recommended?",thanks reply actually trouble generating evaluation tool visualization fine different visualization result image image sequence err rot err ate deg sequence err rot err ate deg sequence err rot err ate deg sequence err rot err ate deg definition ate would share evaluation reference,issue,negative,positive,positive,positive,positive,positive
620232319,"@player1321 I looked up the checkpoint we used for KITTI odometry (with given intrinsics), and its depth prediction metric is 0.1321, which is indeed worse than the KITTI-only depth error that we report in the paper for given intrinsics (0.129). Is that your concern? We did observe that for odometry results tend to improve the longer we train, whereas for depth they tend to become slightly worse and more noise beyond some point. We did not try to evaluate the cityscapes + KITTI checkpoints for odometry, and I don't know how it would perform.

Would you like to share your numbers on both evaluations?",player used odometry given depth prediction metric indeed worse depth error report paper given concern observe odometry tend improve longer train whereas depth tend become slightly worse noise beyond point try evaluate odometry know would perform would like share,issue,negative,negative,negative,negative,negative,negative
619469405,"@studennis911988 Hello, I'm also interested in real depth value, do you how to get the real depth value from the output depth image?",hello also interested real depth value get real depth value output depth image,issue,positive,positive,positive,positive,positive,positive
619298201,"@gariel-google Thanks a lot for your patient guidance.
I tested your released model. It seems that the model trained for odometry works well on trajectory inference but not so well on depth estimation, and the model trained on cityscapes&kitti works well on depth estimation but not so well on trajectory inference.
Is this caused by the gap between cityscapes and kitti? Or are there some tricks for improving depth estimation and trajectory inference respectively?

",thanks lot patient guidance tested model model trained odometry work well trajectory inference well depth estimation model trained work well depth estimation well trajectory inference gap improving depth estimation trajectory inference respectively,issue,positive,positive,neutral,neutral,positive,positive
618720022,"This PR was merged as a commit:
https://github.com/google-research/google-research/commit/85864eeac6794fbb808cca24de32bbbfcbcb658e

So we won't be able to merge this PR. I will close this item.",commit wo able merge close item,issue,negative,positive,positive,positive,positive,positive
616706243,"@StephenStorm by ""observing strong discrepancies even beyond that global factor"" I mean: If you multiply the predicted depth by a factor such that its median matches the median groundtruth depth, do you still see significant discrepancies?

@player1321 In the KITTI format, the first 3 columns are the (x, y, z) position of the car if I'm not mistaken. [This code](https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/trajectory_inference.py)  generates the inferred (x, y, z)-s of the trajectory.
",observing strong even beyond global factor mean multiply depth factor median median depth still see significant player format first position car mistaken code trajectory,issue,negative,positive,positive,positive,positive,positive
615115464,"@gariel-google 
Hello, I'm new to this line. 
I tried to run trajectory_inference.py and evaluate the odometry result, but the output format looks different from the ground truth downloaded from [KITTI odometry](http://www.cvlibs.net/download.php?file=data_odometry_poses.zip).
Will you release some examples about how to translate the format? Or maybe I missed some examples in this repo?
",hello new line tried run evaluate odometry result output format different ground truth odometry release translate format maybe,issue,negative,positive,neutral,neutral,positive,positive
614945544,"Thanks for the reply!
I will try it.
And I'm looking forward to the code being released.",thanks reply try looking forward code,issue,negative,positive,positive,positive,positive,positive
614848648,"Hi octpath@,  we will do our best to release that code too, but meanwhile, here's a snippet of how to do that. If you know that all your frames come from the same camera (which is true for EuRoC but not for KITTI / cityscapes), you'd do something like this

```python
fx = tf.get_variable('fx', initializer=image_width)
fy = tf.get_variable('fy', initializer=image_height)
x0 = tf.get_variable('x0', initializer=image_width * 0.5)
y0 = tf.get_variable('y0', initializer=image_height * 0.5)

intrinsics_mat = tf.convert_to_tenosr([[fx, 0.0, x0], [0.0, fy, y0], [0.0, 0.0, 1.0]])
```

If you have multiple videos, assuming that for each frame / triplet you have a key indicating what video they came from, you can use a tensorflow hash table to keep a separate intrinsic matrix per key. Hope that helps!

",hi best release code meanwhile snippet know come camera true something like python multiple assuming frame triplet key video came use hash table keep separate intrinsic matrix per key hope,issue,positive,positive,positive,positive,positive,positive
613841406,"@hubin111 I met the same problem when trying to set ""exp_decay"", and after replacing `tf.train.exponential_decay` with `tf.compat.v1.train.exponential_decay`, I found that the learning rate starts to decrease **every iter**(not every EXP_DECAY_STEPS) after the global_step exceeds the EXP_DECAY_STEPS. Have you met the same case? Thank you!
Another question is that how many iters are required to realize the results in paper on the ""pouring dataset""? I found the ""reg_loss"" keeping increasing after around 5000 iters.",met problem trying set found learning rate decrease every iter every met case thank another question many realize paper pouring found keeping increasing around,issue,negative,positive,positive,positive,positive,positive
613839207,"You need to add ""name = name.encode()"" before the line 80 in dataset_utils.py",need add name line,issue,negative,neutral,neutral,neutral,neutral,neutral
613255650,"@liyingliu @gariel-google 
Thank you very much for your two patient guidance, I will try again based on your suggestions.
If I encounter other problems, I will consult you again.
@gariel-google 
By the way, I don't quite understand what you mean by ""observing strong discrepancies even beyond that global factor"". Do you mean whether I found  the scale factor does not apply to the entire image ?
thank you both again.",thank much two patient guidance try based encounter consult way quite understand mean observing strong even beyond global factor mean whether found scale factor apply entire image thank,issue,positive,positive,neutral,neutral,positive,positive
613078830,"From what we tried, the results do reproduce with the fix, and are slightly
better in some cases, but we did not re-run systematically all
the experiments in the paper and we do not have checkpoints for all of
them. We will release the fixed code, hopefully over the next few days.

On Mon, Apr 13, 2020 at 12:52 PM Penn <notifications@github.com> wrote:

> Thanks for the response! Yes this would be helpful.
>
> I would hope reusing the variables properly would improve results and/or
> reduce training time. However there is always the chance that the bug is
> actually necessary to achieve the published results.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/230#issuecomment-613066046>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNHOWBSQFSUH4ZZY2EDRMNUPTANCNFSM4LNXM3GQ>
> .
>
",tried reproduce fix slightly better systematically paper release fixed code hopefully next day mon wrote thanks response yes would helpful would hope properly would improve reduce training time however always chance bug actually necessary achieve reply directly view,issue,positive,positive,positive,positive,positive,positive
613066046,"Thanks for the response! Yes this would be helpful.

I would hope reusing the variables properly would improve results and/or reduce training time. However there is always the chance that the bug is actually necessary to achieve the published results.",thanks response yes would helpful would hope properly would improve reduce training time however always chance bug actually necessary achieve,issue,positive,positive,neutral,neutral,positive,positive
613034150,"suvigy@ Thanks so much for your help!

On Thu, Apr 9, 2020 at 8:28 AM suvigy <notifications@github.com> wrote:

> @yanfengliu <https://github.com/yanfengliu> @pigletsc
> <https://github.com/pigletsc>
> Assuming you converted the torch model to tensorflow model and are having
> the following files for imagenet checkpoint:
>
> init/
> checkpoint
> model.ckpt.data-00000-of-00001
> model.ckpt.index
> model.ckpt.meta
>
> in this case you have to specify --imagenet_ckpt=.../init/model.ckpt
> So not even an existing filename (and not the .index file, or the init dir)
> That works for me
>
> @pigletsc <https://github.com/pigletsc>
> Regarding 'Tensor had NaN values'. Issue #162
> <https://github.com/google-research/google-research/issues/162> is about
> this problem. There is an important bug in CUDA 10.0, and the related
> tensorflow 1.x (x>=13.1) binaries are built with CUDA 10.0 til. We built
> tensorflow 14.0 with CUDA 10.1. It works that way. Or you can try the
> training only on CPU. :)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/130#issuecomment-611591025>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNF62B32UAZXYCM3ZFLRLXSRLANCNFSM4JNV7GXQ>
> .
>
",thanks much help wrote assuming converted torch model model following case specify even file work regarding nan issue problem important bug related built til built work way try training reply directly view,issue,positive,positive,positive,positive,positive,positive
613007442,"The predicted depth is up to an unknown scale factor. When evaluating, this
factor is found by normalizing the medians of the predicted and inferred
depth this is standard in this line of publications). Are you
observing strong discrepancies even beyond that global factor?

On Sun, Apr 12, 2020 at 11:26 PM stephen <notifications@github.com> wrote:

> @liyingliu <https://github.com/liyingliu>
> sorry to bother you, but I have some questions I'd like to ask you. I
> tried to infer the depth map using the existing checkpoint cityscape_kitti,
> but the depth value I read directly from the '.npy' file was far from real
> depth, whether it was my own images or images in cityscape. Did I do
> something wrong? Or further operations are required to obtain true depth
> values. Thank you very much.
> 'inference.py' from (
> https://github.com/tensorflow/models/blob/master/research/struct2depth/inference.py
> )
> img_width and img_height are defaults (416,128)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46#issuecomment-612771053>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNC23ORNPVHH5B4GUFTRMKWBLANCNFSM4IMPQMMA>
> .
>
",depth unknown scale factor factor found depth standard line observing strong even beyond global factor sun wrote sorry bother like ask tried infer depth map depth value read directly file far real depth whether cityscape something wrong obtain true depth thank much reply directly view,issue,positive,positive,neutral,neutral,positive,positive
613001153,"Hi liyingliu@ and jenkspt@

I am sorry for the delayed response. Yes, there is a bug here
https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/motion_prediction_net.py#L244-L250

which causes some of the variables to not be reused. We fixed it since in
our internal version, but all the numbers we published in the paper were
obtained with that code, and the checkpoints we released are compatible
with that (buggy) code. If we fix the code now, the checkpoints we
published will not reproduce the numbers in the paper.

Perhaps we can publish the corrected version protected by a flag that will
be off by default? Would that help you guys?

On Sun, Apr 12, 2020 at 11:58 PM juju <notifications@github.com> wrote:

> Hi, I found the same issue as well. And I also believe reusing these
> variables is intended. Could you help to explain it? @gariel-google
> <https://github.com/gariel-google>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/230#issuecomment-612779372>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNDPRWFVJGWR3HVROBLRMKZ3BANCNFSM4LNXM3GQ>
> .
>
",hi sorry response yes bug fixed since internal version paper code compatible buggy code fix code reproduce paper perhaps publish corrected version flag default would help sun juju wrote hi found issue well also believe intended could help explain reply directly view,issue,positive,negative,neutral,neutral,negative,negative
612779372,"Hi, I found the same issue as well. And I also believe reusing these variables is intended. Could you help to explain it? @gariel-google ",hi found issue well also believe intended could help explain,issue,positive,neutral,neutral,neutral,neutral,neutral
612777662,"Hi, there is an unknown scale between the predicted depth value from the network and the real depth value. You need to scale the predicted depth by such a scale factor to have true depth value. You could use the median of your prediction divided by the median of ground truth to be the scale.",hi unknown scale depth value network real depth value need scale depth scale factor true depth value could use median prediction divided median ground truth scale,issue,positive,positive,positive,positive,positive,positive
612771053,"@liyingliu 
sorry to bother you, but I have some questions I'd like to ask you. I tried to infer the depth map using the existing checkpoint cityscape_kitti, but the depth value I read directly from the '.npy' file was far from real depth, whether it was my own images or images in cityscape. Did I do something wrong? Or further operations are required to obtain true depth values. Thank you very much.
'inference.py' from (https://github.com/tensorflow/models/blob/master/research/struct2depth/inference.py)
img_width and img_height are defaults (416,128)",sorry bother like ask tried infer depth map depth value read directly file far real depth whether cityscape something wrong obtain true depth thank much,issue,positive,negative,neutral,neutral,negative,negative
612575144,"Hi,&nbsp;Ishan:


I am sorry that I did not receive your message in time due to the epidemic situation. Our json file production work is marked together through multi-person collaboration. Because I did n’t get permission from others, I ’m sorry I can’t share this file with you. However, I can share with you the format of the&nbsp;author (debidatta)'s label annotation and related data production methods.&nbsp;Author&nbsp;used the following commands to create videos from Penn Action frames:
ffmpeg -r 4 -i penn_action_input_video/%%06d.jpg -c:v libx264 -vf fps=20 -pix_fmt yuv420p -vf ""scale=trunc(iw/2)*2:trunc(ih/2)*2"" /tmp/a.mp4
ffmpeg -f lavfi -i anullsrc=channel_layout=stereo:sample_rate=16000 -i /tmp/a.mp4  -shortest -c:v copy -c:a aac penn_action_output_video.mp4



It would be nice if you used these commands as you wouldn't have to run more experiments to find the right hyperparams.



The JSON file should look like this(https://github.com/google-research/google-research/issues/45#issuecomment-523172127). Ensure the timestamps in this JSON are in MICROSECONDS and synced with the video you generated from Penn Action frames.



Thanks
------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;""DAVEISHAN""<notifications@github.com&gt;;
发送时间:&nbsp;2020年4月12日(星期天) 上午8:41
收件人:&nbsp;""google-research/google-research""<google-research@noreply.github.com&gt;;
抄送:&nbsp;""小米大豆""<479685347@qq.com&gt;;""Mention""<mention@noreply.github.com&gt;;
主题:&nbsp;Re: [google-research/google-research] 【TCC】-json file of Penn-Action (#116)





 
Hi, @hubin111 Could you pl share the phase annotation json file for Penn Action. @debidatta I am asking your consent for this file-sharing.
 
Hope to hear from you soon,
 Ishan
 
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",hi sorry receive message time due epidemic situation file production work marked together collaboration get permission sorry share file however share format author label annotation related data production author used following create action copy would nice used would run find right file look like ensure video action thanks mention mention file hi could share phase annotation file action consent hope hear soon reply directly view,issue,positive,positive,neutral,neutral,positive,positive
612544252,"Hi, @hubin111 Could you pl share the phase annotation json file for Penn Action. @debidatta I am asking your consent for this file-sharing. 

Hope to hear from you soon,
Ishan",hi could share phase annotation file action consent hope hear soon,issue,positive,positive,neutral,neutral,positive,positive
611952868,"Hi Oleg,

Thanks for the pointer; I'll look at it and will ping you in case I have questions. Thanks so much!

Best,
sam",hi thanks pointer look ping case thanks much best sam,issue,positive,positive,positive,positive,positive,positive
611919378,"Hi Sam,

I am glad you find it useful!
I would suggest to explore TF to TFLite conversion with [colab](https://colab.research.google.com/drive/1wASJnQxpu2bEVOIo4FpgF778edo0wOG8#scrollTo=NcIzzCADklYm). 
It has example of running:
1 TF model in non streaming inference mode.
2 Converting TF non streaming model to TFlite non streaming model and running TFLite non streaming inference
3 Converting TF non streaming model to TF streaming model with internal state and running its inference
4 Converting TF non streaming model to TF streaming with external state and running its inference
5 Converting TF non streaming model to TFLite streaming with external state and running its inference.
Please feel free to ping me if you have any questions.

Best,
Oleg.",hi sam glad find useful would suggest explore conversion example running model non streaming inference mode converting non streaming model non streaming model running non streaming inference converting non streaming model streaming model internal state running inference converting non streaming model streaming external state running inference converting non streaming model streaming external state running inference please feel free ping best,issue,positive,positive,positive,positive,positive,positive
611914048,"Hi Oleg,

Wow, this is absolutely terrific! Thank you so much for the link and more importantly for your work! Amazing. I think I'll not have enough data to train my model from scratch but I should be able to use transfer learning based on the your infrastructure and the speech command data set. 

Thanks so much!
sam",hi wow absolutely terrific thank much link importantly work amazing think enough data train model scratch able use transfer learning based infrastructure speech command data set thanks much sam,issue,positive,positive,positive,positive,positive,positive
611906828,"Here is a [colab](https://colab.research.google.com/drive/1wASJnQxpu2bEVOIo4FpgF778edo0wOG8#scrollTo=NcIzzCADklYm) with example of running: 
TF in non streaming
TFlite non streaming
TF streaming with internal state
TF streaming with external state
TFLite streaming with external state

Please have a look at graphs of streaming vs non streaming models in above colab.
It demonstrates a difference between them: streaming with external state has additional inputs for states which you have to intialize 
[inference.zip](https://github.com/google-research/google-research/files/4460251/inference.zip)
and control during inference.
You already figured out your issue :), so hope above colab will be helpful for the next step: running TFLite model in streaming mode.
",example running non streaming non streaming streaming internal state streaming external state streaming external state please look streaming non streaming difference streaming external state additional control inference already figured issue hope helpful next step running model streaming mode,issue,positive,neutral,neutral,neutral,neutral,neutral
611902685,Thanks for the pointers! My problem was that I was not initializing the state inputs to zero before running invoke.,thanks problem state zero running invoke,issue,negative,positive,positive,positive,positive,positive
611891387,"I am not sure about what are you trying to achieve in your colab.
You do not run keras model in streaming mode, so it is not correct to compare tf non streaming with TFLite streaming models - the way you compare them.

If you just need to convert your model to TFLite then you should specify 
mode=Modes.NON_STREAM_INFERENCE
as it is done in this [colab](https://colab.research.google.com/drive/1zbnQjkOYfyLfGVal618t0GjlbCXtYuDL#scrollTo=IYeAWXP8incN) - the output should be correct on any feature size and model depth.

Also there is no need to use TimeDistributed wrapper - Dense layer is applied on last dimension in time any way, as it is show in [dense_test](https://github.com/google-research/google-research/blob/master/kws_streaming/layers/dense_test.py)

**Please close this issue and create another one**, because the last one is not related to kws_streaming.models.utils_test failure and can be confusing for other people (it is a good practice not to mix different issues in one ticket).
In the new issue, I will add another colab which explain the difference between non streaming and streaming models in TF and TFLite with gru example.
Thank you for exploring this lib, I appreciate your feedbacks!",sure trying achieve run model streaming mode correct compare non streaming streaming way compare need convert model specify done output correct feature size model depth also need use wrapper dense layer applied last dimension time way show please close issue create another one last one related failure people good practice mix different one ticket new issue add another explain difference non streaming streaming example thank exploring appreciate,issue,positive,positive,positive,positive,positive,positive
611873551,"I found another issue, possible memory-related with the tflite conversion. Here is an example GRU model I am working with:
![image](https://user-images.githubusercontent.com/1261472/78962535-38d89080-7aa9-11ea-8c67-105c7f70a1cf.png)
If input shape is [1,1,20] and the model has three GRU layers with 2 units, the keras model.predict() and tf.lite interpreter outputs are the same. If however, I change the input shape to [1,1,40], I get NaN output from tflite interpreter. If I leave the input shape as [1,1,40] but use 2 layers with 2 nodes, I get an incorrect output (sometimes 0, 1 or NaN) with the tflite interpreter. 

Here is a colab showing the issue: https://colab.research.google.com/drive/1QC73kZgx6yG7af10-RtWIjrWWLFjOcXc",found another issue possible conversion example model working image input shape model three interpreter however change input shape get nan output interpreter leave input shape use get incorrect output sometimes nan interpreter showing issue,issue,negative,neutral,neutral,neutral,neutral,neutral
611851623,"Good point :)
Just in case here is a working [colab](https://colab.research.google.com/drive/1withkJedDAZtcgTvulVeyLUPnzxpJLnf)",good point case working,issue,negative,positive,positive,positive,positive,positive
611849716,"Ah I found the issue: the model needs to be instantiated after resetting the graph. Thanks for your help, Oleg!",ah found issue model need graph thanks help,issue,positive,positive,positive,positive,positive,positive
611843845,"Hi Oleg,

I suspect there might be some other setting on your machine that is different from mine and google colab (https://colab.research.google.com/drive/1tIreYkJ5mw1xDbmd7MHMtyRbH26GC4-H). I followed your steps with virtualenv on my local machine and google colab and get the same error:
 
```WARNING:absl:There is no need to use Stream on time dim with size 1
WARNING:root:FAILED to convert to mode NON_STREAM_INFERENCE, tflite: Fetch argument <tf.Variable 'gru/cell/kernel:0' shape=(20, 96) dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(""gru/cell/kernel/Read/ReadVariableOp:0"", shape=(20, 96), dtype=float32) is not an element of this graph.)```

",hi suspect might setting machine different mine local machine get error warning need use stream time dim size warning root convert mode fetch argument tensor tensor tensor element graph,issue,negative,positive,neutral,neutral,positive,positive
611824372,"Please find comments below:

_Why not use the tf2.0 tf.lite converter from_keras() method?_ 
It will limit converter to Keras models only. I tried to keep it as generic as possible, so that you can build your model in pure TF or TF.Keras and converter still will work in both cases.

_Why does your tf lite streaming implementation only support external states? Is there something inherently problematic with internal states and tf lite?_
I implemented kws_streaming in a way that you can convert TF to TF streaming with internal states or to TF streaming with internal states. So both streaming modes are supported in this lib.
But TFlite does not support stateful layers now, that is why I test only streaming TFlite with external states (you can build special custom ops with states for TFlite, but I would avoid this path because you will have to customize every stateful layer - it does not scale). TFlite team is working on supporting internal states as general solution and as soon it is enabled, stateful streaming will work out of of the box with this lib.

I tried your colab and it did not work for me (I do not cotroll all env there so it is hard to say why it does not work - it looks like there are some issues with installed TF version).

Please try attached jupyter notebok - it works ok (here all env set up is controlled by yourself):

cd /tmp/kws_streaming
virtualenv --system-site-packages -p python3 ./venv3
source ./venv3/bin/activate
pip install tensorflow==2.1.0-rc1
jupyter notebook

[Untitled.zip](https://github.com/google-research/google-research/files/4459308/Untitled.zip)
",please find use converter method limit converter tried keep generic possible build model pure converter still work lite streaming implementation support external something inherently problematic internal lite way convert streaming internal streaming internal streaming support stateful test streaming external build special custom would avoid path every stateful layer scale team working supporting internal general solution soon stateful streaming work box tried work hard say work like version please try attached work set python source pip install notebook,issue,positive,positive,neutral,neutral,positive,positive
611801943,"Hi Oleg,

Thank you for taking a look at the unittests. They all pass for me now. Unfortunately, I still get errors when I work with the functions in kws_streaming outside the context of model_train_eval.py. 

Here is a colab showing the error from utils.model_to_tflite(). https://colab.research.google.com/drive/1tIreYkJ5mw1xDbmd7MHMtyRbH26GC4-H

I had some other questions:
- Why not use the tf2.0 tf.lite converter from_keras() method? https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter?version=nightly#from_keras_model
- Why does your tf lite streaming implementation only support external states? Is there something inherently problematic with internal states and tf lite? I made a simple attempt at converting a keras model with internal state and ran into other converter issues like this one https://github.com/tensorflow/tensorflow/issues/37659. ",hi thank taking look pas unfortunately still get work outside context showing error use converter method lite streaming implementation support external something inherently problematic internal lite made simple attempt converting model internal state ran converter like one,issue,negative,negative,neutral,neutral,negative,negative
611774340,"Hi Sam, 

Just in case if you are interested in building your own sound detection model or training your speech embedding or using speech feature extractor (MFCC, LFBE) you can try [kws_streaming](https://github.com/google-research/google-research/tree/master/kws_streaming). All models are convertible and quantizable with TFlite. Here is example of converting model to streaming with TFlite [testToStreamInferenceModeTFandTFLite](https://github.com/google-research/google-research/blob/master/kws_streaming/models/utils_test.py#L263).

Best,
Oleg.",hi sam case interested building sound detection model training speech speech feature extractor try convertible example converting model streaming best,issue,positive,positive,positive,positive,positive,positive
611770891,"Hi fmbao,

It is fixed, please confirm and close the ticket.

Best,
Oleg.
",hi fixed please confirm close ticket best,issue,positive,positive,positive,positive,positive,positive
611747469,"Hi Abhipray,

All unit tests are working.
Also I added more tests to demonstrate how gru model is converted to streaming with TFlite at [testToStreamInferenceModeTFandTFLite(self, model_name='gru'):](https://github.com/google-research/google-research/blob/master/kws_streaming/models/utils_test.py#L263). You can try any other models by changing model_name (some models can not be streamed: 'ds_cnn_stride', 'cnn_stride', 'att_rnn', 'att_mh_rnn').
Please confirm and close the issue.

Best,
Oleg.",hi unit working also added demonstrate model converted streaming self try please confirm close issue best,issue,positive,positive,positive,positive,positive,positive
611591025,"@yanfengliu @pigletsc 
Assuming you converted the torch model to tensorflow model and are having the following files for imagenet checkpoint:

init/
  checkpoint
  model.ckpt.data-00000-of-00001 
  model.ckpt.index
  model.ckpt.meta

in this case you have to specify --imagenet_ckpt=.../init/model.ckpt 
So not even an existing filename (and not the .index file, or the init dir)
That works for me

@pigletsc 
Regarding 'Tensor had NaN values'. Issue #162  is about this problem. There is an important bug in CUDA 10.0, and the related tensorflow 1.x (x>=13.1) binaries are built with CUDA 10.0. We built tensorflow 14.0 with CUDA 10.1. It works that way. Or you can try the training only on CPU. :)




",assuming converted torch model model following case specify even file work regarding nan issue problem important bug related built built work way try training,issue,negative,positive,positive,positive,positive,positive
611566929,"I tried as well conversion with Tensorflow 1.15 and the following command:
```
tflite_convert --output_file speech_embedding.tflite --saved_model_dir . --saved_model_tag_set """" --saved_model_signature_key default --input_shapes 1,32000 --input_arrays=samples
```

This resulted in 


> TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. 
> We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. 
> If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXPAND_DIMS, FLOOR_DIV, FULLY_CONNECTED, GATHER, LESS, LOG, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MUL, PACK, PAD, RANGE, RESHAPE, SHAPE, SPLIT_V, SQUEEZE, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: ComplexAbs, LoopCond, RFFT, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArrayV3, TensorArrayWriteV3.
",tried well conversion following command default lite currently support control flow enter exit merge switch working supporting control flow please see issue model standard lite native might able use extended passing setting calling otherwise custom implementation disable error setting calling list add concatenation gather le log maximum pack pad range reshape shape squeeze sub list need custom,issue,positive,positive,positive,positive,positive,positive
611360244,"Hi fmbao,

Please run below script with attention rnn:
cd /tmp/kws_streaming
virtualenv --system-site-packages -p python3 ./venv3
source ./venv3/bin/activate
pip install tensorflow==2.1.0-rc1

// location of kws_streaming
KWS_PATH=/tmp/kws_streaming

// location of unpacked https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz
DATA_PATH=$KWS_PATH/data2

// location of unpacked https://storage.googleapis.com/kws_models/models2.zip
MODELS_PATH=$KWS_PATH/models2
python -m kws_streaming.train.model_train_eval \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/att_rnn/ \
--mel_upper_edge_hertz 7000 \
--how_many_training_steps 10000,10000,10000 \
--learning_rate 0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 40 \
--dct_num_features 20 \
--resample 0.15 \
--alsologtostderr \
--train 0 \
att_rnn \
--cnn_filters '10,1' \
--cnn_kernel_size '(5,1),(5,1)' \
--cnn_act ""'relu','relu'"" \
--cnn_dilation_rate '(1,1),(1,1)' \
--cnn_strides '(1,1),(1,1)' \
--rnn_layers 2 \
--rnn_type 'gru' \
--rnn_units 128 \
--dropout1 0.1 \
--units2 '64,32' \
--act2 ""'relu','linear'""

You should observe:
TF Final test accuracy on non stream model = 96.94% (N=4800)
TF Final test accuracy on non stream model = 96.73% (N=4800)
...
tflite test accuracy, non stream model = 96.94% 4800 out of 4890
tflite Final test accuracy, non stream model = 96.89% (N=4890)
...
FAILED to run TF streaming: slice index 24 of dimension 1 out of bounds. for 'streaming/tf_op_layer_strided_slice/strided_slice' (op: 'StridedSlice') with input shapes: [1,1,256], [3], [3], [3] and with computed input tensors: input[1] = <0 24 0>, input[2] = <0 25 0>, input[3] = <1 1 1>.


Attention RNN model is non streamable (because it uses bi-directional RNN) that is why it fails in streaming inference evaluation step with messages:
FAILED to run TF streaming: slice index 24
FAILED to convert to mode STREAM_EXTERNAL_STATE_INFERENCE, tflite:
FAILED to run TFLite streaming

This script kws_streaming.train.model_train_eval does several things:
1 Can train a model [Ok]
2 Evaluate TF model in non streaming mode [Ok] 
3 Convert non streaming TF model to non streaming TFLite and evaluate it [Ok]
4 Convert non streaming TF to streaming TF [Will Fail because model is not streamable]
5 Convert non streaming TF to streaming TFLite [Will Fail because model is not streamable]

I will disable evaluation of non streamable models in streaming mode, so there will be no confusion in log messages.

Best,
Oleg.
",hi please run script attention python source pip install location location unpacked location unpacked python resample train dropout act observe final test accuracy non stream model final test accuracy non stream model test accuracy non stream model final test accuracy non stream model run streaming slice index dimension input input input input input attention model non streaming inference evaluation step run streaming slice index convert mode run streaming script several train model evaluate model non streaming mode convert non streaming model non streaming evaluate convert non streaming streaming fail model convert non streaming streaming fail model disable evaluation non streaming mode confusion log best,issue,negative,neutral,neutral,neutral,neutral,neutral
611354965,"Hi fmbao,

Sorry for delay in my reply (I did not receive an issue notification).

Please run below script and let me know if cnn model fails:
cd /tmp/kws_streaming
virtualenv --system-site-packages -p python3 ./venv3
source ./venv3/bin/activate
pip install tensorflow==2.1.0-rc1

// location of kws_streaming
KWS_PATH=/tmp/kws_streaming

// location of unpacked https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz
DATA_PATH=$KWS_PATH/data2

// location of unpacked https://storage.googleapis.com/kws_models/models2.zip
MODELS_PATH=$KWS_PATH/models2

python -m kws_streaming.train.model_train_eval \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/cnn/ \
--mel_upper_edge_hertz 7000 \
--how_many_training_steps 10000,10000,10000 \
--learning_rate 0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 40 \
--dct_num_features 20 \
--resample 0.15 \
--alsologtostderr \
--train 0 \
cnn \
--cnn_filters '64,64,64,64,128,64,128' \
--cnn_kernel_size '(3,3),(5,3),(5,3),(5,3),(5,2),(5,1),(10,1)' \
--cnn_act ""'relu','relu','relu','relu','relu','relu','relu'"" \
--cnn_dilation_rate '(1,1),(1,1),(2,1),(1,1),(2,1),(1,1),(2,1)' \
--cnn_strides '(1,1),(1,1),(1,1),(1,1),(1,1),(1,1),(1,1)' \
--dropout1 0.5 \
--units2 '128,256' \
--act2 ""'linear','relu'"" 

You should observe something like:
...
TF Final test accuracy on non stream model = 95.10% (N=4800)
...
tflite Final test accuracy, non stream model = 95.03% (N=4890)
etc

Before running above script, I would also try
export CUDA_VISIBLE_DEVICES=-1
to validate that issue is related to running TFLite on CPU or GPU.

Best,
Oleg.",hi sorry delay reply receive issue notification please run script let know model python source pip install location location unpacked location unpacked python resample train dropout act observe something like final test accuracy non stream model final test accuracy non stream model running script would also try export validate issue related running best,issue,positive,positive,neutral,neutral,positive,positive
611312897,"I see, in my tests tfv2 was running with disabled eager mode by default, that is why I did not see this error. Goog catch! Tomorrow a patch (with disabled eager execution) will be submitted for all unit tests.

If you are building a key words detector or sounds detector with GRU model then you can try:

python -m kws_streaming.train.model_train_eval
--data_url ''
--data_dir $DATA_PATH/
--train_dir $MODELS_PATH/gru/
--mel_upper_edge_hertz 7000
--how_many_training_steps 400,400,400
--learning_rate 0.0005,0.0001,0.00002
--window_size_ms 40.0
--window_stride_ms 20.0
--mel_num_bins 40
--dct_num_features 20
--resample 0.15
--alsologtostderr
--train 1
gru
--gru_units 400
--return_sequences 0
--dropout1 0.1
--units1 128,256
--act1 ""'linear','relu'""

I reduced number of training iterations to 400,400,400 just for test purpose.
This script will:
1. train GRU model and store it to /tf
2. Convert TF non streaming model to TFLite and store it to /tflite_non_stream
3. Convert TF non streaming model to TF streaming one (with external states) then convert it to TFLite and store it to tflite_stream_state_external
4 It can also can do a TFLite model quantization (if you need to reduce model size and make it faster)

The third step is what you are interested.

/Oleg.",see running disabled eager mode default see error catch tomorrow patch disabled eager execution unit building key detector detector model try python resample train dropout act reduced number training test purpose script train model store convert non streaming model store convert non streaming model streaming one external convert store also model quantization need reduce model size make faster third step interested,issue,positive,negative,neutral,neutral,negative,negative
611272267,"Hi Oleg,

I ran `python -m kws_streaming.models.utils_test` as it is and it fails with the error message I shared.  The default script tests dnn model. I got the test to pass by disabling eager execution:
```
if __name__ == '__main__':
    # tf.compat.v1.enable_resource_variables()
    disable_eager_execution()
    tf.test.main()
```
I am using tf2.1 and python 3.7.1 and have no GPUs. 

I ran the same script you shared and see no errors output from running kws_streaming.train.model_train_eval. 

My ultimate goal is to build a streaming GRU model with external state from this library and convert it to tflite, however these errors in convert_to_tflite() show up. 

",hi ran python error message default script model got test pas eager execution python ran script see output running ultimate goal build streaming model external state library convert however show,issue,negative,neutral,neutral,neutral,neutral,neutral
611256534,"Hi Abhipray,

Please let me know how do you test GRU model and configure GRU testing parameters?

To validate GRU model I run below scripts with env:
Python 3.6.9
tf.__version__ '2.1.0-rc1'

// path where kws_streaming lib is located
KWS_PATH=/tmp/kws_streaming

// path where data2 are unpacked
DATA_PATH=$KWS_PATH/data2

// path where models2 are unpacked
MODELS_PATH=$KWS_PATH/models2

python -m kws_streaming.train.model_train_eval \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/gru/ \
--mel_upper_edge_hertz 7000 \
--how_many_training_steps 10000,10000,10000 \
--learning_rate 0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 40 \
--dct_num_features 20 \
--resample 0.15 \
--alsologtostderr \
--train 0 \
gru \
--gru_units 400 \
--return_sequences 0 \
--dropout1 0.1 \
--units1 128,256 \
--act1 ""'linear','relu'""

It should start prinitng something like:
 tflite test accuracy, non stream model = 94.53% 200 out of 4890
 tflite test accuracy, non stream model = 96.51% 400 out of 4890
 tflite test accuracy, non stream model = 95.67% 600 out of 4890
etc..

After you run above script, please let me know if you see an error again?
Before running above script, I would also try
export CUDA_VISIBLE_DEVICES=-1
to validate that issue is related to running TFLite on CPU or GPU

Thanks,
Oleg.
",hi please let know test model configure testing validate model run python path path data unpacked path unpacked python resample train dropout act start something like test accuracy non stream model test accuracy non stream model test accuracy non stream model run script please let know see error running script would also try export validate issue related running thanks,issue,positive,positive,neutral,neutral,positive,positive
606340080,"Hi, I think the best way is to upgrade your gcc and g++ to >=9. For example, this [link](https://linuxize.com/post/how-to-install-gcc-compiler-on-ubuntu-18-04/) shows how to install and configure gcc and g++ compilers on ubuntu. 

I just tested it on a ubuntu machine. Upgrading to gcc9 and g++9 resolves the error you showed above. ",hi think best way upgrade example link install configure tested machine error,issue,negative,positive,positive,positive,positive,positive
606090595,"Unsubstantiated and unreproducable claims should not be considered factual.

On Mon, Mar 30, 2020, 9:09 AM ArashHosseini <notifications@github.com>
wrote:

> @appl044 <https://github.com/appl044> please be patient and remain
> factual. These comments are of no help.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/187#issuecomment-605988579>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AKLIUXASI2J76WKVK2BRVNLRKCKZLANCNFSM4KNFMTKA>
> .
>
",unsubstantiated considered factual mon mar wrote please patient remain factual help reply directly view,issue,positive,positive,neutral,neutral,positive,positive
605988579,@appl044 please be patient and remain factual. These comments are of no help.,please patient remain factual help,issue,positive,neutral,neutral,neutral,neutral,neutral
605973501,"Nobody really believes that their chatbot really exists. If it really
existed, we would have seen something by now.


On Sun, Mar 29, 2020 at 10:30 PM Igor Kosyanenko <notifications@github.com>
wrote:

> I just want to talk to meena, just have a chat that would be lovely
>
> Same here
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/187#issuecomment-605749833>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AKLIUXCBHPY4GBKU5AYM3ADRJ7753ANCNFSM4KNFMTKA>
> .
>
",nobody really really really would seen something sun mar wrote want talk chat would lovely reply directly view,issue,positive,positive,positive,positive,positive,positive
605749833,"> I just want to talk to meena, just have a chat that would be lovely

Same here",want talk chat would lovely,issue,positive,positive,positive,positive,positive,positive
605705200,"> Hi,
> 
> We just release the script/data for generating Amazon2M.
> Please download it [here](http://web.cs.ucla.edu/~chohsieh/data/Amazon2M.tar.gz), in which you can see how this data set is processed from the raw metadata.

Hello, the amazon 3M data on this [website](http://manikvarma.org/downloads/XC/XMLRepository.html) doesn't include metadata.json. It's not in both ""BoW Features"" and ""Raw text""",hi release generating please see data set raw hello data include bow raw text,issue,negative,negative,negative,negative,negative,negative
605193562,"> Hello, In your paper, It is mentioned that you used pytorch, but this is a Tensorflow implementation. Could you please share the pytorch code, I am not familiar with Tensorflow.Many thanks!

Hi, I think the official implementation of ClusterGCN is realized by Tensorflow and there is no Pytorch version. They said they implemented some other methods by Pytorch :) However, if you wanna realize ClusterGCN you can use  torch_geometric by ""from torch_geometric.data import ClusterData, ClusterLoader"" where they have been encapsulated as two easily invoked functions so you don't need to implement by yourself. (code for reference (https://github.com/snap-stanford/ogb/blob/master/examples/nodeproppred/proteins/cluster_gin.py))",hello paper used implementation could please share code familiar thanks hi think official implementation version said however wan na realize use import two easily need implement code reference,issue,positive,positive,positive,positive,positive,positive
604208357,Should be all set now. Thanks again! Do let me know if anything else comes up / how things go.,set thanks let know anything else come go,issue,negative,positive,positive,positive,positive,positive
604149776,"Yeah, I would not recommend using any form of schedule when using this optimizer (we manage it internally). That being said, this is an easy fix for improved compatibility. I will make it.",yeah would recommend form schedule manage internally said easy fix compatibility make,issue,positive,positive,positive,positive,positive,positive
604092150,"@lukemetz No problem! I'm already thankful that you provided a PyTorch version to begin with ;).

One more thing however, in my own version I also changed. 
```python
   defaults = dict(
        learning_rate=learning_rate,
        beta1=beta1,
```` 
and 
```python
        lr = get_cosine_learning_rate_fn(group[""training_steps""],
                                         group[""learning_rate""],
                                         group[""min_learning_rate_mult""],
                                         group[""constant_fraction""],
                                         group[""warmup_fraction""])(
                                             state[""step""])
```` 
to 
```python
   defaults = dict(
        lr=learning_rate,
        beta1=beta1,
```` 
and 
```python
        lr = get_cosine_learning_rate_fn(group[""training_steps""],
                                         group[""lr""],
                                         group[""min_learning_rate_mult""],
                                         group[""constant_fraction""],
                                         group[""warmup_fraction""])(
                                             state[""step""])
```` 
(i.e. `learning_rate` to `lr`)

Because in PyTorch learning rate schedulers will look for the `'lr'` group.
See https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py

Now I know that you're not supposed to use a learning rate scheduler with this optimizer (or am I wrong and are you still advised to use on?) but in my code base (and I assume in others as well) even a dummy learning rate scheduler will look for this parameter.

Do you want me to also create a pull-request or add an extra commit here for that?
",problem already thankful provided version begin one thing however version also python python group group group group group state step python python group group group group group state step learning rate look group see know supposed use learning rate wrong still advised use code base assume well even dummy learning rate look parameter want also create add extra commit,issue,positive,negative,negative,negative,negative,negative
604084839,"@Mxbonn : I am tracking down how to get this merged now. It might take a few days though, sorry! ",get might take day though sorry,issue,negative,negative,negative,negative,negative,negative
604076851,"Thank you so much! I am a bit of a pytorch noob so this is very much appreciated!
This looks good to me. LGTM",thank much bit much good,issue,positive,positive,positive,positive,positive,positive
603958058,"@digantamisra98 Good catch, thanks for pointing that out.",good catch thanks pointing,issue,positive,positive,positive,positive,positive,positive
603956694,@n2cholas Thanks for the link. I noticed the implementation might have some mistakes as pointed out in the issues on that repository. ,thanks link implementation might pointed repository,issue,negative,positive,positive,positive,positive,positive
603952315,"I'm looking forward to see the official code as well. There is an unofficial implementation by @MerHS (https://github.com/MerHS/SASA-pytorch), though it is marked obsolete and I'm not sure why.",looking forward see official code well unofficial implementation though marked obsolete sure,issue,negative,positive,positive,positive,positive,positive
603153973,"Hi all. I am facing the same issue, tried deleting and re-creating the TPUs but it didn't solve the problem. @dhruvsakalley @josecannete would you mind telling me how you solved the problem?",hi facing issue tried solve problem would mind telling problem,issue,negative,neutral,neutral,neutral,neutral,neutral
602476434,Please release so we can fork and iterate. Thank you.,please release fork iterate thank,issue,positive,neutral,neutral,neutral,neutral,neutral
601799693,"


try your demo ,but error as : why ? --
ERROR: /home/fuxiai/google-research/automl_zero/BUILD:286:1: C++ compilation of rule '//:fec_hashing' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 72 argument(s) skipped)




i already install the bazel 2.2.0 and C++>=14 , I  think may be you can give me the note on how to install C++>=14

THINK YOU VERY MUCH .








At 2020-03-15 14:59:47, ""Chen Liang"" <notifications@github.com> wrote:

Hi, as specified in README, please try bazel>=2.2.0 and C++>=14.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.",try error error compilation rule exit error command argument already install think may give note install think much liang wrote hi please try thread reply directly view,issue,negative,positive,positive,positive,positive,positive
600932266,"@gariel-google  Very appreciate your detailed instructions, and I'll test it and report the results!
But I've two little questions about inferring depth from EuRoC MAV dataset.
1. After reading the code and depth_in_the_wild paper, I think I need to specified the crop size for my input image to 256x384 in the command line.
2. Is the output depth image with real depth value in meters or the disparity map? That means do I need father steps to obtain the real depth value from the output depth image?

Since I'm not familiar with deep learning field, sorry for asking detailed questions.",appreciate detailed test report two little depth reading code paper think need crop size input image command line output depth image real depth value disparity map need father obtain real depth value output depth image since familiar deep learning field sorry detailed,issue,positive,positive,positive,positive,positive,positive
600832381,"Hi Dennis,
Sorry for the delayed response, I was sprinting for ECCV, then out of
office.
We did not release a script for depth inference, nor for its eval, because
they were release elsewhere and our code is compatible with the other
libraries.
This
<https://github.com/tensorflow/models/blob/master/research/struct2depth/inference.py>
script
form struct2depth should do the job. You'd have to change their model
<https://github.com/tensorflow/models/blob/master/research/struct2depth/model.py>
 to ours
<https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py>,
but otherwise everything should be compatible. Please let me know how it
goes!
Best,
Ariel

On Sat, Feb 29, 2020 at 2:19 AM Dennis Cychuang <notifications@github.com>
wrote:

> @gariel-google <https://github.com/gariel-google>
> Sorry for opening a new issue about rendering the depth image from the
> checkpoint you provide of EuRoc MAV dataset by the the sequence of image,
> since I can't find any script specific for inference the depth image for
> EuRoc MAV dataset.
>
> Wish you could provide some instruction about it, thanks in advanced!
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/208?email_source=notifications&email_token=ADXKUNGXHU3TLQ6QQJZ7KVTRFDQJLA5CNFSM4K6TWPGKYY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4IRLMCYA>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNBEGKARGTYPHR67ZHLRFDQJLANCNFSM4K6TWPGA>
> .
>
",hi sorry response office release script depth inference release elsewhere code compatible script form job change model otherwise everything compatible please let know go best ariel sat wrote sorry opening new issue rendering depth image provide sequence image since ca find script specific inference depth image wish could provide instruction thanks advanced reply directly view,issue,positive,positive,positive,positive,positive,positive
600780964,"@studennis911988 Sorry for the delayed response, I was sprinting for ECCV,
then out of office.
This
<https://github.com/tensorflow/models/blob/master/research/struct2depth/inference.py>
script form struct2depth should do the job. You'd have to change their model
<https://github.com/tensorflow/models/blob/master/research/struct2depth/model.py>
to ours
<https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py>,
but otherwise everything should be compatible. Please let me know how it
goes!


On Tue, Mar 17, 2020 at 5:43 PM Dennis Cychuang <notifications@github.com>
wrote:

> @gariel-google <https://github.com/gariel-google> Sorry for bothering
> you, I'm wondering is there father update for euroc dataset ?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61#issuecomment-600368086>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNDWJYJ5RNKUFCCGFZDRIAKLXANCNFSM4I73I7CA>
> .
>
",sorry response office script form job change model otherwise everything compatible please let know go tue mar wrote sorry wondering father update reply directly view,issue,negative,negative,negative,negative,negative,negative
600368086,"@gariel-google Sorry for bothering you, I'm wondering is there father update for euroc dataset ?",sorry wondering father update,issue,negative,negative,negative,negative,negative,negative
600241811,"Here are the variables names from the [KITTI checkpoint](https://www.googleapis.com/download/storage/v1/b/gresearch/o/depth_from_video_in_the_wild%2Fcheckpoints%2Fkitti_learned_intrinsics.zip?generation=1566493768934649&alt=media).
[kitti_checkpoint_var_names.txt](https://github.com/google-research/google-research/files/4345235/kitti_checkpoint_var_names.txt)

And here are the variable names after using the modified code from the previous comment.
[kitti_checkpoint_var_names_single_init.txt](https://github.com/google-research/google-research/files/4345242/kitti_checkpoint_var_names_single_init.txt)",variable code previous comment,issue,negative,negative,negative,negative,negative,negative
599370115,"When I use att_rnn .Another error happend
ValueError: slice index 24 of dimension 1 out of bounds. for '{{node streaming/tf_op_layer_strided_slice/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, _cloned=true, begin_mask=5, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=2](streaming/bidirectional_1/concat, streaming/tf_op_layer_strided_slice/strided_slice/stack, streaming/tf_op_layer_strided_slice/strided_slice/stack_1, streaming/tf_op_layer_strided_slice/strided_slice/stack_2)' with input shapes: [1,1,128], [3], [3], [3] and with computed input tensors: input[1] = <0 24 0>, input[2] = <0 25 0>, input[3] = <1 1 1>.",use error slice index dimension node input input input input input,issue,negative,neutral,neutral,neutral,neutral,neutral
599363874,"Now I meet another error when i update  TF to 2.2.0
RuntimeError: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (17920 != 16896)Node number 35 (RESHAPE) failed to prepare.",meet another error update node number reshape prepare,issue,negative,neutral,neutral,neutral,neutral,neutral
599168470,"I'm wondering why some checks failed as I just modified one line in the `README.md` file.

",wondering one line file,issue,negative,neutral,neutral,neutral,neutral,neutral
599167520,">The ""full algorithm"" involves searching the Setup, Predict, and Learn programs simultaneously, all starting from random instructions. The ""only-the-learning"" searches the Learn program starting from random instructions, while the Setup and Predict are initialized to the correct solution (forward pass of the linear or affine regressor). The latter is a simpler search problem than the former; we did this to show the trend.

Reply from Esteban Real.",full algorithm searching setup predict learn simultaneously starting random learn program starting random setup predict correct solution forward pas linear affine regressor latter simpler search problem former show trend reply real,issue,negative,negative,neutral,neutral,negative,negative
598130535,"This is resolved with updating my docker container to latest.

```
root@7700d0b7e682:/app/sejong/AutoML/google-research/automl_zero# g++ --version
g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

root@7700d0b7e682:/app/sejong/AutoML/google-research/automl_zero# bazel --version
bazel 2.2.0
```

Output is like this:

```
# ./run_demo.sh 
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: SHA256 (https://github.com/bazelbuild/rules_cc/archive/master.zip) = 07d957496cc77efa14c995e491ca78a7819c6c75fa285fa39616fd25136a1b3c
DEBUG: Rule 'rules_cc' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""07d957496cc77efa14c995e491ca78a7819c6c75fa285fa396
16fd25136a1b3c""
DEBUG: Call stack for the definition of repository 'rules_cc' which is a http_archive (rule definition at /root/.cache/bazel/_bazel_root/c7435eb598c0e52a3e7d09039ce632
a1/external/bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:26:1
INFO: SHA256 (https://github.com/abseil/abseil-cpp/archive/master.zip) = 1dcf8d59a1baeb683bc98339de375c69a581f6d7ab00bb47f13144dbe3c9f190
DEBUG: Rule 'com_google_absl' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""1dcf8d59a1baeb683bc98339de375c69a581f6d7ab0
0bb47f13144dbe3c9f190""
DEBUG: Call stack for the definition of repository 'com_google_absl' which is a http_archive (rule definition at /root/.cache/bazel/_bazel_root/c7435eb598c0e52a3e7d090
39ce632a1/external/bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:14:1
INFO: SHA256 (https://github.com/google/googletest/archive/master.zip) = 51048d8977c45dc5eff96355f186ae0ce9c281cc50f9dbfaff2f4d3bdc0eed6c
INFO: SHA256 (https://github.com/google/glog/archive/master.zip) = 43550f5e8140f819a889a3cd32fcf13888faaa6a441c179cd1163d96bdb82eb3
DEBUG: Rule 'com_google_googletest' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""51048d8977c45dc5eff96355f186ae0ce9c28
1cc50f9dbfaff2f4d3bdc0eed6c""
DEBUG: Call stack for the definition of repository 'com_google_googletest' which is a http_archive (rule definition at /root/.cache/bazel/_bazel_root/c7435eb598c0e52a3
e7d09039ce632a1/external/bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:20:1
DEBUG: Rule 'com_google_glog' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""43550f5e8140f819a889a3cd32fcf13888faaa6a441
c179cd1163d96bdb82eb3""
DEBUG: Call stack for the definition of repository 'com_google_glog' which is a http_archive (rule definition at /root/.cache/bazel/_bazel_root/c7435eb598c0e52a3e7d090
39ce632a1/external/bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:78:1
INFO: SHA256 (https://github.com/gflags/gflags/archive/master.zip) = 5a95d2354322ffd4b7b25b634ee1b23ce8167faa32a9c8fa06df0ef42ec9a174
DEBUG: Rule 'com_github_gflags_gflags' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""5a95d2354322ffd4b7b25b634ee1b23ce8
167faa32a9c8fa06df0ef42ec9a174""
DEBUG: Call stack for the definition of repository 'com_github_gflags_gflags' which is a http_archive (rule definition at /root/.cache/bazel/_bazel_root/c7435eb598c0e5
2a3e7d09039ce632a1/external/bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:3:1
INFO: Analyzed target //:run_search_experiment (38 packages loaded, 1474 targets configured).
INFO: Found 1 target...
INFO: From Compiling compute_cost.cc:
compute_cost.cc: In function 'double automl_zero::ComputeCost(const automl_zero::Instruction&)':
compute_cost.cc:173:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
INFO: From Compiling algorithm.cc:
algorithm.cc: In member function 'const std::vector<std::shared_ptr<const automl_zero::Instruction> >& automl_zero::Algorithm::ComponentFunction(automl_zero::Component
FunctionT) const':
algorithm.cc:172:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
algorithm.cc: In member function 'std::vector<std::shared_ptr<const automl_zero::Instruction> >* automl_zero::Algorithm::MutableComponentFunction(automl_zero::Componen
tFunctionT)':
algorithm.cc:184:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
INFO: From Compiling fec_hashing.cc:
In file included from fec_hashing.h:21:0,
                 from fec_hashing.cc:15:
executor.h:803:10: warning: variable templates only available with -std=c++14 or -std=gnu++14
     128> kOpIndexToExecuteFunction = {
          ^~~~~~~~~~~~~~~~~~~~~~~~~
INFO: From Compiling fec_cache.cc:
In file included from fec_cache.h:24:0,
                 from fec_cache.cc:15:
executor.h:803:10: warning: variable templates only available with -std=c++14 or -std=gnu++14
     128> kOpIndexToExecuteFunction = {
          ^~~~~~~~~~~~~~~~~~~~~~~~~
fec_cache.cc: In member function 'void automl_zero::LRUCache::MaybeResize()':
fec_cache.cc:92:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   while (list_.size() > max_size_) {
          ~~~~~~~~~~~~~^~~~~~~~~~~
INFO: From Compiling mutator.cc:
mutator.cc: In member function 'void automl_zero::Mutator::InsertInstruction(automl_zero::Algorithm*)':
mutator.cc:253:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       if (algorithm->setup_.size() >= setup_size_max_ - 1) return;
           ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~
mutator.cc:259:38: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       if (algorithm->predict_.size() >= predict_size_max_ - 1) return;
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~
mutator.cc:265:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       if (algorithm->learn_.size() >= learn_size_max_ - 1) return;
           ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~
In file included from definitions.h:34:0,
                 from instruction.h:24,
                 from algorithm.h:24,
                 from mutator.h:21,
                 from mutator.cc:15:
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h: In instantiation of 'std::__cxx11::string* google::Check_GTImpl(const T1&, const T2&, const char*) [with T1 = long unsigned int; T2 = int; std::__cxx11::string = std::__cxx11::basic_string<char>]':
mutator.cc:343:3:   required from here
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:732:32: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
 DEFINE_CHECK_OP_IMPL(Check_GT, > )
                                ^
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:148:53: note: in definition of macro 'GOOGLE_PREDICT_TRUE'
 #define GOOGLE_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                     ^
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:732:1: note: in expansion of macro 'DEFINE_CHECK_OP_IMPL'
 DEFINE_CHECK_OP_IMPL(Check_GT, > )
 ^~~~~~~~~~~~~~~~~~~~
mutator.cc:271:35: warning: 'component_function' may be used uninitialized in this function [-Wmaybe-uninitialized]
   InsertInstructionUnconditionally(op, component_function);
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~
mutator.cc:271:35: warning: 'op' may be used uninitialized in this function [-Wmaybe-uninitialized]
mutator.cc: In member function 'void automl_zero::Mutator::RemoveInstruction(automl_zero::Algorithm*)':
mutator.cc:293:35: warning: 'component_function' may be used uninitialized in this function [-Wmaybe-uninitialized]
   RemoveInstructionUnconditionally(component_function);
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~
mutator.cc: In member function 'void automl_zero::Mutator::TradeInstruction(automl_zero::Algorithm*)':
mutator.cc:317:35: warning: 'component_function' may be used uninitialized in this function [-Wmaybe-uninitialized]
   RemoveInstructionUnconditionally(component_function);
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~
mutator.cc:316:35: warning: 'op' may be used uninitialized in this function [-Wmaybe-uninitialized]
   InsertInstructionUnconditionally(op, component_function);
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~
INFO: From Compiling regularized_evolution.cc:
In file included from fec_cache.h:24:0,
                 from evaluator.h:28,
                 from regularized_evolution.h:26,
                 from regularized_evolution.cc:15:
executor.h:803:10: warning: variable templates only available with -std=c++14 or -std=gnu++14
     128> kOpIndexToExecuteFunction = {
          ^~~~~~~~~~~~~~~~~~~~~~~~~
INFO: From Compiling run_search_experiment.cc:
In file included from task_util.h:25:0,
                 from run_search_experiment.cc:25:
executor.h:803:10: warning: variable templates only available with -std=c++14 or -std=gnu++14
     128> kOpIndexToExecuteFunction = {
          ^~~~~~~~~~~~~~~~~~~~~~~~~
INFO: From Compiling task_util.cc:
In file included from task_util.h:25:0,
                 from task_util.cc:15:
executor.h:803:10: warning: variable templates only available with -std=c++14 or -std=gnu++14
     128> kOpIndexToExecuteFunction = {
          ^~~~~~~~~~~~~~~~~~~~~~~~~
task_util.cc: In function 'void automl_zero::FillTasksFromTaskSpec(const automl_zero::TaskSpec&, std::vector<std::unique_ptr<automl_zero::TaskInterface> >*)':
task_util.cc:93:11: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
         i < first_param_seeds.size() ? first_param_seeds[i] : param_seed + 1;
         ~~^~~~~~~~~~~~~~~~~~~~~~~~~~
task_util.cc:95:11: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
         i < first_data_seeds.size() ? first_data_seeds[i] : data_seed + 1;
         ~~^~~~~~~~~~~~~~~~~~~~~~~~~
In file included from definitions.h:34:0,
                 from task.h:27,
                 from task_util.h:22,
                 from task_util.cc:15:
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h: In instantiation of 'std::__cxx11::string* google::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = long unsigned int; T2 = int; std::__cxx11::string = std::__cxx11::basic_string<char>]':
task_util.h:393:3:   required from 'std::unique_ptr<automl_zero::Task<F> > automl_zero::CreateTask(automl_zero::IntegerT, automl_zero::RandomSeedT, automl_zero::RandomSeedT, const automl_zero::TaskSpec&) [with int F = 2; automl_zero::IntegerT = long int; automl_zero::RandomSeedT = unsigned int]'
task_util.cc:101:73:   required from here
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:727:32: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
 DEFINE_CHECK_OP_IMPL(Check_EQ, ==)  // Compilation error with CHECK_EQ(NULL, x)?
                                ^
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:148:53: note: in definition of macro 'GOOGLE_PREDICT_TRUE'
 #define GOOGLE_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                     ^
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:727:1: note: in expansion of macro 'DEFINE_CHECK_OP_IMPL'
 DEFINE_CHECK_OP_IMPL(Check_EQ, ==)  // Compilation error with CHECK_EQ(NULL, x)?
 ^~~~~~~~~~~~~~~~~~~~
task_util.cc:95:37: warning: 'data_seed' may be used uninitialized in this function [-Wmaybe-uninitialized]
         i < first_data_seeds.size() ? first_data_seeds[i] : data_seed + 1;
         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
task_util.cc:93:38: warning: 'param_seed' may be used uninitialized in this function [-Wmaybe-uninitialized]
         i < first_param_seeds.size() ? first_param_seeds[i] : param_seed + 1;
         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
INFO: From Compiling evaluator.cc:
In file included from fec_cache.h:24:0,
                 from evaluator.h:28,
                 from evaluator.cc:15:
executor.h:803:10: warning: variable templates only available with -std=c++14 or -std=gnu++14
     128> kOpIndexToExecuteFunction = {
          ^~~~~~~~~~~~~~~~~~~~~~~~~
evaluator.cc: In member function 'double automl_zero::Evaluator::Evaluate(const automl_zero::Algorithm&)':
evaluator.cc:89:26: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (IntegerT i = 0; i < tasks_.size(); ++i) {
                        ~~^~~~~~~~~~~~~~~
In file included from definitions.h:34:0,
                 from instruction.h:24,
                 from algorithm.h:24,
                 from evaluator.h:23,
                 from evaluator.cc:15:
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h: In instantiation of 'std::__cxx11::string* google::Check_GTImpl(const T1&, const T2&, const char*) [with T1 = long unsigned int; T2 = int; std::__cxx11::string = std::__cxx11::basic_string<char>]':
evaluator.cc:78:3:   required from here
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:732:32: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
 DEFINE_CHECK_OP_IMPL(Check_GT, > )
                                ^
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:148:53: note: in definition of macro 'GOOGLE_PREDICT_TRUE'
 #define GOOGLE_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                     ^
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:732:1: note: in expansion of macro 'DEFINE_CHECK_OP_IMPL'
 DEFINE_CHECK_OP_IMPL(Check_GT, > )
 ^~~~~~~~~~~~~~~~~~~~
In file included from evaluator.h:24:0,
                 from evaluator.cc:15:
task.h: In instantiation of 'void automl_zero::TaskIterator<F>::Next() [with int F = 2]':
executor.h:1300:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 2]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 2]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 2; automl_zero::IntegerT = long int]'
evaluator.cc:118:75:   required from here
task.h:317:26: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (current_example_ >= features_->size()) {
task.h: In instantiation of 'bool automl_zero::TaskIterator<F>::Done() const [with int F = 2]':
executor.h:1301:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 2]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 2]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 2; automl_zero::IntegerT = long int]'
evaluator.cc:118:75:   required from here
task.h:311:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     return current_epoch_ >= epochs_->size();
            ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
task.h: In instantiation of 'void automl_zero::TaskIterator<F>::Next() [with int F = 4]':
executor.h:1300:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 4]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 4]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 4; automl_zero::IntegerT = long int]'
evaluator.cc:122:75:   required from here
task.h:317:26: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (current_example_ >= features_->size()) {
task.h: In instantiation of 'bool automl_zero::TaskIterator<F>::Done() const [with int F = 4]':
executor.h:1301:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 4]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 4]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 4; automl_zero::IntegerT = long int]'
evaluator.cc:122:75:   required from here
task.h:311:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     return current_epoch_ >= epochs_->size();
            ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
task.h: In instantiation of 'void automl_zero::TaskIterator<F>::Next() [with int F = 8]':
executor.h:1300:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 8]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 8]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 8; automl_zero::IntegerT = long int]'
evaluator.cc:126:75:   required from here
task.h:317:26: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (current_example_ >= features_->size()) {
task.h: In instantiation of 'bool automl_zero::TaskIterator<F>::Done() const [with int F = 8]':
executor.h:1301:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 8]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 8]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 8; automl_zero::IntegerT = long int]'
evaluator.cc:126:75:   required from here
task.h:311:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     return current_epoch_ >= epochs_->size();
            ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
task.h: In instantiation of 'void automl_zero::TaskIterator<F>::Next() [with int F = 16]':
executor.h:1300:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 16]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 16]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 16; automl_zero::IntegerT = long int]'
evaluator.cc:130:76:   required from here
task.h:317:26: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (current_example_ >= features_->size()) {
task.h: In instantiation of 'bool automl_zero::TaskIterator<F>::Done() const [with int F = 16]':
executor.h:1301:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 16]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 16]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 16; automl_zero::IntegerT = long int]'
evaluator.cc:130:76:   required from here
task.h:311:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     return current_epoch_ >= epochs_->size();
            ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
task.h: In instantiation of 'void automl_zero::TaskIterator<F>::Next() [with int F = 32]':
executor.h:1300:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 32]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 32]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 32; automl_zero::IntegerT = long int]'
evaluator.cc:134:76:   required from here
task.h:317:26: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (current_example_ >= features_->size()) {
task.h: In instantiation of 'bool automl_zero::TaskIterator<F>::Done() const [with int F = 32]':
executor.h:1301:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 32]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 32]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 32; automl_zero::IntegerT = long int]'
evaluator.cc:134:76:   required from here
task.h:311:27: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     return current_epoch_ >= epochs_->size();
            ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
In file included from definitions.h:34:0,
                 from instruction.h:24,
                 from algorithm.h:24,
                 from evaluator.h:23,
                 from evaluator.cc:15:
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h: In instantiation of 'std::__cxx11::string* google::Check_LEImpl(const T1&, const T2&, const char*) [with T1 = long int; T2 = long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':
task.h:315:5:   required from 'void automl_zero::TaskIterator<F>::Next() [with int F = 2]'
executor.h:1300:5:   required from 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 2]'
executor.h:1027:44:   required from 'double automl_zero::Executor<F>::Execute(std::vector<double>*, std::vector<double>*) [with int F = 2]'
evaluator.cc:159:5:   required from 'double automl_zero::Evaluator::ExecuteImpl(const automl_zero::Task<F>&, automl_zero::IntegerT, const automl_zero::Algorithm&) [with int F = 2; automl_zero::IntegerT = long int]'
evaluator.cc:118:75:   required from here
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:729:32: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
 DEFINE_CHECK_OP_IMPL(Check_LE, <=)
                                ^
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:148:53: note: in definition of macro 'GOOGLE_PREDICT_TRUE'
 #define GOOGLE_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                     ^
bazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/default_glog_headers/glog/logging.h:729:1: note: in expansion of macro 'DEFINE_CHECK_OP_IMPL'
 DEFINE_CHECK_OP_IMPL(Check_LE, <=)
 ^~~~~~~~~~~~~~~~~~~~
In file included from fec_cache.h:24:0,
                 from evaluator.h:28,
                 from evaluator.cc:15:
executor.h: In member function 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 2]':
executor.h:1307:10: warning: 'fitness' may be used uninitialized in this function [-Wmaybe-uninitialized]
   double fitness;
          ^~~~~~~
executor.h: In member function 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 4]':
executor.h:1307:10: warning: 'fitness' may be used uninitialized in this function [-Wmaybe-uninitialized]
executor.h: In member function 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 32]':
executor.h:1307:10: warning: 'fitness' may be used uninitialized in this function [-Wmaybe-uninitialized]
executor.h: In member function 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 16]':
executor.h:1307:10: warning: 'fitness' may be used uninitialized in this function [-Wmaybe-uninitialized]
executor.h: In member function 'double automl_zero::Executor<F>::Validate(std::vector<double>*) [with int F = 8]':
executor.h:1307:10: warning: 'fitness' may be used uninitialized in this function [-Wmaybe-uninitialized]
Target //:run_search_experiment up-to-date:
  bazel-bin/run_search_experiment
INFO: Elapsed time: 48.343s, Critical Path: 30.05s
INFO: 410 processes: 410 processwrapper-sandbox.
INFO: Build completed successfully, 426 total actions
INFO: Running command line: bazel-bin/run_search_experiment '--search_experiment_spec=     search_tasks { 
      tasks {         scalar_linear_regression_task {}         features_size: 4         num_train_examples: 100         num_valid_examples: 100         num_tasks: 10         eval_type: RMS_ERROR       }     }     setup_ops: [SCALAR_CONST_SET_OP, VECTOR_INNER_PRODUCT_OP, SCALAR_DIFF_OP, SCALAR_PRODUCT_OP, SCALAR_VECTOR_PRODUCT_OP, VECTOR_SUM_OP]     predict_ops: [SCALAR_CONST_SET_OP, VECTOR_INNER_PRODUCT_OP, SCALAR_DIFF_OP, SCALAR_PRODUCT_OP, SCALAR_VECTOR_PRODUCT_OP, VECTOR_SUM_OP]     learn_ops: [SCALAR_CONST_SET_OP, VECTOR_INNER_PRODUCT_OP, SCALAR_DIFF_OP, SCALAR_PRODUCT_OP, SCALAR_VECTOR_PRODUCT_OP, VECTOR_SUM_OP]     learn_size_init: 8     setup_size_init: 10     predict_size_init: 2     fec {num_train_examples: 10 num_valid_examples: 10}     fitness_combination_mode: MEAN_FITNESS_COMBINATION     population_size: 1000     tournament_size: 10     initial_population: RANDOM_ALGORITHM     max_train_steps: 200000000     allowed_mutation_types {
      mutation_types: [ALTER_PARAM_MUTATION_TYPE, RANDOMIZE_INSTRUCTION_MUTATION_TYPE, RANDOMIZE_COMPONENT_FUNCTION_MUTATION_TYPE]     }     mutate_prob: 0.9     progress_every: 10000     ' '--final_tasks=     tasks {       scalar_linear_regression_task {}       features_size: 4       num_train_examples: 1000       num_valid_examples: 100       num_tasks: 100       eval_type: RMS_ERROR       data_seeds: [1000000]       param_seeds: [2000000]     }     ' '--max_experiments=0' --randomize_task_seeds '--select_tasks=     tasks {       scalar_linear_regression_task {}       features_size: 4       num_train_examples: 1000       num_valid_examples: 100       num_tasINFO: Build completed successfully, 426 total actions
Random seed = 2273927841
Running evolution experiment (on the T_search tasks)...
indivs=1000, elapsed_secs=0, mean=0.257662, stdev=0.098007, best fit=0.310922,
indivs=11000, elapsed_secs=1, mean=0.296089, stdev=0.071710, best fit=0.327938,
indivs=21000, elapsed_secs=1, mean=0.338708, stdev=0.151846, best fit=0.504255,
indivs=31000, elapsed_secs=2, mean=0.371073, stdev=0.160415, best fit=0.526472,
indivs=41000, elapsed_secs=2, mean=0.378918, stdev=0.182983, best fit=0.586537,
indivs=51000, elapsed_secs=3, mean=0.370984, stdev=0.194690, best fit=0.586740,
indivs=61000, elapsed_secs=3, mean=0.371626, stdev=0.205506, best fit=0.586801,
indivs=71000, elapsed_secs=3, mean=0.375018, stdev=0.194975, best fit=0.590414,
indivs=81000, elapsed_secs=4, mean=0.409807, stdev=0.192511, best fit=0.606084,
indivs=91000, elapsed_secs=5, mean=0.397361, stdev=0.196334, best fit=0.617153,
indivs=101000, elapsed_secs=5, mean=0.413907, stdev=0.270555, best fit=0.760055,
indivs=111000, elapsed_secs=6, mean=0.416433, stdev=0.267181, best fit=0.760055,
indivs=121000, elapsed_secs=6, mean=0.419480, stdev=0.263440, best fit=0.760055,
indivs=131000, elapsed_secs=7, mean=0.408816, stdev=0.265666, best fit=0.760055,
indivs=141000, elapsed_secs=7, mean=0.658768, stdev=0.389252, best fit=0.999924,
indivs=151000, elapsed_secs=8, mean=0.652780, stdev=0.407509, best fit=0.999924,
indivs=161000, elapsed_secs=8, mean=0.601341, stdev=0.426214, best fit=0.999924,
indivs=171000, elapsed_secs=8, mean=0.602564, stdev=0.419431, best fit=0.999924,
indivs=181000, elapsed_secs=9, mean=0.616774, stdev=0.408933, best fit=0.999924,
indivs=191000, elapsed_secs=9, mean=0.655147, stdev=0.411179, best fit=0.999924,
indivs=201000, elapsed_secs=10, mean=0.605780, stdev=0.431867, best fit=0.999924,
indivs=211000, elapsed_secs=10, mean=0.635949, stdev=0.428022, best fit=0.999924,
indivs=221000, elapsed_secs=11, mean=0.625224, stdev=0.421139, best fit=0.999924,
indivs=231000, elapsed_secs=11, mean=0.607377, stdev=0.429841, best fit=0.999924,
indivs=241000, elapsed_secs=11, mean=0.626994, stdev=0.419501, best fit=0.999924,
indivs=251000, elapsed_secs=12, mean=0.625516, stdev=0.424169, best fit=0.999924,
indivs=261000, elapsed_secs=12, mean=0.629389, stdev=0.416612, best fit=0.999924,
indivs=271000, elapsed_secs=13, mean=0.641109, stdev=0.397963, best fit=0.999924,
indivs=281000, elapsed_secs=13, mean=0.631153, stdev=0.410096, best fit=0.999924,
indivs=291000, elapsed_secs=14, mean=0.622386, stdev=0.407571, best fit=0.999924,
indivs=301000, elapsed_secs=14, mean=0.633474, stdev=0.416199, best fit=0.999924,
indivs=311000, elapsed_secs=14, mean=0.630881, stdev=0.409512, best fit=0.999924,
indivs=321000, elapsed_secs=15, mean=0.588123, stdev=0.423522, best fit=0.999924,
indivs=331000, elapsed_secs=15, mean=0.610386, stdev=0.418939, best fit=0.999924,
indivs=341000, elapsed_secs=16, mean=0.614032, stdev=0.417746, best fit=0.999924,
indivs=351000, elapsed_secs=16, mean=0.597986, stdev=0.433914, best fit=0.999924,
indivs=361000, elapsed_secs=17, mean=0.606158, stdev=0.425257, best fit=0.999924,
indivs=371000, elapsed_secs=17, mean=0.643484, stdev=0.415669, best fit=0.999924,
indivs=381000, elapsed_secs=18, mean=0.621760, stdev=0.423698, best fit=0.999924,
indivs=391000, elapsed_secs=18, mean=0.611664, stdev=0.418208, best fit=0.999924,
indivs=401000, elapsed_secs=18, mean=0.617776, stdev=0.414705, best fit=0.999924,
indivs=411000, elapsed_secs=19, mean=0.599760, stdev=0.419455, best fit=0.999924,
indivs=421000, elapsed_secs=19, mean=0.635844, stdev=0.415546, best fit=0.999924,
indivs=431000, elapsed_secs=20, mean=0.617242, stdev=0.418901, best fit=0.999924,
indivs=441000, elapsed_secs=20, mean=0.605797, stdev=0.420787, best fit=0.999924,
indivs=451000, elapsed_secs=21, mean=0.658004, stdev=0.413875, best fit=0.999924,
indivs=461000, elapsed_secs=21, mean=0.632975, stdev=0.416101, best fit=0.999924,
indivs=471000, elapsed_secs=21, mean=0.630914, stdev=0.409561, best fit=0.999924,
indivs=481000, elapsed_secs=22, mean=0.656346, stdev=0.406625, best fit=0.999924,
indivs=491000, elapsed_secs=22, mean=0.618857, stdev=0.420721, best fit=0.999924,
indivs=501000, elapsed_secs=23, mean=0.607742, stdev=0.427038, best fit=0.999924,
indivs=511000, elapsed_secs=23, mean=0.631585, stdev=0.409466, best fit=0.999924,
indivs=521000, elapsed_secs=24, mean=0.637856, stdev=0.407921, best fit=0.999924,
indivs=531000, elapsed_secs=24, mean=0.628126, stdev=0.412077, best fit=0.999924,
indivs=541000, elapsed_secs=25, mean=0.636830, stdev=0.414297, best fit=0.999924,
indivs=551000, elapsed_secs=25, mean=0.632956, stdev=0.406085, best fit=0.999924,
indivs=561000, elapsed_secs=26, mean=0.627347, stdev=0.411754, best fit=0.999924,
indivs=571000, elapsed_secs=26, mean=0.642939, stdev=0.414434, best fit=0.999924,
indivs=581000, elapsed_secs=26, mean=0.623814, stdev=0.417970, best fit=0.999924,
indivs=591000, elapsed_secs=27, mean=0.642933, stdev=0.414460, best fit=0.999924,
indivs=601000, elapsed_secs=27, mean=0.615805, stdev=0.414817, best fit=0.999924,
indivs=611000, elapsed_secs=28, mean=0.621599, stdev=0.419061, best fit=0.999924,
indivs=621000, elapsed_secs=28, mean=0.634927, stdev=0.419389, best fit=0.999924,
indivs=631000, elapsed_secs=29, mean=0.630413, stdev=0.412933, best fit=0.999924,
indivs=641000, elapsed_secs=29, mean=0.617456, stdev=0.421756, best fit=0.999924,
indivs=651000, elapsed_secs=30, mean=0.616297, stdev=0.422633, best fit=0.999924,
indivs=661000, elapsed_secs=30, mean=0.623868, stdev=0.424319, best fit=0.999924,
indivs=671000, elapsed_secs=30, mean=0.586118, stdev=0.426872, best fit=0.999925,
indivs=681000, elapsed_secs=31, mean=0.553973, stdev=0.445875, best fit=0.999925,
indivs=691000, elapsed_secs=31, mean=0.610995, stdev=0.427036, best fit=0.999925,
indivs=701000, elapsed_secs=32, mean=0.589184, stdev=0.432101, best fit=0.999925,
indivs=711000, elapsed_secs=32, mean=0.615177, stdev=0.415815, best fit=0.999925,
indivs=721000, elapsed_secs=33, mean=0.606812, stdev=0.426325, best fit=0.999925,
indivs=731000, elapsed_secs=33, mean=0.599124, stdev=0.430342, best fit=0.999925,
indivs=741000, elapsed_secs=34, mean=0.570718, stdev=0.436637, best fit=0.999925,
indivs=751000, elapsed_secs=34, mean=0.608367, stdev=0.435638, best fit=0.999925,
indivs=761000, elapsed_secs=34, mean=0.598017, stdev=0.430631, best fit=0.999925,
indivs=771000, elapsed_secs=35, mean=0.605815, stdev=0.425129, best fit=0.999925,
indivs=781000, elapsed_secs=35, mean=0.606116, stdev=0.423532, best fit=0.999925,
indivs=791000, elapsed_secs=36, mean=0.584681, stdev=0.427701, best fit=0.999925,
indivs=801000, elapsed_secs=36, mean=0.567159, stdev=0.430966, best fit=0.999925,
indivs=811000, elapsed_secs=37, mean=0.602772, stdev=0.430653, best fit=0.999925,
indivs=821000, elapsed_secs=37, mean=0.586737, stdev=0.431570, best fit=0.999925,
indivs=831000, elapsed_secs=38, mean=0.575602, stdev=0.431212, best fit=0.999925,
indivs=841000, elapsed_secs=38, mean=0.613113, stdev=0.432275, best fit=0.999925,
indivs=851000, elapsed_secs=39, mean=0.567290, stdev=0.436873, best fit=0.999925,
indivs=861000, elapsed_secs=39, mean=0.606541, stdev=0.432605, best fit=0.999925,
indivs=871000, elapsed_secs=39, mean=0.573645, stdev=0.432728, best fit=0.999925,
indivs=881000, elapsed_secs=40, mean=0.607933, stdev=0.427361, best fit=0.999925,
indivs=891000, elapsed_secs=40, mean=0.584814, stdev=0.431185, best fit=0.999925,
indivs=901000, elapsed_secs=41, mean=0.616949, stdev=0.426699, best fit=0.999925,
indivs=911000, elapsed_secs=41, mean=0.577200, stdev=0.440839, best fit=0.999925,
indivs=921000, elapsed_secs=42, mean=0.599291, stdev=0.433413, best fit=0.999925,
indivs=931000, elapsed_secs=42, mean=0.568398, stdev=0.435464, best fit=0.999925,
indivs=941000, elapsed_secs=43, mean=0.595542, stdev=0.436714, best fit=0.999925,
indivs=951000, elapsed_secs=43, mean=0.578625, stdev=0.437732, best fit=0.999925,
indivs=961000, elapsed_secs=44, mean=0.590917, stdev=0.433129, best fit=0.999925,
Experiment done. Retrieving candidate algorithm.
Search fitness for candidate algorithm = 0.999925
Evaluating candidate algorithm from experiment (on T_select tasks)... 
Select fitness for candidate algorithm = 1.000000
Select fitness is the best so far. 

Final evaluation of best algorithm (on unseen tasks)...
Final evaluation fitness (on unseen data) = 1.000000
Algorithm found: 
def Setup():
  s2 = s1 - s3
  s2 = s1 - s3
  s1 = s1 * s0
  v1 = v2 + v2
  v1 = v0 + v1
  s2 = -0.0723455
  v1 = s3 * v2
  v1 = s1 * v1
  s3 = s1 - s1
  v2 = v1 + v1
def Predict():
  s3 = 0.145016
  s1 = dot(v1, v0)
def Learn():
  s1 = s0 - s1
  v1 = s1 * v0
  s3 = s0 - s3
  v2 = v2 + v1
  v1 = s2 * v2
  s1 = s3 * s1
  s1 = s3 * s0
  s2 = s0 - s3
```",resolved docker container latest root version copyright free foundation free see source warranty even fitness particular purpose root version output like installation starting local server sha rule canonical reproducible form sha call stack definition repository rule definition sha rule canonical reproducible form sha call stack definition repository rule definition sha sha rule canonical reproducible form sha call stack definition repository rule definition rule canonical reproducible form sha call stack definition repository rule definition sha rule canonical reproducible form sha call stack definition repository rule definition target loaded found target function warning control end function member function warning control end function member function warning control end function file included warning variable available file included warning variable available member function warning comparison unsigned integer member function warning comparison unsigned integer return warning comparison unsigned integer return warning comparison unsigned integer return file included char long unsigned char warning comparison unsigned integer note definition macro define note expansion macro warning may used function warning may used function member function warning may used function member function warning may used function warning may used function file included warning variable available file included warning variable available file included warning variable available function warning comparison unsigned integer warning comparison unsigned integer file included char long unsigned char long unsigned warning comparison unsigned integer compilation error null note definition macro define note expansion macro compilation error null warning may used function warning may used function file included warning variable available member function warning comparison unsigned integer file included char long unsigned char warning comparison unsigned integer note definition macro define note expansion macro file included double double double long warning comparison unsigned integer size double double double long warning comparison unsigned integer return size double double double long warning comparison unsigned integer size double double double long warning comparison unsigned integer return size double double double long warning comparison unsigned integer size double double double long warning comparison unsigned integer return size double double double long warning comparison unsigned integer size double double double long warning comparison unsigned integer return size double double double long warning comparison unsigned integer size double double double long warning comparison unsigned integer return size file included char long long unsigned char double double double long warning comparison unsigned integer note definition macro define note expansion macro file included member function double warning may used function double fitness member function double warning may used function member function double warning may used function member function double warning may used function member function double warning may used function target time critical path build successfully total running command line build successfully total random seed running evolution experiment best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best best experiment done candidate algorithm search fitness candidate algorithm candidate algorithm experiment select fitness candidate algorithm select fitness best far final evaluation best algorithm unseen final evaluation fitness unseen data algorithm found setup predict dot learn,issue,positive,positive,positive,positive,positive,positive
597252109,"Hello,
See this repository for an example:
https://github.com/google-research/libsoftwaresync/tree/master/app/src/main/java/com/googleresearch/capturesync",hello see repository example,issue,negative,neutral,neutral,neutral,neutral,neutral
597251311,"Hello, Sorry for the delayed response. Can you send me the version of the OS you are using? You can check this by going to Settings->About Phone->Build number ",hello sorry response send version o check going build number,issue,negative,negative,negative,negative,negative,negative
596277781,"I hope this doesn't turn into the OpenAI GPT-2 ordeal. Where it took months for them to release their model because it was ""too dangerous"".",hope turn ordeal took release model dangerous,issue,negative,negative,negative,negative,negative,negative
594990377,"Hi, @NataliaDiaz! `tf.contrib` has been removed from TensorFlow 2.0, but `sparsemax` has been added to the [TensorFlow Addons](https://www.tensorflow.org/addons/api_docs/python/tfa/activations/sparsemax) package. Could you try swapping out the symbol names, and seeing if the `tensorflow_addons` equivalent works for you?",hi removed added package could try swapping symbol seeing equivalent work,issue,negative,neutral,neutral,neutral,neutral,neutral
590592904,"Understood, I hope that the legal department can clear the release in the future.
Meanwhile yes, the training code and IDs are sufficient for reproducibility, thank you.",understood hope legal department clear release future meanwhile yes training code sufficient reproducibility thank,issue,positive,positive,positive,positive,positive,positive
589661367,"I am hopeful that it is released as open source, for the benefit of the world.   I would implement it.  I would use it within my conversational chatbot, ElfChat [iOS and android].

It is likely I would have to implement a control script to choose the best response.  Some of the responses would be this engine, some would be responses that are more focused on the long term conversation, what has been learned about the individual talking, the mood, other factors and the incoming intent.    If a conversational avatar relies on strictly machine learning for a response, the choice will be based on the training data, which will be accurate, but not specific to the individual.  It will be the most likely response given the training. 

In a long term conversation,  this falls apart.  There has to be context within the reply, and this can be generated by manually created models.  
I propose that a good model is a mix where the avatar has a choice of responding with a machine learned response or a response that is more predetermined based on personal conversation factors.  
In reality, humans talk this way.  They respond based on what they know or they respond based on a pattern or flow that is sensible to the long term conversation.   

There are many use cases where one type of response [ML or predetermined] is preferable.  
",hopeful open source benefit world would implement would use within conversational android likely would implement control script choose best response would engine would long term conversation learned individual talking mood incoming intent conversational strictly machine learning response choice based training data accurate specific individual likely response given training long term conversation apart context within reply manually propose good model mix choice machine learned response response based personal conversation reality talk way respond based know respond based pattern flow sensible long term conversation many use one type response preferable,issue,positive,positive,positive,positive,positive,positive
589509853,"Hello, I also want to know the hyper-param of Amazon2M dataset.",hello also want know,issue,negative,neutral,neutral,neutral,neutral,neutral
589474266,"Hi, sorry for the delayed response. We would have to get clearance for that form legal, it turns out to be a bit more complicated than we thought. I hope that the training code and the YouTube8M IDs can be of some help meanwhile...",hi sorry response would get clearance form legal turn bit complicated thought hope training code help meanwhile,issue,positive,negative,negative,negative,negative,negative
589462107,"@hyeokhyen Figure 9 estimates shows the accuracy at which intrinsics are
estimated by our model. The limitation is explained in the paper.
Alternatively, we can impose constancy of the intrinsics over the entire
sequence, as we did in EuRoC, and to get very accurate results. This is as
accurate as we can go.

On Thu, Feb 20, 2020 at 4:51 PM Hyeokhyen Alan Kwon <
notifications@github.com> wrote:

> Any news for model used for the paper that can predict intrinsic parameter
> correctly?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNBNDSNTCNUXYVKUKILRD4QQRA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMRDPSI#issuecomment-589445065>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNH4THJHO6WZWASC2DTRD4QQRANCNFSM4IMPQMMA>
> .
>
",figure accuracy model limitation paper alternatively impose constancy entire sequence get accurate accurate go alan wrote news model used paper predict intrinsic parameter correctly reply directly view,issue,negative,positive,positive,positive,positive,positive
589445065,Any news for model used for the paper that can predict intrinsic parameter correctly?,news model used paper predict intrinsic parameter correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
587551716,"> Hi @hazimehh.
> 
> Very interesting with a tree ensemble layer for neural networks. Will there be a paper at some point comparing this to regular tree ensemble methods and neural networks with ""standard"" layers?
> 
> I apologize if this is slightly off topic for the pull request.
> 
> Best regards
> Mads

Hi Mads,

Yeah we will post the paper in the next few days. It has comparisons against gradient boosted trees and dense layers.",hi interesting tree ensemble layer neural paper point regular tree ensemble neural standard apologize slightly topic pull request best hi yeah post paper next day gradient dense,issue,positive,positive,positive,positive,positive,positive
587424108,"> Zhenyu, can you take a look. How do we build a custom kernel from source and make sure it is available with bazel

Done",take look build custom kernel source make sure available done,issue,negative,positive,positive,positive,positive,positive
587355178,"Hi @hazimehh. 

Very interesting with a tree ensemble layer for neural networks. Will there be a paper at some point comparing this to regular tree ensemble methods and neural networks with ""standard"" layers?

I apologize if this is slightly off topic for the pull request.

Best regards
Mads",hi interesting tree ensemble layer neural paper point regular tree ensemble neural standard apologize slightly topic pull request best,issue,positive,positive,positive,positive,positive,positive
587340851,"@gariel-google 
 Thanks for the clarification, and I'll look deep in to the code to find the bug.
By the way, which script should I run to use the checkpoints you provided for all 11 sequences to produce learned depth images?",thanks clarification look deep code find bug way script run use provided produce learned depth,issue,negative,positive,neutral,neutral,positive,positive
586558459,First of all why do we have to do a dot product and not a sparse dense matmul? According to the paper shouldn't train_adj be a sparse N x N matrix and feats be a dense N x F matrix?,first dot product sparse dense according paper sparse matrix dense matrix,issue,negative,positive,positive,positive,positive,positive
585520158,"As far as I understand, it gives true depth, because the EuRoC point clouds
are metric. Regarding the strange values: Negative values should be pruned
here:
https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/render_euroc_depth.py#L94,
so I don't know where they are coming form. If you could help in debugging
(by stepping into the script and seeing how come the negative depths
survive the filter) that would be of great help!

On Wed, Feb 12, 2020 at 5:30 PM Dennis Cychuang <notifications@github.com>
wrote:

> @gariel-google <https://github.com/gariel-google> Thanks for the detailed
> explanation!
>
> If I understand correctly, the render_euroc_depth.py is just a script that
> provides groundtruth depth map for evaluating the learned depth map which
> is produced by trained model as mentioned in the paper.
>
> So now I'm wondering how *can I use the downloaded EuRoC cam0 data to
> produced the depth map from the model* , moreover, is the depth map
> produced by model gives as the *true real world depth value* ?( since I
> have noticed that the groundtruth depth gives us some strange depth value
> like nagative 26 or positive 198)
>
> Thanks again!
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNDNDYW6EXAUAMHSXE3RCSPCTA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELTBK7Q#issuecomment-585504126>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNBYIALDHEL7XM5N4TDRCSPCTANCNFSM4I73I7CA>
> .
>
",far understand true depth point metric regarding strange negative know coming form could help stepping script seeing come negative survive filter would great help wed wrote thanks detailed explanation understand correctly script depth map learned depth map produced trained model paper wondering use cam data produced depth map model moreover depth map produced model true real world depth value since depth u strange depth value like positive thanks reply directly view,issue,positive,positive,positive,positive,positive,positive
585504126,"@gariel-google Thanks for the detailed  explanation!

 If I understand correctly, the render_euroc_depth.py is just a script that provides groundtruth depth map for evaluating the learned depth map which is produced by trained model as mentioned in the paper.

As now I'm wondering how _can I use the downloaded EuRoC cam0 data to produced the depth map from the model_(checkpoint you provide for all 11 sequences) , moreover, is the depth map produced by model gives as the **true real world depth value** ?( since I have noticed that the groundtruth depth gives us some strange depth value like nagative 26 or positive 198)

Thanks again!
 ",thanks detailed explanation understand correctly script depth map learned depth map produced trained model paper wondering use cam data produced depth map provide moreover depth map produced model true real world depth value since depth u strange depth value like positive thanks,issue,positive,positive,positive,positive,positive,positive
585362923,"render_euroc_depth.py renders a depth map from the grountruth, not form a
model / checkpoint. The groundtruth in EuRoC is in the form of a fused
point cloud, a single point cloud for the entire room. We need to render a
depth map out of that, given a certain camera position. That's
what render_euroc_depth.py does. It projects the fused point cloud onto a
given camera position, removes points that are out of frame and occluded
points (of course, for the latter we make some assumptions about the
angular width of a point depth discontinuity etc), and finally, resamples
the points more or less evenly in image space.

Since all these steps require some assumptions  approximations, we were
asked to provide a script that renders the groundtruth depths in the same
way as we did in the paper. Does it make sense?






On Tue, Feb 11, 2020 at 11:53 PM Dennis Cychuang <notifications@github.com>
wrote:

> @gariel-google <https://github.com/gariel-google> Thanks for the
> explanation, but I still have two questions about render the depth from
> EuRoc dataset.
> First of all, how can I get the *real* depth value from the depth image
> produced by the render_euroc_depth.py?
> Second, I had noticed that I don't have to provide model (checkpoints) to
> run the script(render_euroc_depth.py), though it still give the depth
> images, so I'm wondering if I use the wrong script ?
> Thanks for your attention!
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNBWHAWLJPEKGJ7GKM3RCOTHXA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELPZDXQ#issuecomment-585077214>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNFRQDDYSIG225PIGETRCOTHXANCNFSM4I73I7CA>
> .
>
",depth map form model form fused point cloud single point cloud entire room need render depth map given certain camera position fused point cloud onto given camera position frame course latter make angular width point depth discontinuity finally le evenly image space since require provide script way paper make sense tue wrote thanks explanation still two render depth first get real depth value depth image produced second provide model run script though still give depth wondering use wrong script thanks attention reply directly view,issue,positive,positive,neutral,neutral,positive,positive
585077214,"@gariel-google  Thanks for the explanation, but I still have two questions about render the depth from EuRoc dataset.
First of all, how can I get the **real** depth value from the depth image produced by the render_euroc_depth.py?
Second, I had noticed that I don't have to provide model (checkpoints) to run the script(render_euroc_depth.py),  though it still give the depth images, so I'm wondering if I use the wrong script ?
Thanks for your attention!",thanks explanation still two render depth first get real depth value depth image produced second provide model run script though still give depth wondering use wrong script thanks attention,issue,positive,positive,neutral,neutral,positive,positive
584391211,"The checkpoints and links are out.

On Fri, Feb 7, 2020 at 9:53 AM Ariel Gordon <gariel@google.com> wrote:

> Sure, I will add the links and instructions over the next few days.
>
> On Thu, Feb 6, 2020 at 5:28 PM Dennis Cychuang <notifications@github.com>
> wrote:
>
>> @gariel-google <https://github.com/gariel-google> Thanks for all your
>> hard work on releasing the code for Euroc dataset, but could you provide
>> some instructions about it in readme section?
>> Thanks in advanced!
>>
>> —
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNCT3GC354L6OUHVHP3RBS2MTA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELBM3FY#issuecomment-583191959>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/ADXKUNC2AUUKP5AH2I5MJFLRBS2MTANCNFSM4I73I7CA>
>> .
>>
>
",link ariel wrote sure add link next day wrote thanks hard work code could provide section thanks advanced reply directly view,issue,positive,positive,positive,positive,positive,positive
584250698,So building tensorfow 1.15.2 with CUDA 10.1 solved the problem for me (without changing any code).,building problem without code,issue,negative,neutral,neutral,neutral,neutral,neutral
583815418,"> @hubin111 Hi, have you been able to train TCC on the Penn Action dataset?

I made my own json file according to the author's annotation rules, and then trained the model. Please ask the author if you need it. Or ask the author's consent and I will send you these rules. Thank you.",hi able train action made file according author annotation trained model please ask author need ask author consent send thank,issue,positive,positive,positive,positive,positive,positive
583805458,"@hubin111 Hi, have you been able to train TCC on the Penn Action dataset?",hi able train action,issue,negative,positive,positive,positive,positive,positive
583724747,"@cwerner we released a bugfix yesterday on pytorch-tabnet, so everything should be working fine now!",yesterday everything working fine,issue,negative,positive,positive,positive,positive,positive
583527315,"Sure, I will add the links and instructions over the next few days.

On Thu, Feb 6, 2020 at 5:28 PM Dennis Cychuang <notifications@github.com>
wrote:

> @gariel-google <https://github.com/gariel-google> Thanks for all your
> hard work on releasing the code for Euroc dataset, but could you provide
> some instructions about it in readme section?
> Thanks in advanced!
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNCT3GC354L6OUHVHP3RBS2MTA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELBM3FY#issuecomment-583191959>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNC2AUUKP5AH2I5MJFLRBS2MTANCNFSM4I73I7CA>
> .
>
",sure add link next day wrote thanks hard work code could provide section thanks advanced reply directly view,issue,positive,positive,positive,positive,positive,positive
583296881,We’re looking into it but a study wasn’t really ever done to compare how fastai does on baselines (doing so **now**) and so plenty to be discovered :) ,looking study really ever done compare plenty discovered,issue,negative,positive,positive,positive,positive,positive
583296229,"Thanks @muellerzr ! Will jump right in...

And thanks for your work and contributions in the fast.ai community. Any intuition why the fast.ai model beats? Not that I am complaining ;-) ",thanks jump right thanks work community intuition model,issue,positive,positive,positive,positive,positive,positive
583295548,"The issue was how the data was being prepared. Everything was treated as categorical (the notebook above shows, also showing fastai’s model could beat their benchmark) ",issue data prepared everything categorical notebook also showing model could beat,issue,negative,neutral,neutral,neutral,neutral,neutral
583294662,"Hi there.

I figure this problem was resolved? Could you briefly explain if there was a bug in the PyTorch implementation of the code...? I'd be interested in using the PyTorch model and want to be sure that there's no implementation bug. Thanks",hi figure problem resolved could briefly explain bug implementation code interested model want sure implementation bug thanks,issue,positive,positive,positive,positive,positive,positive
583191959,"@gariel-google Thanks for all your hard work on releasing the code for Euroc dataset, but could you provide some instructions about it in readme section?
Thanks in advanced! ",thanks hard work code could provide section thanks advanced,issue,positive,positive,positive,positive,positive,positive
581270893,"To add to this, we understand that there’s a lot of hyperparameter tuning to be done. We also tried following both the ones suggested in the paper and our own hyper parameter search which only achieved ~62% accuracy. ",add understand lot tuning done also tried following paper hyper parameter search accuracy,issue,negative,neutral,neutral,neutral,neutral,neutral
581098512,@sjblim can you please help to share more info about 'feeding in a dummy input of all zero' as I would like to try out not feeding future input with existing dataset?,please help share dummy input zero would like try feeding future input,issue,positive,neutral,neutral,neutral,neutral,neutral
580986529,"I tried both way and get different loss value

first way :
    inputs[:, self.num_encoder_steps:, :] = 0

second way:
    future_inputs =tf.random_normal([1,5, 5, 5])

The loss should be the same right?",tried way get different loss value first way second way loss right,issue,negative,positive,positive,positive,positive,positive
580812028,"@cometta -- unfortunately the current implementation uses LSTM cells which require at least a single input at each time point. However, as a workaround, would feeding in a dummy input of all zeros do the trick?",unfortunately current implementation require least single input time point however would feeding dummy input trick,issue,negative,negative,negative,negative,negative,negative
580775070,"may i know when use this model to train on other dataset and we do not know the 
 future_inputs because it not yet happen . Is it possible to train using this model? Any example?",may know use model train know yet happen possible train model example,issue,negative,neutral,neutral,neutral,neutral,neutral
580741186,"They answered this into the blog: 

> While we have focused solely on sensibleness and specificity in this work, other attributes such as personality and factuality are also worth considering in subsequent works. Also, tackling safety and bias in the models is a key focus area for us, and given the challenges related to this, we are not currently releasing an external research demo. We are evaluating the risks and benefits associated with externalizing the model checkpoint, however, and may choose to make it available in the coming months to help advance research in this area.

https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html

Basically, no release date.",solely sensibleness specificity work personality factuality also worth considering subsequent work also tackling safety bias key focus area u given related currently external research associated model however may choose make available coming help advance research area basically release date,issue,positive,positive,positive,positive,positive,positive
580670381,"@hihihihiwsf -- please note that the model does separate the into the relevant categories (i.e. observed inputs, known future inputs,  targets and static inputs) as per:
https://github.com/google-research/google-research/blob/1fec1abdc5a54d6f2ac8b5ef3420b36d4e6509f0/tft/libs/tft_model.py#L795-L796

All historical time-varying information known at forecast time (i.e. observed inputs, known inputs and target) is then combined together and fed only into the LSTM encoder, i.e.:
https://github.com/google-research/google-research/blob/1fec1abdc5a54d6f2ac8b5ef3420b36d4e6509f0/tft/libs/tft_model.py#L800-L805

Known future information (i.e. known inputs only) is then fed into the LSTM decoder only:
https://github.com/google-research/google-research/blob/1fec1abdc5a54d6f2ac8b5ef3420b36d4e6509f0/tft/libs/tft_model.py#L814


",please note model separate relevant known future static per historical information known forecast time known target combined together fed known future information known fed,issue,negative,positive,positive,positive,positive,positive
579669773,"I also noticed bugs in calculating the L2 regularization.

In child.py:
https://github.com/google-research/google-research/blob/master/enas_lm/src/child.py#L318
```
self.l2_reg_loss = tf.add_n([tf.nn.l2_loss(w ** 2) for w in var_s])
```
The regularization is implemented as ""l4"" regularization.

However in fixed_lib is correct:
https://github.com/google-research/google-research/blob/master/enas_lm/src/tpu/fixed_lib.py#L409

@phthhieu
@melodyguan
",also calculating regularization regularization regularization however correct,issue,negative,neutral,neutral,neutral,neutral,neutral
579667998,"I noticed that the TPU version is correctly implemented, but the GPU version is not. The two versions are not consistent.
I suspected that the GPU version is not fully tested.

TPU related codes:
https://github.com/google-research/google-research/blob/master/enas_lm/src/tpu/fixed_lib.py#L277
GPU related codes:
https://github.com/google-research/google-research/blob/master/enas_lm/src/child.py#L125

@phthhieu
@melodyguan",version correctly version two consistent suspected version fully tested related related,issue,negative,positive,neutral,neutral,positive,positive
579226192,"Hi, I had a similiar issue. I was getting the following error:  _tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /home/bob/Dev/denoising/unprocessing/mirflickr/train/]
	 [[{{node list_files/assert_not_empty/Assert}}]]_

What fixed it was to change the command to:

_python -m unprocessing.train --model_dir=/home/bob/Dev/denoising/google-research/unprocessing/unprocessing_models/models/unprocessing_mir_flickr/ --train_pattern='/home/bob/Dev/denoising/unprocessing/mirflickr/train/&ast;.jpg' --test_pattern='/home/bob/Dev/denoising/unprocessing/mirflickr/test/&ast;.jpg'_

Note the additon of the *.jpg and comments around the train_pattern and test_pattern parameters if you are running this from the bash command line. You can remove the comments if you are running it from a pycharm configuration.",hi issue getting following error assertion pattern node fixed change command ast ast note around running bash command line remove running configuration,issue,negative,positive,neutral,neutral,positive,positive
575753935,"@fxtentacle I see, that's sad. Just one last thing, do you think it's worthwhile to compile tensorflow 1.15 with CUDA 10.1 instead (since it should not require to change the code substantially)?",see sad one last thing think compile instead since require change code substantially,issue,negative,negative,negative,negative,negative,negative
575751683,"@vibudh2209 I had to make a lot of changes because `tf.contrib.image.transform` was dropped from TF1 to TF2, so you basically need to re-implement parts from scratch. And if you get there, the mathematical issues kick in and make learning expensive or impossible. So in the end, I abandoned my quest to make the paper work, because it did not seem worthwhile, even after the crash, NaN and TF2 incompatibility issues were solved.",make lot basically need scratch get mathematical kick make learning expensive impossible end abandoned quest make paper work seem even crash nan incompatibility,issue,negative,negative,negative,negative,negative,negative
575747875,"@fxtentacle thanks for the info, is it possible for you to share your tensorflow wheel with me? 
Do you have to make a lot of changes to the code for switching to tensorflow 2.0?",thanks possible share wheel make lot code switching,issue,positive,positive,neutral,neutral,positive,positive
575675888,"@vibudh2209  I compiled TensorFlow 2.0 with VS2017 and CUDA10.1 on Windows. It took 2 hours and 30+ GB of hard disk space, and now the model works without NaNs.

That said, I am also seeing gradient propagation problems, so training this model from scratch probably involves some luck in dropping into the right local minima upon initialization, in addition to recompiling TF.",took hard disk space model work without said also seeing gradient propagation training model scratch probably luck dropping right local minimum upon addition,issue,negative,negative,neutral,neutral,negative,negative
575261565,"Hi @dmrd @cclauss @dbieber ,I checked the tokenizer_util.py file and I can try implementing the NltkAndPunctTokenizer() ;there are minor typo corrections in the same file (comment area),I can rectify those as well. ",hi checked file try minor typo file comment area rectify well,issue,negative,negative,neutral,neutral,negative,negative
572002553,"> @cognitiveRobot thanks for the reply.
> What kind of error messages did you get?

I meant to say loss is too much.",thanks reply kind error get meant say loss much,issue,negative,positive,positive,positive,positive,positive
571876437,"@cognitiveRobot thanks for the reply.
What kind of error messages did you get? 
",thanks reply kind error get,issue,positive,positive,positive,positive,positive,positive
571845376,"@fxtentacle Thanks for the reply!
@asherRSIP After tracing the code and print all loss terms, I think @fxtentacle is right. 
If I train the model by CPU, there is no Nan errors, and the inference results look correct.

I will try to train on GPU by CUDA 10.1, and if there are some results, I will post them here.
 btw, @asherRSIP what is your pre-trained model? Do you mean resnet-18 which train on ImageNet?
because I can not load the ImageNet pre-trained model, all I can do is train from scratch.

Thanks
",thanks reply tracing code print loss think right train model nan inference look correct try train post model mean train load model train scratch thanks,issue,positive,positive,neutral,neutral,positive,positive
571558665,"any progress?
I also get: ""LossTensor is inf or nan : Tensor had NaN values"" message when continuing to train the pre-trained model. ",progress also get nan tensor nan message train model,issue,negative,neutral,neutral,neutral,neutral,neutral
571519520,"@gariel-google Hi, any update on releasing the code and the checkpoint file? Thanks
",hi update code file thanks,issue,negative,positive,positive,positive,positive,positive
571518404,"@pigletsc yes. I did and can train. But loss is too much, as I didn't have the checkpoint file to initialize. ",yes train loss much file initialize,issue,negative,positive,positive,positive,positive,positive
570900197,"@pigletsc  Yes, except that the error occurs not only during training from scratch, but also during inferrence and/or when one continues training the pre-trained model for a few steps.",yes except error training scratch also one training model,issue,negative,neutral,neutral,neutral,neutral,neutral
569226699,"@fxtentacle 
Hi, Are you talking about a error message like ""LossTensor is inf or nan : Tensor had NaN values"" when training from scratch?

Correct me if I am wrong
Thanks",hi talking error message like nan tensor nan training scratch correct wrong thanks,issue,negative,negative,negative,negative,negative,negative
569222394,"@yanfengliu @gariel-google
Hi,
I got same issue, when training from scratch, there is Error ""LossTensor is inf or nan: Tensor had NaN values"" (it happend sometimes)

And I can not load the imageNet pretrained model like @yanfengliu siad, I download the imagenet pretrain model here: [Issue](https://github.com/facebookresearch/agem/issues/4)

Do you have any solution for above? Thanks",hi got issue training scratch error nan tensor nan sometimes load model like pretrain model issue solution thanks,issue,positive,positive,positive,positive,positive,positive
568958275,"@cognitiveRobot Btw, did you use all-black mask image like @yifuhu said?
Thanks!",use mask image like said thanks,issue,positive,positive,positive,positive,positive,positive
568958117,"@cognitiveRobot Hi, How did you train the model without the cam.txt file?
Just set the flag ""learn_intrinsics"" to false? 

I am trying to train on my own dataset, and thinking about set the flag ""learn_intrinsics"" to True, and set cam.txt content to nine zeros, is that correct?

Thanks!",hi train model without file set flag false trying train thinking set flag true set content nine correct thanks,issue,positive,positive,neutral,neutral,positive,positive
567575013,"I have faced the same problem. It worked for me when I installed the exact version of libraries in requirements.txt in the same order. I just did ""pip install -r requirements.txt"" in a new anaconda environment. I have used python=3.6, tensorflow-gpu==1.12, networkx==1.11.",faced problem worked exact version order pip install new anaconda environment used,issue,negative,positive,positive,positive,positive,positive
565681781,"@gariel-google Really appreciate for your hard work, however I have a little problem here.
Since I wish to get the real depth data from the disparity map produced by the model, I'm wondering is it right to calculate the depth by the traditional stereo disparity to depth formula ? Or there is another way to do that.
Thanks in advanced! ",really appreciate hard work however little problem since wish get real depth data disparity map produced model wondering right calculate depth traditional stereo disparity depth formula another way thanks advanced,issue,positive,positive,positive,positive,positive,positive
565665917,"Absolutely, we will release the model and the code. SO sorry it is taking
long, I am swamped with work, but I am committed to this. Sorry again for
the delay.

On Tue, Dec 10, 2019 at 8:09 PM Beniko_J <notifications@github.com> wrote:

> @gariel-google <https://github.com/gariel-google> Hi, thanks for your
> work! I am also looking forward to seeing your training code for the EuRoC
> dataset. BTW, is it possible for you to release the trained model for the
> EuRoC dataset too?
>
> Best regards
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNFZKDOYOIHNITXFXMTQYBRZLA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGRZ76Y#issuecomment-564371451>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNHQZKNLBTYXKEFPLCTQYBRZLANCNFSM4I73I7CA>
> .
>
",absolutely release model code sorry taking long work sorry delay tue wrote hi thanks work also looking forward seeing training code possible release trained model best reply directly view,issue,positive,positive,neutral,neutral,positive,positive
564960842,"I didn't see this, thanks. If I replace the `<q>` token with a newline character and use the ""rougeLsum"" type, I get results that are close enough.

Updated scores:
```
rouge_1_f_score      0.41897
rouge_2_f_score      0.19420
rouge_lsum_f_score   0.38942
```

Updated code:
```python
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)
aggregator = BootstrapAggregator()
candidates = [line.strip() for line in open(candidates_path, encoding='utf-8')]
references = [line.strip() for line in open(references_path, encoding='utf-8')]
assert len(candidates) == len(references)
for i, (c, r) in enumerate(zip(candidates, references)):
  c = c.replace('<q>', '\n')
  r = r.replace('<q>', '\n')
  aggregator.add_scores(scorer.score(r, c))
results = aggregator.aggregate()
```",see thanks replace token character use type get close enough code python scorer aggregator line open line open assert enumerate zip,issue,negative,positive,neutral,neutral,positive,positive
564632507,"the main difference should be sentence splitting, in PreSumm, there is a \<q\> token to indicate sentence boundaries, while rouge-score splits sentences by newline.  Rouge-L is very sensitive to sentence splitting.",main difference sentence splitting token indicate sentence sensitive sentence splitting,issue,negative,positive,positive,positive,positive,positive
564371451,"@gariel-google Hi, thanks for your work! I am also looking forward to seeing your training code for the EuRoC dataset. BTW, is it possible for you to release the trained model for the EuRoC dataset too?

Best regards",hi thanks work also looking forward seeing training code possible release trained model best,issue,positive,positive,positive,positive,positive,positive
562751053,"I also find the same problem, without segmentation result, the training progress cannot be continued.",also find problem without segmentation result training progress continued,issue,negative,neutral,neutral,neutral,neutral,neutral
561951608,i was thinking if the ckpts used for do_eval and do_predict are not same? or how to specify which ckpt will be used?,thinking used specify used,issue,negative,neutral,neutral,neutral,neutral,neutral
559322445,"> seems like xlarge config requires larger batch size on default. Mine didn't converge as well.

I set learning rate to 5e-6, it worked. 

",like batch size default mine converge well set learning rate worked,issue,positive,neutral,neutral,neutral,neutral,neutral
559297782,"After 70,000 epoch of training, by obtaining the aligned frame numbers, and extracting the mapping vector of each frame according to the index， and then visualizing with T-SNE, it is found that the T-sne visualization in Figure 8 of your paper is not available. The alignment vectors I get based on the reference video cannot overlap. Do you have any visual skills for setting up? Another question, I would like to ask for the annotation of Penn-action, get the json file of your thesis, what kind of annotation tool can be used for you, and how to get the json file, I hope to get your help.@debidatta",epoch training frame vector frame according found visualization figure paper available alignment get based reference video overlap visual setting another question would like ask annotation get file thesis kind annotation tool used get file hope get help,issue,positive,positive,positive,positive,positive,positive
559015268,seems like xlarge config requires larger batch size on default. Mine didn't converge as well.,like batch size default mine converge well,issue,positive,neutral,neutral,neutral,neutral,neutral
558963483,"Hi, it seems you use the albert base model. 
![image](https://user-images.githubusercontent.com/7105813/69701556-28420200-1128-11ea-9102-3a79632714f0.png)
As far as I know, the hidden size and transformer layers of albert are the same with bert base, which two are the main factors to inference time of bert-alike models. The reason of the albert model is much smaller is because it use shared parameters of  transformer layers.",hi use base model image far know hidden size transformer base two main inference time reason model much smaller use transformer,issue,negative,negative,negative,negative,negative,negative
558820477,"@dhruvsakalley @josecannete Google Cloud has had a bug internally for about a month.  They just pushed a fix.  I had to delete and re-recreate my TPUs, but now I see uninterrupted training.  It's unclear if your issue is due to the same bug, but you might try re-creating your TPUs.  (Don't just stop-start them, you have to delete them apparently).  https://github.com/tensorflow/tpu/issues/609#issuecomment-558819717  ",cloud bug internally month fix delete see uninterrupted training unclear issue due bug might try delete apparently,issue,negative,negative,neutral,neutral,negative,negative
558426504,"Thanks!  I ll try and that is really helpful😁

Also, could you share therun_amazon2m.sh script to reproduce the experiments results, thanks very much",thanks try really helpful also could share script reproduce thanks much,issue,positive,positive,positive,positive,positive,positive
558292858,"Hi,

We just release the script/data for generating Amazon2M.
Please download it [here](http://web.cs.ucla.edu/~chohsieh/data/Amazon2M.tar.gz), in which you can see how this data set is processed from the raw metadata.",hi release generating please see data set raw,issue,negative,negative,negative,negative,negative,negative
558288083,"@studennis911988 Sorry it's taking a bit long. This week is a holiday week,
and things take time to get approved, so I am aiming for getting the code
out in two weeks. Sorry again for the slowness.

On Fri, Nov 22, 2019 at 3:17 PM Dennis Cychuang <notifications@github.com>
wrote:

> Yes, it does. It helps a lot. We can release the code for training on
> EuRoC, which would read from time-sorted sequences of files, will use no
> masks and will learn the camera matrix. Please confirm that this is what
> you'd need. It may take a week or two to release.
> … <#m_8087750674146224854_>
>
> @gariel-google <https://github.com/gariel-google> Hi, thanks for your
> open-source code and can I know is there any schedule on releasing the code
> for training EuRoc dataset? Thanks in advanced.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNHHJ6PUMMHKPZV4573QVBR7FA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE7EPAA#issuecomment-557729664>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNAOZUGKFBFGJ7XLCW3QVBR7FANCNFSM4I73I7CA>
> .
>
",sorry taking bit long week holiday week take time get aiming getting code two sorry wrote yes lot release code training would read use learn camera matrix please confirm need may take week two release hi thanks code know schedule code training thanks advanced reply directly view,issue,positive,negative,neutral,neutral,negative,negative
557729664,"> Yes, it does. It helps a lot. We can release the code for training on EuRoC, which would read from time-sorted sequences of files, will use no masks and will learn the camera matrix. Please confirm that this is what you'd need. It may take a week or two to release.
> […](#)

@gariel-google Hi, thanks for your open-source code and can I know is there any schedule on releasing the code for training EuRoc dataset? Thanks in advanced.
",yes lot release code training would read use learn camera matrix please confirm need may take week two release hi thanks code know schedule code training thanks advanced,issue,positive,positive,positive,positive,positive,positive
557342954,"@liyingliu Yes, you're right. When is_train is set to false, the inference
results do not depend on the batch size. In the struct2depth code,
upon which our code is based, is_train is set to True at inference, and
with batch normalization the results will depend on batch size.

I don't know why the results would depend on batch size with randomized
later normalization, however, we did discover that we have a bug in the
code, and not all batch normalization layers were replaced by
randomized layer normalization. For example, here
<https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/depth_prediction_net.py#L83>
batch-norm
is still there. You made a good argument that batch-norm in inference mode
should also be insensitive to the batch size at inference time, so I""m not
sure I know the answer, but maybe it can give you a lead?


I believe that the reason for what you're seeing is a bug that we have in
the code.



This is true, if is_train is set to False at inference, the result should
not depend on the batch size

On Thu, Nov 21, 2019 at 11:15 AM juju <notifications@github.com> wrote:

> Regarding the batch size, we tested at 1. If batch-normalization is
> replaced everywhere by randomized layer normalization, the inference
> results do not depend on the batch size, as it should be. Due to an
> oversight, when we were obtaining the results for the paper, we left a few
> batch normalization layers in place. We fixed that since, but to be
> compatible with the checkpoints used for the paper, we needed to leave the
> batch-norms there, hence the dependence on the batch size.
>
> @gariel-google <https://github.com/gariel-google> Hi, I realize that for
> batch-normalization, in the code is_train is set to False during
> inference. Therefore, in this case, the depth result shouldn't depend on
> the batch size.
>
> Another experiment I did is: I trained a model with all the
> batch-normalization replaced by randomized layer normalization. However,
> when I evaluated this checkpoint, the depth result is still inconsistent
> with different batch size. It seems randomized layer normalization is the
> reason causing inconsistent results? Can you help to explain this? Thanks.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNHO2IBVECXQEC4DFXLQU3M4DA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE3KYZQ#issuecomment-557231206>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNHIH53PKKMBKSO4WITQU3M4DANCNFSM4IMPQMMA>
> .
>
",yes right set false inference depend batch size code upon code based set true inference batch normalization depend batch size know would depend batch size later normalization however discover bug code batch normalization layer normalization example still made good argument inference mode also insensitive batch size inference time sure know answer maybe give lead believe reason seeing bug code true set false inference result depend batch size juju wrote regarding batch size tested everywhere layer normalization inference depend batch size due oversight paper left batch normalization place fixed since compatible used paper leave hence dependence batch size hi realize code set false inference therefore case depth result depend batch size another experiment trained model layer normalization however depth result still inconsistent different batch size layer normalization reason causing inconsistent help explain thanks reply directly view,issue,positive,positive,neutral,neutral,positive,positive
557231206,"> Regarding the batch size, we tested at 1. If batch-normalization is replaced everywhere by randomized layer normalization, the inference results do not depend on the batch size, as it should be. Due to an oversight, when we were obtaining the results for the paper, we left a few batch normalization layers in place. We fixed that since, but to be compatible with the checkpoints used for the paper, we needed to leave the batch-norms there, hence the dependence on the batch size.

@gariel-google  Hi, I realize that for batch-normalization, in the code `is_train` is set to `False` during inference. Therefore, in this case, the depth result shouldn't depend on the batch size.  

Another experiment I did is: I trained a model with all the batch-normalization replaced by randomized layer normalization. However, when I evaluated this checkpoint, the depth result is still inconsistent with different batch size.  It seems randomized layer normalization is the reason causing inconsistent results? Can you help to explain this?  Thanks.",regarding batch size tested everywhere layer normalization inference depend batch size due oversight paper left batch normalization place fixed since compatible used paper leave hence dependence batch size hi realize code set false inference therefore case depth result depend batch size another experiment trained model layer normalization however depth result still inconsistent different batch size layer normalization reason causing inconsistent help explain thanks,issue,positive,negative,neutral,neutral,negative,negative
556505209,@gariel-google Thank you for your reply! The program doesn't crash. It just hangs. I tried to train with struct2depth and it had the same problem. Looks like this might have been inherited from it. I will keep investigating. ,thank reply program crash tried train problem like might keep investigating,issue,negative,neutral,neutral,neutral,neutral,neutral
554599364,"@yanfengliu I am sorry for the delayed response. Regarding loading the
imagene checkpoint, I admit that I am not 100% sure how to import it from
the pytorch checkpoint, but perhaps this code
https://github.com/tensorflow/models/blob/master/research/struct2depth/util.py#L215
can
provide a clue? These seems to be some special treatment there for the
batchnorm variables. Unfortunately I do not have access to the code that
was used to import the imagenet checkpoint, and I m not a pytorch user
myself, so I am sorry that I cannot help more.

Regarding the training getting stuck - could you explain more? Does it
crash? Or just hangs? We never observed anything like that.

On Fri, Nov 15, 2019 at 1:58 PM Yanfeng Liu <notifications@github.com>
wrote:

> A second issue is that the network gets stuck on certain examples when
> training from scratch. The exact input that it gets stuck on is different
> every run. @gariel-google <https://github.com/gariel-google>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/130?email_source=notifications&email_token=ADXKUNEBINQCOTUV42MJ5ITQT4LRBA5CNFSM4JNV7GX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEEG26LY#issuecomment-554544943>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNAYK5HDUBMLEIYLBV3QT4LRBANCNFSM4JNV7GXQ>
> .
>
",sorry response regarding loading admit sure import perhaps code provide clue special treatment unfortunately access code used import user sorry help regarding training getting stuck could explain crash never anything like wrote second issue network stuck certain training scratch exact input stuck different every run reply directly view,issue,negative,negative,neutral,neutral,negative,negative
554544943,A second issue is that the network gets stuck on certain examples when training from scratch. The exact input that it gets stuck on is different every run. @gariel-google ,second issue network stuck certain training scratch exact input stuck different every run,issue,negative,positive,positive,positive,positive,positive
554290387,"No, this is not mandatory. Actually, it is better to train from scratch for all models except xxlarge we our results showed section 4.7 (https://arxiv.org/pdf/1909.11942.pdf) that fine-tune from lower layer network did not help. Your training loss look reasonable. ",mandatory actually better train scratch except section lower layer network help training loss look reasonable,issue,negative,positive,positive,positive,positive,positive
554237985,"Upon further inspection, it appears to be having issues with all weights, not just batchnorm. I couldn't load the kernel weights for the convolution layers either. This is interesting because loading weights for inference worked just fine. ",upon inspection could load kernel convolution either interesting loading inference worked fine,issue,positive,positive,positive,positive,positive,positive
554223136,@cognitiveRobot did you have trouble loading pre-trained weights from the ImageNet checkpoint? It seems that the beta weights for batchnorms are not properly saved in the checkpoint file. ,trouble loading beta properly saved file,issue,negative,negative,neutral,neutral,negative,negative
553644632,"Sorry, here's the link to where we took the pretrained checkpoint from:
https://pytorch.org/docs/stable/torchvision/models.html



On Wed, Nov 13, 2019 at 3:00 PM Ariel Gordon <gariel@google.com> wrote:

> @cognitiveRobot the pretrained checkpoint from resnet18 that we used was
> taken from here <http://cognitiveRobot>. We cannot release it here
> because it was taken from somewhere else. Sorry about that...
>
> On Wed, Nov 13, 2019 at 12:40 AM juju <notifications@github.com> wrote:
>
>> @cognitiveRobot <https://github.com/cognitiveRobot> If your question is
>> to restore and train the checkpoint that the author provided. Then, you
>> could try to add a file named ""checkpoint"" in your checkpoint folder (the
>> folder contains .index, .meta .data-xxxx). The content in ""checkpoint"" file
>> can be the following:
>> model_checkpoint_path:""path_to_kitti_learned_intrinsics/model-248900""
>>
>> —
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUND5PBGYDGCEQCQEC6LQTO4QBA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOED5KOAQ#issuecomment-553297666>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/ADXKUNEFXKA322UKAGSW7WDQTO4QBANCNFSM4IMPQMMA>
>> .
>>
>
",sorry link took wed ariel wrote used taken release taken somewhere else sorry wed juju wrote question restore train author provided could try add file folder folder content file following reply directly view,issue,negative,negative,negative,negative,negative,negative
553644004,"@cognitiveRobot the pretrained checkpoint from resnet18 that we used was
taken from here <http://cognitiveRobot>. We cannot release it here because
it was taken from somewhere else. Sorry about that...

On Wed, Nov 13, 2019 at 12:40 AM juju <notifications@github.com> wrote:

> @cognitiveRobot <https://github.com/cognitiveRobot> If your question is
> to restore and train the checkpoint that the author provided. Then, you
> could try to add a file named ""checkpoint"" in your checkpoint folder (the
> folder contains .index, .meta .data-xxxx). The content in ""checkpoint"" file
> can be the following:
> model_checkpoint_path:""path_to_kitti_learned_intrinsics/model-248900""
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUND5PBGYDGCEQCQEC6LQTO4QBA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOED5KOAQ#issuecomment-553297666>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNEFXKA322UKAGSW7WDQTO4QBANCNFSM4IMPQMMA>
> .
>
",used taken release taken somewhere else sorry wed juju wrote question restore train author provided could try add file folder folder content file following reply directly view,issue,negative,negative,negative,negative,negative,negative
553598000,"The reason I ask this is because I'm finding my network suddenly plateaus early  in training. It's likely there's a bug but it would still be useful to know if you deem the finetuning step as you add layers an optimisation or necessary .

Many thanks!

![image](https://user-images.githubusercontent.com/10864294/68803502-c6799680-0657-11ea-938a-6d30f60dc247.png)
 ",reason ask finding network suddenly early training likely bug would still useful know deem step add necessary many thanks image,issue,positive,positive,positive,positive,positive,positive
553297666,"@cognitiveRobot  If your question is to restore and train the checkpoint that the author provided. Then, you could try to add a file named ""checkpoint"" in your checkpoint folder (the folder contains .index, .meta .data-xxxx). The content in ""checkpoint"" file can be the following:
model_checkpoint_path:""path_to_kitti_learned_intrinsics/model-248900""",question restore train author provided could try add file folder folder content file following,issue,negative,neutral,neutral,neutral,neutral,neutral
553244840,"Thanks for your reply.
I know that TPU could be great choice but I have 8 x Titan RTX 24GB.
So I just wanted to check if it's possible to run on multi GPU.",thanks reply know could great choice check possible run,issue,positive,positive,positive,positive,positive,positive
552970792,"If i am not wrong, pretrain on gpu for albert is time consuming and need lots of work before it can wot. But you can train it on TPU with existing codes and run 10 times faster or more than GPU.",wrong pretrain time consuming need lot work wot train run time faster,issue,negative,negative,negative,negative,negative,negative
552862574,@clarkkev Looking forward to your reply! Thanks.,looking forward reply thanks,issue,negative,positive,positive,positive,positive,positive
552757006,"training can be computed like this:  time of single step * how many steps for one epoch. 
how many steps for one epoch: total number of examples/ batch size",training like time single step many one epoch many one epoch total number batch size,issue,negative,positive,positive,positive,positive,positive
552376097,"> @liyingliu We just added the code for initialization from Imagenet, as well as some corrections in the hyperparameters for training. Unfortunately I was unable to obtain clearance to release the specific ImageNet checkpoint itself yet - sorry about that, things sometimes get more bureaucratic than expected. @buaafish Thanks for sharing your code, it's not easy for me though to spot a bug if there is one. Is there a chance you have an answer for me whether you were able to reproduce the depth inference metrics and/or whether the trajectories look reasonable? The Intrinsic matrix is so much off that I still suspect there is some sort of crude error somewhere. My next steps are to release the checkpoints we used for calculating odometry, with learned and given intrinsics, as well as the respective odometry trajectories. Then I can try to add a small piece of code for generating Fig. 9 in the paper for the intrinsics, which should hopefully resolve the intrinsics issue. Thank you all for your help debugging this, our goal is that everyone will be able to reproduce out results.
> […](#)
> On Mon, Aug 26, 2019 at 8:31 PM buaafish ***@***.***> wrote: @gariel-google <https://github.com/gariel-google> Test code like this. def main(_): seed = FLAGS.seed tf.set_random_seed(seed) np.random.seed(seed) random.seed(seed) if not gfile.Exists(FLAGS.checkpoint_dir): gfile.MakeDirs(FLAGS.checkpoint_dir) test_model = model.Model( boxify=FLAGS.boxify, data_dir=FLAGS.data_dir, file_extension=FLAGS.file_extension, is_training=False, foreground_dilation=FLAGS.foreground_dilation, learn_intrinsics=FLAGS.learn_intrinsics, learning_rate=FLAGS.learning_rate, reconstr_weight=FLAGS.reconstr_weight, smooth_weight=FLAGS.smooth_weight, ssim_weight=FLAGS.ssim_weight, translation_consistency_weight=FLAGS.translation_consistency_weight, rotation_consistency_weight=FLAGS.rotation_consistency_weight, batch_size=FLAGS.batch_size, img_height=FLAGS.img_height, img_width=FLAGS.img_width, weight_reg=FLAGS.weight_reg, depth_consistency_loss_weight=FLAGS.depth_consistency_loss_weight, queue_size=FLAGS.queue_size, input_file=FLAGS.input_file) _test(test_model, FLAGS.checkpoint_dir) def readImages(path, subdir, name): filename = name+"".png"" filepath = os.path.join(path, subdir, filename) im = Image.open(filepath) im_array = np.array(im) img1 = im_array[:, 0:416, :] img2 = im_array[:, 416:832, :] return img1[np.newaxis, :, :, :], img2[np.newaxis, :, :, :] def readMat(path, subdir, name): filename = name+""_cam.txt"" filepath = os.path.join(path, subdir, filename) data_temp=[] with open(filepath) as fdata: line=fdata.readline() data_temp.append([float(i) for i in line.split(',')]) return np.array(data_temp).reshape((3,3)) def readFileList(list_data_dir): with gfile.Open(list_data_dir) as f: frames = f.readlines() frames = [k.rstrip() for k in frames] subfolders = [x.split(' ')[0] for x in frames] frame_ids = [x.split(' ')[1] for x in frames] return subfolders, frame_ids def _test(test_model, checkpoint_dir): checkpointpath = ""./pretrained/cityscapes_kitti_learned_intrinsics/"" saver = tf.train.import_meta_graph(checkpointpath+'model-1000977.meta') checkpoint = checkpointpath+""model-1000977"" with tf.device('/cpu:0'): with tf.Session() as sess: sess.run(tf.local_variables_initializer()) sess.run(tf.global_variables_initializer()) logging.info('Loading checkpoint...') saver.restore(sess, checkpoint) print(reader.IMAGENET_MEAN) print(reader.IMAGENET_SD) logging.info('Reading data...') path = ""./kitti/format_data"" list_data_dir = ""test.txt"" subfolders, frame_ids = readFileList(list_data_dir) for (subdir, name) in zip(subfolders, frame_ids): img1, img2 = readImages(path, subdir, name) logging.info('Start testing...') ret = test_model.inference_egomotion(img1, img2,sess) print(ret[2]) mat = readMat(path, subdir, name) print(mat) logging.info('End testing...') if __name__ == '__main__': app.run(main) — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#46?email_source=notifications&email_token=ADXKUNHNC2IDOE7JDYVIPN3QGSN25A5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5GLS5Q#issuecomment-525121910>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ADXKUNHON74T2IYYN5MCBTTQGSN25ANCNFSM4IMPQMMA> .

@gariel-google did you get clearance to release the specific ImageNet checkpoint? I want to try with that. 
In my case, it not learning if I train without any checkpoint. If I provide checkpoint from https://github.com/google-research/google-research/tree/master/depth_from_video_in_the_wild#pretrained-checkpoints-and-respective-depth-metrics 
I get the following 

```
I1111 21:44:55.547858 140513105188608 train.py:167] Attempting to resume training from depth_from_video_in_the_wild/kitti_learned_intrinsics/...
I1111 21:44:55.548219 140513105188608 train.py:169] Last checkpoint found: None
I1111 21:44:55.548329 140513105188608 train.py:176] Training...
```
Not learning. 
Tried with 'learning_rate', 1e-4 to 1e-6  and 'batch_size', 4 and 2.
Any thoughts? Thanks.
",added code well training unfortunately unable obtain clearance release specific yet sorry sometimes get bureaucratic thanks code easy though spot bug one chance answer whether able reproduce depth inference metric whether look reasonable intrinsic matrix much still suspect sort crude error somewhere next release used calculating odometry learned given well respective odometry try add small piece code generating fig paper hopefully resolve issue thank help goal everyone able reproduce mon wrote test code like main seed seed seed seed path name path return path name path open float return return saver sess sess print print data path name zip path name testing ret sess print ret mat path name print mat testing main reply directly view mute thread get clearance release specific want try case learning train without provide get following resume training last found none training learning tried thanks,issue,positive,positive,neutral,neutral,positive,positive
552279232,"Hi Md,
It sounds like you got it right. You concatenate the triplets along the
width dimension, as exemplified here
<https://github.com/google-research/google-research/tree/master/depth_from_video_in_the_wild/data_example/erfurt_93>
.

On Sun, Nov 10, 2019 at 2:19 PM Md Z Hossain <notifications@github.com>
wrote:

> @gariel-google <https://github.com/gariel-google> I going to train on my
> images, but not sure exactly how to make samples.
> I have sequence, lets say, img1, img2, img3, img4, img5, img6...
> sample1: img1, img2, img3
> sample2: img2, img3, img4
> sample3: img3, img4, img5
> and so on
>
> I appreciate your reply.
> Thanks
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNFQ4PPEOMBKOXPLMRLQTCCFPA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDVIPDY#issuecomment-552241039>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNFEEFNA3UV4KMXJGO3QTCCFPANCNFSM4I73I7CA>
> .
>
",hi like got right concatenate along width dimension sun wrote going train sure exactly make sequence say sample sample sample appreciate reply thanks reply directly view,issue,positive,positive,positive,positive,positive,positive
552241039,"@gariel-google I going to train on my images, but not sure exactly how to make samples.  
I have sequence, lets say, img1, img2, img3, img4, img5, img6...
sample1: img1, img2, img3
sample2: img2, img3, img4
sample3: img3, img4, img5
and so on

I appreciate your reply.
Thanks",going train sure exactly make sequence say sample sample sample appreciate reply thanks,issue,positive,positive,positive,positive,positive,positive
550503594,Yeah. It works with python2. Thanks again @gariel-google ,yeah work python thanks,issue,positive,positive,positive,positive,positive,positive
550503289,@yifuhu yeah. I did it. I managed to train using the current code. ,yeah train current code,issue,negative,neutral,neutral,neutral,neutral,neutral
550485686,"I guess you can create an all-black mask image for every input frame. And use struct2depth's gen_data scripts to generate the training data (horizontally stacked frames). Depending on your dataset, the camera matrix can be calculated by rescaling the provided ground truth intrinsics. Or you can simply set the flag  ""learn_intrinsics"" to False. ",guess create mask image every input frame use generate training data horizontally depending camera matrix calculated provided ground truth simply set flag false,issue,positive,negative,negative,negative,negative,negative
550406553,Seems `tf.train.exponential_decay` was removed in TF2. Can you try replacing `tf.train.exponential_decay` with `tf.compat.v1.train.exponential_decay`? I'll add a more permanent TF2 fix in some time. Thanks for finding this.,removed try add permanent fix time thanks finding,issue,negative,positive,positive,positive,positive,positive
550403417," In the train learning rate, choose to set ""exp_decay"" in the config .py, however in Utils.py line 122, in <lambda>
    lr_fn = lambda lr, global_step: tf.train.exponential_decay(
AttributeError: module 'tensorflow._api.v2.compat.v2.train' has no attribute 'exponential_decay'
my tensorflow is tensorflow-gpu==2.0.0, Can you tell me why I am getting an error: AttributeError and can tell you the configuration. I am sorry to bother you.",train learning rate choose set however line lambda lambda module attribute tell getting error tell configuration sorry bother,issue,negative,negative,negative,negative,negative,negative
550125289,Close it as already have an exact same issue there https://github.com/google-research/google-research/issues/108,close already exact issue,issue,negative,positive,positive,positive,positive,positive
550053954,"Sorry about that, this is probably due to a python 2 / python 3 issue (
https://www.tutorialspoint.com/What-is-the-difference-between-dict-items-and-dict-iteritems-in-Python).
I seems unrelated to our code, I guess you can use python 2 or use a more
advanced version of tensorflow?

On Tue, Nov 5, 2019 at 2:21 PM Md Z Hossain <notifications@github.com>
wrote:

> Yes. That's the code. :) I would be grateful if you release them. Thanks.
> By this time, I made segmentation masks (easy for me, as there is no
> moving objects in my case) and fake camera matrix (by guessing the focal
> length and the principal point).
>
> Unfortunately, it's not working. The error message I get
>
> I1106 11:13:31.460314 140258611402496 train.py:164] Attempting to resume training from depth_from_video_in_the_wild/kitti_learned_intrinsics/model-248900...
> I1106 11:13:31.460670 140258611402496 train.py:166] Last checkpoint found: None
> I1106 11:13:31.460769 140258611402496 train.py:173] Training...
> INFO:tensorflow:Error reported to Coordinator: <class 'AttributeError'>, 'dict' object has no attribute 'iteritems'
> I1106 11:14:45.539946 140258611402496 coordinator.py:224] Error reported to Coordinator: <class 'AttributeError'>, 'dict' object has no attribute 'iteritems'
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNBBJTNWTKDREPFYXE3QSHWVRA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDERLUY#issuecomment-550049235>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNBKULTW3ILIVRRIWM3QSHWVRANCNFSM4I73I7CA>
> .
>
",sorry probably due python python issue unrelated code guess use python use advanced version tue wrote yes code would grateful release thanks time made segmentation easy moving case fake camera matrix guessing focal length principal point unfortunately working error message get resume training last found none training error class object attribute error class object attribute reply directly view,issue,negative,negative,neutral,neutral,negative,negative
550049235,"Yes. That's the code. :) I would be grateful if you release them. Thanks.
By this time, I made segmentation masks (easy for me, as there is no moving objects in my case) and fake camera matrix (by guessing the focal length and the principal point).

Unfortunately, it's not working. The error message I get
```
I1106 11:13:31.460314 140258611402496 train.py:164] Attempting to resume training from depth_from_video_in_the_wild/kitti_learned_intrinsics/model-248900...
I1106 11:13:31.460670 140258611402496 train.py:166] Last checkpoint found: None
I1106 11:13:31.460769 140258611402496 train.py:173] Training...
INFO:tensorflow:Error reported to Coordinator: <class 'AttributeError'>, 'dict' object has no attribute 'iteritems'
I1106 11:14:45.539946 140258611402496 coordinator.py:224] Error reported to Coordinator: <class 'AttributeError'>, 'dict' object has no attribute 'iteritems'
```

",yes code would grateful release thanks time made segmentation easy moving case fake camera matrix guessing focal length principal point unfortunately working error message get resume training last found none training error class object attribute error class object attribute,issue,negative,negative,neutral,neutral,negative,negative
550044327,"Yes, it does. It helps a lot. We can release the code for training on
EuRoC, which would read from time-sorted sequences of files, will use no
masks and will learn the camera matrix. Please confirm that this is what
you'd need. It may take a week or two to release.

On Tue, Nov 5, 2019 at 1:53 PM Md Z Hossain <notifications@github.com>
wrote:

> @gariel-google <https://github.com/gariel-google> thanks for your quick
> reply. Looks like this current training code will only work if we provide
> the following three items.
>
>    1. a png file which is a stitched image from three consecutive frames.
>    2. a camera matrix file.
>    3. a png file is a mask image for moving objects.
>
> But, I have only image frames.
>
> In the paper, I find
> @ 5.1 EuRoC Micro Aerial Vehicle Dataset:
> .............. Vicon scene 3d scans, and camera calibration, we only used
> the monocular videos for
> training....
> So, I thought you have some other training code for only images frames.
> Does it make sense?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNAV4A5JCNZBNAK2BVTQSHTNNA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDEO2SY#issuecomment-550038859>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNDTQAX2QPHDOMTCSKTQSHTNNANCNFSM4I73I7CA>
> .
>
",yes lot release code training would read use learn camera matrix please confirm need may take week two release tue wrote thanks quick reply like current training code work provide following three file image three consecutive camera matrix file file mask image moving image paper find micro aerial vehicle scene camera calibration used monocular training thought training code make sense reply directly view,issue,positive,positive,positive,positive,positive,positive
550038859,"@gariel-google thanks for your quick reply. Looks like this current training code will only work if we provide the following three items.
1. a png file which is a stitched image from three consecutive frames.
2. a camera matrix file.
3. a png file is a mask image for moving objects. 

But, I have only image frames.

In the paper, I find 
@ 5.1 EuRoC Micro Aerial Vehicle Dataset:
.............. Vicon scene 3d scans, and camera calibration, we only used the monocular videos for
training....
So, I thought you have some other training code for only image frames. Does it make sense?",thanks quick reply like current training code work provide following three file image three consecutive camera matrix file file mask image moving image paper find micro aerial vehicle scene camera calibration used monocular training thought training code image make sense,issue,positive,positive,positive,positive,positive,positive
550027501,@TengFeiHan0 I didn't try with kitti datasets. I was trying with my own data. Can you show the error? ,try trying data show error,issue,negative,neutral,neutral,neutral,neutral,neutral
550023037," Hi Md Z Hossain,
Thanks for reaching out.
I'm not sure I understand your question - image sequences only as opposed
to what? You mean without segmentation masks? Our code otherwise is image
sequences only.

On Tue, Nov 5, 2019, 12:09 PM Md Z Hossain <notifications@github.com> wrote:

> @gariel-google <https://github.com/gariel-google> Can you release the
> code to train the model from image sequence only. Thanks in advance.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNB637C6J7W4FYRNYODQSHHGRA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDEE2GY#issuecomment-549997851>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNF4D6LIH7VIWDHUAPTQSHHGRANCNFSM4I73I7CA>
> .
>
",hi thanks reaching sure understand question image opposed mean without segmentation code otherwise image tue wrote release code train model image sequence thanks advance reply directly view,issue,positive,positive,positive,positive,positive,positive
549997851,@gariel-google Can you release the code to train the model from image sequence only. Thanks in advance. ,release code train model image sequence thanks advance,issue,negative,positive,positive,positive,positive,positive
549939416,Very sorry for the delay. I am still working on releasing the labels. I'll let you know when they are ready. ,sorry delay still working let know ready,issue,negative,negative,negative,negative,negative,negative
549924542,"Thank you very much for your help, I have found the problem caused by the server.",thank much help found problem server,issue,negative,positive,positive,positive,positive,positive
549686528,"Here is the link to the paper
http://openaccess.thecvf.com/content_ICCV_2019/papers/Gordon_Depth_From_Videos_in_the_Wild_Unsupervised_Monocular_Depth_Learning_ICCV_2019_paper.pdf

And here is the supplementary material
http://openaccess.thecvf.com/content_ICCV_2019/supplemental/Gordon_Depth_From_Videos_ICCV_2019_supplemental.pdf

On Mon, Nov 4, 2019 at 10:36 PM Quei-An Chen <notifications@github.com>
wrote:

> @gariel-google <https://github.com/gariel-google> Could you release the
> full paper that you submitted to ICCV? I downloaded the paper from ICCV QR
> code but can't find the supplementary matrials. Thank you.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNGMKW4YZC47AM3UZE3QSEH6DA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDBX6ZI#issuecomment-549683045>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNGODL4BONFEUSTAYLLQSEH6DANCNFSM4I73I7CA>
> .
>
",link paper supplementary material mon wrote could release full paper paper code ca find supplementary thank reply directly view,issue,negative,positive,positive,positive,positive,positive
549683045,@gariel-google Could you release the full paper that you submitted to ICCV? I downloaded the paper from ICCV QR code but can't find the supplementary matrials. Thank you.,could release full paper paper code ca find supplementary thank,issue,negative,positive,positive,positive,positive,positive
549623393,"by the way, @cognitiveRobot, what kind of KITTI datasets did you use to test their source code? it would be failed when using the standard KITTI datasets.",way kind use test source code would standard,issue,positive,positive,positive,positive,positive,positive
549608193,"I am so sorry to ask a very stupid question, thank you for your kindness. @cognitiveRobot ",sorry ask stupid question thank kindness,issue,negative,negative,negative,negative,negative,negative
549524511,"No. Go to the folder where you saved ""depth_from_video_in_the_wild"" folder. then run the command from there. So in your case.
```
$ cd /home/tengfeihan/Music
$ python -m depth_from_video_in_the_wild.train --data_dir=depth_from_video_in_the_wild/data_example --checkpoint_dir=/tmp/my_experiment --train_steps=1
```",go folder saved folder run command case python,issue,negative,neutral,neutral,neutral,neutral,neutral
549331502,I would also like to know.,would also like know,issue,negative,neutral,neutral,neutral,neutral,neutral
549323621,"@cognitiveRobot thank you for your reply, I followed what you said, but it doesn't make sense.
![Screenshot from 2019-11-04 21-00-06](https://user-images.githubusercontent.com/38068286/68119470-22e90180-ff46-11e9-8fde-30865f9048e0.png)
",thank reply said make sense,issue,negative,neutral,neutral,neutral,neutral,neutral
549318234,"Go out of the depth_from_video_in_the_wild folder (i.e. to do the Music Folder) and then run the following
`python -m depth_from_video_in_the_wild.train --data_dir=depth_from_video_in_the_wild/data_example --checkpoint_dir=/tmp/my_experiment --train_steps=1`",go folder music folder run following python,issue,negative,neutral,neutral,neutral,neutral,neutral
549216645,"@stefan-it Sorry to reply late, it works, thank you!",sorry reply late work thank,issue,negative,negative,negative,negative,negative,negative
548831730,Thanks for sharing this. Are you setting the GPUs visible to this script or changing the number of GPUs while setting distribution strategy? By default the script should be using all GPUs on machine. Even then it shouldn't take so long for one iteration. Can you print the batch size being fetched by the dataset (total_batch_size in train.py)? Also can you share the full logs? ,thanks setting visible script number setting distribution strategy default script machine even take long one iteration print batch size fetched also share full,issue,positive,positive,positive,positive,positive,positive
548597414,"> Config seems correct. What are the training set metrics?
> 
> Can you run the code with NUM_FRAMES: 40 and check if that helps?

Ok, I will try. Thank you",correct training set metric run code check try thank,issue,negative,neutral,neutral,neutral,neutral,neutral
548559549,Very strange. Time per iteration is also quite high. Are you using a GPU to run the training script?,strange time per iteration also quite high run training script,issue,negative,positive,neutral,neutral,positive,positive
548343068,Could you try to remove the `.py` ending when calling the command 🤔 Then it should work 🤗,could try remove ending calling command work,issue,negative,neutral,neutral,neutral,neutral,neutral
548341324,"Thank you for replying.
The work of <SpanBERT: Improving Pre-training by Representing and Predicting Spans> shows that higher probability for shorter gram is useful, so, in general, I think the code of albert seems not wrong. Only the comments and flags confused readers.

",thank work improving higher probability shorter gram useful general think code wrong confused,issue,negative,negative,neutral,neutral,negative,negative
548229583,"@stefan-it Hi, I tried to call albert code from the parent directory of albert (i.e Projects/albert) , but still  `Error while finding module specification for 'albert.run_classifier_with_tfhub.py' (AttributeError: module 'albert.run_classifier_with_tfhub' has no attribute '__path__')` 
I think it's might not the directory problem, could you please give me some suggestions?",hi tried call code parent directory still error finding module specification module attribute think might directory problem could please give,issue,negative,neutral,neutral,neutral,neutral,neutral
548205596,"> Hi @gariel-google, are the models that you provide trained on images 416x128? When I tried inference with other resolutions it doesn't work well at all.
> 
> If it's indeed 416x128, have you tried training with higher resolutions? I know some previous work use 416x128 for training, but recently most methods use higher resolutions and experiments have demonstrated higher resolutions lead to better results. Is it something related to the GPU memory issue?

Hi, did you retrain for higher resolution? If so, can you share the training script? Thanks.",hi provide trained tried inference work well indeed tried training higher know previous work use training recently use higher higher lead better something related memory issue hi retrain higher resolution share training script thanks,issue,positive,positive,positive,positive,positive,positive
548176669,"Config seems correct. What are the training set metrics? 

Can you run the code with NUM_FRAMES: 40 and check if that helps?

",correct training set metric run code check,issue,negative,neutral,neutral,neutral,neutral,neutral
548156745,"ALIGNMENT:
  CYCLE_LENGTH: 2
  FRACTION: 1.0
  HUBER_DELTA: 0.1
  LABEL_SMOOTHING: 0.1
  LOSS_TYPE: classification
  NORMALIZE_INDICES: true
  SIMILARITY_TYPE: l2
  SOFTMAX_TEMPERATURE: 0.1
  STOCHASTIC_MATCHING: false
  VARIANCE_LAMBDA: 0.001
ALIGNMENT_SAL_TCN:
  ALIGNMENT_LOSS_WEIGHT: 0.33
  SAL_LOSS_WEIGHT: 0.33
AUGMENTATION:
  BRIGHTNESS: true
  BRIGHTNESS_MAX_DELTA: 0.12549019607843137
  CONTRAST: true
  CONTRAST_LOWER: 0.5
  CONTRAST_UPPER: 1.5
  HUE: false
  HUE_MAX_DELTA: 0.2
  RANDOM_CROP: false
  RANDOM_FLIP: true
  SATURATION: false
  SATURATION_LOWER: 0.5
  SATURATION_UPPER: 1.5
CHECKPOINT:
  SAVE_INTERVAL: 1000
CLASSIFICATION:
  DROPOUT_RATE: 0.0
  LABEL_SMOOTHING: 0.0
DATA:
  FRAME_LABELS: true
  FRAME_STRIDE: 15
  NUM_PREFETCH_BATCHES: 1
  NUM_STEPS: 2
  PER_CLASS: false
  PER_DATASET_FRACTION: 1.0
  RANDOM_OFFSET: 1
  SAMPLE_ALL_STRIDE: 1
  SAMPLING_STRATEGY: offset_uniform
  SHUFFLE_QUEUE_SIZE: 0
  STRIDE: 16
DATASETS:
- pouring
DECODER: false
DECODER_VISUAL: false
EVAL:
  BATCH_SIZE: 2
  CLASSIFICATION_FRACTIONS:
  - 0.1
  - 0.5
  - 1.0
  FEW_SHOT_NUM_EPISODES: 50
  FEW_SHOT_NUM_LABELED:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  FRAMES_PER_BATCH: 25
  KENDALLS_TAU_DISTANCE: sqeuclidean
  KENDALLS_TAU_STRIDE: 5
  NUM_FRAMES: 20
  TASKS:
  - classification
  VAL_ITERS: 20
IMAGE_SIZE: 224
LOGDIR: /home/cxu-serve/u1/zkou2/Code/tcc/save/cls_fixed
LOGGING:
  REPORT_INTERVAL: 100
MODEL:
  BASE_MODEL:
    LAYER: conv4_block3_out
    NETWORK: Resnet50_pretrained
  CONVGRU_EMBEDDER_MODEL:
    CONV_LAYERS:
    - - 512
      - 3
      - true
    - - 512
      - 3
      - true
    DROPOUT_RATE: 0.0
    GRU_LAYERS:
    - 128
    USE_BN: true
  CONV_EMBEDDER_MODEL:
    BASE_DROPOUT_RATE: 0.0
    BASE_DROPOUT_SPATIAL: false
    CAPACITY_SCALAR: 2
    CONV_LAYERS:
    - - 256
      - 3
      - true
    - - 256
      - 3
      - true
    EMBEDDING_SIZE: 128
    FC_DROPOUT_RATE: 0.1
    FC_LAYERS:
    - - 256
      - true
    - - 256
      - true
    FLATTEN_METHOD: max_pool
    L2_NORMALIZE: false
    USE_BN: true
  EMBEDDER_TYPE: conv
  L2_REG_WEIGHT: 1.0e-05
  RESNET_PRETRAINED_WEIGHTS: /home/cxu-serve/u1/zkou2/Code/tcc/repo/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5
  TRAIN_BASE: only_bn
  TRAIN_EMBEDDING: true
  VGGM:
    USE_BN: true
OPTIMIZER:
  LR:
    DECAY_TYPE: fixed
    EXP_DECAY_RATE: 0.97
    EXP_DECAY_STEPS: 1000
    INITIAL_LR: 0.0001
    MANUAL_LR_DECAY_RATE: 0.1
    MANUAL_LR_STEP_BOUNDARIES:
    - 5000
    - 10000
    NUM_WARMUP_STEPS: 0
  TYPE: AdamOptimizer
PATH_TO_TFRECORDS: /home/cxu-serve/p1/zkou2/ad/
SAL:
  DROPOUT_RATE: 0.0
  FC_LAYERS:
  - - 128
    - true
  - - 64
    - true
  - - 2
    - false
  LABEL_SMOOTHING: 0.0
  NUM_SAMPLES: 8
  SHUFFLE_FRACTION: 0.75
TCN:
  POSITIVE_WINDOW: 5
  REG_LAMBDA: 0.002
TRAIN:
  BATCH_SIZE: 2
  MAX_ITERS: 150000
  NUM_FRAMES: 20
  VISUALIZE_INTERVAL: 200
TRAINING_ALGO: alignment
",alignment fraction classification true false augmentation brightness true contrast true hue false false true saturation false classification data true false stride pouring false false classification logging model layer network true true true false true true true true false true true true fixed type sal true true false train alignment,issue,positive,positive,neutral,neutral,positive,positive
547802655,"You can control it with FLAGS.favor_shorter_ngram (line 474-475), but I also think that it was implemented conversely.

```python
if FLAGS.favor_shorter_ngram:
   pvals = pvals[::-1]
```",control line also think conversely python,issue,negative,neutral,neutral,neutral,neutral,neutral
547702447,"enviroment use: tf-gpu  1.12.0 , linux, python 3.6.8
there is the same error!
really hope to give me some advice ~",use python error really hope give advice,issue,negative,positive,positive,positive,positive,positive
547375597,"You should try to call the albert code from the root directory (not from the `albert` folder):

```bash
python -m albert.run_classifier_with_tfhub [params]
```

But there are some issues with the code, see for example https://github.com/google-research/google-research/issues/84

So I think we should give the albert team a little more time :)",try call code root directory folder bash python code see example think give team little time,issue,negative,negative,negative,negative,negative,negative
546699735,"Hi Soroosh, thanks for surfacing this question. I suspect this type of error means there is an issue with the data pipeline. 

a few troubleshooting steps:
- Try training with a fake test data to make sure the pipeline is working correctly (to do this set FLAGS.test_small_sample to True in train_resnet.py). If this works, the issue is with the food_101 dataset. You may want to check whether your tfrecords are stored using the same key expected by the tf.parse_single_example keys in data_input.py.
- If training still fails when with the FLAGS.test_small_sample to True, I will need more details to understand where the failure point happened.  Please provide more information using a similar format to the issue template used for the tensorflow repository. Here is an example of what fields would be helpful: https://github.com/tensorflow/tensorflow/issues/33724.

Thanks @Soroosh-Bsl.",hi thanks surfacing question suspect type error issue data pipeline try training fake test data make sure pipeline working correctly set true work issue may want check whether key training still true need understand failure point please provide information similar format issue template used repository example would helpful thanks,issue,positive,positive,neutral,neutral,positive,positive
546548581,@dattias Please take a look! Thanks!,please take look thanks,issue,positive,positive,positive,positive,positive,positive
545382477,"I have a simple [jupyter notebook](https://gist.github.com/kwea123/aa40fd240fcef284e70f9fdae446cd6a) for predicting depth on a single image.
For visual odometry between two frames, I didn't do and am not intending to do. You might need to figure out by yourself.",simple notebook depth single image visual odometry two intending might need figure,issue,negative,negative,neutral,neutral,negative,negative
544147867,I find the problem: the training codes use the variance instead of shot_noise and read_noise. You should replace the shot_noise and read_noise with variance.,find problem training use variance instead replace variance,issue,negative,neutral,neutral,neutral,neutral,neutral
542526312,"Hi Dong-Hyun,

Thanks for pointing out the issue. You are correct that these two values should be the same. We'll update the code soon. Next time, could you please post on https://github.com/google-research-datasets/dstc8-schema-guided-dialogue so that it's easy for us to keep track of the issues and get notifications.

Thanks,
Xiaoxue",hi thanks pointing issue correct two update code soon next time could please post easy u keep track get thanks,issue,positive,positive,positive,positive,positive,positive
542487204,"hi，Thank you very much for your prompt reply. Can you tell me after uploading the Penn_Action dataset, because you are particularly interested in your work. It is best to provide json files for the original dataset Penn_Action and Poring to help with future work.  I hope to get your consent. Thank you",much prompt reply tell particularly interested work best provide original poring help future work hope get consent thank,issue,positive,positive,positive,positive,positive,positive
542339685,"Hi @hubin111, I am working on releasing the PennAction labels. I apologize for the delay. In the meantime, the Pouring labels can be found in the [Pouring TFRecords ](https://github.com/google-research/google-research/blob/master/tcc/dataset_preparation/download_pouring_data.sh).",hi working apologize delay pouring found pouring,issue,negative,neutral,neutral,neutral,neutral,neutral
541980380,"There are missing * between train/ and .jpg, which is also applied for between test/ and .jpg. *.jpg

/google-research$ python -m unprocessing.train
--model_dir=/path/to/models/unprocessing_mirflickr
--train_pattern=/path/to/mirflickr/train/*.jpg --test_pattern=/path/to/mirflickr/test/*.jpg",missing also applied python,issue,negative,negative,negative,negative,negative,negative
541288408,We have used TPUEstimator because our code is derived from BERT but we haven't run it on TPU ourselves. We welcome pull requests for making the code TPU compatible.,used code derived run welcome pull making code compatible,issue,negative,positive,positive,positive,positive,positive
541276289,"> Solved by using command.
> 
> /google-research$ python -m unprocessing.train 
> --model_dir=/path/to/models/unprocessing_mirflickr 
> --train_pattern=/path/to/mirflickr/train/_.jpg --test_pattern=/path/to/mirflickr/test/_.jpg

Could you tell me what is your dataset catalog like? I set the train_pattern, but always have incomprehensible mistakes. Thank you!",command python could tell like set always incomprehensible thank,issue,positive,neutral,neutral,neutral,neutral,neutral
541273169,It works well with GPU. But it doesn't work with Cloud TPU. I don't think the authors have run it on TPU in spite of TPUEstimator in the code.,work well work cloud think run spite code,issue,negative,neutral,neutral,neutral,neutral,neutral
541232229,Thanks for reporting the issue. Are you able to train the model on GPU?,thanks issue able train model,issue,negative,positive,positive,positive,positive,positive
541207964,"Hi @kwea123,
Yes, it was trained at 416x128. The main reason that we stayed with 416x128
is that we wanted to not change too many things at the same time, so that
we can study the effect of each change we made - occlusion-awareness,
learning intrinsics, pooling datasets etc. For getting the best depth
prediction accuracy, I would definitely train at higher resolutions. We
could easily double the resolution (and have a batch size of 4 instead of
16) and everything would still fit on a p100 or a v100 GPU and train.

On Fri, Oct 11, 2019 at 8:31 AM kwea123 <notifications@github.com> wrote:

> Hi @gariel-google <https://github.com/gariel-google>, are the models that
> you provide trained on images 416x128? When I tried inference with other
> resolutions it doesn't work well at all.
>
> If it's indeed 416x128, have you tried training with higher resolutions? I
> know some previous work use 416x128 for training, but recently most methods
> use higher resolutions and experiments have demonstrated higher resolutions
> lead to better results. Is it something related to the GPU memory issue?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/61?email_source=notifications&email_token=ADXKUNCD5ZISHGFVPPUDRMTQOCL5HA5CNFSM4I73I7CKYY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4HRHXKGA>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADXKUNDYD7HIU3ZPJQVYHG3QOCL5HANCNFSM4I73I7CA>
> .
>
",hi yes trained main reason stayed change many time study effect change made learning getting best depth prediction accuracy would definitely train higher could easily double resolution batch size instead everything would still fit train wrote hi provide trained tried inference work well indeed tried training higher know previous work use training recently use higher higher lead better something related memory issue reply directly view,issue,positive,positive,positive,positive,positive,positive
534646884,"Please check out [these lines](https://github.com/google-research/google-research/blob/master/enas_lm/src/tpu/fixed_lib.py#L277).

We have options to average all loose ends, or to average all outputs.",please check average loose average,issue,negative,negative,negative,negative,negative,negative
531635906,Per-frame labels are only required to evaluate the model. You can still train a model without those labels. The default config has `CONFIG.DATA.FRAME_LABELS = True`. Setting it to False should allow you train a model and evaluate on 2 tasks: `algo_loss` and `kendalls_tau`.,evaluate model still train model without default true setting false allow train model evaluate,issue,negative,negative,neutral,neutral,negative,negative
531573820,"I thought that is unsupervised model, but it requires to label something :-\",thought unsupervised model label something,issue,negative,neutral,neutral,neutral,neutral,neutral
531412264,"@liyingliu we just released the odometry results, and code for generating
trajectories form checkpoints.
@buaafish intrinsics is coming next.

On Sat, Aug 31, 2019 at 4:18 AM juju <notifications@github.com> wrote:

> @gariel-google <https://github.com/gariel-google> Understand and thanks!
> Looking forward to exchanging the odometry results.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNCLGJXYI4KIP2SS55TQHJHRBA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5TKYBY#issuecomment-526822407>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNGKRK7EEGWIDAEODYDQHJHRBANCNFSM4IMPQMMA>
> .
>
",odometry code generating form coming next sat juju wrote understand thanks looking forward odometry reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
531075364,Thank you for the quick fix!! Tested again and now it's consistent.,thank quick fix tested consistent,issue,negative,positive,positive,positive,positive,positive
531028238,Thanks for catching this. This [commit](https://github.com/google-research/google-research/commit/3702c4396b6f045ffe0dd809f1261ddd149a4451) should take care of that.,thanks catching commit take care,issue,positive,positive,positive,positive,positive,positive
526822407,@gariel-google Understand and thanks! Looking forward to exchanging the odometry results.,understand thanks looking forward odometry,issue,negative,positive,positive,positive,positive,positive
526774117,"Found it, need to set `CONFIG.DATA.FRAME_LABELS = False`",found need set false,issue,negative,negative,negative,negative,negative,negative
526753963,"Hi @debidatta sorry to pin you again. Maybe you can help with this issue too when you have time: https://github.com/google-research/google-research/issues/52 as well. Basically do we actually need per frame label to train TCC? Seems like per frame label is just for evaluation, right? Thank you!

",hi sorry pin maybe help issue time well basically actually need per frame label train like per frame label evaluation right thank,issue,positive,negative,neutral,neutral,negative,negative
526351731,"@liyingliu We just added the code for initialization from Imagenet, as well
as some corrections in the hyperparameters for training. Unfortunately I
was unable to obtain clearance to release the specific ImageNet checkpoint
itself yet - sorry about that, things sometimes get more bureaucratic than
expected.

@buaafish Thanks for sharing your code, it's not easy for me though to spot
a bug if there is one. Is there a chance you have an answer for me whether
you were able to reproduce the depth inference metrics and/or whether the
trajectories look reasonable? The Intrinsic matrix is so much off that I
still suspect there is some sort of crude error somewhere.

My next steps are to release the checkpoints we used for calculating
odometry, with learned and given intrinsics, as well as the respective
odometry trajectories. Then I can try to add a small piece of code for
generating Fig. 9 in the paper for the intrinsics, which should hopefully
resolve the intrinsics issue.

Thank you all for your help debugging this, our goal is that everyone will
be able to reproduce out results.


On Mon, Aug 26, 2019 at 8:31 PM buaafish <notifications@github.com> wrote:

> @gariel-google <https://github.com/gariel-google> Test code like this.
>
> def main(_):
>   seed = FLAGS.seed
>   tf.set_random_seed(seed)
>   np.random.seed(seed)
>   random.seed(seed)
>
>   if not gfile.Exists(FLAGS.checkpoint_dir):
>     gfile.MakeDirs(FLAGS.checkpoint_dir)
>
>   test_model = model.Model(
>       boxify=FLAGS.boxify,
>       data_dir=FLAGS.data_dir,
>       file_extension=FLAGS.file_extension,
>       is_training=False,
>       foreground_dilation=FLAGS.foreground_dilation,
>       learn_intrinsics=FLAGS.learn_intrinsics,
>       learning_rate=FLAGS.learning_rate,
>       reconstr_weight=FLAGS.reconstr_weight,
>       smooth_weight=FLAGS.smooth_weight,
>       ssim_weight=FLAGS.ssim_weight,
>       translation_consistency_weight=FLAGS.translation_consistency_weight,
>       rotation_consistency_weight=FLAGS.rotation_consistency_weight,
>       batch_size=FLAGS.batch_size,
>       img_height=FLAGS.img_height,
>       img_width=FLAGS.img_width,
>       weight_reg=FLAGS.weight_reg,
>       depth_consistency_loss_weight=FLAGS.depth_consistency_loss_weight,
>       queue_size=FLAGS.queue_size,
>       input_file=FLAGS.input_file)
>
>   _test(test_model, FLAGS.checkpoint_dir)
>
> def readImages(path, subdir, name):
>   filename = name+"".png""
>   filepath = os.path.join(path, subdir, filename)
>   im = Image.open(filepath)
>   im_array = np.array(im)
>   img1 = im_array[:, 0:416, :]
>   img2 = im_array[:, 416:832, :]
>   return img1[np.newaxis, :, :, :], img2[np.newaxis, :, :, :]
>
>
> def readMat(path, subdir, name):
>   filename = name+""_cam.txt""
>   filepath = os.path.join(path, subdir, filename)
>   data_temp=[]
>   with open(filepath) as fdata:
>     line=fdata.readline()
>     data_temp.append([float(i) for i in line.split(',')])
>   return np.array(data_temp).reshape((3,3))
>
> def readFileList(list_data_dir):
>   with gfile.Open(list_data_dir) as f:
>     frames = f.readlines()
>     frames = [k.rstrip() for k in frames]
>   subfolders = [x.split(' ')[0] for x in frames]
>   frame_ids = [x.split(' ')[1] for x in frames]
>   return subfolders, frame_ids
>
>
> def _test(test_model, checkpoint_dir):
>   checkpointpath = ""./pretrained/cityscapes_kitti_learned_intrinsics/""
>
>   saver = tf.train.import_meta_graph(checkpointpath+'model-1000977.meta')
>   checkpoint = checkpointpath+""model-1000977""
>   with tf.device('/cpu:0'):
>     with tf.Session() as sess:
>       sess.run(tf.local_variables_initializer())
>       sess.run(tf.global_variables_initializer())
>       logging.info('Loading checkpoint...')
>       saver.restore(sess, checkpoint)
>       print(reader.IMAGENET_MEAN)
>       print(reader.IMAGENET_SD)
>       logging.info('Reading data...')
>       path = ""./kitti/format_data""
>       list_data_dir = ""test.txt""
>       subfolders, frame_ids = readFileList(list_data_dir)
>       for (subdir, name) in zip(subfolders, frame_ids):
>         img1, img2 = readImages(path, subdir, name)
>         logging.info('Start testing...')
>         ret = test_model.inference_egomotion(img1, img2,sess)
>         print(ret[2])
>         mat = readMat(path, subdir, name)
>         print(mat)
>         logging.info('End testing...')
>
> if __name__ == '__main__':
>   app.run(main)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNHNC2IDOE7JDYVIPN3QGSN25A5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5GLS5Q#issuecomment-525121910>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNHON74T2IYYN5MCBTTQGSN25ANCNFSM4IMPQMMA>
> .
>
",added code well training unfortunately unable obtain clearance release specific yet sorry sometimes get bureaucratic thanks code easy though spot bug one chance answer whether able reproduce depth inference metric whether look reasonable intrinsic matrix much still suspect sort crude error somewhere next release used calculating odometry learned given well respective odometry try add small piece code generating fig paper hopefully resolve issue thank help goal everyone able reproduce mon wrote test code like main seed seed seed seed path name path return path name path open float return return saver sess sess print print data path name zip path name testing ret sess print ret mat path name print mat testing main reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
525258136,Now on line 424 but still a Syntax Error... https://github.com/google-research/google-research/blob/master/resolve_ref_exp_elements_ml/train.py#L424,line still syntax error,issue,negative,neutral,neutral,neutral,neutral,neutral
525130361,"Hi. I made a PR #50 some hours ago, but got a Travis CI build failing.
And I think this PR is related to the failure. Is there any updates about this request?",hi made ago got travis build failing think related failure request,issue,negative,negative,negative,negative,negative,negative
525121910,"@gariel-google Test code like this.
```
def main(_):
  seed = FLAGS.seed
  tf.set_random_seed(seed)
  np.random.seed(seed)
  random.seed(seed)

  if not gfile.Exists(FLAGS.checkpoint_dir):
    gfile.MakeDirs(FLAGS.checkpoint_dir)

  test_model = model.Model(
      boxify=FLAGS.boxify,
      data_dir=FLAGS.data_dir,
      file_extension=FLAGS.file_extension,
      is_training=False,
      foreground_dilation=FLAGS.foreground_dilation,
      learn_intrinsics=FLAGS.learn_intrinsics,
      learning_rate=FLAGS.learning_rate,
      reconstr_weight=FLAGS.reconstr_weight,
      smooth_weight=FLAGS.smooth_weight,
      ssim_weight=FLAGS.ssim_weight,
      translation_consistency_weight=FLAGS.translation_consistency_weight,
      rotation_consistency_weight=FLAGS.rotation_consistency_weight,
      batch_size=FLAGS.batch_size,
      img_height=FLAGS.img_height,
      img_width=FLAGS.img_width,
      weight_reg=FLAGS.weight_reg,
      depth_consistency_loss_weight=FLAGS.depth_consistency_loss_weight,
      queue_size=FLAGS.queue_size,
      input_file=FLAGS.input_file)
  
  _test(test_model, FLAGS.checkpoint_dir)

def readImages(path, subdir, name):
  filename = name+"".png""
  filepath = os.path.join(path, subdir, filename)
  im = Image.open(filepath)
  im_array = np.array(im)
  img1 = im_array[:, 0:416, :]
  img2 = im_array[:, 416:832, :]
  return img1[np.newaxis, :, :, :], img2[np.newaxis, :, :, :]


def readMat(path, subdir, name):
  filename = name+""_cam.txt""
  filepath = os.path.join(path, subdir, filename)
  data_temp=[]
  with open(filepath) as fdata:
    line=fdata.readline()
    data_temp.append([float(i) for i in line.split(',')])
  return np.array(data_temp).reshape((3,3))
 
def readFileList(list_data_dir):
  with gfile.Open(list_data_dir) as f:
    frames = f.readlines()
    frames = [k.rstrip() for k in frames]
  subfolders = [x.split(' ')[0] for x in frames]
  frame_ids = [x.split(' ')[1] for x in frames]
  return subfolders, frame_ids


def _test(test_model, checkpoint_dir):
  checkpointpath = ""./pretrained/cityscapes_kitti_learned_intrinsics/""
  
  saver = tf.train.import_meta_graph(checkpointpath+'model-1000977.meta')
  checkpoint = checkpointpath+""model-1000977""
  with tf.device('/cpu:0'):
    with tf.Session() as sess:
      sess.run(tf.local_variables_initializer())
      sess.run(tf.global_variables_initializer())
      logging.info('Loading checkpoint...')
      saver.restore(sess, checkpoint)
      logging.info('Reading data...')
      path = ""./kitti/format_data""
      list_data_dir = ""test.txt""
      subfolders, frame_ids = readFileList(list_data_dir)
      for (subdir, name) in zip(subfolders, frame_ids):  
        img1, img2 = readImages(path, subdir, name)
        logging.info('Start testing...')
        ret = test_model.inference_egomotion(img1, img2,sess)
        print(ret[2])
        mat = readMat(path, subdir, name)
        print(mat)
        logging.info('End testing...')
            
if __name__ == '__main__':
  app.run(main)
```",test code like main seed seed seed seed path name path return path name path open float return return saver sess sess data path name zip path name testing ret sess print ret mat path name print mat testing main,issue,negative,positive,positive,positive,positive,positive
525120432,"@gariel-google 
1. Indeed I made incorrect normalization of the images(0-255). Then I add imagenet_norm operation to your test code.

The inference code like this :
```
  def _build_egomotion_test_graph(self):
    """"""Builds graph for inference of egomotion given two images.""""""
    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):
      self._image1 = tf.placeholder(
          tf.float32, [self.batch_size, self.img_height, self.img_width, 3],
          name='image1')
      self._image2 = tf.placeholder(
          tf.float32, [self.batch_size, self.img_height, self.img_width, 3],
          name='image2')
      if self.imagenet_norm:
        self._image1 = (self._image1 - reader.IMAGENET_MEAN) / reader.IMAGENET_SD
        self._image2 = (self._image2 - reader.IMAGENET_MEAN) / reader.IMAGENET_SD
        
      # The ""compute_loss"" scope is needed for the checkpoint to load properly.
      with tf.name_scope('compute_loss'):
        rot, trans, _, mat = motion_prediction_net.motion_field_net(
            images=tf.concat([self._image1, self._image2], axis=-1))
        inv_rot, inv_trans, _, inv_mat = (
            motion_prediction_net.motion_field_net(
                images=tf.concat([self._image2, self._image1], axis=-1)))
      intrinsic_mat = 0.5 * (mat + inv_mat)
      rot = transform_utils.matrix_from_angles(rot)
      inv_rot = transform_utils.matrix_from_angles(inv_rot)
      trans = tf.squeeze(trans, axis=(1, 2))
      inv_trans = tf.squeeze(inv_trans, axis=(1, 2))

      # rot and inv_rot should be the inverses on of the other, but in reality
      # they slightly differ. Averaging rot and inv(inv_rot) gives a better
      # estimator for the rotation. Similarly, trans and rot*inv_trans should
      # be the negatives one of the other, so we average rot*inv_trans and trans
      # to get a better estimator. TODO(gariel): Check if there's an estimator
      # with less variance.
      self.rot = 0.5 * (tf.linalg.inv(inv_rot) + rot)
      self.trans = 0.5 * (-tf.squeeze(
          tf.matmul(self.rot, tf.expand_dims(inv_trans, -1)), axis=-1) + trans)
      self.inf_intrinsic_mat = intrinsic_mat
      
  def inference_egomotion(self, image1, image2, sess):
    return sess.run([self.rot, self.trans, self.inf_intrinsic_mat],
                    feed_dict={
                        self._image1: image1,
                        self._image2: image2
                    })
```

I modified your code :
```
      if self.imagenet_norm:
        self._image1 = (self._image1 - reader.IMAGENET_MEAN) / reader.IMAGENET_SD
        self._image2 = (self._image2 - reader.IMAGENET_MEAN) / reader.IMAGENET_SD
```
```
     intrinsic_mat = 0.5 * (mat + inv_mat)
```

Then I read RGB images to feed to image1 and image2.

The intrinsic matrix is not right too.
[[ 375.67065    0.      -579.6008 ]                                                                    
  [   0.        89.65622  -65.86415]                                                                    
  [   0.         0.         1.     ]]",indeed made incorrect normalization add operation test code inference code like self graph inference given two scope load properly rot mat mat rot rot rot reality slightly differ rot better estimator rotation similarly rot one average rot get better estimator check estimator le variance rot self image image sess return image image code mat read feed image image intrinsic matrix right,issue,positive,positive,positive,positive,positive,positive
524959862,"@liyingliu Cool, so I am aiming for releasing the code for initializing
from an imagenet checkpoint and the respective checkpoint this week.

@buaafish Let's try to debug it together, let me start by asking you some
questions:

1. Do we have enough evidence to rule out an incorrect normalization of the
images (0-1 vs 0-255), or some other error in running the inference? For
example, were you able to reproduce the depth metrics? Were you able to
obtain reasonable trajectories on the KITTI odometry set?

2. How did you calculate the intrinsics at inference time? Did you run the
inference twice, swapping the order of images, and taking the average, like
here
<https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py#L258>
?

3. As we write in the paper, there are two settings we used for learning
the intrinsics: One with a constraint that the intrinsics are the same
throughout the dataset (like in EuRoC), and the other is where we predict
the intrinsics independently from each pair of images. In the second case,
which is the one you are referring to, Eq. 3 and Fig. 9 show that the
intrinsics are only correct within the accuracy imposed by rotations, and
when there are no rotations, the error can be large. Have you tried to
create a plot similar to Fig. 9 or the example you're showing is the only
one you ran? Does that example have rotations?


On Mon, Aug 26, 2019 at 12:58 AM buaafish <notifications@github.com> wrote:

> Hi @gariel-google <https://github.com/gariel-google> , thanks for your
> checkpoint. But I test intrinsic inference with your kitti checkpoint, the
> intrinsic matrix is not right.
> The input images is one pair kitti images like this:
> [image: kitti2]
> <https://user-images.githubusercontent.com/6870525/63674721-b81a9c00-c819-11e9-90b4-17f32a181d82.png>
>
> We use top two images to infer intrinsic matrix:
> [[119.80293 0. 702.5139 ]
> [ 0. 74.126114 -29.449604]
> [ 0. 0. 1. ]]
>
> But the ground truth intrinsic matrix:
> [[241.67446312 0. 204.16801031]
> [ 0. 246.28486827 59.000832 ]
> [ 0. 0. 1. ]]
>
> Why the intrinsic matrix is not right?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNBFXD3JOZ3HJO72TFDQGOEKLA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5DTFVQ#issuecomment-524759766>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNHNQN4NLZB4R73PVATQGOEKLANCNFSM4IMPQMMA>
> .
>
",cool aiming code respective week let try together let start enough evidence rule incorrect normalization error running inference example able reproduce depth metric able obtain reasonable odometry set calculate inference time run inference twice swapping order taking average like write paper two used learning one constraint throughout like predict independently pair second case one fig show correct within accuracy error large tried create plot similar fig example showing one ran example mon wrote hi thanks test intrinsic inference intrinsic matrix right input one pair like image use top two infer intrinsic matrix ground truth intrinsic matrix intrinsic matrix right reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
524759766,"Hi @gariel-google , thanks for your checkpoint. But I test intrinsic inference with your kitti checkpoint, the intrinsic matrix is not right.
The input images is one pair kitti images like this:
![kitti2](https://user-images.githubusercontent.com/6870525/63674721-b81a9c00-c819-11e9-90b4-17f32a181d82.png)

We use top two images to infer intrinsic matrix:
[[119.80293    0.       702.5139  ]                                                                    
  [  0.        74.126114 -29.449604]                                                                    
  [  0.         0.         1.      ]]

But the ground truth intrinsic matrix:
[[241.67446312   0.         204.16801031]                                                               
 [  0.         246.28486827  59.000832  ]                                                               
 [  0.           0.           1.        ]]

Why the intrinsic matrix is not right?",hi thanks test intrinsic inference intrinsic matrix right input one pair like use top two infer intrinsic matrix ground truth intrinsic matrix intrinsic matrix right,issue,positive,positive,positive,positive,positive,positive
524580809,"Yes, you are right. Thanks for the suggestion. This [commit](https://github.com/google-research/google-research/commit/078702ba9721ccdb18b22b9f313f467c9662878c) changes the order of query and candidate.",yes right thanks suggestion commit order query candidate,issue,positive,positive,positive,positive,positive,positive
524546709,"Hi @gariel-google, Sorry for the late reply and thanks for your explanations.  
Being able to reproduce the training would be a nice start for me. Thanks again for the work.
Also, Thank you in advance for releasing the odometry checkpoint and the respective inferred trajectories.",hi sorry late reply thanks able reproduce training would nice start thanks work also thank advance odometry respective,issue,positive,positive,neutral,neutral,positive,positive
524160933,"Thanks much @liyingliu for testing the checkpoints so quickly. I am happy
that we are getting the exact same result on depth prediction.
Regarding the batch size, we tested at 1. If batch-normalization is
replaced everywhere by randomized layer normalization, the inference
results do not depend on the batch size, as it should be. Due to an
oversight, when we were obtaining the results for the paper, we left a few
batch normalization layers in place. We fixed that since, but to be
compatible with the checkpoints used for the paper, we needed to leave the
batch-norms there, hence the dependence on the batch size.

Regarding odometry, there may be a few explanations to that:
1. We used a more mature checkpoint for odometry, which seemed to converge
slower than depth prediction.
2. We used inference time correction for the intrinsics (even though the
result you're showing seems worse than even our uncorrected one).
3. There might be a difference in the way we stack the rotations and
translations together to obtain a trajectory - this is a bit tricky.

We can debug odometry together by us releasing the checkpoints used for
odometry and the respective inferred trajectories.

At this time I would like to ask you what you would like to prioritize -
getting the code and checkpoint for imagenet initialization, so that you
can reproduce the training, or getting the odometry evaluation right?
Please let me know which one you prefer and I'll start there. It will take
at least a few days, in any case.

Thank you again for your help in debugging this.

On Thu, Aug 22, 2019 at 7:08 PM juju <notifications@github.com> wrote:

> Hi @gariel-google <https://github.com/gariel-google>, thanks for the
> work. The new zip files work for me. I have tested the checkpoint trained
> on KITTI. The following is what I have:
>
>    - depth result is different when inference with different batch_size:
>
> abs_rel sq_rel rms log_rms a1 a2 a3
> batch_size=1 0.1262 0.9462 5.2214 0.2086 0.8470 0.9475 0.9774
> batch_size=16 0.1305 1.0186 5.3237 0.2136 0.8389 0.9430 0.9751
>
> When batch_size=1, we have the same result, therefore we should be having
> the same evaluating metrics. However, depth output doesn't have a
> consistent result when batch_size changes, is it the same case for you?
> where does the variation come from?
>
>    - odometry result (ATE) when inference with batch_size=1:
>
> seq_09 seq_10
> 5-point 0.0231 0.0195
> 3-point 0.0170 0.0149
>
> The plotted trajectories:
> seq_09
> [image: dfvauthorseq09]
> <https://user-images.githubusercontent.com/18667188/63561747-a02ee800-c58d-11e9-8462-285ec61e84e7.png>
> seq_10
> [image: dfvauthorseq10]
> <https://user-images.githubusercontent.com/18667188/63561754-aa50e680-c58d-11e9-9f53-762a795cba98.png>
> The odometry result looks quite bad for me. Do you have the same result?
> Since eigen split training set has overlap with odometry sequence 09 and
> 10, the ATE should be better than what you have stated in the paper (0.0231
> vs 0.012 and 0.0195 vs 0.010)?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNERJ6UPVBOJ67HWQILQF5BBBA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD46446Y#issuecomment-524144251>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNHCTPAV6C2AHV2CWCLQF5BBBANCNFSM4IMPQMMA>
> .
>
",thanks much testing quickly happy getting exact result depth prediction regarding batch size tested everywhere layer normalization inference depend batch size due oversight paper left batch normalization place fixed since compatible used paper leave hence dependence batch size regarding odometry may used mature odometry converge depth prediction used inference time correction even though result showing worse even uncorrected one might difference way stack together obtain trajectory bit tricky odometry together u used odometry respective time would like ask would like getting code reproduce training getting odometry evaluation right please let know one prefer start take least day case thank help juju wrote hi thanks work new zip work tested trained following depth result different inference different result therefore metric however depth output consistent result case variation come odometry result ate inference plotted image image odometry result quite bad result since split training set overlap odometry sequence ate better stated paper reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
524144251,"Hi @gariel-google, thanks for the work. The new zip files work for me. I have tested the checkpoint trained on KITTI. The following is what I have: 
* depth result is different when inference with different batch_size:

|               	| abs_rel 	| sq_rel 	| rms    	| log_rms 	| a1     	| a2     	| a3     	|
|---------------	|:-------:	|--------	|--------	|---------	|--------	|--------	|--------	|
| batch_size=1  	| 0.1262  	| 0.9462 	| 5.2214 	| 0.2086  	| 0.8470 	| 0.9475 	| 0.9774 	|
| batch_size=16 	| 0.1305  	| 1.0186 	| 5.3237 	| 0.2136  	| 0.8389 	| 0.9430 	| 0.9751 	|

When batch_size=1, we have the same result, therefore we should be having the same evaluating metrics. However, depth output doesn't have a consistent result when batch_size changes, is it the same case for you? where does the variation come from?

* odometry result (ATE) when inference with batch_size=1:
 
|         	| seq_09 	| seq_10 	|
|:-------:	|--------	|--------	|
| 5-point 	| 0.0231   | 0.0195 |
| 3-point 	| 0.0170 	| 0.0149  	|

The plotted trajectories: 
seq_09
![dfvauthorseq09](https://user-images.githubusercontent.com/18667188/63561747-a02ee800-c58d-11e9-8462-285ec61e84e7.png)
seq_10
![dfvauthorseq10](https://user-images.githubusercontent.com/18667188/63561754-aa50e680-c58d-11e9-9f53-762a795cba98.png)
The odometry result looks quite bad for me. Do you have the same result? Since eigen split training set has overlap with odometry sequence 09 and 10, the ATE should be better than what you have stated in the paper (0.0231 vs 0.012 and 0.0195 vs 0.010)? 
",hi thanks work new zip work tested trained following depth result different inference different result therefore metric however depth output consistent result case variation come odometry result ate inference plotted odometry result quite bad result since split training set overlap odometry sequence ate better stated paper,issue,negative,positive,neutral,neutral,positive,positive
524017097,"YouTube8M IDs are out (see the README file).

On Thu, Aug 22, 2019 at 10:34 AM Ariel Gordon <gariel@google.com> wrote:

> Sorry about that. I replaced the links, they now link to zip files that
> contain all the checkpoint components. Could you check them out? Thanks!
>
> On Wed, Aug 21, 2019 at 11:14 PM juju <notifications@github.com> wrote:
>
>> Thanks for the release. Yes, I am ready to try! However, I noticed that
>> only the data files are released, and as I understand that to restore a
>> model in TensorFlow, we need 3 files (correct me if I am wrong)--index,
>> data, meta. Therefore, could you release the complete checkpoints?
>>
>> —
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUND5Z64FBXF64Z3OLC3QFYVGDA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD44AFLA#issuecomment-523764396>,
>> or mute the thread
>> <https://github.com/notifications/unsubscribe-auth/ADXKUNFFXKE4JEXM3LIAY63QFYVGDANCNFSM4IMPQMMA>
>> .
>>
>
",see file ariel wrote sorry link link zip contain could check thanks wed juju wrote thanks release yes ready try however data understand restore model need correct wrong index data meta therefore could release complete reply directly view mute thread,issue,positive,negative,neutral,neutral,negative,negative
524003232,"Sorry about that. I replaced the links, they now link to zip files that
contain all the checkpoint components. Could you check them out? Thanks!

On Wed, Aug 21, 2019 at 11:14 PM juju <notifications@github.com> wrote:

> Thanks for the release. Yes, I am ready to try! However, I noticed that
> only the data files are released, and as I understand that to restore a
> model in TensorFlow, we need 3 files (correct me if I am wrong)--index,
> data, meta. Therefore, could you release the complete checkpoints?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUND5Z64FBXF64Z3OLC3QFYVGDA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD44AFLA#issuecomment-523764396>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNFFXKE4JEXM3LIAY63QFYVGDANCNFSM4IMPQMMA>
> .
>
",sorry link link zip contain could check thanks wed juju wrote thanks release yes ready try however data understand restore model need correct wrong index data meta therefore could release complete reply directly view mute thread,issue,positive,negative,neutral,neutral,negative,negative
523764396,"Thanks for the release. Yes, I am ready to try! However, I noticed that only the data files are released, and as I understand that to restore a model in TensorFlow, we need 3 files (correct me if I am wrong)--index, data, meta.  Therefore, could you release the complete checkpoints?",thanks release yes ready try however data understand restore model need correct wrong index data meta therefore could release complete,issue,positive,positive,neutral,neutral,positive,positive
523727063,"We just released some checkpoints (links in the README file) with the
respective metrics. Note that there is a slight change in the code (in
depth_prediction_net). Would you be ready to try them and see what metrics
do you obtain? YouTube8M IDs coming soon.

On Mon, Aug 19, 2019 at 10:41 PM juju <notifications@github.com> wrote:

> Yes, as we exchange results it should be enough for comparing odometry
> evals. Thanks again for the efforts.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNABRKRBNTNND4SDVPDQFN7YNA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4VDZ3Q#issuecomment-522861806>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNE3WQWT3JJD2M35E2TQFN7YNANCNFSM4IMPQMMA>
> .
>
",link file respective metric note slight change code would ready try see metric obtain coming soon mon juju wrote yes exchange enough odometry thanks reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
523182691,"Really appreciate the quick response!! That example helps a lot, thanks!",really appreciate quick response example lot thanks,issue,positive,positive,positive,positive,positive,positive
523172127,"Apologies for the inconvenience. 

JSON should be in following format:

```
{
  ""VIDEO_NAME1"": {
    ""LABELER_ID"": {
      ""PHASE LABEL 1"": [
        START TIME IN MICROSECOND, 
        END TIME IN MICROSECOND
      ], 
     ""PHASE LABEL 2"": [
        START TIME IN MICROSECOND, 
        END TIME IN MICROSECOND
      ], 
    }
  },
""VIDEO_NAME2"": {
    ""LABELER_ID"": {
      ""PHASE LABEL 1"": [
        START TIME IN MICROSECOND, 
        END TIME IN MICROSECOND
      ], 
     ""PHASE LABEL 2"": [
        START TIME IN MICROSECOND, 
        END TIME IN MICROSECOND
      ], 
    }
  },
} 
```

An example of JSON entry looks like this:
```
{
  ""clearsoda_to_white12_real_view_0"": {
    ""0"": {
      ""0"": [
        42246.0, 
        1232406.0
      ], 
      ""1"": [
        1265775.0, 
        2111123.0
      ], 
      ""2"": [
        2144492.0, 
        4469198.0
      ], 
      ""3"": [
        4502567.0, 
        4847380.0
      ], 
      ""4"": [
        4880749.0, 
        6226631.0
      ]
    }
  }, 
}
```

Please note that by default we extend the first label to beginning of video and the last label to the end of video. We also assign the nearest timestamp's label to to any timestamp that doesn't fall in the segments in the JSON file. This is done to ensure that each frame has a label. You might want to provide your own dummy labels for frames not covered by any segments. Also consider modifying the [annotation generating function](https://github.com/google-research/google-research/blob/master/tcc/dataset_preparation/dataset_utils.py#L186 ). ",inconvenience following format phase label start time microsecond end time microsecond phase label start time microsecond end time microsecond phase label start time microsecond end time microsecond phase label start time microsecond end time microsecond example entry like please note default extend first label beginning video last label end video also assign nearest label fall file done ensure frame label might want provide dummy covered also consider annotation generating function,issue,positive,positive,neutral,neutral,positive,positive
523168119,"Hi @debidatta 

I'm trying to create my own dataset too, but have trouble to convert per frame labels and converted to the tfrecord? I saw the `create_tfrecords` function which will take a JSON label file, do you have an example of how this JSON file will look like?",hi trying create trouble convert per frame converted saw function take label file example file look like,issue,negative,negative,negative,negative,negative,negative
522861806,"Yes, as we exchange results it should be enough for comparing odometry evals. Thanks again for the efforts.",yes exchange enough odometry thanks,issue,positive,positive,neutral,neutral,positive,positive
522851212,"We will release the KITTI-trained checkpoints, that should be enough for
comparing odometry evals, am I correct?
It will take at least a few days though - please bear with me ;-)

On Mon, Aug 19, 2019 at 9:22 PM juju <notifications@github.com> wrote:

> Hi @gariel-google <https://github.com/gariel-google>, I am also
> evaluating the egomotion prediction by using inference_egomotion
> <https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py#L423>
> to obtain egomition, and using sfmlearner
> <https://github.com/tinghuiz/SfMLearner/blob/master/kitti_eval/eval_pose.py>
> to compute 5-point/3-point ATE.
>
> Since I am just evaluating on my trained model (the one with
> Abs_Rel=0.147, trained on eigen split training set) and eigen split
> training set has overlap frames with odometry sequence 09 and 10, the ATE
> should be reasonably good. However, the result I got it's quite bad
> compared to what you stated in the paper. (I apologize if I am evaluating
> the egomotion prediction wrongly.)
> | | Seq. 09 | Seq. 10 |
> |---------|:-------:|---------|
> | 5-point | 0.0296 | 0.0245 |
> | 3-point | 0.0212 | 0.0180 |
>
> For your information, I also attached the plotted trajectories.
> [image: seq09]
> <https://user-images.githubusercontent.com/18667188/63317302-e0e7f080-c344-11e9-9916-90d3327fd694.png>
> [image: seq10]
> <https://user-images.githubusercontent.com/18667188/63317304-e3e2e100-c344-11e9-9780-5357bfd24a6b.png>
>
> Maybe we could exchange the odometry evaluation result as well?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNHMCKHUBUG2TGK5WZDQFNWRVA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4VAFVA#issuecomment-522846932>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNH5Z6TWRLZYN6HYUGDQFNWRVANCNFSM4IMPQMMA>
> .
>
",release enough odometry correct take least day though please bear mon juju wrote hi also prediction obtain compute ate since trained model one trained split training set split training set overlap odometry sequence ate reasonably good however result got quite bad stated paper apologize prediction wrongly information also attached plotted image image maybe could exchange odometry evaluation result well reply directly view mute thread,issue,negative,negative,negative,negative,negative,negative
522846932,"Hi @gariel-google, I am also evaluating the egomotion prediction by using [inference_egomotion](https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py#L423) to obtain egomition, and using [sfmlearner](https://github.com/tinghuiz/SfMLearner/blob/master/kitti_eval/eval_pose.py) to compute 5-point/3-point ATE.

Since I am just evaluating on my trained model (the one with Abs_Rel=0.147, trained on eigen split training set) and eigen split training set has overlap frames with odometry sequence 09 and 10, the ATE should be reasonably good. However, the result I got it's quite bad compared to what you stated in the paper. (I apologize if I am evaluating the egomotion prediction wrongly.)
|         | Seq. 09 | Seq. 10 |
|---------|:-------:|---------|
| 5-point | 0.0296  | 0.0245  |
| 3-point | 0.0212  | 0.0180  |

For your information, I also attached the plotted trajectories.
![seq09](https://user-images.githubusercontent.com/18667188/63317302-e0e7f080-c344-11e9-9916-90d3327fd694.png)
![seq10](https://user-images.githubusercontent.com/18667188/63317304-e3e2e100-c344-11e9-9780-5357bfd24a6b.png)

Maybe we could exchange the odometry evaluation result as well? 
",hi also prediction obtain compute ate since trained model one trained split training set split training set overlap odometry sequence ate reasonably good however result got quite bad stated paper apologize prediction wrongly information also attached plotted maybe could exchange odometry evaluation result well,issue,negative,negative,negative,negative,negative,negative
522846228,"Yes, we will release the IDs soon, and also pretrained checkpoints.
Regarding the initialization mechanism, we're trying to release it soon,
but it might take a bit longer. We will update as soon as possible.

On Mon, Aug 19, 2019 at 7:37 PM linxin <notifications@github.com> wrote:

> In the paper,it said that some videos selected from 3079 YouTube8M videos
> labled 'Quadcopter', so does the IDs of them will be public soon?
> And I realized that it need much time for process so many videos into
> three-frames-split and generate its masks and alignment，so the pretrained
> YouTube8M checkpoint will be also released soon?
> I also notice that the current release does not yet support initialization
> form the resnet18 checkpoint pretrained on ImageNet, I'm trying to write
> codes to implement that since struct2depth has the similar codes
> organization...
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNDZJMA5PUZ3NXGIHHLQFNKE7A5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4U3TRQ#issuecomment-522828230>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNFFSD6GL4UA4PINNG3QFNKE7ANCNFSM4IMPQMMA>
> .
>
",yes release soon also regarding mechanism trying release soon might take bit longer update soon possible mon wrote paper said selected public soon need much time process many generate also soon also notice current release yet support form trying write implement since similar organization reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
522828230,"In the paper,it said that some videos selected from 3079 YouTube8M videos  labled 'Quadcopter', so does the IDs of them will be public soon?  
And I realized that it need much time for process so many videos into three-frames-split and generate its masks and alignment，so the pretrained YouTube8M checkpoint will be also released soon?
I also notice that the current release does not yet support initialization form the resnet18 checkpoint pretrained on ImageNet, I'm trying to write codes to implement that since  struct2depth has the similar codes organization...",paper said selected public soon need much time process many generate also soon also notice current release yet support form trying write implement since similar organization,issue,negative,positive,positive,positive,positive,positive
522593747,"Hi, it sounds great to me, Let's do it together. ",hi great let together,issue,positive,positive,positive,positive,positive,positive
522379168,"I feel that it's too early to draw a conclusion, we need to investigate
this more. If you're ready to do it together, that would be great. We are
planning to release a pretrained KITTI checkpoint in the next few days, and
a first step can be establishing that we agree on how eval metrics are
calculated. We can both run evals on the same checkpoint and exchange
results. Based on that, we'll see how to proceed. How does that sound?

On Sun, Aug 18, 2019 at 8:12 AM juju <notifications@github.com> wrote:

> Thanks for the reply.
>
> Yes, for depth inference, I am using the link
> <https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py#L388>
> you mentioned in your previous comment.
>
> In Figure 5 of the paper, for *Evaluated on KITTI*, the training
> converges at around 1 million training images. So if we assume that batch_size=4,
> learning_rate=0.0002 has similar convergence, and as I found that my
> training has Abs_Rel=0.147 for the best checkpoint within around 370k-th
> step (370k steps=1.48 million images). Therefore, can I conclude that the
> pretrained ImageNet checkpoint has huge impact (0.147 vs 0.128) on the
> result?
>
> Looking forward to your release and thanks for the efforts.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNFWASS3J5Q4QMRTSDTQFFRHBA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4RB75Q#issuecomment-522330102>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNBF5JPKUAW3SO7EAX3QFFRHBANCNFSM4IMPQMMA>
> .
>
",feel early draw conclusion need investigate ready together would great release next day first step agree metric calculated run exchange based see proceed sound sun juju wrote thanks reply yes depth inference link previous comment figure paper training around million training assume similar convergence found training best within around step million therefore conclude huge impact result looking forward release thanks reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
522330102,"Thanks for the reply.

Yes, for depth inference, I am using the [link](https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py#L388) you mentioned in your previous comment. 

In Figure 5 of the paper, for *Evaluated on KITTI*, the training converges at around 1 million training images. So if we assume that `batch_size=4, learning_rate=0.0002` has similar convergence, and as I found that my training has `Abs_Rel=0.147` for the best checkpoint within around 370k-th step (370k steps=1.48 million images). Therefore, can I conclude that the pretrained ImageNet checkpoint has huge impact (0.147 vs 0.128) on the result? 

Looking forward to your release and thanks for the efforts.",thanks reply yes depth inference link previous comment figure paper training around million training assume similar convergence found training best within around step million therefore conclude huge impact result looking forward release thanks,issue,positive,positive,positive,positive,positive,positive
522289925,"Thanks for the information, that helps a lot. On KITTI we trained with a
barch size of 16 and a learning rate of 1e-4. All other parameters are at
their default values. In addition, we initialized from an ImageNet
checkpoint for ResNet 18. The current release does not yet support
initialization form a checkpoint, but it should be easy to set up. Fig. 5
in the paper indicates when you should expect convergence. Lastly, are you
using
https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py#L388
to
infer depth?

We are planning to release pretrained checkpoints, more code and more
dicumentation before ICCV. We will do our best to do it sooner than later.

On Sat, Aug 17, 2019 at 6:34 PM juju <notifications@github.com> wrote:

> Hi, Sorry for the missing information in my previous comment.
>
> Yes, I am using KITTI, eigen split, using data generation code from
> vid2depth
> <https://github.com/tensorflow/models/tree/master/research/vid2depth/dataset>
> as what struct2depth does.
>
> Yes, I have created a ""possibility mobile"" mask for each image. I am using
> the same masks of struth2depth (each object has different object ID and the
> objects are being tracked across three consecutive sequences). I am using
> Mask-RCNN to obtain the mask. Also, I have turned boxify=True, so the
> masks will become bounding boxes as I understand.
> For your information, I also attached the the image in tensorboard of
> variable seg_stack of line 172, model.py
> <https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py>
> .
> [image: Screenshot 2019-08-18 at 9 24 02 AM]
> <https://user-images.githubusercontent.com/18667188/63219007-1355ea00-c19b-11e9-88ef-e1da17bf8943.png>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNFFVIYEE5PMU3XG3VLQFCRKLA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4QWRKA#issuecomment-522283176>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNBXUHPWYVEWW6EDMXDQFCRKLANCNFSM4IMPQMMA>
> .
>
",thanks information lot trained size learning rate default addition current release yet support form easy set fig paper expect convergence lastly infer depth release code best sooner later sat juju wrote hi sorry missing information previous comment yes split data generation code yes possibility mobile mask image object different object id tracked across three consecutive obtain mask also turned become bounding understand information also attached image variable line image reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
522283176,"Hi, Sorry for the missing information in my previous comment.   

Yes, I am using KITTI, eigen split, using data generation code from [vid2depth](https://github.com/tensorflow/models/tree/master/research/vid2depth/dataset) as what struct2depth does.  

Yes, I have created a ""possibility mobile"" mask for each image. I am using the same masks of struth2depth (each object has different object ID and the objects are being tracked across three consecutive sequences). I am using Mask-RCNN to obtain the mask. Also, I have turned `boxify=True`, so the masks will become bounding boxes as I understand.   
For your information, I also attached the the image in tensorboard of variable `seg_stack` of line 172, [model.py](https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py). 
<img width=""801"" alt=""Screenshot 2019-08-18 at 9 24 02 AM"" src=""https://user-images.githubusercontent.com/18667188/63219007-1355ea00-c19b-11e9-88ef-e1da17bf8943.png"">
",hi sorry missing information previous comment yes split data generation code yes possibility mobile mask image object different object id tracked across three consecutive obtain mask also turned become bounding understand information also attached image variable line,issue,negative,negative,negative,negative,negative,negative
522253581,"Hi
I am assuming you're training on KITTI?

Did you create a ""possibility mobile"" mask for each image? Did you use an
segmentation network to do that? Which one?



On Sat, Aug 17, 2019, 7:11 AM juju <notifications@github.com> wrote:

> @gariel-google <https://github.com/gariel-google> Dear author, Thanks for
> sharing the source code of the paper.
> I was trying to reproduce the result of the paper using your code.
> However, with your default setting (batch size=4, learning_rate=0.0002,
> etc.) training from scratch, the result I got it's quite far from what you
> stated in the paper (Abs Rel 0.147 for the best checkpoint within around
> 370k-th step vs 0.128 in the paper). For your information, I am using the
> evaluation code from sfmlearner
> <https://github.com/tinghuiz/SfMLearner/tree/master/kitti_eval> as what
> struct2depth does.
> Therefore, may I know what's setting for obtaining the paper's result? Or
> is there anything critical part missing in the current released code (maybe
> pretrained checkpoint for example)?
> Thank you in advance.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/46?email_source=notifications&email_token=ADXKUNH2XZHXPU5SHQ6YEDDQFABJPA5CNFSM4IMPQMMKYY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4HFZCAQA>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXKUNAY4ZPYW5AVT3TDSE3QFABJPANCNFSM4IMPQMMA>
> .
>
",hi assuming training create possibility mobile mask image use segmentation network one sat juju wrote dear author thanks source code paper trying reproduce result paper code however default setting batch training scratch result got quite far stated paper rel best within around step paper information evaluation code therefore may know setting paper result anything critical part missing current code maybe example thank advance reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
522162945,"This means that your dataset doesn't have per-frame labels but your config.py does.  You can set

`DATA.FRAME_LABELS: false`

in `config.py`. Also remove `'classification', 'few_shot_classification' , 'event_completion'` from `EVAL.TASKS` in `config.py`. These tasks also need per-frame labels for evaluation. ",set false also remove also need evaluation,issue,negative,negative,negative,negative,negative,negative
521738670,"you should see the example in the README. 

get a tensor of your generated videos and the real ones, and than work according the example",see example get tensor real work according example,issue,negative,positive,positive,positive,positive,positive
519267035,"Works. Thanks for the prompt response!

On Wed, Aug 7, 2019 at 11:40 PM YiDing Jiang <notifications@github.com>
wrote:

> I think the issue is that in the internal code the tensor shapes are
> initialized as [c] and reshaped to [1, c, 1,1] but it was changed later to
> initializing the tensorshape with [1, c, 1, 1] directly. My bad that I
> didn't catch it. It might take a me bit of time to push the change, but if
> you do the following it should fix the issue:
>
>    1. Go to `models/resent.py'
>    2. Go to the function group_norm
>    3. Change:
>
>     gamma = tf.get_variable('gamma', [1, c, 1, 1],
>                             initializer=tf.constant_initializer(1.0))
>     beta = tf.get_variable('beta', [1, c, 1, 1],
>                            initializer=tf.constant_initializer(0.0))
>
> to
>
>     gamma = tf.get_variable('gamma', [c],
>                             initializer=tf.constant_initializer(1.0))
>     beta = tf.get_variable('beta', [c],
>                            initializer=tf.constant_initializer(0.0))
>     gamma = tf.reshape(gamma, [1, c, 1, 1])
>     beta = tf.reshape(beta, [1, c, 1, 1])
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/39?email_source=notifications&email_token=AAFM4ZBXQAIVKAVKN7GFKTLQDMXNBA5CNFSM4IIFIF22YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3ZUTCA#issuecomment-519260552>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAFM4ZBYCZZK4BD3DRDROSLQDMXNBANCNFSM4IIFIF2Q>
> .
>


-- 

             Uri Cohen
              PhD candidate for computational neuroscience
              Edmond and Lily Safra Center for Brain Sciences
              Hebrew University of Jerusalem, Israel
",work thanks prompt response wed wrote think issue internal code tensor later directly bad catch might take bit time push change following fix issue go go function change gamma beta gamma beta gamma gamma beta beta thread reply directly view mute thread candidate computational lily center brain university,issue,negative,negative,neutral,neutral,negative,negative
519260552,"I think the issue is that in the original code the tensor shapes are initialized as [c] and reshaped to [1, c,  1,1] but it was changed later to initializing the tensorshape with [1, c, 1, 1] directly. My bad that I didn't catch it. It might take a me bit of time to push the change, but if you do the following it should fix the issue:

1. Go to `models/resent.py`
2. Go to the function `group_norm`
3. Change:
```
    gamma = tf.get_variable('gamma', [1, c, 1, 1],
                            initializer=tf.constant_initializer(1.0))
    beta = tf.get_variable('beta', [1, c, 1, 1],
                           initializer=tf.constant_initializer(0.0))
```
to
```
    gamma = tf.get_variable('gamma', [c],
                            initializer=tf.constant_initializer(1.0))
    beta = tf.get_variable('beta', [c],
                           initializer=tf.constant_initializer(0.0))
    gamma = tf.reshape(gamma, [1, c, 1, 1])
    beta = tf.reshape(beta, [1, c, 1, 1])
```",think issue original code tensor later directly bad catch might take bit time push change following fix issue go go function change gamma beta gamma beta gamma gamma beta beta,issue,negative,negative,neutral,neutral,negative,negative
519236846,"This indeed solve the issue for all batchnorm models in resnet, but not for groupnorm!

The following error are no longer there:
* Most fail with Not found: Key resnet/group_norm/beta not found in checkpoint
* Few fail with ValueError: Trying to share variable resnet/conv2d/kernel, but specified shape (3, 3, 3, 32) and found shape (3, 3, 3, 16).

The following error is still there, in all groupnorm models:
* Many fail with Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,32,1,1] rhs shape= [32]

That is, for resnet I could read 108 / 216 cifar10 models and 162 / 324 of cifar100 models.",indeed solve issue following error longer fail found key found fail trying share variable shape found shape following error still many fail invalid argument assign match could read,issue,negative,negative,negative,negative,negative,negative
518975208,"I've added the missing file and added the line you have suggested and it
does solve part of the problem, I will check it more systematically and
will share the results.

On Wed, Aug 7, 2019 at 2:28 AM YiDing Jiang <notifications@github.com>
wrote:

> I couldn't find extract_layers_util in your repo, but one quick thing to
> try: can you try to call tf.reset_default_graph between loading different
> models?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/39?email_source=notifications&email_token=AAFM4ZDMUC2GVYFRIDLANDDQDICI5A5CNFSM4IIFIF22YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3WXYTQ#issuecomment-518880334>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAFM4ZHK64RVNDXRNK6FTMLQDICI5ANCNFSM4IIFIF2Q>
> .
>


-- 

             Uri Cohen
              PhD candidate for computational neuroscience
              Edmond and Lily Safra Center for Brain Sciences
              Hebrew University of Jerusalem, Israel
",added missing file added line solve part problem check systematically share wed wrote could find one quick thing try try call loading different thread reply directly view mute thread candidate computational lily center brain university,issue,negative,positive,neutral,neutral,positive,positive
518880334,"I couldn't find `extract_layers_util` in your repo, but one quick thing to try: can you try to call `tf.reset_default_graph` between loading different models?",could find one quick thing try try call loading different,issue,negative,positive,positive,positive,positive,positive
518861273,"Yes, nin models loads correctly.
Moreover, there are 5 resnet models which do load...

On Tue, Aug 6, 2019, 23:58 YiDing Jiang <notifications@github.com> wrote:

> Hi, are the problems with resnet's only?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/google-research/google-research/issues/39?email_source=notifications&email_token=AAFM4ZHTZPKMJINLETYDPELQDHQYXA5CNFSM4IIFIF22YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3WOJMQ#issuecomment-518841522>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAFM4ZFNSQZHSZARXTCVPGTQDHQYXANCNFSM4IIFIF2Q>
> .
>
",yes correctly moreover load tue wrote hi thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
516796538,"An example which reproduce those failures is available at [my fork](https://github.com/uricohen/google-research):
```
python demogen/parse_tuning.py
```",example reproduce available fork python,issue,negative,positive,positive,positive,positive,positive
516401774,"Distillation is only used during training, not at test time. So although we write out the test predictions because they might be useful for evaluation or error analysis, they are never needed for training a BAM model.",distillation used training test time although write test might useful evaluation error analysis never training bam model,issue,negative,positive,positive,positive,positive,positive
515671665,"Thanks for your reply, after another trying, it fixed.
But i wonder that when writting the test outputs in `run_classifier.py` Line 269, the distill_input perhaps should be loaded from the corresponding files like: `{$BAM_DIR}/models/{teachers[task]}/output/{task}_test_predictions_1.pkl`? Due to the testing examples are pickling from the `test.tsv`
Is there any mismatch between the testing examples and distill inputs during the prediction?",thanks reply another trying fixed wonder test line perhaps loaded corresponding like task task due testing mismatch testing distill prediction,issue,positive,positive,neutral,neutral,positive,positive
515604065,"I don't see a bug. Line 131 loads from `{$BAM_DIR}/models/{teachers[task]}/output/{task}_train_predictions_1.pkl`. Therefore it should work as long as you have the files 
```
$BAM_DIR/models/mrpc-model/outputs/mrpc_train_predictions_1.pkl
$BAM_DIR/models/rte-model/outputs/rte_train_predictions_1.pkl
```
Based on the error message, it looks like the predictions file is present but incorrect in some way. Perhaps you trained the single-task models with {""debug"": True} and are now training in non-debug-mode? Debug mode uses a tiny train set, so the written-out predictions won't cover the full train set.",see bug line task task therefore work long based error message like file present incorrect way perhaps trained true training mode tiny train set wo cover full train set,issue,negative,positive,positive,positive,positive,positive
514020478,"Hi @timothybrooks and @jonbarron 

Regarding your questions -
1) I thought the default setting would be to be run it on GPU? Sorry, I'm a very new to TF so not much aware. Could you please let me know how that can be done? You can have a look at my default command I used for training in my original post above. 

2) I added Tim's suggestions in ```train.py``` 
```
...
config = tf.estimator.RunConfig(FLAGS.model_dir, log_step_count_steps=1)
...
tf.logging.set_verbosity(tf.logging.INFO)
```
The simulation is still at the same point I mentioned. 

3) Launching ```tensorboard```, I can see the model graph, but I cannot see the training profile. ",hi regarding thought default setting would run sorry new much aware could please let know done look default command used training original post added simulation still point see model graph see training profile,issue,positive,positive,neutral,neutral,positive,positive
514002668,"Thanks Tim, CL is in flight, I'll closes this issue once it's landed.",thanks flight issue landed,issue,negative,positive,positive,positive,positive,positive
514000905,"To print loss in the terminal during training, add `tf.logging.set_verbosity(tf.logging.INFO)` to set a high enough verbosity to see the training metrics. You can add this line right before the call to `tf.estimator.train_and_evaluate(...)` in train.py.

By default, Estimator will log every 100 steps. You can change this by modifying the config in train.py:
`config = tf.estimator.RunConfig(FLAGS.model_dir, log_step_count_steps=[num of steps])`

You may find it easier to use TensorBoard to visualize training progress, which can be done by running `tensorboard --logdir=[path to model dir]` in a separate terminal during or after training, and opening the printed URL in a web browser. ",print loss terminal training add set high enough verbosity see training metric add line right call default estimator log every change may find easier use visualize training progress done running path model separate terminal training opening printed web browser,issue,positive,positive,positive,positive,positive,positive
513992722,"I just tried training out the current code, and it seems to produce model checkpoints as output. @aasharma90 , can you confirm that model checkpoints aren't being produced when you run this? It's a little confusing because training doesn't produce loss/epoch print statements, but that seems to be a visualization issue, and not a correctness issue.",tried training current code produce model output confirm model produced run little training produce print visualization issue correctness issue,issue,negative,negative,neutral,neutral,negative,negative
513983052,"I second Jon's analysis of this: it looks like CPU is being used here, which can be extremely slow and appear stuck. After waiting a long time (say, an hour) do you see anything written to your model directory?

I would recommend training with a GPU if that is possible, as it will be much faster. The last log is relating to Intel's OpenMP* thread mapping, which is probably because Intel's MKL-DNN is being used. But I have not seen those logs while training before, and see no reason why this would cause stalling.",second analysis like used extremely slow appear stuck waiting long time say hour see anything written model directory would recommend training possible much faster last log thread probably used seen training see reason would cause stalling,issue,negative,negative,neutral,neutral,negative,negative
513960704,Thank you I understand! I think I was thrown off since the results were reported for RGB images as well -- but after reading the paper carefully I figured denoising is still done for Raw bayer pattern and just PSNR is computed by converting both GT and denoised images to RGB space. Thank you for your replies!,thank understand think thrown since well reading paper carefully figured still done raw pattern converting space thank,issue,positive,negative,negative,negative,negative,negative
513945668,"Please see the ""Evaluation on Different Real Data"" section of our README, which explains what inputs are required, and how to attain those inputs:
https://github.com/google-research/google-research/tree/master/unprocessing#evaluation-on-different-real-data

It is possible to calibrate a new camera to attain shot and read noise as a function of ISO, and it is possible to approximate shot and read noise parameters from a single image. However, the models are only for denoising raw Bayer images, so you will not be able to denoise an RGB image directly.",please see evaluation different real data section attain possible calibrate new camera attain shot read noise function iso possible approximate shot read noise single image however raw able image directly,issue,negative,positive,neutral,neutral,positive,positive
513942404,"Are you training on the GPU? I'm not sure what ""stuck"" means here, but it sounds like training could just be proceeding very slowly, and TF using the CPU for training is a very common reason for that.",training sure stuck like training could proceeding slowly training common reason,issue,negative,negative,neutral,neutral,negative,negative
513942135,"This model relies very heavily on having metadata for your image. If you can't get it, you could try using the metadata from an image in our unprocessed training data, or from one of the images in the Darmstadt dataset, but all of our experiments suggest that performance will get significantly worse if your metadata is incorrect.",model heavily image ca get could try image unprocessed training data one suggest performance get significantly worse incorrect,issue,negative,negative,negative,negative,negative,negative
513940447,"Hi, sorry for the delay, this problem has been resolved, just re-download the models linked to in the README. Ping me via email if you continue having issues.",hi sorry delay problem resolved linked ping via continue,issue,negative,negative,negative,negative,negative,negative
513674802,Great! Can you close the issue if it has been properly addressed?,great close issue properly,issue,positive,positive,positive,positive,positive,positive
513412094,"CIFAR10 and 100 have fixed training and test sets, so I don't see any problems. (There have been some changes to the data pipeline, please pull to get them)",fixed training test see data pipeline please pull get,issue,negative,positive,neutral,neutral,positive,positive
513093961,"Thanks, @yidinggiang! But are you sure the data split into training/validation/test set is determenistic?
That is, that the CIFAR10/100 training set I got is the same as the one used to train the models?",thanks sure data split set training set got one used train,issue,positive,positive,positive,positive,positive,positive
513042549,Thank you for working this out. I am working on potentially easier way to get the dataset for external users.,thank working working potentially easier way get external,issue,positive,neutral,neutral,neutral,neutral,neutral
513042344,"Hi @uricohen, thank you for using the code base!
By any chance you are running this on CPU? The ResNet I use is optimized for performance with channel first format but I think the NCHW format is only supported on GPU.",hi thank code base chance running use performance channel first format think format,issue,positive,negative,negative,negative,negative,negative
511756905,"It seems that [tensor2tensor's problem.dataset()](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/problem.py) has changed and now require the second argument `data_dir` not to be left empty.

I could solve the problem by changing `demogen/data_util.py` as follows:
```
  def input_data():
    """"""Input function to be returned.""""""
    prob = problems.problem(problem_name)
    data_dir = '<<<MY_HOME_DIRECTORY>>>/t2t_data/';
    if data == 'image_cifar100':
      dataset = prob.dataset(mode, data_dir, preprocess=augmented)
      if not augmented: dataset = dataset.map(map_func=standardization)
    else:
      dataset = prob.dataset(mode, data_dir)
      dataset = dataset.map(map_func=standardization)

```

and I have previously downloaded the data into this directory using:
```
  t2t-datagen --generate_data --data_dir=~/t2t_data --problem=image_cifar100
  t2t-datagen --generate_data --data_dir=~/t2t_data --problem=image_cifar10
```",require second argument left empty could solve problem input function returned prob data mode augmented else mode previously data directory,issue,negative,negative,neutral,neutral,negative,negative
511737970,"Same for me! 

Perhaps this is a setup issue of tensorflow-datasets? I didn't configure it to download CIFAR, etc.",perhaps setup issue configure,issue,negative,neutral,neutral,neutral,neutral,neutral
509060496,"> Thanks for the link, it looks like our model checkpoints are missing a .checkpoint file. We'll address this as soon as possible, but it's a US holiday this week.

One more question: shot_noise_tensor = graph.get_tensor_by_name('stddev/shot_noise:0')
Read_noise_tensor = graph. Get_tensor_by_name (' stddev/read_noise: 0 ')
It doesn't seem to exist in the original graph.",thanks link like model missing file address soon possible u holiday week one question graph seem exist original graph,issue,positive,positive,neutral,neutral,positive,positive
508777484,"Thanks for the link, it looks like our model checkpoints are missing a .checkpoint file. We'll address this as soon as possible, but it's a US holiday this week.",thanks link like model missing file address soon possible u holiday week,issue,positive,neutral,neutral,neutral,neutral,neutral
507740488,"I'm also getting the same error on Ubuntu with the following setup:
Tensorflow: 1.13.1, Python: 3.7.3 ",also getting error following setup python,issue,negative,neutral,neutral,neutral,neutral,neutral
507674507,"The error is regarding the **unprocessing** code.
I get the same error, when I try to run the supplied dnd_denoise.py with any of the uploaded models.
I am using Python 3.6.8, tensorflow 1.13.1.
I get the same error on Windows and Ubuntu.",error regarding code get error try run python get error,issue,negative,neutral,neutral,neutral,neutral,neutral
495009257,"May I ask how do you mean by ""each line in the targets needs to align with each line in the predictions"". Does it mean that for every system summary there should be only one gold summary. The rouge measure can essentially evaluate a system summary against multiple gold summaries. Does it mean that your package only support single gold summary?!

I am wondering if it is possible to provide more detail information on using your rouge package ? Should I call your package similar to pyrouge (https://pypi.org/project/pyrouge/)? ( I have copied the highlights below)

----------------------------------------------------------------------------------------------------------------

""To automatically match a system summary with the corresponding model summaries, pyrouge uses regular expressions. For example, let’s assume your system summaries are named with a combination of a fixed name and a variable numeric ID like this:

some_name.001.txt
some_name.002.txt
some_name.003.txt
…
and the model summaries like this, with uppercase letters identifying multiple model summaries for a given document:

some_name.A.001.txt
some_name.B.001.txt
some_name.C.001.txt
some_name.A.002.txt
some_name.B.002.txt
…
The group in the system_filename_pattern tells pyrouge which part of the filename is the ID – in this case (\d+). You have to use round brackets to indicate a group, or else pyrouge won’t be able to tell apart the ID from the rest of the filename. pyrouge then uses that ID to find all matching model summaries. The special placeholder #ID# tells pyrouge where it should expect the ID in the model_filename_pattern. The [A-Z] part matches multiple model summaries for that ID.""",may ask mean line need align line mean every system summary one gold summary rouge measure essentially evaluate system summary multiple gold mean package support single gold summary wondering possible provide detail information rouge package call package similar copied automatically match system summary corresponding model regular example let assume system combination fixed name variable id like model like multiple model given document group part id case use round indicate group else able tell apart id rest id find matching model special id expect id part multiple model id,issue,positive,negative,neutral,neutral,negative,negative
488460324,"> Please add the serialized DataFrame to this PR so we can see the raw data used to generate the figure.

Done",please add see raw data used generate figure done,issue,negative,negative,negative,negative,negative,negative
488444652,Please add the serialized DataFrame to this PR so we can see the raw data used to generate the figure.,please add see raw data used generate figure,issue,negative,negative,negative,negative,negative,negative
488367903,"> Please include the commands you ran so we can see the flags...
Done.
",please include ran see done,issue,negative,neutral,neutral,neutral,neutral,neutral
475145902,"sorry, The issue has been put in the wrong place. It should be in https://github.com/google-research/bert",sorry issue put wrong place,issue,negative,negative,negative,negative,negative,negative
475109307,"After more experimentation, we found:

1. having a max number of training steps 2x to 2.5x of the number of collected episodes gives the best training result.

2. The grasping environment selects random objects to spawn the first time the environment runs setup and doesn't re-select after. So if episodes are collected on small number of CPU's, the environment should be tweaked to re-select objects by calling _get_random_objects to ensure episodes cover a diverse set of objects.

3. The urdf_pattern defined in _get_random_objects for training object did not work properly on our machine and only picked out 100 objects. Changing [^0] to [!0] solved the problem for us.

With the above fixes we were able to reproduce results similar to those in the paper.",experimentation found number training number collected best training result grasping environment random spawn first time environment setup collected small number environment calling ensure cover diverse set defined training object work properly machine picked problem u able reproduce similar paper,issue,positive,positive,positive,positive,positive,positive
469938603,"We re-collected 100k episodes using 8 CPUs and re-trained with on-policy, got the results below.

![on_eval2](https://user-images.githubusercontent.com/46766340/53850178-32efb200-3f6f-11e9-8a88-78ea94db663f.png)
![on_test2](https://user-images.githubusercontent.com/46766340/53850182-35eaa280-3f6f-11e9-939f-72543daaa76b.png)

Both eval and test rewards peaked at 250~300k training steps but subsequently decreased to values similar to our previous results. @ericjang the re-ran results you posted show training with 200k steps, while we've been training with the default 2M steps, can this be the problem? It seems strange the policy would get worse after more training steps.

We also re-trained off-policy using the new episodes and only for 200k steps, with the following results. Test reward is still relatively low.

![off_eval2](https://user-images.githubusercontent.com/46766340/53850687-0046b900-3f71-11e9-9e87-08eb3abb4aa3.png)
![off_test2](https://user-images.githubusercontent.com/46766340/53850690-050b6d00-3f71-11e9-9f81-97a586c3cdc9.png)
",got test peaked training subsequently similar previous posted show training training default problem strange policy would get worse training also new following test reward still relatively low,issue,negative,negative,neutral,neutral,negative,negative
469024375,"I think Sara meant to say that you should set the begin_pruning_step = final_step - 1 and end_pruning_step = final_step to mimic zero-shot pruning.  You'll also need to set the threshold_decay parameter to 0, otherwise the threshold won't immediately jump to the necessary value to get the sparsity level you want.

Based on previous experience I've had with zero-shot pruning (see for example the last line of Table 4 in https://arxiv.org/pdf/1704.05119.pdf where the error rate more than doubles at 90% pruning), I would guess that zero-shot pruning will actually lead to worse accuracies than the random fixed sparsity patterns trained from scratch.  If you try this, would love to know the results.",think meant say set mimic pruning also need set parameter otherwise threshold wo immediately jump necessary value get sparsity level want based previous experience pruning see example last line table error rate pruning would guess pruning actually lead worse random fixed sparsity trained scratch try would love know,issue,negative,negative,neutral,neutral,negative,negative
468823413,"Hi Ian, 
 
Wonderful to hear you enjoyed our work! Thanks for these comments, I've put together some thoughts below. I'll tag an owner of this shared research repo to close this issue, but feel free to move this to email if you have additional questions (author email address for correspondence is listed in our [paper](https://arxiv.org/abs/1902.09574).
 
1) Lottery ticket experiments using one-shot sparsification instead of iterative pruning
 
I agree, it would be fun to evaluate whether the lottery ticket results hold on these large scale tasks with “one-shot” sparsification. In fact, one of the variants in [The lottery ticket hypothesis](https://arxiv.org/pdf/1803.03635.pdf) is whether lottery tickets occur in both _one shot pruning_ and _iteratively pruned_ networks. 
 
However, for both _one shot_ and  _iteratively pruned_ networks the authors compare the **1)** performance of the sparse substructure trained from scratch (with same weights as initial random initialization) to **2)** the performance of the original network. 
 
However, the variant you propose appears to quite different, because you are comparing the performance of the sparse substructure trained from scratch (with same weights as initial random initialization) to the _one shot pruned_ structure at the end of training.

Since both variants would likely perform substantially worse than the original model, it is unclear what information we gain here. I.e., you won't be able to tell whether one’s ability to match accuracy when re-training is a product of your hypothesis or just because the accuracy to match is worse (We suspect it is the latter). It's an interesting question, but I don't see a clear way to clearly disentangle the answer. Still easy to run this variant, and perhaps the results will surprise. :)  You can simply run the magnitude pruning for a desired fraction of sparsity once at the end of pruning (I believe by setting the begin_pruning_step and end_pruning_step both to equal one before the last step of training).
 
2) Knowledge reconstitution
 
Hmmm, this I know less about.  I believe Erich Elsen, one of my co-authors worked on a project related to this idea called [dense-sparse-dense](https://arxiv.org/pdf/1607.04381.pdf). 
 
Hope these answers are somewhat helpful. Thanks again Ian for taking the time to put together these thoughts.
",hi wonderful hear work thanks put together tag owner research close issue feel free move additional author address correspondence listed paper lottery ticket instead iterative pruning agree would fun evaluate whether lottery ticket hold large scale fact one lottery ticket hypothesis whether lottery occur shot however compare performance sparse substructure trained scratch initial random performance original network however variant propose quite different performance sparse substructure trained scratch initial random shot structure end training since would likely perform substantially worse original model unclear information gain wo able tell whether one ability match accuracy product hypothesis accuracy match worse suspect latter interesting question see clear way clearly disentangle answer still easy run variant perhaps surprise simply run magnitude pruning desired fraction sparsity end pruning believe setting equal one last step training knowledge reconstitution know le believe one worked project related idea hope somewhat helpful thanks taking time put together,issue,positive,positive,positive,positive,positive,positive
468474152,"Are all the different lines different runs? The variance looks to be pretty high (at a glance, higher than as shown in paper, though ofc hard to tell), your lowest outcome (orange) is close to our outcome.

We will try scaling data collection to just 10 CPUs + running a few times, see what we get.",different different variance pretty high glance higher shown paper though hard tell outcome orange close outcome try scaling data collection running time see get,issue,negative,positive,neutral,neutral,positive,positive
468459841,"We re-ran our Q-learning  experiments with replicated trials on P100 gpus - here's a screenshot of the tensorboard.
![image](https://user-images.githubusercontent.com/433201/53602136-ba4db780-3b62-11e9-956e-ef56cba47215.png)

To be honest I haven't run this code in a non-Google environment (e.g. on GCP / AWS), perhaps a difference could arise from there. One other thing to try - when gathering 100k episodes, we distribute across 1000 different collect workers, each which resets with a different random seed.  ",replicated image honest run code environment perhaps difference could arise one thing try gathering distribute across different collect different random seed,issue,positive,positive,neutral,neutral,positive,positive
467220875,"@ericjang continuing discussion here as per email. 

After collecting 100k grasps as you suggested , we got ~0.55 final test perf for both on and off policy. We are hoping to match the results in your paper of ~0.7 & ~0.8 performance for off and on policy respectively.

We don't have infra to distribute over many CPUs, so would rather not try with 1M - since your provide results for 100k it seems fine to try and replicate those. We don't modify any other aspects of the configs in the repo. Could you suggest any other things to try or check to get replication working?",discussion per got final test policy match paper performance policy respectively infra distribute many would rather try since provide fine try replicate modify could suggest try check get replication working,issue,negative,positive,positive,positive,positive,positive
462115602,"Good performance results (for both on/off-policy) require seeding the training with quite a bit of data collected with the random policy before training. Try collecting at least 100k episodes of random policy first (it is really important to run this collection in parallel on a CPU cluster, otherwise gathering this data takes a long time).",good performance require training quite bit data collected random policy training try least random policy first really important run collection parallel cluster otherwise gathering data long time,issue,positive,negative,neutral,neutral,negative,negative
460350847,"Sorry, I just realized this PR existed. I don't get an email when a new one is created...",sorry get new one,issue,negative,negative,negative,negative,negative,negative
456575314,"By default we treat each line as a separate example, meaning each line in the targets needs to align with each line in the predictions. Rouge scores are computed on each pair of examples before being aggregated. Removing the check will mean any extra lines will be discarded, but I imagine the alignment will still be off, producing the low scores you're seeing.

It sounds like for your use case you might want to compute rouge between entire files, rather than treating each line separately? In this case you could try adding the flag --delimiter=++EOF++ (using any string not appearing in the files as a delimiter), so it will treat each file as a single example.

If your use case is different let me know. Some sample inputs and expected outputs might help in this case.",default treat line separate example meaning line need align line rouge pair removing check mean extra imagine alignment still low seeing like use case might want compute rouge entire rather treating line separately case could try flag string delimiter treat file single example use case different let know sample might help case,issue,positive,negative,neutral,neutral,negative,negative
454526020,"that's a bug, you can replace with regular tensorflow. someone should have a PR soon to update.",bug replace regular someone soon update,issue,negative,neutral,neutral,neutral,neutral,neutral
453376619,"@skearnes I can try to find the server manager to ask for an upgrade. But I don't expect this too soon as my previous request to install rdkit takes almost 3 weeks.

In terms of changes, There is actually no changes to the logics of the code. The only changes are those that make the code able to run locally. And of course the configs.",try find server manager ask upgrade expect soon previous request install almost actually code make code able run locally course,issue,negative,positive,neutral,neutral,positive,positive
453180295,"A Googler has manually verified that the CLAs look good.

(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)

<!-- cla_yes -->",manually look good please make sure reason status clearly,issue,positive,positive,positive,positive,positive,positive
452508973,"> > @skearnes The `Contrib` folder is not included in a normal RDKit install (e.g. pip or conda), at least in the version (2018.09.1) I am using. Users cannot directly import the SAScore module from rdkit unless they manually download the RDKit source and put the files in corresponding directories. I think putting in those files allow users to run our script without messing with rdkit.
> 
> I don't want to duplicate that code here. Please provide instructions for downloading the SA_Score directory and putting it in the proper location. You could also provide a script that does this.

Done",folder included normal install pip least version directly import module unless manually source put corresponding think allow run script without messing want duplicate code please provide directory proper location could also provide script done,issue,negative,negative,neutral,neutral,negative,negative
452439661,"> @skearnes The `Contrib` folder is not included in a normal RDKit install (e.g. pip or conda), at least in the version (2018.09.1) I am using. Users cannot directly import the SAScore module from rdkit unless they manually download the RDKit source and put the files in corresponding directories. I think putting in those files allow users to run our script without messing with rdkit.

I don't want to duplicate that code here. Please provide instructions for downloading the SA_Score directory and putting it in the proper location. You could also provide a script that does this.",folder included normal install pip least version directly import module unless manually source put corresponding think allow run script without messing want duplicate code please provide directory proper location could also provide script,issue,negative,negative,neutral,neutral,negative,negative
452122634,"@skearnes The `Contrib` folder is not included in a normal RDKit install (e.g. pip or conda), at least in the version (2018.09.1) I am using. Users cannot directly import the SAScore module from rdkit unless they manually download the RDKit source and put the files in corresponding directories. I think putting in those files allow users to run our script without messing with rdkit. ",folder included normal install pip least version directly import module unless manually source put corresponding think allow run script without messing,issue,negative,negative,neutral,neutral,negative,negative
451710161,"> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).
> 
> 📝 **Please visit https://cla.developers.google.com/ to sign.**
> 
> Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.
> 
> #### What to do if you already signed the CLA
> ##### Individual signers
> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
> 
> ##### Corporate signers
> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).
> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).

Done!",thanks pull request like may first contribution open source project look help look pull request need sign contributor license agreement memo please visit sign fixed please reply verify already individual possible different address commit check data verify set git corporate company point contact authorized participate ask added group authorized know point contact direct project maintainer public version used register authorized contributor must used git commit check data verify set git used register authorized contributor must also attached account done,issue,positive,positive,neutral,neutral,positive,positive
451710069,"
Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

:memo: **Please visit <https://cla.developers.google.com/> to sign.**

Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.

----

#### What to do if you already signed the CLA

##### Individual signers

*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).

##### Corporate signers

*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).
*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).
*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).
		

<!-- need_sender_cla -->",thanks pull request like may first contribution open source project look help look pull request need sign contributor license agreement memo please visit sign fixed please reply verify already individual possible different address commit check data verify set git corporate company point contact authorized participate ask added group authorized know point contact direct project maintainer public version used register authorized contributor must used git commit check data verify set git used register authorized contributor must also attached account,issue,positive,positive,neutral,neutral,positive,positive
447221727,"[flake8](http://flake8.pycqa.org) testing of https://github.com/google-research/google-research on Python 3.7.1

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./resolve_ref_exp_elements_ml/train.py:230:17: E999 SyntaxError: invalid syntax
    print 'DEBUG'
                ^
1     E999 SyntaxError: invalid syntax
1
```",flake testing python count invalid syntax print invalid syntax,issue,negative,neutral,neutral,neutral,neutral,neutral
