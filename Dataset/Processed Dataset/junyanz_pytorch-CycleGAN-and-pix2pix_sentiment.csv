id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
2003430450,"@wang-TJ-20  Hello , I wonder if you solved the problem! I have exactly the same problem and would appreciate any help",hello wonder problem exactly problem would appreciate help,issue,negative,positive,positive,positive,positive,positive
2003423472,"Hello , @saeedehj  I wonder if you solved the problem! I have the exact same problem here. ",hello wonder problem exact problem,issue,negative,positive,positive,positive,positive,positive
2003290613,"@RRRROBOT  @taozhuang123  @junyanz  @niuliang42  @guotong1988  @shimiaoli  @ZZG-Z  @tellurion-kanata  @taesungp Hello, I have the same issue I have done everything you guys said : I have set the --model to pix2pix I checked the netG it's the same as train, the directory is correct,  but it does not work! I don't get it! it says AttributeError: 'Sequential' object has no attribute 'model'. I have tried PyTorch 2.2.0 , 1.12 .0, 1.9.0 but none of them work. I tried to install 1.4.0 but it is deprecated ! help please what is the problem?
",hello issue done everything said set model checked train directory correct work get object attribute tried none work tried install help please problem,issue,negative,neutral,neutral,neutral,neutral,neutral
2000126610,"Oh that's what you mean. It was ambiguous from your description. 

The model is convolutional so it can probably take in any reasonably sized image and spit out a similarly sized (exact size would depend on strides and padding). However, the pretrained model are not trained on other sizes so no performance guarantees. 

You can modify the code to train new models on arbitrarily sized data. We make no promises on how well it works.",oh mean ambiguous description model convolutional probably take reasonably sized image spit similarly sized exact size would depend padding however model trained size performance modify code train new arbitrarily sized data make well work,issue,negative,positive,neutral,neutral,positive,positive
1999165068,"Hi,

the problem with this is that, If I pass input image size 1024*1024, then the output will be 800*800.
but I want the output to be 1024*1024.",hi problem pas input image size output want output,issue,negative,neutral,neutral,neutral,neutral,neutral
1999050257,"What’s the problem with resizing to 800x800?

On Fri, Mar 15, 2024 at 02:36 arrrrr3186 ***@***.***> wrote:

> Thank you for response.
>
> Simply resizing the input image won't address my issue. Let's say I resize
> an image to a standard size like 800x800 pixels. But what if your original
> input size is larger, like 1024x1024 pixels? In that case, resizing would
> change the dimensions to 800x800 pixels.
>
> What I'm looking for is consistency in output size, regardless of the
> input image's dimensions. Whether it's 800x800, 1024x1024, or even 256x256
> pixels, I want the output image to maintain the same dimensions as the
> input.
>
> Do you know if there's a way to achieve this?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1633#issuecomment-1999028958>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABLJMZPEIFXV3SDJEOU5VGTYYKJF3AVCNFSM6AAAAABEQEDEAGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSOJZGAZDQOJVHA>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
",problem mar wrote thank response simply input image wo address issue let say resize image standard size like original input size like case would change looking consistency output size regardless input image whether even want output image maintain input know way achieve reply directly view id,issue,positive,positive,positive,positive,positive,positive
1999028958,"Thank you for response.

Simply resizing the input image won't address my issue. Let's say I resize an image to a standard size like 800x800 pixels. But what if your original input size is larger, like 1024x1024 pixels? In that case, resizing would change the dimensions to 800x800 pixels.

What I'm looking for is consistency in output size, regardless of the input image's dimensions. Whether it's 800x800, 1024x1024, or even 256x256 pixels, I want the output image to maintain the same dimensions as the input.

Do you know if there's a way to achieve this?

",thank response simply input image wo address issue let say resize image standard size like original input size like case would change looking consistency output size regardless input image whether even want output image maintain input know way achieve,issue,positive,positive,positive,positive,positive,positive
1997388047,"> In training of generator, if dropout is used in discriminator, the GAN loss for generator calculated will be affected by the dropout effect of discriminator.
> 
> ```
>     # GAN loss D_A(G_A(A))
>     self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)
>     # GAN loss D_B(G_B(B))
>     self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)
> ```
> 
> So should we use netD_A.eval() and netD_B.eval() before the calculation of GAN loss in order to eliminate the dropout effect?

if there is BN in discriminator, should we use eval mode?",training generator dropout used discriminator gan loss generator calculated affected dropout effect discriminator gan loss true gan loss true use calculation gan loss order eliminate dropout effect discriminator use mode,issue,negative,positive,positive,positive,positive,positive
1996988352,"@czh886 Hello, could I have a look at your prediction code?  There are always problems with the prediction script I wrote myself. Thanks！",hello could look prediction code always prediction script wrote,issue,negative,neutral,neutral,neutral,neutral,neutral
1995427188,You can resize the input to the same size before feeding into the model.,resize input size feeding model,issue,negative,neutral,neutral,neutral,neutral,neutral
1993590784,"Hi there, anyone can help with this problem, 
it would be a great help to me.
",hi anyone help problem would great help,issue,positive,positive,positive,positive,positive,positive
1972575472,"let's say, I want to continue the training process from 151th epoch, and  i want my model to read the 150th epoch's LR, is there a way to automate the step or we have to do it externally. 

Like this: 
    first train: --n_epoch 100 --n_epochs_decay 100 --lr 0.0002 --epoch_count 1
    continue: --n_epoch 100 --n_epochs_decay 100 --lr 0.0001253 --epoch_count 150 --epoch 151 --continue_train
",let say want continue training process th epoch want model read th epoch way step externally like first train continue epoch,issue,negative,positive,positive,positive,positive,positive
1970678053,"I am facing the same problem, set  --gpu_ids is can't work.",facing problem set ca work,issue,negative,neutral,neutral,neutral,neutral,neutral
1960990783,"is it working for anyone for you?
I am using below command. which execute till web directory creations:

python3 train.py --dataroot ./Custom_data/v_2/urban/  --name Sar-Opt_CGAN  --continue_train --n_epochs 70 --n_epochs_decay 70 --epoch_count 201 --direction BtoA
/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (4.0.0) doesn't match a supported version!
  warnings.warn(""urllib3 ({}) or chardet ({}) doesn't match a supported ""
unaligned
unaligned
data.unaligned_dataset
----------------- Options ---------------
               batch_size: 1                             
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: True                                 [default: False]
                crop_size: 256                           
                 dataroot: ./Custom_data/v_2/urban/             [default: None]
             dataset_mode: unaligned                     
                direction: BtoA                                 [default: AtoB]
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 201                                  [default: 1]
                 gan_mode: lsgan                         
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                                 [default: None]
                 lambda_A: 10.0                          
                 lambda_B: 10.0                          
          lambda_identity: 0.5                           
                load_iter: 0                                    [default: 0]
                load_size: 286                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: cycle_gan                     
                 n_epochs: 70                                   [default: 100]
           n_epochs_decay: 70                                   [default: 100]
               n_layers_D: 3                             
                     name: Sar-Opt_CGAN                         [default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: resnet_9blocks                
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: False                         
                  no_html: False                         
                     norm: instance                      
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 50                            
               preprocess: resize_and_crop               
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                use_wandb: False                         
                  verbose: False                         
       wandb_project_name: CycleGAN-and-pix2pix          
----------------- End -------------------
unaligned
data.unaligned_dataset
dataset [UnalignedDataset] was created
500
The number of training images = 500
initialize network with normal
initialize network with normal
initialize network with normal
initialize network with normal
model [CycleGANModel] was created
loading the model from ./checkpoints/Sar-Opt_CGAN/latest_net_G_A.pth
loading the model from ./checkpoints/Sar-Opt_CGAN/latest_net_G_B.pth
loading the model from ./checkpoints/Sar-Opt_CGAN/latest_net_D_A.pth
loading the model from ./checkpoints/Sar-Opt_CGAN/latest_net_D_B.pth
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 11.378 M
[Network G_B] Total number of parameters : 11.378 M
[Network D_A] Total number of parameters : 2.765 M
[Network D_B] Total number of parameters : 2.765 M
-----------------------------------------------
Setting up a new session...
create web directory ./checkpoints/Sar-Opt_CGAN/web...
____________________________",working anyone command execute till web directory python name direction match version match unaligned unaligned beta true default false default none unaligned direction default main epoch latest default normal true default none default linear model default default name default basic true false false norm instance phase train false false suffix false verbose false end unaligned number training initialize network normal initialize network normal initialize network normal initialize network normal model loading model loading model loading model loading model network total number network total number network total number network total number setting new session create web directory,issue,positive,negative,neutral,neutral,negative,negative
1958900401,"@shreyasbapat Hello, I would like to ask why the result of your rectangular image is OK and the image has not changed. When I was training, I used --preprocess crop --crop_size 256. When I used --preprocess none during testing, an error was reported.",hello would like ask result rectangular image image training used crop used none testing error,issue,positive,neutral,neutral,neutral,neutral,neutral
1930224995,"For testing B to A direction, I have to manually rename the model to switch A and B which is so dumb, which is latest_net_G_A.pth  to latest_net_G_B.pth  and vice versa. Then adding `—direction BtoA`  for test.py run will give the correct result",testing direction manually rename model switch dumb vice run give correct result,issue,negative,negative,negative,negative,negative,negative
1921351805,"wgangp works poorly on cycleGAN, however WGAN with gradient clipping works really good",work poorly however gradient clipping work really good,issue,negative,positive,positive,positive,positive,positive
1902202325,"Hi AGRocky,

To help troubleshoot your CycleGAN model issue, could you provide:

1. **Versions**: Exact versions of Python, PyTorch, and other libraries used.
2. **System Specs**: Your GPU model and overall system configuration.
3. **Error Details**: Any specific error messages or warnings during model loading.

These details will help in accurately replicating the issue and providing a solution.

Thanks!",hi help model issue could provide exact python used system spec model overall system configuration error specific error model loading help accurately issue providing solution thanks,issue,positive,positive,positive,positive,positive,positive
1884708084,It is recommended to crop the image as there is too much interference from the black background,crop image much interference black background,issue,negative,positive,neutral,neutral,positive,positive
1884120445,"This warning message warns us that if we call lr_scheduler.step() before optimizer.step() , PyTorch will skip the first value of the learning rate schedule. But in theory, our learning rate remains unchanged in the first 100 epochs, which means that if we ignore this warning, the learning rate will begin to decay after the completion of the 99th epoch, so I think that in 200 epochs , this impact is very small.
So if you don't want to change, just let it go.",warning message u call skip first value learning rate schedule theory learning rate remains unchanged first ignore warning learning rate begin decay completion th epoch think impact small want change let go,issue,negative,positive,neutral,neutral,positive,positive
1881462327,"> > or maybe they have defaults?
> 
> It appears that for colorization dataset, the defaults are input_nc 1 and output_nc 2. So the issue may be somewhere else. T Checking the code and data you use for training would be useful.

If I change the output to 3 should it work?",maybe colorization issue may somewhere else code data use training would useful change output work,issue,negative,positive,positive,positive,positive,positive
1873961364,"> Hi sir, I have Infrared and visual images with same information I want to train the GAN on these images and then generate new infrared images by using some other visual images. Which project do you recommend and Can i work with my own dataset?
> 
> Training = both infrared and visual images with same information.
> 
> Generate = infrared images from given new visual images.

Hello, I want to achieve the same goal as you, but I have used both pix2pix and cyclegan networks which have not been very effective. Have you solved your problem? Can you share your experience?",hi sir infrared visual information want train gan generate new infrared visual project recommend work training infrared visual information generate infrared given new visual hello want achieve goal used effective problem share experience,issue,positive,positive,positive,positive,positive,positive
1870840487,"import torch
from models.networks import define_G
from PIL import Image
from torchvision import transforms
from IPython.display import display

# Define the generator model
generator = define_G(input_nc=3, output_nc=3, ngf=64, netG='resnet_9blocks', norm='instance', use_dropout=False,init_type='normal',init_gain=0.02)

# Load the pre-trained weights from a saved checkpoint
generator_checkpoint_path = 'latest_net_G.pth'
checkpoint = torch.load(generator_checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

# Load the generator state_dict
generator.load_state_dict(checkpoint, strict=False)

# Set the model to evaluation mode (important if using dropout during training)
generator.eval()

# Load your input image
input_image_path = 'nt6.jpg'
input_image = Image.open(input_image_path).convert('RGB')

# Resize the input image to the expected size
input_image = input_image.resize((512, 512))

# Convert the input image to a PyTorch tensor
input_tensor = transforms.ToTensor()(input_image).unsqueeze(0)  # Add batch dimension

# Move the input tensor to the GPU if available
if torch.cuda.is_available():
    input_tensor = input_tensor.to('cuda')

# Move the generator to the same device as the input tensor
generator = generator.to(input_tensor.device)

# Generate the output image
with torch.no_grad():
    output_tensor = generator(input_tensor)

# Move the output tensor to the CPU if necessary
output_tensor = output_tensor.cpu()

# Convert the output tensor to a PIL image
output_image = transforms.ToPILImage()(output_tensor.squeeze(0))

# Display the generated image
display(output_image)

# Save the generated image
output_image.save('generated_image.jpg')


this is my code ",import torch import import image import import display define generator model generator load saved else load generator set model evaluation mode important dropout training load input image resize input image size convert input image tensor add batch dimension move input tensor available move generator device input tensor generator generate output image generator move output tensor necessary convert output tensor image display image display save image code,issue,positive,positive,positive,positive,positive,positive
1870073423,"Hey @JustinasLekavicius thank you for replying to my issue. I am using CycleGAN for training purpose is to denoise the image. However I have the pre trained weight which works well  when it is used with the python command line code 
""python test.py --dataroot directory, e.g. (/content/data/A) --name model_name --model test...""

But when I try the same thing by creating a class to load the model and give image as input and get the output as denoised image, I am unable to do it. However if I try to load the model the prediction or the testing output image which is denoised image isn't getting generated accurately but it's happening in with the above python command code. 
please help me with this 


heartily thank you in advance


",hey thank issue training purpose image however trained weight work well used python command line code python directory name model test try thing class load model give image input get output image unable however try load model prediction testing output image image getting accurately happening python command code please help heartily thank advance,issue,positive,negative,neutral,neutral,negative,negative
1869431049,"> or maybe they have defaults?

It appears that for colorization dataset, the defaults are input_nc 1 and output_nc 2. So the issue may be somewhere else. T
Checking the code and data you use for training would be useful. ",maybe colorization issue may somewhere else code data use training would useful,issue,negative,positive,positive,positive,positive,positive
1869180933,"> > is that I do not put those arguments
> > > Check what your input_nc and output_nc values are. 1 is for grayscale, 3 is for RGB.
> 
> Check if you put those arguments in your training configuration. If your input images are grayscale, use input_nc=1. If the images are colored, use input_nc=3

no, I didn't use them, I used the ones that came with the colab and with the one to resume training, but nothing else.",put check check put training configuration input use colored use use used came one resume training nothing else,issue,negative,neutral,neutral,neutral,neutral,neutral
1868995627,"> is that I do not put those arguments
> 
> > Check what your input_nc and output_nc values are. 1 is for grayscale, 3 is for RGB.
> 
> 

Check if you put those arguments in your training configuration. If your input images are grayscale, use input_nc=1. If the images are colored, use input_nc=3",put check check put training configuration input use colored use,issue,negative,neutral,neutral,neutral,neutral,neutral
1867591415,"Was it CycleGAN or Pix2pix model?

To test it on a different data set, you could use this command as an example:

python test.py --dataroot *directory, e.g. (/content/data/A)* --name *model_name* --model test --netD n_layers --n_layers_D 3 --netG=unet_256 --norm=instance --direction AtoB --dataset_mode single --preprocess none  --input_nc 1 --output_nc 3 --ndf 64 --ngf 64 --num_test 512

Make sure to replace the parameters with the same ones you used for training of the model (netD, n_layers, netG, ndf, ngf, etc.)
",model test different data set could use command example python directory name model test direction single none make sure replace used training model,issue,negative,positive,positive,positive,positive,positive
1867568067,When are you getting this error? During training or testing? Can you provide a snippet of the code that produces the error?,getting error training testing provide snippet code error,issue,negative,neutral,neutral,neutral,neutral,neutral
1867564971,"Default settings may not be suitable for all tasks, and they would need to be tweaked for your specific task. What is your input data, and expectations for output data? What are the A and B domains? Also, you may need to train for more than 200 epochs for better results.",default may suitable would need specific task input data output data also may need train better,issue,negative,positive,positive,positive,positive,positive
1867563752,"What generator parameters are you using? And what are your training results? The results may be improved by strengthening the generator, i.e. increasing the ngf value, or training for more epochs. Discriminator may also need to be either strengthened or weakened. Depends on the balance between them, and how the training results look.",generator training may strengthening generator increasing value training discriminator may also need either balance training look,issue,positive,neutral,neutral,neutral,neutral,neutral
1867560569,"Sorry for the late response, but I believe the amount of steps depends on your batch size and number of input samples. I think it may also be dependant on training options such as display_freq and print_freq. 

If for example you have 200 epochs, but you have batch size of 1 and 6 input samples, then you'll have 1200 steps. Unless you have more samples than that, then the wandb steps value may be impacted by display_freq and print_freq values (how often the results are logged to Weights and Biases).",sorry late response believe amount batch size number input think may also training example batch size input unless value may impacted often logged,issue,negative,negative,negative,negative,negative,negative
1857422570,which version of torch should I choose? The default is torch1.4,version torch choose default torch,issue,negative,neutral,neutral,neutral,neutral,neutral
1855443836,"> I have solved it, my bad, misspelling in the file. Thanks for your help. I will close this thread.

hi,

I have the same problem as you. How did you solve it?
",bad misspelling file thanks help close thread hi problem solve,issue,negative,negative,negative,negative,negative,negative
1838493324,"I trained with: ## Traing Model with Generator_unet256 and Discriminator PatchGaN 
train_SPSCDvsALSHD_random_Gunet_256_DPatchGAN: 
	nohup $(PYTHON_INTERPRETER) ../train.py --dataroot ../datasets/SPSCDvsALSHD_random --name SPSCDvsALSHD_random_Gunet_256_DPatchGAN --model cycle_gan --gpu_ids 2 --netG unet_256 --gan_mode vanilla --pool_size 50 --batch_size 1 --checkpoints_dir ../checkpoints --display_id -1 --preprocess scale_width_and_crop --load_size 1920 --crop_size 512 --save_epoch_freq 20> ./checkpoints/SPSCDvsALSHD_random_Gunet_256_DPatchGAN/SPSCDvsALSHD_random_Gunet_256_DPatchGAN.log 2>&1 &

And I test with: ## Testinng Model with Generator_resnet9B and Discriminator PatchGaN 
test_SPSCDvsALSHD_random_Gunet_256_DPatchGAN: 
	$(PYTHON_INTERPRETER) ../test.py --dataroot ../datasets/SPSCDvsALSHD_random/testB --name SPSCDvsALSHD_random_Gunet_256_DPatchGAN --model test --no_dropout --checkpoints_dir ../checkpoints --preprocess none --load_size 1920 --gpu_ids 3 

I got the following error. Sorry I'm new to this area and I could not figure out what kind parameters I missed for testing, could you please let me know? 

Traceback (most recent call last):
  File ""../test.py"", line 52, in <module>
    model.setup(opt)               # regular setup: load and print networks; create schedulers
  File ""/ediss_data/ediss6/ali/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 88, in setup
    self.load_networks(load_suffix)
  File ""/ediss_data/ediss6/ali/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 198, in load_networks
    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))
  File ""/ediss_data/ediss6/ali/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 174, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
  File ""/ediss_data/ediss6/ali/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 174, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
  File ""/ediss_data/ediss6/ali/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 947, in __getattr__
    raise AttributeError(""'{}' object has no attribute '{}'"".format(
AttributeError: 'Sequential' object has no attribute 'model'
make: *** [Makefile:63: test_SPSCDvsALSHD_random_Gunet_256_DPatchGAN] Error 1
",trained model discriminator name model vanilla test model discriminator name model test none got following error sorry new area could figure kind testing could please let know recent call last file line module opt regular setup load print create file line setup file line net file line module key file line module key file line raise object attribute object attribute make error,issue,positive,positive,neutral,neutral,positive,positive
1826294340,"hello, i don't undestand why you don't use a validation set during training. 
Also if i need the loss on the test set , where can i find it? it appears only during the training",hello use validation set training also need loss test set find training,issue,negative,neutral,neutral,neutral,neutral,neutral
1819082949,Thank you! I was using --display_id=0 to avoid errors about visdom not being installed. (I was hoping to avoid installing it since I'm using wandb instead.),thank avoid avoid since instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1817941613,"I had the same issue. In my case, it was because I used the --display_id 0 parameter (as I was running the code via Colab). I removed the --display_id 0 parameter, and the loss graphs started login in wandb.",issue case used parameter running code via removed parameter loss login,issue,negative,neutral,neutral,neutral,neutral,neutral
1805513869,"> I have solved the problem. You can disable the visdom by setting --display_id 0

hi，can u explain why this work？SUPER THANKS！",problem disable setting explain,issue,negative,neutral,neutral,neutral,neutral,neutral
1797841718,"Hello, I have reviewed the changes you made to the code and tried applying them to my code.

Although color jitter augmentation is a good technique, when I apply it in the get_transform function, it seems to affect both image A and B, and also impacts the output image. If we are training from B to A, it might be better to apply the color jitter only to B (the input).

My response may not be accurate!
Thank you.",hello made code tried code although color jitter augmentation good technique apply function affect image also output image training might better apply color jitter input response may accurate thank,issue,positive,positive,positive,positive,positive,positive
1787630037,"Hello, I am also generating images that are all black.",hello also generating black,issue,negative,negative,negative,negative,negative,negative
1774194346,"> You need to rename it to `latest_net_G.pth` manually. I am thinking about adding a new flag `checkpoint_path` so that people can specify the path directly. I will add it once I have time.

This is not implemented yet am I right? So for now just manual renaming? Then it works for me.",need rename manually thinking new flag people specify path directly add time yet right manual work,issue,negative,positive,positive,positive,positive,positive
1754163671,"> honestly I have no idea what teh solution was...
> 
> I have found [this](https://github.com/Woodyet/Pix2Pix_Auto_Prune/blob/master/Pix2Pix_Auto_Prune/Tensorflow2.5.0/Auto_test.py) script that I used to organise the files for evaluation
> 
> Sorry i can't help more

Thank you very much for your reply",honestly idea solution found script used evaluation sorry ca help thank much reply,issue,positive,positive,neutral,neutral,positive,positive
1752928548,"actually I think [Evaluating Labels2Photos on Cityscapes](https://github.com/phillipi/pix2pix/tree/master#evaluating-labels2photos-on-cityscapes) was the solution 

> Images stored under --result_dir should contain your model predictions on the Cityscapes validation split, and have the original Cityscapes naming convention (e.g., frankfurt_000001_038418_leftImg8bit.png). The script will output a text file under --output_dir containing the metric.",actually think solution contain model validation split original naming convention script output text file metric,issue,positive,positive,positive,positive,positive,positive
1752915914,"honestly I have no idea what teh solution was...

I have found [this](https://github.com/Woodyet/Pix2Pix_Auto_Prune/blob/master/Pix2Pix_Auto_Prune/Tensorflow2.5.0/Auto_test.py) script that I used to organise the files for evaluation

Sorry i can't help more",honestly idea solution found script used evaluation sorry ca help,issue,positive,positive,neutral,neutral,positive,positive
1752482412,"> Sorry for raising duplicate issues. I have read instructions and previous issues carefully. I still have some concerns about evaluation. python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/
> 
> So the cityscapes_dir should be leftImg8bit_trainvaltest/leftImg8bit/ or gtFine_trainvaltest/gtFine/? If I use the first one, I can run it but producing all 0 outputs even using real images in results. If I use the second one, error ""no module names labels"" occur. Sorry for bothering you at the time approaching the CVPR deadline. Many thanks.

I would like to know, how did you solve this problem, I have the same problem on my side and look forward to your reply.",sorry raising duplicate read previous carefully still evaluation python use first one run even real use second one error module occur sorry time approaching deadline many thanks would like know solve problem problem side look forward reply,issue,negative,negative,neutral,neutral,negative,negative
1752458723,"> repo instructions

I would like to know, how did you solve it, I have the same problem on my side and look forward to your reply.",would like know solve problem side look forward reply,issue,negative,neutral,neutral,neutral,neutral,neutral
1752455153,"> I want to evaluate cityscapes datasets,because I try to reproduce the fcn score results from pip2pip by using https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix code. I try many ways.I can not get any results when I run the scripts. All parameters are zero. I still want to know how to configure the cityspaces dataset There are three folders with gtFine, originals image, and predictions. Are the gtFine and predictions color or grayscale? And what is the size of these three types of pictures? I use python2,caffe.I Is that possible that it was because i was using python2.7 but not python3 ? @hshihua

I wonder, did you solve it? I have the same issue on my side and look forward to your reply",want evaluate try reproduce score code try many get run zero still want know configure three image color size three use python possible python python wonder solve issue side look forward reply,issue,negative,positive,positive,positive,positive,positive
1749795595,I am also trying this but has same issue. I have stacked my input to 6 channel NPZ and changed the dataset loader accordingly. My output channels would be just 1. Has anyone found a solution for this ?,also trying issue input channel loader accordingly output would anyone found solution,issue,negative,neutral,neutral,neutral,neutral,neutral
1747109643,"> It depends on the type of your dataset. For example in my experiment I used `--dataset_mode unaligned`, so I've changed the next few lines from [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L57) to something like:
> 
> ```
>         # A_img = Image.open(A_path).convert('RGB')
>         # B_img = Image.open(B_path).convert('RGB')
>         # apply image transformation
>         # A = self.transform_A(A_img)
>         # B = self.transform_B(B_img)
>         A = np.array([np.load(A_path)['data']]).astype(np.float32)
>         B = np.array([np.load(B_path)['data']]).astype(np.float32)	
> 
>         return {'A': A, 'B': B, 'A_paths': A_path, 'B_paths': B_path}
> ```

Hello, I am trying to use the same code with ""unaligned"" dataset and I have NPZ files in trainA,testA trainB,testB folders in dataset. But when I am trying to run the code, I am getting below error and it looks like dataloader can't see the NPZ files. Do you have any idea on how to solve this ? Thanks in advance.

```
    self.dataloader = torch.utils.data.DataLoader(
  File ""k:\Miniconda3\envs\pyt\lib\site-packages\torch\utils\data\dataloader.py"", line 277, in __init__
    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]
  File ""k:\Miniconda3\envs\pyt\lib\site-packages\torch\utils\data\sampler.py"", line 97, in __init__
    raise ValueError(""num_samples should be a positive integer ""
ValueError: num_samples should be a positive integer value, but got num_samples=0
```

UPDATE:

looks like image extension list at `pytorch-CycleGAN-and-pix2pix-master\data\image_folder.py` should be updated to include `NPZ` files as it is used by `make_dataset(self.dir_A, opt.max_dataset_size)` at `unaligned_dataset.py` ",type example experiment used unaligned next something like apply image transformation return hello trying use code unaligned testa trying run code getting error like ca see idea solve thanks advance file line sampler type ignore file line raise positive integer positive integer value got update like image extension list include used,issue,positive,positive,positive,positive,positive,positive
1738330467,"Hello, how do you modify the code to realize the 7 input channels and 1 output channel?",hello modify code realize input output channel,issue,negative,neutral,neutral,neutral,neutral,neutral
1734882962,"> I am using cycle_gan with resnet_9blocks so it may be different, but adding --no_dropout solved the problem for me.

also solved for me, thanks",may different problem also thanks,issue,negative,positive,neutral,neutral,positive,positive
1732345764,"Hello, I have also encountered this problem. May I ask how you solved this problem?",hello also problem may ask problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1731920362,"i have the same problem, could you please tell me how you solve it?",problem could please tell solve,issue,negative,neutral,neutral,neutral,neutral,neutral
1729657656,"> Found the solution here: [#782 (comment)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/782#issuecomment-538537477)
> 
> Changing from instance to batch norm solves this specific border issue. The style transfer is less dramatic with batch norm apparently, but I will try to remedy this by tweaking other parameters.

 I had a similar problem. Have you found any other solutions?",found solution comment instance batch norm specific border issue style transfer le dramatic batch norm apparently try remedy similar problem found,issue,negative,negative,neutral,neutral,negative,negative
1726794232,"> 2\. update the discriminator only when accuracy of prediction by the discriminator for fake image is less than 50%: there were less updates in the discriminator weights because of this new rule and discriminator loss did not go to zero.

Hi, may I ask if you used bce loss or mse loss? if you use mse loss, the loss won't be bounded between 0-1.",update discriminator accuracy prediction discriminator fake image le le discriminator new rule discriminator loss go zero hi may ask used loss loss use loss loss wo bounded,issue,negative,negative,negative,negative,negative,negative
1722182527,"> The default pix2pix network is U-Net. To test pix2pix model with resnet9blks, you need to explicitly set the flag `--netG resnet_9blocks` during test time.

@junyanz  -- thanks for the excellent code and documentation , am facing the same issue with the default params . 
```
 netD: basic                         
netG: unet_256
```
Kindly refer here for detailed stackt-race -- thanks https://github.com/RohitDhankar/Machine-Learning-with-Python_ML_Py/issues/112",default network test model need explicitly set flag test time thanks excellent code documentation facing issue default basic kindly refer detailed thanks,issue,positive,positive,positive,positive,positive,positive
1713935657,"Hello @phuocnguyen2008 
I have the same problem, did you get the answer?
",hello problem get answer,issue,negative,neutral,neutral,neutral,neutral,neutral
1697906791,Hey. Unfortunately no. I just switched back to 3 color channels from the example. Then it worked.,hey unfortunately switched back color example worked,issue,negative,negative,negative,negative,negative,negative
1697816131,"Hey, did you find any solution for this? Getting ""RuntimeError: Trying to resize storage that is not resizable""",hey find solution getting trying resize storage,issue,negative,neutral,neutral,neutral,neutral,neutral
1679033345,"@nuonuoxiangyaofeigaogao @tjnkyqcy 
In https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/233#issuecomment-379134078 it is mentioned that you need to manually change the name of the .pth file.",need manually change name file,issue,negative,neutral,neutral,neutral,neutral,neutral
1678611785,"@docasy You're Welcome!

Lets have the link if required for further investigation. [LINK](https://github.com/phillipi/pix2pix)",welcome link investigation link,issue,negative,positive,positive,positive,positive,positive
1678470756,"> Have you read the following?
> 
> ![Capture7](https://user-images.githubusercontent.com/104098902/252651547-b370be1c-8a32-4ea4-beff-e747611bb391.JPG)

@docasy Is that helpful? ",read following capture helpful,issue,negative,neutral,neutral,neutral,neutral,neutral
1676556216,"Hello, I would like to ask you how did you implement the training of more than 3 channels of data, and which part of the code was modified?",hello would like ask implement training data part code,issue,negative,neutral,neutral,neutral,neutral,neutral
1676099867,"> Have you read the following?
> 
> ![Capture7](https://user-images.githubusercontent.com/104098902/252651547-b370be1c-8a32-4ea4-beff-e747611bb391.JPG)

it's 2023.8.13, I haven‘t seen it in the README.  dont know where you get this
",read following capture seen dont know get,issue,negative,neutral,neutral,neutral,neutral,neutral
1663231682,"`-dataroot ./datasets/opaGan --name opa_cyclegan --model cycle_gan --epoch_count 200 --continue_train`

can I use this command to train a model base on a pre-trained model? or not? what does  '--model cycle_gan' mean? using the pre-trained model in the path './checkpoint/cycle_gan' ?

I've got the official dataset from [https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/)
however, it needs a huge graphics memory to train (using the official dataset ), I have only a laptop with one RTX4060, how can I do some option to only train some of part of the whole network instead of training all of it?

If can, are there any suggestions about fine-tuning a pre-trained model? like adding some codes in certain places of the project, since I'm not an expert in using cycleGAN.",name model use command train model base model model mean model path got official however need huge graphic memory train official one option train part whole network instead training model like certain project since expert,issue,positive,negative,neutral,neutral,negative,negative
1650909863,"I met the same problem, but it still worked well, so let it go :)",met problem still worked well let go,issue,negative,neutral,neutral,neutral,neutral,neutral
1645112668,"> For crop_size=512, batch_size=2 will require a significant amount of GPU memory. I am not sure how much memory you have per GPU.
> 
> I am not sure if you need to divide the batch_size by the number of threads before feeding it to DataLoader.
> 
> A permanent solution is to replace the Data Parallel with DDP. But we currently don't have the capacity to add it. Any pull request regarding it would be greatly appreciated.

This means do not ddp right now?",require significant amount memory sure much memory per sure need divide number feeding permanent solution replace data parallel currently capacity add pull request regarding would greatly right,issue,positive,positive,positive,positive,positive,positive
1642522904,Just want to confirm that the solution by @wgansir (adding `--model cycle_ gan` when running the `test` command) worked for me too. Big thanks!,want confirm solution model gan running test command worked big thanks,issue,positive,positive,neutral,neutral,positive,positive
1640161368,"Hi, did you succeed with transfer learning? If yes, can you share some insights? Thanks",hi succeed transfer learning yes share thanks,issue,positive,positive,positive,positive,positive,positive
1640156606,"In the script `train.py`, just put the ""model.update_learning_rate()"" at the end of the inner for loop, and it will work.",script put end inner loop work,issue,negative,neutral,neutral,neutral,neutral,neutral
1632467655, Saving the combined image as .png instead of .jpg solves the issue.,saving combined image instead issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1630682771,"@Yash-10 Kindly swap the validation and test folder. Otherwise use random splitting after merging train, val and test.",kindly swap validation test folder otherwise use random splitting train test,issue,negative,positive,neutral,neutral,positive,positive
1630678136,@junyanz How can we know the the details about this '--preprocess' option. I want to know more about this option to estimate its suitability to my problem before implementing.  ,know option want know option estimate suitability problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1630669617,"@vandana-sreenivasan3 It seems that your dataset is too small relatively. 

1. If you're not looking for a very accurate model, try replicating your training images in the val and test folders.  

2. Try performing classical data augmentation techniques that are very simple and not much time-consuming. ",small relatively looking accurate model try training test try classical data augmentation simple much,issue,negative,positive,neutral,neutral,positive,positive
1630462780,"@ak9250 Are you splitting the train, val and test yourself inside the A and B folder. If not, then you need to do this otherwise you'll get this error.",ak splitting train test inside folder need otherwise get error,issue,negative,neutral,neutral,neutral,neutral,neutral
1620401087,"l also meet this question, do you know how to solve? thank you!
",also meet question know solve thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1604201060,"It doesn't, just write your own prediction script and use the model's `set_input` function. The forward function should only need the input to the prediction.",write prediction script use model function forward function need input prediction,issue,negative,neutral,neutral,neutral,neutral,neutral
1595667134,maybe. Thanks for update. Looking back at it after 1+ year. ,maybe thanks update looking back year,issue,negative,positive,neutral,neutral,positive,positive
1589623072,"You probably meant to post this in the [contrastive unpaired translation](https://github.com/taesungp/contrastive-unpaired-translation/blob/master/models/base_model.py#L109) repo, which originated from this one it seems. The cyclegan / pix2pix repo doesn't have this function in the code.
",probably meant post contrastive unpaired translation one function code,issue,negative,neutral,neutral,neutral,neutral,neutral
1587510126,"A bit late to the party, in reply to GZeta95's message: I used the last row of each epoch.

How I plotted the same (on my own dataset ofcourse):

      import matplotlib.pyplot as plt
      import re
      import pandas as pd
      from datetime import datetime
      
      def parse_date(logline):
          return datetime.strptime(logline[32:56], '%a %b   %d %H:%M:%S %Y')
      
      def read_log(filepath, run=""final""):
          '''
          Read a i2i loss_log.txt from given path
          Parameters:
              - filepath: path to txt file
              - run: either final (for final run) or an integer indicating the number of the run (eg 1, 2, 3)
      
          return: a pandas dataframe with all relevant columns
          '''
          with open(filepath, 'r') as file:
              lines=file.readlines()
              run_nr = 0
              all_data = []
              current_logdate = """"
              for ix, line in enumerate(lines):
                  if ""Training Loss"" in line:
                      run_nr+=1
                      current_logdate = parse_date(line).isoformat().replace(""T"", "" "")
                      print(f""new log with starting logdate {current_logdate}"")
                  else:
                      #if ix < 10:
                      line = re.sub('[^0-9a-zA-Z ._]+', '', line)
                      #print(line)
                      line_arr = line.split("" "")
                      line_data = {k: v for k, v in zip(line_arr[0::2], line_arr[1::2])}
                      line_data.update({""run"": run_nr, ""run_date"": current_logdate})
                      all_data.append(line_data)
                      #print({k: v for k, v in zip(k, v)})
              df = pd.DataFrame(all_data)
              df.epoch = df.epoch.astype(int)
              df.iters = df.iters.astype(int)
              for col in [""cycle_A"", ""cycle_B"", ""G_A"", ""G_B"", ""idt_A"", ""idt_B"", ""D_A"", ""D_B""]:
                  df[col] = df[col].astype(float)
              if type(run) == int:
                  df = df[df[""run""] == run]
              elif run == ""final"":
                  df =  df[df[""run""] ==  run_nr]
              df = df.groupby(by=""epoch"", group_keys=False).last().reset_index().sort_values(by=""epoch"")
              return df
      
      df = read_log(loss_log_file, run=""final"")
      for col in [""cycle_A"", ""cycle_B"", ""G_A"", ""G_B"", ""idt_A"", ""idt_B"", ""D_A"", ""D_B""]:
          df.plot(""epoch"", col)
          plt.title(col)
          plt.show()",bit late party reply message used last row epoch plotted import import import import return final read given path path file run either final final run integer number run return relevant open file line enumerate training loss line line print new log starting else line line print line zip run print zip col col col float type run run run run final run epoch epoch return final col epoch col col,issue,negative,positive,neutral,neutral,positive,positive
1584049857,"Hi @junyanz @taesungp, I would like to compare the performance of the CycleGANs with UNET generator v/s ResNet generator. I had a few questions:
1. Do you have training details of your pre-trained models? (How many epochs, loss curves, etc.)
2. Since the CycleGAN class is using the ResNet 9-blocks generator by default, I am assuming that the pre-trained models are for this generator. In such case, Do you have pre-trained models using the UNET generator? (256 or 128)",hi would like compare performance generator generator training many loss since class generator default assuming generator case generator,issue,negative,positive,positive,positive,positive,positive
1582067995,"> 你好！我訓練了40個epochs，batch_size=1，但是生成器的損失函數好像沒有收斂，這是為什麼呢？能否請您提供一些建議，謝謝

-For  the  generator,  its  Loss  drops  rapidly,  and  it  is  likely that the discriminator is too weak, causing thegenerator to easily ""fool"" the discriminator.
-For  the  discriminator,  the  Loss  drops  quickly,  which means  that  the  discriminator  is  very  strong,  and  thestrong discriminator means that the image generated by the generator is not realistic enough, which makesit easy for  the  discriminator  to  distinguish,  resulting  in  a  rapid loss of loss.
-In  other  words,  whether  it  is  a  discriminator  or  a generator. The level of loss does not represent the quality ofthe generator. For a good GAN network, its GAN Loss is often fluctuating.",generator loss rapidly likely discriminator weak causing easily fool discriminator discriminator loss quickly discriminator strong discriminator image generator realistic enough easy discriminator distinguish resulting rapid loss loss whether discriminator generator level loss represent quality generator good gan network gan loss often,issue,negative,positive,positive,positive,positive,positive
1573414742,"@nik13 sorry for the late reply I moved on to work on other projects. If you still need the solution post your email and I will share a jupyter notebook of the process

here is a the solution
`!python train.py --model pix2pix --dataroot /content/gdrive/MyDrive/projects/pix2pix_official/dataset/rectangle_640_watermark_v1/dataset --name watermark_remover_pix2pix_rectangle_datasetv1 --direction AtoB --save_epoch_freq 50 --n_epochs 800 --n_epochs_decay 800 --preprocess crop  --crop_size 256 --continue_train --epoch_count 1250 --display_id -1
`



 During training, you can train do augmentation by resizing and random cropping. Since the network is fully convolutional, this means that you can test on any size of the image, and it won't be affected.
 To elaborate, assume that you have an image of 800x600, you can train on square patches of 256x26, but you set the --resize_or_crop none, during testing, so the full image (original resolution) will be translated normally based on your saved weights/model",sorry late reply work still need solution post share notebook process solution python model name direction crop training train augmentation random since network fully convolutional test size image wo affected elaborate assume image train square set none testing full image original resolution normally based saved,issue,positive,positive,neutral,neutral,positive,positive
1572041527,"Did you solve this? I am having the exact same problem, I trained pix2pix with the resnet_9blocks generator and I get the exact same error when trying to load.",solve exact problem trained generator get exact error trying load,issue,negative,positive,positive,positive,positive,positive
1560545682,"> What did you do to address the reproducibility in both training and testing?
",address reproducibility training testing,issue,negative,neutral,neutral,neutral,neutral,neutral
1556708156,"Using a paired dataset, training the recurrent network was found to be very close rec_B to the real_B, and the fake_B was much different from both rec_B and real_B",paired training recurrent network found close much different,issue,negative,neutral,neutral,neutral,neutral,neutral
1550742039,"> I am not sure if we have this function `data_dependent_initialize` in our repo. Did you post it in the wrong repo?

Line 109 in base_model.py. I've fixed this problem by chaging the data_dependet_initialize function in cycle_gan_model.py. But there seems to have another problem that I can't get the training result in checkpoints. Sad.",sure function post wrong line fixed problem function another problem ca get training result sad,issue,negative,negative,neutral,neutral,negative,negative
1550736993,"i think the real_A means the source image of domain A, and the fake_B means the image of converting from domain A to domain B, and the rec_A means the image of converting from domain B to domain A.
Hope to help you",think source image domain image converting domain domain image converting domain domain hope help,issue,positive,neutral,neutral,neutral,neutral,neutral
1550338010,I am not sure if we have this function `data_dependent_initialize` in our repo. Did you post it in the wrong repo? ,sure function post wrong,issue,negative,neutral,neutral,neutral,neutral,neutral
1545695772,"Hi there, I am a beginner and try to build up a model with 48*48 input but meet some problems.
Could you please send me the all parameters you use for running train.py?

> 

",hi beginner try build model input meet could please send use running,issue,negative,neutral,neutral,neutral,neutral,neutral
1543280934,"after tarining, the out put imgs in /checkpoints,how did the fake_B looks like?
try python3 test.py
`--dataroot ./datasets/testA`
`--`",put like try python,issue,negative,neutral,neutral,neutral,neutral,neutral
1541110559,"Got it! I have tried it, and it works!!

Appreciate your help! Thanks",got tried work appreciate help thanks,issue,positive,positive,positive,positive,positive,positive
1532239199,No idea. @ayulockin and team might be able to help.,idea team might able help,issue,negative,positive,positive,positive,positive,positive
1532153757,It should take 1-2 days on a GTX 1080 GPU. I expect the training time to be much shorter as the GPUs have been much faster compared to 2017. ,take day expect training time much shorter much faster,issue,negative,positive,positive,positive,positive,positive
1532150360,"You may also need to train a new model on your own aerial images, as different aerial images have different data distributions. ",may also need train new model aerial different aerial different data,issue,negative,positive,neutral,neutral,positive,positive
1532148756,"The U-Net does not support 262x262 images. You need to use a different preprocess flag and resize images to 256x256 for example. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L49) for more details. (e.g., `--preprocess resize_and_crop ) ",support need use different flag resize example see line,issue,negative,neutral,neutral,neutral,neutral,neutral
1532125960,"People find that normalizing data into zero-mean data helps model training. See this [post] for more [details](https://developers.google.com/machine-learning/data-prep/transform/normalization). Also, our generator's final layer has a TanH layer, which will only produce data at [-1, 1]. ",people find data data model training see post also generator final layer tanh layer produce data,issue,negative,neutral,neutral,neutral,neutral,neutral
1532123452,One sanity check is that you manually copy and paste the images from the test data to the validation data and see if there is a difference. ,one sanity check manually copy paste test data validation data see difference,issue,negative,neutral,neutral,neutral,neutral,neutral
1532120564,You can crop the images during the training loop. We have implemented several options in `--preprocess'. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/14422fb8486a4a2bd991082c1cda50c3a41a755e/options/base_options.py#L49) for more details. ,crop training loop several see line,issue,negative,neutral,neutral,neutral,neutral,neutral
1532114351,"The `strict=False` option can deal with missing parameters, but it cannot deal with size mismatch. For example, in your case, `model.26.weight` has shape [3, 64, 7, 7] from the checkpoint, but the model architecture requires it to be [64, 128, 3, 3]. 

There is no good way to resolve this, because it just means they are two different layers, which are not meant to be loaded from each other. If you'd still like to load the rest of the weights from the checkpoints, you can do so by removing the problematic weights from the state dict. For example, after you load state_dict from the checkpoint, you can remove `model.26.weight` and `model.26.bias` from the dictionary. ",option deal missing deal size mismatch example case model weight shape model architecture good way resolve two different meant loaded still like load rest removing problematic state example load remove model weight model bias dictionary,issue,negative,positive,positive,positive,positive,positive
1522822422,"Thanks for your reply!
The further question is the cropping method should I implement on both domain at the same time?
eg, (imageA, imageB) -> cropping by paired, and put in training loop
or (bunch of imageA already include cropping, bunch of imageB already include cropping) -> as a dataset to input",thanks reply question method implement domain time paired put training loop bunch already include bunch already include input,issue,negative,positive,positive,positive,positive,positive
1522752762,"Hello @junyanz, thank you so much for your reply!

Sorry if it was not clear before. I meant the results on the test data are worse than the results on the validation data and not the training data. It does seem that images from the test set are not very different from those in the validation set. Hence, the performance difference between validation and test sets is confusing.

Thank you for the suggestion about using the same augmentations throughout. I will try it soon!",hello thank much reply sorry clear meant test data worse validation data training data seem test set different validation set hence performance difference validation test thank suggestion throughout try soon,issue,negative,negative,negative,negative,negative,negative
1522379656,"It depends on your application (whether you want to learn something simple or not, or whether your images are complex or not). You may want to use cropping during preprocessing step. ",application whether want learn something simple whether complex may want use step,issue,negative,negative,negative,negative,negative,negative
1522378566,"Not sure. ""Worse"" means that your results on test data are worse than results on training data or validation data? 

One suggestion is to use the same augmentation/preprocessing steps across training, validation, and test. ",sure worse test data worse training data validation data one suggestion use across training validation test,issue,negative,negative,negative,negative,negative,negative
1515617261,"> it might be related to data loading.
> 
> If you use our datasets, have you downloaded the data? If you use your own datasets, you may want to check whether the data files are corrupt or not.

Thanks. You map's data can run smoothly.  I'm not sure whether the data files are corrupt when I transfer datas. Howerver, I found these data can run  in windows 11/10 system.  Are there some differences between ubuntu and windows?",might related data loading use data use may want check whether data corrupt thanks map data run smoothly sure whether data corrupt transfer found data run system,issue,positive,negative,neutral,neutral,negative,negative
1514247182,"> There is no reason, and it's really an imperfection of the training code.

I got it, thank you for replying!",reason really imperfection training code got thank,issue,negative,positive,positive,positive,positive,positive
1514244584,"> I am not exactly how you visualize this, but one issue could be that you are doing followed by . The resulting tensor may not be of BCHW shape.`B_img = np.load(B_path)``B = torch.from_numpy(B_img).to(torch.float32).unsqueeze(0)`

Thank you very much for your answer! I'll give it a try!!",exactly visualize one issue could resulting tensor may thank much answer give try,issue,negative,positive,positive,positive,positive,positive
1513779034,"it might be related to data loading. 

If you use our datasets, have you downloaded the data? 
If you use your own datasets, you may want to check whether the data files are corrupt or not. ",might related data loading use data use may want check whether data corrupt,issue,negative,negative,negative,negative,negative,negative
1513759885,"We don't have a Gradio/HF GUI as the model can be used in many different applications, each of which requires a separate UI. People have built GUI for sketch2photo applications, such as https://affinelayer.com/pixsrv/. 

Feel free to make a GUI!",model used many different separate people built feel free make,issue,positive,positive,positive,positive,positive,positive
1513757719,"I am not exactly how you visualize this, but one issue could be that you are doing `B_img = np.load(B_path)` followed by `B = torch.from_numpy(B_img).to(torch.float32).unsqueeze(0)`. The resulting tensor may not be of BCHW shape. ",exactly visualize one issue could resulting tensor may shape,issue,negative,positive,positive,positive,positive,positive
1513741575,"There is no reason, and it's really an imperfection of the training code. ",reason really imperfection training code,issue,negative,positive,positive,positive,positive,positive
1513737802,"We don't have such a model. Actually, the setting of CycleGAN is likely not the best for tackling image colorization task, because image colorization can be trained in a paired image-to-image setting, rather than unpaired. ",model actually setting likely best tackling image colorization task image colorization trained paired setting rather unpaired,issue,positive,positive,positive,positive,positive,positive
1490351250,"> We normalized the input from [0, 1] to [-1, 1]. To undo it, you can call tensor2im. https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/util.py#L9

Where do you do the conversion to [-1, 1], in `get_transform`?",input undo call conversion,issue,negative,neutral,neutral,neutral,neutral,neutral
1484550342,"&nbsp;python test.py --dataroot ___ --name ___ --model test --no_dropout






------------------&nbsp;原始邮件&nbsp;------------------
发件人:                                                                                                                        ""junyanz/pytorch-CycleGAN-and-pix2pix""                                                                                    ***@***.***&gt;;
发送时间:&nbsp;2023年3月27日(星期一) 中午11:57
***@***.***&gt;;
***@***.***&gt;;""State ***@***.***&gt;;
主题:&nbsp;Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to only save the conversion results of AtoB when testing? (Issue #1544)





 
hello ,can you tell me how you did it
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you modified the open/close state.Message ID: ***@***.***&gt;",python name model test state save conversion testing issue hello tell reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1483815439,"Even if the default parameter for -- model is cycle_ gan, but when we execute the test command, we still need to add the parameter **-- model cycle_ gan**, this problem will be solved. As for the specific reasons, I am not clear",even default parameter model gan execute test command still need add parameter model gan problem specific clear,issue,negative,positive,neutral,neutral,positive,positive
1476968364,"> What is your training command? and how big is your training image?

 python train.py --dataroot cell_data --model pix2pix --direction BtoA --display_id -1 --gpu_id 1 --preprocess none 
Training image is 262 to 262 pixels.",training command big training image python model direction none training image,issue,negative,neutral,neutral,neutral,neutral,neutral
1472295694,check net.eval() function.. disable it and you get better results,check function disable get better,issue,negative,positive,positive,positive,positive,positive
1469089634,"@taesungp 
Thanks for your reply!
Yeah I guess I should stick to CycleGAN.

Yeah I was quite surprised to see how dififusion models generate much higher quality images than GANs. So I was tempted to try diffusion models, but then most of the diffusion models have something to do with texts, such as image generation from a text or image to image translation based on a input text. So I was wondering if diffusion models are not suitable for img2img translation without any texts. Diffusion process (denoising from gaussian noise) seems irrelevant to texts, but it might be unclear to me because I haven't actually read the paper yet.",thanks reply yeah guess stick yeah quite see generate much higher quality try diffusion diffusion something image generation text image image translation based input text wondering diffusion suitable translation without diffusion process noise irrelevant might unclear actually read paper yet,issue,positive,positive,neutral,neutral,positive,positive
1468808485,"The model might overfit the training set. To prevent overfitting, you can either use a larger dataset or apply more aggressive augmentation (see the option `--preprocess` for more details.)",model might overfit training set prevent either use apply aggressive augmentation see option,issue,negative,neutral,neutral,neutral,neutral,neutral
1468807130,"Hello, 

there should be no immediate downside of running it at higher resolution. 

Pix2Pix works better than CycleGAN **_if_** the underlying layout remains the same. For example, in the case of Facades and Cityscapes, one domain describes the layout of the other. In medical imaging, aligning the inputs and outputs of the training set may be nontrivial. For example, in [this paper](https://www.doc.ic.ac.uk/~bglocker/public/mednips2017/med-nips_2017_paper_8.pdf), the authors try image translation between MR and CT images. But because it's nearly impossible to get the MR and CT image pair of the same patient in the exactly same camera view, CycleGAN ends up performing better than Pix2Pix. 

Diffusion models will be great if you want text-based control of natural images. Look at [these images](https://pix2pixzero.github.io/) for example. They are much higher quality, thanks to 6 years of progress in research, modelling, data, and compute. ",hello immediate downside running higher resolution work better underlying layout remains example case one domain layout medical training set may example paper try image translation nearly impossible get image pair patient exactly camera view better diffusion great want control natural look example much higher quality thanks progress research data compute,issue,positive,positive,positive,positive,positive,positive
1468781431,What is your training command? and how big is your training image? ,training command big training image,issue,negative,neutral,neutral,neutral,neutral,neutral
1465849859,"RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 65 but got size 64 for tensor number 1 in the list
Can you help me pleae?
I used option --preprocess none.
The image size needs to be a multiple of 4. The loaded image size was (775, 522), so it was adjusted to (776, 520). This adjustment will be done to all images whose sizes are not multiples of 4. I got this message first.",size must match except dimension size got size tensor number list help used option none image size need multiple loaded image size adjustment done whose size got message first,issue,negative,positive,positive,positive,positive,positive
1462217343,"> Another thing might be evaluating the model with eval() mode turned on and off ([link](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L66)). Could you run the test mode with and without the `--eval` option and see if that makes a difference?

I am also working on same project, to convert RGB to IR. I also have same observation as above. But in my case, running test without '--eval' does not do much good. My results are not as bad as above. But, there is certainly significant loss of detail in the fake images when running test. I have attached some images below as an example.

While training, the fake image (left) and real image (right) looks something like this.
![image](https://user-images.githubusercontent.com/106657733/224066573-a47b3799-1b1b-485c-af39-a76e732195b3.png)



And during test the fake image (left) and real image (right) looks like this.
![image](https://user-images.githubusercontent.com/106657733/224065930-20a7f550-eb0c-4648-95aa-965370e23695.png)


Is there any other way for me to improve these results? I am currently using part of KAIST dataset.",another thing might model mode turned link could run test mode without option see difference also working project convert also observation case running test without much good bad certainly significant loss detail fake running test attached example training fake image left real image right something like image test fake image left real image right like image way improve currently part,issue,negative,negative,neutral,neutral,negative,negative
1461619836,"@oliver0922 Hello！
I'm also trying to change the style from day to night recently. Do you find a similar model?",also trying change style day night recently find similar model,issue,negative,neutral,neutral,neutral,neutral,neutral
1448295449,"Hello, I met the same problem as you when I was training. May I ask how you solved it? My training instruction is python train.py --dataroot./datasets/ xx/--name xx_cyclegan --model cycle_gan --batch_size 4

The test is

 python test.py --dataroot./datasets/xx/ --name xx_cyclegan --model cycle_gan",hello met problem training may ask training instruction python name model test python name model,issue,negative,neutral,neutral,neutral,neutral,neutral
1442439336,"Any update @workingpotato !?

> > Have you solve the problem? I am also working on using A1,A2,A3 to predict B
> 
> No not yet, do u have any idea for it?

",update solve problem also working predict yet idea,issue,negative,neutral,neutral,neutral,neutral,neutral
1439486629,"Okay Thanks!

On Tue, 21 Feb 2023, 7:27 am Kudo Khang, ***@***.***> wrote:

> The default is latest_net_G.pth or you can set checkpoint which argument
> --epoch 195 (or any epoch number)
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1543#issuecomment-1437774169>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AY2GYVTMHM64JWWCH4RYAPTWYQR2FANCNFSM6AAAAAAU4UPQP4>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",thanks tue wrote default set argument epoch epoch number reply directly view id,issue,negative,positive,positive,positive,positive,positive
1438744554,"> @GiorgioBarnabo Hi, I also used MelSpectrogram (e.g., float32) as input to do the translation. As @junyanz mentioned earlier, the generator only outputs values between [-1, 1]. I was wondering how to convert the generated values back to melspectrogram, which will be fed into the subsequent CNN model (e.g., a 2-class classification model). How did you do that?

Hi, did you ever manage to work out how to output your generated values back to melspectrogram? Stuck on what to edit to get my output values back as melspoectrograms rather than png",hi also used float input translation generator wondering convert back fed subsequent model classification model hi ever manage work output back stuck edit get output back rather,issue,negative,neutral,neutral,neutral,neutral,neutral
1437774169,The default is latest_net_G.pth or you can set checkpoint which argument --epoch 195 (or any epoch number),default set argument epoch epoch number,issue,negative,neutral,neutral,neutral,neutral,neutral
1431074258,"> I am not familiar with onnx conversion. But the default CycleGAN uses instance normalization.

yep indeed the pretrained cycleGAN es horse2zebra don't have accuracy loss ( for pix2pix GAN examples es. sat2map it decreased a lot, tho). I indeed edit a bit the CycleGan model for learning the structure better, and i could not find why there were this output differences.",familiar conversion default instance normalization yep indeed e accuracy loss gan e lot tho indeed edit bit model learning structure better could find output,issue,negative,positive,positive,positive,positive,positive
1430757116,"If dataset A is cropped and dataset B is not, you probably need to randomly crop patches from dataset B as well before you start CycleGAN training. ",probably need randomly crop well start training,issue,negative,negative,negative,negative,negative,negative
1430754861,You need to run `python -m visdom.server` first. ,need run python first,issue,negative,positive,positive,positive,positive,positive
1430438515,I am not familiar with onnx conversion. But the default CycleGAN uses instance normalization. ,familiar conversion default instance normalization,issue,negative,positive,positive,positive,positive,positive
1430430406,"It's more of a GPU memory constraint rather than a mathematic constraint. In practice, people just try different image resolutions until they hit the memory limit. ",memory constraint rather mathematic constraint practice people try different image hit memory limit,issue,negative,neutral,neutral,neutral,neutral,neutral
1429686167,"I meet the same error.But I set  --model=pix2pix  --netG=unet_256 --norm=batch solve this problem. I think if you meet the same error, it maybe your test env different from train env. Hope this helps.",meet set solve problem think meet error maybe test different train hope,issue,negative,neutral,neutral,neutral,neutral,neutral
1425872130,"Hey, the latest models are being saved as latest_model_G_A...

You should check out the names first. Then you can change their names.

I hope this helps.",hey latest saved check first change hope,issue,positive,positive,positive,positive,positive,positive
1425540805,I understood the issue is coming from a pytorch function model.eval() that causes a freeze of some operations in the model to inference mode that drop accuracy. Es. batch normalization.,understood issue coming function freeze model inference mode drop accuracy e batch normalization,issue,negative,neutral,neutral,neutral,neutral,neutral
1408684798,"Padding size probably didn't impact on receptive field of discriminator. 
Receptive field size stems from layers kernel size and stride size. This post provide good explanation of the idea (it consider different architecture, but main idea the same): https://sahiltinky94.medium.com/understanding-patchgan-9f3c8380c207

IMHO, Padding give us ability consider even 70x70 images as several patches, where each have some spacial offset (i.e. patches may partially lying outside the image), and thus we can even pass 24x24 input tensor.",padding size probably impact receptive field discriminator receptive field size kernel size stride size post provide good explanation idea consider different architecture main idea padding give u ability consider even several offset may partially lying outside image thus even pas input tensor,issue,negative,positive,positive,positive,positive,positive
1400078847,"Traceback (most recent call last):
  File ""train.py"", line 29, in <module>
    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options
  File ""C:\Users\ADMIN\Downloads\face-sketch-pix2pix-main\face-sketch-pix2pix-main\data\__init__.py"", line 57, in create_dataset
    data_loader = CustomDatasetDataLoader(opt)
  File ""C:\Users\ADMIN\Downloads\face-sketch-pix2pix-main\face-sketch-pix2pix-main\data\__init__.py"", line 73, in __init__
    self.dataset = dataset_class(opt)
  File ""C:\Users\ADMIN\Downloads\face-sketch-pix2pix-main\face-sketch-pix2pix-main\data\aligned_dataset.py"", line 22, in __init__
    self.AB_paths = sorted(make_dataset(self.dir_AB, opt.max_dataset_size))  # get image paths
  File ""C:\Users\ADMIN\Downloads\face-sketch-pix2pix-main\face-sketch-pix2pix-main\data\image_folder.py"", line 25, in make_dataset
    assert os.path.isdir(dir), '%s is not a valid directory' % dir
AssertionError: ./datasets/cuhk\train is not a valid directory
(pytorch-CycleGAN-and-pix2pix) PS C:\Users\ADMIN\Downloads\face-sketch-pix2pix-main\face-sketch-pix2pix-main>


Someone please help me to solve this error",recent call last file line module opt create given file line opt file line opt file line sorted get image file line assert valid directory valid directory someone please help solve error,issue,positive,neutral,neutral,neutral,neutral,neutral
1386147660,"The links are missing, so I cannot tell the quality. It sounds like there's some discrepancy between training and testing, like the input image resolution for example. Reducing this gap may help. ",link missing tell quality like discrepancy training testing like input image resolution example reducing gap may help,issue,positive,negative,negative,negative,negative,negative
1386126666,"The size on disk does not exactly correlate with the image resolution, because the images can be compressed. Flat monotonic images will take smaller space than detailed complex images.",size disk exactly correlate image resolution compressed flat monotonic take smaller space detailed complex,issue,negative,positive,neutral,neutral,positive,positive
1380013392,I solved the issue if any needs help in that case contact me.,issue need help case contact,issue,negative,neutral,neutral,neutral,neutral,neutral
1379905260,you can either resize the image b or add padding or empty pixels,either resize image add padding empty,issue,negative,negative,neutral,neutral,negative,negative
1367989461,"Found the solution here: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/782#issuecomment-538537477

Changing from instance to batch norm solves this specific border issue. The style transfer is less dramatic with batch norm apparently, but I will try to remedy this by tweaking other parameters.",found solution instance batch norm specific border issue style transfer le dramatic batch norm apparently try remedy,issue,negative,negative,negative,negative,negative,negative
1366369131,"place all frames of the input video as images, run the model on all of them, compile them back into a video
",place input video run model compile back video,issue,negative,neutral,neutral,neutral,neutral,neutral
1363543958,"> Have you solve the problem? I am also working on using A1,A2,A3 to predict B

No not yet, do u have any idea for it?",solve problem also working predict yet idea,issue,negative,neutral,neutral,neutral,neutral,neutral
1362570893,"Have you solve the problem? I am also working on using A1,A2,A3 to predict B",solve problem also working predict,issue,negative,neutral,neutral,neutral,neutral,neutral
1360400641,"> How many images did you use in the dataset? This dataset may be a challenging one because the animal faces are not aligned.

TrainA - 1300 (photos of cats and dogs), TrainB - 500 (stylistic paint brush background). Would you please elaborate on the alignment of the faces and how to improve the data? Should I be avoiding style transfer with CycleGAN, and try for object transfiguration goals instead?",many use may one animal dog stylistic paint brush background would please elaborate alignment improve data style transfer try object transfiguration instead,issue,positive,positive,positive,positive,positive,positive
1360399848,"> Maybe you could use some data augmentation. Currently, both your load_size and crop_size are 512. Maybe use load_size 580 and crop_size 512. Certain artifacts will go away if the model is trained long enough.

Images are all 512x512, so would I still benefit from load_size 580? I trained out 400 epochs with same issue.",maybe could use data augmentation currently maybe use certain go away model trained long enough would still benefit trained issue,issue,positive,positive,neutral,neutral,positive,positive
1360381956,"For crop_size=512, batch_size=2 will require a significant amount of GPU memory. I am not sure how much memory you have per GPU. 

I am not sure if you need to divide the batch_size by the number of threads before feeding it to DataLoader. 

A permanent solution is to replace the Data Parallel with DDP. But we currently don't have the capacity to add it. Any pull request regarding it would be greatly appreciated. ",require significant amount memory sure much memory per sure need divide number feeding permanent solution replace data parallel currently capacity add pull request regarding would greatly,issue,positive,positive,positive,positive,positive,positive
1360370395,"Maybe you could use some data augmentation. Currently, both your load_size and crop_size are 512. Maybe use load_size 580 and crop_size 512. Certain artifacts will go away if the model is trained long enough. ",maybe could use data augmentation currently maybe use certain go away model trained long enough,issue,negative,positive,neutral,neutral,positive,positive
1360367427,How many images did you use in the dataset? This dataset may be a challenging one because the animal faces are not aligned. ,many use may one animal,issue,negative,positive,positive,positive,positive,positive
1355503232,"I had the issues with multi-GPU as well. I could not use higher batch sizes than the number of GPUs, otherwise I was getting memory errors. Two things resolved that problem: 
1) The batch_size given to torch.utils.data.DataLoader should be per_worker. So, I changed the code under data/__init__.py as follows: 
 `class CustomDatasetDataLoader():
    """"""Wrapper class of Dataset class that performs multi-threaded data loading""""""

    def __init__(self, opt):
        """"""Initialize this class

        Step 1: create a dataset instance given the name [dataset_mode]
        Step 2: create a multi-threaded data loader.
        """"""
        self.opt = opt
        dataset_class = find_dataset_using_name(opt.dataset_mode)
        self.dataset = dataset_class(opt)
        print(""dataset [%s] was created"" % type(self.dataset).__name__)
        per_worker_batch_size = opt.batch_size//int(opt.num_threads) if opt.batch_size>=int(opt.num_threads) else 1
        self.dataloader = torch.utils.data.DataLoader(
            self.dataset,
            batch_size=per_worker_batch_size,
            shuffle=not opt.serial_batches,
            num_workers=int(opt.num_threads),
            pin_memory=True)`

2) Then for high batch sizes, I faced with shared memory problem. Since I am using docker, I just increased the --shm-size to 1G while starting the docker container. Now I can use all the GPUs up with a batch size that can fit to their memory. Hope that helps. 
Of course dont forget setting --gpu_ids 0,1,2,3 and batch_size=per_worker_batch_size*num_workers (16 for example) ",well could use higher batch size number otherwise getting memory two resolved problem given code class wrapper class class data loading self opt initialize class step create instance given name step create data opt opt print type else high batch size faced memory problem since docker starting docker container use batch size fit memory hope course dont forget setting example,issue,positive,positive,positive,positive,positive,positive
1351129177,"Hello I just tested your model and it worked, use for testing: `--model pix2pix --no_dropout --direction AtoB --preprocess scale_width_and_crop --load_size 768 --crop_size 512`
you can set the crop size to 768 too",hello tested model worked use testing model direction set crop size,issue,negative,neutral,neutral,neutral,neutral,neutral
1350994836,"According to my experience, you may need to check your model structure, espcially norm and act function. I attemped to use attention block in my gan network , but i foud colorful spot always, i think it relate to the attention block. And I find I forget to add  norm and act funtion follow since i do some change on other's code. Hope this experience can help others meet the same problem",according experience may need check model structure norm act function use attention block gan network foud colorful spot always think relate attention block find forget add norm act follow since change code hope experience help meet problem,issue,negative,positive,positive,positive,positive,positive
1346331376,"hello what option you used aligned or unaligned?
",hello option used unaligned,issue,negative,neutral,neutral,neutral,neutral,neutral
1344393053,"HAVE found out why: for those newbies like me, its because my test dataset wasn't divisive by 128/256",found like test divisive,issue,negative,neutral,neutral,neutral,neutral,neutral
1343992999,"Hi, i am using rectangular images. Have heeded your advice and trained my model using --preprocess crop, in my test.py i have used --preprocess None but encounter this error : RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 5 but got size 4 for tensor number 1 in the list. 

I have a feeling its because my input data isn't in powers of 2 ( 355 x750 ) . is this why? ",hi rectangular advice trained model crop used none encounter error size must match except dimension size got size tensor number list feeling input data,issue,negative,neutral,neutral,neutral,neutral,neutral
1339028175,"> > > I see the paper set identity loss as 0.5lambda and cycle-consistent loss as lambda to control the relative importance. What if GAN loss just about 0.5-1.0 around but idt loss and cycle loss up to at least 10 at the first. Will it be ignoring the adversarial loss while training? thanks alot. : ) by the way... when i change the framework to 3d, my gpu memory ran out and I exchange to torch.cuda.amp mode, with gradscaler(), will it make the generator performance worse? I use it as medical image generating and my MAE just around 90... BAD performance... : (
> > 
> > 
> > Hi and sorry to interrupt, I am also working on generating 3d medical image, but I found some difficulties in adjusting the net framework. Did you also generated 3d results that represented by lots of images ?
> 
> Sure. What's your problem? I used to stuck in here for much long time.

Thanks for your reply, I wonder if you could please show your changed cycle-gan code generating 3d images on your github? I found it difficult to change the code to generate 3d images for a deep learning beginner like me, but i really want to watch the results on cycle-gan generating 3d images.",see paper set identity loss loss lambda control relative importance gan loss around loss cycle loss least first loss training thanks way change framework memory ran exchange mode make generator performance worse use medical image generating mae around bad performance hi sorry interrupt also working generating medical image found net framework also lot sure problem used stuck much long time thanks reply wonder could please show code generating found difficult change code generate deep learning beginner like really want watch generating,issue,negative,negative,neutral,neutral,negative,negative
1336453097,"> Hello The following worked for me: The images in folder A and B must have the same name and size set the flag --fold_AB to an existing folder

yes, i found the problems that the folder A and B didn't have the same size",hello following worked folder must name size set flag folder yes found folder size,issue,negative,neutral,neutral,neutral,neutral,neutral
1336316386,"Hello The following worked for me:
The images in folder A and B must have the same name and size
set the flag --fold_AB to an existing folder
",hello following worked folder must name size set flag folder,issue,negative,neutral,neutral,neutral,neutral,neutral
1328217256,"I encountered the same issue, and passed the gpu_ids as such:
--gpu_ids 0,1,2,3
when using 'nvidia-smi' command it seems that only one GPU is being utilized.
Am I doing something wrong?",issue command one something wrong,issue,negative,negative,negative,negative,negative,negative
1328203072,"> At present, I want to color infrared video. Is there any good way? #1436 #

may be no, i also want to know how to generate video.
",present want color infrared video good way may also want know generate video,issue,positive,positive,positive,positive,positive,positive
1327541567,"Hi, there. I am sticking for a more stable training also strengthen D for clearer synthesis.
Would it work for just add GP in PatchGAN?
Thanks.",hi sticking stable training also strengthen clearer synthesis would work add thanks,issue,positive,positive,positive,positive,positive,positive
1324737035,"@GiorgioBarnabo Hi, I also used MelSpectrogram (e.g., float32) as input to do the translation. As @junyanz mentioned earlier, the generator only outputs values between [-1, 1]. I was wondering how to convert the generated values back to melspectrogram, which will be fed into the subsequent CNN model (e.g., a 2-class classification model). How did you do that? ",hi also used float input translation generator wondering convert back fed subsequent model classification model,issue,negative,neutral,neutral,neutral,neutral,neutral
1321052671,"you  can refer to#1477
A reply as follows:""One reason could be that our datasets are small, so larger batch size leads to faster overfitting.""",refer reply one reason could small batch size faster,issue,negative,negative,negative,negative,negative,negative
1321049127,"> > I see the paper set identity loss as 0.5lambda and cycle-consistent loss as lambda to control the relative importance. What if GAN loss just about 0.5-1.0 around but idt loss and cycle loss up to at least 10 at the first. Will it be ignoring the adversarial loss while training? thanks alot. : ) by the way... when i change the framework to 3d, my gpu memory ran out and I exchange to torch.cuda.amp mode, with gradscaler(), will it make the generator performance worse? I use it as medical image generating and my MAE just around 90... BAD performance... : (
> 
> Hi and sorry to interrupt, I am also working on generating 3d medical image, but I found some difficulties in adjusting the net framework. Did you also generated 3d results that represented by lots of images ?

Sure. What's your problem? I used to stuck in here for much long time.",see paper set identity loss loss lambda control relative importance gan loss around loss cycle loss least first loss training thanks way change framework memory ran exchange mode make generator performance worse use medical image generating mae around bad performance hi sorry interrupt also working generating medical image found net framework also lot sure problem used stuck much long time,issue,negative,negative,neutral,neutral,negative,negative
1320002151,"> I see the paper set identity loss as 0.5lambda and cycle-consistent loss as lambda to control the relative importance. What if GAN loss just about 0.5-1.0 around but idt loss and cycle loss up to at least 10 at the first. Will it be ignoring the adversarial loss while training? thanks alot. : ) by the way... when i change the framework to 3d, my gpu memory ran out and I exchange to torch.cuda.amp mode, with gradscaler(), will it make the generator performance worse? I use it as medical image generating and my MAE just around 90... BAD performance... : (

Hi and sorry to interrupt, I am also working on generating 3d medical image, but I found some difficulties in adjusting the net framework. Did you also generated 3d results that represented by lots of images ?",see paper set identity loss loss lambda control relative importance gan loss around loss cycle loss least first loss training thanks way change framework memory ran exchange mode make generator performance worse use medical image generating mae around bad performance hi sorry interrupt also working generating medical image found net framework also lot,issue,negative,negative,negative,negative,negative,negative
1312668816,Thanks @junyanz . We tested it too and it seems to be so. Maybe attention U-net would improve the results. Will update you on it... ,thanks tested maybe attention would improve update,issue,positive,positive,positive,positive,positive,positive
1308352301,"And the key setting of train script below:
      --gan_mode vanilla \
      --direction BtoA \
      --no_dropout \
      --batch_size 36 \
      --gpu_ids 0,1,2,3,4,5,6,7 \
      --pretrained_path ***.

the key setting of test script below:
--model_suffix _B
--model test
--phase test
--no_dropout",key setting train script vanilla direction key setting test script model test phase test,issue,negative,neutral,neutral,neutral,neutral,neutral
1306728143,no need to push to coda in this case,need push coda case,issue,negative,neutral,neutral,neutral,neutral,neutral
1296130294,"> hello, I have tried your code to train the DICOM image of scanner. but the Dicom image is 16bit grayscale image which can not traited by PIL.image objet. Have you any idea how to overcome this problem? can we use mode 'I' (32bit int) for data input?
> 
> Thank you

hi! Have you solved this?",hello tried code train image scanner image bit image idea overcome problem use mode bit data input thank hi,issue,negative,neutral,neutral,neutral,neutral,neutral
1291156730,We just updated the pix2pix colab. Could you download the latest code and try it again? ,could latest code try,issue,negative,positive,positive,positive,positive,positive
1291147587,This seems to be a cuda-related issue. Are you able to run basic PyTorch command on GPU? ,issue able run basic command,issue,negative,positive,positive,positive,positive,positive
1291145500,"Model training without any augmentation (i.e., `-preprocess none`) might cause model overfitting. We recommend that you apply certain preprocessing steps unless you have a huge dataset. ",model training without augmentation none might cause model recommend apply certain unless huge,issue,positive,positive,positive,positive,positive,positive
1291140913,"Not sure if we can help you with that, as the details can be very diverse. I think you can write your own code by looking at `test.py`, which shows how to instantiate the model and do inference.",sure help diverse think write code looking model inference,issue,positive,positive,positive,positive,positive,positive
1291135914,"Right, so there is a tradeoff as you pointed out. Either you can compare among losses within the same model, or compare them across different lambdas. For simplicity, we chose the former. Perhaps you can modify `get_current_losses()` so that it returns both numbers (unnormalized and normalized by the lambda)?",right pointed either compare among within model compare across different simplicity chose former perhaps modify lambda,issue,negative,positive,neutral,neutral,positive,positive
1291134578,"This is hard to resolve. A few solutions: 
1. You can use `--continue_train` to restore your training epoches, and see if it fixed your issues. 
2. You need to add many printing functions (to the data loader, to forward/backward functions) and see where the program is stuck. If it is the data loader, you may want to print the image file and check whether the image has been corrupted.
3. It might be related to memory. You may want to monitor the memory usage. ",hard resolve use restore training see fixed need add many printing data loader see program stuck data loader may want print image file check whether image corrupted might related memory may want monitor memory usage,issue,negative,positive,neutral,neutral,positive,positive
1291131956,"I suppose you can skip loading the discriminator weights by modifying [load_networks()](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/551117b13ff017d852d067a7fa76e138d43dada5/models/base_model.py#L189). Basically, if there is not an existing file at `load_path`, skip loading. Insert the below code block at [line 189](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/551117b13ff017d852d067a7fa76e138d43dada5/models/base_model.py#L189).

```python
if not os.path.exists(load_path):
    continue
```",suppose skip loading discriminator basically file skip loading insert code block line python continue,issue,negative,neutral,neutral,neutral,neutral,neutral
1288112386,"Hi there,

I am trying to use a pretrained monet2photo model which has a similar domain to my own dataset (I theorise as such).
However, when I use the `--continue_train` flag, the training tries to load G_A, G_B, D_A, and D_B.
monet2photo is the only generator direction available for download as far as I can tell. So what I'm doing is copying latest_net_G from monet2photo into two identical generators which will serve as the pretrained generators for G_A and G_B.
However, the training won't continue unless I also have latest_net_D_A.pth and latest_net_D_B.pth in the checkpoint folder. How can I tell the training to not look for them, and randomly initialise new discriminators?

Thanks for your help! :)",hi trying use model similar domain however use flag training load generator direction available far tell two identical serve however training wo continue unless also folder tell training look randomly new thanks help,issue,positive,positive,positive,positive,positive,positive
1283085968,"Issue was training dataset size only, so no issue.",issue training size issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1282251763,"> OOOOh! I note that I used resne_9blocks as backbone in some training and got some parameter files with the size of 44M.

Maybe...OK...It is also not the parameter file of resnet_9blocks or unet_128 or resnet_6block.

It is the mater about gan_mode='lsgan'.",note used backbone training got parameter size maybe also parameter file mater,issue,negative,neutral,neutral,neutral,neutral,neutral
1282237182,"> 

OOOOh! I note that I used resne_9blocks as backbone in some training and got
some parameter files with the size of 44M.",note used backbone training got parameter size,issue,negative,neutral,neutral,neutral,neutral,neutral
1282201430,"This may be due to incomplete model parameters.

I trained several models with `pix2pix unet 256` and note that those generators can't be loaded which file size is little then 208M.",may due incomplete model trained several note ca loaded file size little,issue,negative,negative,negative,negative,negative,negative
1279645551,"You are right. After I use a higher learning rate, the test effect is very poor when the loss has reached an ideal level. There are many noises in the pictures, and I perform well when I keep the learning rate unchanged in large batches. Of course, because the ""scale_width"" option cannot be used normally before, I use the default preprocess. I don't know whether it is related to it.",right use higher learning rate test effect poor loss ideal level many perform well keep learning rate unchanged large course option used normally use default know whether related,issue,negative,positive,positive,positive,positive,positive
1278798638,"Please let me know if the branch name need to be changed, if so I will create a new pull request.",please let know branch name need create new pull request,issue,positive,positive,positive,positive,positive,positive
1276936114,"> I also have this problem, how did you solve it.

did you solve it",also problem solve solve,issue,negative,neutral,neutral,neutral,neutral,neutral
1275268288,You can remove `--use_wandb` and add `--display_id -1` in your colab training script. ,remove add training script,issue,negative,neutral,neutral,neutral,neutral,neutral
1275227788,"You are right to point out the general rule of thumb about determining the learning rate. However, it's still empirical particularly for the GAN training. I'd just try it with the same learning rate, and also with a larger learning rate.

We actually recommend sticking to the batch size of 1, because overfitting is a big issue on Pix2Pix and CycleGAN. Note that the datasets we used in this work are quite small. To prevent overfitting, having a small batch size helps. ",right point general rule thumb learning rate however still empirical particularly gan training try learning rate also learning rate actually recommend sticking batch size big issue note used work quite small prevent small batch size,issue,negative,positive,neutral,neutral,positive,positive
1271202332,"I have another question. I used your code in RTX3090 and set the batchsize to 4. In the past, I should increase the learning rate at the same time, but I did not see the tip document that doubled the learning rate. So, do I need to double the learning rate? If so, please ask me how to set it",another question used code set past increase learning rate time see tip document doubled learning rate need double learning rate please ask set,issue,positive,negative,negative,negative,negative,negative
1268664524,"> ![Screen Shot 2022-10-04 at 4 39 38 PM](https://user-images.githubusercontent.com/1924757/193922725-51018560-2c2b-463c-a14e-1476a59afef0.png) Not sure if you have succeeded to run all the steps

I guess so but I am not sure. Every other step looks fine. Thank you.

![image](https://user-images.githubusercontent.com/110073344/194110450-032de64c-367e-4005-ada7-ae826ab0e733.png)
![image](https://user-images.githubusercontent.com/110073344/194110725-262ced71-b085-404d-b116-3485cf082ee9.png)
![image](https://user-images.githubusercontent.com/110073344/194110957-78c3cf41-80b7-4c26-a7ac-28780bd47f33.png)
![image](https://user-images.githubusercontent.com/110073344/194111108-18fd6c52-0e72-4245-969b-d05d8b7f958d.png)
![image](https://user-images.githubusercontent.com/110073344/194111179-906a277a-ff7c-4f33-aed8-02fa72099938.png)
![image](https://user-images.githubusercontent.com/110073344/194111250-2f7fc459-1dee-4d33-90a7-011ab6f3ebd0.png)
![image](https://user-images.githubusercontent.com/110073344/194111393-b274d078-8bc7-46c5-9994-94195c2ddafb.png)
![image](https://user-images.githubusercontent.com/110073344/194111589-1549ea0a-1706-48fa-852a-29e054b8d4ca.png)
![image](https://user-images.githubusercontent.com/110073344/194111665-34202c31-00d3-45c8-9e94-0526e91fcce2.png)
![image](https://user-images.githubusercontent.com/110073344/194111762-b870eb28-a3a3-4a0c-ba10-de996f56f75a.png)
![image](https://user-images.githubusercontent.com/110073344/194111819-ceea1d97-29fb-4014-96c6-79353f29ab7d.png)
![image](https://user-images.githubusercontent.com/110073344/194111906-c7c70a05-507e-4abf-a4c1-ef38d9cbe296.png)
![image](https://user-images.githubusercontent.com/110073344/194111981-420be436-3f34-4828-b919-ce384842debb.png)
![image](https://user-images.githubusercontent.com/110073344/194112042-81017ed1-1445-444f-b905-045e95f8610a.png)
![image](https://user-images.githubusercontent.com/110073344/194112244-4c55909b-328a-4e90-9126-f7307031c7c7.png)
![image](https://user-images.githubusercontent.com/110073344/194112382-49ba8d55-1285-4315-b38f-e7aba48b7c07.png)
![image](https://user-images.githubusercontent.com/110073344/194112471-67314506-97b8-4705-a566-75b0760ff204.png)
![image](https://user-images.githubusercontent.com/110073344/194112545-ba793811-a3c4-4f3a-b9c9-c5bbde61b656.png)
![image](https://user-images.githubusercontent.com/110073344/194112647-880da07b-8cb5-46ef-ab5f-bfc7f067b04e.png)
![image](https://user-images.githubusercontent.com/110073344/194112847-1c09acc5-ccdd-4e1f-9482-d704b64ec526.png)
![image](https://user-images.githubusercontent.com/110073344/194112879-bc41c596-6e54-4160-9a57-f5180fa36d84.png)





",screen shot sure run guess sure every step fine thank image image image image image image image image image image image image image image image image image image image image image,issue,positive,positive,positive,positive,positive,positive
1267581373,It's not related to GPU memory. I am not sure how to fix it. Check out this [thread](https://discuss.pytorch.org/t/training-crashes-due-to-insufficient-shared-memory-shm-nn-dataparallel/26396). ,related memory sure fix check thread,issue,negative,positive,positive,positive,positive,positive
1267577466,It's hard to know. Every case is different. Would it be possible to share with us some console output / debugging output? ,hard know every case different would possible share u console output output,issue,negative,negative,neutral,neutral,negative,negative
1267576085,"You can try fewshot vid2vid. If you have a smaller dataset, you need to reduce the network's capacity as well. It's hard to make any above methods work out of the box. ",try smaller need reduce network capacity well hard make work box,issue,negative,negative,negative,negative,negative,negative
1267576070,Another thing might be evaluating the model with eval() mode turned on and off ([link](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L66)). Could you run the test mode with and without the `--eval` option and see if that makes a difference?,another thing might model mode turned link could run test mode without option see difference,issue,negative,neutral,neutral,neutral,neutral,neutral
1267574031,"Could you share with us the training and test command lines? 
Did you use the same flags (e.g., `-preprocess`)? ",could share u training test command use,issue,negative,neutral,neutral,neutral,neutral,neutral
1267569765,+1. please refer to [training & test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images) for high-res model training. ,please refer training test model training,issue,negative,neutral,neutral,neutral,neutral,neutral
1267564573,"Using perceptual loss and GANs loss together: please refer to [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) (also used in [SPADE/GauGAN](https://github.com/NVlabs/SPADE)). Perceptual loss can stabilize GANs training. 

Use GAN loss on top of pre-trained features: please refer to percpetual discriminator [paper](https://arxiv.org/abs/1809.01396). It is used in recent GANs papers ([projected-gans](https://arxiv.org/abs/2111.01007) and [vision-aided gans](https://www.cs.cmu.edu/~vision-aided-gan/)) as well. ",perceptual loss loss together please refer also used perceptual loss stabilize training use gan loss top please refer discriminator paper used recent well,issue,negative,positive,positive,positive,positive,positive
1267559219,Smooth-l1 loss might also work. Feel free to try it. ,loss might also work feel free try,issue,negative,positive,positive,positive,positive,positive
1267559138,Sorry about that. There was some mismatch in typing interpolation methods between `torchvision.transforms.InterpolationMode` and `PIL.Image`. It's a new bug introduced as we tried to suppress warnings ([PR #1414](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/1414)). I pushed the fix to the master branch ([d5e62dd021d51b](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/d5e62dd021d51b042ee3228d36bf2a41dedfb8cb)). ,sorry mismatch interpolation new bug tried suppress fix master branch,issue,negative,negative,negative,negative,negative,negative
1267557786,"The default pix2pix network is U-Net. To test pix2pix model with resnet9blks, you need to explicitly set the flag `--netG resnet_9blocks` during test time. ",default network test model need explicitly set flag test time,issue,negative,neutral,neutral,neutral,neutral,neutral
1267552045,"![Screen Shot 2022-10-04 at 4 39 38 PM](https://user-images.githubusercontent.com/1924757/193922725-51018560-2c2b-463c-a14e-1476a59afef0.png)
Not sure if you have succeeded to run all the steps ",screen shot sure run,issue,negative,positive,positive,positive,positive,positive
1266436102,"and I find if I use the default preprocess, and set --load_size 800 --crop_size 400,this error don't happend.",find use default set error,issue,negative,neutral,neutral,neutral,neutral,neutral
1261902356,"And if multiple repeated experiments are necessary, how many times should I do this experiment？",multiple repeated necessary many time,issue,negative,positive,positive,positive,positive,positive
1261898887,"Your answer helped me a lot. Now I am investigating on medical image denoising and SSIM and PSNR are usually utilized to evaluate the performance of model, if I want to evaluate the performance of my model(based on CycleGAN), does it mean that I have to do multiple repeated experiments with  different train and test set.",answer lot investigating medical image usually evaluate performance model want evaluate performance model based mean multiple repeated different train test set,issue,negative,negative,negative,negative,negative,negative
1260014904,"One challenge about evaluating CycleGAN (or image manipulation model in general) is that there is no absolute automated metric that we can rely on. There is FID that measures realism, but it has its own problems like validity of the metric, sensitivity toward the number of samples, and most importantly not measuring its alignment with the inputs. Therefore, it'd be difficult to do tricks like five-fold cross validation. Nevertheless, one reasonable thing to do would be doing multiple repeated experiments and measuring FIDs. ",one challenge image manipulation model general absolute metric rely fid realism like validity metric sensitivity toward number importantly measuring alignment therefore difficult like cross validation nevertheless one reasonable thing would multiple repeated measuring,issue,negative,positive,neutral,neutral,positive,positive
1260008423,"Do you mean the same training runs well on one device, and it produces NaN on the other device? Or did you try to do multi-gpu training? In the case of former, yes, it's likely a GPU issue... ",mean training well one device nan device try training case former yes likely issue,issue,positive,negative,negative,negative,negative,negative
1256387884,"can confirm, that using identity loss of 0 can cause ""wrong"" colors: https://github.com/aladdinpersson/Machine-Learning-Collection/issues/124

Code is not from this repo, but I had same issue there. Setting identity lambda to something above 0 is making results ok ;)",confirm identity loss cause wrong color code issue setting identity lambda something making,issue,negative,negative,negative,negative,negative,negative
1256386478,"can confirm, that using identity loss of 0, causes ""bad"" colors: https://github.com/aladdinpersson/Machine-Learning-Collection/issues/124

Code is not from this repo, but I had same issue there",confirm identity loss bad color code issue,issue,negative,negative,negative,negative,negative,negative
1255837629,@bentaculum Hi I am running into a similar problem. I have visdom running on the remote server and ssh tunnel to it on my local machine. All I see is blank interface. I am not familiar with http proxy and not sure what is the issue here. Do you think it's possibly the same issue and know how to tackle it?,hi running similar problem running remote server tunnel local machine see blank interface familiar proxy sure issue think possibly issue know tackle,issue,negative,positive,positive,positive,positive,positive
1254001771,"Thank you. Yes, I run it on a local notebook with the same codes on Colab. How could I make it work?",thank yes run local notebook could make work,issue,positive,neutral,neutral,neutral,neutral,neutral
1253298244,"Hey, @leemengwei . I met the same question. May I ask did you also occur this error when using resnet9blks as the generator network. I can successfully run the test.py when using unet_256 as the generator . 

@junyanz Will this related to the version of Pytorch or torchvision version, I found that it can be trained when setting the generator network as **resnet9blks** but can't be tested for all different tasks.

I'm using Pytorch '1.5.0' and torchvision '0.6.0 a0+82fd1c8'",hey met question may ask also occur error generator network successfully run generator related version version found trained setting generator network ca tested different,issue,negative,positive,positive,positive,positive,positive
1253107638,"@junyanz thanks for you reply and sorry to distrub you again! i have another question is if i want to increase low dimensional features of the target image domain in the converted image,where can i use the Perceptual loss in the cyclegan model ? i once read a document said add the Perceptual loss in the Discriminator net to improve network performance, but it no open source and Explain the structure carefully. hope you can answer my questions!",thanks reply sorry another question want increase low dimensional target image domain converted image use perceptual loss model read document said add perceptual loss discriminator net improve network performance open source explain structure carefully hope answer,issue,positive,negative,neutral,neutral,negative,negative
1253103754,"Yes, I still have the issue after setting `--display_id 0` . I stopped debugging and change to Linux. 
Tks anw.",yes still issue setting stopped change,issue,negative,neutral,neutral,neutral,neutral,neutral
1252912760,"Yeah, you can use it. You may also consider using the pix2pix data loader since you have paired dataset. ",yeah use may also consider data loader since paired,issue,negative,neutral,neutral,neutral,neutral,neutral
1252911076,"Not completely sure. What is your crop_size? Sometimes people use a smaller crop_size, and this error might happen. Also, could you check if all the images in your dataset are bigger than 256x256.
Issue #776 might be relevant. ",completely sure sometimes people use smaller error might happen also could check bigger issue might relevant,issue,negative,positive,positive,positive,positive,positive
1252903391,Could you share more details without us? You still have the issue even after you set ``-display_id 0``? Could you share the screenshot of your debugging output? ,could share without u still issue even set could share output,issue,positive,neutral,neutral,neutral,neutral,neutral
1252902189,"Are you running it on the CPU? If it is the case, the training will be extremely slow. You may want to set the `--print_freq 1` so that the program will print the loss more frequently. ",running case training extremely slow may want set program print loss frequently,issue,negative,negative,negative,negative,negative,negative
1252899618,"![Screen Shot 2022-09-20 at 4 52 22 PM](https://user-images.githubusercontent.com/1924757/191362125-cd42f99b-a6a6-4a84-9e7b-cc9b732260f4.png)

I just tested it. It seems to be working for me. Did you run the pix2pix colab with Google colab, or run it on a local notebook?
",screen shot tested working run run local notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
1252895351,"Sometimes, your cropping might get unlucky. It not only depends on the image sizes, but also depends on where you crop the patches. ",sometimes might get unlucky image size also crop,issue,negative,neutral,neutral,neutral,neutral,neutral
1252891968,"step 1: you need to download the datasets from the Cityscapes website: https://www.cityscapes-dataset.com/
step 2: In our experiments, we download the images to 128x128 or 256x256. You need to also put images and semantic label maps in different folders (`trainA`, `trainB`, `testA`, `testB`).  
step 3: Follow the CycleGAN training and test [README](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix) + change the `--dataroot` and `--name` accordingly. ",step need step need also put semantic label different testa step follow training test change name accordingly,issue,negative,neutral,neutral,neutral,neutral,neutral
1252890675,"Training with such a small dataset will be very challenging. In general, you can try higher cycle consistency loss and also the identity loss to constrain the model more. You can also try adding more data augmentation such as rotation or random affine transform. This is not present in the current codebase, but it's not hard to add them in `base_dataset.py`.",training small general try higher cycle consistency loss also identity loss constrain model also try data augmentation rotation random affine transform present current hard add,issue,negative,negative,negative,negative,negative,negative
1252886984,"Yes. That is what we found too. One reason could be that our datasets are small, so larger batch size leads to faster overfitting. ",yes found one reason could small batch size faster,issue,negative,negative,negative,negative,negative,negative
1252876088,More data often help. You can also run the same experiment with 10K vs. 1K images and do a comparison. ,data often help also run experiment comparison,issue,negative,neutral,neutral,neutral,neutral,neutral
1249068308,"```
import cv2
import numpy as np
import onnxruntime as ort
class pix2pix():
    def __init__(self,onnx_file):
        self.net = ort.InferenceSession(onnx_file)
        self.input_size = 256
        self.input_name = self.net.get_inputs()[0].name
        self.output_name = self.net.get_outputs()[0].name
        print(""input_name = "" + self.input_name)
        print(""output_name = "" + self.output_name)
    def generation(self, image):
        if isinstance(image, str):
            image=cv2.imdecode(np.fromfile(image, dtype=np.uint8), -1)
        elif isinstance(image, np.ndarray):
            image=image.copy()
        # image=image[0:256, 0:256]
        img = cv2.resize(image, (self.input_size, self.input_size))
        input_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        input_image = input_image.transpose(2, 0, 1)
        input_image = np.expand_dims(input_image, axis=0)
        input_image = input_image / 255.0
        input_image = (input_image - 0.5) / 0.5 
        input_image = input_image.astype('float32')
        print(input_image.shape)
        # x = x[None,:,:,:]
        outs = self.net.run(None, {self.input_name: input_image})[0].squeeze(axis=0)
        outs = np.clip(((outs*0.5+0.5) * 255), 0, 255).astype(np.uint8) 
        outs = outs.transpose(1, 2, 0).astype('uint8')
        outs = cv2.cvtColor(outs, cv2.COLOR_RGB2BGR)
        outs=np.hstack((img, outs))
        print(""outs"",outs.shape)
        # return cv2.resize(outs, (image.shape[1], image.shape[0]))
        return outs

if __name__ == '__main__':
    gan_model=pix2pix(""GoPro_pix2pixA2B512_2000_ld512_batch.onnx"")
    result=gan_model.generation(""103_real_A.png"")
    cv2.imshow(""result"", result)
    cv2.waitKey(0)

```",import import import ort class self print print generation self image image image image image print none none print return return result result,issue,negative,neutral,neutral,neutral,neutral,neutral
1247948751,"Thank you for solution! I thought this was only true for CPU mode, and since I wanted to process it with GPU, I didn't type it.",thank solution thought true mode since process type,issue,positive,positive,positive,positive,positive,positive
1247470996,You can get around this error by using the --no_multiprocessing flag ,get around error flag,issue,negative,neutral,neutral,neutral,neutral,neutral
1241891278,I am getting the same problem when running it on the maps dataset. Any ideas what's happening?,getting problem running happening,issue,negative,neutral,neutral,neutral,neutral,neutral
1241798812,Found my mistake in the training loop (hoping I don't have any other mistakes),found mistake training loop,issue,negative,neutral,neutral,neutral,neutral,neutral
1241741698,"I am using ""resize_and_crop"" as my --preprocess flag.
What makes me confused is the fact that it was running until this epoch(148) and this error came from my input wich did not changes during the train processing. Then when I continue the training with --continue_train it works sometimes and the other time the same error come after some epochs",flag confused fact running epoch error came input train continue training work sometimes time error come,issue,negative,negative,negative,negative,negative,negative
1238642883,"This could be because of the limitation of `nn.DataParallel` we use [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/14422fb8486a4a2bd991082c1cda50c3a41a755e/models/networks.py#L115), which was a common approach when we published the git repo. But it does suffer from suboptimal GPU utilization because the data loading is inefficient. A better way would be utilizing `DistributedDataParallel` [link](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html). We don't plan to support this for now, but if someone could create a PR I'd appreciate it. ",could limitation use common approach git suffer suboptimal utilization data loading inefficient better way would link plan support someone could create appreciate,issue,positive,positive,neutral,neutral,positive,positive
1238642613,"We didn't report PNSR and UQI in our original pix2pix paper. I recommend that you contact the authors of ""The effect of loss function on conditional generative adversarial networks"" for more details. ",report original paper recommend contact effect loss function conditional generative,issue,negative,positive,positive,positive,positive,positive
1238639576,I think it's possible. you just need to call `create_model' in [L51](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L51) and then call `model.setup(opt)`. ,think possible need call call opt,issue,negative,neutral,neutral,neutral,neutral,neutral
1238631773,"It's hard to know. Maybe this image is corrupt, or this image is too small (smaller than your crop size). Which `--preprocess` flag did you use? ",hard know maybe image corrupt image small smaller crop size flag use,issue,negative,negative,negative,negative,negative,negative
1238627202,"Hi, there are two possible ways. 

One is just doing random cropping without resizing. You specify `--preprocess crop`, and set `--crop_size` to be the smallest side length (310 in this case). And at test time, you can run it at the original rectangular resolution, because the network is fully convolutional. However, this method may not be optimal, because the size difference between training and test time is quite large: 310x310 square crops at training time, but 1000x520 full images at test time. 

The other way is to first apply resizing to put them at similar resolution. You specify `--preprocess scale_width_and_crop`, and set `--load_size 1000` and `--crop_size 520` so that the images are loaded with scaling to have 1000px width, and make random crop at the smallest side length (520 in this case, since 500x310 will become 1000x620 after resizing). 

At test time, you test with `--preprocess scale_width`, so that all images are processed at 1000px width. After this, you can manually resize the output images to the original sizes like 500x310. ",hi two possible way one random without specify crop set side length case test time run original rectangular resolution network fully convolutional however method may optimal size difference training test time quite large square training time full test time way first apply put similar resolution specify set loaded scaling width make random crop side length case since become test time test width manually resize output original size like,issue,positive,positive,neutral,neutral,positive,positive
1238612841,"If you want to use wandb, you can [install](https://docs.wandb.ai/quickstart) it before running the program. 
If you want to disable wandb, you can simply remove the flag ``-use_wandb``.",want use install running program want disable simply remove flag,issue,negative,neutral,neutral,neutral,neutral,neutral
1236137294,"@raylu1314coding Please tell me how this problem was solved, thank you very much",please tell problem thank much,issue,negative,positive,positive,positive,positive,positive
1233871557,"Hello @cankur007, sorry for the late reply.

By my experiments, the testing results of zebra2horse are similar to the official repo. You can check the visualizations in the following table.

<table>
  <tr>
    <td>Description</td>
    <td>Input image</td>
    <td>Result</td>
  </tr>
  <tr>
    <td>Horse to zebra</td>
    <td><img src=""https://github.com/KevinChen880723/CycleGAN/blob/master/results/horse2zebra/horse.png?raw=true""></td>
    <td><img src=""https://github.com/KevinChen880723/CycleGAN/blob/master/results/horse2zebra/horse2zebra.png?raw=true""></td>
  </tr>
  <tr>
    <td>Zebra to horse</td>
    <td><img src=""https://github.com/KevinChen880723/CycleGAN/blob/master/results/horse2zebra/zebra.png?raw=true""></td>
    <td><img src=""https://github.com/KevinChen880723/CycleGAN/blob/master/results/horse2zebra/zebra2horse.png?raw=true""></td>
  </tr>
</table>

Besides, I tried using CycleGAN to do the visual translation between the Cityscapes and the IDD dataset. You can also check the visualizations in the following table.

<table>
  <tr>
    <td>Domain</td>
    <td>Input image</td>
    <td>Output image</td>
  </tr>
  <tr>
    <td>Cityscapes to IDD</td>
    <td><img src=""https://github.com/KevinChen880723/CycleGAN/blob/master/results/German2India/A2B/input.png?raw=true""></td>
    <td><img src=""https://github.com/KevinChen880723/CycleGAN/blob/master/results/German2India/A2B/ResidualBlock15_withIdentity_Crop1024.png?raw=true""></td>
  </tr>
  <tr>
    <td>IDD to Cityscapes</td>
    <td><img src=""https://github.com/KevinChen880723/CycleGAN/blob/master/results/German2India/B2A/original.png?raw=true""></td>
    <td><img src=""https://github.com/KevinChen880723/CycleGAN/blob/master/results/German2India/B2A/Residual15_withIdentity_Crop1024.png?raw=true""></td>
  </tr>
</table>

",hello sorry late reply testing similar official check following table table description input image result horse zebra zebra horse besides tried visual translation also check following table table domain input image output image,issue,negative,negative,negative,negative,negative,negative
1232391362,"@andyli I had a similar problem.
I have read “Training and Test Tips” but still have some questions on how to start training if my dataset is 1000x520, 500x310, 870x600 type of images with no pattern? And when using the model, I would like to output the original size images.
The dataset is somewhat similar to this one: iphone2dslr_flower, but I didn't find an example of this, so I don't understand how to train and use the model. Any guidance would be much appreciated.",similar problem read training test still start training type pattern model would like output original size somewhat similar one find example understand train use model guidance would much,issue,negative,positive,positive,positive,positive,positive
1232390464,"@junyanz I have read “Training and Test Tips” but still have some questions on how to start training if my dataset is 1000x520, 500x310, 870x600 type of images with no pattern? And when using the model, I would like to output the original size images.
The dataset is somewhat similar to this one: iphone2dslr_flower, but I didn't find an example of this, so I don't understand how to train and use the model. Any guidance would be much appreciated.",read training test still start training type pattern model would like output original size somewhat similar one find example understand train use model guidance would much,issue,positive,positive,positive,positive,positive,positive
1232382474,"@junyanz batch_size  is ""4"", after the use of --gpu_ids 0,1,2,3，  the model is only trained on one GPU",use model trained one,issue,negative,neutral,neutral,neutral,neutral,neutral
1232167706,"It's possible to synthesize high-res, rectangular images. Both ` --preprocess scale_width_and_crop` and `--preprocess crop` could work. If all of your images share the same image size, you can just use the 2nd option `--preprocess crop'. 
If images come at different sizes (some are 1600x900, some are 360x256), you may want to use ` --preprocess scale_width_and_crop` to force them to be at the same scale. ",possible synthesize rectangular crop could work share image size use option crop come different size may want use force scale,issue,negative,neutral,neutral,neutral,neutral,neutral
1232146278,"What is your batch_size? By mentioning ""does not work"", are you referring to (1) the model is only trained on one GPU, or (2) the model is trained on multiple GPUs, but the training speed is not as fast as you expect? ",work model trained one model trained multiple training speed fast expect,issue,negative,positive,neutral,neutral,positive,positive
1232136777,"When we save images for visualization, we convert the image to a 3-channel tensor, even if `--input_nc` and `--output_nc` are 1. ([link](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9bcef69d5b39385d18afad3d5a839a02ae0b43e7/util/util.py#L22)). This could be the reason?

This is a guess, but if `--norm` is set to ""none"", there is no adjustment on scaling. And CycleGAN networks are initialized with a small variance closed to zero. Therefore, the final output of the generator network may be very small (very small weights are multiplied at every layer without normalization) and it may appear ""empty"". Could you try setting `--init_gain 1.0` and see if the outputs look less empty?",save visualization convert image tensor even link could reason guess norm set none adjustment scaling small variance closed zero therefore final output generator network may small small every layer without normalization may appear empty could try setting see look le empty,issue,negative,negative,negative,negative,negative,negative
1232136166,"Even if you use chain, the parameters are independently updated. By ""the same effect"", I meant that it should not affect the final result of the training. ",even use chain independently effect meant affect final result training,issue,negative,neutral,neutral,neutral,neutral,neutral
1232132475,It's possible. Please refer to [Training and Test Tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-rectangular-images). ,possible please refer training test,issue,negative,neutral,neutral,neutral,neutral,neutral
1232130332,"`--norm` refers to using instance normalization or batch normalization [instance | batch | none]. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L37). You probably don't want to touch it. 

`--preprocess` is for data preprocessing. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L50). 

`--preprocess None` doesn't work for every dataset. If your image comes in different sizes, you might crash the training. 

",norm instance normalization batch normalization instance batch none see line probably want touch data see line none work every image come different size might crash training,issue,negative,neutral,neutral,neutral,neutral,neutral
1231092037,"Very good question, I have the same problem and await guidance.",good question problem await guidance,issue,negative,positive,positive,positive,positive,positive
1230572590,"It was the issue that Snap Lens Studio could only import compressed GANs.
I could solve it by training with the Snap-released GAN repo.",issue snap lens studio could import compressed could solve training gan,issue,negative,neutral,neutral,neutral,neutral,neutral
1230019487,"@gabgren I have 4 GPUs and want to use these 4 GPUs for accelerated training at the same time, how can I modify the code? At present, it can only be trained on one GPU, and the training speed is very slow, --gpu_ids 0,1,2,3 does not work，thank you!",want use accelerated training time modify code present trained one training speed slow,issue,negative,negative,negative,negative,negative,negative
1229245949,Why is the effect the same when using one vs two different optimizers for each network?,effect one two different network,issue,negative,neutral,neutral,neutral,neutral,neutral
1216076485,@junyanz I meet the same problem. I set `--display_id 0`. I run on Windows. Do you have any suggesstion?,meet problem set run,issue,negative,neutral,neutral,neutral,neutral,neutral
1211983680,"Hi I have the same issue, did you resolve the problem ? Thank you !
",hi issue resolve problem thank,issue,negative,neutral,neutral,neutral,neutral,neutral
1211634456,"I have 4 GPUs and want to use these 4 GPUs for accelerated training at the same time, how can I modify the code? At present, it can only be trained on one GPU, and the training speed is very slow, thank you!",want use accelerated training time modify code present trained one training speed slow thank,issue,negative,negative,negative,negative,negative,negative
1208256375,"I got the same issue today and I solved it. I forgot to link my dataroot to the folder AB using pix2pix model.
hope it will help distracted people like me.
",got issue today forgot link folder model hope help distracted people like,issue,positive,neutral,neutral,neutral,neutral,neutral
1207314274,"For anyone that finds themselves contending with this issue, --crop_size is the specific parameter that allowed me to train the CycleGAN model without bumping into a memory issue. Modified from the default 256 to 128. Training and testing results look reasonable.",anyone contending issue specific parameter train model without bumping memory issue default training testing look reasonable,issue,negative,positive,neutral,neutral,positive,positive
1205250311,"In the link above by @c1a1o1 they also add the self attention layers to the generator. 
![image](https://user-images.githubusercontent.com/60389356/182857471-eb55f671-b1e9-4fde-963d-e1c8921210a7.png)
",link also add self attention generator image,issue,negative,neutral,neutral,neutral,neutral,neutral
1204666254,"@junyanz Im,sorry to disturb you.If we input the fake_B and the real_B as the paried data,can we use the Perceptual loss in Cyclegan?
",sorry disturb input data use perceptual loss,issue,negative,negative,negative,negative,negative,negative
1198133310,"I had no problems to run a training before this pull request but now I have an error `AttributeError: module 'torchvision.transforms' has no attribute 'InterpolationMode'`.

I've recreated the environment from the `environment.yml` which use the `0.5.0` torchvision version, this error might come from this version since the class `InterpolationMode` was not implemented yet.",run training pull request error module attribute environment use version error might come version since class yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1193262164,"> I saw in commit [9bcef69](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/9bcef69d5b39385d18afad3d5a839a02ae0b43e7), it uses `Image.BICUBIC` but not `transforms.InterpolationMode.BICUBIC`. So I just change the code back to `Image.BICBIC`, and it works. I believe you forget to update the `environment.yml`

thanks",saw commit change code back work believe forget update thanks,issue,negative,positive,neutral,neutral,positive,positive
1193085949,"> thanks @emilwallner <img alt=""Screen Shot 2021-06-15 at 3 18 18 pm"" width=""967"" src=""https://user-images.githubusercontent.com/289994/121997010-f3220100-cdec-11eb-87c6-3a0be338c397.png"">

Great picture, like it!",thanks screen shot great picture like,issue,positive,positive,positive,positive,positive,positive
1192297764,"At present, I want to color infrared video. Is there any good way?",present want color infrared video good way,issue,negative,positive,positive,positive,positive,positive
1191693623,@KevinChen880723 how about the results after training? Did you get good results in testing?,training get good testing,issue,negative,positive,positive,positive,positive,positive
1190962864,"If this structure is added to the generator, will it have a good effect? Is there any Ablation Experiment in this regard",structure added generator good effect ablation experiment regard,issue,negative,positive,positive,positive,positive,positive
1189712935,"> It should not affect a lot if you use instance normalization. Things might get interesting as we use bn=1 with batchnorm in many pix2pix experiments. In that case, you may want to keep train mode during test time. Feel free to try both.

Okay, I have tried. It seems like in my task it seems like instance norm is less perform than bn>1, and I have to use eval mode during inference otherwise the output when I testing will be worse",affect lot use instance normalization might get interesting use many case may want keep train mode test time feel free try tried like task like instance norm le perform use mode inference otherwise output testing worse,issue,positive,positive,positive,positive,positive,positive
1189387769,"Hi,

Have you tried U-net blocks on Cycle-GAN generator? if yes, how were the results compared to Res blocks?",hi tried generator yes,issue,negative,neutral,neutral,neutral,neutral,neutral
1189073513,"@Sarmadfismael try reducing the image size (using crop or other options listed in the [Training/test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md) section). Also, it will depend on the memory available on your setup. 
As far as I understood, there is an option to resume training if needed. So, yes you can stop and resume it. Search for the --continue_train flag in the Training/Test tips page.",try reducing image size crop listed section also depend memory available setup far understood option resume training yes stop resume search flag page,issue,negative,positive,positive,positive,positive,positive
1186139862,"> Use `-gpu_ids`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L26)

It cannot work. I set gpu_ids=0,1,2; but it only uses gpu0",use see work set,issue,negative,neutral,neutral,neutral,neutral,neutral
1183933755,"I had the same problem. After that I tried  --continue_train but it didn't work . Is the problem because of my GPU only 8G? I used 20000 images to train pix2pix , imgsize 512*256(paired img).",problem tried work problem used train paired,issue,negative,neutral,neutral,neutral,neutral,neutral
1181155212,"Looks like this was due to the default 'num_test=50' parameter taken by the code. The fact that my data also had only 50 images for the first city (aachen) kind of confused me initially, as it just stopped after processing the files with 'aachen' in the name.
Working now!
",like due default parameter taken code fact data also first city kind confused initially stopped name working,issue,negative,positive,neutral,neutral,positive,positive
1181131312,"Even when I moved all the images (from different cities) into the testA folder, it is somehow picking only the first city related images i.e. for 'aachen' city. None of the other images are getting picked up for conversion. ",even different testa folder somehow first city related city none getting picked conversion,issue,negative,positive,neutral,neutral,positive,positive
1180181137,"> I am not sure if our current code supports visualization of 6-channel images. Two potential fixes: (1) you can try using the wandb visualization and see if they handle it or not. (2) you may want to modify the visualizer code. We often call the tensor2im function (here: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/visualizer.py#L41) , which only works for 1 channel or 3 channel images. You may want to modify it. https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/util.py#L9

Thanks a lot for pointing me to that. I was able to modify the tensor2im and it's training now. For anyone with the same issue I just included the following at the same indentation level as # grayscale to RGB in tensor2im:

```
        if image_numpy.shape[0] == 6:  
            image_numpy, b = np.vsplit(image_numpy, 2)
```
This will just split the two input images that were concatenated to feed the network and return one of them for visualization as real A for reference.
",sure current code visualization two potential try visualization see handle may want modify visualizer code often call function work channel channel may want modify thanks lot pointing able modify training anyone issue included following indentation level split two input feed network return one visualization real reference,issue,positive,positive,positive,positive,positive,positive
1178387417,"Hi, junyanz.
Thank you for reply.
I understood your comments.
In Addition, I have a question.
I think the output image (*.png) had a pixel value of [0 to 256].
Are these images normalized before they are entered into the network?
Or is it done when the output image is saved?

Kind regards,
hisanori.",hi thank reply understood addition question think output image value network done output image saved kind,issue,positive,positive,positive,positive,positive,positive
1178341413,"Got the same issue, command:
`python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout`",got issue command python name model test,issue,negative,neutral,neutral,neutral,neutral,neutral
1178265547,"I saw in commit 9bcef69d5b39385d18afad3d5a839a02ae0b43e7, it uses `Image.BICUBIC` but not `transforms.InterpolationMode.BICUBIC`.
So I just change the code back to `Image.BICBIC`, and it works.
I believe you forget to update the `environment.yml`",saw commit change code back work believe forget update,issue,negative,neutral,neutral,neutral,neutral,neutral
1174583113,"Thanks for your reply, I use the command line is “python train.py --dataroot ./datasets/maps --name maps_cyclegan --gpu_ids 0 --model cycle_gan”. 
The same error occurs when I use the  command ""python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA"".
Download is the default data set, built in Windows 10 above the environment. Pytorch version is '1.4.0+ CU92 '. With CPU training, everything is fine, but with GPU acceleration, the number of parameters explodes and this happens. Looking forward to your reply.",thanks reply use command line python name model error use command python name model direction default data set built environment version training everything fine acceleration number looking forward reply,issue,negative,positive,positive,positive,positive,positive
1174057702,"IIRC, there were some issues with certain pytorch versions and CUDA 11.0. Glad to know it works for 10.2. Another thing to try is to use the latest PyTorch with supported CUDA version.",certain glad know work another thing try use latest version,issue,positive,positive,positive,positive,positive,positive
1174034671,It may not be so related to the models you are using. You need to either experiment with different types of data augmentation or expand your dataset. ,may related need either experiment different data augmentation expand,issue,negative,neutral,neutral,neutral,neutral,neutral
1174033022,"There could be due to several reasons. 
1. You may use different preprocessing codes during training and test time. You want to make sure that you use the same. One test is that you feed a training set image to your model, and it should produce the roughly the same result as you have overserved during training stage. It seems that you are using --resize_or_crop crop during training but scale-width during test time. 
2. Your model may overfit to your training set if your training set is small. You may consider early stopping and using an earlier checkpoint. ",could due several may use different training test time want make sure use one test feed training set image model produce roughly result training stage crop training test time model may overfit training set training set small may consider early stopping,issue,negative,positive,neutral,neutral,positive,positive
1174027983,Could you share with us the training script and debugging information (loss plots)? ,could share u training script information loss,issue,negative,neutral,neutral,neutral,neutral,neutral
1174027542,model.set_input(data) does not do too much work except moving data to gpu devices. Most of the preprocessing code is implemented in the data loader.  You can also just modify the code within model.test(),data much work except moving data code data loader also modify code within,issue,negative,positive,positive,positive,positive,positive
1174024409,"Would it be possible to provide more information? It's hard to identify the reasons without knowing your task (input and output), your data (dataset size), and your training and test scripts. ",would possible provide information hard identify without knowing task input output data size training test,issue,negative,negative,negative,negative,negative,negative
1174022715,"#1445 looks like a good fix. Let us know if it works for @Sroebel. We are still evaluating whether we want to merge the PR as it may require many changes to the interface, while most of the users still use NVIDIA GPUs. ",like good fix let u know work still whether want merge may require many interface still use,issue,positive,positive,positive,positive,positive,positive
1174020006,"I am not sure if our current code supports visualization of 6-channel images. Two potential fixes: 
(1) you can try using the wandb visualization and see if they handle it or not. 
(2) you may want to modify the visualizer code. We often call the tensor2im function (here: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/visualizer.py#L41) , which only works for 1 channel or 3 channel images. You may want to modify it. https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/util.py#L9
",sure current code visualization two potential try visualization see handle may want modify visualizer code often call function work channel channel may want modify,issue,negative,positive,positive,positive,positive,positive
1174016481,"It should not affect a lot if you use instance normalization. Things might get interesting as we use bn=1 with batchnorm in many pix2pix experiments. In that case, you may want to keep train mode during test time. Feel free to try both. ",affect lot use instance normalization might get interesting use many case may want keep train mode test time feel free try,issue,positive,positive,positive,positive,positive,positive
1174007532,it might be a data loading issue. You may want to use SSD or other fast file systems. ,might data loading issue may want use fast file,issue,negative,positive,positive,positive,positive,positive
1174006872,Have you finished the entire training run and saved the models? ,finished entire training run saved,issue,negative,neutral,neutral,neutral,neutral,neutral
1174006312,"Sorry for the confusion. There are only train and val in some CycleGAN datasets. So they are not being used in the training test, and only serve as a test set. Feel free to create your own splits. ",sorry confusion train used training test serve test set feel free create,issue,negative,negative,neutral,neutral,negative,negative
1174003350,Which training command line did you use? Did you train a model on the default datasets? ,training command line use train model default,issue,negative,neutral,neutral,neutral,neutral,neutral
1171842994,"Hello
Similar question here. I am using cycle GAN and want to implement early stopping using the validation set. Is the validation set used in cycleGAN? I am kind of lost on how to import the validation set in the training stage. 
Thanks in advance :) ",hello similar question cycle gan want implement early stopping validation set validation set used kind lost import validation set training stage thanks advance,issue,positive,positive,positive,positive,positive,positive
1170893367,Where do you run the training script? The folder where the checkpoints are saved is relative to the path where the training script is run.,run training script folder saved relative path training script run,issue,negative,neutral,neutral,neutral,neutral,neutral
1170149631,"Looks like its your first theory: it takes a long time feeding the 8 gpus. the actual processing seems to be faster, but is slowed down between iterations. See this comparison between the GPU utilization of 1xA6000 vs 8xA6000:
![1gpu](https://user-images.githubusercontent.com/1724721/176479590-cf27233f-f9ca-4b06-975d-be0d4f4e3e69.gif)
![8gpus](https://user-images.githubusercontent.com/1724721/176479601-77a14499-c061-4af2-8253-26be0447f0c8.gif)

How can I speed this up ?",like first theory long time feeding actual faster see comparison utilization speed,issue,negative,positive,neutral,neutral,positive,positive
1170002149,"@bgjeroska 

I think this comment is for you. you can see the comment above

> The possible reasons could be two: (1) the PatchGAN discriminator is already quite weak. Thus, adding GP loss will make it too weak compared to the generator. (2) the GP loss assumes that the inputs are independent according to the original paper, while PatchGAN takes overlapping patches, and breaks this assumption.",think comment see comment possible could two discriminator already quite weak thus loss make weak generator loss independent according original paper assumption,issue,negative,negative,neutral,neutral,negative,negative
1169469612,"I'm confused , while testing Why should we use train mode?
Because if you use train mode, the parameter of bn will be changed, and drop-out will be use.
But if you want to get the same result with the same input, the bn shouldnt be changed .",confused testing use train mode use train mode parameter use want get result input shouldnt,issue,negative,negative,negative,negative,negative,negative
1169124355,"Hello,

I am currently trying to train WGAN-GP with Patch-Discriminator. But it is somehow impossible to make it work. I also tried not-overlapping convents, but it didn't help much. Did someone find a way to train a good model?
Or have any Ideas-why WGAN doesn't work with Patch-Discriminator?
",hello currently trying train somehow impossible make work also tried help much someone find way train good model work,issue,positive,positive,neutral,neutral,positive,positive
1167265551,"Hi, I have the same problem, can I fix it without lowering the CUDA version?",hi problem fix without lowering version,issue,negative,neutral,neutral,neutral,neutral,neutral
1164986883,"Hi, junyanz

Thanks for reply, And sorry for the delay in replying.
I succeed create a model.netG.
In addition, I tried feature extraction.
Example, 
`model.netG('testA(image)')`

Your implementation test.py,
    model.set_input(data)  # unpack data from data loader

I would like to use the images from test A for feature extraction.
Does model.set_input(data) cause the image to be unpack?
If so, would it be possible to specify test A for the extracted image and input it into feature_extractor (a function I have defined myself) function?
I am afraid that this is not directly related to the code you have implemented, but I would like your opinion.

KInd Regards,
hisanori yosimura.",hi thanks reply sorry delay succeed create addition tried feature extraction example image implementation data unpack data data loader would like use test feature extraction data cause image unpack would possible specify test extracted image input function defined function afraid directly related code would like opinion kind,issue,positive,negative,neutral,neutral,negative,negative
1156186701,"Got it working now with CUDA 11.4.

_Tesla T4_
_NVIDIA-SMI 470.129.06 Driver Version: 470.129.06 CUDA Version: 11.4_ 

Dockerfile:

```
FROM nvidia/cuda:11.4.0-base

RUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub

RUN apt update && apt install -y wget unzip curl bzip2 git
RUN curl -LO http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
RUN bash Miniconda3-latest-Linux-x86_64.sh -p /miniconda -b
RUN rm Miniconda3-latest-Linux-x86_64.sh
ENV PATH=/miniconda/bin:${PATH}
RUN conda update -y conda

RUN conda install -y pytorch torchvision -c pytorch
RUN mkdir /workspace/ && cd /workspace/ && git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git && cd pytorch-CycleGAN-and-pix2pix && pip install -r requirements.txt

WORKDIR /workspace
```
",got working driver version version run run apt update apt install curl git run curl run bash run path run update run install run git clone pip install,issue,negative,positive,positive,positive,positive,positive
1155861441,@junyanz  So which method (including but not limited to) do you think is more suitable for the scenario I described？,method limited think suitable scenario,issue,negative,positive,positive,positive,positive,positive
1155683607,I didn't keep track of the actual distance in the original dataset and it would take some effort to track it down. I recommend using @taesungp's instead!,keep track actual distance original would take effort track recommend instead,issue,positive,positive,positive,positive,positive,positive
1155666254,Please see the original papers for more details. ([pix2pix](https://arxiv.org/pdf/1611.07004.pdf) Section 6.2 and [CycleGAN](https://arxiv.org/pdf/1703.10593.pdf) Section 7.1),please see original section section,issue,positive,positive,positive,positive,positive,positive
1155659375,"The warning is probably caused by the PyTorch version difference, which can be ignored. Could you share with us your training and test script? ",warning probably version difference could share u training test script,issue,negative,neutral,neutral,neutral,neutral,neutral
1155654590,Haven't seen this before. Does `--continue_train` work for you? You can resume the model training from epoch 195. ,seen work resume model training epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
1155652862,Could you share with us your command line? ,could share u command line,issue,negative,neutral,neutral,neutral,neutral,neutral
1155646978,"Could you check if the GPU utilization is at 100%? It could be because the data loader does not feed training images fast enough. Another possibility is that the progress in the total number of images used for training is actually faster with more GPUs, but if you are monitoring the number of iterations, it won't be different. ",could check utilization could data loader feed training fast enough another possibility progress total number used training actually faster number wo different,issue,positive,positive,neutral,neutral,positive,positive
1155644659,Most of the methods tend to overfit the training set if you have limited data. You may consider applying data augmentation to combat model overfitting. ,tend overfit training set limited data may consider data augmentation combat model,issue,negative,negative,neutral,neutral,negative,negative
1155643937,"Hello @DerekSunYH , for your information, I happened to collect a larger dataset of sat2map images. 

https://github.com/taesungp/larger-google-sat2maps-dataset

It also has a script to download Google maps images, with [adjustable zoom level](https://github.com/taesungp/larger-google-sat2maps-dataset/blob/main/download_sat2maps.py#L116). Feel free to use this. 

Lastly, the map style is slightly different from our original Google Maps dataset. 

",hello information collect also script adjustable zoom level feel free use lastly map style slightly different original,issue,positive,positive,positive,positive,positive,positive
1155642873,I am not quite familiar with the function `get_graph_node_names`. The model is not a network module. Maybe you could try `model.netG` or `model.netD`. ,quite familiar function model network module maybe could try,issue,negative,positive,positive,positive,positive,positive
1150570048,"@junyanz  I use the pix2pixhd method to verify that the effect is very good, but the traffic scenarios are limited. How can I use it to predict so many different traffic scenarios? Is expanding datasets a viable approach?",use method verify effect good traffic limited use predict many different traffic expanding viable approach,issue,negative,positive,positive,positive,positive,positive
1150567814,"@junyanz thanks, for your reply. 
Now I use the pix2pixHD method to verify, and the effect is very good, but the scene is very single. I have a problem: I want to convert IR to RGB, and the scene is traffic. Because the distribution of different traffic roads is certainly different, and I can't have all the traffic scene data sets for training, so what do I need to do to ensure better results?",thanks reply use method verify effect good scene single problem want convert scene traffic distribution different traffic certainly different ca traffic scene data training need ensure better,issue,positive,positive,positive,positive,positive,positive
1149815691,"I have this error testing the script provided by @GuigzoS.
`AttributeError: 'NoneType' object has no attribute 'group'`, I checked the regex pattern used I was able to retrieve the data.",error testing script provided object attribute checked pattern used able retrieve data,issue,negative,positive,positive,positive,positive,positive
1149370298,"@junyanz Thank you for your reply! Still, I am wondering why we set require False in Line 185. Is it because it is mandatory or it is for any other purpose such as speed-up or something? ",thank reply still wondering set require false line mandatory purpose something,issue,negative,negative,negative,negative,negative,negative
1149123092,"Currently, images are chosen randomly based on the data loader. To achieve your goal, you can use the test script with different epoch numbers. Or you can try to modify the visualization code [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/train.py#L57). ",currently chosen randomly based data loader achieve goal use test script different epoch try modify visualization code,issue,negative,negative,negative,negative,negative,negative
1149118714,"If you have paired datasets of infrared images -> RGB images, you should consider using paired image-to-image translation (e.g., pix2pix, pix2pixHD, GauGAN). CycleGAN might not work for your application. ",paired infrared consider paired translation might work application,issue,negative,neutral,neutral,neutral,neutral,neutral
1149113516,"These are two separate questions: 
(1) should we optimize G and D jointly or not? 
(2) If we optimize G and D separately, do we need to compute gradients for D while updating G. 

For (2), As long as we don't do optimizer_D.step(), the gradients for D will not be used in SGD. Therefore, we set require False in Line 185. 

For (1), most authors optimize them separately, following the original paper's practice. ",two separate optimize jointly optimize separately need compute long used therefore set require false line optimize separately following original paper practice,issue,positive,negative,neutral,neutral,negative,negative
1149100622,"It seems that your problem setting is quite different from colorization in which the goal is to predict ab channels given the L channel. If you have paired datasets, I will just use `--model pix2pix` and set `--input_nc` and `--output_nc` properly. ",problem setting quite different colorization goal predict given channel paired use model set properly,issue,negative,neutral,neutral,neutral,neutral,neutral
1149098320,Not sure what happended. Could you post the command line that you are using? ,sure could post command line,issue,negative,positive,positive,positive,positive,positive
1149097624,"This limitation is documented by a well-written article ""[CycleGAN, a Master of Steganography](https://arxiv.org/abs/1712.02950)"". I am wondering if you are using the default parameters for the facades results. We have obtained better results compared to yours. ",limitation article master steganography wondering default better,issue,negative,positive,positive,positive,positive,positive
1149093718,We adopted the network architectures from the fast neural style transfer paper. They did not use reflection padding for the downsampling layers. See this [line](https://github.com/jcjohnson/fast-neural-style/blob/master/fast_neural_style/models.lua#L95).,adopted network fast neural style transfer paper use reflection padding see line,issue,negative,positive,positive,positive,positive,positive
1146844594,Was someone able to solve the problem? I am meeting the same problem.,someone able solve problem meeting problem,issue,negative,positive,positive,positive,positive,positive
1145318567,"> For running torch.cat((self.real_A, self.fake_B), 1), we just compare the current fake_B and current real_A, however we could use history image pool data to help the discriminator doesn't forget what it has done wrong before. You can refer to SimGAN, and the following image ![screen shot 2017-08-17 at 7 08 49 pm](https://user-images.githubusercontent.com/11957155/29437617-a1dbfb04-837f-11e7-91c0-55b237c2e619.png)

So in simple terms, does ImagePool make the discriminator more powerful?",running compare current current however could use history image pool data help discriminator forget done wrong refer following image screen shot simple make discriminator powerful,issue,negative,negative,neutral,neutral,negative,negative
1139199593,"@wave-transmitter @junyanz
 I also used IR to convert RGB and found rec_ B works well, but fake_ B is the same as the IR input, and the color becomes very dark. How to solve this? Similarly, I use the cyclegan model. My data resolution is 2560 * 1440. 160 training data and 40 test data. Here are my training and testing commands:
1. python train.py --dataroot ./single/testIR --name test_IR --model cycle_gan --batch_size 8 --load_size 2560 --preprocess scale_width_and_crop --crop_size 360 --gpu_ids 0,1,2,3,4,5,6,7 --norm instance
2. python test.py --dataroot single/testIR --name test_IR --model cycle_gan --preprocess none  --no_dropout ",also used convert found work well input color becomes dark solve similarly use model data resolution training data test data training testing python name model norm instance python name model none,issue,negative,negative,neutral,neutral,negative,negative
1136923290,"I am getting error as below
```
RuntimeError: Unsupported: ONNX export of batch_norm for unknown channel size.
```",getting error unsupported export unknown channel size,issue,negative,negative,neutral,neutral,negative,negative
1133925299,"Thanks a lot! My GPU is [NVIDIA GeForce RTX 3050 Ti Laptop GPU]. I changed CUDA from version 11 to 10.2 and download the corresponding 'torch' package, then the codes run through without Nan problem!",thanks lot ti version corresponding package run without nan problem,issue,positive,positive,positive,positive,positive,positive
1133670064,"I have updated all Conda dependencies and now the AI runs through successfully on the GPU. In addition, I had to adjust some places in the code as they were overhauled with the newer versions.",ai successfully addition adjust code,issue,negative,positive,positive,positive,positive,positive
1133107015,"I debugged around a bit. When I train with the CPU it runs through without problems. Only on the GPU does the error appear. However, here from the beginning the results are not usable, which makes me think that either CUDA is doing a problem or I'm overlooking some settings that could fix the problem.

I would be very happy if someone could help me with the settings.",around bit train without error appear however beginning usable think either problem could fix problem would happy someone could help,issue,positive,positive,positive,positive,positive,positive
1130184932,"the same question. I adjusted the batch size and learning rate, but 'nan issue' still occured. ",question batch size learning rate issue still,issue,negative,neutral,neutral,neutral,neutral,neutral
1128661997,"Hello!   I trained for 40 epochs, batch_size=1, but the loss function of the generator does not seem to converge, why is this? Can you please provide some advice, thanks",hello trained loss function generator seem converge please provide advice thanks,issue,negative,positive,positive,positive,positive,positive
1126971635,"Hi, can you explain what D_real, D_fake and G_GAN stand for, please?
I find these terms in an article, and did not understand the purpose of them.
Thank you.
",hi explain stand please find article understand purpose thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1118766734,"python train.py --dataroot ./datasets/cezanne2photo --name cezanne_cyclegan --model cycle_gan --gpu_ids 0 --n_epochs 3
----------------- Options ---------------
               batch_size: 1
                    beta1: 0.5
          checkpoints_dir: ./checkpoints
           continue_train: False
                crop_size: 256
                 dataroot: ./datasets/cezanne2photo             [default: None]
             dataset_mode: unaligned
                direction: AtoB
              display_env: main
             display_freq: 400
               display_id: 1
            display_ncols: 4
             display_port: 8097
           display_server: http://localhost
          display_winsize: 256
                    epoch: latest
              epoch_count: 1
                 gan_mode: lsgan
                  gpu_ids: 0
                init_gain: 0.02
                init_type: normal
                 input_nc: 3
                  isTrain: True                                 [default: None]
                 lambda_A: 10.0
                 lambda_B: 10.0
          lambda_identity: 0.5
                load_iter: 0                                    [default: 0]
                load_size: 286
                       lr: 0.0002
           lr_decay_iters: 50
                lr_policy: linear
         max_dataset_size: inf
                    model: cycle_gan
                 n_epochs: 3                                    [default: 100]
           n_epochs_decay: 100
               n_layers_D: 3
                     name: cezanne_cyclegan                     [default: experiment_name]
                      ndf: 64
                     netD: basic
                     netG: resnet_9blocks
                      ngf: 64
               no_dropout: True
                  no_flip: False
                  no_html: False
                     norm: instance
              num_threads: 4
                output_nc: 3
                    phase: train
                pool_size: 50
               preprocess: resize_and_crop
               print_freq: 100
             save_by_iter: False
          save_epoch_freq: 5
         save_latest_freq: 5000
           serial_batches: False
                   suffix:
         update_html_freq: 1000
                use_wandb: False
                  verbose: False
----------------- End -------------------
dataset [UnalignedDataset] was created
The number of training images = 6287
initialize network with normal
initialize network with normal
initialize network with normal
initialize network with normal
model [CycleGANModel] was created
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 11.378 M
[Network G_B] Total number of parameters : 11.378 M
[Network D_A] Total number of parameters : 2.765 M
[Network D_B] Total number of parameters : 2.765 M
-----------------------------------------------
Setting up a new session...
create web directory ./checkpoints\cezanne_cyclegan\web...
C:\Users\FLo\.conda\envs\pytorch-CycleGAN-and-pix2pix\lib\site-packages\torch\optim\lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` be` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failto dure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.htm-adjl#how-to-adjust-learning-rate
  ""https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"", UserWarning)
learning rate 0.0002000 -> 0.0002000
Traceback (most recent call last):
  File ""train.py"", line 52, in <module>
    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
  File ""C:\Users\FLo\source\repos\cycleGAN\pytorch-CycleGAN-and-pix2pix\models\cycle_gan_model.py"", line 208, in optimize_parameters
    self.backward_G()             # calculate gradients for G_A and G_B
  File ""C:\Users\FLo\source\repos\cycleGAN\pytorch-CycleGAN-and-pix2pix\models\cycle_gan_model.py"", line 195, in backward_G
    out.backward()
  File ""C:\Users\FLo\.conda\envs\pytorch-CycleGAN-and-pix2pix\lib\site-packages\torch\tensor.py"", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""C:\Users\FLo\.conda\envs\pytorch-CycleGAN-and-pix2pix\lib\site-packages\torch\autograd\__init__.py"", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
  File ""C:\Users\FLo\.conda\envs\pytorch-CycleGAN-and-pix2pix\lib\site-packages\torch\autograd\function.py"", line 77, in apply
    return self._forward_cls.backward(self, *args)
  File ""C:\Users\FLo\source\repos\cycleGAN\pytorch-CycleGAN-and-pix2pix\models\cycle_gan_model.py"", line 188, in backward
    raise RuntimeError(""Some error in backward"")
RuntimeError: Some error in backward",python name model beta false default none unaligned direction main epoch latest normal true default none default linear model default name default basic true false false norm instance phase train false false suffix false verbose false end number training initialize network normal initialize network normal initialize network normal initialize network normal model network total number network total number network total number network total number setting new session create web directory call later call opposite order dure result skipping first value learning rate schedule see learning rate recent call last file line module calculate loss get update network file line calculate file line file line backward self gradient file line backward flag file line apply return self file line backward raise error backward error backward,issue,positive,negative,neutral,neutral,negative,negative
1118585286,">  For example, it will work if the dimensions of the input are powers of 2
> 3. Modify the architecture to handle downsampling/upsampling odd numbered dimensions

I've seen other U-Net implementations that handle arbitrary sizes - why not support that in this library? 1080p dimensions would be a great example for most video-based computer-vision applications",example work input modify architecture handle odd seen handle arbitrary size support library would great example,issue,positive,positive,positive,positive,positive,positive
1118461053,"@junyanz  I was able to get the training process initiated.At epoch 67 the current translation for zebra2horse looks like this.
Any reason why the results are coming out to be like this
zebra->horse 
![epoch067_fake_A](https://user-images.githubusercontent.com/34626942/166917706-7425c4c4-ba68-43e7-bd42-0bc070c97f6f.png)

",able get training process epoch current translation like reason coming like horse,issue,positive,positive,positive,positive,positive,positive
1113988348,"Hi, I encountered the same problem. There is an excellent [website](https://distill.pub/2016/deconv-checkerboard/) explaining the reason and providing the solution. Hope this can help u :)))",hi problem excellent explaining reason providing solution hope help,issue,positive,positive,positive,positive,positive,positive
1113918904,"I have face the same problem. How do you solve this problem？

> Occasionally during a training epoch, the training will seem to ""stop"", without exiting the program and without any error messages, and there will be no new outputs for hours. The program just seems to be frozen after a line of ""epoch: xx, iters: xxxx, ...""
> 
> Is this perhaps because I'm using Colab?




> Also having this issue when running train.py as nohup on a cloud VM. It's frozen after 36 epochs. Did you have to restart the training, or will it automatically continue after a few hours?


I also face the same problem. How do you solve this problem？",face problem solve occasionally training epoch training seem stop without program without error new program frozen line epoch perhaps also issue running cloud frozen restart training automatically continue also face problem solve,issue,negative,positive,neutral,neutral,positive,positive
1113033699,"hello，@tongzhou Wang Why is the picture of a face wearing a mask generated by cycleGan so blurry?
![企业微信截图_16512210962462](https://user-images.githubusercontent.com/49524647/165911118-4d661bd2-9bdf-4d10-9bd4-73e2de2d136a.png)
",wang picture face wearing mask blurry,issue,negative,neutral,neutral,neutral,neutral,neutral
1112674141,"> > The default folder `checkpoints` had the right permissions, but even applying `chmod 777` didn't solve the problem. Changing directoty with `--checkpoints_dir` solved instead the problem. Thanks.
> 
> Where should I put the '--checkpoints_dir‘

I have the same question.",default folder right even solve problem instead problem thanks put question,issue,negative,positive,positive,positive,positive,positive
1109419010,"I solved this issue by the following steps

1. update the **def update_learning_rate(self):** in *base_model.py* as below
```
def update_learning_rate(self):
      pass
```
2. Add **def update_learning_rate(self):** in *pix2pix_model.py* as below
```
def update_learning_rate(self):
    lrd = self.opt.lr / self.opt.n_epochs_decay
    lr = self.old_lr - lrd
    for param_group in self.optimizer_D.param_groups:
        param_group['lr'] = lr
    for param_group in self.optimizer_G.param_groups:
        param_group['lr'] = lr
    print('update learning rate: %f -> %f' % (self.old_lr, lr))
    self.old_lr = lr
```
3. Define **self.old_lr = opt.lr** in *pix2pix_model.py* as below
```
if self.isTrain:
    # define loss functions
    self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)
    self.criterionL1 = torch.nn.L1Loss()
    self.old_lr = opt.lr  # insert this line
    self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
    self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
    self.optimizers.append(self.optimizer_G)
    self.optimizers.append(self.optimizer_D)
```

4. Move the **model.update_learning_rate()** to last line in *train.py* as below
.........
print('End of epoch %d / %d \t Time Taken: %d sec' % (epoch, opt.n_epochs + opt.n_epochs_decay, time.time() - epoch_start_time))
if epoch > opt.n_epochs:
    model.update_learning_rate() # update learning rates at the end of every epoch.",issue following update self self pas add self self print learning rate define define loss insert line move last line print epoch time taken sec epoch epoch update learning end every epoch,issue,negative,negative,neutral,neutral,negative,negative
1109049953,"Hi!

If you go to [`visualizer.py`](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/003efc4c8819de47ff11b5a0af7ba09aee7f5fc1/util/visualizer.py#L241), the `print_current_losses()` function describes what metrics are being printed.

This function is referenced in the main training loop at [`train.py`](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/003efc4c8819de47ff11b5a0af7ba09aee7f5fc1/train.py#L62), and the times are measured using `time.time()`, which measures time in `seconds`.

Hope this answers your question!",hi go function metric printed function main training loop time measured time hope question,issue,negative,positive,positive,positive,positive,positive
1104456000,"You might try Rdutta solution on the bottom of this page:
https://discuss.pytorch.org/t/optimizer-step-before-lr-scheduler-step-error-using-gradscaler/92930
Others have had this same problem before.

On Wed, Apr 20, 2022 at 8:21 AM Kokowaah ***@***.***> wrote:

> could anyone tell me how to solve this problem?followd is the discrible of
> the probelm.
>
> E:\Anaconda\envs\pytorch-CycleGAN-and-pix2pix\lib\site-packages\torch\optim\lr_scheduler.py:134:
> UserWarning: Detected call of lr_scheduler.step() before optimizer.step().
> In PyTorch 1.1.0 and later, you should call them in the
> opposite order: optimizer.step() before lr_scheduler.step(). Failure to
> do this will result in PyTorch skipping the first value of the learning
> rate schedule. See more details at
> https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
> ""https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"",
> UserWarning)
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1412>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AI6H6KZ3OHEOIAZUEFSHEHTVGAOGNANCNFSM5T4M3MKA>
> .
> You are receiving this because you are subscribed to this thread.Message
> ID: ***@***.***>
>
",might try solution bottom page problem wed wrote could anyone tell solve problem call later call opposite order failure result skipping first value learning rate schedule see reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1100782004,"> @fire717 did you achieved result with 128x128 images? How did you do this?

Oh its such a long time,
Unfortunately, I still did not train with 128 sucessfully, and not using this repo after that.",fire result oh long time unfortunately still train,issue,negative,negative,negative,negative,negative,negative
1088229292,"Hi, @taesungp, the following are my recently experiment results:
My datasets have 9411 depth images, 9081 RGB images, all images are 256x256, and contain three actions: forward/backward, wave hands, and forward bend.
![datasets](https://user-images.githubusercontent.com/49118957/161666337-d9d4a228-1753-40a9-87ff-526ea1dc0740.jpg)

I use this command to train CycleGAN on Colab: `!python train.py --dataroot ./datasets/yicyclepix_0322 --name yivlp2rgbhuman --model cycle_gan --n_epochs 100 --n_epochs_decay 100 --epoch_count 180 --continue_train --lambda_A 25 --lambda_B 25 --batch_size 3 --preprocess crop --load_size 256 --crop_size 224 --display_id -1`

Training options:
```
----------------- Options ---------------
               batch_size: 3                             	[default: 1]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: True                          	[default: False]
                crop_size: 224                           	[default: 256]
                 dataroot: ./datasets/yicyclepix_0322    	[default: None]
             dataset_mode: unaligned                     
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: -1                            	[default: 1]
            display_ncols: 4                             
             display_port: 8097                          
           display_server:[ http://localhost](http://localhost/)              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 180                           	[default: 1]
                 gan_mode: lsgan                         
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                 lambda_A: 25.0                          	[default: 10.0]
                 lambda_B: 25.0                          	[default: 10.0]
          lambda_identity: 0.5                           
                load_iter: 0                             	[default: 0]
                load_size: 256                           	[default: 286]
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: cycle_gan                     
                 n_epochs: 100                           
           n_epochs_decay: 100                           
               n_layers_D: 3                             
                     name: yivlp2rgbhuman                	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: resnet_9blocks                
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: False                         
                  no_html: False                         
                     norm: instance                      
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 50                            
               preprocess: crop                          	[default: resize_and_crop]
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                use_wandb: False                         
                  verbose: False                         
----------------- End -------------------
```

After 200 epochs of training, I plot losses:
![CycleGAN_Loss](https://user-images.githubusercontent.com/49118957/161667185-8e7af728-edf4-48ca-963e-8b1b14e2a3d2.png)
And these videos are test results:
**For forward/backward**: [Input](https://drive.google.com/file/d/1DYRz1WftcSZQ-_4Uyo-VD2yhj-tzfyYI/view?usp=sharing), [Output](https://drive.google.com/file/d/144UN7XEXc_NZX-1rtrgxp3IV_y0mfiBP/view?usp=sharing).
**For wave hands**: [Input](https://drive.google.com/file/d/12exa1mtDEKseutrd3XJqtuOfad4DiDR1/view?usp=sharing), [Output](https://drive.google.com/file/d/1cXlZyZiVLZrHKZMvvYnfmYYC1auHSfQn/view?usp=sharing).
**For forward bend**: [Input](https://drive.google.com/file/d/1KfuMQz524L5xO01tASzSHT6cIVQwtsQY/view?usp=sharing), [Output](https://drive.google.com/file/d/1iQQZCrdSI463eBTUoUylLwFHRdRQf9nt/view?usp=sharing).

I add these flags when test my CycleGAN model: **--batch_size 3 --preprocess crop --load_size 256 --crop_size 224 --no_dropout** 
As these videos show, the result is not good. Could you give me some suggestions?

Besides, I try another training options: I change **--load_size**, **--crop_size 256**, and add **--netG**: `!python train.py --dataroot ./datasets/yicyclepix_0322 --name yivlp2rgbhuman --model cycle_gan --n_epochs 100 --n_epochs_decay 100 --epoch_count 89 --continue_train --lambda_A 25 --lambda_B 25 --batch_size 3 --netG resnet_6blocks --preprocess crop --load_size 286 --crop_size 256 --display_id -1`

Training options:
```
----------------- Options ---------------
               batch_size: 3                             	[default: 1]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: True                          	[default: False]
                crop_size: 256                           
                 dataroot: ./datasets/yicyclepix_0322    	[default: None]
             dataset_mode: unaligned                     
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: -1                            	[default: 1]
            display_ncols: 4                             
             display_port: 8097                          
           display_server:[ http://localhost](http://localhost/)              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 89                            	[default: 1]
                 gan_mode: lsgan                         
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                 lambda_A: 25.0                          	[default: 10.0]
                 lambda_B: 25.0                          	[default: 10.0]
          lambda_identity: 0.5                           
                load_iter: 0                             	[default: 0]
                load_size: 286                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: cycle_gan                     
                 n_epochs: 100                           
           n_epochs_decay: 100                           
               n_layers_D: 3                             
                     name: yivlp2rgbhuman                	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: resnet_6blocks                	[default: resnet_9blocks]
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: False                         
                  no_html: False                         
                     norm: instance                      
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 50                            
               preprocess: crop                          	[default: resize_and_crop]
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                use_wandb: False                         
                  verbose: False                         
----------------- End -------------------
```
Although now I just train CycleGAN to 133 epochs, images generated during the training process let me know that the learning effect may still be bad:
![training_produce](https://user-images.githubusercontent.com/49118957/161672597-f3bfc299-bc3a-443c-8b4c-69db051733c9.jpg)

May i have your suggestions?
Any help is much appreciated:)",hi following recently experiment depth contain three wave forward bend use command train python name model crop training default beta true default false default default none unaligned direction main default epoch latest default normal true default none default default default default linear model name default basic true false false norm instance phase train crop default false false suffix false verbose false end training plot test input output wave input output forward bend input output add test model crop show result good could give besides try another training change add python name model crop training default beta true default false default none unaligned direction main default epoch latest default normal true default none default default default linear model name default basic default true false false norm instance phase train crop default false false suffix false verbose false end although train training process let know learning effect may still bad may help much,issue,positive,negative,neutral,neutral,negative,negative
1083191331,"ok thanks!




------------------&nbsp;原始邮件&nbsp;------------------
发件人:                                                                                                                        ""junyanz/pytorch-CycleGAN-and-pix2pix""                                                                                    ***@***.***&gt;;
发送时间:&nbsp;2022年3月30日(星期三) 晚上10:10
***@***.***&gt;;
***@***.******@***.***&gt;;
主题:&nbsp;Re: [junyanz/pytorch-CycleGAN-and-pix2pix] test_video.py returns error when loading video  (Issue #1400)





 
Hi, I got it from this repo: https://github.com/Vinno97/realtime-pytorch-CycleGAN-and-pix2pix
 
Sorry for sending it here, I am aware that it is a fork project. I am getting the same error on some other tests I did, no idea why. Thats why I thought I could get some help here.
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you commented.Message ID: ***@***.***&gt;",thanks error loading video issue hi got sorry sending aware fork project getting error idea thats thought could get help reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1083189351,"Hi, I got it from this repo: https://github.com/Vinno97/realtime-pytorch-CycleGAN-and-pix2pix

Sorry for sending it here, I am aware that it is a fork project. I am getting the same error on some other tests I did, no idea why. Thats why I thought I could get some help here.",hi got sorry sending aware fork project getting error idea thats thought could get help,issue,negative,negative,negative,negative,negative,negative
1083178664,"Hi, could you please share where did you get the realtime-pytorch-CycleGAN project? Thanks a lot.",hi could please share get project thanks lot,issue,positive,positive,positive,positive,positive,positive
1079609960,"> 
Yea I managed to figure it out later after I deleted comment. Thanks very much.

> In the log file, ""iters"" is the number of the current processed image. In my code, I specify ""nb_data"" to order losses for plotting. I give you an example : if you have 1000 images for training (nb_data=1000). ""epoch: 1, iters: 100"" corresponds to calculated_epoch=1+100/nb_data=1.1 ""epoch: 1, iters: 200"" corresponds to calculated_epoch=1+200/nb_data=1.2 ... ""epoch: 2, iters: 500"" corresponds to epoch 2+500/nb_data=2.5 I use these ""calculated_epoch"" for the horizontal axis in the plot. I hope it is clear, so to answer your question, just put the number of training images for nb_data. Le 25/03/2022 à 08:07, michaelku1 a écrit :
> […](#)
> I also wrote a code to plot losses after training using the log file. The main benefit (in contrast to @phiwei <https://github.com/phiwei> 's code) is that it works for every model and not only CycleGAN. It's probably not perfect but it can be used as a baseline. NB : ""experiment_name"" is the path to the folder where the log file is, and do not forget to change nb_data. |import os import matplotlib.pyplot as plt import re def generate_stats_from_log(experiment_name, line_interval=10, nb_data=10800, enforce_last_line=True): """""" Generate chart with all losses from log file generated by CycleGAN/Pix2pix/CUT framework """""" #extract every lines with open(os.path.join(experiment_name, ""loss_log.txt""), 'r') as f: lines = f.readlines() #choose the lines to use for plotting lines_for_plot = [] for i in range(1,len(lines)): if (i-1) % line_interval==0: lines_for_plot.append(lines[i]) if enforce_last_line: lines_for_plot.append(lines[-1]) #initialize dict with loss names dicts = dict() dicts[""epoch""] = [] parts = (lines_for_plot[0]).split(') ')[1].split(' ') for i in range(0, len(parts)//2): dicts[parts[2*i][:-1]] = [] #extract all data pattern = ""epoch: ([0-9]+), iters: ([0-9]+)"" for l in lines_for_plot: search = re.search(pattern, l) epoch = int(search.group(1)) epoch_floatpart = int(search.group(2))/nb_data dicts[""epoch""].append(epoch+epoch_floatpart) #to allow several plots for the same epoch parts = l.split(') ')[1].split(' ') for i in range(0, len(parts)//2): dicts[parts[2*i][:-1]].append(float(parts[2*i+1])) #plot everything plt.figure() for key in dicts.keys(): if key != ""epoch"": plt.plot(dicts[""epoch""], dicts[key], label=key) plt.legend(loc=""best"") plt.show() | It'd be helpful if you can tell us what nb_data is, it doesn't quite make sense to me. — Reply to this email directly, view it on GitHub <[#1161 (comment)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1161#issuecomment-1078720645)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AWAXYPDV3YIUZSZKTYZ3YODVBVQ2JANCNFSM4SJ3VU3Q>. You are receiving this because you commented.Message ID: ***@***.***>",yea figure later comment thanks much log file number current image code specify order plotting give example training epoch epoch epoch epoch use horizontal axis plot hope clear answer question put number training also wrote code plot training log file main benefit contrast code work every model probably perfect used path folder log file forget change o import import generate chart log file framework extract every open choose use plotting range initialize loss epoch range extract data pattern epoch search pattern epoch epoch allow several epoch range float plot everything key key epoch epoch key best helpful tell u quite make sense reply directly view comment id,issue,positive,positive,positive,positive,positive,positive
1078867795,"In the log file, ""iters"" is the number of the current processed image. 
In my code, I specify ""nb_data"" to order losses for plotting.
I give you an example : if you have 1000 images for training (nb_data=1000).
""epoch: 1, iters: 100"" corresponds to calculated_epoch=1+100/nb_data=1.1
""epoch: 1, iters: 200"" corresponds to calculated_epoch=1+200/nb_data=1.2
...
""epoch: 2, iters: 500"" corresponds to epoch 2+500/nb_data=2.5
I use these ""calculated_epoch"" for the horizontal axis in the plot.

I hope it is clear, so to answer your question, just put the number of 
training images for nb_data.


Le 25/03/2022 à 08:07, michaelku1 a écrit :
>
>     I also wrote a code to plot losses after training using the log
>     file. The main benefit (in contrast to @phiwei
>     <https://github.com/phiwei> 's code) is that it works for every
>     model and not only CycleGAN. It's probably not perfect but it can
>     be used as a baseline. NB : ""experiment_name"" is the path to the
>     folder where the log file is, and do not forget to change nb_data.
>
>     |import os import matplotlib.pyplot as plt import re def
>     generate_stats_from_log(experiment_name, line_interval=10,
>     nb_data=10800, enforce_last_line=True): """""" Generate chart with
>     all losses from log file generated by CycleGAN/Pix2pix/CUT
>     framework """""" #extract every lines with
>     open(os.path.join(experiment_name, ""loss_log.txt""), 'r') as f:
>     lines = f.readlines() #choose the lines to use for plotting
>     lines_for_plot = [] for i in range(1,len(lines)): if (i-1) %
>     line_interval==0: lines_for_plot.append(lines[i]) if
>     enforce_last_line: lines_for_plot.append(lines[-1]) #initialize
>     dict with loss names dicts = dict() dicts[""epoch""] = [] parts =
>     (lines_for_plot[0]).split(') ')[1].split(' ') for i in range(0,
>     len(parts)//2): dicts[parts[2*i][:-1]] = [] #extract all data
>     pattern = ""epoch: ([0-9]+), iters: ([0-9]+)"" for l in
>     lines_for_plot: search = re.search(pattern, l) epoch =
>     int(search.group(1)) epoch_floatpart =
>     int(search.group(2))/nb_data
>     dicts[""epoch""].append(epoch+epoch_floatpart) #to allow several
>     plots for the same epoch parts = l.split(') ')[1].split(' ') for i
>     in range(0, len(parts)//2):
>     dicts[parts[2*i][:-1]].append(float(parts[2*i+1])) #plot
>     everything plt.figure() for key in dicts.keys(): if key !=
>     ""epoch"": plt.plot(dicts[""epoch""], dicts[key], label=key)
>     plt.legend(loc=""best"") plt.show() |
>
> It'd be helpful if you can tell us what nb_data is, it doesn't quite 
> make sense to me.
>
> —
> Reply to this email directly, view it on GitHub 
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1161#issuecomment-1078720645>, 
> or unsubscribe 
> <https://github.com/notifications/unsubscribe-auth/AWAXYPDV3YIUZSZKTYZ3YODVBVQ2JANCNFSM4SJ3VU3Q>.
> You are receiving this because you commented.Message ID: 
> ***@***.***>
>
",log file number current image code specify order plotting give example training epoch epoch epoch epoch use horizontal axis plot hope clear answer question put number training also wrote code plot training log file main benefit contrast code work every model probably perfect used path folder log file forget change o import import generate chart log file framework extract every open choose use plotting range initialize loss epoch range extract data pattern epoch search pattern epoch epoch allow several epoch range float plot everything key key epoch epoch key best helpful tell u quite make sense reply directly view id,issue,positive,positive,positive,positive,positive,positive
1073245297,"Hi, @631543791, i also have this question.

May i have your suggestions to fix this problem?
Any help is much appreciated:)",hi also question may fix problem help much,issue,negative,positive,positive,positive,positive,positive
1072322177,"`model.set_input()` is called first in the training script https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/003efc4c8819de47ff11b5a0af7ba09aee7f5fc1/train.py#L51-L52
Maybe you swapped them by accident?",first training script maybe accident,issue,negative,positive,positive,positive,positive,positive
1067246068,"Hi all 
my training take long time to get the result 1 hour per epoch, is it normal ? 
note: the input image size 512x512
can i stop the training in any step i want?? ",hi training take long time get result hour per epoch normal note input image size stop training step want,issue,negative,positive,neutral,neutral,positive,positive
1066825945,Could something similar to this be used in direct video files?,could something similar used direct video,issue,negative,positive,neutral,neutral,positive,positive
1063332940,Thanks for the replies. I think I understand. If I want to be doing testing with only a single image I should have trained the network using instance normalization. I tried providing multiple test images and it seems the results are much closer to the ones reported during training since I trained with batch normalization. Anyway thanks for the help and appreciate the code I think its a great way to demo how we can do style transfer. ,thanks think understand want testing single image trained network instance normalization tried providing multiple test much closer training since trained batch normalization anyway thanks help appreciate code think great way style transfer,issue,positive,positive,positive,positive,positive,positive
1062915761,"Hi, @taesungp, thank you very much for your reply:)

Yes, I have paired data. But my depth images are distorted, they may not have the accurate correspondence to the same locations with RGB images. This is why I use CycleGAN. Depend on my task. Is pix2pix better than CycleGAN?

In **pix2pix_model.py**, I found:
`parser.add_argument('--lambda_L1', type=float, default=100.0, help='weight for L1 loss')`

And CycleGAN (**cycle_gan_model.py**) has:
```
lambda_A, lambda_B, default=10.0
lambda_identity, default=0.5
```
But I don't know how much to increase these weight, [this](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1359) issue increase lambda weight from 10 to 20. Is there any rule to follow? In other words, I'm wondering the reasonable range of loss.

In addition, I can only train for 100 epochs at a time on colab, then I use `--continue_train` for the next cycle. Compared to training in one go, I'm not sure if my training configuration properly (learning rate decay problem).

May i have your suggestions?
Any help is much appreciated!",hi thank much reply yes paired data depth distorted may accurate correspondence use depend task better found loss know much increase weight issue increase lambda weight rule follow wondering reasonable range loss addition train time use next cycle training one go sure training configuration properly learning rate decay problem may help much,issue,positive,positive,positive,positive,positive,positive
1062604617,"This is super hard to decide. It depends on the number of images in your dataset, the complexity of each domain, the difficulty of the task itself, etc. Your best hope is to save a bunch of checkpoints. If you have a validation metric and a proper metric, you can try to pick up the best model. Or you can choose the model based on the manual inspection of the results. ",super hard decide number complexity domain difficulty task best hope save bunch validation metric proper metric try pick best model choose model based manual inspection,issue,positive,positive,positive,positive,positive,positive
1062603308,"thanks a lot! i have done it




------------------&nbsp;原始邮件&nbsp;------------------
发件人: ""Jun-Yan ***@***.***&gt;; 
发送时间: 2022年3月9日(星期三) 下午2:43
收件人: ***@***.***&gt;; 
抄送: ***@***.***&gt;; ***@***.***&gt;; 
主题: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to generate more samples from trained models? What does the number of samples generated depend on? (Issue #1390)





 
You can change num_test. See this line
 
—
Reply to this email directly, view it on GitHub, or unsubscribe.
Triage notifications on the go with GitHub Mobile for iOS or Android. 
You are receiving this because you authored the thread.Message ID: ***@***.***&gt;",thanks lot done generate trained number depend issue change see line reply directly view triage go mobile android id,issue,negative,positive,positive,positive,positive,positive
1062602252,"Yes, make sure that you use the same normalization during training and test. 
@awhelan-school You can try to use the test code with epoch 21. ",yes make sure use normalization training test try use test code epoch,issue,positive,positive,positive,positive,positive,positive
1062598870,"For grayscale images, you need to use the flag `-input_nc 1 --output_nc`.
You can directly use `combine_A_and_B` for the original images. 
During training time, you can use different `--preprocessing` flags to crop patches on the fly. See [Training & Test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images) for more details. ",need use flag directly use original training time use different crop fly see training test,issue,negative,positive,positive,positive,positive,positive
1062573392,"> 

Sry for I may resolve my problem. I use batch norm in CycleGAN for test before and when eval it runs model.eval(). Normalization does greatly affect the generated results. After I delete model.eval(), the test results returned to good training result. Hope it will be helpful to you.",may resolve problem use batch norm test normalization greatly affect delete test returned good training result hope helpful,issue,positive,positive,positive,positive,positive,positive
1062518105,"I have the same problem in [CycleGAN]. The middle results (200epoch/400epoch in total) in training are very good, but when I use the same image in test mode (with just A to B generator and ""single_dataset"" and not modify other setting), it shows a huge difference.",problem middle total training good use image test mode generator modify setting huge difference,issue,negative,positive,positive,positive,positive,positive
1062349559,"I am using 100K images so I don't think the amount of data is the issue. I also tried to make sure the options are the same for test and train. If I am not mistaken the only options that are mentioned in the FAQ that we must verify are the same are 
--norm=batch, --netG=unet_256, batch_size=16. I also diffed them to make sure they are the same.

batch_size: 16 16
checkpoints_dir: ./checkpoints ./checkpoints
crop_size: 256 256
dataroot: Test/ dataset.p2p/C
dataset_mode: single aligned
direction: AtoB AtoB
display_winsize: 256 256
epoch: latest latest
gpu_ids: 0 0
init_gain: 0.02 0.02
init_type: normal normal
input_nc: 3 3
isTrain: False True
load_iter: 0 0
load_size: 256 286
max_dataset_size: inf inf
model: test pix2pix
n_layers_D: 3 3
name: logoP2P logoP2P_V2
ndf: 64 64
netD: basic basic
netG: unet_256 unet_256
ngf: 64 64
no_dropout: False False
no_flip: False False
norm: batch batch
num_threads: 4 4
output_nc: 3 3
phase: test train
preprocess: resize_and_crop resize_and_crop
serial_batches: False False
use_wandb: False False
verbose: False False

The only option that is different is dataset_mode which is single in test and aligned in train which is to be expected or am I wrong?

 Could it be that when you output the fake result at a specific epoch that result is metric wise the best one deemed by the discriminator for that epoch and at a later epoch it may not hold true. Because, the result illustrated is from epoch 21 and I am using the model at epoch 30. ",think amount data issue also tried make sure test train mistaken must verify also make sure single direction epoch latest latest normal normal false true model test name basic basic false false false false norm batch batch phase test train false false false false verbose false false option different single test train wrong could output fake result specific epoch result metric wise best one discriminator epoch later epoch may hold true result epoch model epoch,issue,positive,negative,neutral,neutral,negative,negative
1062265475,"It could be caused by several potential reasons: 
(1) Your test and training scripts use different options: You can run the test.py with the same input image as used during the training time. You should expect to get results of similar quality. 
(2) Your model might overfit the training set if (1) is resolved but your model does not work for test inputs. You may consider collecting more images or use more data augmentation. 
(3) Bleeding artifacts: You might get bleeding artifacts when your input logo contours are not closed (due to data pre-processing and augmentation). You recommend that your data augmentation should not create open contours. ",could several potential test training use different run input image used training time expect get similar quality model might overfit training set resolved model work test may consider use data augmentation bleeding might get bleeding input closed due data augmentation recommend data augmentation create open,issue,positive,negative,neutral,neutral,negative,negative
1062204807,"It depends on many factors: the number of images in your dataset, the complexity of your input/output domains, the task itself, the image resolution, etc. 30 epochs might not be enough unless you have lots of images in your dataset. ",many number complexity task image resolution might enough unless lot,issue,negative,positive,positive,positive,positive,positive
1062188372,It is because the primary goal of the repo is to reproduce the training used in the original paper of pix2pix. Enabling Imagepool may be helpful for pix2pix as well. ,primary goal reproduce training used original paper may helpful well,issue,positive,positive,positive,positive,positive,positive
1062187740,"Hello, 

1. it sounds like a reasonable task. But do you have paired data in your dataset? If you have the ground-truth RGB for the input images, you can use pix2pix instead of CycleGAN. 
2. Yes. `--preprocess none` will disable cropping. Cropping can be beneficial when you don't have enough samples in your dataset, to prevent overfitting.
3. This depends on your task and the complexity of the data, so I can't say for sure, but you probably need more than a few thousand images, or hopefully 50k+ images. 

If the goal is to pass the output images to pose estimation networks, perhaps you can increase the weight on the L1 loss. It will generate less diverse images but will be more faithful to the input data and with less artifacts. ",hello like reasonable task paired data input use instead yes none disable beneficial enough prevent task complexity data ca say sure probably need thousand hopefully goal pas output pose estimation perhaps increase weight loss generate le diverse faithful input data le,issue,positive,positive,positive,positive,positive,positive
1061581338,"It depends on your data. Intuitively, however, >100 epoch is more likely to give promising result. I've tried pix2pix on my own dataset and it starts to generate good result after 100 epoch. Also you could try learning rate decay.",data intuitively however epoch likely give promising result tried generate good result epoch also could try learning rate decay,issue,negative,positive,positive,positive,positive,positive
1061569756,"It could be the case. If possible, just try running for more epochs and compare results. Also, you can use checkpoints to verify that the model is still improving.",could case possible try running compare also use verify model still improving,issue,negative,neutral,neutral,neutral,neutral,neutral
1060264703,"you try make the '--use_AB' is 'default= True', and make the '--no_multiprocessing' is 'default= True', and it worked.",try make true make true worked,issue,positive,positive,positive,positive,positive,positive
1059890701,"> Hi @Youqi970124 ,
> 
> Try pip removing visdom and conda installing the package [here](https://anaconda.org/conda-forge/visdom).
> 
> You also might have to install a `jsonpatch` library.

Many thanks!!!it is really helpful",hi try pip removing package also might install library many thanks really helpful,issue,positive,positive,positive,positive,positive,positive
1058757086,"> I added `--gpu_ids -1` and this issue was resolved. Thanks.

I was having the same issue and found other people over the net having the same problem as well... 
This argument actually solve the problem to run the code on CPU, however I was going to test 80k files, so I needed it to run on my GPU... to do so, you need to install a torch version compatible with the CUDA version of your graphic card (see `!nvidia-smi`).

In my case the problem was installing the package using:
`conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch` 
But when I ran `torch.cuda.is_available()` it returned `False` 

So to install the correct package instead of conda install, I used pip:
 `pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html`
only then `torch.cuda.is_available()` returned `True` and I got it to work on `--gpu_ids 0`


PS: You can also check if your Torch Version has enabled CUDA just type:

```
import torch
print(torch.__version__)
```
`1.9.0+cu111` --> use `--gpu_ids 0`
`1.9.0+cpu` --> use `--gpu_ids -1`",added issue resolved thanks issue found people net problem well argument actually solve problem run code however going test run need install torch version compatible version graphic card see case problem package install ran returned false install correct package instead install used pip pip install returned true got work also check torch version type import torch print use use,issue,negative,positive,neutral,neutral,positive,positive
1056691108,thank you so much. can you help me please about epoches? I have a paired and unpaired dataset and I don't know how many epoch is enough for supervised section and unsupervised section,thank much help please paired unpaired know many epoch enough section unsupervised section,issue,positive,positive,positive,positive,positive,positive
1055853868,"Hello, 

I am sorry to hear the quality is not great. It could be because of many reasons, ranging from the size of the dataset to the fundamental limitation of our model. 

In case of pix2pix, the precise alignment of the two images is important. It works well when the similar content appears at the same location of the image. For example, in all result below, the underlying content is the same between input and output. The model just ""renders"" the content into different styles. 
![image](https://user-images.githubusercontent.com/1088344/156247557-31998118-87dd-4d45-8c9f-2c47ee4b3495.png)
This alignment may not be perfect for some datasets and hurt performance. For example, in case of medical imaging, two images of the same patient won't be aligned well if the patient needs to be scanned in two settings. Therefore, pix2pix actually worked less well than CycleGAN, which was designed to be more lenient to misalignment. 

Also, if you have enough images in the dataset, using more latest formulation such as [Co-Modulation GAN](https://github.com/zsyzzsoft/co-mod-gan). It is a more powerful architecture, and will work better if you have many samples in the dataset to prevent overfitting. 
",hello sorry hear quality great could many ranging size fundamental limitation model case precise alignment two important work well similar content location image example result underlying content input output model content different image alignment may perfect hurt performance example case medical two patient wo well patient need two therefore actually worked le well designed lenient misalignment also enough latest formulation gan powerful architecture work better many prevent,issue,positive,positive,positive,positive,positive,positive
1055848786,"You can detect the region that contains the text using an OCR detection method, and add a loss function to preserve the pixels within the region. ",detect region text detection method add loss function preserve within region,issue,negative,neutral,neutral,neutral,neutral,neutral
1055847363,You don't need a validation set for training. ,need validation set training,issue,negative,neutral,neutral,neutral,neutral,neutral
1055846821,I believe that you can call one `backward` (for the total loss) and two `step` (for netG_A and netG_B).,believe call one backward total loss two step,issue,negative,neutral,neutral,neutral,neutral,neutral
1055841070,You are right. The current code does not save the optimizer parameters. You probably have to modify the code. ,right current code save probably modify code,issue,negative,positive,positive,positive,positive,positive
1048982801,"yes I have checked by putting breakpoints while saving and loading checkpoints, they have only model parameters for 4 models(netG_A, netG_B, netD_A, netD_B) but there is no optimizer parameters there for the 2 optimizers.",yes checked saving loading model,issue,negative,neutral,neutral,neutral,neutral,neutral
1048805478,"> 

Thank you for your answer, but i have one more question: if i use two different optimizers for netG_A and netG_B, when i calculate the cycle loss, do i have to call ``` backward() ``` on the losses of both generators before calling ``` step() ```? Or can I first calculate the loss of ```netG_A(netG_B)``` and update the gradient using ```step()```, and then calculate the loss of ```netG_B(netG_A)``` and update the gradient using ```step```? Which method should I use?",thank answer one question use two different calculate cycle loss call backward calling step first calculate loss update gradient step calculate loss update gradient step method use,issue,negative,positive,positive,positive,positive,positive
1048505791,"hi!
I have some questions about cyclegan and semi supervised cyclegan, can you help me please? 
I create a big dataset with 60k images in it, and I want to use semi supervised cyclegan for it. I don't know how many epoch is enough for training? I'm a little confused about validation set. I also, don't know that we need validation set or not?
tnx for your information ",hi semi help please create big want use semi know many epoch enough training little confused validation set also know need validation set information,issue,positive,negative,neutral,neutral,negative,negative
1048205263,"It is probably because the optimizer states of Adam are not saved in the checkpoints. Therefore, it's possible that the losses initially go a bit high for the first few iterations after resuming from checkpoints. Still, we found that it doesn't affect the overall training a lot because the losses stabilize fairly quickly. (disclaimer: we haven't done thorough study on this)

About the performance decrease between epoch 200 and 177, it could indeed be because of the unsaved optimizer states. Or it could be just because of the fluctuation during the course of training. ",probably saved therefore possible initially go bit high first still found affect overall training lot stabilize fairly quickly disclaimer done thorough study performance decrease epoch could indeed unsaved could fluctuation course training,issue,negative,positive,positive,positive,positive,positive
1048202333,"Yes, `-dataset_mode unaligned` means that we are using unpaired data. For unpaired data, you should use cyclegan rather than pix2pix. For more details regarding hyperparameters, please refer to [train options](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py) and test [options](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/test_options.py). Also refer to [Q&A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md) and [training & test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md).  ",yes unaligned unpaired data unpaired data use rather regarding please refer train test also refer training test,issue,positive,neutral,neutral,neutral,neutral,neutral
1048196302,You can use two different optimizers for two different networks. The effect should be the same. Using `chain` simplifies the code. ,use two different two different effect chain code,issue,negative,neutral,neutral,neutral,neutral,neutral
1040845921,"Thanks for your tips, but I still can't display the results, I tried to change the IP but I can't access the website, I also disabled visdom by setting --display_id 0 (how can I see the results if it's disabled?).",thanks still ca display tried change ca access also disabled setting see disabled,issue,negative,negative,neutral,neutral,negative,negative
1040782974,They are training set images. You need to run `test.py` for test image results. ,training set need run test image,issue,negative,neutral,neutral,neutral,neutral,neutral
1040779593,The naming conversion in the paper and code is different. You can find more details [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L30). ,naming conversion paper code different find,issue,negative,neutral,neutral,neutral,neutral,neutral
1040770846,You can use `--preprocess none`. See [Training & Test Tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-rectangular-images) for more details. ,use none see training test,issue,negative,neutral,neutral,neutral,neutral,neutral
1040764664,"It seems that your visdom server has been activated. 
The error is `Cannot assign requested address`.  Try this [tip](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#connection-errorhttpconnectionpool-230-24-38) first. 
You can try a different IP address that works for you using the flag `--display_port` . 
If visdom still doesn't work for you, you can try wandb using the flag `--use_wandb` flag. ",server error assign address try tip first try different address work flag still work try flag flag,issue,negative,positive,positive,positive,positive,positive
1040761509,"You probably need to proprocess your data before feeding them to CycleGAN. I recommend that you (1) apply portrait segmentation (e.g., this [one](https://github.com/dongwu92/AutoPortraitMatting)) to remove the background and (2) use face alignment for both source and target domains. ",probably need data feeding recommend apply portrait segmentation one remove background use face alignment source target,issue,negative,neutral,neutral,neutral,neutral,neutral
1040757455,"@duke023456  It is hard for CycleGAN to preserve the text in an image. If you can detect the text using an OCR method, you can try to preserve them using an additional loss. ",duke hard preserve text image detect text method try preserve additional loss,issue,negative,negative,negative,negative,negative,negative
1040752702,#1018 might be relevant to your issues. Would it be possible to share with us your python command? ,might relevant would possible share u python command,issue,negative,positive,positive,positive,positive,positive
1040065596,"It depends on `save_latest_freq`:
`parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')`
By default the latest net is cached every 5000 iterations. So its not necessarily a duplicate as it's by iteration rather than epoch.",saving latest default latest net every necessarily duplicate iteration rather epoch,issue,negative,positive,positive,positive,positive,positive
1038032516,"Hi @adnan0819 !
You can change the extension of the image file being saved in the 'save_image' function defined at /util/[visualizer.py] (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/003efc4c8819de47ff11b5a0af7ba09aee7f5fc1/util/visualizer.py)

 
    def save_images(webpage, visuals, image_path, aspect_ratio=1.0, width=256, use_wandb=False):
    
    """"""Save images to the disk.
    Parameters:
        webpage (the HTML class) -- the HTML webpage class that stores these imaegs (see html.py for more details)
        visuals (OrderedDict)    -- an ordered dictionary that stores (name, images (either tensor or numpy) ) pairs
        image_path (str)         -- the string is used to create image paths
        aspect_ratio (float)     -- the aspect ratio of saved images
        width (int)              -- the images will be resized to width x width
    ``This function will save images stored in 'visuals' to the HTML file specified by 'webpage'.
    """"""``
  
    image_dir = webpage.get_image_dir()
    short_path = ntpath.basename(image_path[0])
    name = os.path.splitext(short_path)[0]
    webpage.add_header(name)
    ims, txts, links = [], [], []
    ims_dict = {}
    for label, im_data in visuals.items():
        im = util.tensor2im(im_data)

    #Look here -----------------------------------------
        image_name = '%s_%s.png' % (name, label)     # Here the image name is being defined you can change it what way you want to.
    #Change here --------------------------------------
        save_path = os.path.join(image_dir, image_name)
        util.save_image(im, save_path, aspect_ratio=aspect_ratio)
        ims.append(image_name)
        txts.append(label)
        links.append(image_name)
        if use_wandb:
            ims_dict[label] = wandb.Image(im)
    webpage.add_images(ims, txts, links, width=width)
    if use_wandb:
        wandb.log(ims_dict)``

Hope it helps!",hi change extension image file saved function defined save disk class class see ordered dictionary name either tensor string used create image float aspect ratio saved width width width function save file name name link label look name label image name defined change way want change label label link hope,issue,positive,neutral,neutral,neutral,neutral,neutral
1031528695,"> You would need to run an ssh tunnel for the used port.
> […](#)
> On Thu, Dec 6, 2018 at 20:31 HuiZHANG ***@***.***> wrote: @SsnL <https://github.com/SsnL> Thank you. But it said ""This site can’t be reached"" when I click the http link. I run cycleGAN with ssh to remote server. Can I open it in my personal computer? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#371 (comment)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/371#issuecomment-444855896)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFaWZXCwQaHcD-vT3nf08OdQigJhbIt5ks5u2Q4zgaJpZM4WeLLP> .

How to do this please?",would need run tunnel used port wrote thank said site click link run remote server open personal computer reply directly view comment mute thread please,issue,positive,neutral,neutral,neutral,neutral,neutral
1025240675,"Nevermind the first part. The arguments `--batch_size 1 --serial_batches` made it happen.

Still, any way to have outputs as jpg's (the same format as the inputs)?",first part made happen still way format,issue,negative,positive,positive,positive,positive,positive
1019961724,"I was able to train with Sagemaker after reading some AWS Sagemaker blogs and official documents. I am closing this issue. 
Thanks",able train reading official issue thanks,issue,negative,positive,positive,positive,positive,positive
1018263473,Modify the parameters in the test.py，    --load_size 512，and then you will get 512pix image.,modify get pix image,issue,negative,neutral,neutral,neutral,neutral,neutral
1018253232,i have the same problem! try to add a roi loss on my objective!,problem try add roi loss objective,issue,negative,neutral,neutral,neutral,neutral,neutral
1018016321,"Most of the applications used in the paper only require local color and texture transfer. In these cases, 70x70 patches might be enough (for a 256x input image). Later work (e.g., [pix2pixHD](https://github.com/NVIDIA/pix2pixHD)) has explored using multi-scale discriminators, which can look at more pixels. ",used paper require local color texture transfer might enough input image later work look,issue,negative,neutral,neutral,neutral,neutral,neutral
1018012747,It could be caused by several reasons. (1) The monet2photo models are trained on landscape images. It may not work well for other types of inputs. (2) the scale of the test images might be different from the scale of training images. You may consider fine-tuning the models if you have additional training data. ,could several trained landscape may work well scale test might different scale training may consider additional training data,issue,negative,neutral,neutral,neutral,neutral,neutral
1018010721,"Your understanding is correct. ``plot_current_losses`` is for creating loss graphs. If visdom doesn't work well for you, you can also use wandb instead via ``--use_wandb``. ",understanding correct loss work well also use instead via,issue,negative,neutral,neutral,neutral,neutral,neutral
1018007490,Could you provide some pictures? It could be because of Dropout but it won't produce a very different image. ,could provide could dropout wo produce different image,issue,negative,neutral,neutral,neutral,neutral,neutral
1018002941,You should be able to call ``scheduler.get_last_lr()`` within the update_learning_rate() [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f13aab8148bd5f15b9eb47b690496df8dadbab0c/models/base_model.py#L119).  ,able call within function,issue,negative,positive,positive,positive,positive,positive
1017998840,"Yeah, PatchGAN will produce an output map given a whole image. You do not need to produce patches explicitly. ",yeah produce output map given whole image need produce explicitly,issue,negative,positive,positive,positive,positive,positive
1017997751,Not sure. You can also do crop_size 360 or something smaller. ,sure also something smaller,issue,negative,positive,positive,positive,positive,positive
1015129599,"the different is model.
pair is pix2pix
single is test",different model pair single test,issue,negative,negative,neutral,neutral,negative,negative
1013096811,@junyanz Yes. I think that would be good! Thanks,yes think would good thanks,issue,positive,positive,positive,positive,positive,positive
1012662580,"I see. Thank you very much!

> Our code will handle it automatically. It will sample a random image from domain A and a random image from domain B. The entire dataset will be used, not only the 4k out of 20k. We use the maximum of the sizes of two datasets. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L71).

",see thank much code handle automatically sample random image domain random image domain entire used use maximum size two see line,issue,negative,negative,negative,negative,negative,negative
1012630678,Thank you for reporting the issue. It should've been fixed by the [latest commit](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/003efc4c8819de47ff11b5a0af7ba09aee7f5fc1),thank issue fixed latest commit,issue,positive,positive,positive,positive,positive,positive
1012622911,You don't need to use activation functions for least square GANs. The network will learn to regress to 0 and 1 (otherwise it will pay a penalty). You only need a sigmoid for cross-entropy loss. See the original [paper](https://arxiv.org/pdf/1611.04076.pdf) for more details. ,need use activation least square network learn regress otherwise pay penalty need sigmoid loss see original paper,issue,negative,positive,neutral,neutral,positive,positive
1012619518,It is hard to tell without knowing your data and task. Feel free to send an email to Taesung and me.,hard tell without knowing data task feel free send,issue,negative,positive,neutral,neutral,positive,positive
1012617635,@AyushExel I am wondering why we need to add `or self.use_wandb` in this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/visualizer.py#L121)? Shall we remove it? ,wondering need add line shall remove,issue,negative,neutral,neutral,neutral,neutral,neutral
1012608572,"Our code will handle it automatically. It will sample a random image from domain A and a random image from domain B. The entire dataset will be used, not only the 4k out of 20k. We use the maximum of the sizes of two datasets. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L71). ",code handle automatically sample random image domain random image domain entire used use maximum size two see line,issue,negative,negative,negative,negative,negative,negative
1011238840,"@synthetica3d Thank you! After applying your suggestion, re-training, and then converting to an onnx file, I can get the expected result from onnx.
So, can I summarize that appearing this condition because converting onnx API will change the batch normalization behaviors but not change instance normalization?",thank suggestion converting file get result summarize condition converting change batch normalization change instance normalization,issue,negative,neutral,neutral,neutral,neutral,neutral
1004567659,"> Hi @liuhh02, I wondered if you ever found any interesting results with transfer learning on pix2pix? Either on the discriminator or the generator?

Yes I did! I applied pix2pix to the medical domain, specifically to translate between MRI and PET scans. I have published my findings [here](https://ieeexplore.ieee.org/abstract/document/9182970) and [here](https://link.springer.com/article/10.1007/s00259-020-05131-z).",hi ever found interesting transfer learning either discriminator generator yes applied medical domain specifically translate pet,issue,positive,positive,positive,positive,positive,positive
1001594217,"> > Hi,
> > Also need to convert pix2pix to ONNX. I don't know if you still need it, but I want share it here in case anyone wants to solve this issue.
> > first, modify (or you can define a new function) ""def load_networks"" in models/base_model.py:
> > add this to the end:
> > ```
> >             net.eval()
> >             net.cuda()  # in this example using cuda()
> > 
> >             """"""
> >             if len(self.gpu_ids) > 0 and torch.cuda.is_available():
> >                 torch.save(net.module.cpu().state_dict(), save_path)
> >                 net.cuda(self.gpu_ids[0])
> >             else:
> >                 torch.save(net.cpu().state_dict(), save_path)
> >             """"""
> > 
> >             batch_size = 1  
> >             input_shape = (3, 512, 512)  # in my case its 512
> >             export_onnx_file = load_filename[:-4]+"".onnx""  
> >             save_path = os.path.join(self.save_dir, export_onnx_file)
> > 
> >             dinput = torch.randn(batch_size, *input_shape).cuda()   #same with net: cuda()
> >             torch.onnx.export(net, dinput, save_path)
> > 
> >             print('The ONNX file ' + export_onnx_file + ' is saved at %s' % save_path)
> > ```
> > 
> > 
> >     
> >       
> >     
> > 
> >       
> >     
> > 
> >     
> >   
> > Run test.py again, you will get latest_net_G.onnx in the same dir with latest_net_G.pth. Run netron on this and it is quite good.
> > Again many thanks to all group members in Pix2Pix, it's really good!
> 
> Hi, is there a different workflow for cyclegan because the above doesn't work for me and I get the same error as the original poster. @jam0ss



> Hi,
> 
> Also need to convert pix2pix to ONNX. I don't know if you still need it, but I want share it here in case anyone wants to solve this issue.
> 
> first, modify (or you can define a new function) ""def load_networks"" in models/base_model.py: add this to the end:
> 
> ```
>             net.eval()
>             net.cuda()  # in this example using cuda()
> 
>             """"""
>             if len(self.gpu_ids) > 0 and torch.cuda.is_available():
>                 torch.save(net.module.cpu().state_dict(), save_path)
>                 net.cuda(self.gpu_ids[0])
>             else:
>                 torch.save(net.cpu().state_dict(), save_path)
>             """"""
> 
>             batch_size = 1  
>             input_shape = (3, 512, 512)  # in my case its 512
>             export_onnx_file = load_filename[:-4]+"".onnx""  
>             save_path = os.path.join(self.save_dir, export_onnx_file)
> 
>             dinput = torch.randn(batch_size, *input_shape).cuda()   #same with net: cuda()
>             torch.onnx.export(net, dinput, save_path)
> 
>             print('The ONNX file ' + export_onnx_file + ' is saved at %s' % save_path)
> ```
> 
> Run test.py again, you will get latest_net_G.onnx in the same dir with latest_net_G.pth. Run netron on this and it is quite good. Again many thanks to all group members in Pix2Pix, it's really good!

hi, I try the way you mentioned, it works. thank you. 
however, it seems the size can not change.
could you have any advance?
thank you again!",hi also need convert know still need want share case anyone solve issue first modify define new function add end example else case net net print file saved run get run quite good many thanks group really good hi different work get error original poster hi also need convert know still need want share case anyone solve issue first modify define new function add end example else case net net print file saved run get run quite good many thanks group really good hi try way work thank however size change could advance thank,issue,positive,positive,positive,positive,positive,positive
1000837875,I also meet this problem. Have you solved this problem now? @785256592 ,also meet problem problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1000720171,"File ""C: \Users\wangh\anaconda3\envS\pytorch\Lib\site packages requests \adapters.py"", Line 502, in send
raise ConnectionError(e, request=request)
requests . exceptions . ConnectionError: HTTPConnectionPool(host='LocaLhost', port=8097): Max retries exceeded with urL: /events (Caused by NewConnectionError('<requests . packages . urLLib
ection . HTTPConnection object at 00000024900700668>: Failed to establish a new connection: [WinError 10061]由于目标计算机积极拒绝，无法连接。',))
",file line send raise object establish new connection,issue,negative,positive,positive,positive,positive,positive
999709727,"Hello guys my code works but i still get this in the beginning 
Could not connect to Visdom server. 
 Trying to start a server....
Command: /home/bvde103/anaconda3/envs/pytorch-CycleGAN-and-pix2pix/bin/python -m visdom.server -p 8097 &>/dev/null &

and while training i dont see any graph plots for loss, i only see print visualizer printing different losses but plot visualizer for loss graphs i dont see. The function def plot_current_losses(self, epoch, counter_ratio, losses): in visualizer.py is for creating loss graphs for the training right ? Pls help",hello code work still get beginning could connect server trying start server command training dont see graph loss see print visualizer printing different plot visualizer loss dont see function self epoch loss training right help,issue,positive,positive,positive,positive,positive,positive
997502545,"> It's hard for pix2pix to produce high-res images. For high-res image synthesis, feel free to try pix2pixHD, [SPADE](https://github.com/NVlabs/SPADE), [SEAN](https://arxiv.org/abs/1911.12861), or other recent models. In pix2pixHD, maybe you can increase the weight of L1 loss and VGG loss to make the model more conservative.
> 
> I am not familiar with stochasticity during test time. Maybe you want to check if you can produce the same result with the same random seed or not.

Thank you for your useful reply, I will try it later. Thanks again!",hard produce image synthesis feel free try spade recent maybe increase weight loss loss make model conservative familiar test time maybe want check produce result random seed thank useful reply try later thanks,issue,positive,positive,neutral,neutral,positive,positive
996269084,"there can actually be many reasons. to find out i'd try and get some images put out whenever G_L1 is 2x its running average or something like that, and look at the images for clues. maybe just very different inputs - maybe a rare bug with your normalization or augmentation...",actually many find try get put whenever running average something like look maybe different maybe rare bug normalization augmentation,issue,negative,positive,positive,positive,positive,positive
996267865,Your application probably gives an error telling you the problem. My guess is that you do not have enough GPU memory to hold larger batches. If you just want the effects of a larger batch size and don't need the performance it *may* give - there are other ways to achieve the averaging effect of larger batches. Like collecting and averaging gradients over multiple smaller batches. ,application probably error telling problem guess enough memory hold want effect batch size need performance may give way achieve effect like multiple smaller,issue,negative,neutral,neutral,neutral,neutral,neutral
992587929,"Hi @morgan-bc,

Its very good use case. Do you mind asking whether you were able to solve this issue with pix2pix or you opted for any other model infra.

Thank you",hi good use case mind whether able solve issue model infra thank,issue,positive,positive,positive,positive,positive,positive
989787687,"The one thing I'm struggling to understand is that the discriminator looks at 70 x 70 patches. But if I understand correctly, it's input is the conditional image concatenated with either the real image or synthesised image. So if it's only looking at small patches at a time, how does it learn the relationship between the two images? How does it check that the conditional input has actually informed the image that has been generated?",one thing struggling understand discriminator understand correctly input conditional image either real image image looking small time learn relationship two check conditional input actually informed image,issue,negative,negative,neutral,neutral,negative,negative
989575481,"> You could either (1) align the images for two datasets, (2) train a model on cropped patches rather than the full images. There are probably other techniques for medical imaging, which I am not familiar with. Here are a few papers. [[1]](https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.13617?casa_token=dDzeVHO-LhoAAAAA%3A7UsgISYQr8Hn4UniDwbZFH-w8SYjv8ZKyQXS2ADnXRLn6bqDfdi0jNNhgxDuqCIJvXwHmFmuPY9U), [[2]](https://www.sciencedirect.com/science/article/pii/S0167814019331172?casa_token=d5tiTpO-eBsAAAAA:Z_plLWFfrIn36Gp01zGsIf1OHrPAOX2H3JKBngOCrhVQb6XYEHeLx2yphePs9qkpzsCtZdY),

thanks, i will try with the patch-based approach.",could either align two train model rather full probably medical familiar thanks try approach,issue,negative,positive,positive,positive,positive,positive
989213148,"@VishalP1975 You can use a pre-trained network as a discriminator. For segmentation, I recommend that you use a segmentation network, rather than pix2pix/cyclegan. ",use network discriminator segmentation recommend use segmentation network rather,issue,negative,neutral,neutral,neutral,neutral,neutral
989212138,@Chenziyang7799  Sorry I missed your questions.  here are some answers: (1) We calculate the cross-entropy loss for each element in the map and average them. (2) We use a smaller learning rate for D. (3) You can see this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f13aab8148bd5f15b9eb47b690496df8dadbab0c/models/networks.py#L240) for the False flag. ,sorry calculate loss element map average use smaller learning rate see line false flag,issue,negative,negative,negative,negative,negative,negative
989207424,You need to set the data path correctly. ,need set data path correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
989193611,"It's hard for pix2pix to produce high-res images. For high-res image synthesis, feel free to try pix2pixHD, [SPADE](https://github.com/NVlabs/SPADE), [SEAN](https://arxiv.org/abs/1911.12861), or other recent models. 
In pix2pixHD, maybe you can increase the weight of L1 loss and VGG loss to make the model more conservative. 

I am not familiar with stochasticity during test time. Maybe you want to check if you can produce the same result with the same random seed or not.
",hard produce image synthesis feel free try spade recent maybe increase weight loss loss make model conservative familiar test time maybe want check produce result random seed,issue,negative,negative,neutral,neutral,negative,negative
989188534,"It looks like that the generator is hiding information for the reconstruction purpose. See this [study](https://arxiv.org/abs/1712.02950) for more details. It is hard to know what's going on without seeing the entire dataset/task and trying multiple models. If you haven't tried it, you could train a model on smaller patches rather than the entire image. ",like generator information reconstruction purpose see study hard know going without seeing entire trying multiple tried could train model smaller rather entire image,issue,negative,negative,neutral,neutral,negative,negative
989184409,"Are you training the model on Linux or Windows? Are you able to train the model with the default dataset? It seems that you might get stuck in visdom. You can also disable the visdom by setting `--display_id 0`.

",training model able train model default might get stuck also disable setting,issue,negative,positive,positive,positive,positive,positive
989179317,"The network can support rectangle images (in the CycleGAN setting). The height and width need to be divisible by 4. 
See this [training and test tip](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-rectangular-images) for more details. ",network support rectangle setting height width need divisible see training test tip,issue,negative,neutral,neutral,neutral,neutral,neutral
988459548,"Moreover，if network don't support rectangle images, why test phase can inferrence successfully and output rectangle images with --preprocess none. So I think train phase can input rectangle images too. But I failed. And it should not be GPU memeory problem I think.",network support rectangle test phase successfully output rectangle none think train phase input rectangle problem think,issue,negative,positive,positive,positive,positive,positive
985258920,"Hey junyanz,

i checked the got for Aligned Dataset again. You are right, as default the same flipping is applied to both A and B. 
At first hand i just saw https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/00d5574908eb66fe0127b32d7b030001453f21d0/data/base_dataset.py#L102
The torch transformers.RandomHorizontalFlip

But there is a param for flipping randomly initialized at https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/00d5574908eb66fe0127b32d7b030001453f21d0/data/base_dataset.py#L76
",hey checked got right default applied first hand saw torch param randomly,issue,negative,positive,neutral,neutral,positive,positive
985009697,"It seems that you are using Windows. You probably need to fix the directory path (e.g.,  \ rather than / )",probably need fix directory path rather,issue,negative,neutral,neutral,neutral,neutral,neutral
985006990,Both should work. It probably doesn't matter in practice. ,work probably matter practice,issue,negative,neutral,neutral,neutral,neutral,neutral
985004928,See this [training and test tip](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images),see training test tip,issue,negative,neutral,neutral,neutral,neutral,neutral
985001915,"This is a strange error. I have not run into this error. Could you help us debug this? Next time this happens, could you put `torch.autograd.detect_anomaly` around the [backward function of G](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L178) and [D](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L138) following the instruction of [this link](https://pytorch.org/docs/stable/autograd.html) and share the output with us? Thank you in advance!",strange error run error could help u next time could put around backward function following instruction link share output u thank advance,issue,negative,negative,neutral,neutral,negative,negative
984992688,"Unfortunately, we cannot find this model anymore. Maybe you can find a similar model in this [repo](https://github.com/AAnoosheh/ToDayGAN). ",unfortunately find model maybe find similar model,issue,negative,negative,negative,negative,negative,negative
984989943,"This seems to be a difficult task, as you have to synthesize the entire background from a flat input image. Maybe you can insert your synthetic object into a natural background (without the object), and then try to learn a network to blend them. ",difficult task synthesize entire background flat input image maybe insert synthetic object natural background without object try learn network blend,issue,negative,negative,negative,negative,negative,negative
984985767,"I am not sure if changing hyper-parameters will help your case. Depending on the type of data you are working on, you might need a new reconstruction loss rather than the default L1 loss.",sure help case depending type data working might need new reconstruction loss rather default loss,issue,negative,positive,positive,positive,positive,positive
984983944,"If you set `--input_nc 1 --output_nc 1`, it will require a single-channel image as input. For 1D data, you probably need to implement your own data loader, generator, and discriminator. ",set require image input data probably need implement data loader generator discriminator,issue,negative,neutral,neutral,neutral,neutral,neutral
984981256,"The AlignedDataset should apply the same flipping, as noted in this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/aligned_dataset.py#L51). You don't have to apply the same flipping to unaligned cases, as they are different images. ",apply noted line apply unaligned different,issue,negative,neutral,neutral,neutral,neutral,neutral
979702154,"If your data is already grayscale.  Just add following parameters when you run `train.py`.  
`--input_nc 1 --output_nc 1`",data already add following run,issue,negative,neutral,neutral,neutral,neutral,neutral
978085572,"Hi @liuhh02, I wondered if you ever found any interesting results with transfer learning on pix2pix? Either on the discriminator or the generator?",hi ever found interesting transfer learning either discriminator generator,issue,negative,positive,positive,positive,positive,positive
975017495,"I have found the proble, when I save image by `transforms.ToPILImage()`,I have not converted the image from normalization to original `[0,255],unit 8` type;So my picture doesn't look like a normal picture.Can someone tell me how to use `transforms.ToPILImage()` correctly to save pictures？",found save image converted image normalization original unit type picture look like normal someone tell use correctly save,issue,positive,positive,positive,positive,positive,positive
972050269,"Hello, this error is quite strange, because File ""/data/pytorch-CycleGAN-and-pix2pix-master/models/networks.py"", line 584 is not inside `def __init__`. Did you make modifications to the code? Could you pull the latest code and re-run it?",hello error quite strange file line inside make code could pull latest code,issue,negative,positive,positive,positive,positive,positive
971146360,"> If you want to try some new loss, you can try [Hinge loss](https://paperswithcode.com/method/gan-hinge-loss). I don't think there exists a GAN objective loss that can improve results dramatically.

Thank you for the recommendation, I'll go ahead and have a try. Appreciate your help.
",want try new loss try hinge loss think gan objective loss improve dramatically thank recommendation go ahead try appreciate help,issue,positive,positive,neutral,neutral,positive,positive
968481656,"> If you want to generate 1024+ images, you may want to use HD. I guess pix2pix can handle 256. For 512, you can try both. I think pix2pixHD can support images with different sizes. If you have questions regarding this, please post your question on the pix2pixHD repo.

Hi @junyanz ,  recently I try pix2pix for 512, it seems like some generated image missing some detail or it didn't match the groudtruth well, then I try pix2pixHD, it does a good job, but the huge problem is that the model has artifacts randomly generated on image，I mean It training stage, the generate image has nothing artifacts at all. But in testing stage, same testing dataset folder, if I generate multiple times, the result is difference( here for example I mean ,1st gen_img1.jpg has no artifacts, 2nd time generated result (gen_img1.jpg has an artifacts on it as I showed in attached)
![artifacts](https://user-images.githubusercontent.com/70758744/141714464-ae7da7fb-909c-4fde-9d27-278eb817b2a0.png)

So my question will be, do you have any suggestions or advices for me ? like for pix2pix, should I set --num_D= pixel when training? Would that be help to improve some detail generated?  Or gan_loss model default  is lsgan, using vanilla would help?  For artifacts problem, I try 
 the solution in #issue 190, It seems like it didn't change much. Looking forward your professional advices, any helps I will very appreciate it! Thanks",want generate may want use guess handle try think support different size regarding please post question hi recently try like image missing detail match well try good job huge problem model randomly mean training stage generate image nothing testing stage testing folder generate multiple time result difference example mean st time result attached question like set training would help improve detail model default vanilla would help problem try solution issue like change much looking forward professional appreciate thanks,issue,positive,positive,neutral,neutral,positive,positive
968335061,"Thank you @junyanz  for your kind reply.
I'll try with them. I have achieved notable results by using wgan-gp loss.",thank kind reply try notable loss,issue,positive,positive,positive,positive,positive,positive
968160521,You need to modify the data loader for supporting multiple inputs. See this [post](1335) for more details. ,need modify data loader supporting multiple see post,issue,negative,positive,positive,positive,positive,positive
968160362,Sorry for the late reply. Is your task similar to GTA2Cityscapes? Do they have similar camera angles between source and target domains as well as to object category distributions? I noted that it will not work very well if source and target domains have different camera poses. It's quite hard to know the issues without seeing the data.  Feel free to send your images here or via email. ,sorry late reply task similar similar camera source target well object category noted work well source target different camera quite hard know without seeing data feel free send via,issue,positive,negative,neutral,neutral,negative,negative
968159899,"Sorry for the late reply. It's a little hard to know what's going on without seeing your images and results. You may want to have a close look at the GAN loss plot. Also consider (1) lowering cycle-consistency loss, (2) using a small number of paired data, (3) using additional auxiliary information (semantic label map)",sorry late reply little hard know going without seeing may want close look gan loss plot also consider lowering loss small number paired data additional auxiliary information semantic label map,issue,negative,negative,negative,negative,negative,negative
968159682,You can download pre-trained [models](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_pix2pix_model.sh#L3) and use our test code. See [README](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix) for more details. ,use test code see,issue,negative,neutral,neutral,neutral,neutral,neutral
968158915,"If you want to try some new loss, you can try [Hinge loss](https://paperswithcode.com/method/gan-hinge-loss). I don't think there exists a GAN objective loss that can improve results dramatically. ",want try new loss try hinge loss think gan objective loss improve dramatically,issue,negative,positive,neutral,neutral,positive,positive
967871296,"> You can modify the data loader by adapting this data loader template [class](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) to your format. Also, see the code [overview](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/overview.md) for more details (see the `data` directory).

Thank you very much! ",modify data loader data loader template class format also see code overview see data directory thank much,issue,negative,positive,positive,positive,positive,positive
966822061,"Sure. You can do that. But you can also just reduce the learning rate. It is up to you. 
You can replace the tanh() with other layers or just remove it. 
If your input data range is [-2, 2], you could normalize it into [-1, 1] in the data loader. You need to modify the line [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L111). You might also want to modify the tensor2im [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/361f8b00d671b66db752e66493b630be8bc7d67b/util/util.py#L24) which is used for saving images.  
",sure also reduce learning rate replace tanh remove input data range could normalize data loader need modify line might also want modify function used saving,issue,negative,positive,positive,positive,positive,positive
966820791,"You can modify the data loader by adapting this data loader template [class](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) to your format.  Also, see the code [overview](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/overview.md) for more details (see the `data` directory).",modify data loader data loader template class format also see code overview see data directory,issue,negative,neutral,neutral,neutral,neutral,neutral
966819472,See this post for a similar [discussion](#1319).,see post similar discussion,issue,negative,neutral,neutral,neutral,neutral,neutral
964803008,"Hey,

I think this is due to you specified the `norm=batch` in your train_opt.txt. In the code, if you apply the batchNom instead of the instanceNorm, then the bias in each conv2d layer will be removed (check [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/361f8b00d671b66db752e66493b630be8bc7d67b/models/networks.py#L335-#L338) and [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/361f8b00d671b66db752e66493b630be8bc7d67b/models/networks.py#L341) for details). So your trained weights actually store no bias at all, that's why Missing key(s) errors were raised.

I think your test option should follow the same setting, i.e., specifying the `norm=batch`. Hope this helps.",hey think due code apply instead bias layer removed check trained actually store bias missing key raised think test option follow setting hope,issue,negative,negative,neutral,neutral,negative,negative
964547311,"> WGAN loss is not very compatible with PatchGAN, as WGAN assumes that each input sample is independent which is not the case in PatchGAN. We did not have good success with GAN.

Sorry for the late response. Since WGAN loss is not compatible with PatchGAN, do you have any suggestions on what kind of discriminator structure would be a proper candidate? Pixel-discriminator might be a good choice? ",loss compatible input sample independent case good success gan sorry late response since loss compatible kind discriminator structure would proper candidate might good choice,issue,positive,positive,positive,positive,positive,positive
964305953,"> WGAN loss is not very compatible with PatchGAN, as WGAN assumes that each input sample is independent which is not the case in PatchGAN. We did not have good success with GAN.

In my case, the  the Loss_D_real and Loss_D_fake decrease to a very small value(small than 1e-2) after several steps, dose it means the model is collapsed?",loss compatible input sample independent case good success gan case decrease small value small several dose model,issue,positive,positive,neutral,neutral,positive,positive
962554562,"Hi, @junyanz , when I click on the following URL https://people.eecs.berkeley.edu/~taesung_park/pytorch-CycleGAN-and-pix2pix/models/$FILE.pth, it shows ""Resource not found"". 
Could you please upload the pretrained models and related datasets again? Thanks!",hi click following resource found could please related thanks,issue,positive,positive,neutral,neutral,positive,positive
962540007,"it seems the web socket caused this issue. Currently, I mute the web logging func and related code. This can be bypassed in a not elegant way 🤕 ",web socket issue currently mute web logging related code elegant way,issue,negative,positive,positive,positive,positive,positive
961282966,I replaced the model URL to a new one. The script should work now. ,model new one script work,issue,negative,positive,positive,positive,positive,positive
961172855,"> You might want to change lambda if you use a different GAN loss as each loss has a different range.

Thanks alot!",might want change lambda use different gan loss loss different range thanks,issue,negative,positive,neutral,neutral,positive,positive
960495942,Check [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/586) for the hyperparameters used in the CycleGAN. They work better than the default values. ,check used work better default,issue,negative,positive,positive,positive,positive,positive
960495179,The code does not support different numbers of training iterations for G and D. You can add FOR loop for Ds for pix2pix ([here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L119)) and CycleGAN ([here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L190)). People also find that using a larger learning rate for D ([TTUR](https://arxiv.org/abs/1706.08500)) is a simpler solution. ,code support different training add loop people also find learning rate simpler solution,issue,positive,neutral,neutral,neutral,neutral,neutral
960469596,"The current `contine_train` only works for your own model training. It requires all the generators and discriminators. Unfortunately, we haven't saved the discriminators.",current work model training unfortunately saved,issue,negative,negative,negative,negative,negative,negative
960468102,"Unfortunately, our personal space was removed after graduation. @tinghuiz Tinghui, do you still have the model? @phillipi @taesungp ",unfortunately personal space removed graduation still model,issue,negative,neutral,neutral,neutral,neutral,neutral
960467751,"yeah, make sure your path is consistent. We also haven't tested the code on Windows. It should work for Linux and MAC.",yeah make sure path consistent also tested code work mac,issue,positive,positive,positive,positive,positive,positive
960467395,You can use `--model test` to load only one model. See [FAQ](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#out-of-memory-174) for more details.  Also [README](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan).,use model test load one model see also,issue,negative,neutral,neutral,neutral,neutral,neutral
960465645,"Could you share with us more details? For example, are you trying to improve pix2pix or CycleGAN? Which aspects of the model are you trying to improve? 
Regarding the idea of using a mask, several papers have used the idea to improve the quality and localization of the results. You may want to check them out. See [1](https://proceedings.neurips.cc/paper/2018/file/4e87337f366f72daa424dae11df0538c-Paper.pdf) and [2](https://openaccess.thecvf.com/content_ECCV_2018/papers/Liang_Generative_Semantic_Manipulation_ECCV_2018_paper.pdf).",could share u example trying improve model trying improve regarding idea mask several used idea improve quality localization may want check see,issue,positive,neutral,neutral,neutral,neutral,neutral
959928941,"To further preserve the color, you can either increase the cycle-consistency loss or the identity mapping loss.  See these [flags](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L42). ",preserve color either increase loss identity loss see,issue,negative,neutral,neutral,neutral,neutral,neutral
959910423,The code will round up your input size 355 to 356. See [Training & Test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-rectangular-images) for more details on how to train on rectangular images. ,code round input size see training test train rectangular,issue,negative,negative,negative,negative,negative,negative
959887181,Both could work. Probably doesn't matter in practice. ,could work probably matter practice,issue,negative,neutral,neutral,neutral,neutral,neutral
959885452,It is hard to say. Try both the number of filters and the number of layers. It might not be a good idea to add too many downsampling layers (You can add 1 for 512x input). You can add more ResNet generator blocks,hard say try number number might good idea add many add input add generator,issue,negative,positive,positive,positive,positive,positive
959882015,"You could either (1) align the images for two datasets, (2) train a model on cropped patches rather than the full images. 
There are probably other techniques for medical imaging, which I am not familiar with. Here are a few papers. [[1]](https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.13617?casa_token=dDzeVHO-LhoAAAAA%3A7UsgISYQr8Hn4UniDwbZFH-w8SYjv8ZKyQXS2ADnXRLn6bqDfdi0jNNhgxDuqCIJvXwHmFmuPY9U), [[2]](https://www.sciencedirect.com/science/article/pii/S0167814019331172?casa_token=d5tiTpO-eBsAAAAA:Z_plLWFfrIn36Gp01zGsIf1OHrPAOX2H3JKBngOCrhVQb6XYEHeLx2yphePs9qkpzsCtZdY),  ",could either align two train model rather full probably medical familiar,issue,negative,positive,positive,positive,positive,positive
959865970,You might want to change lambda if you use a different GAN loss as each loss has a different range. ,might want change lambda use different gan loss loss different range,issue,negative,neutral,neutral,neutral,neutral,neutral
959863824,"WGAN loss is not compatible with PatchGAN, as WGAN assumes that each input sample is independent which is not the case in PatchGAN. We did not have good success with GAN. ",loss compatible input sample independent case good success gan,issue,positive,positive,positive,positive,positive,positive
958569452,"> Not sure what happened. It might depend on your server setting. You may want to try to use `wandb` if `visdom` doesn't work for you. To log training progress and test images to W&B dashboard, set the --use_wandb flag with the train and test script

Thank you for replying!  The issue was related to the parameter that I build docker, and I already sovled it, thanks again!",sure might depend server setting may want try use work log training progress test dashboard set flag train test script thank issue related parameter build docker already thanks,issue,positive,positive,positive,positive,positive,positive
957083737,"> This is pretty normal if you use LSGAN or vanilla GAN loss. They are not supposed to decrease. As long as they are not too small (1e-2), it should be fine. A decreasing Loss_G_L1 is also a good sign.

Thank you for the quick reply and your precious time. One follow up question, I also tried wgan, the Loss_D_real and Loss_D_fake have the similar behavior as I mentioned above, so I guess it should be fine?",pretty normal use vanilla gan loss supposed decrease long small fine decreasing also good sign thank quick reply precious time one follow question also tried similar behavior guess fine,issue,positive,positive,positive,positive,positive,positive
957081430,"Could you check this repo (https://github.com/jhoffman/cycada_release)? 
We don't have all the segmentation metrics in this repo. ",could check segmentation metric,issue,negative,neutral,neutral,neutral,neutral,neutral
957080783,"We don't have an ETA now. But we have per iteration timing, you can multiply it by the total number of iterations per epoch. ",eta per iteration timing multiply total number per epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
957080287,"This is pretty normal if you use LSGAN or vanilla GAN loss. They are not supposed to decrease. As long as they are not too small (1e-2), it should be fine. A decreasing Loss_G_L1 is also a good sign. ",pretty normal use vanilla gan loss supposed decrease long small fine decreasing also good sign,issue,positive,positive,positive,positive,positive,positive
957058654,"> I added a Wasserstein loss and gradient penalty loss in our codebase. Feel free to try it.

Thank you for the great work! I noticed that D_real and D_fake are all negative throughout the training, is it normal?",added loss gradient penalty loss feel free try thank great work negative throughout training normal,issue,negative,positive,positive,positive,positive,positive
953583785,"> A bigger lambda for cycle-consistency loss and identity loss will encourage the mapping to be more conservative (i.e., changing less). Extending it to 3D is non-trivial. If you use voxel representation, memory will be often an issue.

Thank you! I raise this problem because I use different adversarial loss while training, in MSE the Idt and Cycle will up to 4 and 8 at first, and when I use BCEWithLogits that two losses will drop to 0.4 and 0.8 at first epoch. I ask some people told me that lambda=10 and sigma=5 is always suit for common situation.
Thanks a lot again for replying me.",bigger lambda loss identity loss encourage conservative le extending use representation memory often issue thank raise problem use different loss training cycle first use two drop first epoch ask people told always suit common situation thanks lot,issue,negative,positive,neutral,neutral,positive,positive
953277255,It seems that you are using the custom code. Does the original code work for your datasets?,custom code original code work,issue,negative,positive,positive,positive,positive,positive
953276344,"What do you mean by ""full result image""? You could also use `wandb` to see more results. ",mean full result image could also use see,issue,negative,positive,neutral,neutral,positive,positive
953275409,"Not sure what happened. It might depend on your server setting. You may want to try to use `wandb` if `visdom` doesn't work for you. To log training progress and test images to W&B dashboard, set the --use_wandb flag with the train and test script",sure might depend server setting may want try use work log training progress test dashboard set flag train test script,issue,positive,positive,positive,positive,positive,positive
953273276,"It is a limitation of CycleGAN. Several recent papers try to learn shape deformation. You may want to check it out. 
[TransGaGa](https://arxiv.org/abs/1904.09571), [Improving shape deformation](https://arxiv.org/abs/1808.04325),  and [WarpGAN](https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_WarpGAN_Automatic_Caricature_Generation_CVPR_2019_paper.pdf)",limitation several recent try learn shape deformation may want check improving shape deformation,issue,negative,neutral,neutral,neutral,neutral,neutral
953269555,"Yes, you can. See [training and test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-rectangular-images) for more details. ",yes see training test,issue,negative,neutral,neutral,neutral,neutral,neutral
953266393,"I recommend that you use a card with bigger memory. (at least 12 GB for 600x600 images). If you have an 8GB card, you need to train models on cropped patches. See [training & test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images) here.  
You can get a 24-GB card such as RTX 3090.",recommend use card bigger memory least card need train see training test get card,issue,negative,negative,negative,negative,negative,negative
953259649,"A bigger lambda for cycle-consistency loss and identity loss will encourage the mapping to be more conservative (i.e., changing less). Extending it to 3D is non-trivial. If you use voxel representation, memory will be often an issue. ",bigger lambda loss identity loss encourage conservative le extending use representation memory often issue,issue,negative,neutral,neutral,neutral,neutral,neutral
953258036,Not sure how to fix it. There is an earlier [post](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/251) regarding the same issue. ,sure fix post regarding issue,issue,negative,positive,positive,positive,positive,positive
953257314,"Yes, for pix2pix, it works better without model.eval(). Without model.eval() and with BatchSize=1, the test code will use the statistics within each test image, rather than using the dataset statistics. That's part of the reasons why people use [InstanceNorm](https://arxiv.org/abs/1607.08022) later. ",yes work better without without test code use statistic within test image rather statistic part people use later,issue,positive,positive,positive,positive,positive,positive
953249422,"You are correct. You need to backward D when you calculate G's gradients. (even though D's weights are not being updated. Check this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L185). ). In GANs, people often update D twice. ",correct need backward calculate even though check line people often update twice,issue,negative,neutral,neutral,neutral,neutral,neutral
953109861,"> Question 1
> 
> According to WGAN-GP paper and [this](https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py#L219) implementation, loss should be `D_fake - D_real + gradient_penalty` Shouldn't it be `-` instead of `+` in `self.loss_D = (self.loss_D_real - self.loss_D_fake + gradient_penalty) `
> 
> Question 2
> 
> In your implementation of `backward_D_basic` function, you used `loss_D.backward()`, but I found this: ![image](https://user-images.githubusercontent.com/28640563/53292878-a18b7d80-37c1-11e9-9fbe-fadba2c22c6f.png)
> 
> Any suggestion?

Hi beerboaa,

Based on the implemented code here, the negative value will apply when they calculate the mean for real targets. So, they have multiplied the mean value to -1 when we have a real target. Therefore, instead of `-` we can use the sum of the D_fake and D_real as what we had for conventional gans. 

You should also do the backward on D_real and D_fake, I think. 
",question according paper implementation loss instead question implementation function used found image suggestion hi based code negative value apply calculate mean real mean value real target therefore instead use sum conventional also backward think,issue,negative,negative,negative,negative,negative,negative
953100064,"Hi,

I have a question regarding the wgan-gp when we train the pix2pix model generator. If we use the wgan-gp, the adversarial loss to train the generator can be negative values and when we add this to the l1 distance of the generator output to the target, we might end up getting very large negatives from adversarial loss and very large l1 distance, while the final weighted sum of these two cancels out each other! I am wondering if I have missed something here?",hi question regarding train model generator use loss train generator negative add distance generator output target might end getting large loss large distance final weighted sum two wondering something,issue,negative,positive,neutral,neutral,positive,positive
952936000,"
    def backward_D_basic(self, netD, real, fake):
        # Real
        pred_real = netD(real)
        loss_D_real = self.criterionGAN(pred_real, True)
        # Fake
        pred_fake = netD(fake.detach())
        loss_D_fake = self.criterionGAN(pred_fake, False)
        # Combined loss and calculate gradients
        loss_D = (loss_D_real + loss_D_fake) * 0.5
        loss_D.backward()
        return loss_D

    def backward_D_A(self):
        """"""Calculate GAN loss for discriminator D_A""""""
        fake_B = self.fake_B_pool.query(self.fake_B)
        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)

    def backward_D_B(self):
        """"""Calculate GAN loss for discriminator D_B""""""
        fake_A = self.fake_A_pool.query(self.fake_A)
        self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)

    def backward_G(self):
        """"""Calculate the loss for generators G_A and G_B""""""
        lambda_idt = self.opt.lambda_identity
        lambda_A = self.opt.lambda_A
        lambda_B = self.opt.lambda_B
        # Identity loss
        if lambda_idt > 0:
            # G_A should be identity if real_B is fed: ||G_A(B) - B||
            self.idt_A = self.netG_A(self.real_B)
            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt
            # G_B should be identity if real_A is fed: ||G_B(A) - A||
            self.idt_B = self.netG_B(self.real_A)
            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt
        else:
            self.loss_idt_A = 0
            self.loss_idt_B = 0

        # GAN loss D_A(G_A(A))
        self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)
        # GAN loss D_B(G_B(B))
        self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)
        # Forward cycle loss || G_B(G_A(A)) - A||
        self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A
        # Backward cycle loss || G_A(G_B(B)) - B||
        self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B
        # combined loss and calculate gradients
        self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B
        self.loss_G.backward()",self real fake real real true fake false combined loss calculate return self calculate gan loss discriminator self calculate gan loss discriminator self calculate loss identity loss identity fed identity fed else gan loss true gan loss true forward cycle loss backward cycle loss combined loss calculate,issue,negative,negative,neutral,neutral,negative,negative
951740886,"谢谢您在百忙之中回复我的问题，目前问题已经解决了。之前的分布式训练是在npu上训练的，gpu上效果对比单gpu效果还可以。然后我就猜测可能是npu有问题，后面更换了新的驱动，再微调下参数效果就起来了。

---原始邮件---
发件人: ***@***.***&gt;
发送时间: 2021年10月20日(周三) 凌晨1:39
收件人: ***@***.***&gt;;
抄送: ***@***.******@***.***&gt;;
主题: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] 您好，目前我使用分布式时出现训练效果和单gpu的差距 (Issue #1327)




 
From your question, I wonder will training on multiple GPUs be different on training on single GPU even if everything else remains the same?
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.
Triage notifications on the go with GitHub Mobile for iOS or Android.",issue question wonder training multiple different training single even everything else remains thread reply directly view triage go mobile android,issue,negative,positive,neutral,neutral,positive,positive
948534395,"uh oh, setting `num_threads` to 0 will avoid this problem but I don't know a fix. ",oh setting avoid problem know fix,issue,negative,neutral,neutral,neutral,neutral,neutral
948243049,"@wenjingyi either
 (i) use much smaller images, 
(ii) try different batch size combinations (if you have batch size >2)
(iii) trim the datasize (in case you are using custom dataset)

Either of these three might help you out",either use much smaller try different batch size batch size trim case custom either three might help,issue,negative,neutral,neutral,neutral,neutral,neutral
947613339,"thanks ,i have meet the same problenm,and i will try your solution.",thanks meet try solution,issue,positive,positive,positive,positive,positive,positive
947437627,"i have a the similar question, too. rather than using parallel to train, I use autocast() to train Cyclegan3D for using torch.cuda.amp, I wonder how much performence will drop by doing this like i did, hoping for replying! Thanks a lot.",similar question rather parallel train use train wonder much drop like thanks lot,issue,positive,positive,neutral,neutral,positive,positive
946951339,"From your question, I wonder will training on multiple GPUs be different on training on single GPU even if everything else remains the same?",question wonder training multiple different training single even everything else remains,issue,negative,negative,neutral,neutral,negative,negative
945451454,"I use a normalization like  (x - x.min) / (x.max - x.min) just align the CT scans from [-1000, 1000] to [-1, 1], did you do the same and show that it didn't work well, and just adjust to standardazation then fixed?
I meet the same problem now... Fake A looks like Real B and (fakeB realA does the same). But I don't know how to fix this problem? will the normliazation data is the key to solve the problem?? @momentator plz. thanks alot",use normalization like align show work well adjust fixed meet problem fake like real know fix problem data key solve problem thanks,issue,negative,neutral,neutral,neutral,neutral,neutral
940976270,"I also wrote a code to plot losses after training using the log file. The main benefit (in contrast to @phiwei 's code) is that it works for every model and not only CycleGAN. It's probably not perfect but it can be used as a baseline. NB : ""experiment_name"" is the path to the folder where the log file is, and do not forget to change nb_data.

```
import os
import matplotlib.pyplot as plt
import re

def generate_stats_from_log(experiment_name, line_interval=10, nb_data=10800, enforce_last_line=True):
    """"""
    Generate chart with all losses from log file generated by CycleGAN/Pix2pix/CUT framework
    """"""
    #extract every lines
    with open(os.path.join(experiment_name, ""loss_log.txt""), 'r') as f:
        lines = f.readlines()
    #choose the lines to use for plotting
    lines_for_plot = []
    for i in range(1,len(lines)):
        if (i-1) % line_interval==0:
            lines_for_plot.append(lines[i])
    if enforce_last_line:
        lines_for_plot.append(lines[-1])
    #initialize dict with loss names
    dicts = dict()
    dicts[""epoch""] = []
    parts = (lines_for_plot[0]).split(') ')[1].split(' ')
    for i in range(0, len(parts)//2):
        dicts[parts[2*i][:-1]] = []
    #extract all data
    pattern = ""epoch: ([0-9]+), iters: ([0-9]+)""
    for l in lines_for_plot:
        search = re.search(pattern, l)
        epoch = int(search.group(1))
        epoch_floatpart = int(search.group(2))/nb_data
        dicts[""epoch""].append(epoch+epoch_floatpart) #to allow several plots for the same epoch
        parts = l.split(') ')[1].split(' ')
        for i in range(0, len(parts)//2):
            dicts[parts[2*i][:-1]].append(float(parts[2*i+1]))
    #plot everything
    plt.figure()
    for key in dicts.keys():
        if key != ""epoch"":
            plt.plot(dicts[""epoch""], dicts[key], label=key)
    plt.legend(loc=""best"")
    plt.show()
```",also wrote code plot training log file main benefit contrast code work every model probably perfect used path folder log file forget change import o import import generate chart log file framework extract every open choose use plotting range initialize loss epoch range extract data pattern epoch search pattern epoch epoch allow several epoch range float plot everything key key epoch epoch key best,issue,positive,positive,positive,positive,positive,positive
939433629,"hi  @ksh2h 
Have you solved it? I have the same problem. 
The first time I trained, the second time I couldn't.I didn't change any parameters.
but it is giving this error :
RuntimeError: CUDA out of memory. Tried to allocate 8.16 GiB (GPU 0; 24.00 GiB total capacity; 2.07 GiB already allocated; 13.25 GiB free; 8.49 GiB reserved in total by PyTorch)

I am using Anaconda on windows with pytorch 1.8, python 3.7..

",hi problem first time trained second time change giving error memory tried allocate gib gib total capacity gib already gib free gib reserved total anaconda python,issue,negative,positive,positive,positive,positive,positive
938418874,"> @lanzhoushaobing in the latest version of this repository, if you train your models with `--use_wandb` you'll see the trianing progress with inputs and outputs in your W&B dashboard. See example -> https://wandb.ai/cayush/CycleGAN&pix2pix/runs/3jdluzpq

I really appreciate it！i'll try it later.@AyushExel",latest version repository train see progress dashboard see example really appreciate try later,issue,positive,positive,positive,positive,positive,positive
937788405,"> AssertionError: ./datasets/dehazing\train is not a valid directory
> 
> Can you kindly help me to fix this error!

I have the same question, have you ever solved this problem? can you kindly help me to fix this error? thank you!",valid directory kindly help fix error question ever problem kindly help fix error thank,issue,positive,positive,positive,positive,positive,positive
932136963,"@lanzhoushaobing in the latest version of this repository, if you train your models with `--use_wandb` you'll see the trianing progress with inputs and outputs in your W&B dashboard. See example -> https://wandb.ai/cayush/CycleGAN&pix2pix/runs/3jdluzpq",latest version repository train see progress dashboard see example,issue,negative,positive,positive,positive,positive,positive
930163291,"You can try putting `torch.cuda.empty_cache()` after each iteration. This isn't really recommended though since it will mean slow training and you might still get the same OOM error. For cycleGAN I would think a 4Gb card is too small, if you don't have access to a bigger card you will need to use smaller images.",try iteration really though since mean slow training might still get error would think card small access bigger card need use smaller,issue,negative,negative,negative,negative,negative,negative
930068842,"@junyanz Hello, I want to try to train a 288x800 data set, how should I set options during test time? ",hello want try train data set set test time,issue,negative,neutral,neutral,neutral,neutral,neutral
929564273,"Thank you for your response @junyanz.
Did you mean to increase the number of filters of the generator?  for example --ngf to 128
or should I increase the num_downs of the generator? 
I am using a (512,512) image size.

Your answer really helps me a lot...!!  
",thank response mean increase number generator example increase generator image size answer really lot,issue,positive,negative,neutral,neutral,negative,negative
928963751,I have same problem too. I couldn't find a solution for this. I am doing the same instructions given; but I keep getting nan value.,problem could find solution given keep getting nan value,issue,negative,neutral,neutral,neutral,neutral,neutral
922235699,"Hi GZeta95,
May I ask how to turn 'iterations' into 'epochs' from loss_log.txt?
Could you share your codes on plotting these diagrams?
Thank you very much.",hi may ask turn could share plotting thank much,issue,positive,positive,positive,positive,positive,positive
917095488,It's possible. But you might also need to also increase the capacity of the generator. ,possible might also need also increase capacity generator,issue,negative,neutral,neutral,neutral,neutral,neutral
917094829,"Hello, I think the web server at Berkeley went offline for some period. I did move the link to a more stable server. Thank you for the heads up. ",hello think web server went period move link stable server thank,issue,positive,neutral,neutral,neutral,neutral,neutral
917094155,You don't need to have testB.  Just need to use `--model test`. See this [guideline](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) for more details. ,need need use model test see guideline,issue,negative,neutral,neutral,neutral,neutral,neutral
917092166,You can use different `--preprocess` options. See this [tip](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-rectangular-images) for more details.,use different see tip,issue,negative,neutral,neutral,neutral,neutral,neutral
917087082,"We cannot accept this PR at this time, due to two reasons: 
1. this PR (training time FID) may introduce too many changes to `train.py`. To add metrics to the training code, we need to add a metric abstract base class `metric.py` that can work for different metrics, implement an FID metric class `metric_FID.py`, and keep the training code simple. 
2. Pytorch-FID also has anti-aliasing issues.  FID calculation is quite complicated as it also involves which reference dataset you are using and how many samples you are using.  Please see the clean-fid [project](https://github.com/GaParmar/clean-fid) for more details. 

It might be great to first add the FID evaluation code for pre-trained checkpoints (after training is done). We will see if we could do that in the near future. ",accept time due two training time fid may introduce many add metric training code need add metric abstract base class work different metric implement fid metric class keep training code simple also fid calculation quite complicated also reference many please see project might great first add fid evaluation code training done see could near future,issue,positive,positive,neutral,neutral,positive,positive
917086830,"Thank you for PR. It may be helpful, but doesn't fit our intention of providing the Notebook as a way to showcase the use case of the whole repo. I will leave this PR without merging it so that people can use it in case they are interested. ",thank may helpful fit intention providing notebook way showcase use case whole leave without people use case interested,issue,positive,positive,positive,positive,positive,positive
917077642,"We will leave it as is. It might introduce too many changes to add a new GAN to the current repo. But if anyone is interested in progressive gans, you can refer to this PR. ",leave might introduce many add new gan current anyone interested progressive refer,issue,negative,positive,positive,positive,positive,positive
917076966,"For `scipy.misc.imresize`, we would like to keep the original even though it's deprecated, because it's been known that different resizing operations may lead to different scores. I will accept the PR but revert back that line. ",would like keep original even though known different may lead different accept revert back line,issue,positive,positive,neutral,neutral,positive,positive
917070542,"A common use case for using `preprocess == ""none""` is when the input images are already preprocessed. In particular, the image sizes at training need to be the same if batch size is larger than 1. So keeping the benchmark on seems reasonable. ",common use case none input already particular image size training need batch size keeping reasonable,issue,negative,positive,neutral,neutral,positive,positive
916973984,"After some more experimentation, this is a working version of `translate`:

```
from PIL import Image
import torchvision.transforms as transforms

def translate(img: np.ndarray, model: torch.nn.Module, device: torch.device):
    from PIL import Image
    img_p = Image.fromarray(img).convert(""RGB"")
    tr = transforms.Compose([transforms.Grayscale(1), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])
    with torch.no_grad():
        fake = (
            model(tr(img_p).to(device).view(1,1,256,256))
            .detach()
            .cpu()
            .numpy()
        ).reshape(img.shape)
    return fake
```

Naively I tried to replace the chain `PIL.Image -> RGB -> Grayscale -> ToTensor` by just converting my numpy array to a Tensor but that produced images like above... 

Now I must figure out what those operations do to the input compared to my naïve conversion.
Seems:
* PIL RGB Image is using `uint8` in range [0,255]
* `Grayscale` converts to gray but keeps `dtype` and range
* `ToTensor` converts to `torch.Tensor` and scales to [0,1] if input `dtype` is `uint8`",experimentation working version translate import image import translate model device import image fake model device return fake naively tried replace chain converting array tensor produced like must figure input conversion image range gray range scale input,issue,negative,negative,negative,negative,negative,negative
916883652,"I am working on the same thing, see issue https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1314

Did you implement a solution?",working thing see issue implement solution,issue,negative,neutral,neutral,neutral,neutral,neutral
916797241,"The aligned data looks like this (left MRI, right CT): 
![image](https://user-images.githubusercontent.com/12702862/132839259-834be8ef-4a77-4e0f-8e8c-8925b056d88a.png)

Given a real MRI I get a fake CT like this:
![image](https://user-images.githubusercontent.com/12702862/132870020-4595bd4b-b88a-4251-8a18-71b7170e9526.png)

or like this:
![image](https://user-images.githubusercontent.com/12702862/132870125-badd8b60-9527-4c14-b497-01bb49cbdf34.png)

After checking the options I pass to `define_G` are identical to the call from `test.py` I have the feeling the problem may be in how I load the image as numpy array (instead of through the dataset, skipping e.g. the transforms applied in `AlignedDataset`).
",data like left right image given real get fake like image like image pas identical call feeling problem may load image array instead skipping applied,issue,negative,negative,neutral,neutral,negative,negative
913140818,"This issue persists for me, even with anaconda environment. How about you @ApoorvaSuresh ?

@simon-eda, would you be so kind to provide details about your environment and which package versions you used?",issue even anaconda environment would kind provide environment package used,issue,positive,positive,positive,positive,positive,positive
910604031,"> > @LilNader I'm curious, were you working with 3-channel dumpy data (3 layers) or 4? I am currently trying to directly pass 4 layer/channels arrays but am having lots of trouble modifying the code since the PIL library seems to bank on 3 channels
> 
> @patrick-han In my case I was passing single channel numpy data (binary images) directly to the Network without using PIL and I had to set --input_nc and --output_nc to 1
> In your case, I guess you will need to indicate the number of input and output channels using --input_nc or --output_nc

Hello, I have tried to same as you (load npy files after all the changes you wrote here), but it doesn't really work for me.
So, I have used instead just np.load()
        A_img = Image.fromarray(np.load(A_path))
        B_img = Image.fromarray(np.load(B_path))
and in base_dataset.py I have commented this part (my images are already grayscale)
def get_transform(opt, params=None, grayscale=False, method=Image.BICUBIC, convert=True):
    transform_list = []
    # if grayscale:
    #     transform_list.append(transforms.Grayscale(1))

So, now the code works, but my images are super dark. And also, the results are still in png. So, I was wondering if you managed to change the saving of results to npy? Because maybe that would help me... After all, my npy files are type float32 and the png files are uint8, so there is probably some loss of precision....?
Thank you for any advice!",curious working dumpy data currently trying directly pas lot trouble code since library bank case passing single channel data binary directly network without set case guess need indicate number input output hello tried load wrote really work used instead part already opt code work super dark also still wondering change saving maybe would help type float probably loss precision thank advice,issue,positive,positive,neutral,neutral,positive,positive
906002120,"> Thank you so much, weight demodulation solved all my problems.
> In case someone wants to try it out, here's my PyTorch implementation:
> 
> ```
> class DemodulatedConv2d(nn.Module):
>     def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=0, bias=False, dilation=1):
>         super().__init__()
> 
>         self.eps = 1e-8
>         self.kernel_size = kernel_size
>         self.in_channel = in_channel
>         self.out_channel = out_channel
> 
>         self.weight = nn.Parameter(
>             torch.randn(1, out_channel, in_channel, kernel_size, kernel_size)
>         )
>         self.bias = None
>         if bias:
>             self.bias = nn.Parameter(torch.randn(out_channel))
> 
>         self.stride = stride
>         self.padding = padding
>         self.dilation = dilation
> 
>     def forward(self, input):
>         batch, in_channel, height, width = input.shape
> 
>         demod = torch.rsqrt(self.weight.pow(2).sum([2, 3, 4]) + 1e-8)
>         weight = self.weight * demod.view(batch, self.out_channel, 1, 1, 1)
> 
>         weight = weight.view(
>             batch * self.out_channel, in_channel, self.kernel_size, self.kernel_size
>         )
> 
>         input = input.view(1, batch * in_channel, height, width)
>         if self.bias is None:
>             out = F.conv2d(input, weight, padding=self.padding, groups=batch, dilation=self.dilation, stride=self.stride)
>         else:
>             out = F.conv2d(input, weight, bias=self.bias, padding=self.padding, groups=batch, dilation=self.dilation, stride=self.stride)
>         _, _, height, width = out.shape
>         out = out.view(batch, self.out_channel, height, width)
> 
>         return out
> ```

Thanks for your sharing!
I am very confused that your code is only the part of demodulation, do you need to modulate before this?
",thank much weight demodulation case someone try implementation class self super none bias stride padding dilation forward self input batch height width weight batch weight batch input batch height width none input weight else input weight height width batch height width return thanks confused code part demodulation need modulate,issue,positive,positive,neutral,neutral,positive,positive
905343327,"model inference,  in  test model,   For the same problem, I have tried these methods and the problem has not been solved. What is the cause?

@JamesChenChina 
@ghost 
@ctmackay",model inference test model problem tried problem cause ghost,issue,negative,neutral,neutral,neutral,neutral,neutral
904371389,"First thing to try is make your slash consistent. 
See the error - './datasets/data/mars/A/train\100_A.jpg'
",first thing try make slash consistent see error,issue,negative,positive,positive,positive,positive,positive
901109773,"Hi, Is it possible to use ResNet50 or other classifiers instead of PatchGAN classifier in discriminator? 
I am trying to segment only the fully visible objects in an image which contain a more number of objects but my model not able to detect all the completely visible objects. Do you have any suggestions on how to proceed for this task?

Thank you & Any suggestions would be appreciated.!!",hi possible use instead classifier discriminator trying segment fully visible image contain number model able detect completely visible proceed task thank would,issue,negative,positive,positive,positive,positive,positive
898414147,"I also have same problem when I train cyclegan on the data of both horse2zebra and maps, have you solve this problem?",also problem train data solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
886660625,"i have error too, after i run this""python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan"", the cmd show me ""Traceback (most recent call last):
  File ""train.py"", line 29, in <module>
    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options
  File ""C:\Users\USER\Downloads\pytorch-CycleGAN-and-pix2pix-master\pytorch-CycleGAN-and-pix2pix-master\data\__init__.py"", line 57, in create_dataset
    data_loader = CustomDatasetDataLoader(opt)
  File ""C:\Users\USER\Downloads\pytorch-CycleGAN-and-pix2pix-master\pytorch-CycleGAN-and-pix2pix-master\data\__init__.py"", line 73, in __init__
    self.dataset = dataset_class(opt)
  File ""C:\Users\USER\Downloads\pytorch-CycleGAN-and-pix2pix-master\pytorch-CycleGAN-and-pix2pix-master\data\unaligned_dataset.py"", line 29, in __init__
    self.A_paths = sorted(make_dataset(self.dir_A, opt.max_dataset_size))   # load images from '/path/to/data/trainA'
  File ""C:\Users\USER\Downloads\pytorch-CycleGAN-and-pix2pix-master\pytorch-CycleGAN-and-pix2pix-master\data\image_folder.py"", line 25, in make_dataset
    assert os.path.isdir(dir), '%s is not a valid directory' % dir
AssertionError: ./datasets/maps\trainA is not a valid directory""

Can someone help me ""AssertionError: ./datasets/maps\trainA is not a valid directory"" means what?",error run python name model show recent call last file line module opt create given file line opt file line opt file line sorted load file line assert valid directory valid directory someone help valid directory,issue,negative,neutral,neutral,neutral,neutral,neutral
884364096,"@ApoorvaSuresh  I could also connect to visdom, but I had to run on one CMD as administrator `python -m visdom.server` and then run `python train.py` in a different cmd... if you still can't do it, try `pip install -r requirements.txt` again, or install a different version of visdom server. 
",could also connect run one administrator python run python different still ca try pip install install different version server,issue,negative,neutral,neutral,neutral,neutral,neutral
884158747,"May I ask why are you in the ProGanModel doing generation from torch.randn and not from source Image A? It seems also that you don't even save the image A as input, this contradicts with the whole cycle consistency of the CycleGan as there's no loss between source image A and reconstruction of A.",may ask generation source image also even save image input whole cycle consistency loss source image reconstruction,issue,negative,positive,positive,positive,positive,positive
883262314,"> 
> 
> @simon-eda hey, did you find out what has to be changed to make it work? Please let me know :) Thank you!

i switched to anaconda enviroment and the issues does not come up anymore ",hey find make work please let know thank switched anaconda come,issue,positive,neutral,neutral,neutral,neutral,neutral
882631421,"@rickkk856 Thank you! Did the visdom server work for you? I always get ""Could not connect to Visdom server""",thank server work always get could connect server,issue,negative,neutral,neutral,neutral,neutral,neutral
882591685,"@simon-eda hey, did you find out what has to be changed to make it work? Please let me know :) Thank you!",hey find make work please let know thank,issue,positive,neutral,neutral,neutral,neutral,neutral
882552927,"@ApoorvaSuresh I'm using custom datasets, however, you can find it on the code or search for it on other sources like [Kaggle - Labels2Facades](https://www.kaggle.com/balraj98/facades-dataset?select=trainA)  you can also find them [Here](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)",custom however find code search like also find,issue,negative,neutral,neutral,neutral,neutral,neutral
882511283,"@rickkk856 @omid-ghozatlou could you please let me know how you downloaded the datasets? (the linux implementation uses bash, but how is it done in windows?)
Thank you :)",could please let know implementation bash done thank,issue,positive,neutral,neutral,neutral,neutral,neutral
882509429,@stefenmax did you find a way to do it? Please let me know :),find way please let know,issue,negative,neutral,neutral,neutral,neutral,neutral
882443351,"Hello, I am facing similar issue. Were you able to solve?",hello facing similar issue able solve,issue,negative,positive,positive,positive,positive,positive
879678727,"> > Did any1 ever find a good solution to this? I try to teach the model to remove specific objects from an image, so the input and output image is very similar.
> 
> Removing instance normalisation and changing all convolutions to ""demodulated"" convolutions (as said in StyleGANv2 paper in 2.2) helped in my case.

I face the same issue right now and I try to substitute all the conv2d to demodulated_conv as said in stylegan2, the problem seems fixed however I must train the model at a very small learning rate (i.e. 1e-5) or the model will collapse at the very beginning.
So I wonder are you substitute all the conv2d layers (including conv2d in D) to  demodulated_conv ? Have you faced the same problem with the learning rate? ",ever find good solution try teach model remove specific image input output image similar removing instance said paper case face issue right try substitute said problem fixed however must train model small learning rate model collapse beginning wonder substitute faced problem learning rate,issue,negative,positive,positive,positive,positive,positive
879390873,"Yes, I see. By using the cmd code in the Jupyter notebook, my GPU can be launched normally.",yes see code notebook normally,issue,negative,positive,positive,positive,positive,positive
879008941,"If you train with --norm instance the problem should go away. I noticed that if you run the test program after calling eval() on netG, the result will also be wrong. This is very strange because the dropout in batch norm is supposed to behave differently in train and inference, but pix2pix seems to want the exact same behavior in both training and inference. Since onnx is by definition for inference only, it is possible that batch norm, behaving as expected for inference, breaks everything. Anyway, using batch instance removes the problem.",train norm instance problem go away run test program calling result also wrong strange dropout batch norm supposed behave differently train inference want exact behavior training inference since definition inference possible batch norm inference everything anyway batch instance problem,issue,negative,negative,neutral,neutral,negative,negative
878723572,"Sorry for disturbing you, because of the invalid link, could you please share ""fcn-8s-cityscapes"" weight to me? thank you!

2436904681@qq.com",sorry disturbing invalid link could please share weight thank,issue,negative,negative,negative,negative,negative,negative
878076883,"Try using --norm instance because batch norm gave me endless headaches, and I found an issue with the batchnorm's behaviour after eval().
",try norm instance batch norm gave endless found issue behaviour,issue,negative,negative,negative,negative,negative,negative
878074759,"Hi, did you try using instance norm instead of batch norm?",hi try instance norm instead batch norm,issue,negative,neutral,neutral,neutral,neutral,neutral
878073353,How about that --crop size 256? Doesn't sound like what you want.,crop size sound like want,issue,negative,positive,positive,positive,positive,positive
877630535,As far as I know it doesn't matter since it is unpaired.,far know matter since unpaired,issue,negative,positive,neutral,neutral,positive,positive
876331474,"As far as I know it will run on GPU by default. But it is normal to take lots of time for training if your PC has single GPU.
During the training try this command:
$ nvidia-smi 
and check the third row,last colomn of the table to see how many percent of your GPU is using, probably it shows 100%,  then it means the training uses 100% of the GPU. you need to run the model on a PC having more GPUs to get higher speed.
This is what I know but I am not sure how much it is correct.",far know run default normal take lot time training single training try command check third row last table see many percent probably training need run model get higher speed know sure much correct,issue,negative,positive,positive,positive,positive,positive
876323974,"It seems it used to take this long time for training through single gpu.
I tried training with a PC having 4 GPUs and using this command below (don't forget to change batch size from 1 to 64):
$ python train.py --dataroot ./[name of your data folder] -- batch_size 64 --gpu_ids 1,2,3
and the speed increased. ",used take long time training single tried training command forget change batch size python name data folder speed,issue,negative,negative,neutral,neutral,negative,negative
873995083,"> 0.5 applies to both LSGAN and DCGAN. criterionGAN is a class for both objectives. 0.5 should make no big difference. In practice, we divide the objective by 2 while optimizing D, which slows down the rate at which D learns relatively to G.

Hi,
""... In practice, we divide the objective by 2 while optimizing D..."" does it mean you use loss_weights=0.5 in the compiling ? Thanks mate.

Steve
",class make big difference practice divide objective slows rate relatively hi practice divide objective mean use thanks mate,issue,negative,negative,neutral,neutral,negative,negative
873435880,"can you please provide a description on where and how to put lr_scheduler.step() AFTER optimizer.step()?

many thanks",please provide description put many thanks,issue,positive,positive,positive,positive,positive,positive
873112020,Facing the same issue with a 115k dataset. It takes around 2 hours to complete a single epoch. GPU utilization is at 100%,facing issue around complete single epoch utilization,issue,negative,positive,neutral,neutral,positive,positive
871278960,"If I want save this perceptual loss to the log_loss.txt，and visualization it, where I need to change.

very thanks
",want save perceptual loss visualization need change thanks,issue,positive,positive,positive,positive,positive,positive
870823830,"> Dear Jun-Yan Zhu and Taesung Park,
> 
> I'm making a new dataset using CycleGAN and I got satisfactory result.
> 
> However, the CycleGAN resizes images size to 256 when training or testing.
> 
> I want to learn the same size as original images even though original images size vary.
> 
> When testing, Is there any way for images to be outputted as original image size?
> e.g. original image 3EA: 1000x500, 500x300, 800x600 -> test result: 1000x500, 500x300, 800x600

set load_size to your input dimensions and fine size to your test dimensions. For example load_size = 1000. However, it is not possible to feed cyclegan rectangular images unless you modify the code yourself. You must use square inputs

",dear park making new got satisfactory result however size training testing want learn size original even though original size vary testing way original image size original image ea test result set input fine size test example however possible feed rectangular unless modify code must use square,issue,positive,positive,positive,positive,positive,positive
870754263,"Hi there

Thanks for your email.
I don't know why github doesn't inform me when people answer me in the
posts.
By the way I'm trying to find a network that is supervised and changes the
pictures to some other pictures.

Best

On Sun, May 23, 2021, 18:39 Mercury-ML ***@***.***> wrote:

> If you like, you can email me at stephena3b3c3 at gmail dot com and I can
> give you some guidance
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/582#issuecomment-846569324>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ASMJBK5KLMOGK4BHEEELVBLTPEEBPANCNFSM4G7BZ6YA>
> .
>
",hi thanks know inform people answer way trying find network best sun may wrote like dot give guidance reply directly view,issue,positive,positive,positive,positive,positive,positive
868985420,"Hi, just wondering are you using ubuntu or windows when running cycle gan? My 3090 is not working at all",hi wondering running cycle gan working,issue,negative,neutral,neutral,neutral,neutral,neutral
868294694,"Hello, can someone help me please !!!!",hello someone help please,issue,positive,neutral,neutral,neutral,neutral,neutral
866942297,"Hi. In the training after using --continue_train, you should use --epoch_count n =(the epoch you want to continue from). ",hi training use epoch want continue,issue,negative,neutral,neutral,neutral,neutral,neutral
864373849,"Also having this issue when running train.py as nohup on a cloud VM. It's frozen after 36 epochs. Did you have to restart the training, or will it automatically continue after a few hours?",also issue running cloud frozen restart training automatically continue,issue,negative,neutral,neutral,neutral,neutral,neutral
861891543,"Have you been able to convert this torch model to pytorch? 
",able convert torch model,issue,negative,positive,positive,positive,positive,positive
860090787,"Hi! i have the same problem. My Cycle-GAN generates images with big grid like this. Is this the same problem or not?
![generated7027](https://user-images.githubusercontent.com/79103055/121785709-94d70180-cbe5-11eb-8a46-b0f275b9c719.jpg)
",hi problem big grid like problem,issue,negative,neutral,neutral,neutral,neutral,neutral
858365125,"You have a UserWarning, check it! There is a little mistake in the code, you need to fork it and put lr_scheduler.step() AFTER optimizer.step()",check little mistake code need fork put,issue,negative,negative,negative,negative,negative,negative
858025313,@vis-opt @omid-ghozatlou I also got much noise when added `strict=False`. ,also got much noise added,issue,negative,positive,positive,positive,positive,positive
857556724,Hey ! You use 2 generators with one being the running average of the other and set adam parameter beta1 to 0. Why this choice ? Why not use the same generator for training and testing ?,hey use one running average set parameter beta choice use generator training testing,issue,negative,negative,negative,negative,negative,negative
853518176,"the same problem with CycleGAN using 'unet_256', any one can give me some help?",problem one give help,issue,negative,neutral,neutral,neutral,neutral,neutral
851427919,"> Hmm, do you have an NVIDIA GPU on your Mac? If not, you can set `--gpu_ids -1` to use CPU mode.

That worked for me as well. Thank you.",mac set use mode worked well thank,issue,positive,neutral,neutral,neutral,neutral,neutral
851356154,I opened a new issue  here #1286  since I noticed a similar to some extent behavior of CycleGAN in case of RGB to IR/thermal translation. @kpagels if you have any thoughts please feel free to share.,new issue since similar extent behavior case translation please feel free share,issue,positive,positive,positive,positive,positive,positive
850459498,As the generator doesn't get the feedback from the discriminator it's a simple Unet with l1 loss,generator get feedback discriminator simple loss,issue,negative,neutral,neutral,neutral,neutral,neutral
846569324,"If you like, you can email me at stephena3b3c3 at gmail dot com and I can give you some guidance ",like dot give guidance,issue,negative,neutral,neutral,neutral,neutral,neutral
846529477,"> Are you using Pix2Pix or Cyclegan? If pix2pix, this video is helpful: https://www.youtube.com/watch?v=vrvwfFej_r4&t=1352s

I want to use both of them(test my own data on them to see which one is better) but my problem is that I don't know how to give it my own data. can anyone help?",video helpful want use test data see one better problem know give data anyone help,issue,positive,positive,positive,positive,positive,positive
845167234,"> I am glad that you figured it out.

Hi There
Do you know how we can test the network on other datasets? please help and describe throughly.
Thanks in advance",glad figured hi know test network please help describe thanks advance,issue,positive,positive,positive,positive,positive,positive
845165791,"> You should set `--continue_train --epoch 3 --epoch_count 4` (3 is the current final epoch).
> `--epoch` option means which weights is loaded for initialization of networks.
> (Default value of this option is `latest`.)
> `--epoch_count` option means the start number of epoch count.
> 
> In your case, when only `--continue_train` was set, latest weights were supposed to be loaded because you didn't specify `--epoch` option.
> Then ""no such file"" error occurred because latest weights were not saved yet.
> 
> And when only `--epoch_count 3` was set, the program started training from scratch with start epoch number 3, because you didn't set `--continue_train` option.
> That's why your curve after epoch 3 had no relationship with epoch 1 and 2.

Hi There
Do you know how we can test the network on other datasets? please help.
Thanks in advance",set epoch current final epoch epoch option loaded default value option latest option start number epoch count case set latest supposed loaded specify epoch option file error latest saved yet set program training scratch start epoch number set option curve epoch relationship epoch hi know test network please help thanks advance,issue,positive,positive,positive,positive,positive,positive
845015336,"I had met the same problem, and here is a checklist that may help you.

1. Refer to issue #233, the test script should be like [one-direction](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) or [bi-direction](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#cyclegan-traintest). 
2. Check your `test_opt.txt` for checkpoint file details. Even you use default argparse, it's better to check one more time. (`./scripts/test_single.sh` use different model and params with default).
3. Remember to change the `latest_net_G_A/B.pth` to `latest_net_G.pth`. It depends on your input `--direction BtoA/AtoB`.",met problem may help refer issue test script like check file even use default better check one time use different model default remember change input direction,issue,positive,positive,positive,positive,positive,positive
843903333,"![image](https://user-images.githubusercontent.com/84323401/118786323-24211b80-b8ab-11eb-9397-6e654da48c92.png)
What's the problem? Please help!",image problem please help,issue,negative,neutral,neutral,neutral,neutral,neutral
842959936,"> It's because the generators use downsampling and upsampling so the image sizes need to be match at some levels. There is some discussion of this in the docs [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md).

Thanks for your reply~ ^^",use image size need match discussion thanks,issue,negative,positive,positive,positive,positive,positive
841947864,"Damn near blew my PC up trying to run combine_A_and_B due to this... ended up spawning 32 Python.exe processes even though PyCharm terminal was terminated, and they would just keep respawning in task manager when I manually ended the tasks until I eventually used a cmd prompt to kill them all.

`tasklist | find /i ""python.exe"" && taskkill /im ""python.exe"" /F || echo process ""python.exe"" not running`",damn near trying run due ended spawning even though terminal would keep task manager manually ended eventually used prompt kill find echo process running,issue,negative,negative,neutral,neutral,negative,negative
841784740,I have similar issue at epoch 25 with 3000 images for epoch running at my pc. There are some operation that are done periodically with that frequency?,similar issue epoch epoch running operation done periodically frequency,issue,negative,neutral,neutral,neutral,neutral,neutral
841775133,"Thanks  @JunlinHan, I’m sorry for not replying in time. For the solution , @KingOnTheStar, I had not continued to follow up on the issue  later. But I think the author's suggestion might be right. ",thanks sorry time solution continued follow issue later think author suggestion might right,issue,positive,negative,neutral,neutral,negative,negative
841235500,"If you want more control over storing results you can try just directly change `train.py`. You can grab the images using something like 
```
if total_iters % opt.display_freq == 0:
    if opt.model == 'pix2pix':
        model.compute_visuals()
        visuals = model.get_current_visuals()
    
        image_realA = visuals['real_A'].data
        arr_realA = image_realA[0][0].cpu().float().numpy()
```
Then you have your image as a numpy array (mine is grayscale so I remove the channel dimension with the second [0]) and you can do whatever suits you, `visuals` is a dictionary containing the images as tensors at the current iteration.",want control try directly change grab something like image array mine remove channel dimension second whatever dictionary current iteration,issue,negative,positive,neutral,neutral,positive,positive
840468098,"Thank you very much for your suggestions, I will try it. 


sincerely You Qi",thank much try sincerely,issue,positive,positive,positive,positive,positive,positive
840415910,"Hello, as far as I've understood, you can reduce --update_html_freq and --display_freq parameters. 
For instance, if you have 1000 images in training set, setting --update_html_freq 100 --display_freq 100 will result in saving 10 experimental results each epoch.",hello far understood reduce instance training set setting result saving experimental epoch,issue,negative,positive,neutral,neutral,positive,positive
840037724,It's because the generators use downsampling and upsampling so the image sizes need to be match at some levels. There is some discussion of this in the docs [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md).,use image size need match discussion,issue,negative,neutral,neutral,neutral,neutral,neutral
838025345,"In file pix2pix_model.py, line 29
"" By default, we use vanilla GAN loss, UNet with batchnorm, and aligned datasets.""",file line default use vanilla gan loss,issue,negative,neutral,neutral,neutral,neutral,neutral
838012183,"Hello, I have the same problem, how do you solve it
I solved it, pip install visdom==0.1.8.8",hello problem solve pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
837905293,"It's related to the training generator net you selected. By default, it will choose unet, it need input image size with n*256. If you want to train other size, you may need to train the model using resnet9blocks(something like that) for the generator. @fire717 ",related training generator net selected default choose need input image size want train size may need train model something like generator fire,issue,negative,neutral,neutral,neutral,neutral,neutral
834651435,"#Update - It seems if you train on windows, specially using `pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=10.1` you won't be able  to reproduce results on google colab...

I'm not sure if it's a Torch / Torchvision / Cuda incompatibility error
OR if it's a windows-unbutu error. ",update train specially wo able reproduce sure torch incompatibility error error,issue,negative,positive,positive,positive,positive,positive
833122683,"> Still doesn't give me any index.html

Verbosity only increases the log level. For further analysis by some maintainer, I would suggest you to send the log through a paste instead of partial screenshots. See here: https://paste.linux.community/ . Before sending the log, make sure there's no sensitive info on the logs.",still give verbosity log level analysis maintainer would suggest send log paste instead partial see sending log make sure sensitive,issue,negative,positive,positive,positive,positive,positive
833093353,"Ah thanks.

Just gives me this information.
![image](https://user-images.githubusercontent.com/83720730/117217942-2ccd1880-adfa-11eb-8182-b5dd3d642822.png)

Still doesn't give me any index.html
",ah thanks information image still give,issue,negative,positive,positive,positive,positive,positive
833090389,"You should only add `--verbose` option, not `logging`",add verbose option logging,issue,negative,neutral,neutral,neutral,neutral,neutral
833089687,"Hi, thank you for your answer.

I'm running this on Google Colab.

These are the past arguments on training:
![image](https://user-images.githubusercontent.com/83720730/117216986-8f251980-adf8-11eb-96a4-aae8d38f0168.png)

And these are the past arguments on testing:
![image](https://user-images.githubusercontent.com/83720730/117217030-aa902480-adf8-11eb-836f-b1f1b0c95a0a.png)

When I add the `--verbose logging` it gives me this error:
![image](https://user-images.githubusercontent.com/83720730/117217173-e2976780-adf8-11eb-9ba4-f36fd9bb5c93.png)
",hi thank answer running past training image past testing image add verbose logging error image,issue,negative,negative,negative,negative,negative,negative
833012386,"Hi @GoncaloCJG,

Can you describe your running environment, such as running OS and passed arguments ? Also, consider running it with verbose logging by adding `--verbose` to your arguments list.",hi describe running environment running o also consider running verbose logging verbose list,issue,negative,neutral,neutral,neutral,neutral,neutral
827578136,"> 
> 
> Hi,
> Thank you for this great implementation!
> When I was trying to use your model I faced some issues when trying to remove the checkerboard artifact.
> 
> As I read the posts before, I tried using upsample + conv (kernel 3_3, stride 1) to replace the conv(4_4, stride 2).
> 
> After training the CycleGAN model for 100 epochs, I ran into the stage that both generators decide to do nothing. Real_A and Fake_B looks similar, Real_B and Fake_A looks similar. Loss_D change from 1 to 0.02 and Loss_G change from 0.6 to 0.9.
> 
> I have a few thoughts about this:
> 
>     1. Changing the capacity of Generators and Discriminator:
>        Since now the parameter of G is 39M and Discriminator is 2M
> 
>     2. The conv size 3*3 is too small, I should try to make it larger.
> 
> 
> I am not sure about my approaches, would you have any suggestion on these thoughts or have other ideas?

Excuse me,can you commit your changed code? Thank you very much.
",hi thank great implementation trying use model faced trying remove checkerboard artifact read tried kernel stride replace stride training model ran stage decide nothing similar similar change change capacity discriminator since parameter discriminator size small try make sure would suggestion excuse commit code thank much,issue,positive,positive,positive,positive,positive,positive
826863965,"> 
> 
> > @Scienceseb Hi, can you explain the role of this perceptual loss in your training ?, you use it to improve the generators ?
> 
> Perceptual loss make the resulting images sharper.

how is it？
",hi explain role perceptual loss training use improve perceptual loss make resulting sharper,issue,negative,neutral,neutral,neutral,neutral,neutral
826863561,"> 
> 
> > @Scienceseb Hi, can you explain the role of this perceptual loss in your training ?, you use it to improve the generators ?
> 
> Perceptual loss make the resulting images sharper.

Excuse me,what is the effect of adding perceptual loss",hi explain role perceptual loss training use improve perceptual loss make resulting sharper excuse effect perceptual loss,issue,negative,negative,neutral,neutral,negative,negative
825324024,"we have a similar problem. we are trying to train a model that translate video data captured by one camera to a video that looks like captured by another camera.  we have video clips from both cameras.  Some video clips are paired but not registered (two cameras are bundled together and capture the same scene) and some clips are not paired. 
I think the model will be more robust if the pairing information can be leveraged during the training. 
 ",similar problem trying train model translate video data one camera video like another camera video clip video clip paired registered two together capture scene clip paired think model robust information training,issue,negative,neutral,neutral,neutral,neutral,neutral
824597819,"> i'm having the same issue as @ArchitectTaeyoon with python3.8.
> @ArchitectTaeyoon how were u able to solve/fix your error?

An example will be like (suppose I want to apply a pre-trained model):
`python ./test.py --dataroot ./datasets/monet2photo/testA/ --name monet2photo_pretrained --model test --no_dropout --gpu_ids -1`
Just add --gpu_ids -1 to your python command.",issue python able error example like suppose want apply model python name model test add python command,issue,negative,positive,positive,positive,positive,positive
821926023,"> @turian changed the number of input_nc and output_nc before, but it didn't work. Seems like it failed to match the Unet structure.
> ![error](https://user-images.githubusercontent.com/38985624/115132384-3c92e180-a032-11eb-807a-cc5d3e7a45a7.png)
> I'm a beginner, it can be very puzzling.

This suggests that the net knows it should get 4 channels, but you are using code from this repo that only loads 3 channels. So you should change the image reading code so that it retrieves 4 channels",number work like match structure error beginner puzzling net get code change image reading code,issue,negative,neutral,neutral,neutral,neutral,neutral
821925906,Make sure it's batchsize x 4 x width x height and width and height must be multiples of 256. Maybe even 512. Otherwise use resnet9 not unet,make sure width height width height must maybe even otherwise use,issue,negative,positive,positive,positive,positive,positive
821920576,"@turian   changed the number of input_nc and output_nc before, but it didn't work. Seems like it failed to match the Unet structure.
![error](https://user-images.githubusercontent.com/38985624/115132384-3c92e180-a032-11eb-807a-cc5d3e7a45a7.png)
I'm a beginner, it can be very puzzling.

",number work like match structure error beginner puzzling,issue,negative,neutral,neutral,neutral,neutral,neutral
821893524,"You can easily change the number of input channels using input_nc. From float16 to float32, I am not sure.",easily change number input float float sure,issue,positive,positive,positive,positive,positive,positive
820462808,"Check line 30 in train_options.py: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f13aab8148bd5f15b9eb47b690496df8dadbab0c/options/train_options.py#L30
Setting `--n_epochs` to 20 will train for 20 epochs with the initial learning rate and training continues for `n_epochs_decay`. You may stop the training at 20 epochs by distributing epochs between the n_epochs and n_epochs_decay (say 10 and 10 ?)as per your requirement. 


Check line 53 in the base_options.py :  https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f13aab8148bd5f15b9eb47b690496df8dadbab0c/options/base_options.py#L53

If you have saved the 20th model during training, you should be able to load it when you test either by passing` --epoch 20 `with your test command. ",check line setting train initial learning rate training may stop training say per requirement check line saved th model training able load test either passing epoch test command,issue,negative,positive,positive,positive,positive,positive
819628763,The value for dropout is hard-coded as 0.5 (see models/networks.py: lines 415 and 525). You can try changing them to see what happens.,value dropout see try see,issue,negative,neutral,neutral,neutral,neutral,neutral
819352183,"

> I have a similar problem and I'm wondering if you can help me. I am trying to use some `np.int16` images. I assume that I could somehow rescale each pixel to be in the range [0,255] but not sure if there is a better way of dealing with my data. I feel that going from `np.int16` to `np.int8` is losing some information. Do you think I can just train using the np.int16 data? or somehow float data that is not in the range [0,255]? Is this range a requirement?
> 
> Any help is appreciated. Thank you in advance.

@carluri 
-Did you find a solution to your problem. I am trying to do the same and would appreciate advice.

",similar problem wondering help trying use assume could somehow range sure better way dealing data feel going losing information think train data somehow float data range range requirement help thank advance find solution problem trying would appreciate advice,issue,positive,positive,positive,positive,positive,positive
817608662,"I meet the same error. It seems that there are dropout layers when training but no dropout layers when test.
(10): ResnetBlock(
        (conv_block): Sequential(
          (0): ReflectionPad2d((1, 1, 1, 1))
          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU(inplace=True)
          (4): Dropout(p=0.5, inplace=False)
          (5): ReflectionPad2d((1, 1, 1, 1))
          (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)
          (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
(10): ResnetBlock(
        (conv_block): Sequential(
          (0): ReflectionPad2d((1, 1, 1, 1))
          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)
          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU(inplace=True)
          (4): ReflectionPad2d((1, 1, 1, 1))
          (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)
          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )",meet error dropout training dropout test sequential dropout sequential,issue,negative,neutral,neutral,neutral,neutral,neutral
816601773,"I have experienced similar problems. E.g., the training stopped after the first completed epoch without exiting - that was after executing train.py in a jobscript on a server. Other times it was fine.",experienced similar training stopped first epoch without server time fine,issue,negative,positive,positive,positive,positive,positive
815804995,"Also still getting this warning, and I noticed that at least in the first 25 training epochs, the learning rate never updates (remains at 0.0002000). Though I noticed that the PR referenced above hasn't merged

https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/1101",also still getting warning least first training learning rate never remains though,issue,negative,negative,neutral,neutral,negative,negative
814682502,"While we still don't have the implemented metrics I prepared simple way to get FID after training using test data _real images on domain B_ and _generated images from domain B_  --> input domain is simply ignored 

https://github.com/rickkk856/Batch_FID_PyTorchPix2Pix",still metric prepared simple way get fid training test data domain domain input domain simply,issue,negative,neutral,neutral,neutral,neutral,neutral
814615929,"Hello @villanuevab, Did you train the model on multiple input images(A) and single-output (B). Can you share the idea? ",hello train model multiple input share idea,issue,negative,neutral,neutral,neutral,neutral,neutral
812740493,"Closed after finding ""How can I make it work for my own data (e.g., 16-bit png, tiff, hyperspectral images)? (#309, #320, #202)
"" in QA.md",closed finding make work data tiff,issue,negative,negative,neutral,neutral,negative,negative
810979718,"@rickkk856 I rewrote a code of my own, and it worked. Thanks anyway!",code worked thanks anyway,issue,negative,positive,positive,positive,positive,positive
810871227,"@rickkk856 Hello, it didn't work at first, I thought it was because of the route path/to/fold/A did not follow the structure datasets/experiment_name/foldA. I changed the code to ""python combine_A_and_B.py --foldA ./experiment_name/foldA --foldB ./experiment_name/foldB --foldAB --foldA ./experiment_name/foldAB"" but it turned out I was wrong. Could you please help me work it out?",hello work first thought route follow structure code python turned wrong could please help work,issue,negative,negative,negative,negative,negative,negative
808908517,Much possible. I used it once on a long job and it fails me. DO NOT use Colab.,much possible used long job use,issue,negative,negative,neutral,neutral,negative,negative
808121557,"You can add --norm batch to the configuration, it looks like --norm instance is selected by default and does not match the options used in training process",add norm batch configuration like norm instance selected default match used training process,issue,negative,neutral,neutral,neutral,neutral,neutral
807303423,"Update:
When I Run ""model test"" dataset single mode, works fine. But this is the correct aproach, since i've trained with the pix2pix model (dataset mode aligned) and now I'm testing it with ""model test"" (single mode)?
Thank you so much. ",update run model test single mode work fine correct since trained model mode testing model test single mode thank much,issue,positive,positive,positive,positive,positive,positive
806395218,"I had a similar issue so I needed to look for another 'combine method'
The code bellow worked for me

`python combine_A_and_B.py --foldA path/to/fold/A --foldB path/to/fold/B --foldAB --foldA path/to/fold/AB`


```
import os
import numpy as np
import cv2
import argparse

parser = argparse.ArgumentParser('create image pairs')
parser.add_argument('--fold_A', dest='fold_A', help='input directory for image A', type=str, default='../dataset/50kshoes_edges')
parser.add_argument('--fold_B', dest='fold_B', help='input directory for image B', type=str, default='../dataset/50kshoes_jpg')
parser.add_argument('--fold_AB', dest='fold_AB', help='output directory', type=str, default='../dataset/test_AB')
parser.add_argument('--num_imgs', dest='num_imgs', help='number of images', type=int, default=1000000)
parser.add_argument('--use_AB', dest='use_AB', help='if true: (0001_A, 0001_B) to (0001_AB)', action='store_true')
args = parser.parse_args()

for arg in vars(args):
    print('[%s] = ' % arg, getattr(args, arg))

splits = os.listdir(args.fold_A)

for sp in splits:
    img_fold_A = os.path.join(args.fold_A, sp)
    img_fold_B = os.path.join(args.fold_B, sp)
    img_list = os.listdir(img_fold_A)
    if args.use_AB:
        img_list = [img_path for img_path in img_list if '_A.' in img_path]

    num_imgs = min(args.num_imgs, len(img_list))
    print('split = %s, use %d/%d images' % (sp, num_imgs, len(img_list)))
    img_fold_AB = os.path.join(args.fold_AB, sp)
    if not os.path.isdir(img_fold_AB):
        os.makedirs(img_fold_AB)
    print('split = %s, number of images = %d' % (sp, num_imgs))
    for n in range(num_imgs):
        name_A = img_list[n]
        path_A = os.path.join(img_fold_A, name_A)
        if args.use_AB:
            name_B = name_A.replace('_A.', '_B.')
        else:
            name_B = name_A
        path_B = os.path.join(img_fold_B, name_B)
        if os.path.isfile(path_A) and os.path.isfile(path_B):
            name_AB = name_A
            if args.use_AB:
                name_AB = name_AB.replace('_A.', '.')  # remove _A
            path_AB = os.path.join(img_fold_AB, name_AB)
            
            im_A = cv2.imread(path_A, -1) # python2: cv2.CV_LOAD_IMAGE_COLOR; python3: cv2.IMREAD_COLOR
            im_B = cv2.imread(path_B, -1) # python2: cv2.CV_LOAD_IMAGE_COLOR; python3: cv2.IMREAD_COLOR
            
            #im_A = cv2.imread(path_A, 1) # python2: cv2.CV_LOAD_IMAGE_COLOR; python3: cv2.IMREAD_COLOR
            #im_B = cv2.imread(path_B, 1) # python2: cv2.CV_LOAD_IMAGE_COLOR; python3: cv2.IMREAD_COLOR
            im_AB = np.concatenate([im_A, im_B], 1)
            cv2.imwrite(path_AB, im_AB)

```

Remember that the data folder structure needs to be:

```
└── datasets
    └── experiment_name
       ├── foldA
       │    └──train
       │    └──test
       │    └──eval(optional)
       ├── foldB
       │    └──train
       │    └──test
       │    └──eval(optional)
       └──foldAB
```
After you run the code the train, test and eval folders will be created in foldAB, and the files will be combined there
",similar issue look another code bellow worked python import o import import import parser image directory image directory image directory true print min print use print number range else remove python python python python python python python python remember data folder structure need optional optional run code train test combined,issue,negative,positive,positive,positive,positive,positive
806390374,"> fixed: [jolibrain/joliGAN#31](https://github.com/jolibrain/joliGAN/pull/31)

A final solution is to downgrade `torch==1.2.0` and `torchvision==0.4.0`, without changing the `base_dataset.py` as in the above link. The training is successful after downgrading these two packages.",fixed final solution downgrade without link training successful two,issue,negative,positive,positive,positive,positive,positive
806388748,"Before start training run on a different cmd `python -m visdom.server`

and open `http://localhost:8097/` on browser

If you shutdown or stop to `--continue_train` you'll need to process the `loss_logs.txt` with other software like excel for example.",start training run different python open browser shutdown stop need process like excel example,issue,positive,neutral,neutral,neutral,neutral,neutral
805979425,"I had the same issue

Trained for 200 epoch with initial learning rate... than dropout for more 200 epoch
But my computer shutted down on epoch 350 so i needed to run
```
python train.py --dataroot ./datasets/experiment_name --name experiment_name --model pix2pix --netG unet_128  --preprocess none --epoch_count 351 --n_epochs 200 --n_epochs_decay 200 --continue_train
```

Hope this helps other people",issue trained epoch initial learning rate dropout epoch computer epoch run python name model none hope people,issue,negative,neutral,neutral,neutral,neutral,neutral
795030357,"This is the full error

(imaging) C:\Users\sinie\Documents\Utrecht_Universiteit\Stage\pytorch-CycleGAN-and-pix2pix>python datasets/combine_A_and_B.py --fold_A A --fold_B B --fold_AB AB
[fold_A] =  A
[fold_B] =  B
[fold_AB] =  AB
[num_imgs] =  1000000
[use_AB] =  False
[no_multiprocessing] =  False
split = test, use 22/22 images
split = test, number of images = 22
split = train, use 176/176 images
split = train, number of images = 176
split = val, use 22/22 images
split = val, number of images = 22
[fold_A] =  A
[fold_B] =  B
[fold_AB] =  AB
[num_imgs] =  1000000
[use_AB] =  False
[no_multiprocessing] =  False
[fold_A] =  A
[fold_B] =  B
[fold_AB] =  AB
[num_imgs] =  1000000
[use_AB] =  False
[no_multiprocessing] =  False
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 125, in _main
    prepare(preparation_data)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 265, in run_path
Traceback (most recent call last):
    return _run_module_code(code, init_globals, run_name,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 97, in _run_module_code
  File ""<string>"", line 1, in <module>
    _run_code(code, mod_globals, init_globals,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 87, in _run_code
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 125, in _main
    prepare(preparation_data)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 236, in prepare
[fold_A] =  A
[fold_B] =  B
[fold_AB] =  AB
[num_imgs] =  1000000
[use_AB] =  False
[no_multiprocessing] =  False
    _fixup_main_from_path(data['init_main_from_path'])
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\sinie\Documents\Utrecht_Universiteit\Stage\pytorch-CycleGAN-and-pix2pix\datasets\combine_A_and_B.py"", line 30, in <module>    exec(code, run_globals)
  File ""C:\Users\sinie\Documents\Utrecht_Universiteit\Stage\pytorch-CycleGAN-and-pix2pix\datasets\combine_A_and_B.py"", line 30, in <module>    pool=Pool()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\context.py"", line 119, in Pool
    pool=Pool()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\context.py"", line 119, in Pool
[fold_A] =  A
    return Pool(processes, initializer, initargs, maxtasksperchild,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 212, in __init__
[fold_B] =  B
    self._repopulate_pool()
[fold_AB] =  AB
    return Pool(processes, initializer, initargs, maxtasksperchild,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 212, in __init__
    self._repopulate_pool()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 303, in _repopulate_pool
    return self._repopulate_pool_static(self._ctx, self.Process,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 326, in _repopulate_pool_static
[num_imgs] =  1000000
[use_AB] =  False
    w.start()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\process.py"", line 121, in start
[no_multiprocessing] =  False
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 303, in _repopulate_pool
    self._popen = self._Popen(self)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\context.py"", line 327, in _Popen
    return Popen(process_obj)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 116, in spawn_main
    _check_not_importing_main()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main
    return self._repopulate_pool_static(self._ctx, self.Process,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 326, in _repopulate_pool_static
    raise RuntimeError('''
    exitcode = _main(fd, parent_sentinel)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 125, in _main
    prepare(preparation_data)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 265, in run_path
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
    return _run_module_code(code, init_globals, run_name,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\sinie\Documents\Utrecht_Universiteit\Stage\pytorch-CycleGAN-and-pix2pix\datasets\combine_A_and_B.py"", line 30, in <module>    pool=Pool()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\context.py"", line 119, in Pool
    return Pool(processes, initializer, initargs, maxtasksperchild,
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 125, in _main
    w.start()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\process.py"", line 121, in start
    prepare(preparation_data)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 236, in prepare
    self._popen = self._Popen(self)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\context.py"", line 327, in _Popen
    _fixup_main_from_path(data['init_main_from_path'])
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path
    return Popen(process_obj)
    main_content = runpy.run_path(main_path,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\runpy.py"", line 87, in _run_code
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 212, in __init__
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__
    prep_data = spawn.get_preparation_data(process_obj._name)
    self._repopulate_pool()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 303, in _repopulate_pool
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data
    exec(code, run_globals)
  File ""C:\Users\sinie\Documents\Utrecht_Universiteit\Stage\pytorch-CycleGAN-and-pix2pix\datasets\combine_A_and_B.py"", line 30, in <module>    _check_not_importing_main()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main
    pool=Pool()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\context.py"", line 119, in Pool
    return Pool(processes, initializer, initargs, maxtasksperchild,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 212, in __init__
    self._repopulate_pool()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 303, in _repopulate_pool
    return self._repopulate_pool_static(self._ctx, self.Process,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 326, in _repopulate_pool_static
    raise RuntimeError('''
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
    return self._repopulate_pool_static(self._ctx, self.Process,
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\pool.py"", line 326, in _repopulate_pool_static
    w.start()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\process.py"", line 121, in start
    self._popen = self._Popen(self)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\context.py"", line 327, in _Popen
    return Popen(process_obj)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__
    w.start()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\process.py"", line 121, in start
    prep_data = spawn.get_preparation_data(process_obj._name)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data
    self._popen = self._Popen(self)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\context.py"", line 327, in _Popen
    _check_not_importing_main()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main
    return Popen(process_obj)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__
    raise RuntimeError('''
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
    prep_data = spawn.get_preparation_data(process_obj._name)
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data
    _check_not_importing_main()
  File ""C:\Users\sinie\anaconda3\envs\imaging\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.",full error python false false split test use split test number split train use split train number split use split number false false false false recent call last file string line module file line file line prepare file line prepare data file line file line recent call last return code file line file string line module code file line file line file line prepare file line prepare false false data file line file line return code file line code file line code file line module code file line module file line pool file line pool return pool file line return pool file line file line return file line false file line start false recent call last file string line module file line self file line return file line file line file line file line return file line raise file line prepare file line prepare data file line file line attempt made start new process current process finished phase probably fork start child forgotten use proper idiom main module line program going frozen produce executable return code file line code file line code file line module file line pool return pool recent call last file string line module file line file line file line start prepare file line prepare self file line data file line return file line return code file line code file line file line file line file line file line code file line module file line file line pool return pool file line file line return file line raise attempt made start new process current process finished phase probably fork start child forgotten use proper idiom main module line program going frozen produce executable return file line file line start self file line return file line file line start file line self file line file line return file line raise attempt made start new process current process finished phase probably fork start child forgotten use proper idiom main module line program going frozen produce executable file line file line raise attempt made start new process current process finished phase probably fork start child forgotten use proper idiom main module line program going frozen produce executable,issue,negative,negative,neutral,neutral,negative,negative
794351246,"@junyanz The flag direction only switches the input data but not the model. Suppose I set the default flag as AtoB during training and during testing I set direction to BtoA,  the result will be incorrect as the model does not receive inputs from the correct domain. Could you please help clarify this? Thank you.",flag direction input data model suppose set default flag training testing set direction result incorrect model receive correct domain could please help clarify thank,issue,positive,neutral,neutral,neutral,neutral,neutral
793704630,"Hi, I don't understand why you don't need a sigmoid or something similar. Your discriminator final layer is a convolution that can output any value, and your range is [0,1]=(fake, real). Could you help me understand?

It would be the same for multilabel discriminator? (your discriminator distinguish between multiple classes instead of fake/real)

@junyanz ",hi understand need sigmoid something similar discriminator final layer convolution output value range fake real could help understand would discriminator discriminator distinguish multiple class instead,issue,negative,negative,neutral,neutral,negative,negative
790808475,"Regarding [L2 regularization](https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch)
I can't see visual improvements adding `self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999), weight_decay=1e-4)`, but for larger values of `weight_decay` like `1e-1`, it seems breaks things a bit.


Example for `weight_decay=1e-1`:
<img width=""794"" alt=""1e-1"" src=""https://user-images.githubusercontent.com/4003908/110007523-acfbd280-7d12-11eb-974d-5063bf1ce17d.png"">

",regarding regularization ca see visual like bit example,issue,negative,neutral,neutral,neutral,neutral,neutral
790068878,"I wonder how weight demodulation should work in the case when generator don't have adain like additional inputs?

Looking at the stylegan2 code: there is `modulated_conv2d_layer`
Here is `modulate` https://github.com/NVlabs/stylegan2/blob/6af5afc72dbeb77bb2bd49919a7b8dcfc8ea644d/training/networks_stylegan2.py#L97-L100
Here is `demodulate`
https://github.com/NVlabs/stylegan2/blob/6af5afc72dbeb77bb2bd49919a7b8dcfc8ea644d/training/networks_stylegan2.py#L102-L105

As I understand based on examples above suggestion is to omit modulation part and only use demodulation part, but this way it looks like some kind of weight normalization, I wonder just clipping weights or adding l2 regularisation for weights will have about the same effect?
",wonder weight demodulation work case generator like additional looking code modulate understand based suggestion omit modulation part use demodulation part way like kind weight normalization wonder clipping effect,issue,positive,positive,positive,positive,positive,positive
789566566,"> Another solution to this is to modify `net.load_state_dict(state_dict, strict=False)` where I added the `strict=False` option. This allows the network to load weights as long as the sizes and the number of parameters fit, even if the key-names aren't exact.

Thank you! It is working but the results are not as good as samples saved during training. It includes much noise",another solution modify added option network load long size number fit even exact thank working good saved training much noise,issue,positive,positive,positive,positive,positive,positive
789384060,"@junyanz and @taesungp Is there any updates on this? :) Thank you!

**Update:** Found #1096 for anyone who is still looking for this ",thank update found anyone still looking,issue,negative,neutral,neutral,neutral,neutral,neutral
786993517,Thanks a lot @phillipi. Really appreciate it.,thanks lot really appreciate,issue,positive,positive,positive,positive,positive,positive
786953773,"I think it took a week or so on one GPU in 2016, maybe it was a GTX 1080? Here is the torch model from that time: https://www.dropbox.com/s/4cs4skja7h1njj0/1_net_G.t7?dl=0",think took week one maybe torch model time,issue,negative,neutral,neutral,neutral,neutral,neutral
786946842,"> We don't have a PyTorch pre-trained model. But feel free to train one by yourself. Please contact @phillipi for the Torch model.

Can you tell me how long did it take for training this? the colorization model on an imagenet dataset with the same configurations that you describe in the paper - 6 epochs and a batch size of 4 and what hardware did you use?
Thanks",model feel free train one please contact torch model tell long take training colorization model describe paper batch size hardware use thanks,issue,positive,positive,positive,positive,positive,positive
785548223,"Yeah this happens to me at 87. What I did is interrupting the kernel and resuming training from the same epoch. To resume training from the same epoch use the same training line you used before but add --continue_train and epoch_count 15 to it.
It will start the training again from epoch 17 and use the latest saved model. Worked for me. Don't quite understand why it happens though.",yeah interrupting kernel training epoch resume training epoch use training line used add start training epoch use latest saved model worked quite understand though,issue,negative,positive,positive,positive,positive,positive
785472396,You can use the option --continue_train. Also set --epoch_count to specify a different starting epoch count.,use option also set specify different starting epoch count,issue,negative,neutral,neutral,neutral,neutral,neutral
783375189,"dataset size is totally OK, 5000 is fairly a large dataset for I2I.
Try to train your network with higher resolution training inputs, you may try load images in 512 res then cropping to 400 res... ( depends on your GPU)",size totally fairly large try train network higher resolution training may try load,issue,negative,positive,positive,positive,positive,positive
782612183,combining datasets A + B should have a change in main see --> [this link](https://medium.com/@mackie__m/running-a-cifar-10-image-classifier-on-windows-with-pytorch-9094e29089cd),combining change main see link,issue,negative,positive,positive,positive,positive,positive
781697653,"I'm a windows user and I have already used this repo... just follow the installation instructions, and make sure you installed all de dependencies... 

I just couldn't figure out yet how to plot loss from loss.txt and how to use the combine_A_and_B.py",user already used follow installation make sure de could figure yet plot loss use,issue,negative,positive,positive,positive,positive,positive
779254378,"Hi, could it be that you have to change the channel of your image from 3 to 1 (if greyscale)? You do this via ""--input_nc 1 --output_nc 1"".",hi could change channel image via,issue,negative,neutral,neutral,neutral,neutral,neutral
778530644,"> > Hello,I run your train.py with GPU and the newest pytorch >>> torch.**version** '0.4.0a0+5463a4a',THEN I have this problem ,I haven't use pytorch and visdom before ,so I don't know how to fix it ,Could you please help me with this problem?
> > Traceback (most recent call last):
> > File ""/home/yt/anaconda3/lib/python3.6/site-packages/visdom/**init**.py"", line 261, in _send
> > data=json.dumps(msg),
> > File ""/home/yt/anaconda3/lib/python3.6/json/**init**.py"", line 231, in dumps
> > return _default_encoder.encode(obj)
> > File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 199, in encode
> > chunks = self.iterencode(o, _one_shot=True)
> > File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 257, in iterencode
> > return _iterencode(o, 0)
> > File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 180, in default
> > o.**class**.**name**)
> > TypeError: Object of type 'Tensor' is not JSON serializable
> > (epoch: 1, iters: 300, time: 0.278, data: 0.001) D_A: 0.164 G_A: 0.396 cycle_A: 1.978 idt_A: 1.903 D_B: 0.312 G_B: 0.730 cycle_B: 4.014 idt_B: 0.921
> > Exception in user code:
> > Traceback (most recent call last):
> > File ""/home/yt/anaconda3/lib/python3.6/site-packages/visdom/**init**.py"", line 261, in _send
> > data=json.dumps(msg),
> > File ""/home/yt/anaconda3/lib/python3.6/json/**init**.py"", line 231, in dumps
> > return _default_encoder.encode(obj)
> > File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 199, in encode
> > chunks = self.iterencode(o, _one_shot=True)
> > File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 257, in iterencode
> > return _iterencode(o, 0)
> > File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 180, in default
> > o.**class**.**name**)
> > TypeError: Object of type 'Tensor' is not JSON serializable
> > Traceback (most recent call last):
> > File ""train.py"", line 35, in
> > visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)
> > File ""/media/yt/SearchforGraduationDesign/pytorch-CycleGAN-and-pix2pix/util/visualizer.py"", line 53, in display_current_results
> > image_numpy = util.tensor2im(image)
> > File ""/media/yt/SearchforGraduationDesign/pytorch-CycleGAN-and-pix2pix/util/util.py"", line 30, in tensor2im
> > image_numpy = image_tensor[0].cpu().float().numpy()
> > RuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.
> 
> did this help? [fossasia/visdom#554](https://github.com/fossasia/visdom/issues/554)

If like me you just wanted to save the dict of tensors in a human readable way check this out:

https://discuss.pytorch.org/t/typeerror-tensor-is-not-json-serializable/36065/3 or https://stackoverflow.com/questions/12943819/how-to-prettyprint-a-json-file/66180687#66180687.",hello run torch version problem use know fix could please help problem recent call last file line file line return file line encode file line return file line default class name object type epoch time data exception user code recent call last file line file line return file line encode file line return file line default class name object type recent call last file line epoch file line image file line ca call variable grad use instead help like save human readable way check,issue,positive,neutral,neutral,neutral,neutral,neutral
778506875,"> Hello,I run your train.py with GPU and the newest pytorch >>> torch.**version** '0.4.0a0+5463a4a',THEN I have this problem ,I haven't use pytorch and visdom before ,so I don't know how to fix it ,Could you please help me with this problem?
> Traceback (most recent call last):
> File ""/home/yt/anaconda3/lib/python3.6/site-packages/visdom/**init**.py"", line 261, in _send
> data=json.dumps(msg),
> File ""/home/yt/anaconda3/lib/python3.6/json/**init**.py"", line 231, in dumps
> return _default_encoder.encode(obj)
> File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 199, in encode
> chunks = self.iterencode(o, _one_shot=True)
> File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 257, in iterencode
> return _iterencode(o, 0)
> File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 180, in default
> o.**class**.**name**)
> TypeError: Object of type 'Tensor' is not JSON serializable
> (epoch: 1, iters: 300, time: 0.278, data: 0.001) D_A: 0.164 G_A: 0.396 cycle_A: 1.978 idt_A: 1.903 D_B: 0.312 G_B: 0.730 cycle_B: 4.014 idt_B: 0.921
> Exception in user code:
> Traceback (most recent call last):
> File ""/home/yt/anaconda3/lib/python3.6/site-packages/visdom/**init**.py"", line 261, in _send
> data=json.dumps(msg),
> File ""/home/yt/anaconda3/lib/python3.6/json/**init**.py"", line 231, in dumps
> return _default_encoder.encode(obj)
> File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 199, in encode
> chunks = self.iterencode(o, _one_shot=True)
> File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 257, in iterencode
> return _iterencode(o, 0)
> File ""/home/yt/anaconda3/lib/python3.6/json/encoder.py"", line 180, in default
> o.**class**.**name**)
> TypeError: Object of type 'Tensor' is not JSON serializable
> Traceback (most recent call last):
> File ""train.py"", line 35, in
> visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)
> File ""/media/yt/SearchforGraduationDesign/pytorch-CycleGAN-and-pix2pix/util/visualizer.py"", line 53, in display_current_results
> image_numpy = util.tensor2im(image)
> File ""/media/yt/SearchforGraduationDesign/pytorch-CycleGAN-and-pix2pix/util/util.py"", line 30, in tensor2im
> image_numpy = image_tensor[0].cpu().float().numpy()
> RuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.

did this help? https://github.com/fossasia/visdom/issues/554",hello run torch version problem use know fix could please help problem recent call last file line file line return file line encode file line return file line default class name object type epoch time data exception user code recent call last file line file line return file line encode file line return file line default class name object type recent call last file line epoch file line image file line ca call variable grad use instead help,issue,negative,neutral,neutral,neutral,neutral,neutral
777931844,"I also got the same error. I had cloned my repo in Jan 2021, so I don't understand why it shows up, since it should have been resolved by now. Has it been updated? ",also got error understand since resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
777705154,@phyuphyuthaw Did you solve the problem?? I have the same issue.. How long does it take to wait? ,solve problem issue long take wait,issue,negative,negative,neutral,neutral,negative,negative
772511115,"I also have same problem when I train cycleGAN to transform horses to zebras. However untill now no person answer my question which I issued last month.
Starting from the first epoch ,  all of losses are 'NaN'. It's really wired. And I deleted  grayscale images mixed in train datasets, but it not worked.",also problem train transform however untill person answer question last month starting first epoch really wired mixed train worked,issue,negative,positive,neutral,neutral,positive,positive
772311922,"If we used Aligned Dataset in Cycle Gan , we can add losses with Real_B and Fake_B comparison (Supervised comparison) ? Right ?

I just want to confirm this , I am writing a project on this ... Any help will be highly appreciated @junyanz ",used cycle gan add comparison comparison right want confirm writing project help highly,issue,negative,positive,positive,positive,positive,positive
771670905,Hi Please refer this paper: https://arxiv.org/abs/1611.04076,hi please refer paper,issue,negative,neutral,neutral,neutral,neutral,neutral
770250612,"@junyanz 
Why doesn't `TestModel` set `no_dropout=True` by default? It seems strange to me because `CycleGANModel` does this: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f13aab8148bd5f15b9eb47b690496df8dadbab0c/models/cycle_gan_model.py#L39 `TestModel` is only used for testing `CycleGANModel`'s results so it should be logical to have the same behavior by default for them.",set default strange used testing logical behavior default,issue,negative,positive,neutral,neutral,positive,positive
768011558,"Solution 1: Increase the weights for idt loss and G loss for mapping (image2label).
Solution2 : change your training epochs to default setting. ( 200 epochs in total)",solution increase loss loss solution change training default setting total,issue,negative,neutral,neutral,neutral,neutral,neutral
768008673," Definitely, you can. But I assume you will get inferior results compared to Pix2Pix.
Aligned dataset is easier for the training of network. Using aligned dataset for training may reduce mode collapse.",definitely assume get inferior easier training network training may reduce mode collapse,issue,negative,neutral,neutral,neutral,neutral,neutral
761994266,"> Another solution to this is to modify `net.load_state_dict(state_dict, strict=False)` where I added the `strict=False` option. This allows the network to load weights as long as the sizes and the number of parameters fit, even if the key-names aren't exact.

Yes, this method works. One has to make the changes in `models/base_model.py` file. 
Btw, `--no_dropout` also works. But the results are different. ",another solution modify added option network load long size number fit even exact yes method work one make file also work different,issue,positive,positive,positive,positive,positive,positive
758709095,"Hi, 

you can use this code to create a dataframe with the losses, where `path_txt` is the path to the generated log file. This skips the part in brackets though, but this should be relatively easy to fix if you are interested in that as well.

```
import pandas as pd

path_txt = '/path/to/txt'

file1 = open(path_txt, 'r') 
lines = file1.readlines()
dicts = list()
for i, line in enumerate(lines):
    if i < 2:
        continue
    parts = line.split(') ')[1].split(' ')
    parts.pop(-1)
    dict_tmp = dict()
    dict_tmp['D_A'] = float(parts[1])
    dict_tmp['G_A'] = float(parts[3])
    dict_tmp['cycle_A'] = float(parts[5])
    dict_tmp['idt_A'] = float(parts[7])
    dict_tmp['D_B'] = float(parts[9])
    dict_tmp['G_B'] = float(parts[11])
    dict_tmp['cycle_B'] = float(parts[13])
    dict_tmp['idt_B'] = float(parts[15])
    dicts.append(dict_tmp)
df = pd.DataFrame(dicts)
```",hi use code create path log file part though relatively easy fix interested well import file open list line enumerate continue float float float float float float float float,issue,positive,positive,positive,positive,positive,positive
758337214,I add 'n_epochs' and 'n_epochs_decay' to train_options.py file to address this issue. Then it started to run and not finished yet. This could be a solution for my issue?,add file address issue run finished yet could solution issue,issue,negative,neutral,neutral,neutral,neutral,neutral
757115025,"I think the problem is related to image size, if you wanna keep the image dimension intact, the width and height have to be a multiple of 4.  when you satisfied the condition use --preprocess none",think problem related image size wan na keep image dimension intact width height multiple satisfied condition use none,issue,negative,positive,neutral,neutral,positive,positive
755828424,"@phillipi,

It looks like you have forgotten to fix it, because I could not find a fix in the v3.",like forgotten fix could find fix,issue,negative,neutral,neutral,neutral,neutral,neutral
755708159,"run the code without multiprocessing flag. Most likely there was some error in the code and pool.async just buried it. Once you are sure there are no bugs, I think it should work as expected. 

That worked for me! ",run code without flag likely error code buried sure think work worked,issue,positive,positive,positive,positive,positive,positive
755031546,"> Hi @junyanz  I'm trying to load the pretrained cyclegan (horse2zebra) model as below code template,
> 
> model = XXXModel()
> model.load_state_dict(torch.load('latest_net_G.pth'))
> 
> dumb question please. which XXXModel should I use in your code base please? I'm assuming that it should be a child class of torch.nn.Module.
> 
> or could you save and share the whole model for reference please? :)
> 
> Thank you very much!
>   
>   Forget about it. Figured out from the test_model.py. :)

Can you write please how did you solve this problem? ",hi trying load model code template model dumb question please use code base please assuming child class could save share whole model reference please thank much forget figured write please solve problem,issue,positive,negative,negative,negative,negative,negative
754792227,"@drscotthawley 
thanks for the symbolic link suggestion!  I also found this flag (in bold) that can help to refer the model to run by suffix :

""Then at test time, you can load only one generator to produce the results in a single direction. This greatly saves GPU memory as you are not loading the discriminators and the other generator in the opposite direction. You can probably take the whole image as input. You can do this using --model test --dataroot [path to the directory that contains your test images (e.g., ./datasets/horse2zebra/trainA)] **--model_suffix _A** --preprocess none. You can use either --preprocess none or --preprocess scale_width --crop_size [your_desired_image_width]. Please see the model_suffix and preprocess for more details.""",thanks symbolic link suggestion also found flag bold help refer model run suffix test time load one generator produce single direction greatly memory loading generator opposite direction probably take whole image input model test path directory test none use either none please see,issue,positive,positive,positive,positive,positive,positive
754651318,"@junyanz 
I set --n_epochs to 20, but it did not stop.   How can I get it to stop training? Is there another parameter?  Also, how can I only use checkpoint 20 for testing. Thanks!",set stop get stop training another parameter also use testing thanks,issue,negative,positive,positive,positive,positive,positive
753599010,"Hello @junyanz, @taesungp, @SsnL,

Thank you very much for making this amazing tool and making it readily available.

I am also interested in using 2 or 3 images to predict a single output.
When using 3 greyscale images would you concatenate them or would you set each image as R, G or B?
When using 2 greyscale images would you concatenate them or would you set each image as R, G and set B to zero?

I guess that setting the images as RGB will reduce computational cost but I am ensure if that's okay for pix2pix?

Thank you very much for your help",hello thank much making amazing tool making readily available also interested predict single output would concatenate would set image would concatenate would set image set zero guess setting reduce computational cost ensure thank much help,issue,positive,positive,positive,positive,positive,positive
753572242,"> The BaseDataset itself is loading images into the range (-1, 1). Note the norm layer [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L109).
> 
> `ToTensor` first brings the input range to (0, 1), after which `Normalize` brings it to (-1, 1) range.

Got it. Thanks :)",loading range note norm layer first input range normalize range got thanks,issue,negative,positive,positive,positive,positive,positive
752652783,"The BaseDataset itself is loading images into the range (-1, 1). Note the norm layer [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L109). 

`ToTensor` first brings the input range to (0, 1), after which `Normalize` brings it to (-1, 1) range.",loading range note norm layer first input range normalize range,issue,negative,positive,positive,positive,positive,positive
752137948,"I know the XLA:CPU and XLA:GPU devices are no longer registered by default. But I heard that with XLA, the use of GPU is almost 100%, and much faster to finish the job. Without, it is taking much more time, with less than 40% usage.",know longer registered default use almost much faster finish job without taking much time le usage,issue,negative,positive,positive,positive,positive,positive
751520048,"> ![cuda error](https://user-images.githubusercontent.com/58145952/95647380-57672500-0ae8-11eb-834a-bca2fae00318.png)
> Hey guys!
> I am using windows 10 with only CPU and getting the error mentioned in the screenshot.
> How to fix it?
> Thank you.

hey!
were u able to figure out what the problem you had was?",error hey getting error fix thank hey able figure problem,issue,negative,positive,positive,positive,positive,positive
751346740,Mine seems to block at a specific iteration.,mine block specific iteration,issue,negative,neutral,neutral,neutral,neutral,neutral
751316561,Hi! I'm seeking to convert this model to ONNX too. I wouldn't even know where to start though.,hi seeking convert model would even know start though,issue,negative,neutral,neutral,neutral,neutral,neutral
751304253,"> Hi,
> 
> Also need to convert pix2pix to ONNX. I don't know if you still need it, but I want share it here in case anyone wants to solve this issue.
> 
> first, modify (or you can define a new function) ""def load_networks"" in models/base_model.py:
> add this to the end:
> 
> ```
>             net.eval()
>             net.cuda()  # in this example using cuda()
> 
>             """"""
>             if len(self.gpu_ids) > 0 and torch.cuda.is_available():
>                 torch.save(net.module.cpu().state_dict(), save_path)
>                 net.cuda(self.gpu_ids[0])
>             else:
>                 torch.save(net.cpu().state_dict(), save_path)
>             """"""
> 
>             batch_size = 1  
>             input_shape = (3, 512, 512)  # in my case its 512
>             export_onnx_file = load_filename[:-4]+"".onnx""  
>             save_path = os.path.join(self.save_dir, export_onnx_file)
> 
>             dinput = torch.randn(batch_size, *input_shape).cuda()   #same with net: cuda()
>             torch.onnx.export(net, dinput, save_path)
> 
>             print('The ONNX file ' + export_onnx_file + ' is saved at %s' % save_path)
> ```
> 
> Run test.py again, you will get latest_net_G.onnx in the same dir with latest_net_G.pth. Run netron on this and it is quite good.
> Again many thanks to all group members in Pix2Pix, it's really good!

Hi, is there a different workflow for cyclegan because the above doesn't work for me and I get the same error as the original poster. @jam0ss ",hi also need convert know still need want share case anyone solve issue first modify define new function add end example else case net net print file saved run get run quite good many thanks group really good hi different work get error original poster,issue,positive,positive,positive,positive,positive,positive
750929953,"i'm having the same issue as @ArchitectTaeyoon with python3.8. 
@ArchitectTaeyoon how were u able to solve/fix your error?",issue python able error,issue,negative,positive,positive,positive,positive,positive
750199595,"Hello

There is num_test option that handle the number of test images it will generate.

Best,
Dongyun


Sent from Naver Mail app.
-----Original Message-----
From: ""ZeroSpace""<notifications@github.com>
To: ""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com>
Cc: ""Dongyun Kim""<time2605@naver.com>, ""Mention""<mention@noreply.github.com>
Sent: 2020. 12. 23 AM 4:08:38
Subject: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] Missing images when testing cycleGAN (#1202)

@dongyunkim-arch hello, I have the same problem, how do you solve it, Can you tell me the detail? Thank you
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",hello option handle number test generate best sent mail message kim time mention mention sent subject missing testing hello problem solve tell detail thank reply directly view,issue,positive,positive,positive,positive,positive,positive
749505930,"Hi! I have also tried to get the similar results to the paper, and looks like it works :) For the **day2night** I have got for the val subset:
![Screenshot from 2020-12-22 14-34-39](https://user-images.githubusercontent.com/2491976/102884589-e8506a00-4462-11eb-91d9-0d7392e5455b.png)
The first scene is not very lucky, but quite frequent in the test set, so it may be the reason of unexpected results. But the rest two scenes look reasonable. For the **facades** results are similar to the paper:
![Screenshot from 2020-12-22 14-50-10](https://user-images.githubusercontent.com/2491976/102886092-88a78e00-4465-11eb-955e-8734122e6a7c.png)
And for the **sat2map** result with default input resolution (256x256) does not look good, but for the 512x512 (as outlined in the paper) result is similar:
![Screenshot from 2020-12-22 14-45-35](https://user-images.githubusercontent.com/2491976/102886458-331fb100-4466-11eb-84cf-2c436bd096da.png)

> P.S. The code is nice and easy to read, thanks @junyanz!",hi also tried get similar paper like work got subset first scene lucky quite frequent test set may reason unexpected rest two look reasonable similar paper result default input resolution look good outlined paper result similar code nice easy read thanks,issue,positive,positive,positive,positive,positive,positive
748635840,"I solved this problem.
Rename the checkpoint from the default to any other name.
See ./options/base_options, change --checkpoints_dir",problem rename default name see change,issue,negative,neutral,neutral,neutral,neutral,neutral
748625109,Mine isn't even generating any checkpoints.  Any advice?,mine even generating advice,issue,negative,neutral,neutral,neutral,neutral,neutral
747996573,"> > Hi
> > Im trying to run the day2night pretrained model but the results Im getting are not what I expected.
> > I have followed the instructions for pytorch, tried to run the facades example an it works as expected.
> > I have replaced the facades model by the day2night model and the dataroot with the proper dataset.
> > The direction is BtoA and the model pix2pix.
> > Am I doing something wrong?
> > Thanks.
> 
> hello，how did you get the day2night pretrained model, and would you like share the model with me, please. Limited by my GPU 1070ti, it dont support large size image，and training are very very slow, thanks..

Hi,

What I have done is to clone this repo and follow the instructions posted in the google colab pix2pix pytorch version.
https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb

I took the day2night pretrained model from scripts/download_pix2pix_model.sh
http://efrosgans.eecs.berkeley.edu/pix2pix/models-pytorch/

Finally I run the test instrcutions 
python3 test.py --dataroot ./datasets/day2night --direction BtoA --model pix2pix --name day2night_pix2pix
(I have tried --direction AtoB too)

Thanks for your reply.",hi trying run model getting tried run example work model model proper direction model something wrong thanks get model would like share model please limited ti dont support large size training slow thanks hi done clone follow posted version took model finally run test python direction model name tried direction thanks reply,issue,positive,negative,neutral,neutral,negative,negative
747987686,"> You may used the wrong direction if your results are roughly identical after translation. (Try A2B if that's the case)

Hi, 
I have used both directions and the results are bad. The image translated is blurred and it doesn't reconstruct correctly the input image. ",may used wrong direction roughly identical translation try case hi used bad image blurred reconstruct correctly input image,issue,negative,negative,negative,negative,negative,negative
747366401,"> Hi
> 
> Im trying to run the day2night pretrained model but the results Im getting are not what I expected.
> I have followed the instructions for pytorch, tried to run the facades example an it works as expected.
> I have replaced the facades model by the day2night model and the dataroot with the proper dataset.
> The direction is BtoA and the model pix2pix.
> Am I doing something wrong?
> 
> Thanks.

hello，how did you get the day2night pretrained model, and would you like share the model with me, please.  Limited by my GPU 1070ti, it dont support large size image，and training are very very slow,  thanks..",hi trying run model getting tried run example work model model proper direction model something wrong thanks get model would like share model please limited ti dont support large size training slow thanks,issue,positive,negative,neutral,neutral,negative,negative
747233533,"> > > Yes. It definitely affects the training.
> > 
> > 
> > Thanks for you reply.
> > In another issues, authors say that it's a small speedup trick. set_requires_grad=False will stop calculating gradients for the discriminator while we update the generator. It can save some time and memory. Generators will still get the gradients.
> > Will set_requires_grad=False only affect the training efficiency, or it will also affect the final trained model accuracy?
> 
> I think it won't hurt the performance if you calculate the gradient of discriminator when updating generator. So it only affect the training efficiency.

Thanks for your reply.",yes definitely training thanks reply another say small trick stop calculating discriminator update generator save time memory still get affect training efficiency also affect final trained model accuracy think wo hurt performance calculate gradient discriminator generator affect training efficiency thanks reply,issue,positive,positive,neutral,neutral,positive,positive
747223368,"> 
> 
> > Yes. It definitely affects the training.
> 
> Thanks for you reply.
> In another issues, authors say that it's a small speedup trick. set_requires_grad=False will stop calculating gradients for the discriminator while we update the generator. It can save some time and memory. Generators will still get the gradients.
> 
> Will set_requires_grad=False only affect the training efficiency, or it will also affect the final trained model accuracy?

I think it won't hurt the performance if you calculate the gradient of discriminator when updating generator. So it only affect the training efficiency.",yes definitely training thanks reply another say small trick stop calculating discriminator update generator save time memory still get affect training efficiency also affect final trained model accuracy think wo hurt performance calculate gradient discriminator generator affect training efficiency,issue,positive,negative,neutral,neutral,negative,negative
747218988,"> Yes. It definitely affects the training.

Thanks for you reply.
In another issues, authors say that it's a small speedup trick. set_requires_grad=False will stop calculating gradients for the discriminator while we update the generator. It can save some time and memory. Generators will still get the gradients.

Will set_requires_grad=False only affect the training efficiency, or it will also affect the final trained model accuracy?",yes definitely training thanks reply another say small trick stop calculating discriminator update generator save time memory still get affect training efficiency also affect final trained model accuracy,issue,positive,negative,neutral,neutral,negative,negative
747097718,I guess there's no much difference due to the property of convolution.,guess much difference due property convolution,issue,negative,positive,neutral,neutral,positive,positive
746973219,So the 1x1 patch analyses 3 channels and is essentially a 3x1 prediction? Do you have any idea of how the code adapts to this when the input and output are set for a single channel?,patch analysis essentially prediction idea code input output set single channel,issue,negative,negative,neutral,neutral,negative,negative
746255166,`conda install torchvision` and `conda install pytorch` solved the problem in my case,install install problem case,issue,negative,neutral,neutral,neutral,neutral,neutral
746181883,You may used the wrong direction if your results are roughly identical after translation. (Try A2B if that's the case),may used wrong direction roughly identical translation try case,issue,negative,negative,negative,negative,negative,negative
746178083,"Good question! I can not exactly answer your question, my hypothesis is about distribution of image domains. For instance, your domain A has a green tone but your domain B has a red tone. Then it's possible for discriminators to learn with even 1x1 patches.",good question exactly answer question hypothesis distribution image instance domain green tone domain red tone possible learn even,issue,negative,positive,positive,positive,positive,positive
746163952,This is due to your environment. I would suggest you try setting num_threads to 0 and reinstall some packages (follow the requirements). ,due environment would suggest try setting reinstall follow,issue,negative,negative,negative,negative,negative,negative
746159196,Chcek CUT. https://github.com/taesungp/contrastive-unpaired-translation. They modify StyleGAN generator for single image translation.,cut modify generator single image translation,issue,negative,negative,neutral,neutral,negative,negative
744422050,"Hi!
Not sure that the following line would work correctly: 

> demod.view(batch, self.out_channel, 1, 1, 1)

",hi sure following line would work correctly batch,issue,negative,positive,positive,positive,positive,positive
742377863,"> I faced the same issue. But I added ""--no_dropout"" when I tested, the issue was gone. As follows:
> python test.py --no_dropout

Amazing!   I found when I use unet_256, it is ok. The error happens when I use resnet_6blocks.  
By the way, use --no_dropout  works for me!!!",faced issue added tested issue gone python amazing found use error use way use work,issue,negative,positive,positive,positive,positive,positive
741864079,"I'm struggling with the same problem, but I'm training on a remote GPU cluster where I submit my training script to the job scheduler. Therefore, I cannot start up multiple terminals and run the Visdom server in some other terminal. Any advice how to proceed? I want to view the results locally using port forwarding, which is all set up. ",struggling problem training remote cluster submit training script job therefore start multiple run server terminal advice proceed want view locally port forwarding set,issue,negative,negative,neutral,neutral,negative,negative
740515664,"谢谢兄弟





------------------ 原始邮件 ------------------
发件人: theidealist2 <notifications@github.com&gt;
发送时间: 2020年12月8日 16:17
收件人: junyanz/pytorch-CycleGAN-and-pix2pix <pytorch-CycleGAN-and-pix2pix@noreply.github.com&gt;
抄送: Demo Xu <531232693@qq.com&gt;, Author <author@noreply.github.com&gt;
主题: 回复：[junyanz/pytorch-CycleGAN-and-pix2pix] GAN的渲染结果有一些明显的噪声 (#1204)





 
单就我的个人使用来说，cyclegan的效果一般，能产生像上面这样的结果已经很不错了，上面黄色的东西无非就是噪声而已，如果一摸一样才应该奇怪呢。
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.",author author thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
737819036,"Thank you so much, weight demodulation solved all my problems. 
In case someone wants to try it out, here's my PyTorch implementation:
```
class DemodulatedConv2d(nn.Module):
    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=0, bias=False, dilation=1):
        super().__init__()

        self.eps = 1e-8
        self.kernel_size = kernel_size
        self.in_channel = in_channel
        self.out_channel = out_channel

        self.weight = nn.Parameter(
            torch.randn(1, out_channel, in_channel, kernel_size, kernel_size)
        )
        self.bias = None
        if bias:
            self.bias = nn.Parameter(torch.randn(out_channel))

        self.stride = stride
        self.padding = padding
        self.dilation = dilation

    def forward(self, input):
        batch, in_channel, height, width = input.shape

        demod = torch.rsqrt(self.weight.pow(2).sum([2, 3, 4]) + 1e-8)
        weight = self.weight * demod.view(batch, self.out_channel, 1, 1, 1)

        weight = weight.view(
            batch * self.out_channel, in_channel, self.kernel_size, self.kernel_size
        )

        input = input.view(1, batch * in_channel, height, width)
        if self.bias is None:
            out = F.conv2d(input, weight, padding=self.padding, groups=batch, dilation=self.dilation, stride=self.stride)
        else:
            out = F.conv2d(input, weight, bias=self.bias, padding=self.padding, groups=batch, dilation=self.dilation, stride=self.stride)
        _, _, height, width = out.shape
        out = out.view(batch, self.out_channel, height, width)

        return out
```",thank much weight demodulation case someone try implementation class self super none bias stride padding dilation forward self input batch height width weight batch weight batch input batch height width none input weight else input weight height width batch height width return,issue,positive,positive,positive,positive,positive,positive
736543279,"Oh,I have replace the RandomCrop to the CenterCrop and have resolved the problem.",oh replace resolved problem,issue,negative,neutral,neutral,neutral,neutral,neutral
735273558,welp I found my silly error. I assumed when you continued training it would train for the number of epochs you told it to. Not the number of epochs minus the number of epochs already trained. I was basically telling it to train for zero epochs and confused why it stopped after it created the web directory.,found silly error assumed continued training would train number told number minus number already trained basically telling train zero confused stopped web directory,issue,negative,negative,negative,negative,negative,negative
734060829,"You can set the number of epochs by using the flag `--n_epochs 300` and `--n_epochs_decay 200`(300 epochs for initial learning rate and 200 epochs for a decaying learning rate), and there will be 500 epochs of training .",set number flag initial learning rate learning rate training,issue,negative,neutral,neutral,neutral,neutral,neutral
734031827,The code actually stops running at this point for me too. I'm using google colab,code actually running point,issue,negative,neutral,neutral,neutral,neutral,neutral
733896126,"Thanks a lot sebastian !!

On Wed, Nov 25, 2020 at 7:06 PM Sebastian Rietsch <notifications@github.com>
wrote:

> Hi, there are many follow-up works based on CycleGAN which might be better
> suited to tackle your problem, probably also the semi-supervised setting. I
> can very much recommend this repository to look up some alternative
> methods: Awesome Image Translation
> <https://github.com/weihaox/awesome-image-translation>
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1194#issuecomment-733706984>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ANNEXP4VA64JDKTAT3S6SI3SRUBFFANCNFSM4T4JLDPA>
> .
>
",thanks lot wed wrote hi many work based might better tackle problem probably also setting much recommend repository look alternative awesome image translation thread reply directly view,issue,positive,positive,positive,positive,positive,positive
733863613,I'm not sure this is an issue. Go into cycle_gan_model.py. Not sure which criterion you want that L2 regularization so I can't really give you a precise line number.,sure issue go sure criterion want regularization ca really give precise line number,issue,positive,positive,positive,positive,positive,positive
733706984,"Hi, there are many follow-up works based on CycleGAN which might be better suited to tackle your problem, probably also the semi-supervised setting. I can very much recommend this repository to look up some alternative methods: [Awesome Image Translation](https://github.com/weihaox/awesome-image-translation)",hi many work based might better tackle problem probably also setting much recommend repository look alternative awesome image translation,issue,positive,positive,positive,positive,positive,positive
730238101," 

> It is possible. I think the self-attention layer should be added to the discriminator. You don't need to change the generator. You can just take the D from self-att GANs.

@junyanz We just need to take the self-attention layer from SAGAN and put it in the discriminator and not the entire discriminator architecture right?",possible think layer added discriminator need change generator take need take layer put discriminator entire discriminator architecture right,issue,negative,positive,neutral,neutral,positive,positive
729642418,"Thank you for answering the question.
I saved the model more often, but it didn't solve the problem.",thank question saved model often solve problem,issue,positive,neutral,neutral,neutral,neutral,neutral
728681741,"Hi, 

Also need to convert pix2pix to ONNX. I don't know if you still need it, but I want share it here in case anyone wants to solve this issue. 

first, modify (or you can define a new function) ""def load_networks"" in models/base_model.py: 
add this to the end:

                net.eval()
                net.cuda()  # in this example using cuda()

                """"""
                if len(self.gpu_ids) > 0 and torch.cuda.is_available():
                    torch.save(net.module.cpu().state_dict(), save_path)
                    net.cuda(self.gpu_ids[0])
                else:
                    torch.save(net.cpu().state_dict(), save_path)
                """"""

                batch_size = 1  
                input_shape = (3, 512, 512)  # in my case its 512
                export_onnx_file = load_filename[:-4]+"".onnx""  
                save_path = os.path.join(self.save_dir, export_onnx_file)

                dinput = torch.randn(batch_size, *input_shape).cuda()   #same with net: cuda()
                torch.onnx.export(net, dinput, save_path)

                print('The ONNX file ' + export_onnx_file + ' is saved at %s' % save_path)

Run test.py again, you will get latest_net_G.onnx in the same dir with latest_net_G.pth. Run netron on this and it is quite good.
Again many thanks to all group members in Pix2Pix, it's really good!",hi also need convert know still need want share case anyone solve issue first modify define new function add end example else case net net print file saved run get run quite good many thanks group really good,issue,positive,positive,positive,positive,positive,positive
728144866,"Dear @kw01sg ,
Very thank for your help.
I have been able to use the visdom  successfully.

Many thanks.",dear thank help able use successfully many thanks,issue,positive,positive,positive,positive,positive,positive
727871042,Thank you very much! The training eventually worked! Just another quick question: after training the model can I test it in both directions right?,thank much training eventually worked another quick question training model test right,issue,negative,positive,positive,positive,positive,positive
727531140,"> custom

just use batch_size 1, do not try large batch_size at first.
As usual, split two domains to trainA and trainB.",custom use try large first usual split two,issue,negative,positive,neutral,neutral,positive,positive
727530739,"for 800x600 resolution, what should be the batch_size for that?
Also, how to train using custom dataset?",resolution also train custom,issue,negative,neutral,neutral,neutral,neutral,neutral
727524786,"See ./options/train_options, change --save_epoch_freq and --update_html_freq to some small numbers and train again. ",see change small train,issue,negative,negative,negative,negative,negative,negative
727524501,"Make sure you have created trainA,trainB inside ./datasets/datasetchd, name them properly.",make sure inside name properly,issue,negative,positive,positive,positive,positive,positive
727524335,"Epoch has 2 parts, see ./options/train_option.
parser.add_argument('--n_epochs', type=int, default=100, help='number of epochs with the initial learning rate')
parser.add_argument('--n_epochs_decay', type=int, default=100, help='number of epochs to linearly decay learning rate to zero')
So if you want to train max to 400 epoch and you want to keep the learning rate unchanged before 300. You can set n_epochs=300 and n_epochs_decay = 100.",epoch see initial learning rate linearly decay learning rate zero want train epoch want keep learning rate unchanged set,issue,negative,neutral,neutral,neutral,neutral,neutral
727524080,"If you have a 24GB memory GPU, that's possible.
or you can load your dataset in 800x600 resolution, then crop images to 600x450, 16GB should be fine.",memory possible load resolution crop fine,issue,negative,positive,positive,positive,positive,positive
727025300,"> --model pix2pix (x)
> --model test(0)

I'm not sure I understand what you are using fox x and 0 here.",model model test sure understand fox,issue,negative,positive,positive,positive,positive,positive
726580940,"Hi @Youqi970124 ,

Try pip removing visdom and conda installing the package [here](https://anaconda.org/conda-forge/visdom). 

You also might have to install a `jsonpatch` library.",hi try pip removing package also might install library,issue,negative,neutral,neutral,neutral,neutral,neutral
726333914,"After seeing a related (but not the same) Issue https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1169, I made a symbolic link:

```
$ cd checkpoints/spnet_cyclegan/
$ ln -s latest_net_G_A.pth experiment_name/latest_net_G.pth
```
then reran the test command, and it seems to have worked!  ",seeing related issue made symbolic link test command worked,issue,negative,neutral,neutral,neutral,neutral,neutral
725486405,"I assume the line you are looking for is [this](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f13aab8148bd5f15b9eb47b690496df8dadbab0c/data/base_dataset.py#L109) line?
",assume line looking line,issue,negative,neutral,neutral,neutral,neutral,neutral
725280893,"> @ taozhuang123 hi ，I got the trick， IN 'test_option.py' Line20 ,the model should set as: model='pix2pix'.....hhha~~~~

you are right! thanks!",hi got line model set right thanks,issue,negative,positive,positive,positive,positive,positive
725210994,"Here's another example of ""garbage"". 
![epoch007_fake_B_rgb copy](https://user-images.githubusercontent.com/352383/98772985-2e66d800-23a5-11eb-94b2-d53ccbe22d61.jpg)


",another example garbage copy,issue,negative,neutral,neutral,neutral,neutral,neutral
721159448,"Hello, I am experiencing the same problem. How can I solve it?
Do you mind sharing the instruction for adding checkpoints?",hello problem solve mind instruction,issue,negative,neutral,neutral,neutral,neutral,neutral
719980688,"Seems like an encoding problem. If you have Notepad++ on your Windows machine, use it to change the encoding by:
Opening the file -> Edit -> EOL Conversion -> Unix format

",like problem machine use change opening file edit conversion format,issue,negative,neutral,neutral,neutral,neutral,neutral
718273397,I found my problem. I needed to specify the --eval flag during testing as I was training with a batch_size > 1,found problem specify flag testing training,issue,negative,neutral,neutral,neutral,neutral,neutral
717993178,"Alright thanks, I got it implemented in PyTorch now and it is currently training. I'll let you know how it works in some days! Thanks again",alright thanks got currently training let know work day thanks,issue,positive,positive,positive,positive,positive,positive
717924615,"I copied convolutional layer into separate py file, changed call function and imported it as usual",copied convolutional layer separate file call function usual,issue,negative,negative,negative,negative,negative,negative
717921950,"Thanks a lot, i'll have a look. So, you redid the entire implementation in Keras?",thanks lot look entire implementation,issue,negative,positive,neutral,neutral,positive,positive
717911372,"@i-regular, I copied basic keras convolution layer and changed call function to
```
  def call(self, inputs):
    # Check if the input_shape in call() is different from that in build().
    # If they are different, recreate the _convolution_op to avoid the stateful
    # behavior.
    call_input_shape = inputs.get_shape()
    recreate_conv_op = (
        call_input_shape[1:] != self._build_conv_op_input_shape[1:])

    if recreate_conv_op:
      self._convolution_op = nn_ops.Convolution(
          call_input_shape,
          filter_shape=self.kernel.shape,
          dilation_rate=self.dilation_rate,
          strides=self.strides,
          padding=self._padding_op,
          data_format=self._conv_op_data_format)

    # Demodulation
    weights = self.kernel
    d = K.sqrt(K.sum(K.square(weights), axis=[1, 2, 3], keepdims=True) + 1e-8)
    weights = weights / d
    outputs = self._convolution_op(inputs, weights)

    if self.use_bias:
      if self.data_format == 'channels_first':
        if self.rank == 1:
          # nn.bias_add does not accept a 1D input tensor.
          bias = array_ops.reshape(self.bias, (1, self.filters, 1))
          outputs += bias
        else:
          outputs = nn.bias_add(outputs, self.bias, data_format='NCHW')
      else:
        outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')

    if self.activation is not None:
      return self.activation(outputs)
    return outputs
```",copied basic convolution layer call function call self check call different build different recreate avoid stateful behavior demodulation accept input tensor bias bias else else none return return,issue,negative,neutral,neutral,neutral,neutral,neutral
717907316,@lrunaways Thanks a lot for the suggestions. Do you by any chance have some sample code for this?,thanks lot chance sample code,issue,positive,positive,positive,positive,positive,positive
717904369,"> Did any1 ever find a good solution to this? I try to teach the model to remove specific objects from an image, so the input and output image is very similar.

Removing instance normalisation and changing all convolutions to ""demodulated"" convolutions (as said in StyleGANv2 paper in 2.2) helped in my case.",ever find good solution try teach model remove specific image input output image similar removing instance said paper case,issue,positive,positive,positive,positive,positive,positive
717885779,"Did any1 ever find a good solution to this? I try to teach the model to remove specific objects from an image, so the input and output image is very similar. ",ever find good solution try teach model remove specific image input output image similar,issue,positive,positive,positive,positive,positive,positive
717879566,"hello, sir! Have you soled this problem? I faced this as well. Does the test folder pocess some pattern?",hello sir problem faced well test folder pattern,issue,negative,neutral,neutral,neutral,neutral,neutral
717007957,"@FranciscoReveriano I found similar issues(https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/703) and the problem was sloved, so I am running code again with this solution to try whether I could go beyond epoch 39 this time.",found similar problem running code solution try whether could go beyond epoch time,issue,negative,neutral,neutral,neutral,neutral,neutral
716930387,"I have the similar problem in my practice, and I think this may because I drop the normalization in image pre-processing.",similar problem practice think may drop normalization image,issue,negative,neutral,neutral,neutral,neutral,neutral
716837380,"@AlexanderGuan  I am also running into this problem. My stops in an earlier epoch. 
Is it because the model is not able to learn more? ",also running problem epoch model able learn,issue,negative,positive,positive,positive,positive,positive
716375250,I tried several times and it stop in epoch 39 every time,tried several time stop epoch every time,issue,negative,neutral,neutral,neutral,neutral,neutral
715888655,"> Ah I see.
> 
> I tried researching a bit on google and it seems it could be due to wrong pytorch versions. Maybe one of these links can help you.
> 
> [pytorch/pytorch#43766](https://github.com/pytorch/pytorch/issues/43766)
> 
> https://discuss.pytorch.org/t/problem-with-my-checkpoint-file-when-using-torch-load/92903/3
> 
> https://stackoverflow.com/questions/63406621/something-wrong-with-my-checkpoint-file-when-using-torch-load

Normal files don't open with zip, but my files open. There seems to be a problem with training, but I don't know what's wrong...",ah see tried bit could due wrong maybe one link help normal open zip open problem training know wrong,issue,negative,negative,negative,negative,negative,negative
715362711,"Ah I see.

I tried researching a bit on google and it seems it could be due to wrong pytorch versions. Maybe one of these links can help you.

https://github.com/pytorch/pytorch/issues/43766

https://discuss.pytorch.org/t/problem-with-my-checkpoint-file-when-using-torch-load/92903/3

https://stackoverflow.com/questions/63406621/something-wrong-with-my-checkpoint-file-when-using-torch-load",ah see tried bit could due wrong maybe one link help,issue,negative,negative,negative,negative,negative,negative
715358122,"> It does looks like you are trying to test something that doesn't exist.
> 
> You either have to train your own model:
> https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#pix2pix-traintest
> 
> Or you can download a pre-trained model and dataset:
> https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix
> 
> Both of these will give you a /checkpoints/ directory with an experiment name and the ""latest_net_G.pth"" file that you are looking for and need for running test.py

I already have a trained model. If you run the test with that file, the above error appears.",like trying test something exist either train model model give directory experiment name file looking need running already trained model run test file error,issue,negative,neutral,neutral,neutral,neutral,neutral
715294221,"It does looks like you are trying to test something that doesn't exist.

You either have to train your own model:
https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#pix2pix-traintest

Or you can download a pre-trained model and dataset:
https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix

Both of these will give you a /checkpoints/ directory with an experiment name and the ""latest_net_G.pth"" file that you are looking for and need for running test.py",like trying test something exist either train model model give directory experiment name file looking need running,issue,negative,neutral,neutral,neutral,neutral,neutral
715171209,"> I assume the test.py script is set up to search for ""latest_net_G.pth"" by default so renaming it might screw with things.

If I don't change the name, the file doesn't exist.",assume script set search default might screw change name file exist,issue,negative,neutral,neutral,neutral,neutral,neutral
714822442,"I assume the test.py script is set up to search for ""latest_net_G.pth"" by default so renaming it might screw with things.",assume script set search default might screw,issue,negative,neutral,neutral,neutral,neutral,neutral
712675665,"Thx,and i wonder whether 'PatchGAN' discriminator (convnet in fact in your responsed) is applied to a 3-d model(C-H-W-L 4dim in code)still work?
if so,use conv3d() instead right?and so called'3-d PatchGAN' can discriminate the local of 3-d model which is real or fake?",wonder whether discriminator fact applied model dim code still work use instead right discriminate local model real fake,issue,negative,negative,neutral,neutral,negative,negative
711513408,I doubt it has a big effect. You could try removing it and see what happens.,doubt big effect could try removing see,issue,negative,neutral,neutral,neutral,neutral,neutral
711493348,"Hello phillipi,
i am wondering whether 'padding' is necessary in conv processing?",hello wondering whether necessary,issue,negative,neutral,neutral,neutral,neutral,neutral
711491249,"It seems that the code breaks in the following line. 
```python
index_B = random.randint(0, self.B_size - 1)
```
It indicates that self.B_size is 0. (the number of images in dataset B is 0). Have you checked if there are images in the training directory?",code following line python number checked training directory,issue,negative,neutral,neutral,neutral,neutral,neutral
711030012,"Hello，I am a new guy ,so I want to ask a stupid question: **Where can I set the epoch times** as I want , Thanks. ",new guy want ask stupid question set epoch time want thanks,issue,negative,negative,negative,negative,negative,negative
710727981,You still need to use the flag `--gpu_ids`.,still need use flag,issue,negative,neutral,neutral,neutral,neutral,neutral
710690659,"I am trying to run metrics using the original implementation of Progressive Growing of GAN by tkarras but I am not able to, has anyone any luck? or can help me with it? 

https://github.com/tkarras/progressive_growing_of_gans",trying run metric original implementation progressive growing gan able anyone luck help,issue,positive,positive,positive,positive,positive,positive
709662675,"This does not let me use other gpu its just increasing the batch size of one gpu other gpu is still empty..
I increased the batch size to 128 that is showing cuda0 out of memory no matter if i am using 1 gpu of 8 gpu. ",let use increasing batch size one still empty batch size showing memory matter,issue,negative,negative,neutral,neutral,negative,negative
709622755,"See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L42). It decides if you want to shuffle the data or not. During test time, you probably want to keep the order of the results. ",see line want shuffle data test time probably want keep order,issue,negative,neutral,neutral,neutral,neutral,neutral
709622027,LSGAN works better for CycleGAN. No big difference for pix2pix. ,work better big difference,issue,negative,positive,positive,positive,positive,positive
708148593,"Thanks for the reply! I had this issue because I switched the training set into aligned data and dramatically increase the weight for identity loss. Generally for unpaired data I think the original loss can help the model to learn sort of structure similarity even the mapping is not exactly what we want, which makes sense to me now.",thanks reply issue switched training set data dramatically increase weight identity loss generally unpaired data think original loss help model learn sort structure similarity even exactly want sense,issue,positive,positive,positive,positive,positive,positive
708142653,"> 
> 
> We haven't observed the performance gain from our experiments.

Thank you very much! So how about LSGAN loss? Is it better than the vanilla GAN loss?",performance gain thank much loss better vanilla gan loss,issue,positive,positive,positive,positive,positive,positive
708138203,It is currently not supported by the repo. Feel free to write your own script such as matplotlib script. ,currently feel free write script script,issue,positive,positive,positive,positive,positive,positive
708137407,"Meanwhile, do you think WGAN-GP loss for pix2pix or CycleGAN would yield better visual performances than the vanilla GAN loss?",meanwhile think loss would yield better visual vanilla gan loss,issue,negative,positive,positive,positive,positive,positive
708136337,"> 
> 
> It was not used in CycleGAN/pix2pix. I added it in case other people want to use it for their own development.

Thank you @junyanz for the answer. I implemented the WGAN-GP in the following way. In my [`Pix2pixGAN` class](https://github.com/zhulingchen/my_pix2pix/blob/f223e7fe3d6b57ee66ad738500be30bec14a3dfb/model.py#L190), I have the gradient penalty term calculation private method similar to your implementation:

```
def __get_gradient_penalty_loss(self, real, fake, constant=1.0):
    batch_size = real.shape[0]
    alpha = torch.rand(batch_size, 1, 1, 1)
    alpha = alpha.expand_as(real).to(self.device)
    interpolated = alpha * real + (1 - alpha) * fake
    interpolated.requires_grad_(True)
    dummy = torch.empty(batch_size, 0, self.config['image_rows'], self.config['image_cols']).to(self.device)  # to fit the discriminator input argument list
    disc_interpolated = self.discriminator(interpolated, dummy)
    grad_interpolated = torch.autograd.grad(outputs=disc_interpolated, inputs=interpolated,
                                            grad_outputs = torch.ones_like(disc_interpolated),
                                            create_graph = True, retain_graph = True, only_inputs = True)[0]
    grad_interpolated = grad_interpolated.view(batch_size, -1)  # flat the data
    grad_norm = torch.sqrt(torch.sum(grad_interpolated ** 2, dim=1) + 1e-16)
    return torch.mean((grad_norm - constant) ** 2)
```

Then, I calculate the gradient penalty term `loss_gp` in the following way:
```
loss_gp = self.config['lambda_gp'] * self.__get_gradient_penalty_loss(real=torch.cat([real_src, real_tgt], dim=1), 
                                                                      fake=torch.cat([real_src, fake_tgt.detach()], dim=1))
```

The point is, I think in pix2pix implementation using WGAN-GP loss with the ""mixed"" setting, the ""real data"" should be `torch.cat([real_src, real_tgt], dim=1)` and the ""fake data"" should be `torch.cat([real_src, fake_tgt.detach()], dim=1)`.

Do you think I am doing the correct way?

Thanks!",used added case people want use development thank answer following way class gradient penalty term calculation private method similar implementation self real fake alpha alpha real alpha real alpha fake true dummy fit discriminator input argument list dummy true true true flat data return constant calculate gradient penalty term following way point think implementation loss mixed setting real data fake data think correct way thanks,issue,negative,positive,neutral,neutral,positive,positive
708134955,It was not used in CycleGAN/pix2pix. I added it in case other people want to use it for their own development. ,used added case people want use development,issue,negative,neutral,neutral,neutral,neutral,neutral
706747033,I found GP term has not been used for CycleGAN either.,found term used either,issue,negative,neutral,neutral,neutral,neutral,neutral
706690374,"Yeah, at first i trained the model with Visdom server turned on, but then my model gets stuck, therefore i turned it off, now after training i want to visualize the loss plots. Looking forward for a way to display the loss plots after training is completed.

________________________________
From: Jun-Yan Zhu <notifications@github.com>
Sent: Saturday, October 10, 2020 10:57 PM
To: junyanz/pytorch-CycleGAN-and-pix2pix <pytorch-CycleGAN-and-pix2pix@noreply.github.com>
Cc: Maria-Siddiqua <mariasiddiqua@hotmail.com>; Author <author@noreply.github.com>
Subject: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to plot losses after training is complete ? (#1161)


Could you try to start the visdom server before the training?

python -m visdom.server

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1161#issuecomment-706587940>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ANTU64FEOH7UFKSYC7C7B33SKCOCBANCNFSM4SJ3VU3Q>.
",yeah first trained model server turned model stuck therefore turned training want visualize loss looking forward way display loss training sent author author subject plot training complete could try start server training python thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
706588207,Thank you so much for your suggestion let me go through on pytorch repo.,thank much suggestion let go,issue,negative,positive,positive,positive,positive,positive
706587940,"Could you try to start the visdom server before the training? 
```bash
python -m visdom.server
```",could try start server training bash python,issue,negative,neutral,neutral,neutral,neutral,neutral
706587269,It should work with `train.py` script. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/2595573a7c9fd5bb389a949a3f1b2f06970a4eb8/options/train_options.py#L27). Are you using the latest version?,work script see line latest version,issue,negative,positive,positive,positive,positive,positive
706587132,Not sure. It seems to be related to PyTorch. You could post/find the issue on the PyTorch repo. ,sure related could issue,issue,negative,positive,positive,positive,positive,positive
706494736,"![cuda error](https://user-images.githubusercontent.com/58145952/95647380-57672500-0ae8-11eb-834a-bca2fae00318.png)
Hey guys!
I am using windows 10 with only CPU and getting the error mentioned in the screenshot.
How to fix it?
Thank you.",error hey getting error fix thank,issue,negative,neutral,neutral,neutral,neutral,neutral
705239219,You can use the flag `--model test`. See more details [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix). ,use flag model test see,issue,negative,neutral,neutral,neutral,neutral,neutral
705085356,"> Hello @kalai2033 , I was wondering if you figured this out and if you would share the solution?

I just printed the network and formed the table manually from the summary of the network.",hello wondering figured would share solution printed network formed table manually summary network,issue,positive,neutral,neutral,neutral,neutral,neutral
705080354,"Hello @kalai2033 , I was wondering if you figured this out and if you would share the solution? ",hello wondering figured would share solution,issue,positive,neutral,neutral,neutral,neutral,neutral
704733823,"From my experience, you probably need to train it for more epochs. ",experience probably need train,issue,negative,neutral,neutral,neutral,neutral,neutral
704733123,"G_A is G(A->B). If input is already B, B->B should keep B unchanged. That's the purpose of identity loss.
For your understanding, it seems make sense. But remember we are using unpaired data, A->G_A(A) has no corresponding B.
Hope my reply helps.",input already keep unchanged purpose identity loss understanding make sense remember unpaired data corresponding hope reply,issue,negative,neutral,neutral,neutral,neutral,neutral
704668355,"> It should be [0, 255].
Thanks for your help. Will close this issue for now. ",thanks help close issue,issue,positive,positive,positive,positive,positive,positive
704283812,"> You can add your code [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/util.py#L49).

Hi Junyan, 

Thank you so much for help. I actually have a follow up question about this. 

Before saving the images to disk, we call a function `util.tensor2im(im_data) at here, does this function aim to re-normalize these out images to [0, 255], as coming out from Tensor? 

https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/c94161791aaf050dfffb4f2b2fac815997b37aa6/util/visualizer.py#L35-L43",add code hi thank much help actually follow question saving disk call function function aim coming tensor,issue,positive,positive,neutral,neutral,positive,positive
703892195,Not sure what happened. Not sure if the default generator and discriminator will work for 16x16 patches. It was designed for 256x256 or 360x360 images. ,sure sure default generator discriminator work designed,issue,positive,positive,positive,positive,positive,positive
703311070,"The resolution during training and test should be the same. If you want to produce 720x640 results during test time, you can crop 360x360 patches from 720x640 training images during training. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images) for more details. ",resolution training test want produce test time crop training training see,issue,negative,neutral,neutral,neutral,neutral,neutral
703309877,"I am not sure if your artifacts are caused by mode collapse. From the loss curve, the loss will become extremely small or large when mode collapse happens. ",sure mode collapse loss curve loss become extremely small large mode collapse,issue,negative,positive,positive,positive,positive,positive
703309338,"Yes, please use `--model test`. See more details [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix).",yes please use model test see,issue,positive,neutral,neutral,neutral,neutral,neutral
701053001,"@HeoJinLareine 

Can't it be achieved with the following code?

===Comand Line===

pip install coremltools==4.0b3

===Python Code====
import coremltools from coremltools.proto import FeatureTypes_pb2 as ft 
spec = coremltools.utils.load_spec('./CycleGAN.mlmodel')

builder = coremltools.models.neural_network.NeuralNetworkBuilder(spec=spec) 

rgb = ft.ImageFeatureType.ColorSpace.Value('RGB') 
input_image_type = builder.spec.description.input[0].type.imageType
input_image_type.width = 256 input_image_type.height = 256 input_image_type.colorSpace = rgb

output_image_type=builder.spec.description.output[0].type.imageType
output_image_type.width = 256 output_image_type.height = 256 output_image_type.colorSpace = rgb

builder.inspect_input_features() 
builder.inspect_output_features() 

mlmodel_modified = coremltools.models.MLModel(spec) 
mlmodel_modified.save('./NewCycleGAN.mlmodel') 
",ca following code pip install import import spec builder spec,issue,negative,neutral,neutral,neutral,neutral,neutral
700126859,"I got same problem, too. In my case, I trained pix2pix on Colab and just changed some parameters on the config file and the training process stuck at the 15th epoch. ",got problem case trained file training process stuck th epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
699725015,"> Is there a problem with the way you write loss_d? Loss_d = (loss_D_real + loss_D_fake) * 0.5， 0.5 is your hyperparameter, but should it be in this form: loss_d = (loss_d_fake-loss_d_real) * 0.5？

No need to change. The method self.criterionGAN has taken the sign into account.",problem way write form need change method taken sign account,issue,negative,neutral,neutral,neutral,neutral,neutral
696904025,0 might means that D is overfitting the training set and winning the optimization game. 0.1-0.3 is a normal range. ,might training set winning optimization game normal range,issue,positive,positive,neutral,neutral,positive,positive
696689329,"while when i train D_real and D_fake convergence 0，is this mean training model collapse? normal D_real and D_fake will convergence 0.5, which can not distinguish real or fake.",train convergence mean training model collapse normal convergence distinguish real fake,issue,negative,negative,negative,negative,negative,negative
696476744,"Several people reported about this issue, but we do not know yet why this happens. If anyone could dig into the problem, we'd very appreciate it! Sorry about that. ",several people issue know yet anyone could dig problem appreciate sorry,issue,negative,negative,negative,negative,negative,negative
696473985,"same problem on my own dataset, all images are valid. I think it may be the problem with dataloader.",problem valid think may problem,issue,negative,neutral,neutral,neutral,neutral,neutral
696248796,"I was expecting the model to have less weights since my images are smaller (1/16 in number of pixels).

Thanks for the clarification!",model le since smaller number thanks clarification,issue,negative,positive,neutral,neutral,positive,positive
696240906,The number of parameters in the first 1-2 downsampling layers is small compared to the parameters of the entire network. Removing the first 1-2 layers will not reduce the number of parameters by 1/16.,number first small entire network removing first reduce number,issue,negative,positive,neutral,neutral,positive,positive
696239625,The `colorization_dataset` takes a directory of RGB images. You don't need to create pairs by yourself. It can automatically create L->AB pairs. ,directory need create automatically create,issue,negative,neutral,neutral,neutral,neutral,neutral
693153856,You can apply CycleGAN to paired images. But I think pix2pix or more recent models such as GauGAN should work better. ,apply paired think recent work better,issue,negative,positive,positive,positive,positive,positive
692310552,"Thanks for your reply! Yes, we don't need to add this flag if using the PyTorch before 1.6. But for users who train and test models using google colab (which I assume would update the python environment regularly), who may need to modify it to save the model as a binary file rather than the compressed file. ",thanks reply yes need add flag train test assume would update python environment regularly may need modify save model binary file rather compressed file,issue,positive,positive,neutral,neutral,positive,positive
692019047,Pix2pix has a L1 loss. I think it's better to align images.,loss think better align,issue,negative,positive,positive,positive,positive,positive
691739156,"> My guess is that the code is dependent on having all values between 0 and 1. Maybe it screws with the loss values, weight updates, or something else.
> 
> If the original code works for you it's probably safer to just scale up the images to 0 to 100 after the output image is produced, i.e through using a .py file on an output image collection, or find the code where output image is written to file and scale up there.

Thank you Johan, I will try that.",guess code dependent maybe loss weight something else original code work probably scale output image produced file output image collection find code output image written file scale thank try,issue,negative,positive,positive,positive,positive,positive
691621724,"`epoch_count` is mainly used for ""continue training"".  It allows users to continue the model training from a particular epoch `epoch_count`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#fine-tuningresume-training) for more details. ",mainly used continue training continue model training particular epoch see,issue,negative,positive,positive,positive,positive,positive
691620816,"We noticed the [change](https://github.com/pytorch/pytorch/pull/40394/files). If we add this flag, it might not work for PyTorch before 1.6. I guess it should be fine if you use the same PyTorch version for training and test? torch.load can still zipfile format? @taesungp ",change add flag might work guess fine use version training test still format,issue,negative,positive,positive,positive,positive,positive
691620112,Slight misalignment should be fine. But you are right that better alignment can lead to better results. ,slight misalignment fine right better alignment lead better,issue,positive,positive,positive,positive,positive,positive
691620062,"There is no optimal training epochs/iterations for GANs training (or for deep learning models in general). The number depends on your dataset size, data type, and application. Authors often use the same epochs/iterations for a relatively fair comparison. You can save the checkpoints frequently and evaluate all the saved checkpoints afterward. ",optimal training training deep learning general number size data type application often use relatively fair comparison save frequently evaluate saved afterward,issue,positive,positive,positive,positive,positive,positive
691619760,"Unfortunately, U-Net only supports image sizes with a multiple of 256. If you train the model with `resnet_9blocks`, it can support image sizes with a multiple of 4.",unfortunately image size multiple train model support image size multiple,issue,negative,negative,negative,negative,negative,negative
691553461,Both `--model pix2pix` and `--model test` will work. `--eval` is not recommended. ,model model test work,issue,negative,neutral,neutral,neutral,neutral,neutral
691478936,"My guess is that the code is dependent on having all values between 0 and 1. Maybe it screws with the loss values, weight updates, or something else.

If the original code works for you it's probably safer to just scale up the images to 0 to 100 after the output image is produced, i.e through using a .py file on an output image collection, or find the code where output image is written to file and scale up there.",guess code dependent maybe loss weight something else original code work probably scale output image produced file output image collection find code output image written file scale,issue,negative,positive,positive,positive,positive,positive
691271198,Got it. I don't have one for pix2pix. Feel free to train one. We also update this repo quite frequently and PyTorch version changes. The pre-trained model was trained with an earlier version of this repo. ,got one feel free train one also update quite frequently version model trained version,issue,positive,positive,positive,positive,positive,positive
690939423,"Hi @junyanz ,

Thanks for your response.
I think I was not clear in my question. What I meant was the pix2pix PyTorch model trained on Cityscapes. The script you provided seems to be for CycleGAN, and in `download_pix2pix_model.sh` I could not find the model for Cityscapes.
",hi thanks response think clear question meant model trained script provided could find model,issue,positive,positive,positive,positive,positive,positive
690932427,"We do provide a pre-trained PyTorch model on cityscapes. But the model is not the same model as the Lua model used in the paper. You can download the model using the following script. 
```bash
bash ./scripts/download_cyclegan_model.sh cityscapes_label2photo

```",provide model model model model used paper model following script bash bash,issue,negative,neutral,neutral,neutral,neutral,neutral
690930524,"Resnet-based generators are more memory-intensive as it has fewer downsampling layers and the bottleneck is bigger. It keeps the resolution of the tensor after 3 downsampling layers. For U-net, it has more downsampling layers (6 or 7), and the resolution of the tensor becomes quite small. ",bottleneck bigger resolution tensor resolution tensor becomes quite small,issue,negative,negative,negative,negative,negative,negative
690083929,"Hi,

I am interested in generating images in the Cityscapes dataset. As far as I know, there is not PyTorch pre-trained model on Cityscapes, and the only pre-trained model on Cityscapes is the one in the Lua repo. So I have to train the Pytorch model from scratch.

Am I right? If not, could you please refer me to the Pytorch pre-trained model on Cityscapes?
Thanks in advance.",hi interested generating far know model model one train model scratch right could please refer model thanks advance,issue,positive,positive,positive,positive,positive,positive
688893039,"@ibro45 sorry for the late reply. I got it to work using the following approaches:
1. Crop the MR scans such that it covers the same area as that of CT (the torso)
2. Make sure to centre the data (e.g x - x.mean) over the current volume of scan, followed by a normalisation of your choice (e.g division by x.std() should be fine). Before I used (x - x.min) / (x.max - x.min) and this did not work very well.
3. The images are too big, so I used a patch-based approach using 256x256 patches and then reconstruct the big scan accordingly. 

The implementation is here: https://github.com/momenator/CycleGAN if you are interested.
",sorry late reply got work following crop area torso make sure data current volume scan choice division fine used work well big used approach reconstruct big scan accordingly implementation interested,issue,positive,positive,neutral,neutral,positive,positive
688063853,"x is relevant and can be used to check if the output and input condition match or not. For example, in the task of the semantic layout (e.g., tree labels, road labels) -> image in the Cityscapes, if the discriminator knows that the generator wants to synthesize a tree region (from x), the discriminator can determine whether it looks like a real or fake tree region. ",relevant used check output input condition match example task semantic layout tree road image discriminator generator synthesize tree region discriminator determine whether like real fake tree region,issue,negative,negative,negative,negative,negative,negative
687887157,You can use the `--model test`.  See the [README](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) for more details. You need to manually rename the model file. ,use model test see need manually rename model file,issue,negative,neutral,neutral,neutral,neutral,neutral
687886736,"I hypothesize that the ResNet has fewer parameters and fewer downsampling, which are both good for color and style transfer, while U-Net has many more parameters (hard to learn these parameters without paired data) and has lots of downsampling layers. But this is just my speculation. ",hypothesize good color style transfer many hard learn without paired data lot speculation,issue,negative,positive,positive,positive,positive,positive
687865469,"Implicitly yes due to the convolutional operation, although we don't explicitly do it. ",implicitly yes due convolutional operation although explicitly,issue,negative,negative,negative,negative,negative,negative
687285787,"Hi, as the discriminator outputs 30x30x1 matrix, does that mean the 70x70 patch was moved over the input image 30 times in each direction (horizontal and vertical) to map to single output for all of them?",hi discriminator matrix mean patch input image time direction horizontal vertical map single output,issue,negative,negative,negative,negative,negative,negative
687002386,"@junyanz, As you mentioned , ""For CycleGAN, Resnet-based generators often work much better than UNet."", is there any specific reason because i tried with UNet and burnt my gpu for 200 epochs to find no good result. ",often work much better specific reason tried burnt find good result,issue,positive,positive,positive,positive,positive,positive
686275491,@poppopting Thank you so much !. I'll try it and see how it goes,thank much try see go,issue,negative,positive,positive,positive,positive,positive
686259513,"@superkido511  In my case, I made a mistake in the below part and got both weak generator :
```
#correct version
loss_D_real = self.criterionGAN(pred_real, True)
loss_D_fake = self.criterionGAN(pred_fake, False)
```
But I think you only have a problem with generator A (it seems generator B is good),  your situation may differ from mine.
Besides, I found there is a repo fitting CycleGan with manga colorization dataset: [link](https://github.com/OValery16/Manga-colorization---cycle-gan), hope it can help you.",case made mistake part got weak generator correct version true false think problem generator generator good situation may differ mine besides found fitting manga colorization link hope help,issue,positive,positive,positive,positive,positive,positive
686235599,"@poppopting Hello, I didn't use pretrained model. By ""bad"", I mean some part of the image are colored, but most of the time, it's the wrong color. I also noticed that after about 100, the discriminator loss of domain A(classify uncolored images) are nearly 0 (0.001 ~ 0.002) most of the time and occasionally go up (0.1 ~ 0.2). Here are my training result:
![image](https://user-images.githubusercontent.com/26477048/92068412-079e8b00-edd1-11ea-9b77-3b6e9fe6c7b1.png)
You can see that the reconstructed images look very close to the real images, while the generated images are quite bad. I tried to lower cycle loss weight, make generator more powerful, use separate learning rates, optimizers for each networks. Nothing seems to work so far",hello use model bad mean part image colored time wrong color also discriminator loss domain uncolored nearly time occasionally go training result image see reconstructed look close real quite bad tried lower cycle loss weight make generator powerful use separate learning nothing work far,issue,negative,negative,negative,negative,negative,negative
686226859,"@superkido511  yes, I have solved the problems. But in my case, there are some typos in the Discriminator written by me and the problem was solved after I corrected them. 
But in your case, you used pre-trained model, maybe the problems are just in your parameter setting. Does the ""bad"" in your generated images is they are hardly colored? or the images are painted in the wrong color?
",yes case discriminator written problem corrected case used model maybe parameter setting bad hardly colored painted wrong color,issue,negative,negative,negative,negative,negative,negative
684768214,"Thank you very much for your advice, I benefited a lot from the excellent project. It's really great.
",thank much advice lot excellent project really great,issue,positive,positive,positive,positive,positive,positive
683686970,"> @NeilDG how did it go? I encountered the same problem. Thanks!

@ibro45 I realized my image domain and methodology is not suitable for the vanilla CycleGAN. I've discontinued my experiments regarding this. But so far I did the following that somewhat fixed the solution (but produced blurry images):

- Check if you're correctly passing the images in the cycle-consistency loss. See their paper. I have mistakenly mixed images A and B during training. This might be the primary issue.
- Identity loss only changes the color of my images, but not necessarily enhancing them, which is what I want. Reducing this created blurry images.
- Training on randomly cropped patches as suggested above greatly helps rather than training them as a whole. I've started using this approach on my future GANs.",go problem thanks image domain methodology suitable vanilla regarding far following somewhat fixed solution produced blurry check correctly passing loss see paper mistakenly mixed training might primary issue identity loss color necessarily want reducing blurry training randomly greatly rather training whole approach future,issue,negative,positive,positive,positive,positive,positive
682457245,"> I faced the same issue. But I added ""--no_dropout"" when I tested, the issue was gone. As follows:
> python test.py --no_dropout

Thank you @SunLeL ,it works for me!!!!",faced issue added tested issue gone python thank work,issue,negative,neutral,neutral,neutral,neutral,neutral
682359910,"Tanh() is not affected by your input range. If your output is [0, 1], you can either pre-process your input to [-1, 1], or change the TanH to Sigmoid. ",tanh affected input range output either input change tanh sigmoid,issue,negative,neutral,neutral,neutral,neutral,neutral
681835907,"Thank you very much for your reply.  I am also puzzled about the tanh() in the last layer of the generator. If  the input need to be in [0,1] , is it also necessary to use tanh() in the last  layer. ",thank much reply also puzzled tanh last layer generator input need also necessary use tanh last layer,issue,negative,positive,neutral,neutral,positive,positive
681834245,Thank you very much for your help.  I will try the reduced D in the project .,thank much help try reduced project,issue,positive,positive,positive,positive,positive,positive
681074575,"You can use [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) or [SPADE](https://github.com/NVlabs/SPADE) for high-resolution image synthesis. If you want to use pix2pix, you can try to train a model on 256x256 crops rather than original images. See this [Tip](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images).",use spade image synthesis want use try train model rather original see tip,issue,negative,positive,positive,positive,positive,positive
680792288,"The input is exactly the same, here I show you the configuration in both tests:

Single dataset mode:

python test.py --dataroot ./datasets/single_test --name pix2pix_lidar --model test --input_nc 1 --output_nc 1 --netD pixel --preprocess none --epoch 200 --netG unet_256 --norm batch --dataset_mode single
----------------- Options ---------------
             aspect_ratio: 1.0                           
               batch_size: 1                             
          checkpoints_dir: ./checkpoints                 
                crop_size: 256                           
                 dataroot: ./datasets/single_test        	[default: None]
             dataset_mode: single                        
                direction: AtoB                          
          display_winsize: 256                           
                    epoch: 200                           	[default: latest]
                     eval: False                         
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 1                             	[default: 3]
                  isTrain: False                         	[default: None]
                load_iter: 0                             	[default: 0]
                load_size: 256                           
         max_dataset_size: inf                           
                    model: test                          
             model_suffix:                               
               n_layers_D: 3                             
                     name: pix2pix_lidar                 	[default: experiment_name]
                      ndf: 64                            
                     netD: pixel                         	[default: basic]
                     netG: unet_256                      	[default: resnet_9blocks]
                      ngf: 64                            
               no_dropout: False                         
                  no_flip: False                         
                     norm: batch                         	[default: instance]
                 num_test: 50                            
              num_threads: 4                             
                output_nc: 1                             	[default: 3]
                    phase: test                          
               preprocess: none                          	[default: resize_and_crop]
              results_dir: ./results/                    
           serial_batches: False                         
                   suffix:                               
                  verbose: False                         
----------------- End -------------------
dataset [SingleDataset] was created
initialize network with normal
model [TestModel] was created
loading the model from ./checkpoints/pix2pix_lidar/200_net_G.pth
---------- Networks initialized -------------
[Network G] Total number of parameters : 54.408 M

creating web directory ./results/pix2pix_lidar/test_200
processing (0000)-th image... ['./datasets/single_test/test/1.png']
input:
![image](https://user-images.githubusercontent.com/61839677/91290386-d51edd80-e793-11ea-84bc-47a176f84f48.png)

output:
![image](https://user-images.githubusercontent.com/61839677/91290276-b6b8e200-e793-11ea-8b0e-af9be7b9ce37.png)

Aligned dataset mode

python test.py --dataroot ./datasets/256x256_pc6blanco --name pix2pix_lidar --model pix2pix --input_nc 1 --output_nc 1 --netD pixel --preprocess none --epoch 200 --netG unet_256 --norm batch 
----------------- Options ---------------
             aspect_ratio: 1.0                           
               batch_size: 1                             
          checkpoints_dir: ./checkpoints                 
                crop_size: 256                           
                 dataroot: ./datasets/256x256_pc6blanco  	[default: None]
             dataset_mode: aligned                       
                direction: AtoB                          
          display_winsize: 256                           
                    epoch: 200                           	[default: latest]
                     eval: False                         
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 1                             	[default: 3]
                  isTrain: False                         	[default: None]
                load_iter: 0                             	[default: 0]
                load_size: 256                           
         max_dataset_size: inf                           
                    model: pix2pix                       	[default: test]
               n_layers_D: 3                             
                     name: pix2pix_lidar                 	[default: experiment_name]
                      ndf: 64                            
                     netD: pixel                         	[default: basic]
                     netG: unet_256                      
                      ngf: 64                            
               no_dropout: False                         
                  no_flip: False                         
                     norm: batch                         
                 num_test: 50                            
              num_threads: 4                             
                output_nc: 1                             	[default: 3]
                    phase: test                          
               preprocess: none                          	[default: resize_and_crop]
              results_dir: ./results/                    
           serial_batches: False                         
                   suffix:                               
                  verbose: False                         
----------------- End -------------------
dataset [AlignedDataset] was created
initialize network with normal
model [Pix2PixModel] was created
loading the model from ./checkpoints/pix2pix_lidar/200_net_G.pth
---------- Networks initialized -------------
[Network G] Total number of parameters : 54.408 M

creating web directory ./results/pix2pix_lidar/test_200
processing (0000)-th image... ['./datasets/256x256_pc6blanco/test/1.png']

input:
![image](https://user-images.githubusercontent.com/61839677/91290724-4494cd00-e794-11ea-9e8e-ae0cf801a049.png)
(I fill the folderB with black images, and before i build the dataset to the pix2pic with ""combine_A_and_B.py"")
output:
![image](https://user-images.githubusercontent.com/61839677/91291422-53c84a80-e795-11ea-8515-24a4a5458ebb.png)

I hope you can help me, Thanks!",input exactly show configuration single mode python name model test none epoch norm batch single default none single direction epoch default latest false normal default false default none default model test name default default basic default false false norm batch default instance default phase test none default false suffix verbose false end initialize network normal model loading model network total number web directory image input image output image mode python name model none epoch norm batch default none direction epoch default latest false normal default false default none default model default test name default default basic false false norm batch default phase test none default false suffix verbose false end initialize network normal model loading model network total number web directory image input image fill black build output image hope help thanks,issue,positive,negative,negative,negative,negative,negative
680779442,"thanks you for the answer) it is possible to use pix2pix with a resolution of 512x512? I already tried, but I got bad results, even though I changed the --ngf and --ndf parameters. With a resolution of 256x256 works well!

",thanks answer possible use resolution already tried got bad even though resolution work well,issue,negative,negative,negative,negative,negative,negative
680675447,"We did a grayscale2RGB in the code. If you don't want it, you can modify this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/util.py#L22).",code want modify line,issue,negative,neutral,neutral,neutral,neutral,neutral
680674595,"This is interesting. If you use Dropout, pix2pix might produce stochastic results. But the different should not be that different. I am wondering if can check if the input to the network is exactly the same or not. ",interesting use dropout might produce stochastic different different wondering check input network exactly,issue,negative,positive,positive,positive,positive,positive
680667789,Not sure what happened. These two training command lines should create the same training instance. The arguments used in the command line are the same as default arguments. The results might be different across different runs in your application. You can double-check it by looking at the printed arguments on the console. ,sure two training command create training instance used command line default might different across different application looking printed console,issue,positive,positive,positive,positive,positive,positive
680625521,You can also try a reduced D. But sometimes a reduced D cannot capture the difference between real and fake samples. ,also try reduced sometimes reduced capture difference real fake,issue,negative,negative,negative,negative,negative,negative
680571089,"Using pix2pix is possible. But you need to collect many pairs of input face and transformed face, which might be challenging in pratice. ",possible need collect many input face face might,issue,negative,positive,positive,positive,positive,positive
680568467,It seems that instancenorm works quite well for style transfer tasks in general.  Figure 1 in [AdaIN](https://arxiv.org/pdf/1703.06868.pdf) has provided a nice explanation. ,work quite well style transfer general figure provided nice explanation,issue,positive,positive,positive,positive,positive,positive
680466126,"> When batch_size = 1, it's instance_normalization. When batch_size > 1, it's batch_normalization. instance_normalization is better than batch_normalization for image2image transfer.

Why is istance_normalization better than batch_normalization for image2image transfer?",better transfer better transfer,issue,positive,positive,positive,positive,positive,positive
680287213,"@momenator it's been a while, but could you tell if you were able to solve this issue and how? Thanks!",could tell able solve issue thanks,issue,positive,positive,positive,positive,positive,positive
679822692,"Thanks a lot. I added it to ""Getting Started"".",thanks lot added getting,issue,negative,positive,positive,positive,positive,positive
678831363,"> They don't have to be square images. During training time, you can crop square patches using the flag `--preprocess crop`. During test time, you can apply the model to images with arbitrary sizes (rectangle or not) using the flag `--preprocess none`.

Hi @junyanz. I'm following these steps for my tests with pix2pix. I'm using `--preprocess crop` during training with 256x256 crops, and `--preprocess none` for testing 432x1072 images. However, I'm getting the following error for the tests:

`RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 2 and 3 in dimension 3 at`

Do you have any idea where the problem might be?
Thank you.",square training time crop square flag crop test time apply model arbitrary size rectangle flag none hi following crop training none testing however getting following error invalid argument size must match except dimension got dimension idea problem might thank,issue,negative,negative,neutral,neutral,negative,negative
677458456,"> You can consider using a G with a higher capacity.

Hello, Professor, Thank you for your reply.  I am puzzled about this issue. Why the loss of D drop very fast , we should using a G with higher capacity ?   maybe  because  the result of G is bad ?  Is it  possible to use a reduced Discriminator?",consider higher capacity hello professor thank reply puzzled issue loss drop fast higher capacity maybe result bad possible use reduced discriminator,issue,negative,positive,neutral,neutral,positive,positive
675489004,"Thank you very much for your excellent project. I am puzzled about this point. Usually , we always want the D learn faster than the G,   why slow down the rate here ?  if there art three items in the D_loss ,such as fake_loss item 1, false_item 2 and the true item,  what should we do for the D in this situation ?   * 0.3  or 0.5 ?  Looking forward  your help.",thank much excellent project puzzled point usually always want learn faster slow rate art three item true item situation looking forward help,issue,positive,positive,positive,positive,positive,positive
674352662,"@diaosiji I am getting same issue with selfie2anime dataset and deleting dstore did not solve problem
https://www.kaggle.com/arnaud58/selfie2anime",getting issue solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
671875684,"Sorry for a late response, I've only tested it on our private dataset, need help to evaluate it on Celeba",sorry late response tested private need help evaluate,issue,negative,negative,negative,negative,negative,negative
671694115,"@poppopting Did you managed to solve the issue? I'm facing the same issue. I'm trying to use cycle gan for manga colorization (without coverting color spaces). My dataset contain 1000 color images and 1000 uncolor images. Generated images are very bad, while reconstructed images are very good. I used the original code base. Here is my training opts:
 python3 train.py --dataroot ./datasets/unprocessed --name manga_cyclegan4 --model cycle_gan --no_dropout --loadSize 512 --fineSize 256 --lr_decay_iters 100 --niter_decay 200 --save_epoch_freq 30 --display_freq 1000 --which_model_netD n_layers --display_id 0 --resize_or_crop scale_width_and_crop --lambda_identity 0.1
Can you help me please prof @junyanz ?",solve issue facing issue trying use cycle gan manga colorization without color contain color bad reconstructed good used original code base training python name model help please prof,issue,positive,negative,negative,negative,negative,negative
671381510,"You probably also need to change `--n_epochs` and `--n_epochs_decay`. (e.g., `--n_epochs 200 --n_epochs_decay 200 --epoch_count 200 --continue_train`.)   See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L30).",probably also need change see line,issue,negative,neutral,neutral,neutral,neutral,neutral
671379791,Hard to know when to stop training ML models. I guess until you get reasonable results. ,hard know stop training guess get reasonable,issue,negative,negative,neutral,neutral,negative,negative
671187133,"Thank you very much for your help, I learn from this paper. lsgan is used in my project like the pix2pix.  I am also puzzled when I should stop the training process.  ",thank much help learn paper used project like also puzzled stop training process,issue,positive,positive,positive,positive,positive,positive
671161668,0.25 D loss looks normal to me. Some recent [work](https://arxiv.org/pdf/2008.00951.pdf) used a pre-trained network as part of image-to-image translation networks. You may want to have a look. ,loss normal recent work used network part translation may want look,issue,negative,positive,neutral,neutral,positive,positive
671060119,"> from the error message, you got a tensor with shape 1x512x1x1. This will break InstanceNorm as both H and W =1.

yes, thank you for your help~~",error message got tensor shape break yes thank,issue,negative,neutral,neutral,neutral,neutral,neutral
671025279,"Yes, the code assumes that input and output images have the same size. ",yes code input output size,issue,negative,neutral,neutral,neutral,neutral,neutral
671025136,"from the error message, you got a tensor with shape 1x512x1x1. This will break InstanceNorm as both H and W =1. ",error message got tensor shape break,issue,negative,neutral,neutral,neutral,neutral,neutral
671007354,"I'am still confused about my assumption above. In the train stage, I also use instance normalization not BN. When I set batch_size >1, it can be trained well. But the point is IN has nothing to do with B and C dimensions, right? That is to say, no matter the B =1 or B>1, it would not affect IN, so why when I set batch_size>1, this problem would disappear? doesn't make sense.",still confused assumption train stage also use instance normalization set trained well point nothing right say matter would affect set problem would disappear make sense,issue,negative,negative,neutral,neutral,negative,negative
671006660,"Yes sir , i had checked AB/train and AB/test.
Is resolution mismatch causing the problem of paired images??",yes sir checked resolution mismatch causing problem paired,issue,negative,neutral,neutral,neutral,neutral,neutral
671005516,"> Even for `--norm instance`, given a BxCxHxW tensor, the layer should assume that HxW > 1. It may not work for 1x512x1x1.

your answer reminders me. Perhaps, I know why I met this problem but you didn't
Actually, I referred another reference (https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825802)and made a little change to original cyclegan network. 
![image](https://user-images.githubusercontent.com/69395938/89724792-83293a00-d9d5-11ea-9200-bcf1badb2e11.png)
As you see (red rectangle), the reference adds a normalization layer (BN) in the encode of the innermost. The problem is this normalization layer, because the tensor must be Bx512x1x1 after the last downconvolution, here HxW=1, so your original code doesn't have normalization layer in this place, which makes sense. However, in the train stage, I set batch_size>1 , the problem would not appear. So, can I assume if B>1, the normalization layer could accept HxW=1? If this assumption is right, it could explain why I would meet this problem at test stage, because when B=1 and HxW=1, I still put the tensor into a normalization layer which is different from you original code.
",even norm instance given tensor layer assume may work answer perhaps know met problem actually another reference made little change original network image see red rectangle reference normalization layer encode innermost problem normalization layer tensor must last original code normalization layer place sense however train stage set problem would appear assume normalization layer could accept assumption right could explain would meet problem test stage still put tensor normalization layer different original code,issue,negative,positive,positive,positive,positive,positive
670994497,"Even for `--norm instance`, given a BxCxHxW tensor, the layer should assume that HxW > 1. It may not work for 1x512x1x1. ",even norm instance given tensor layer assume may work,issue,negative,neutral,neutral,neutral,neutral,neutral
670990078,"> Could you add `--norm instance` to both training and test scripts?
> You mentioned,"" the results would skip half of the images""? Are you referring to the test script? Currently, the test script only supports batch_size=1.

Thank you for your attention. Yes, currently, we could only use batch_size=1 in the test script. I checked the code where referring the norm layer several times, I am sure the norm is instance normalization not batch normalization. Moreover, the option list which would automatically prints out when running the code also shows  'norm: instance' no matter as train stage or test stage. Now, I try to annotate the two related lines of codes which cause this problem in the .../torch/nn/functional.py. It seems work, but I am not sure whether it would affect the quality of results.",could add norm instance training test would skip half test script currently test script thank attention yes currently could use test script checked code norm layer several time sure norm instance normalization batch normalization moreover option list would automatically running code also instance matter train stage test stage try annotate two related cause problem work sure whether would affect quality,issue,positive,positive,positive,positive,positive,positive
670989025,Have you checked the folder `AB/train` and `AB/test`? You may also want to remove the .ipynb_checkpoints,checked folder may also want remove,issue,negative,neutral,neutral,neutral,neutral,neutral
670988899,"Could you add `--norm instance` to both training and test scripts? 
You mentioned,"" the results would skip half of the images""?  Are you referring to the test script? Currently, the test script only supports batch_size=1. ",could add norm instance training test would skip half test script currently test script,issue,negative,negative,neutral,neutral,negative,negative
670838782,"There is a [debate](https://stackoverflow.com/questions/2724348/should-i-use-import-os-path-or-import-os) about `import os` and `import os.path`. I don't prefer one way or the other. Yeah,  we should not import both. ",debate import o import prefer one way yeah import,issue,negative,neutral,neutral,neutral,neutral,neutral
670281094,"Thanks for your reply.


________________________________
发件人: Jun-Yan Zhu <notifications@github.com>
发送时间: 2020年8月7日 0:07
收件人: junyanz/pytorch-CycleGAN-and-pix2pix <pytorch-CycleGAN-and-pix2pix@noreply.github.com>
抄送: #GUO LANQING# <LANQING001@e.ntu.edu.sg>; Author <author@noreply.github.com>
主题: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] Always out of memory when testing (#1119)


The current test code is on a single GPU. You need to change the test.py if you would like to support multi-GPUs. For 2000*3000 images, you might get out of memory. It's hard to fix unless you use a GPU with more memory. But you can also use CPU mode just for a few large images while using GPUs for the rest of test set. (you can just create two test sets)

―
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1119#issuecomment-670023350>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AJOVWJ5QYZO3RONSR2CGAIDR7LINZANCNFSM4PWTL3IA>.
",thanks reply author author always memory testing current test code single need change would like support might get memory hard fix unless use memory also use mode large rest test set create two test thread reply directly view,issue,positive,positive,neutral,neutral,positive,positive
670023350,"The current test code is on a single GPU. You need to change the `test.py` if you would like to support multi-GPUs. For 2000*3000 images, you might get out of memory. It's hard to fix unless you use a GPU with more memory. But you can also use CPU mode just for a few large images while using GPUs for the rest of test set. (you can just create two test sets)",current test code single need change would like support might get memory hard fix unless use memory also use mode large rest test set create two test,issue,positive,negative,neutral,neutral,negative,negative
669759766,"> Change this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/dca9002f598fada785b92d496fcdc7b04528b393/util/visualizer.py#L37).

OK. Thanks a lot.",change line thanks lot,issue,negative,positive,positive,positive,positive,positive
669733227,@ajaykmr2292 Can you tell me where you get the one piece dataset? I've been looking all over the manga sites but cannot find paired color/uncolor version,tell get one piece looking manga find paired version,issue,negative,neutral,neutral,neutral,neutral,neutral
669711196,"What is the difference between better style transfer results vs. better image quality results? 
The background color is supposed to change as the color distribution in horse and zebra backgrounds are different. You can use an object mask if your goal is to keep the background color.  See this nice [work](https://arxiv.org/pdf/1806.02311.pdf) for an example.  ",difference better style transfer better image quality background color supposed change color distribution horse zebra different use object mask goal keep background color see nice work example,issue,positive,positive,positive,positive,positive,positive
669620940,"Sorry for the late reply. 

You mentioned that you got better results by using the resnet rather unet architecture in other Issue. So what's the better means? In my experiments (using horse2zebra dataset), I got better style transfer results with resnet but the better image quality results with unet. 

I am wondering if I should build the network with unet when I want to keep details of background as much as possible (especially the color information), or can you give me some tips? I also added the SSIM and perceptual loss to the loss function, but it seems the reconstruction of the  background color still not very good. ",sorry late reply got better rather architecture issue better got better style transfer better image quality wondering build network want keep background much possible especially color information give also added perceptual loss loss function reconstruction background color still good,issue,positive,positive,positive,positive,positive,positive
669518563,"Yes, you need to modify the data loader. If you use CycleGAN, please modify this [one](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py). For pix2pix, change this [one](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/aligned_dataset.py)",yes need modify data loader use please modify one change one,issue,positive,neutral,neutral,neutral,neutral,neutral
669511722,"For `--continue_train`, yes. 

No worry for the data loader, as the data loader will read images randomly from the dataset. ",yes worry data loader data loader read randomly,issue,negative,negative,negative,negative,negative,negative
668995908,"Will uploading my checkpoints folder be sufficient to use --continue_train? I'm using some part of the dataset. When training stops and I restart with my uploaded checkpoints folder, will be the same part of the dataset? If it's not the same part of the dataset, will it cause a problem? 

Thank you",folder sufficient use part training restart folder part part cause problem thank,issue,negative,neutral,neutral,neutral,neutral,neutral
668892610,I am not sure. Maybe you can manually download them periodically or write a script to send the checkpoints files to your local machine periodically. ,sure maybe manually periodically write script send local machine periodically,issue,negative,positive,positive,positive,positive,positive
668381398,"Many thanks to your relpy. Setting the option of ‘preprocess’ to ‘crop’ when training, and ‘none’ when testing is right?

Thank you!

Get Outlook for iOS<https://aka.ms/o0ukef>
________________________________
From: Jun-Yan Zhu <notifications@github.com>
Sent: Tuesday, August 4, 2020 1:00:00 PM
To: junyanz/pytorch-CycleGAN-and-pix2pix <pytorch-CycleGAN-and-pix2pix@noreply.github.com>
Cc: #GUO LANQING# <LANQING001@e.ntu.edu.sg>; Author <author@noreply.github.com>
Subject: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to keep the size of output image same with input image? (#1115)


Your test option looks great. It depends on how you trained your model. Make sure that you train and test at the same resolution. During training, you can crop patches, but please do not resize the images. See this [Q&A] for more details.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1115#issuecomment-668380009>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AJOVWJ5QGRDTJDDN3FLNHPDR66IVBANCNFSM4PT7YHRA>.
",many thanks setting option crop training none testing right thank get outlook sent august author author subject keep size output image input image test option great trained model make sure train test resolution training crop please resize see thread reply directly view,issue,positive,positive,positive,positive,positive,positive
668380009,"Your test option looks great. It depends on how you trained your model. Make sure that you train and test models at the same resolution. During training, you can crop patches, but please do not resize the images.  See this [Q&A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#out-of-memory-174) for more details. ",test option great trained model make sure train test resolution training crop please resize see,issue,positive,positive,positive,positive,positive,positive
668128587,We converted grayscale images to 3-channels image using `np.tile`. You can just take the first channel of saved images OR change this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9e6fff7b7d5215a38be3cac074ca7087041bea0d/util/util.py#L22).,converted image take first channel saved change line,issue,negative,positive,positive,positive,positive,positive
667697223,You can change the file name in this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L65).,change file name line,issue,negative,neutral,neutral,neutral,neutral,neutral
667631051,"Thanks author!By overriding defaults,run successfully.",thanks author run successfully,issue,positive,positive,positive,positive,positive,positive
667628197,"Thanks author,I tried your suggestion ,it seems works,but I got the following Attribute error,how to solve it then?

loading the model from D:/vgan_pix2pix5/experiment_name\latest_net_G.pth
Traceback (most recent call last):
  File ""test.py"", line 49, in <module>
    model.setup(opt)               # regular setup: load and print networks; create schedulers #在base_model.py中
  File ""D:\360安全浏览器下载\pytorch-CycleGAN-and-pix2pix-master\pytorch-CycleGAN-and-pix2pix-master\models\base_model.py"", line 94, in setup
    self.load_networks(load_suffix)
  File ""D:\360安全浏览器下载\pytorch-CycleGAN-and-pix2pix-master\pytorch-CycleGAN-and-pix2pix-master\models\base_model.py"", line 204, in load_networks
    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))
  File ""D:\360安全浏览器下载\pytorch-CycleGAN-and-pix2pix-master\pytorch-CycleGAN-and-pix2pix-master\models\base_model.py"", line 180, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
  File ""D:\360安全浏览器下载\pytorch-CycleGAN-and-pix2pix-master\pytorch-CycleGAN-and-pix2pix-master\models\base_model.py"", line 180, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
  File ""D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py"", line 594, in __getattr__
    type(self).__name__, name))
AttributeError: 'Sequential' object has no attribute 'model'",thanks author tried suggestion work got following attribute error solve loading model recent call last file line module opt regular setup load print create file line setup file line net file line module key file line module key file line type self name object attribute,issue,negative,positive,neutral,neutral,positive,positive
667621895,"It is possible. You can use the flag `--model test`.  See the [instruction](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix). If you have issues, also see this [Q&A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#runtimeerror-errors-in-loading-state_dict-812-671461-296).",possible use flag model test see instruction also see,issue,negative,neutral,neutral,neutral,neutral,neutral
667184342,The output images after training and testing are called differently than what they are as input (e.g. epoch001_realA.jpg). Is it possible to have them keep the same name as what I choose in the input? I don't understand in the code where the name is changed. ,output training testing differently input possible keep name choose input understand code name,issue,negative,neutral,neutral,neutral,neutral,neutral
666955597,"After you run the test code, you can find images saved at the directory `./results/{your_experiment_name}/latest_test/indeximages`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#cyclegan-traintest) for more details. ",run test code find saved directory see,issue,negative,neutral,neutral,neutral,neutral,neutral
666733718,I think Windows has a different path format compared to Linux. Try something like `.\datasets\dehazing\train`.,think different path format try something like,issue,negative,neutral,neutral,neutral,neutral,neutral
666446816,"AssertionError: ./datasets/dehazing\train is not a valid directory

Can you kindly help me to fix this error!",valid directory kindly help fix error,issue,negative,positive,positive,positive,positive,positive
665871401,"You can change the data directories using `dataroot`. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L23). For pip usage, refer to this [webpage](https://www.shellhacks.com/python-how-to-use-pip-basic-commands/).",change data see line pip usage refer,issue,negative,neutral,neutral,neutral,neutral,neutral
665792405,"


> Got this to run in Windows properly now!
> 
> I changed the directories in image_folder.py, and downgraded the version of visdom.
> 
> Thanks again ! :)

How do you change the directories in image_folder.py! Can you kindly explain with code? Also could you kindly guide me how to downgrade the visdom's version using pip!",got run properly version thanks change kindly explain code also could kindly guide downgrade version pip,issue,positive,positive,positive,positive,positive,positive
665671328,"To reproduce the numbers in the paper, please refer to the original Lua [repo](https://github.com/phillipi/pix2pix) with pre-trained Torch models. Also, make sure that you can reproduce the numbers for the real images first. ",reproduce paper please refer original torch also make sure reproduce real first,issue,positive,positive,positive,positive,positive,positive
665670073,"Yes, we found that LSGAN is more effective in our paper.",yes found effective paper,issue,positive,positive,positive,positive,positive,positive
665265233,What is your solution to this problem? I encountered the same issue. Thanks! @wadesunyang ,solution problem issue thanks,issue,negative,positive,positive,positive,positive,positive
664763436,"thank you for your reply again. 

I tried the 64 of batch size, it failed to achieve any reasonable results yet (just noise map. Even thought the Wasserstein distance is more stable than batch size of 1, and it's decreasing). The discriminator seems still too weak to feedback any useful gradients to generator, so the results might not correct in this experiment. 

And I also replaced the patchGAN with DCGAN's, but the model cannot be trained too. 

Is that why you use LSGAN loss instead of WGAN loss in the paper?

Thank you for your time, have a nice day",thank reply tried batch size achieve reasonable yet noise map even thought distance stable batch size decreasing discriminator still weak feedback useful generator might correct experiment also model trained use loss instead loss paper thank time nice day,issue,positive,positive,positive,positive,positive,positive
664754895,"Unfortunately, we don't have reasonable images with WGAN-GP. I am not sure if it is related to batch_size. As I mentioned, GP seems to be not compatible with PatchGAN. The loss might work for other types of discriminators and tasks. ",unfortunately reasonable sure related compatible loss might work,issue,negative,positive,positive,positive,positive,positive
663936179,"Thank you for your reply and I agree with your ideas. And I think the batch size is one of the factor that makes cyclegan-with-wgan-gp training very unstable, do you agree with that? I'm trying to set the batch size as 64 instead of 1 (use InstanceNorm), is this worthy to try? 

I also wondering that if you have any reasonable images using WGAN-GP loss? (Even though the training was not very stable) If yes, how many training steps it takes? Because I trained CycleGAN with WGAN/WGAN-GP/WGAN-DIV loss, thier results were very bad (jusy a noise map, or sometimes looks like ""ghost horses""). It seems the discriminators were too weak to give any useful guidiance to the generators. 

And I realized that the WGAN (including GP and DIV version) loss are very sensitive to network structures and input size. It seems not very easy to apply the WGAN loss to other models. ",thank reply agree think batch size one factor training unstable agree trying set batch size instead use worthy try also wondering reasonable loss even though training stable yes many training trained loss bad noise map sometimes like ghost weak give useful div version loss sensitive network input size easy apply loss,issue,negative,positive,neutral,neutral,positive,positive
663916779,"The WGAN loss itself doesn't work without GP. Even with GP, we also haven't made it work better than the vanilla CycleGAN/pix2pix. The loss is also not very stable and meaningful for us. The WGAN-GP loss was added to the repo in case users want to use it for other models. The possible reasons could be two: (1) the PatchGAN discriminator is already quite weak. Thus, adding GP loss will make it too weak compared to the generator. (2) the GP loss assumes that the inputs are independent according to the original paper, while PatchGAN takes overlapping patches, and breaks this assumption.   ",loss work without even also made work better vanilla loss also stable meaningful u loss added case want use possible could two discriminator already quite weak thus loss make weak generator loss independent according original paper assumption,issue,negative,positive,neutral,neutral,positive,positive
663915942,Thanks for your contribution. Have you been able to reproduce the FID number on the ProgGAN datasets?,thanks contribution able reproduce fid number,issue,negative,positive,positive,positive,positive,positive
663424377,"Does it work with our official repo? I recommend that you first follow the publically available repo, and then make your own changes. ",work official recommend first follow available make,issue,negative,positive,positive,positive,positive,positive
662648254,I will try to fix the code so that we can get proper learning rate decay for continued train.,try fix code get proper learning rate decay continued train,issue,negative,neutral,neutral,neutral,neutral,neutral
662397839,"> @AllAwake Cool results!
> 
> Here is the implementation of resize-conv I used. It remove the checkerboard artifacts during early training. You may find it useful.
> 
> ```
>                           nn.Upsample(scale_factor = 2, mode='bilinear'),
>                           nn.ReflectionPad2d(1),
>                           nn.Conv2d(ngf * mult, int(ngf * mult / 2),
>                                              kernel_size=3, stride=1, padding=0),
> ```
> 
> It should replace the `ConvTranspose2d` in `ResnetGenerator`.

I like how it totally gets rid of the checkerboard artefacts, but it comes at a cost of actually learning useful features. I'm trying to balance these effects. ",cool implementation used remove checkerboard early training may find useful mult mult replace like totally rid checkerboard come cost actually learning useful trying balance effect,issue,positive,positive,positive,positive,positive,positive
662225374,"Hi daofeng, I met this question recently, have you handled this issue? @daofeng2007 @",hi met question recently handled issue,issue,negative,neutral,neutral,neutral,neutral,neutral
662156567,"I have one question regarding the starting epoch without continue_train option.

When continue_train does not specified, does it always starts train from epoch 1?
In this case, [discussion from the PyTorch forum](https://discuss.pytorch.org/t/loading-optimizer-dict-starts-training-from-initial-lr/36328) might be useful.",one question regarding starting epoch without option always train epoch case discussion forum might useful,issue,negative,positive,positive,positive,positive,positive
662155197,I did not considered continue training while writing that code and will take a look whether there is a good solution to handle it.,considered continue training writing code take look whether good solution handle,issue,positive,positive,positive,positive,positive,positive
662151223,"Thanks for the PR. If this condition is added, it seems that the learning rate will not change during the first epoch of `contine_train`.  I am wondering if there is a workaround solution.  @hyecheol123  @taesungp ",thanks condition added learning rate change first epoch wondering solution,issue,positive,positive,positive,positive,positive,positive
661867829,"I successfully locate the variable, and fixed it with simple conditional code.
The PR has been submitted.",successfully locate variable fixed simple conditional code,issue,negative,positive,positive,positive,positive,positive
661533997,The original ratio is 1. You can fix one learning rate and change the other one. ,original ratio fix one learning rate change one,issue,negative,positive,positive,positive,positive,positive
661516168,"May I ask you how can I calculate the G/D based on the modified learning rate? If it is G:D = 1:0.5 = 2:1, would it be okay to understand that the original model's G/D ratio is 1?",may ask calculate based learning rate would understand original model ratio,issue,negative,positive,positive,positive,positive,positive
661198161,"Did it happen for our datasets or your datasets? If you can use `contine_train` and reproduce the crash in the first epoch (after contine_train), you can print the image path and see which image caused the problem. ",happen use reproduce crash first epoch print image path see image problem,issue,negative,positive,positive,positive,positive,positive
661186148,"Not necessary.  `vanilla` is used, as it was done in the original paper. Feel free to try other gan losses. You may want to reduce L1 loss when you use `lsgan`, as the range of `lsgan` is smaller than `vanilla`. ",necessary vanilla used done original paper feel free try gan may want reduce loss use range smaller vanilla,issue,positive,positive,positive,positive,positive,positive
661077239,"Hi, I met this issue recently, have you figured it out? @BongBong87 ",hi met issue recently figured,issue,negative,neutral,neutral,neutral,neutral,neutral
660857559,"When I get the old_lr i.e. current learning rate before continue training, I found it's right value, not the initial value 0.0002 as I thought. You pervious code is exactly right. Sorry for that and appreciated for your help. My issue was perhaps because of the overlapping of loss record. (I used tensorboard to record loss behaviour in colab).",get current learning rate continue training found right value initial value thought pervious code exactly right sorry help issue perhaps loss record used record loss behaviour,issue,negative,positive,neutral,neutral,positive,positive
660749894,You can also change learning rates. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L69).,also change learning see line,issue,negative,neutral,neutral,neutral,neutral,neutral
660719478,I submitted a new [commit](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/fd29199c33bd95704690aaa16f238a4f8e74762c). The code will update the learning rate in the beginning of each epoch. ,new commit code update learning rate beginning epoch,issue,negative,positive,positive,positive,positive,positive
660718006,"Your loss plots look normal. Usually, cycle-consistency loss and identity loss decrease during training, while GAN losses oscillate. To evaluate the quality or detect overfitting/underfitting. you need to apply additional evaluation metrics to your training and test images. The metric is task-specific.  See more discussion at #730.",loss look normal usually loss identity loss decrease training gan oscillate evaluate quality detect need apply additional evaluation metric training test metric see discussion,issue,negative,negative,neutral,neutral,negative,negative
660684126,"Hi, I met the same issue as you @fengyu19 @sangrockEG , have you figured it out? ",hi met issue figured,issue,negative,neutral,neutral,neutral,neutral,neutral
660508480,"@SsnL Thanks for the solution. Did anyone successfully implement it in Generators that monotonically upsample the input? In this example, for example, if you add a padding, the output will be **exactly** twice the size of the input;
```
nn.Upsample(scale_factor = 2, mode='bilinear'),
                          nn.ReflectionPad2d(1),
                          nn.Conv2d(ngf * mult, int(ngf * mult / 2),
                                             kernel_size=3, stride=1, padding=1),
```

",thanks solution anyone successfully implement monotonically input example example add padding output exactly twice size input mult mult,issue,positive,positive,positive,positive,positive,positive
660411806,"I am not sure. You might want to normalize the data to [-1,1]. Our current generator only outputs values between [-1, 1]. For images, you can use `transform_A` and `transform_B`. But they may not work for your numpy array. You need to normalize the data by yourself. For the debugging purpose, you can print the shape of your torch array as well as the min and max value. ",sure might want normalize data current generator use may work array need normalize data purpose print shape torch array well min value,issue,positive,positive,positive,positive,positive,positive
660056981,"Dear @junyanz ,

sorry for disturbing you again. I would need some more advice. I was finally able to train my model with the NumPy arrays, but something went wrong. I need to train an image2image model that takes as input 256*256 2D numpy arrays whose values go from -40 to +40 (they represent melspectrograms), and outputs numpy arrays with the same characteristics. 

Here is how I modified the data loader for the training (UnalignedDataset):

    def __init__(self, opt):
        """"""Initialize this dataset class.
 
        Parameters:
            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions
        """"""
        BaseDataset.__init__(self, opt)
        self.dir_A = os.path.join(opt.dataroot, opt.phase + 'A')  # create a path '/path/to/data/trainA'
        self.dir_B = os.path.join(opt.dataroot, opt.phase + 'B')  # create a path '/path/to/data/trainB'
 
        self.A_paths = sorted(make_dataset(self.dir_A, opt.max_dataset_size))   # load images from '/path/to/data/trainA'
        self.B_paths = sorted(make_dataset(self.dir_B, opt.max_dataset_size))    # load images from '/path/to/data/trainB'
        self.A_size = len(self.A_paths)  # get the size of dataset A
        self.B_size = len(self.B_paths)  # get the size of dataset B
        btoA = self.opt.direction == 'BtoA'
        input_nc = self.opt.output_nc if btoA else self.opt.input_nc       # get the number of channels of input image
        output_nc = self.opt.input_nc if btoA else self.opt.output_nc      # get the number of channels of output image
        self.transform_A = get_transform(self.opt, grayscale=(input_nc == 1))
        self.transform_B = get_transform(self.opt, grayscale=(output_nc == 1))
 
    def __getitem__(self, index):
        """"""Return a data point and its metadata information.
 
        Parameters:
            index (int)      -- a random integer for data indexing
 
        Returns a dictionary that contains A, B, A_paths and B_paths
            A (tensor)       -- an image in the input domain
            B (tensor)       -- its corresponding image in the target domain
            A_paths (str)    -- image paths
            B_paths (str)    -- image paths
        """"""
        A_path = self.A_paths[index % self.A_size]  # make sure index is within then range
        if self.opt.serial_batches:   # make sure index is within then range
            index_B = index % self.B_size
        else:   # randomize the index for domain B to avoid fixed pairs.
            index_B = random.randint(0, self.B_size - 1)
        B_path = self.B_paths[index_B]
        A_img = np.load(A_path)
        A_img = np.expand_dims(A_img, axis=0)
        B_img = np.load(B_path)
        B_img = np.expand_dims(B_img, axis=0)
        # apply image transformation
        A = torch.from_numpy(A_img)
        B = torch.from_numpy(B_img)
 
        return {'A': A, 'B': B, 'A_paths': A_path, 'B_paths': B_path}

And here's the one for the test:

class SingleDataset(BaseDataset):
    """"""This dataset class can load a set of images specified by the path --dataroot /path/to/data.

    It can be used for generating CycleGAN results only for one side with the model option '-model test'.
    """"""

    def __init__(self, opt):
        """"""Initialize this dataset class.

        Parameters:
            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions
        """"""
        BaseDataset.__init__(self, opt)
        self.A_paths = sorted(make_dataset(opt.dataroot, opt.max_dataset_size))
        input_nc = self.opt.output_nc if self.opt.direction == 'BtoA' else self.opt.input_nc
        self.transform = get_transform(opt, grayscale=(input_nc == 1))

    def __getitem__(self, index):
        """"""Return a data point and its metadata information.

        Parameters:
            index - - a random integer for data indexing

        Returns a dictionary that contains A and A_paths
            A(tensor) - - an image in one domain
            A_paths(str) - - the path of the image
        """"""
        A_path = self.A_paths[index]
        A_img = np.load(A_path)
        A_img = np.expand_dims(A_img, axis=0)
        A = torch.from_numpy(A_img)
        return {'A': A, 'A_paths': A_path}

I do not know why but after 7 epochs of training (36000 ""images"" each), the test output makes no sense: numpy array 256*256*3 with values ranging in a very large interval. 

Finally, here is how I launched the code:

python train.py --dataroot ""/content/drive/Shared drives/MusicBERT_Datasets/CycleGAN_Dataset"" --name voice2song --model cycle_gan --input_nc 1 --output_nc 1 --save_epoch_freq 1

python test.py --dataroot ""/content/drive/Shared drives/MusicBERT_Datasets/CycleGAN_Dataset/testA"" --name voice2song --model test --input_nc 1 --output_nc 1 --no_dropout

Any idea?

Thanks a lot!!",dear sorry disturbing would need advice finally able train model something went wrong need train model input whose go represent data loader training self opt initialize class opt option class experiment need subclass self opt create path create path sorted load sorted load get size get size else get number input image else get number output image self index return data point information index random integer data indexing dictionary tensor image input domain tensor corresponding image target domain image image index make sure index within range make sure index within range index else randomize index domain avoid fixed apply image transformation return one test class class load set path used generating one side model option self opt initialize class opt option class experiment need subclass self opt sorted else opt self index return data point information index random integer data indexing dictionary tensor image one domain path image index return know training test output sense array ranging large interval finally code python name model python name model test idea thanks lot,issue,positive,negative,neutral,neutral,negative,negative
659588703,"If we set --epoch_count 150, it will be training from epoch 150. But for epoch 150, the learning rate is still initial value 0.0002, expected is 0.0002 * (1 - (150 - 100) / 101). Because model.update_learning_rate() musst be implemented after model.optimize_parameters(). I tried update learning rate before training, it raises error in PyTorch. That is to say, the first epoch of continue-training can not use the right value of learning rate. 

- first train: --n_epoch 100 --n_epochs_decay 100 --lr 0.0002 --epoch_count 1 
- continue: --n_epoch 100 --n_epochs_decay 100 --lr 0.0002 --epoch_count 150 --epoch 149 --continue_train ",set training epoch epoch learning rate still initial value tried update learning rate training error say first epoch use right value learning rate first train continue epoch,issue,negative,positive,positive,positive,positive,positive
659575283,"Thank you. Also, I'm using Google Colab and I'm trying to install wisdom. But it's been loading for a long time, so I'm not sure how to proceed. The link just goes to ""site cannot be reached"" for me.
![image](https://user-images.githubusercontent.com/66890934/87705713-52eed280-c76c-11ea-8163-08845d226156.png)
",thank also trying install wisdom loading long time sure proceed link go site image,issue,positive,positive,positive,positive,positive,positive
659574906,"Got your question. If you set `--epoch_count 150`, you can keep the original learning rate. ",got question set keep original learning rate,issue,negative,positive,positive,positive,positive,positive
659572746,"Yes. you first need to train a model using `train.py`, and then test it using `test.py`. You don't need to use the downloading script. (The script downloads a model in our model zoo). See the code [overview](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/overview.md) for more details. ",yes first need train model test need use script script model model zoo see code overview,issue,negative,positive,positive,positive,positive,positive
659545366,"So to train my own model I need to run the train.py, and if you're using your pre-trained model you can bypass the train.py and just download the script and run test.py?",train model need run model bypass script run,issue,negative,neutral,neutral,neutral,neutral,neutral
659536487,"But I want the initial learning rate to keep the same. In an other word, lr = lr_lambda * 0.0002 for epoch 151~200, not the lr = lr_lambda * 0.0001. Does it work, that we simply change the --lr to 0.0001?
lr_lambda = 1.0 - max(0, epoch + epoch_count - n_epochs) /( n_epochs_decay + 1);",want initial learning rate keep word epoch work simply change epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
659528443,You could use a new `--lr`. It should affect it.,could use new affect,issue,negative,positive,positive,positive,positive,positive
659527367,See the [tutorial](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) on how to apply a pre-trained model. It will load the model from the directory specified by `--checkpoints_dir` and `--name`.,see tutorial apply model load model directory name,issue,negative,neutral,neutral,neutral,neutral,neutral
659150754,You need to train your model following these [steps](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#cyclegan-traintest).,need train model following,issue,negative,neutral,neutral,neutral,neutral,neutral
658868258,Identity loss only works when `input_nc == output_nc`. You need to disable it by setting `lambda_identity=0`.,identity loss work need disable setting,issue,negative,neutral,neutral,neutral,neutral,neutral
658867332,You may want to also modify the function `make_dataset`. See this [lline](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L29).,may want also modify function see,issue,negative,neutral,neutral,neutral,neutral,neutral
658731301,"Just one quick another thing. Can't I just change can change the following two lines:

        A_img = Image.open(A_path).convert('RGB')
        B_img = Image.open(B_path).convert('RGB')

Into:

        A_img = np.load(A_path)
        B_img = np.load(B_path)

Without modifying the custom data loader template?",one quick another thing ca change change following two without custom data loader template,issue,negative,positive,positive,positive,positive,positive
658383503,It is possible. You need to modify the data loader. Here is a data loader [template](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py).,possible need modify data loader data loader template,issue,negative,neutral,neutral,neutral,neutral,neutral
657226190,@avichapman In my case this problem occurred because I did not have CUDA 9.2 installed. Since I have CUDA 10.2 what I did was update PyTorch in `requirements.txt` for a version that was compatible with that and the problem disappeared.,case problem since update version compatible problem,issue,negative,neutral,neutral,neutral,neutral,neutral
657223966,"yes, for anyone who has the same problem running it with python3 instead of python worked. 

python3 test.py --dataroot ./datasets/facades/ --direction BtoA --model pix2pix --name facades_label2photo_pretrained",yes anyone problem running python instead python worked python direction model name,issue,negative,neutral,neutral,neutral,neutral,neutral
657136880,The ImportError `cannot import name ABC` might be caused by the Python version. I am using Python 3.7.6.,import name might python version python,issue,negative,neutral,neutral,neutral,neutral,neutral
657006051,"@junyanz  I change the server to run this code, and the result was normal. But I still don't know why on the first server,the output is so strange ?",change server run code result normal still know first server output strange,issue,negative,positive,positive,positive,positive,positive
656994333,"@junyanz  Thank you for your reply ， The images under   the directory checkpoints/horsee2zebra/web/images  are same as checkpoints/horse2zebra/web/index.html. it also looks strange. In addition, I did not modify the code that you release.I donot kow why? Thanks.",thank reply directory also strange addition modify code thanks,issue,positive,positive,neutral,neutral,positive,positive
656988131,Not sure about it. It seems that your image display is not functioning. At least images named `real_A` should look normal. Could you manually check if the images under the directory `checkpoints/horsee2zebra/web/images` look normal?,sure image display least look normal could manually check directory look normal,issue,negative,positive,positive,positive,positive,positive
656957087,I think we should run base tests and check it against the baselines,think run base check,issue,negative,negative,negative,negative,negative,negative
656952824,Great feature! I am wondering if you can get the same results with and without apex and gradient checkpointing. ,great feature wondering get without apex gradient,issue,positive,positive,positive,positive,positive,positive
656574193,"
the pictures that are generated in each iteration (under the folder checkpoints/horse2zebra/web/images) are very strange,like this. I want to know why ?

![Uploading cycle_strange.PNG…]()

",iteration folder strange like want know,issue,negative,negative,neutral,neutral,negative,negative
656543436,"@junyanz thanks, I will send PR, just need to test it a little bit more locally to be sure everything is ok",thanks send need test little bit locally sure everything,issue,positive,positive,positive,positive,positive,positive
656523647,"@junyanz  ,OK, ,thank you so much for your help, resize image by pytorch  funtion ,I only find transform funtion  to resize ,by  according to what your say ,Currently, tensor2im and test_transform will stop the gradients,transform  function may stop gradients too,right  , should I use   np.upsamle function ? 
 ,and I am just curious，why my way not work ,I put self.infer_facepic() before net G infer A,B ,the gradients should not be  affected ,
",thank much help resize image find transform resize according say currently stop transform function may stop right use function way work put net infer affected,issue,negative,positive,positive,positive,positive,positive
656521836,"You can resize the image using Pytorch function, before feeding to the model. ",resize image function feeding model,issue,negative,neutral,neutral,neutral,neutral,neutral
656521346,"> I've added apex support and checkpointing (https://pytorch.org/docs/stable/checkpoint.html) mechanism to reduce memory footprint to my fork https://github.com/seovchinnikov/pytorch-CycleGAN-and-pix2pix
> You can run it with --checkpointing --opt_level ""O2"" and increased input crop size (I was able to run with up to 896 on my 2080 RTX).
> Please note that it was tested on pytorch 1.7 nightly build, and behavior of apex is unstable on old versions.

Would you like to send a PR? If you are busy, I can add apex to the official repo. ",added apex support mechanism reduce memory footprint fork run input crop size able run please note tested nightly build behavior apex unstable old would like send busy add apex official,issue,positive,positive,positive,positive,positive,positive
656515757,"any more help,by follow junyanz  suggestion ,my way seem not work，and  feed pred_fake_B directly to self.facemodel ，but image size to self.facemodel need is 112*112,our network is 256*256, how to do ,I am stucked",help follow suggestion way seem feed directly image size need network,issue,negative,positive,neutral,neutral,positive,positive
656405992,"> I've added apex support and checkpointing (https://pytorch.org/docs/stable/checkpoint.html) mechanism to reduce memory footprint to my fork https://github.com/seovchinnikov/pytorch-CycleGAN-and-pix2pix
> You can run it with --checkpointing --opt_level ""O2"" and increased input crop size (I was able to run with up to 896 on my 2080 RTX).
> Please note that it was tested on pytorch 1.7 nightly build, and behavior of apex is unstable on old versions.

Good work! ",added apex support mechanism reduce memory footprint fork run input crop size able run please note tested nightly build behavior apex unstable old good work,issue,positive,positive,positive,positive,positive,positive
656255411,"I've added apex support and checkpointing (https://pytorch.org/docs/stable/checkpoint.html) mechanism to reduce memory footprint to my fork https://github.com/seovchinnikov/pytorch-CycleGAN-and-pix2pix
You can run it  with --checkpointing --opt_level ""O2"" and increased input crop size (I was able to run with up to 896 on my 2080 RTX).
Please note that it was tested on pytorch 1.7 nightly build, and behavior of apex is unstable on old versions. ",added apex support mechanism reduce memory footprint fork run input crop size able run please note tested nightly build behavior apex unstable old,issue,negative,positive,positive,positive,positive,positive
656201471,"OK, I'll have a try. There are not many research results in this area, so I'm a little confused. Thank you very much.",try many research area little confused thank much,issue,negative,positive,neutral,neutral,positive,positive
656189863,I am not sure. It seems to be not easy. But feel free to try it. ,sure easy feel free try,issue,positive,positive,positive,positive,positive,positive
654518660,"For Cityscapes, you can use more recent models such as [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE). 

For GAN loss, you may want to change the lambda for other losses (e.g., `lambda_L1`) as different GAN losses have different ranges. (For example, use `lambda_L1 10` for LSGAN in pix2pix). It's hard to know in advance which loss works better for a new task. ",use recent spade gan loss may want change lambda different gan different example use hard know advance loss work better new task,issue,negative,positive,neutral,neutral,positive,positive
654137696,"I ran the pix2pix code to train cityscapes data. There are many blurs and artifacts in the generated images. The similar problems can also be found in horse2zerbra trained by CycleGAN. Is there any method to solve this problem?
I also trained the pix2pix using different GAN loss (lsgan, vanilla, wgangp). The results of vanilla seems to be the best. The results of lsgan and wgangp are much blurrier. Why dose the choice of loss has significant impact on the training ? Is there any suggestion for selection of loss function in different tasks?",ran code train data many similar also found trained method solve problem also trained different gan loss vanilla vanilla best much dose choice loss significant impact training suggestion selection loss function different,issue,negative,positive,positive,positive,positive,positive
654103919,"@junyanz ，thanks again ，according to what your say，  I try the following code but still not works
```
self.infer_facepic()
self.fake_B = self.netG_A(self.real_A)
self.rec_A = self.netG_B(self.fake_B)
self.fake_A = self.netG_B(self.real_B)
self.rec_B = self.netG_A(self.fake_A)
```
by this way ，the gradients may not  be affected

```
    def backward_G(self):
        lambda_idt = self.opt.lambda_identity
        lambda_A = self.opt.lambda_A
        lambda_B = self.opt.lambda_B
        # Identity loss
        if lambda_idt > 0:
            # G_A should be identity if real_B is fed.
            self.idt_A = self.netG_A(self.real_B)
            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt
            # G_B should be identity if real_A is fed.
            self.idt_B = self.netG_B(self.real_A)
            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt
        else:
            self.loss_idt_A = 0
            self.loss_idt_B = 0
```

        # GAN loss D_A(G_A(A))
        fake_xishu=-1.0*-1
        if(1):
            
            self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)+7*self.lossReAutoGrad  
            print(""7*self.lossReAutoGrad ,self.loss_G_A"",self.loss_G_A,7*self.lossReAutoGrad   )
            # GAN loss D_B(G_B(B))
            self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)+0*self.lossReAutoGrad  
        else:
            self.loss_G_A = fake_xishu*self.criterionGAN(self.netD_A(self.fake_B), True)-7*self.lossReAutoGrad  
            print(""7*self.lossReAutoGrad ,self.loss_G_A"",self.loss_G_A,7*self.lossReAutoGrad   )
            # GAN loss D_B(G_B(B))
            self.loss_G_B = fake_xishu*self.criterionGAN(self.netD_B(self.fake_A), True)+0*self.lossReAutoGrad                  

        # Forward cycle loss
        self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A
        # Backward cycle loss
        self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B
        # combined loss
        #self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B
        self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B     
        self.loss_G.backward()
`
may be pred_fake_B clone  tensor  copy_ function  work？


",try following code still work way may affected self identity loss identity fed identity fed else gan loss true print gan loss true else true print gan loss true forward cycle loss backward cycle loss combined loss may clone tensor function,issue,negative,positive,positive,positive,positive,positive
653847673,"I think there are two issues in your code. 
- You probably should add lossReAutoGrad to the `loss_G`, as only the generator is involved in the computational graph  of this loss. 
-   You should feed `pred_fake_B` directly to  `self.facemodel` (after applying some proper transformation in PyTorch).  Currently, `tensor2im` and `test_transform` will stop the gradients.",think two code probably add generator involved computational graph loss feed directly proper transformation currently stop,issue,negative,neutral,neutral,neutral,neutral,neutral
653847206,"It might be possible. If you have paired data, you can use pix2pix (otherwise, CycleGAN). You might need a generator that can warp an image such as [STN](https://arxiv.org/abs/1506.02025). See a related [projct](https://arxiv.org/pdf/1804.00064.pdf) in dental restoration. ",might possible paired data use otherwise might need generator warp image see related dental restoration,issue,negative,neutral,neutral,neutral,neutral,neutral
653835675,"> Hi,
> I have trained a day to night model using your cyclegans code on a mixture of night images from BDD, Mapillary Vistas and Our own dataset.
> The pretrained model is available in this link : https://drive.google.com/drive/folders/1zFrW5CqlQIbRY1vnTZ54k2wOqn4VDMpe?usp=sharing
> Kindly add this model in your repository for people who want this conversion. I have used this model in a published work in ICIP19 winning a spotlight paper award ( Link : https://github.com/sauradip/night_image_semantic_segmentation ).

thank you so much",hi trained day night model code mixture night model available link kindly add model repository people want conversion used model work winning spotlight paper award link thank much,issue,positive,positive,positive,positive,positive,positive
653609761,"In paper ""Analyzing and Improving the Image Quality of StyleGAN"" they solve the same problem by changing instance normalization to weight demodulation. 

As they say: ""We hypothesize that the droplet artifact is a result of the generator intentionally sneaking signal strength information past instance normalization: by creating a strong, localized spike that dominates the statistics, the generator can effectively scale the signal as it likes elsewhere.""
",paper improving image quality solve problem instance normalization weight demodulation say hypothesize droplet artifact result generator intentionally sneaking signal strength information past instance normalization strong spike statistic generator effectively scale signal elsewhere,issue,positive,positive,positive,positive,positive,positive
653405633,"@junyanz  yeah，thank your for your answer，loss_D = (loss_D_real + loss_D_fake) * 0.5 + self. lossReAutoGrad，I have tried that before，I  added ` (loss_D_real + loss_D_fake) * 0.5 + 7*self. lossReAutoGrad ` to make D loss more depend on face recognition loss，which  seem to not work 。
 **the thing I want to know most is whether changing  D loss by this way,the loss can backward the network or not ,the gradient will be affected or not**",self tried added self make loss depend face recognition seem work thing want know whether loss way loss backward network gradient affected,issue,negative,neutral,neutral,neutral,neutral,neutral
652778021,"Not sure if I completely followed your implementation. But it seems that you need to use both GAN loss and face recognition loss. 
```
loss_D = (loss_D_real + loss_D_fake) * 0.5 + self. lossReAutoGrad
loss_D.backward()
```
Recognizing small faces is a challenging problem and it is not within the area of my expertise. I recommend that you follow some popular methods such as this one (https://www.cs.cmu.edu/~peiyunh/tiny/)",sure completely implementation need use gan loss face recognition loss self small problem within area recommend follow popular one,issue,negative,positive,positive,positive,positive,positive
652712140,"I was missing one line, so I updated my original comment. I found that this helps to prevent the discriminator to overfit. But I was missing one line so I have to repeat the experiments.",missing one line original comment found prevent discriminator overfit missing one line repeat,issue,negative,negative,neutral,neutral,negative,negative
652648117,"> > You can also set the --display_id 0 if you don't want to use visdom
> > It works!

Where to set --display_id 0?",also set want use work set,issue,negative,neutral,neutral,neutral,neutral,neutral
652256844,"I want to know how to decrea the self.lossReAutoGrad （the face recognition assuming name is  the loss A ），and I wonder  my way of adding the lossA to the lossD works。the loss can backward or not ?thanks in advanced
 ",want know face recognition assuming name loss wonder way loss backward thanks advanced,issue,negative,positive,positive,positive,positive,positive
652251028,"self.lossReAutoGrad is the face recognition loss  get from the infer_facepic function
and I want to replace  original loss with it，the loss lossReAutoGrad  drop   slowly ，keep a value as 13 around ，not decrease
`

    def backward_D_basic(self, netD, real, fake):
        # Real
        pred_real = netD(real)
        loss_D_real = self.criterionGAN(pred_real, True)
        # Fake
        pred_fake = netD(fake.detach())
        loss_D_fake = self.criterionGAN(pred_fake, False)
        # Combined loss
        loss_D = (loss_D_real + loss_D_fake) * 0.5
        loss_D = (0*loss_D_real + 0*loss_D_fake+1*self.lossReAutoGrad) * 0.5
        # backward
        loss_D.backward()
        return loss_D


`",face recognition loss get function want replace original loss loss drop slowly value around decrease self real fake real real true fake false combined loss backward return,issue,negative,negative,negative,negative,negative,negative
652244829,"below is somecode i used：

this part code is generate news Images pairs,and  calucating the the loss A （self.lossReAutoGrad）

[""3_0005.jpg"",""3_0007.jpg"",""3_0010.jpg"",""3_0012.jpg"",""3_0013.jpg"",        ]  are images ，they failed in face recognition part due to heavily motion blured ，faces are image pairs

```
 def infer_facepic(self):    
        listSpeacialFiles=[""3_0005.jpg"",""3_0007.jpg"",""3_0010.jpg"",""3_0012.jpg"",""3_0013.jpg"",        ]
        totalfaceloss=0
        iSum=0
        for i ,imgpys in enumerate( faces): 
               ####get imageA generate from cyclegan model 
                imgpy=imgpys[0]
                myimg=cv2.imread(imgpy[0])
                myimg=cv2.resize(myimg,(256,256))                
                    
                real_A=self.test_transform(myimg).to(self.device).unsqueeze(0)
                fake_B=self.test_transform(myimg).to(self.device).unsqueeze(0)
                pred_fake_B = tensor2im(self.netG_A(real_A))
                pred_rec_A = tensor2im(self.netG_B(fake_B))                             
                img=pred_fake_B
                ####get imageA's comparing face vectors
                embs1=self.facemodel(self.test_transform(img).to(self.devicess).unsqueeze(0)).detach().cpu().numpy().reshape(1,-1)


                imgpy=imgpys[1]
                myimg=cv2.imread(imgpy)

                myimg=Image.fromarray(myimg)
                real_A=self.test_transform(myimg).to(self.device).unsqueeze(0)
                fake_B=self.test_transform(myimg).to(self.device).unsqueeze(0)
                ####get imageB generate from cyclegan model 
                pred_fake_B = tensor2im(self.netG_A(real_A))
                pred_rec_A = tensor2im(self.netG_B(fake_B)) 
                img=pred_fake_B
                ####get imageB's comparing face vectors
                embs2=self.facemodel(self.test_transform(img).to(self.devicess).unsqueeze(0)).detach().cpu().numpy().reshape(1,-1)

 
                ####comparing A,B images loss
                totalfaceloss=totalfaceloss+1-cosine_similarity(embs1,embs2)[0][0]
        myReAutoGrad=totalfaceloss
        myReAutoGrad=self.lossReAutoGrad

        min_idx, minimum=0,0


        return min_idx, minimum ,myReAutoGrad  
```
",part code generate news loss face recognition part due heavily motion image self enumerate get generate model get face get generate model get face loss return minimum,issue,negative,negative,negative,negative,negative,negative
650819357,"> Hello,
> I solved this error. Thank you so much. I have one doubt ,since my image size is 143 x 143, do I require to padd it before or if I choose the none parameter for preprocessing ,it will make it to a multpile of 4. I dont want to resize, become my medical image might loose the data. Kindly suggest !

Yes. you can pad 1 pixel by yourself before feeding it to the network.",hello error thank much one doubt since image size require choose none parameter make dont want resize become medical image might loose data kindly suggest yes pad feeding network,issue,negative,positive,positive,positive,positive,positive
650812469,"Hello,

I changed the number of workers to 0. to make this work. I guess this is because, of missing multiprocessor support on windows.So changing number of workers to 0 made it  work. ",hello number make work guess missing support number made work,issue,negative,negative,negative,negative,negative,negative
650802617,"Error Can't pickle local object 'get_transform.<locals>.<lambda>' 
Can you please tell me what can be the possible reason for this error?

Setting up a new session...
create web directory ./checkpoints\maps_cyclegan\web...
Traceback (most recent call last):
  File ""C:/Users/Surbhi/Downloads/pytorch-CycleGAN-and-pix2pix-master/train.py"", line 44, in <module>
    for i, data in enumerate(dataset):  # inner loop within one epoch
  File ""C:\Users\Surbhi\Downloads\pytorch-CycleGAN-and-pix2pix-master\data\__init__.py"", line 90, in __iter__
    for i, data in enumerate(self.dataloader):
  File ""C:\Users\Surbhi\anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 279, in __iter__
    return _MultiProcessingDataLoaderIter(self)
  File ""C:\Users\Surbhi\anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 719, in __init__
    w.start()
  File ""C:\Users\Surbhi\anaconda3\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""C:\Users\Surbhi\anaconda3\lib\multiprocessing\context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Users\Surbhi\anaconda3\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
  File ""C:\Users\Surbhi\anaconda3\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Users\Surbhi\anaconda3\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'get_transform.<locals>.<lambda>'

Process finished with exit code 1",error ca pickle local object lambda please tell possible reason error setting new session create web directory recent call last file line module data enumerate inner loop within one epoch file line data enumerate file line return self file line file line start self file line return file line return file line file line dump file protocol ca pickle local object lambda process finished exit code,issue,negative,positive,neutral,neutral,positive,positive
650800767,"Hello,
I solved this error. Thank you so much. I have one doubt ,since my image size is 143 x 143, do I require to padd it before or if I choose the none parameter for preprocessing ,it will make it to a multpile of 4. I dont want to resize, become my medical image might loose the data. Kindly suggest !",hello error thank much one doubt since image size require choose none parameter make dont want resize become medical image might loose data kindly suggest,issue,negative,positive,positive,positive,positive,positive
650744262,"Thank you,. When I run the train.py I am getting this error
usage: train.py [-h] --dataroot DATAROOT [--name NAME] [--gpu_ids GPU_IDS]
                [--checkpoints_dir CHECKPOINTS_DIR] [--model MODEL]
                [--input_nc INPUT_NC] [--output_nc OUTPUT_NC] [--ngf NGF]
                [--ndf NDF] [--netD NETD] [--netG NETG]
                [--n_layers_D N_LAYERS_D] [--norm NORM]
                [--init_type INIT_TYPE] [--init_gain INIT_GAIN] [--no_dropout]
                [--dataset_mode DATASET_MODE] [--direction DIRECTION]
                [--serial_batches] [--num_threads NUM_THREADS]
                [--batch_size BATCH_SIZE] [--preprocess PREPROCESS]
                [--no_flip] [--display_winsize DISPLAY_WINSIZE]
                [--epoch EPOCH] [--load_iter LOAD_ITER] [--verbose]
                [--suffix SUFFIX] [--display_freq DISPLAY_FREQ]
                [--display_ncols DISPLAY_NCOLS] [--display_id DISPLAY_ID]
                [--display_server DISPLAY_SERVER] [--display_env DISPLAY_ENV]
                [--display_port DISPLAY_PORT]
                [--update_html_freq UPDATE_HTML_FREQ]
                [--print_freq PRINT_FREQ] [--no_html]
                [--save_latest_freq SAVE_LATEST_FREQ]
                [--save_epoch_freq SAVE_EPOCH_FREQ] [--save_by_iter]
                [--continue_train] [--epoch_count EPOCH_COUNT] [--phase PHASE]
                [--n_epochs N_EPOCHS] [--n_epochs_decay N_EPOCHS_DECAY]
                [--beta1 BETA1] [--lr LR] [--gan_mode GAN_MODE]
                [--pool_size POOL_SIZE] [--lr_policy LR_POLICY]
                [--lr_decay_iters LR_DECAY_ITERS]
train.py: error: the following arguments are required: --dataroot

but I already added the dataroot path in base options:
parser.add_argument('--dataroot', type=str, default='./datasets/PETtoCT', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')
",thank run getting error usage name name model model norm norm direction direction epoch epoch verbose suffix suffix phase phase beta beta error following already added path base,issue,negative,negative,negative,negative,negative,negative
650649631,"We provide a dataset [template](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) with step-by-step instructions. You need to implement functions such as `__init__`, and `__getitem__`.",provide template need implement,issue,negative,neutral,neutral,neutral,neutral,neutral
649873763,"Great. Does it help in your experiments?
Maybe @zsyzzsoft can help create a PR or a branch for all the models. 
We are doing some experiments with conditional GANs and standard datasets, and considering adding them to arXiv v2. ",great help maybe help create branch conditional standard considering,issue,positive,positive,positive,positive,positive,positive
649806019,"- Reducing the identity loss might help. 
- It works better when the camera angle is similar across domains. 
- It works better if you train the model on cropped patches rather than entire image. See #586 for more details and hyper-parameters. ",reducing identity loss might help work better camera angle similar across work better train model rather entire image see,issue,positive,positive,positive,positive,positive,positive
649522190,"

> 
> 
> Thanks again. But I am very confused . The original cityscapes dataset in ""Path to the original cityscapes dataset"" exactly mean what. And I have another thing wrong.
> ![image](https://user-images.githubusercontent.com/34617934/80192475-d0151a80-8649-11ea-95eb-d3750021d3ed.png)
> 
> ```
>  File ""/root/pytorch-CycleGAN-and-pix2pix/scripts/eval_cityscapes/cityscapes.py"", line 21, in __init__
>     self.id2trainId = {label.id: label.trainId for label in labels.labels}  # dictionary mapping from raw IDs to train IDs
> AttributeError: module 'labels' has no attribute 'labels'
> ```

What language are you using to reproduce this test code? matlab? python? Lua? ",thanks confused original path original exactly mean another thing wrong image file line label dictionary raw train module attribute language reproduce test code python,issue,negative,negative,neutral,neutral,negative,negative
648095192,"Hey I am currently trying to implement the wgangp for pix2pix. I have read the previous comment in this thread, still have some parts that confuse me. I Also having the problem Trying to backward through the graph a second time, even I only call backward once.

My implementation: (I am updating D multiple times before updating G)
```python
def backward_D(self):
        """"""Calculate GAN loss for the discriminator""""""
        # Fake; stop backprop to the generator by detaching fake_B
        fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator
        pred_fake = self.netD(fake_AB.detach())
        self.loss_D_fake = self.criterionGAN(pred_fake, False)
        # Real
        real_AB = torch.cat((self.real_A, self.real_B), 1)
        pred_real = self.netD(real_AB)
        self.loss_D_real = self.criterionGAN(pred_real, True, soft_label=self.using_soft_label)
        # combine loss and calculate gradients
        # gradient penalty
        self.gradient_penalty, self.gradients = networks.cal_gradient_penalty(self.netD, real_AB, fake_AB, self.device, lambda_gp=self.L_GP)
        self.loss_D = self.loss_D_fake + self.loss_D_real + self.gradient_penalty
        self.loss_D.backward()
```

```
def optimize_parameters(self):
        self.forward()                   # compute fake images: G(A)
        
        for i in range(self.n_d): # updating D n times before updating G
            # update D
            self.set_requires_grad(self.netD, True)  # enable backprop for D
            self.optimizer_D.zero_grad()     # set D's gradients to zero
            self.backward_D()                # calculate gradients for D
            self.optimizer_D.step()          # update D's weights
        
        # update G
        self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G
        self.optimizer_G.zero_grad()        # set G's gradients to zero
        self.backward_G()                   # calculate graidents for G
        self.optimizer_G.step()             # udpate G's weights

```
I have seen the suggested implementation:

```python
self.loss_gp, gradients = cal_gradient_penalty()...
self.loss_gp.backward(retain_graph=True)
self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5
self.loss_D.backward()
```
I have 2 questions:
1. For the recommended code, why we could do the backward separately from gradient_penalty term. As we take the derivative, will the update be the same using these two approaches? 

2. When I am using `self.loss_D = self.loss_D_fake + self.loss_D_real + self.gradient_penalty`, I only call it once for every `self.backward_D() `. However, for the first update of D (i==0) the `self.backward_D()` went through. For the second update of D (i==0) I have the problem of `Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.`
I don't understand in this situation why it is counted as the second time. The second GP is actually wrt the second batch. The same story if I only do `self.loss_D = (self.loss_D_fake + self.loss_D_real)` it won't have the problem. What makes the difference between `self.loss_D_fake` and `self.gradient_penalty` that `self.loss_D_fake` does not need to retain the graph but `self.gradient_penalty` needs.",hey currently trying implement read previous comment thread still confuse also problem trying backward graph second time even call backward implementation multiple time python self calculate gan loss discriminator fake stop generator use conditional need feed input output discriminator false real true combine loss calculate gradient penalty self compute fake range time update true enable set zero calculate update update false set zero calculate seen implementation python code could backward separately term take derivative update two call every however first update went second update problem trying backward graph second time already freed specify calling backward first understand situation second time second actually second batch story wo problem difference need retain graph need,issue,negative,negative,neutral,neutral,negative,negative
648005224,"Yes it is default code and it happens for every datasets. I use continue_train and it works but only for another 36 epochs (so until 72 epoch) and the it crashes. I don't understand why.

Thank you!",yes default code every use work another epoch understand thank,issue,positive,neutral,neutral,neutral,neutral,neutral
647887734,Could you provide train_opt.txt file? I still can't achieve the ideal effect when the dataset is edges2shoes on the pix2pix.,could provide file still ca achieve ideal effect,issue,positive,positive,positive,positive,positive,positive
647194277,"> Ok. Found my error. Had to change the filter dimensions:
> 
> ```
> class UnetSkipConnectionBlock(nn.Module):
>     def __init__(self, outer_nc, inner_nc, input_nc=None,
>                  submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):
>         super(UnetSkipConnectionBlock, self).__init__()
>         self.outermost = outermost
>         self.innermost = innermost
>         if type(norm_layer) == functools.partial:
>             use_bias = norm_layer.func == nn.InstanceNorm2d
>         else:
>             use_bias = norm_layer == nn.InstanceNorm2d
>         if input_nc is None:
>             input_nc = outer_nc
>         downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,
>                              stride=2, padding=1, bias=use_bias)
>         downrelu = nn.LeakyReLU(0.2, True)
>         downnorm = norm_layer(inner_nc)
>         uprelu = nn.ReLU(True)
>         upnorm = norm_layer(outer_nc)
> 
>         if outermost:
>             upconv = UpsampleConLayer(inner_nc * 2, outer_nc, kernel_size=3, stride=1, upsample=2)      
>             #upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2,padding=1, bias=use_bias)
>             down = [downconv]
>             up = [uprelu, upconv, nn.Tanh()]
>             model = down + [submodule] + up
>         elif innermost:
>             upconv = UpsampleConLayer(inner_nc, outer_nc, kernel_size=3, stride=1, upsample=2)
>             #upconv = nn.ConvTranspose2d(inner_nc, outer_nc,kernel_size=4, stride=2,padding=1, bias=use_bias)
>             down = [downrelu, downconv]
>             up = [uprelu, upconv, upnorm]
>             model = down + up
>         else:
>             upconv = UpsampleConLayer(inner_nc * 2, outer_nc, kernel_size=3, stride=1, upsample=2)
>             #upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,kernel_size=4, stride=2,padding=1, bias=use_bias)
>             down = [downrelu, downconv, downnorm]
>             up = [uprelu, upconv, upnorm]
> 
>             if use_dropout:
>                 model = down + [submodule] + up + [nn.Dropout(0.5)]
>             else:
>                 model = down + [submodule] + up
> 
>         self.model = nn.Sequential(*model)
> 
>     def forward(self, x):
>         if self.outermost:
>             return self.model(x)
>         else:
>             return torch.cat([x, self.model(x)], 1)
> ```

I was wondering why you used kernel_size=3 instead of the original kernel_size(k=4) here.",found error change filter class self super self outermost innermost type else none true true outermost model innermost model else model else model model forward self return else return wondering used instead original,issue,negative,positive,positive,positive,positive,positive
647058517,"It might be different. If you crop a patch, your features will be normalized based on the mean/var statistics of each patch rather than the entire image. ",might different crop patch based statistic patch rather entire image,issue,negative,neutral,neutral,neutral,neutral,neutral
647057674,Yes. It is commonly used in deep learning. You can also use other initialization methods. See the flag `--init_type` and `--init_gain` for more details. ,yes commonly used deep learning also use see flag,issue,negative,negative,negative,negative,negative,negative
647057509,It was done in this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L46) `model = create_model(opt) `. It calls the `load_networks` in this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/base_model.py#L88).,done line model opt line,issue,negative,neutral,neutral,neutral,neutral,neutral
647055902,Not sure what happened. Did it happen with the default code (without newly added losses)? You can try (1) printing the image path for all the input images after epoch 36. (2) using `continue_train` to fine-tine the saved model.,sure happen default code without newly added try printing image path input epoch saved model,issue,positive,positive,positive,positive,positive,positive
645862331,"Yes, and I test the model on Macbook, I use vid2vid model.",yes test model use model,issue,negative,neutral,neutral,neutral,neutral,neutral
645361784,"Thank you so much it works now!

On Wed, Jun 17, 2020 at 5:42 PM Jun-Yan Zhu <notifications@github.com>
wrote:

> See this post
> <https://superuser.com/questions/256985/how-to-cd-to-a-directory-that-contains-a-space-in-its-name>
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1063#issuecomment-645269870>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AP47U5K3G7HHU4JERDPPAQTRXCFXNANCNFSM4N2A7FAQ>
> .
>
",thank much work wed wrote see post thread reply directly view,issue,negative,positive,positive,positive,positive,positive
645168808,"[image: image.png]
this is what i got. i copied the path to the folder that is why the there
was a space in between my drive.

On Wed, Jun 17, 2020 at 11:51 AM Jun-Yan Zhu <notifications@github.com>
wrote:

> You have an empty space between My and Drive.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1063#issuecomment-645130914>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AP47U5JRQFQ5DMEB6F6ISODRXA4UVANCNFSM4N2A7FAQ>
> .
>
",image got copied path folder space drive wed wrote empty space drive thread reply directly view,issue,negative,neutral,neutral,neutral,neutral,neutral
645029671,"hi @vict0rsch , I just had the same issue :)), solved 
basically in the ""base_options.py"" file the '--crop_size' was bigger number than '--load_size', forgot to change back after testing
I guess will be better if this parameter go to ""test_options.py"", will avoid confusion ",hi issue basically file bigger number forgot change back testing guess better parameter go avoid confusion,issue,negative,positive,positive,positive,positive,positive
644691919,Wouldn't that be just like a normal CAE with a L1 loss? I'll try it out :D,would like normal loss try,issue,negative,positive,positive,positive,positive,positive
644660015,"!python train.
py --dataroot /content/drive/My Drive/div2k --name div2k --model cycle_gan

i only have this in my command.

On Tue, Jun 16, 2020 at 1:27 PM Jun-Yan Zhu <notifications@github.com>
wrote:

> what is your training command?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/1063#issuecomment-644539041>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AP47U5LJHFCJZJVIFFGF32LRW37FBANCNFSM4N2A7FAQ>
> .
>
",python train name model command tue wrote training command thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
644587375,"!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
import os
os.chdir('pytorch-CycleGAN-and-pix2pix/')
!pip install -r requirements.txt
!python train.py --dataroot /content/drive/My Drive/div2k --name div2k --model cycle_gan",git clone import o pip install python name model,issue,negative,neutral,neutral,neutral,neutral,neutral
644539910,Increasing lambda L1 loss will not change the range of the output. I am not sure how to solve your issue. Maybe try a baseline with L1 only and no GAN loss. ,increasing lambda loss change range output sure solve issue maybe try gan loss,issue,negative,positive,positive,positive,positive,positive
644538767,"You might have used the conditional discriminator in the pix2pix setting. The conditional discriminator takes both the input and output. Therefore, the number of input channels is 2. You can either skip the weights of the first layer during network loading or training pix2pix models with unconditional discriminators. Both require some minor modification of the code. ",might used conditional discriminator setting conditional discriminator input output therefore number input either skip first layer network loading training unconditional require minor modification code,issue,negative,positive,neutral,neutral,positive,positive
644028855,"Hi, I've tried to increase L1 lambda loss (150 and 200) and no improvement was experienced... I had the same greyish colors when it should be white. Which range were you thinking in terms of increase for the L1 lambda?

PS: I do not have the mask, so I cannot use it :(",hi tried increase lambda loss improvement experienced color white range thinking increase lambda mask use,issue,positive,positive,positive,positive,positive,positive
644013624,"Thanks for your answer, I renamed the four latest_net.pth generated by the pix2pix model with `--input_nc 1 --output_nc 1` and put it in cyclegan, using the script 
```
python train.py --dataroot ./datasets/HGCycleGAN --name HG_CycleGAN --model cycle_gan --netG unet_256 --norm batch --gan_mode  vanilla --no_dropout --input_nc 1 --output_nc 1 --continue_train 
```
it does not work,The error is as follows.So how can I fix it

```
initialize network with normal
model [CycleGANModel] was created
loading the model from ./checkpoints\HG14_CycleGAN\latest_net_G_A.pth
loading the model from ./checkpoints\HG14_CycleGAN\latest_net_G_B.pth
loading the model from ./checkpoints\HG14_CycleGAN\latest_net_D_A.pth
Traceback (most recent call last):
  File ""train.py"", line 34, in <module>
    model.setup(opt)               # regular setup: load and print networks; create schedulers
  File ""E:\PycharmProjects\pytorch-CycleGAN-and-pix2pix\models\base_model.py"", line 88, in setup
    self.load_networks(load_suffix)
  File ""E:\PycharmProjects\pytorch-CycleGAN-and-pix2pix\models\base_model.py"", line 198, in load_networks
    net.load_state_dict(state_dict)
  File ""E:\anaconda3\envs\py36\lib\site-packages\torch\nn\modules\module.py"", line 847, in load_state_dict
    self.__class__.__name__, ""\n\t"".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for NLayerDiscriminator:
        size mismatch for model.0.weight: copying a param with shape torch.Size([64, 2, 4, 4]) from checkpoint, the shape in current model is torch.Size([64, 1, 4, 4]).
```",thanks answer four model put script python name model norm batch vanilla work error fix initialize network normal model loading model loading model loading model recent call last file line module opt regular setup load print create file line setup file line file line error loading size mismatch model weight param shape shape current model,issue,negative,positive,neutral,neutral,positive,positive
643983218,Thanks for your reply. My problem is a bit more complex than that the A and B only differ at certain parts. The difference exists all over the image -- it's quite like a super-resolution problem but A and B are samely high resolutional while B contains more minor structures here and there..,thanks reply problem bit complex differ certain difference image quite like problem samely high minor,issue,negative,positive,neutral,neutral,positive,positive
643929679,"Yeah will check. Thanks a lot for help :) 
Also your work and repo is really very helpful !
Thanks :)",yeah check thanks lot help also work really helpful thanks,issue,positive,positive,positive,positive,positive,positive
643884235,"Your solution may not work. Setting L1_loss to 0 will not automatically enable unpaired training. You need to use several training scripts: (1) two pix2pix training scripts: both A->B and B->A, and (2) cyclegan training script with pre-trained models from step (1). ",solution may work setting automatically enable unpaired training need use several training two training training script step,issue,negative,neutral,neutral,neutral,neutral,neutral
643881702,"For `unaligned_dataset`, we assume that there is no correspondence between input and output. We don't know the pairing information of A and B at all. Otherwise, you should use `aligned_dataset`.  Please refer to Figure 2 of the original [paper](https://arxiv.org/pdf/1703.10593.pdf) for more details. ",assume correspondence input output know information otherwise use please refer figure original paper,issue,positive,positive,positive,positive,positive,positive
643880467,Doing it during both training and inference if the mask is given as part of training data. ,training inference mask given part training data,issue,negative,neutral,neutral,neutral,neutral,neutral
643878914,"Feel free to change the number of epochs. I think the number of epochs depends on your datasets and applications, and it is hard to tell in advance.",feel free change number think number hard tell advance,issue,positive,positive,neutral,neutral,positive,positive
643830448,"If you know the mask `m`, you can modify the generation process as `G(x)*m+x*(1-m)`. ",know mask modify generation process,issue,negative,neutral,neutral,neutral,neutral,neutral
643718675,Not sure. It all depends on your application and datasets. Quite hard to predict which learning rate is the best in advance. ,sure application quite hard predict learning rate best advance,issue,positive,positive,positive,positive,positive,positive
643718602,maybe  due to `pytorch` and `torchvision` version incompatibly?,maybe due version incompatibly,issue,negative,negative,negative,negative,negative,negative
643495014,"I can second this. It has to do with the pip dependency on `git+https://github.com/pytorch/vision.git`.

Traceback:

```
(miniconda)[b@e p2p]$ conda env create -f environment.yml
Collecting package metadata (repodata.json): done
Solving environment: done
Preparing transaction: done
Verifying transaction: done
Executing transaction: done
Ran pip subprocess with arguments:
['/home/exacloud/tempwork/ChangLab/burlinge/miniconda/envs/sardana-p2p/bin/python', '-m', 'pip', 'install', '-U', '-r', '/home/exacloud/lustre1/ChangLab/burlinge/proj/SARDANA/p2p/condaenv.teozdspn.requirements.txt']
Pip subprocess output:
Collecting git+https://github.com/pytorch/vision.git (from -r ./p2p/condaenv.teozdspn.requirements.txt (line 2))
  Cloning https://github.com/pytorch/vision.git to /tmp/pip-req-build-6bpk1n6d
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-req-build-6bpk1n6d/setup.py"", line 15, in <module>
        from torch.utils.hipify import hipify_python
    ImportError: No module named 'torch.utils.hipify'
```",second pip dependency create package done environment done transaction done transaction done transaction done ran pip pip output line complete output command python recent call last file string line module file line module import module,issue,negative,positive,neutral,neutral,positive,positive
643431738,"The input and output of G_A and G_B are different. (one is horse->zebra, the other is zebra->horse). Add your loss at this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L169).",input output different one zebra horse add loss line,issue,negative,neutral,neutral,neutral,neutral,neutral
642603179,"Do you think if we play with the learning rate to higher (?) or lower(?), might affects the performance of the networks having better-generated results? else what could help for generating better outcomes? either in oix2pix or Pix2pixHD?",think play learning rate higher lower might performance else could help generating better either,issue,positive,positive,positive,positive,positive,positive
642585479,"> thanks for your reponse and I will try it.
> Another question, I think i can input the dicom image by transfer to TIFF format with 16bit RGB ou grayscale, does the core of GAN train the data with 16Bit?
> I see at the exit of GAN, to changer Tensor --> JPG, the value is multiple by 255, need I changer to 2**16-1?
> thanks à lot

Have you solve the problem? Could you tell me how to load and save DICOM image and transfer to TIFF format?",thanks try another question think input image transfer tiff format bit core gan train data bit see exit gan changer tensor value multiple need changer thanks lot solve problem could tell load save image transfer tiff format,issue,positive,positive,positive,positive,positive,positive
642357915,"Fine! Rightly after I submitted this issue, the problem is solved. I am not sure but it seems like I double checked the hyper parameters I used in the test stage and made them completely the same with those in training stage. I don't know which hyper parameter is the key point but I strongly recommend that we have to keep them the same.",fine rightly issue problem sure like double checked hyper used test stage made completely training stage know hyper parameter key point strongly recommend keep,issue,positive,positive,positive,positive,positive,positive
642243716,"> The training looks normal to me. The noise might go away if the model is trained longer. If not, you can also look at #64 

@junyanz  Thank you so much for the response ! 
#64 seems very similar to the issue I am facing in generated images as the checkerboard effect.
Will check with that solution :)

Also can you please answer one more query , do I need to change no of epochs if dataset is small ? As in original implementation suppose for horse2zebra dataset 200 epochs are used but my dataset is around 500 images for A(underwater images) and 500 for B(plastic bottles and plastic bags - not underwater).

Also since I was getting more noise in generated images I just tried testing with weights for first 10 epochs and generated images were quite smooth and identifiable compared to 100th or 200th epochs weights which were not easily identifying the plastic bottle/bag as in my dataset for B.",training normal noise might go away model trained longer also look thank much response similar issue facing checkerboard effect check solution also please answer one query need change small original implementation suppose used around underwater plastic plastic underwater also since getting noise tried testing first quite smooth identifiable th th easily plastic,issue,positive,positive,positive,positive,positive,positive
642117217,"Sure thing. I added the --no_multiprocessing flag, which when called reverts to using the single CPU script.",sure thing added flag single script,issue,negative,positive,positive,positive,positive,positive
642112766,"I will try to increase the lambda L1 loss, thanks :+1: 
About making the generator only produce pixels inside the mask... you mean doing it during the training itself, before computing the loss, or just in the inference process? Thanks ",try increase lambda loss thanks making generator produce inside mask mean training loss inference process thanks,issue,positive,positive,neutral,neutral,positive,positive
641936445,"I believe the unaligned_dataset might need some additional documentation. The A and B images read by the class are not corresponding to each other unless --serial_batches flag is passed. As the description of the flag implies that if this flag is false, batches are taken randomly, and not that A and B are taken randomly, hence its not very clear.",believe might need additional documentation read class corresponding unless flag description flag flag false taken randomly taken randomly hence clear,issue,negative,negative,negative,negative,negative,negative
641922108,"Ok, thanks a lot for your response, I will edit my code to cater for it.
However, in the original code is it a desired behavior for some reason, or just something that needs to be fixed?",thanks lot response edit code cater however original code desired behavior reason something need fixed,issue,positive,positive,positive,positive,positive,positive
641912150,"> > > It seems that 24GB can fit `480x360` images. Maybe you can further reduce the size of training images (to `256x256`).
> > 
> > 
> > This is correct. Even in a NVIDIA® Tesla® V100 32GB, it is hard to work with images which are larger than 700 by 700. I converted the code to mixed half precision (using NVIDIA Apex) which allows training on 1200 x 1200 images, and am working on gradient checkpointing and possibly model parallelism, with the goal of reaching 2000 x 2000 training (training on small resolution and generating large images seems to not work well).
> > When everything is tested and working, I can make a pull request if you think that might be helpful.
> 
> Hi!
> I know that this is kind of late, but I would be very interested in the apex version of the code. I've just started using it and it seems rather straightforward for many cases, but I just can't figure out how to initialize it on a cycleGAN where there are 4 networks (the networks.define_G is called twice and networks.define_D is also called twice) and 2 optimizers (where the input parameters are chained together via itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()) for netG and netD respectively) and the amp API calls for:
> 
> model, optimizer = amp.initialize(model, optimizer)
> 
> so I am unsure how to fit these together.
> 
> Thank you for your time!

apex support torch.nn.Module list as reference. So just like this:
`[netG_A, netG_B, netD_A, netD_B], [optimizer_g, optimizer_d] = amp.initialize([netG_A, netG_B, netD_A, netD_B], [optimizer_g, optimizer_d])`",fit maybe reduce size training correct even registered registered hard work converted code mixed half precision apex training working gradient possibly model parallelism goal reaching training training small resolution generating large work well everything tested working make pull request think might helpful hi know kind late would interested apex version code rather straightforward many ca figure initialize twice also twice input chained together via respectively model model unsure fit together thank time apex support list reference like,issue,positive,positive,positive,positive,positive,positive
641882772,"Okay, i got it.  I appreciate your help very much !!!",got appreciate help much,issue,positive,positive,positive,positive,positive,positive
641881672,"@junyanz   new a 

> > > It seems that 24GB can fit `480x360` images. Maybe you can further reduce the size of training images (to `256x256`).
> > 
> > 
> > This is correct. Even in a NVIDIA® Tesla® V100 32GB, it is hard to work with images which are larger than 700 by 700. I converted the code to mixed half precision (using NVIDIA Apex) which allows training on 1200 x 1200 images, and am working on gradient checkpointing and possibly model parallelism, with the goal of reaching 2000 x 2000 training (training on small resolution and generating large images seems to not work well).
> > When everything is tested and working, I can make a pull request if you think that might be helpful.
> 
> Hi!
> I know that this is kind of late, but I would be very interested in the apex version of the code. I've just started using it and it seems rather straightforward for many cases, but I just can't figure out how to initialize it on a cycleGAN where there are 4 networks (the networks.define_G is called twice and networks.define_D is also called twice) and 2 optimizers (where the input parameters are chained together via itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()) for netG and netD respectively) and the amp API calls for:
> 
> model, optimizer = amp.initialize(model, optimizer)
> 
> so I am unsure how to fit these together.
> 
> Thank you for your time!

could somebody reopen the issue ?   @junyanz ",new fit maybe reduce size training correct even registered registered hard work converted code mixed half precision apex training working gradient possibly model parallelism goal reaching training training small resolution generating large work well everything tested working make pull request think might helpful hi know kind late would interested apex version code rather straightforward many ca figure initialize twice also twice input chained together via respectively model model unsure fit together thank time could somebody reopen issue,issue,positive,positive,positive,positive,positive,positive
641723866,"> 1. You can use `--no_flip`. 2. Not sure about the color.

Thanks a lot!",use sure color thanks lot,issue,positive,positive,positive,positive,positive,positive
641719588,"The training looks normal to me. The noise might go away if the model is trained longer. If not, you can also look at #64 ",training normal noise might go away model trained longer also look,issue,negative,positive,positive,positive,positive,positive
641716686,"You can increase the lambda for L1 loss (`--lambda_L1`). If you know the mask, you can use the mask `m` and ask the generator to produce pixels inside the mask: `G(x)*m+x*(1-m)`.",increase lambda loss know mask use mask ask generator produce inside mask,issue,negative,neutral,neutral,neutral,neutral,neutral
641713554,This [post](https://stackoverflow.com/questions/46421663/what-are-jupyter-notebook-checkpoint-files-for?rq=1) explains the ipytnb_checkpoints. It might be caused by the creation of a iPython notebook. ,post might creation notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
641712894,Great. I am wondering if you can add a flag so that people can still use the single CPU script. ,great wondering add flag people still use single script,issue,positive,positive,positive,positive,positive,positive
641712133,"Yes, we applied different crops for `unaligned_dataset`. We applied the same crop for the `aligned_dataset`. Feel free to modify the data loader `unaligned_dataset`. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/aligned_dataset.py#L49). ",yes applied different applied crop feel free modify data loader see line,issue,positive,positive,positive,positive,positive,positive
641679735,Photorealistic image stylization was not used in the original work. But it might help you remove the artifacts. ,image stylization used original work might help remove,issue,positive,positive,positive,positive,positive,positive
641322730,"Hi @junyanz ,

I have been training cyclegan on my own dataset of around 500 images of A and 500 images of B. 
But after few epochs I see that there’s random noise added to the fake images being generated while training and after 60 epochs now it’s still having some noise in generated images.
But I see that g_loss is minimising well for 60 epochs as of now. 
Is there anything that’s going wrong in training ? 

![296AA292-470A-44F8-9D5D-C277E280E3B1](https://user-images.githubusercontent.com/24861262/84157901-e146a900-aa88-11ea-8484-5310e629eb76.jpeg)
",hi training around see random noise added fake training still noise see well anything going wrong training,issue,negative,negative,negative,negative,negative,negative
640584464,"From what it seems, I suppose that they have applied a post-processing step, namely a [guided filter](https://github.com/Sundrops/fast-guided-filter) (having the original untranslated image as guide) for removing the random artifacts. As far as I know, this approach is quite common for smoothing photorealistic translations. As an example, I first seen it used like this here,  [A Closed-form Solution to Photorealistic Image Stylization](https://arxiv.org/pdf/1802.06474.pdf), and the code is [here ](https://github.com/NVIDIA/FastPhotoStyle/blob/af0c8fecce58aa71f76488546231214f6684be02/demo.py#L29). 

Hope it helps.",suppose applied step namely filter original untranslated image guide removing random far know approach quite common smoothing example first seen used like solution image stylization code hope,issue,positive,negative,neutral,neutral,negative,negative
640439543,"> > 70
> 
> Thank you!
> But What does 'output_size' mean here?

It just means the width/height of the output feature map,
We can calculate the receptive field of the prior layer according to its output_size ",thank mean output feature map calculate receptive field prior layer according,issue,negative,negative,negative,negative,negative,negative
640282014,"If you are implementing your own cyclegan, I recommend that you do it step by step. For example, you can use part of our code base first (e.g., generator and discriminator),  and test the part you wrote (loss, training).",recommend step step example use part code base first generator discriminator test part wrote loss training,issue,negative,negative,negative,negative,negative,negative
640198234,"Thank you for your kind answer.
It was helpful 😊",thank kind answer helpful,issue,positive,positive,positive,positive,positive,positive
640168678,"per-trained model works for me ! But I'm still want to implement it in my way and adjust hyper-parameter to the differences . i found the structure in my script is a little different from yours yesterday, so i adjust them and retrain again, maybe the results would good after training !  thanks for answering :)) !!!",model work still want implement way adjust found structure script little different yesterday adjust retrain maybe would good training thanks,issue,positive,positive,positive,positive,positive,positive
640146458,Does the pre-trained model work for you? (see the model downloading [script](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_cyclegan_model.sh))  I think that we probably didn't use the identity loss. @taesungp ,model work see model script think probably use identity loss,issue,negative,neutral,neutral,neutral,neutral,neutral
640145737,"#890 might be related. The post suggested that you change the `--checkpoints_dir` and check if you have the write permission. To increase the frequency of model saving, you can set `--save_epoch_freq`.",might related post change check write permission increase frequency model saving set,issue,negative,neutral,neutral,neutral,neutral,neutral
639486601,"> > It seems that 24GB can fit `480x360` images. Maybe you can further reduce the size of training images (to `256x256`).
> 
> This is correct. Even in a NVIDIA® Tesla® V100 32GB, it is hard to work with images which are larger than 700 by 700. I converted the code to mixed half precision (using NVIDIA Apex) which allows training on 1200 x 1200 images, and am working on gradient checkpointing and possibly model parallelism, with the goal of reaching 2000 x 2000 training (training on small resolution and generating large images seems to not work well).
> 
> When everything is tested and working, I can make a pull request if you think that might be helpful.

Hi!
I know that this is kind of late, but  I would be very interested in the apex version of the code. I've just started using it and it seems rather straightforward for many cases, but I just can't figure out how to initialize it on a cycleGAN where there are 4 networks (the networks.define_G is called twice and networks.define_D is also called twice) and 2 optimizers (where the input parameters are chained together via itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()) for netG and netD respectively) and the amp API calls for:

model, optimizer = amp.initialize(model, optimizer)

so I am unsure how to fit these together.

Thank you for your time!",fit maybe reduce size training correct even registered registered hard work converted code mixed half precision apex training working gradient possibly model parallelism goal reaching training training small resolution generating large work well everything tested working make pull request think might helpful hi know kind late would interested apex version code rather straightforward many ca figure initialize twice also twice input chained together via respectively model model unsure fit together thank time,issue,positive,positive,positive,positive,positive,positive
639224783,"Thank you for answer.
I ran the pytorch code on google colab and trained it using the horse2zebra dataset.
I tried training using another dataset.
The apple2orange dataset took 320 seconds in 1 epoch, which took about 16 hours in 200 epoch. However, in the case of colab, the session ends after 12 hours. Checkpoints were saved in the Jupyter notebook used locally, but the trained model isn't saved in the checkpoints in colab. I referenced this [QnA list](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#can-i-continueresume-my-training-350-275-234-87 ) and other issues on this GitHub, but I couldn't get a solution.

![image](https://user-images.githubusercontent.com/47182864/83831098-a3145700-a721-11ea-87ff-b44f5130cf75.png)

![image](https://user-images.githubusercontent.com/47182864/83831137-b58e9080-a721-11ea-85f3-41a751ad5358.png)


How can I save checkpoints in colab?
Thank you.",thank answer ran code trained tried training another took epoch took epoch however case session saved notebook used locally trained model saved list could get solution image image save thank,issue,positive,neutral,neutral,neutral,neutral,neutral
639107454,"Which colab are you using? Which dataset? 7 hours on 50 epoches seems to be reasonable. 
CPU training is plausible but it will be extremely slow.",reasonable training plausible extremely slow,issue,negative,positive,positive,positive,positive,positive
639101043,"During test time, the order is kept by default. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L42).",test time order kept default see line,issue,negative,neutral,neutral,neutral,neutral,neutral
638741652,"Ah I see. Thank you very very much :)
I will do my task for few days and if I got some new problem then would I contact you again?
Thank you!",ah see thank much task day got new problem would contact thank,issue,negative,positive,positive,positive,positive,positive
638689120,"Q1. Yes. You can also use `--direction` to specify A->B or B->A.
Q2. For CycleGAN, the numbers do not have to be the same. ",yes also use direction specify,issue,negative,neutral,neutral,neutral,neutral,neutral
638638160,"I am really thank you for quick response. I clicked the URL which you provided, and I have another question about that code.

Q1. I understood that data_A is transferred by targeting data_B. Is this correct? (If it is correct, in my task, data_A will be KITTI and data_B will be Nuscene data. Is it right?)
Q2. The second question is, should the number of data_B and data_A be the same?

Thank you for your kindness response.
",really thank quick response provided another question code understood transferred correct correct task data right second question number thank kindness response,issue,positive,positive,positive,positive,positive,positive
638548565,You may need to implement a custom data loader for KITTI dataset. See this template [class](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) for an example.,may need implement custom data loader see template class example,issue,negative,neutral,neutral,neutral,neutral,neutral
638033932,"> Can you tell me which line in the code represents patchGAN?

It lies in [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L538). 538th line in the networks.py",tell line code th line,issue,negative,neutral,neutral,neutral,neutral,neutral
637972118,"> Not sure. Maybe you can print the shape of the tensor after each layer, and compare your implementation and ours.

Thanks for your reply. I resolved it by using sequential model when building custom model.
It seems like using sequential model can reducing memory usage.",sure maybe print shape tensor layer compare implementation thanks reply resolved sequential model building custom model like sequential model reducing memory usage,issue,positive,positive,positive,positive,positive,positive
637685810,"Not sure. Maybe you can print the shape of the tensor after each layer, and compare your implementation and ours.  ",sure maybe print shape tensor layer compare implementation,issue,negative,positive,positive,positive,positive,positive
637247535,`resnet_block9` can work for 400x400 images. But using random cropping with smaller patches can prevent overfitting. ,work random smaller prevent,issue,negative,negative,negative,negative,negative,negative
637230762,"You can change the network according to your application. Here is the [model](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/5fd11c27a1b9a5d79ef4ab879146c6b065f6e2a8/models/networks.py#L315). 
1. No. The current dropout flag is applied to many layers. 
2. The entire network. I recommend reflection padding. 
3. The last year has a reflection padding layer according to this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/5fd11c27a1b9a5d79ef4ab879146c6b065f6e2a8/models/networks.py#L365). 

For overfitting, you can increase the `--lambda_identity`. It partially alleviated the issue. ",change network according application model current dropout flag applied many entire network recommend reflection padding last year reflection padding layer according line increase partially issue,issue,positive,positive,neutral,neutral,positive,positive
636828855,"For this research paper, [unpaired image translation](https://arxiv.org/pdf/1703.10593.pdf) 

![Capture](https://user-images.githubusercontent.com/49679122/83407192-cdb69500-a42d-11ea-96aa-4c186ee2d620.JPG)

Hello Sir,  I am trying with dataset **cezanne2photo** ([avaiable](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/)) with image resizing (128x128) from 256x256 (due to limit resource available). It seems that its discriminator gets overfitting. So questions based on the snapshot i have included.
 In generative model
1. Can you specify in which layer should I used dropout layer with dropout rate value?
2. Did you apply reflect padding with in the all 6 resnet blocks only or in a whole network (rather than zero padding)?
3. at the last layer, c7s1-3, did you apply padding or any activation (in actual it is ReLU)? if yes please specify? If not, then the image has to be normalized rather than standardization as ReLU activation will neglected negative values when perform standardization.

In discriminative model,
1. what was the kernel size, padding, stride and activation in the last layer after c512?

I mean after I have done several hyperparameter and testing, its always overfitting.Its was so frustrating. So, kindly can you show the actual architecture with more specific parameters (not from code). I also follow CycleGAN present in tensorflow about data augmentation.
Still resulting are not satisfying.

Thanks",research paper unpaired image translation capture hello sir trying image due limit resource available discriminator based snapshot included generative model specify layer used dropout layer dropout rate value apply reflect padding whole network rather zero padding last layer apply padding activation actual yes please specify image rather standardization activation negative perform standardization discriminative model kernel size padding stride activation last layer mean done several testing always kindly show actual architecture specific code also follow present data augmentation still resulting satisfying thanks,issue,positive,positive,neutral,neutral,positive,positive
636716409,"Hi, thank your for replying on this! 

Do you mean, ""geometrically aligned"" as pairs of images? 
The real_A is simulated cosmic rays, real_B is real cosmic rays in the detector. 
So they look alike, but essentially different.

I see the fake images are denser and way more localized... 
Is there any possibility that resnet_block9 or the image-viewer is not suitable for 400x400 dimension? 
",hi thank mean geometrically cosmic real cosmic detector look alike essentially different see fake way possibility suitable dimension,issue,negative,negative,neutral,neutral,negative,negative
636630346,"> 1 and 2: Yes.
> 
> 1. Maybe try to set `--lambada_identity 0`. This will encourage the model to make more changes.

Thank you so much for your quick reply. I will have a try. Thanks~",yes maybe try set encourage model make thank much quick reply try,issue,positive,positive,positive,positive,positive,positive
636600824,"1 and 2: Yes. 

3. Maybe try to set `--lambada_identity 0`. This will encourage the model to make more changes. ",yes maybe try set encourage model make,issue,positive,neutral,neutral,neutral,neutral,neutral
636590618,Your input tensor might not be processed correctly. See this [post](https://discuss.pytorch.org/t/runtimeerror-given-groups-1-weight-of-size-64-3-7-7-expected-input-3-1-224-224-to-have-3-channels-but-got-1-channels-instead/30153) for an example. ,input tensor might correctly see post example,issue,negative,neutral,neutral,neutral,neutral,neutral
636589580,It might be better to take different images. But the difference should be quite small. You can also just increase the learning rate for D.,might better take different difference quite small also increase learning rate,issue,positive,positive,neutral,neutral,positive,positive
636588451,"I am wondering if your output is geometrically aligned with the input or not. If not, it might be difficult for CycleGAN to learn the mapping. As a side note, maybe you could try smaller `--crop_size` (e.g., 256 or 128). ",wondering output geometrically input might difficult learn side note maybe could try smaller,issue,negative,negative,negative,negative,negative,negative
636586135,I mean multiplying the loss tensor by the mask. ,mean multiplying loss tensor mask,issue,negative,negative,negative,negative,negative,negative
636442512,"I faced the same issue. But I added ""--no_dropout"" when I tested,  the issue was gone. As follows:
python test.py --no_dropout ",faced issue added tested issue gone python,issue,negative,neutral,neutral,neutral,neutral,neutral
636326190,"hello @junyanz , can you tell me what is this **.ipynb_checkpoints** folder in PyTorch-CycleGAN-and-pix2pix/results/face_new6_pix2pix/test_latest/images???? why it is being created and what is the use of this?",hello tell folder use,issue,negative,neutral,neutral,neutral,neutral,neutral
636313367,"@hyecheol123  
Thanks for your reply and plz tell me if your PR submit. My own project based on pix2pix and I hope calculate L1 loss when testing. ActuIly I don't care about discriminator loss when I translate the image using latest generator. ",thanks reply tell submit project based hope calculate loss testing care discriminator loss translate image latest generator,issue,positive,positive,positive,positive,positive,positive
636308066,"@zhouyingji
As I forked and modified a lot of irrelevant parts of the original code (this repository) for my own project, I decided to fork again this repo and only fix the relevant part of the code to get the loss values on testing time before I submit a PR.

Sorry for being late due to my spring semester's final and the start of the summer session. I promise it won't take much time as I already finished that the code is working properly (only for pix2pix, though).",forked lot irrelevant original code repository project decided fork fix relevant part code get loss testing time submit sorry late due spring semester final start summer session promise wo take much time already finished code working properly though,issue,negative,negative,neutral,neutral,negative,negative
636306846,"> I found the reason why.
> I did not specify the network name ['G', 'D'] when I initialize the network. I only use ['G'], and I successfully fixed it.
> Once I finish updating my project's code, then I will work on modifying the model based on this code as my project code has been modified a lot (removed unnecessary part from the original code).
> It may take some time to give you a new PR regarding this issue, but I will do it as soon as I can.

If your code finish? Where I can find your L1 loss calculate code? Thank you! ",found reason specify network name initialize network use successfully fixed finish project code work model based code project code lot removed unnecessary part original code may take time give new regarding issue soon code finish find loss calculate code thank,issue,positive,positive,neutral,neutral,positive,positive
636298978,"> From the train.py --help output:
>   --n_epochs N_EPOCHS   number of epochs with the initial learning rate
>                         (default: 100)
>   --n_epochs_decay N_EPOCHS_DECAY
>                         number of epochs to linearly decay learning rate to
>                         zero (default: 100)
> 
> So in your case, python train.py --n_epochs 300 --n_epochs_decay 100

Get it! Thank you!",help output number initial learning rate default number linearly decay learning rate zero default case python get thank,issue,negative,neutral,neutral,neutral,neutral,neutral
636124883,"From the `train.py --help` output: 

```
  --n_epochs N_EPOCHS   number of epochs with the initial learning rate
                        (default: 100)
  --n_epochs_decay N_EPOCHS_DECAY
                        number of epochs to linearly decay learning rate to
                        zero (default: 100)
```

So in your case, `python train.py --n_epochs 300 --n_epochs_decay 100`",help output number initial learning rate default number linearly decay learning rate zero default case python,issue,negative,neutral,neutral,neutral,neutral,neutral
636045312,"Thank you. I've also recently gone to my next stage, but increasing the training set did seem to help a bit. I seemed to be getting a lot more horizontal lines in the later epochs, so increasing the training set seemed to delay that even further. I ended up just using the images in the earlier epochs, with less lines.",thank also recently gone next stage increasing training set seem help bit getting lot horizontal later increasing training set delay even ended le,issue,negative,neutral,neutral,neutral,neutral,neutral
635937443,I redownloaded and ran the code again. It worked properly this time. ,ran code worked properly time,issue,negative,neutral,neutral,neutral,neutral,neutral
635205287,Multiply the mask in the sense of multiplying the ground truth with predictions? Is it?,multiply mask sense multiplying ground truth,issue,negative,neutral,neutral,neutral,neutral,neutral
634879941,"1. Not sure how to further improve the quality. Maybe you can try different generators such as `--netG resnet_6blocks` or `--netG resnet_9blocks`. But there is no guarantee. 
2. If your training data only contains faces of one person or a few persons, it may not work for your face. 2. Also for your new test data, you need to align and crop it before feeding to the network. 3. For a single test image, you need to use `--model test`. See this [script](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/test_single.sh) for a reference.  ",sure improve quality maybe try different guarantee training data one person may work face also new test data need align crop feeding network single test image need use model test see script reference,issue,positive,positive,positive,positive,positive,positive
634438966,"Still getting bad results after --load_size 143 --crop_size 128 .
Training command :python3 train.py --dataroot ./datasets/combine_faces/ --name face_new12_pix2pix --model pix2pix --direction AtoB --netG unet_128 --n_epochs 200 --n_epochs_decay 200 --preprocess none --load_size 143 --crop_size 128

 and one more thing ,the model only converts rgb to thermal for only the **test folder images**  in combine_faces, but when I am giving random images of mine or someone else's  after resizing it to the trained data dimension , the model fails miserably.
**Fake B**
![1004_fake_B](https://user-images.githubusercontent.com/65701365/82981287-52a64680-a009-11ea-88c5-e762671507ff.png)
**real A**
![1004_real_A](https://user-images.githubusercontent.com/65701365/82981290-53d77380-a009-11ea-957e-14f57a0e3a4d.png)
**real B**
![1004_real_B](https://user-images.githubusercontent.com/65701365/82981292-54700a00-a009-11ea-89fd-1046c37b702a.png)
![1005_fake_B](https://user-images.githubusercontent.com/65701365/82981293-5508a080-a009-11ea-99e8-6435979ce765.png)
![1005_real_A](https://user-images.githubusercontent.com/65701365/82981295-5508a080-a009-11ea-85e5-f59ba035f745.png)
![1005_real_B](https://user-images.githubusercontent.com/65701365/82981296-55a13700-a009-11ea-86e8-1918558597f3.png)
![1001_fake_B](https://user-images.githubusercontent.com/65701365/82981297-5639cd80-a009-11ea-907d-d2501a0b1885.png)
![1001_real_A](https://user-images.githubusercontent.com/65701365/82981299-5639cd80-a009-11ea-809c-aa85f179f8a6.png)
![1001_real_B](https://user-images.githubusercontent.com/65701365/82981301-56d26400-a009-11ea-957e-199e00fd3977.png)
![1002_fake_B](https://user-images.githubusercontent.com/65701365/82981302-576afa80-a009-11ea-9710-c960579b882b.png)
![1002_real_A](https://user-images.githubusercontent.com/65701365/82981303-58039100-a009-11ea-853e-229086b6ca05.png)
![1002_real_B](https://user-images.githubusercontent.com/65701365/82981305-58039100-a009-11ea-8123-5b7677ee3cad.png)
![1003_fake_B](https://user-images.githubusercontent.com/65701365/82981306-589c2780-a009-11ea-9812-38e717a47d29.png)
![1003_real_A](https://user-images.githubusercontent.com/65701365/82981309-5934be00-a009-11ea-879d-8e5a3026be79.png)
![1003_real_B](https://user-images.githubusercontent.com/65701365/82981310-5934be00-a009-11ea-924a-fa7717d48531.png)
 @junyanz please guide.
",still getting bad training command python name model direction none one thing model thermal test folder giving random mine someone else trained data dimension model miserably fake real real please guide,issue,negative,negative,negative,negative,negative,negative
634400656,You can multiply the loss tensor with the given mask in this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/8cda06f7c36b012769efac63adc1a68586b8fb85/models/networks.py#L275).,multiply loss tensor given mask line,issue,negative,neutral,neutral,neutral,neutral,neutral
633945084,Thanks for your suggestion. Can you just explain modifying the loss function part?,thanks suggestion explain loss function part,issue,negative,positive,positive,positive,positive,positive
633823948,You can try both. Pix2pixHD might have different naming conversions for these flags.  ,try might different naming,issue,negative,neutral,neutral,neutral,neutral,neutral
633756502,"Training the model on the full image is better, as the generator and discriminator can see its surrounding context. If you only have a few images, you need more data argumentation such as cropping (e.g., '--load_size 512 crop_size 256`) and flipping. You can even add your custom argumentation using a 3rd party [library](https://github.com/aleju/imgaug). If you have your annotation on the cropped patches, maybe you can modify the loss and only calculate the loss on the annotated regions.  ",training model full image better generator discriminator see surrounding context need data argumentation even add custom argumentation party library annotation maybe modify loss calculate loss,issue,negative,positive,positive,positive,positive,positive
633755186,"You can use the default parameters. I also recommend that you use random cropping (e.g., `--load_size 143 --crop_size 128` and flipping. ",use default also recommend use random,issue,negative,negative,negative,negative,negative,negative
633754086,The super-res network is often different from the image-to-image translation network. I recommend that you use the super-res network as the generator. A basic [version](https://github.com/eriklindernoren/PyTorch-GAN) is included in the PyTorch examples. ,network often different translation network recommend use network generator basic version included,issue,negative,neutral,neutral,neutral,neutral,neutral
633400134,"According to my previous mention, larger training set seems to have helped in solving the issue of the white horizontal line at that time. But honestly, at that time, I was immersed to go to the next step in my analysis so it is not certain whether the larger training set was key among many tries I did. Let me know your solution later. Thank you.",according previous mention training set issue white horizontal line time honestly time go next step analysis certain whether training set key among many let know solution later thank,issue,positive,positive,positive,positive,positive,positive
633382330,@lucid0921 Hello. I'm also getting white horizontal stripes in my generated images. May I ask if you solved the issue by increasing the batch size? Or was it with more training data?,lucid hello also getting white horizontal may ask issue increasing batch size training data,issue,negative,neutral,neutral,neutral,neutral,neutral
633179354,"thanks for the reply @junyanz sir , I have 1400 rgb and thermal image pair 128*128 pixel. will it be ok for pix2pix hd or spade? any tips for parameter tuning while training?
",thanks reply sir thermal image pair spade parameter tuning training,issue,negative,positive,positive,positive,positive,positive
633159567,I don't have a clue. Have you used the same dropout flag `--no_dropout` during training and test? ,clue used dropout flag training test,issue,negative,neutral,neutral,neutral,neutral,neutral
633158666,"If you have many training images, you can try [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) or [SPADE](https://github.com/NVlabs/SPADE). ",many training try spade,issue,negative,positive,positive,positive,positive,positive
633158363,"1. `--num_test` specifies the number of images. The default is 50. But if your dataset has fewer than 50 images, it will only test all the images in the dataset. `ntest` is a legacy flag. I will remove it in the next commit. 
2. It currently only works on single GPUs, as we hard-code some parameters. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L40). You can change them according to your preference. 
3. The test code hard-codes `--batch_size 1`.  We haven't tested a bigger batch_size. ",number default test legacy flag remove next commit currently work single see change according preference test code tested bigger,issue,negative,negative,neutral,neutral,negative,negative
633157268,"For auto-encoder training, you don't need CycleGAN or cycle-consistency loss. You can just add a reconstruction loss between your input image and reconstruction image. ",training need loss add reconstruction loss input image reconstruction image,issue,negative,neutral,neutral,neutral,neutral,neutral
632980468,"see this issue [＃405](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/405 ""Error connecting to Visdom server""). 
open  visidom first  :  python -m visdom.server
or add "" --display_id 0 ""  in train instructions
",see issue error server open first python add train,issue,negative,positive,positive,positive,positive,positive
632128911,I still have this issue even after giving --netG and --norm. Do you know why this problem is occuring,still issue even giving norm know problem,issue,negative,neutral,neutral,neutral,neutral,neutral
631522706,"> What is a conditional reconstruction? Do you mean auto-encoder?

Sorry for clarifying not clearly. 

Yes, as you said, conditional GAN is like an auto-encoder here. Conditional GAN conditions on a celeba image (right image) and generates its reconstruction (left image). 




",conditional reconstruction mean sorry clearly yes said conditional gan like conditional gan image right image reconstruction left image,issue,positive,negative,neutral,neutral,negative,negative
631170005,"> I don't know. What are your input and output?

Thanks for your reply.
The input is an celeba image (the right image). The output is its conditional reconstruction (the left image). ",know input output thanks reply input image right image output conditional reconstruction left image,issue,negative,positive,positive,positive,positive,positive
630999210,"Not sure. Sometimes, the source and target domain may have different object statistics. To prevent bigger changes, you can add an identity loss. ",sure sometimes source target domain may different object statistic prevent bigger add identity loss,issue,negative,positive,positive,positive,positive,positive
629924714,You probably need to implement your custom data loader. You can use the `template_dataset.py` and read the comments. The [overview](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/overview.md) is also a good starting point. ,probably need implement custom data loader use read overview also good starting point,issue,negative,positive,positive,positive,positive,positive
629924098,"Maybe just cite the original paper. If you want, you can add a footnote and include this repo's link. ",maybe cite original paper want add footnote include link,issue,negative,positive,positive,positive,positive,positive
629682620,"I think the padding was a holdover from the DCGAN architecture. I can't remember if there is a good reason for it. Might have been to make a 256x256 input map to a 1x1 output, in the DCGAN discriminator.

Zero padding also has the effect that it helps localize where you are in the image, since you can see this border of zeros when you are near an image boundary. That can sometimes be beneficial.",think padding holdover architecture ca remember good reason might make input map output discriminator zero padding also effect localize image since see border near image boundary sometimes beneficial,issue,positive,positive,positive,positive,positive,positive
629681691,Why is a padding of 1 being used in every convolution in the discriminator? If we feed the discriminator an image of size 70x70 we get an output of 6x6. Wouldn't it make more sense to not use a padding and instead get one single output 1x1 for a 70x70 input?,padding used every convolution discriminator feed discriminator image size get output would make sense use padding instead get one single output input,issue,negative,negative,neutral,neutral,negative,negative
629574991,It should be the same. The D is calculated separately as we can implement a common function `backward_D_basic` and call it twice. ,calculated separately implement common function call twice,issue,negative,negative,negative,negative,negative,negative
629574752,It can be both. Make sure that your training inputs' distribution is similar to test inputs' distribution. ,make sure training distribution similar test distribution,issue,negative,positive,positive,positive,positive,positive
629564666,Thank you @junyanz. When you say that I need more training images do you mean just a larger area? Or the same area with different snow distributions? ,thank say need training mean area area different snow,issue,negative,negative,negative,negative,negative,negative
628797977,"1. By performance, I assume that you mean image quality. batch_size=1 works quite well, and a big batch size works quite well (like 16 or 32). A smaller batch size (2 or 4) might not be helpful. 
2. There is no clear winner. But our batch normalization is not synchronized across GPUs. If you want to use batch norm with multiple GPUs, you need to adopt this [repo](https://github.com/vacancy/Synchronized-BatchNorm-PyTorch). A recent work StyleGANv2 also shows that normalization might hurt the performance. 
3. They are equally distributed. 
",performance assume mean image quality work quite well big batch size work quite well like smaller batch size might helpful clear winner batch normalization synchronized across want use batch norm multiple need adopt recent work also normalization might hurt performance equally distributed,issue,positive,negative,neutral,neutral,negative,negative
628613479,"I did try experimenting with --num_threads. But I did not see any significant boost. So I guess the default --num_threads 4 is working fine. Thanks a lot for your insights.
Also, eventually I would be running my experiments in multi-gpu mode. 
1. Is performance on a single GPU better than performance on multi-GPUs?
2. In your experience does instance normalization work best in such a scenario? 
3. No matter what --batch_size is given, the first GPU will always be loaded to the maximum extent possible and then the next GPU will take over? Or, all the images in --batch_size will be evenly distributed among the GPUs?",try see significant boost guess default working fine thanks lot also eventually would running mode performance single better performance experience instance normalization work best scenario matter given first always loaded maximum extent possible next take evenly distributed among,issue,positive,positive,positive,positive,positive,positive
628373353,"You will get a small improvement but not huge, as you are still using a single GPU. You may also want to check the data loading time and see if it is a bottleneck. You can increase the `--num_threads` to reduce data loading time. ",get small improvement huge still single may also want check data loading time see bottleneck increase reduce data loading time,issue,positive,positive,neutral,neutral,positive,positive
628372030,"Not sure how the images in `test_AB\train` are generated.  For the second screenshot, the pix2pix program saved the input and output images to the disk. You can open the HTML `index.html` to see these images. ",sure second program saved input output disk open see,issue,positive,positive,positive,positive,positive,positive
628101520,"I see. But I do not see significant improvement in 'Time Taken' for each epoch by increasing batch size.
--batch_size 1: Avg. 'Time Taken'/epoch - 1382s
--batch_size 2: Avg. 'Time Taken'/epoch - 1443s
--batch_size 2 --norm batch: Avg. 'Time Taken'/epoch - 1278s
--batch_size 4: Avg. 'Time Taken'/epoch - 1393s
--batch_size 4 --norm batch: Avg. 'Time Taken'/epoch - 1215s
Is this expected?",see see significant improvement taken epoch increasing batch size norm batch norm batch,issue,positive,positive,positive,positive,positive,positive
627943428,"I found the reason why.
I did not specify the network name ['G', 'D'] when I initialize the network. I only use ['G'], and I successfully fixed it.

Once I finish updating my project's code, then I will work on modifying the model based on this code as my project code has been modified a lot (removed unnecessary part from the original code).

It may take some time to give you a new PR regarding this issue, but I will do it as soon as I can.",found reason specify network name initialize network use successfully fixed finish project code work model based code project code lot removed unnecessary part original code may take time give new regarding issue soon,issue,positive,positive,neutral,neutral,positive,positive
627936878,I checked the backward part and select the code that I needed for calculating the loss.,checked backward part select code calculating loss,issue,negative,neutral,neutral,neutral,neutral,neutral
627936512,"@junyanz I try to calculate discriminator loss, but when I enable discriminator loss calculation, it makes weird testing results image (Get image with random noise).

Can you help me with how to handle this problem??

```
        # Calculate L1 loss while testing if needed
        if not(self.isTrain):
            if self.opt.test_loss == 1:
                self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1
                # Calculate Discriminator Loss & GAN Loss
                fake_AB = torch.cat((self.real_A, self.fake_B), 1)
                pred_fake = self.netD(fake_AB.detach())
                self.loss_D_fake = self.criterionGAN(pred_fake, False)
                real_AB = torch.cat((self.real_A, self.real_B), 1)
                pred_real = self.netD(real_AB)
                self.loss_D_real = self.criterionGAN(pred_real, True)
                pred_fake = self.netD(fake_AB)
                self.loss_G_GAN = self.criterionGAN(pred_fake, True)
```",try calculate discriminator loss enable discriminator loss calculation weird testing image get image random noise help handle problem calculate loss testing calculate discriminator loss gan loss false true true,issue,negative,negative,negative,negative,negative,negative
627729799,The iteration counter takes the `bach_size` as consideration. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/train.py#L76). Feel free to change it. ,iteration counter consideration see line feel free change,issue,positive,positive,positive,positive,positive,positive
627567409,The default should be good. Check out Sec 7.1 Training details in the original [paper](https://arxiv.org/pdf/1703.10593.pdf). ,default good check sec training original paper,issue,positive,positive,positive,positive,positive,positive
627556636,I believe in WGAN-GP you are not supposed to use any normalization in the Discriminator/Critic. Has anyone tried this with normalization in the Generator and not in the Discriminator?,believe supposed use normalization anyone tried normalization generator discriminator,issue,negative,neutral,neutral,neutral,neutral,neutral
627303014,"Thank you. I got it.&nbsp;But the accuracy is not good enough. What do I do if I want to improve the accuracy?




------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;""Jun-Yan Zhu""<notifications@github.com&gt;;
发送时间:&nbsp;2020年5月12日(星期二) 凌晨1:08
收件人:&nbsp;""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com&gt;;
抄送:&nbsp;""沉默的企鹅""<1346265656@qq.com&gt;;""Mention""<mention@noreply.github.com&gt;;
主题:&nbsp;Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to see pictures in Google clab? (#1017)





 
Have you checked if ./checkpoints/horse2zebra_pretrained/latest_net_G.pth exist in the file system?
 
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",thank got accuracy good enough want improve accuracy mention mention see checked exist file system reply directly view,issue,positive,positive,positive,positive,positive,positive
627099016,"I used '!python train.py --dataroot ./datasets/hanjie/ --name horse2zebra --checkpoints_dir /content/pytorch-CycleGAN-and-pix2pix/checks --model cycle_gan'
to get documents.
My papers are all in 'checks',there are no 'checkpoints' before ran test.py.


Now I get 'latest_net_G_A.path' and have changed it to 'latest_net_G.path' , but it was in 'checks' and also 'No such file or directory: './checkpoints/hanjie_pretrained/latest_net_G.pth' .&nbsp;
If I should change something to read papers from 'checks'?


------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;""Jun-Yan Zhu""<notifications@github.com&gt;;
发送时间:&nbsp;2020年5月12日(星期二) 凌晨1:08
收件人:&nbsp;""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com&gt;;
抄送:&nbsp;""沉默的企鹅""<1346265656@qq.com&gt;;""Mention""<mention@noreply.github.com&gt;;
主题:&nbsp;Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to see pictures in Google clab? (#1017)





 
Have you checked if ./checkpoints/horse2zebra_pretrained/latest_net_G.pth exist in the file system?
 
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",used python name model get ran get also file directory change something read mention mention see checked exist file system reply directly view,issue,negative,positive,neutral,neutral,positive,positive
626833525,Which kinds of intermediate results are you referring to? The network only takes an image as input and produces the final result. There are no intermediate image results predicted by the network. ,intermediate network image input final result intermediate image network,issue,negative,neutral,neutral,neutral,neutral,neutral
626832410,Yes. It is normalized by the size of the image. The GAN loss is also an average rather than a sum. Most of the papers implement the norm as an average rather than a sum so that the image size does not matter. ,yes size image gan loss also average rather sum implement norm average rather sum image size matter,issue,negative,negative,negative,negative,negative,negative
626831176,Have you checked if `./checkpoints/horse2zebra_pretrained/latest_net_G.pth` exist in the file system?,checked exist file system,issue,negative,neutral,neutral,neutral,neutral,neutral
626637895,"I trained my pictures with out '!bash ./datasets/download_cyclegan_dataset.sh horse2zebra' and 
'!bash ./scripts/download_cyclegan_model.sh horse2zebra'. And then I got it.



The 'checks' is made by myself. I also created folders to upload the data and it named horse2zebra.
The error occurred when I ran the test.py.


FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/horse2zebra_pretrained/latest_net_G.pth'


I want to know what I need to change.
By the way, could I ask you some issues by Chinese? My English is not good.


Thank you for your help.




------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;""Jun-Yan Zhu""<notifications@github.com&gt;;
发送时间:&nbsp;2020年5月8日(星期五) 凌晨1:16
收件人:&nbsp;""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com&gt;;
抄送:&nbsp;""沉默的企鹅""<1346265656@qq.com&gt;;""Mention""<mention@noreply.github.com&gt;;
主题:&nbsp;Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to see pictures in Google clab? (#1017)





 
@YXD123450 You can change both lines to 20.
 
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",trained bash bash got made also data error ran file directory want know need change way could ask good thank help mention mention see change reply directly view,issue,positive,positive,positive,positive,positive,positive
626356611,"> @derekahuang I observed that for some epochs the corresponding images had large number of such patches, but for some epochs, those were comparatively less. Reducing the learning rate of the discriminator helped in a small way to reduce those artefacts.
> I think those artefacts occur when the discriminator gets too powerful, i.e., discriminator achieves approx 100% accuracy.
The same problem, have you solve it?
",corresponding large number comparatively le reducing learning rate discriminator small way reduce think occur discriminator powerful discriminator accuracy problem solve,issue,negative,positive,neutral,neutral,positive,positive
626244571,"See this [post](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/190) on checkboard artifacts. Sometimes it will disappear after many training epochs. If it still exists, you can consider replacing `ConvTranspose2d` with `Upsample` and `Conv2d`. You can also try some conv blocks from [StyleGANv2](https://github.com/NVlabs/stylegan2). ",see post sometimes disappear many training still consider also try,issue,negative,positive,positive,positive,positive,positive
626074204,Hi. Did you ever solve this problem?,hi ever solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
625912412,"Hard to know the reason without seeing the script, data, and results. ",hard know reason without seeing script data,issue,negative,negative,negative,negative,negative,negative
625634593,"谢谢！Thanks!


发自我的iPhone

------------------ Original ------------------
From: Jun-Yan Zhu <notifications@github.com&gt;
Date: 周五,5月 8,2020 1:16 上午
To: junyanz/pytorch-CycleGAN-and-pix2pix <pytorch-CycleGAN-and-pix2pix@noreply.github.com&gt;
Cc: YXD123450 <1346265656@qq.com&gt;, Mention <mention@noreply.github.com&gt;
Subject: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to see pictures in Google clab? (#1017)





 
@YXD123450 You can change both lines to 20.
 
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",original date mention mention subject see change reply directly view,issue,negative,positive,positive,positive,positive,positive
625589517,"> `visdom`可能不适用于Google Colab。您可以在查看保存的结果`./checkpoints/[your_experiment_name]/web/index.html`。

我在pix2pix的训练部分将!python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA改成了!python train.py --dataroot ./datasets/facades --name facades_pix2pix --checkpoints_dir/content/pytorch-CycleGAN-and-pix2pix/checks --model pix2pix --direction BtoA ，为什么在下面报错说train.py: error: unrecognized arguments: --checkpoints_dir/content/pytorch-CycleGAN-and-pix2pix/checks
",python name model direction python name model direction error unrecognized,issue,negative,neutral,neutral,neutral,neutral,neutral
625405717,"I am using resnet_9block and norm as batch. I have used the same during the training also.
",norm batch used training also,issue,negative,neutral,neutral,neutral,neutral,neutral
625403827,See this [Q&A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#runtimeerror-errors-in-loading-state_dict-812-671461-296). Make sure that you specify the `--netG` and `--norm`.,see make sure specify norm,issue,negative,positive,positive,positive,positive,positive
625297001,"My full traceback is as such
         aspect_ratio: 1.0                          
               batch_size: 8                            
          checkpoints_dir: ./checkpoints                
                crop_size: 384                          
                 dataroot: ./datasets/sysuresults        
             dataset_mode: unaligned                    
                direction: AtoB                          
          display_winsize: 256                          
                    epoch: latest                        
                     eval: False                        
                  gpu_ids: 0                            
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 21                            
                  isTrain: False                                [default: None]
                load_iter: 0                                    [default: 0]
                load_size: 384                          
         max_dataset_size: inf                          
                    model: cycle_gan                    
               n_layers_D: 3                            
                     name: simplecyclegan_nopose        
                      ndf: 64                            
                     netD: basic                        
                     netG: resnet_6blocks                
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: False                        
                     norm: batch                        
                    ntest: inf                          
                 num_test: 6289                          
              num_threads: 4                            
                output_nc: 3                            
                    phase: test                          
               preprocess: resize_and_crop              
              results_dir: ./results/                    
           serial_batches: False                        
                   suffix:                              
                  verbose: False                        
----------------- End -------------------
dataset [UnalignedDataset] was created
initialize network with normal
initialize network with normal
model [CycleGANModel] was created
loading the model from ./checkpoints/simplecyclegan_nopose/latest_net_G_A.pth
Traceback (most recent call last):

  File ""<ipython-input-2-2c53477dc693>"", line 1, in <module>
    runfile('/home/sagar18174/Thesis/person_RE-identification/pytorch-CycleGAN-and-pix2pix-master_with_pose_gpu_troubleshoot/test.py', wdir='/home/sagar18174/Thesis/person_RE-identification/pytorch-CycleGAN-and-pix2pix-master_with_pose_gpu_troubleshoot')

  File ""/opt/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""/opt/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/home/sagar18174/Thesis/person_RE-identification/pytorch-CycleGAN-and-pix2pix-master_with_pose_gpu_troubleshoot/test.py"", line 48, in <module>
    model.setup(opt)               # regular setup: load and print networks; create schedulers

  File ""/home/sagar18174/Thesis/person_RE-identification/pytorch-CycleGAN-and-pix2pix-master_with_pose_gpu_troubleshoot/models/base_model.py"", line 88, in setup
    self.load_networks(load_suffix)

  File ""/home/sagar18174/Thesis/person_RE-identification/pytorch-CycleGAN-and-pix2pix-master_with_pose_gpu_troubleshoot/models/base_model.py"", line 194, in load_networks
    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))

  File ""/home/sagar18174/Thesis/person_RE-identification/pytorch-CycleGAN-and-pix2pix-master_with_pose_gpu_troubleshoot/models/base_model.py"", line 170, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)

  File ""/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 576, in __getattr__
    type(self).__name__, name))

AttributeError: 'ResnetGenerator' object has no attribute 'module'",full unaligned direction epoch latest false normal false default none default model name basic true false norm batch phase test false suffix verbose false end initialize network normal initialize network normal model loading model recent call last file line module file line file line compile file line module opt regular setup load print create file line setup file line net file line module key file line type self name object attribute,issue,positive,negative,neutral,neutral,negative,negative
625278000,"看你名字，像是中国人；Please translate these words if you are not Chinese.
训练那部分，的代码，就是：python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan
，这部分需要改成：!python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --checkpoints_dir /content/pytorch-CycleGAN-and-pix2pix/checks --model cycle_gan
这代码里面，checkpoints_dir是为了指定文件夹，checks是我自己改的名字，这个名字可以自己定，训练过程的图片在这里


------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;""Yu700""<notifications@github.com&gt;;
发送时间:&nbsp;2020年5月7日(星期四) 晚上8:53
收件人:&nbsp;""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com&gt;;
抄送:&nbsp;""沉默的企鹅""<1346265656@qq.com&gt;;""Author""<author@noreply.github.com&gt;;
主题:&nbsp;Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to see pictures in Google clab? (#1017)





 
The checkpoints has nothing,how to do it?
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.",translate name model python name model author author see nothing thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
625037846,"If I want to set 40 epochs should I change that 100 to 20 or 40? And do I have to change the 100 on the next row?------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;""Jun-Yan Zhu""<notifications@github.com&gt;;
发送时间:&nbsp;2020年5月7日(星期四) 凌晨4:57
收件人:&nbsp;""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com&gt;;
抄送:&nbsp;""沉默的企鹅""<1346265656@qq.com&gt;;""Author""<author@noreply.github.com&gt;;
主题:&nbsp;Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to see pictures in Google clab? (#1017)





 
See the flags.
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.",want set change change next row author author see see thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
624983103,"There might be overfitting. You can do some flipping and cropping. 200 epochs might be too many for 23K images. Try some models with early epochs. There are two steps: 
1. first make sure that it works for your test images. 
2. make sure that your own drawing has the same stroke width and intensity as training images. ",might might many try early two first make sure work test make sure drawing stroke width intensity training,issue,positive,positive,positive,positive,positive,positive
624908008,"That made it work, thanks. I did not set any flags because I thought the default settings would be used for train and test. But I did not know that ``pix2pix`` rewrites the default values internally.

I want to recreate the edges2cat work but with faces, so I found a dataset of 23000 images of cropped faces ranges from babies to old.

I applied canny edge detection to extract edges and train. Below is the AB sample of this:

![26_1_2_20170116164549452 jpg chip](https://user-images.githubusercontent.com/44167893/81231310-df517c00-8fea-11ea-8143-969cf38f6b5d.jpg)

This was run for 200 epochs and the network seemed to be learning. Now when I give it a hand drawn test sample (one I draw in paint) :

![test_pix2pix_real](https://user-images.githubusercontent.com/44167893/81231433-0c059380-8feb-11ea-9b40-dba54e0624f7.png)

The network generated this:

![test_pix2pix_fake](https://user-images.githubusercontent.com/44167893/81231449-14f66500-8feb-11ea-9d75-038a3f21e6af.png)


But when I give it a unseen sample (not used during training)  from the same dataset:

![86_1_0_20170120134855602 jpg chip_real](https://user-images.githubusercontent.com/44167893/81231546-42dba980-8feb-11ea-9cd2-d1c5145c27fd.png)

It generates a somewhat acceptable result:

![86_1_0_20170120134855602 jpg chip_fake](https://user-images.githubusercontent.com/44167893/81231581-4d963e80-8feb-11ea-8843-82e968724e39.png)


What might be the reason for this?

Thank you",made work thanks set thought default would used train test know default internally want recreate work found old applied canny edge detection extract train sample chip run network learning give hand drawn test sample one draw paint network give unseen sample used training somewhat acceptable result might reason thank,issue,positive,positive,positive,positive,positive,positive
624889478,You need to specify some flags such as `--netG` and `--norm`. See this [Q&A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#runtimeerror-errors-in-loading-state_dict-812-671461-296),need specify norm see,issue,negative,neutral,neutral,neutral,neutral,neutral
624856883,"Thanks for your answer.By the way, how could I set epochs? 200 is too much.


发自我的iPhone

------------------ Original ------------------
From: Jun-Yan Zhu <notifications@github.com&gt;
Date: 周四,5月 7,2020 3:41 上午
To: junyanz/pytorch-CycleGAN-and-pix2pix <pytorch-CycleGAN-and-pix2pix@noreply.github.com&gt;
Cc: YXD123450 <1346265656@qq.com&gt;, Author <author@noreply.github.com&gt;
Subject: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] How to see pictures in Google clab? (#1017)





 
visdom may not work in Google Colab. You can see saved results at ./checkpoints/[your_experiment_name]/web/index.html.
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.",thanks way could set much original date author author subject see may work see saved thread reply directly view,issue,positive,positive,positive,positive,positive,positive
624850340,`visdom` may not work in Google Colab. You can see saved results at `./checkpoints/[your_experiment_name]/web/index.html`.,may work see saved,issue,negative,neutral,neutral,neutral,neutral,neutral
624517449,"> The default folder `checkpoints` had the right permissions, but even applying `chmod 777` didn't solve the problem. Changing directoty with `--checkpoints_dir` solved instead the problem. Thanks.

Where should I put the '--checkpoints_dir‘",default folder right even solve problem instead problem thanks put,issue,negative,positive,positive,positive,positive,positive
624517055,"> 不知道您是否具有对保存模型的目录的写许可权。您可以使用指定它`--checkpoints_dir`。



> Not sure if you have the write permission to the directory that saves models. You can specify it using `--checkpoints_dir`.

Where should I put the '--checkpoints_dir‘",sure write permission directory specify put,issue,negative,positive,positive,positive,positive,positive
623907137,"You can change `load_size` and `crop_size`. See the [flags](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L46) for more details. 

To make the input compatible with the network, the code does the following preprocessing. 
```python
# base=4
ow, oh = img.size
h = int(round(oh / base) * base)
w = int(round(ow / base) * base)
```
It is possible that your image might be smaller than the training image. See this [link](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#about-image-size) for more details.",change see make input compatible network code following python ow oh round oh base base round ow base base possible image might smaller training image see link,issue,negative,negative,negative,negative,negative,negative
623330155,We multiply the training iteration by batch_size. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/train.py#L49).,multiply training iteration see line,issue,negative,neutral,neutral,neutral,neutral,neutral
623079960,"@junyanz Thank you for your reply. And what about the relationship between batch_size and iterations/epoch? 

> BTW, it seems that increasing batch_size will not decrease iterations per epoch. It's kind of confusing for me.",thank reply relationship increasing decrease per epoch kind,issue,positive,positive,positive,positive,positive,positive
623030238,It happens when the transformation is difficult to learn or two domains look drastically different. Not sure if reducing the learning rate will help in your case. I will try (1) adding some paired data or (2) using small cropped patches rather than the entire image. ,transformation difficult learn two look drastically different sure reducing learning rate help case try paired data small rather entire image,issue,negative,negative,neutral,neutral,negative,negative
623029698,"You probably need to collect more than 1 training image. Even you sample 300 patches, the model still only sees one image. ",probably need collect training image even sample model still one image,issue,negative,neutral,neutral,neutral,neutral,neutral
623028972,"According to this [post](https://github.com/open-mmlab/mmdetection/issues/2138), it might be caused by version incompatibility between PyTorch and torchvision. ",according post might version incompatibility,issue,negative,neutral,neutral,neutral,neutral,neutral
623028788,You are right. The direction should not matter for CycleGAN. It matters in the pix2pix setting. ,right direction matter setting,issue,negative,positive,positive,positive,positive,positive
623013901,"i have the same problem. Please can you tell me how you fix it,

thanks",problem please tell fix thanks,issue,negative,positive,positive,positive,positive,positive
622320311,In case you use Docker: I changed the Dockerfile first line to `FROM nvidia/cuda:9.2-base` and the error disappeared.,case use docker first line error,issue,negative,positive,positive,positive,positive,positive
621895511,"Great. A PR might involve more work, as we want an abstraction that works for all the models and losses. If you want to give a try, we can have a look at PR. But no pressure. We will revisit it by ourselves later when we have some cycle. ",great might involve work want abstraction work want give try look pressure revisit later cycle,issue,positive,positive,positive,positive,positive,positive
621326461,"@junyanz I successfully calculated the L1 loss, and I am now trying to calculate discriminator loss after loads D.

Do you want me to create a PR after I finish that? If you need the functionality, I am happy to do so, but otherwise, as I understand creating unhelpful PR is a waste of time and manpower, I will not submit one. How do you think?",successfully calculated loss trying calculate discriminator loss want create finish need functionality happy otherwise understand unhelpful waste time submit one think,issue,negative,positive,positive,positive,positive,positive
621325964,"I think LSGAN is a more stable loss compared to vanilla GAN. It has a better gradient property. I encourage you to read the original [work](https://arxiv.org/abs/1611.04076). You are free to use LSGAN in your task. Maybe you want to change `--lambda_L1` to 10 or 25, as LSGAN's GAN loss has a larger range compared to vanilla GANs.",think stable loss vanilla gan better gradient property encourage read original work free use task maybe want change gan loss range vanilla,issue,positive,positive,positive,positive,positive,positive
621323014,"Yes. You need to modify the `pix2pix_model.py` as each model has different losses. You can calculate the mean in the `test.py` after the main for loop. If you want to load D, you can modify this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L54). ",yes need modify model different calculate mean main loop want load modify line,issue,negative,negative,neutral,neutral,negative,negative
621320121,"Interesting. I am not sure why it happened. `--gpu_ids` is 0-indexed. If you set `--gpu_ids -1`, it will use CPU.",interesting sure set use,issue,positive,positive,positive,positive,positive,positive
621117371,"How can I get the loss for each model structure?
Feels like I need to define/modify some functions on pix2pix_model.py to do so, right?

As we do not load discriminator, we can only calculate L1 loss, right? I want to verify whether my understanding is correct or not.

What I will do follows:
 - define criterionL1() for testing (only when we have paired images for testing)
 - Get all L1 losses for the testing set, and calculate mean, sd, ...
 - Print the statistics to console",get loss model structure like need right load discriminator calculate loss right want verify whether understanding correct define testing paired testing get testing set calculate mean print statistic console,issue,negative,positive,neutral,neutral,positive,positive
621103363,"Hi. I checked the input is float32. 
Anyways, the issue went away when I assigned `gpu_ids 1` option (default was 0).

Just adding the print out from `nvidia-smi` here.

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |
|  0%   28C    P0    58W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |
|  0%   34C    P0    59W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |
|  0%   30C    P0    54W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 108...  Off  | 00000000:84:00.0 Off |                  N/A |
|  0%   27C    P0    52W / 250W |      0MiB / 11178MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+",hi checked input float anyways issue went away assigned option default print driver version version name volatile fan temp compute mib mib default mib mib default mib mib default mib mib default memory type process name usage running found,issue,negative,neutral,neutral,neutral,neutral,neutral
620719747,The bias could be ignored by the following normalization layer. See this [post](#981) for more details. ,bias could following normalization layer see post,issue,negative,neutral,neutral,neutral,neutral,neutral
620703816,Yes. The results look better without eval() mode.,yes look better without mode,issue,positive,positive,positive,positive,positive,positive
620403105,"Ah yeah I see, thanks. 

I asked this because when I used this model in torch's .eval() mode, I get really strange results. I've read that this could happen if you use the same layer across the model, but that is not the case then.
So hopefully if I put the tracking statistics off it behaves normally. ",ah yeah see thanks used model torch mode get really strange read could happen use layer across model case hopefully put statistic normally,issue,positive,positive,positive,positive,positive,positive
620227878,"Your stack trace is exactly the same as I'm investigating in https://github.com/pytorch/pytorch/issues/37157.  However, my minimal repro only fails if the `slow_conv_transpose2d_backward_cuda` runs in FP16.  If you have a repro that fails with FP32 inputs, that would be interesting to know.",stack trace exactly investigating however minimal would interesting know,issue,negative,positive,positive,positive,positive,positive
620157179,"Thank you. I wonder how mixed-precision calculation might happen, since I did not change anything from the build. Can cublas change precision internally? This https://github.com/pytorch/pytorch/issues/37157#issue-605720486 seems related too.",thank wonder calculation might happen since change anything build change precision internally related,issue,negative,neutral,neutral,neutral,neutral,neutral
620143199,Sure. You can modify the `test` or `forward` [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/base_model.py#L98).,sure modify test forward function,issue,negative,positive,positive,positive,positive,positive
620141679,"1. Not sure about the TF version. We only provided PyTorch and Lua versions for pix2pix. 
2. The major difference is coarse-to-fine training in pix2pixHD. Another difference is the loss function (feature matching loss, and VGG loss used in pix2pixHD)
3. Adding two classes is possible. But you need to write your own model loading function that can load part of the layers. 
4. 256x256 to 512x512 is possible. I encourage you to post this question on pix2pixHD repo. 
5. Not sure if increasing `n_downsample_global` will help.  You can also add more ResNet blocks.  If your memory is a concern, you can also reduce `ngf`.",sure version provided major difference training another difference loss function feature matching loss loss used two class possible need write model loading function load part possible encourage post question sure increasing help also add memory concern also reduce,issue,positive,positive,positive,positive,positive,positive
620130360,"If I want to make those by myself, will it be okay just to copy and paste the relevant code sections from the training.py?",want make copy paste relevant code,issue,negative,positive,positive,positive,positive,positive
620128668,It is not supported. We might add evaluation test time metrics (FID/meanAP) in the future project release. But we don't plan to add this functionality now. ,might add evaluation test time metric future project release plan add functionality,issue,negative,neutral,neutral,neutral,neutral,neutral
620127756,Not sure. It might be related to mixed-precision training. But we don't do it by default. This [post](https://github.com/NVIDIA/apex/issues/580)  might be related. ,sure might related training default post might related,issue,negative,positive,positive,positive,positive,positive
620092353,"#229 According to the previous issue (approximately 2 years ago), it says it is not supported.
Is this functionality still not supported?",according previous issue approximately ago functionality still,issue,negative,negative,negative,negative,negative,negative
619433650,"Hi Adrian-1234, the high res dataset is not supposed to be public. So we removed the dataset. The pretrained models were trained to be run on the lower res dataset. Sorry for confusion. ",hi high supposed public removed trained run lower sorry confusion,issue,negative,negative,negative,negative,negative,negative
619433559,One simple hack is to have different lambdas for A->B->A and B->A->B cycle consistency losses. ,one simple hack different cycle consistency,issue,negative,neutral,neutral,neutral,neutral,neutral
619430865,"We use different normalization layers across the model. The [get_norm_layer](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L18) returns a layer class, rather than a layer instance. You can modify this function to track statistics. ",use different normalization across model layer class rather layer instance modify function track statistic,issue,negative,neutral,neutral,neutral,neutral,neutral
619420549,"> We don't have FID numbers for pix2pix. FID was published after pix2ix and became popular much later. If you want to compare to pix2pix regarding FID, I recommend that you download our pre-trained models and use your FID evaluation code. There are multiple versions of FID code, which produce slightly different results. Just make sure that you use the version for your method and pix2pix.

Thanks for your quick response! I got it.",fid fid popular much later want compare regarding fid recommend use fid evaluation code multiple fid code produce slightly different make sure use version method thanks quick response got,issue,positive,positive,positive,positive,positive,positive
619419832,"We don't have FID numbers for pix2pix. FID was published after pix2ix and became popular much later. If you want to compare to pix2pix regarding FID, I recommend that you download our pre-trained models and use your FID evaluation code. There are multiple versions of FID code, which produce slightly different results.  Just make sure that you use the version for your method and pix2pix. ",fid fid popular much later want compare regarding fid recommend use fid evaluation code multiple fid code produce slightly different make sure use version method,issue,positive,positive,positive,positive,positive,positive
619418648,"If you use 3D Conv for the generator, do you also use the 3D Conv in the discriminator? As you change the network architectures, data, and applications all at once, the original hyper-parameters might not work for you. I recommend that you first debug the generator by disabling GAN loss and only using L1 loss, and make sure your 3D generator can produce reasonable results. Then you can experiment with different types of 3D discriminators.  ",use generator also use discriminator change network data original might work recommend first generator gan loss loss make sure generator produce reasonable experiment different,issue,negative,positive,positive,positive,positive,positive
619324195,"> Yes, I have reproduced the results. In the label -> photo setting, you need to set the `batch_size=1`, and in the photo -> label setting, `batch_size=3`. For all settings, `load_size=143, crop_size=128`.


Thanks!
The model you test is the pretrained model or yourself model？",yes label photo setting need set photo label setting thanks model test model,issue,positive,positive,positive,positive,positive,positive
619320607,"Yes, I have reproduced the results. In the label -> photo setting, you need to set the `batch_size=1`, and in the photo -> label setting, `batch_size=3`. For all settings, `load_size=143, crop_size=128`.",yes label photo setting need set photo label setting,issue,negative,neutral,neutral,neutral,neutral,neutral
618887889,"Have you evaluated your model using the script ""evaluate.py"" that author offered?




------------------&nbsp;原始邮件&nbsp;------------------
发件人: ""CR-Gjx""<notifications@github.com&gt;; 
发送时间: 2020年4月15日(星期三) 下午2:57
收件人: ""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com&gt;; 
抄送: ""2862237661""<2862237661@qq.com&gt;; ""Mention""<mention@noreply.github.com&gt;; 
主题: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] The test result of single direction is so weird (#873)





 
I also observe this interesting phenomenon but do not know if I make some mistakes causing it or not. I will try it again. Thanks!
 
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",model script author mention mention test result single direction weird also observe interesting phenomenon know make causing try thanks reply directly view,issue,positive,positive,neutral,neutral,positive,positive
618880031,"Thanks again. But I am very confused . The original cityscapes dataset in ""Path to the original cityscapes dataset""  exactly mean what. And I have another thing wrong. 
![image](https://user-images.githubusercontent.com/34617934/80192475-d0151a80-8649-11ea-95eb-d3750021d3ed.png)
```
 File ""/root/pytorch-CycleGAN-and-pix2pix/scripts/eval_cityscapes/cityscapes.py"", line 21, in __init__
    self.id2trainId = {label.id: label.trainId for label in labels.labels}  # dictionary mapping from raw IDs to train IDs
AttributeError: module 'labels' has no attribute 'labels'
```",thanks confused original path original exactly mean another thing wrong image file line label dictionary raw train module attribute,issue,negative,negative,neutral,neutral,negative,negative
618758189,See this [post](https://github.com/phillipi/pix2pix/issues/187) and this [one](https://github.com/phillipi/pix2pix/issues/112). We have some [code](https://github.com/phillipi/pix2pix/issues/115#issuecomment-575303415) for converting an RGB output to a label map. But it is in Lua. ,see post one code converting output label map,issue,negative,neutral,neutral,neutral,neutral,neutral
618488185,"We encourage G to produce images that look realistic (True). If you set False, you are encouraging G to produce images that look fake. See Eqn (13) in the GANs [tutorial](https://arxiv.org/pdf/1701.00160.pdf) for more details.  ",encourage produce look realistic true set false encouraging produce look fake see tutorial,issue,positive,negative,neutral,neutral,negative,negative
618371365,"The error is from the sbatch code for running the code on server. Solved now! 
Thank you!
",error code running code server thank,issue,negative,neutral,neutral,neutral,neutral,neutral
618212592,"I would like to share some points on why the patch number is counted by:
(output_size - 1) * stride + ksize

Here is what I think. For any i (input feature map size), k (kernel size), p (zero padding size) and s (stride), the output feature map size (o) is:
o = floor((i+2*p-k)/s)+1

when calculating patch number, it is supposed that p=0, so it is very clear that the calculation process above is just the opposite of the patch number calculation process.",would like share patch number stride think input feature map size kernel size zero padding size stride output feature map size floor calculating patch number supposed clear calculation process opposite patch number calculation process,issue,positive,positive,neutral,neutral,positive,positive
617898301,"Hard to say. If the results look good at the training set, your model might already overfit, and you need to do an early stop. ",hard say look good training set model might already overfit need early stop,issue,negative,positive,positive,positive,positive,positive
617707338,@TavRotenberg It depends on the input image size. There are chances that other processes are utilizing the gpu memory as well. Please clear it and just use the preprocess flag as --preprocess resize and crop and try to run now. Hope it helps,input image size memory well please clear use flag resize crop try run hope,issue,positive,positive,positive,positive,positive,positive
617692537,Hi @jlim13 is it similar to semi supervised learning?,hi similar semi learning,issue,negative,neutral,neutral,neutral,neutral,neutral
617663469,"Let me illustrate with my case, a sketch-to-photo process

There are two image domains that this model works upon, `A` which usually is the source image domain (in my case, it's the sketches collection), and `B` which usually is the target image domain we want to generate (in my case, it's the photos collection).

When we process the image, the model saves 3 types of image to help visualize the process happening inside the model
- `real_` images refer to the real images taken from each domain eg. `real_A` refers to image I take from the sketches collection
- `fake_` images are the one generated by the model when it tried to convert your input image to its adjacent domain eg. `fake_B` is a generated photo image as a result of processing `real_A` sketch image
- `rec_` images are the reconstructed images when the model tried to replicate back the original `real_` with the `fake_` images from adjacent domain it has created eg. `rec_A` is a reconstructed sketches of `real_A` sketch image given the output `fake_B` photo it made before

This also applies vice versa. When the model processes a `real_B` photo image, it creates a `fake_A` sketch out of it and `rec_B` as its attempt to reconstruct `real_B`

I hope that helps",let illustrate case process two image model work upon usually source image domain case collection usually target image domain want generate case collection process image model image help visualize process happening inside model refer real taken domain image take collection one model tried convert input image adjacent domain photo image result sketch image reconstructed model tried replicate back original adjacent domain reconstructed sketch image given output photo made also vice model photo image sketch attempt reconstruct hope,issue,positive,positive,neutral,neutral,positive,positive
617613342,"After 2.5 days of training, the resulting test images are quite pixelated - do I need to increase the number of epocs > 200 or do you have any recommendations ?",day training resulting test quite need increase number,issue,negative,neutral,neutral,neutral,neutral,neutral
617608727,It might not be related to CPU/GPU. Maybe you can try removing or renaming your directory `./checkpoints/gan_pix2pix`. ,might related maybe try removing directory,issue,negative,neutral,neutral,neutral,neutral,neutral
616940354,"Thanks for your clear and prompt response, Prof. Zhu! :wink:",thanks clear prompt response wink,issue,positive,positive,positive,positive,positive,positive
616700507,"It's hard to know unless I read your code. Unfortunately, I don't have the capacity to look at your code in detail. My general advice is that you start from our codebase, and change one thing at a time, and see when it breaks. ",hard know unless read code unfortunately capacity look code detail general advice start change one thing time see,issue,negative,negative,negative,negative,negative,negative
616699305,"For edges2shoes, we didn't do cropping and flipping as the dataset is large enough. Also, random cropping sometimes will break the boundary of shoes. For edges2handbags, we might have used flipping. But we didn't use cropping. ",large enough also random sometimes break boundary might used use,issue,negative,negative,neutral,neutral,negative,negative
616616985,"Thank you for the input.   I have added your size flags above plus --batch_size=100  

python3 train.py --dataroot datasets/style_constable --name constable_cyclegan --model cycle_gan --batch_size 100 --preprocess crop --crop_size 600 --gpu_ids -1

The systems utlisation has increased to on average about 40% (50ish  CPUs) with a memory consumption of about 220GB for the training processes.

I assumed that the data loading would be trivial and that the files would sit in memory cache anyway making this fast without the --num_threads option.
",thank input added size plus python name model crop average memory consumption training assumed data loading would trivial would sit memory cache anyway making fast without option,issue,negative,positive,neutral,neutral,positive,positive
616280766,"yes,I already used default parameters! I try to use my custom `generator` with your `train code`,I can get the similar effect,but I can't reproduce the effect on my `train code`,I ever try to change my code in the light of your code,but it's useless!Is there anything else I should pay attention to?thanks!",yes already used default try use custom generator train code get similar effect ca reproduce effect train code ever try change code light code useless anything else pay attention thanks,issue,negative,positive,neutral,neutral,positive,positive
615932122,I see. The PR was from a different Github user and closed now. I haven't looked at the code. Maybe you can post your question under that PR and contact the author of that PR.,see different user closed code maybe post question contact author,issue,negative,negative,neutral,neutral,negative,negative
615931281,"Not familiar with CPU parallelization. I think increasing batch size might help.  We do have a flag called `--num_threads`. But it should only affect data loading. 

An unrelated note: you may want to use some random cropping to prevent overfitting (e.g., `--load_size 800 --crop_size 512`)",familiar parallelization think increasing batch size might help flag affect data loading unrelated note may want use random prevent,issue,negative,negative,neutral,neutral,negative,negative
615858042,"Thanks very much @junyanz .... I was talking about this pull request for semi aligned dataset.
[pull request](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/989).",thanks much talking pull request semi pull request,issue,negative,positive,positive,positive,positive,positive
615455752,"> It was recently added [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L12).



> I just added the `--epoch_count` flag. By default, the program will initialize the epoch count as 1. Set `--epoch_count` to specify a different starting epoch count.

Should I give --epoch_count value the last epoch at which training the model stopped along with --continue_train",recently added added flag default program initialize epoch count set specify different starting epoch count give value last epoch training model stopped along,issue,negative,neutral,neutral,neutral,neutral,neutral
615385612,"1. Batch Size is mostly for training speed especially you have multiple GPUs. 
2. Here is a [post](https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/) about normalization layers. For multi-GPUs, it is better to use synchronized batchnorm rather than the default batchnorm. See this [repo](https://github.com/vacancy/Synchronized-BatchNorm-PyTorch). 
3. We observe that `--dropout` may not help cyclegan as it contradicts the cycle-consistency loss. But you are free to try both. For eval, please see this [comment](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L54). 
4. resnet_9blocks should be good enough. It is a more recent architecture compared to the unet.  
5. Not sure what is a semi-aligned dataset. 
6. You can save the plot from the visdom windows. ",batch size mostly training speed especially multiple post normalization better use synchronized rather default see observe dropout may help loss free try please see comment good enough recent architecture sure save plot,issue,positive,positive,positive,positive,positive,positive
615380118,"You need to add `--dataroot` flag, which specifies the directory of your training images. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L23) for more details. See [Getting Started](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#getting-started) on how to use the python script.  ",need add flag directory training see see getting use python script,issue,negative,neutral,neutral,neutral,neutral,neutral
615350308,"Thanks, I found that I force the model to use wgan.",thanks found force model use,issue,negative,positive,positive,positive,positive,positive
615002608,"> You can set `--num_test`. The default value is 50.

oh! I ignored this option, Thanks!!",set default value oh option thanks,issue,positive,positive,positive,positive,positive,positive
614858652,Sure. It might be worth trying. ,sure might worth trying,issue,negative,positive,positive,positive,positive,positive
614857844,Maybe you can reuse some networks from super-resolution literature. Here is a standard [one](https://github.com/pytorch/examples/blob/master/super_resolution/model.py). ,maybe reuse literature standard one,issue,negative,neutral,neutral,neutral,neutral,neutral
614856228,"I am not sure -3000 is possible as we are using least square GAN objective (LSGAN) for CycleGAN training. The LSGAN loss is non-negative.  If you modify the loss, you might have some bugs there. ",sure possible least square gan objective training loss modify loss might,issue,negative,positive,neutral,neutral,positive,positive
614842039,"@junyanz I am wondering if combining Resnet-based with UNet makes sense? Specifically, adding the connection between down/up sampling layers.",wondering combining sense specifically connection sampling,issue,negative,neutral,neutral,neutral,neutral,neutral
614542563,"> I have experimented with switching the side of dataset. And indeed it helps breaking the confusion. Thank you very much for the help

Can you explain how does the naming works? 
Also how does the naming works in the images stored for each epoch.
Thanks 
",experimented switching side indeed breaking confusion thank much help explain naming work also naming work epoch thanks,issue,positive,positive,positive,positive,positive,positive
614469726,@junyanz thanks for the help. Can you please tell what all changes need to be made in generator??,thanks help please tell need made generator,issue,positive,positive,positive,positive,positive,positive
614248312,Not sure. It might be caused by visdom. You can either disable visdom (set `--display_id -1`) or starting the visdom in a separate terminal (type `python -m visdom.server`).,sure might either disable set starting separate terminal type python,issue,negative,positive,positive,positive,positive,positive
614247359,We haven't experimented with this setting. Not sure if it will work. But it might be worth trying. ,experimented setting sure work might worth trying,issue,negative,positive,positive,positive,positive,positive
614242611,"Thanks Junyan,

One more question, I'm going to explaining my dataset a bit more to get your insight into the application of the method.

I have 2 datasets, dataset 1 has n samples with 10 classes(labels). Dataset 2 has m samples with 5 classes. So I basically don't know what labels in dataset1 are similar to what samples in dataset2. Is CycleGAN still applicable in this scenario? Correct me if I am wrong, but it seems that you only trained the model on specific datasets each type ( for example dataset1 containing only horses and dataset2 only zebras, but not like dataset1 containing hourses+cats+dogs and dataset2 zebra+dog+fox), am I correct?",thanks one question going explaining bit get insight application method class class basically know similar still applicable scenario correct wrong trained model specific type example like correct,issue,negative,negative,neutral,neutral,negative,negative
614239569,"It has been applied to non-image data (e.g., voice conversion [project](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc/)). You probably need to implement your own generator and discriminator. The loss can still be used. ",applied data voice conversion project probably need implement generator discriminator loss still used,issue,negative,neutral,neutral,neutral,neutral,neutral
614238738,It's possible. But you probably need to add a new generator by yourself.  ,possible probably need add new generator,issue,negative,positive,neutral,neutral,positive,positive
614238010,"It is currently not supported. If you use visdom, you should be able to save the plot form visdom console. ",currently use able save plot form console,issue,negative,positive,positive,positive,positive,positive
613854017,I also observe this interesting phenomenon but do not know if I make some mistakes causing it or not. I will try it again. Thanks! ,also observe interesting phenomenon know make causing try thanks,issue,positive,positive,positive,positive,positive,positive
613848616,"> > @junyanz Ok, I see. Thanks a lot! I use `batchSize=20` on 4 gpus to accelerate training. I will train the model again and try your evaluation code in pix2pix.
> 
> Hi, did you re-train the model and get the similar results with them in the paper? Could you provide some details (e.g parameters) for me? Now I cannot reproduce the results using default parameters. Thanks! @FishYuLi

@CR-Gjx Yes, I got similar results as that in the paper just with the default parameters. I think you may try to set your `batchSize=1` on single gpu. I have also tried to enlarge the batchSize but find that the model is very sensitive to batch size. Results become worse with large batch size. Also note that the input size should be 128x128 but not 256. ",see thanks lot use accelerate training train model try evaluation code hi model get similar paper could provide reproduce default thanks yes got similar paper default think may try set single also tried enlarge find model sensitive batch size become worse large batch size also note input size,issue,positive,positive,neutral,neutral,positive,positive
613800949,"> @junyanz Ok, I see. Thanks a lot! I use `batchSize=20` on 4 gpus to accelerate training. I will train the model again and try your evaluation code in pix2pix.

Hi, did you re-train the model and get the similar results with them in the paper? Could you provide some details (e.g parameters) for me? Now I cannot reproduce the results using default parameters. Thanks! @FishYuLi ",see thanks lot use accelerate training train model try evaluation code hi model get similar paper could provide reproduce default thanks,issue,positive,positive,positive,positive,positive,positive
613562481,@junyanz I want to produce enlarged image at output. Is it possible with this cycleGAN model?? It means i want to translate input image to enlarged output image. Thanks,want produce enlarged image output possible model want translate input image enlarged output image thanks,issue,positive,positive,neutral,neutral,positive,positive
613444182,"I don't really understand the reasons behind it. But I think the reason maybe that model for this dataset is not convergent. If you have trained the model, you will find the results in the middle are not that bad. So you can interrupt the process of training or only use the milldle model you have trained. I hope I helped you.




------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;""CR-Gjx""<notifications@github.com&gt;;
发送时间:&nbsp;2020年4月14日(星期二) 晚上7:34
收件人:&nbsp;""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com&gt;;
抄送:&nbsp;""2862237661""<2862237661@qq.com&gt;;""Author""<author@noreply.github.com&gt;;
主题:&nbsp;Re: [junyanz/pytorch-CycleGAN-and-pix2pix] The test result of single direction is so weird (#873)





  
Hi. I have trained the cyclegan model on cityscapes dataset. Now I want to test from testA (the real images) to testB(the label). I have changed latest_net_G_A.pth to latest_net_G.pth, then I run the code "" python test.py --dataroot datasets/cityscapes/testA --name cityscapes_cyclegan --model test --no_dropout "". I have expected the result is ""the real A"" and ""the rec B"". But to my surprise, the result is ""the real A"" and ""the fake B"". If I want to get my diseired result, how should I do?
 
  
I get weird results like yours. Did you solve this problem? If so, please give me some tips, I use the same training script.
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.",really understand behind think reason maybe model convergent trained model find middle bad interrupt process training use model trained hope author author test result single direction weird hi trained model want test testa real label run code python name model test result real surprise result real fake want get result get weird like solve problem please give use training script thread reply directly view,issue,negative,negative,negative,negative,negative,negative
613440541,"> @ZzzackChen Would you mind telling how do you implement evaluation in details since I used the same implementation as yours but all numbers are zero.

I meet the same problem, have you solved it?",would mind telling implement evaluation since used implementation zero meet problem,issue,negative,neutral,neutral,neutral,neutral,neutral
613388608,"> Hi. I have trained the cyclegan model on cityscapes dataset. Now I want to test from testA (the real images) to testB(the label). I have changed latest_net_G_A.pth to latest_net_G.pth, then I run the code "" python test.py --dataroot datasets/cityscapes/testA --name cityscapes_cyclegan --model test --no_dropout "". I have expected the result is ""the real A"" and ""the rec B"". But to my surprise, the result is ""the real A"" and ""the fake B"". If I want to get my diseired result, how should I do?
> ![2019-12-11 22-03-49屏幕截图](https://user-images.githubusercontent.com/34617934/70628931-0be3b080-1c64-11ea-92d3-67cfda496a80.png)

I get weird results like yours. Did you solve this problem? If so, please give me some tips, I use the same training script.",hi trained model want test testa real label run code python name model test result real surprise result real fake want get result get weird like solve problem please give use training script,issue,negative,negative,negative,negative,negative,negative
612971033,"Could you use `--n_epochs 200 --n_epochs_decay 200`? `--epoch` is used for other [purpose](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L53). 
Also, use `--crop_size 256 --load_size 286`. I recommend that you use default parameters first before trying other stuff. ",could use epoch used purpose also use recommend use default first trying stuff,issue,negative,positive,positive,positive,positive,positive
612970203,Thanks a lot. Good luck with your project. ,thanks lot good luck project,issue,positive,positive,positive,positive,positive,positive
612878510,"> Which generator `--netG` are you using? `unet_256` only supports 256x256 inputs. You may want to use `resnet_9blocks`.

with resnet_9blocks I have that error, however I did change this line of code in **model.py**
`sample_images = [load_train_data(batch_file, is_testing=True) for batch_file in batch_files]`
to the following one
 `sample_images = [load_train_data(batch_file,load_size=1024, fine_size=360 ) for batch_file in batch_files]`

is that right????
",generator may want use error however change line code following one right,issue,negative,positive,positive,positive,positive,positive
612783708,"Hi,

I found this project a couple of days ago.....

First thing I'd like to say is thank you very much for sharing this exciting project, and for continuing to support it.

I have been carrying out some experiments on my CPU only system with various sizes of input, which can be summarised in the following script:

for i in style_monet style_cezanne style_ukiyoe style_vangogh
do
     echo $i
     python3 test.py --dataroot datasets/""$i""/testA-1880 --name ""$i""_pretrained --model test  --gpu_ids -1 --no_dropout --preprocess none --load_size 1880 --crop_size 1880 &
     python3 test.py --dataroot datasets/""$i""/testA-1920x1080p --name ""$i""_pretrained --model test  --gpu_ids -1 --no_dropout --preprocess none --load_size 1920 --crop_size 1920 &
     python3 test.py --dataroot datasets/""$i""/testA-3840 --name ""$i""_pretrained --model test  --gpu_ids -1 --no_dropout --preprocess none --load_size 3840 --crop_size 3840 &
     python3 test.py --dataroot datasets/""$i""/testA-orig-small --name ""$i""_pretrained --model test  --gpu_ids -1 --no_dropout &
     wait
done

Where 1880, 1920 and 3840 are the horizontal resolutions  of the input image set.

Not only am I able to process (test/inference) very large images with good results, but your code is parallel (on the CPUs) and runs quickly on my 120 core system. In the above case,  I run the streams in parallel to increase the systems utilisation as much as possible.

Keep up the good work !",hi found project couple day ago first thing like say thank much exciting project support carrying system various size input following script echo python name model test none python name model test none python name model test none python name model test wait done horizontal input image set able process large good code parallel quickly core system case run parallel increase much possible keep good work,issue,positive,positive,positive,positive,positive,positive
612706256,"oh,thanks! I use the default `readme`'s `facade` dataset,and i still can't  get the effect like your model,
my train command as follow:
`python train.py --dataroot ./datasets/facades --name facade1930 --model pix2pix --direction BtoA --dataset_mode aligned --crop_size 512 --load_size 512 --epoch 1000`
Besides,I always get the black image when I test,and I check the test image certainly! what cause this?",oh thanks use default facade still ca get effect like model train command follow python name facade model direction epoch besides always get black image test check test image certainly cause,issue,positive,positive,neutral,neutral,positive,positive
612263449,"Thank you, indeed this was the problem! For some reason, this was not clear to me. May be this could be added to the readme for others who might struggle with the testing phase of pix2pix (but it may be just me, then never mind).",thank indeed problem reason clear may could added might struggle testing phase may never mind,issue,negative,positive,positive,positive,positive,positive
612091477,"There is some confusion about names. Currently, val and test are the same. You can use it as test set.  ",confusion currently test use test set,issue,negative,neutral,neutral,neutral,neutral,neutral
612089174,Yes. But that network was not implemented by me. Please send an email. Thanks. ,yes network please send thanks,issue,positive,positive,positive,positive,positive,positive
611846579,"@junyanz  okay!thank your reply!!I will try it.
but another problem,I use the `facade` dataset with the size 90M,I find out the gan loss is steep,it is likely difficult to restrain oneself. So how many step it need? thank you,boss!",thank reply try another problem use facade size find gan loss steep likely difficult restrain oneself many step need thank bos,issue,negative,neutral,neutral,neutral,neutral,neutral
611843207,@junyanz that's right. I fixed it by uninstalling the pytorch libraries and reinstalling it with another version. Now the code is running.,right fixed another version code running,issue,negative,positive,positive,positive,positive,positive
611808407,@YiruS May I ask you how did you solve this problem?,may ask solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
611808251,"So is there any reason why we divide the entire dataset to train/test/val?
Is there any part of code that the program utilizing the validation part of dataset?",reason divide entire part code program validation part,issue,negative,neutral,neutral,neutral,neutral,neutral
611660176,"Ok. I just thought you were also author of “ CyCADA: Cycle-Consistent Adversarial Domain Adaptation   https://arxiv.org/abs/1711.03213”?

And as far as I can see you are one of the main authors of this paper? And the code I'm using is from the repository linked in that paper. 

And Judy Hoffman that has the repository are not replying on the issues on the github page. But ill try to send her a mail. ",thought also author domain adaptation far see one main paper code repository linked paper repository page ill try send mail,issue,negative,negative,neutral,neutral,negative,negative
611657105,It might be caused by the incompatibility between your PyTorch version and torchvision version. ,might incompatibility version version,issue,negative,neutral,neutral,neutral,neutral,neutral
611655034,"Training should be reasonably fast on a modern GPU. If you want faster training, you can use lower-resolution images and smaller networks. If you use 128x128 images, you can use `unet_128`. ",training reasonably fast modern want faster training use smaller use use,issue,negative,positive,positive,positive,positive,positive
611654450,I am not familiar with `--model cycle_gan_semantic`. I recommend that you post the question on cycada repo or send an email to the leading author. ,familiar model recommend post question send leading author,issue,negative,positive,positive,positive,positive,positive
611482546,"Hi again. There is something I forgot to mention. I am using the CyCADA code and when I change`--model cycle_gan` and run with batchSize=1, image size = 128 it uses 1.7GB of memory and runs without any problem. When I change batchSize=16, image size = 128 it uses 9GB of memory. But with batchSize=32 it runs out of memory. Ok so there is no problem when I run the `--model cycle_gan`.

But when I change the `--model cycle_gan_semantic` to your CyCADA method for **domain adaptation** it does not work with batchSize = 1 and image size = 128. Then it returns the following error:

`RuntimeError: size mismatch, m1: [1 x 12544], m2: [256 x 1024] at /tmp/pip-req-build-zfu01l1o/aten/src/THC/generic/THCTensorMathBlas.cu:290`

When I try to keep the batchSize = 1 and change image size to 64 I get the following error:

`RuntimeError: size mismatch, m1: [1 x 2304], m2: [256 x 1024] at /tmp/pip-req-build-zfu01l1o/aten/src/THC/generic/THCTensorMathBlas.cu:290`

When I keep the batchSize = 1 and change image size = 32 I get the following error:

`ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])` 

When I change the batchSize = 16 and keep image size = 32 it runs and uses 1.7GB

Is there something I am missing about the `--model cycle_gan_semantic`? Should the input dimensions of the two datasets be different? Or cant I use image size = 128 for both datasets? Or should the size of the dataset A and B be of some fixed size?",hi something forgot mention code change model run image size memory without problem change image size memory memory problem run model change model method domain adaptation work image size following error size mismatch try keep change image size get following error size mismatch keep change image size get following error value per channel training got input size change keep image size something missing model input two different cant use image size size fixed size,issue,negative,negative,neutral,neutral,negative,negative
611269874,Same Problem right here even when training with the standard configurations as described in the github,problem right even training standard,issue,negative,positive,positive,positive,positive,positive
611201560,Does the code work for batchSize=1 and image size =128? BatchSize=100 might be too big for your GPU memory. Not sure if the generator will work for images with 32x32. `unet_128` and `unet_256` will not work. `resnet_6blocks` and `resnet_9blocks` might work. ,code work image size might big memory sure generator work work might work,issue,negative,positive,positive,positive,positive,positive
610657296,Thank you so much. Great work.,thank much great work,issue,positive,positive,positive,positive,positive,positive
610543682,"Our codebase does not support parallelization over multiple CPU cores. `num_threads` is for data loading. If you increase `num_threads`, your data loading might be faster. But it is not related to model computation. You may want to modify the code by yourself.  Here is a relevant [post](https://discuss.pytorch.org/t/using-multiple-cpu-cores-for-training/48981). ",support parallelization multiple data loading increase data loading might faster related model computation may want modify code relevant post,issue,positive,positive,positive,positive,positive,positive
610182884,"Hi @mohammadhosseinashoori, I think you are correct in that the bias of the conv layer right before IN is not needed, as it will be canceled in the normalization layer. However, in CycleGAN, we went with the default setting of nn.Conv, which enables bias by default. It was just for simplicity of the coding. Later on we integrated a PR that removes the bias, but the bias for IN was likely still kept to be able to run pretrained models. ",hi think correct bias layer right normalization layer however went default setting bias default simplicity later bias bias likely still kept able run,issue,negative,positive,positive,positive,positive,positive
610170125,@taesungp might be able to answer your question regarding SPADE.,might able answer question regarding spade,issue,negative,positive,positive,positive,positive,positive
610148279,"thank you for your replay.

**1) ok**
**2)** I think, after normalization, bias term will be ignored, wheater affine = True or False

in SPADE project
at discriminator, instanceNorm used in several layers (with affine = False) 
but they removed bias in this code below: 

https://github.com/NVlabs/SPADE/blob/master/models/networks/normalization.py 
(Line 33, at get_nonspade_norm_layer function)

>         # remove bias in the previous layer, which is meaningless
>         # since it has no effect after normalization
>         if getattr(layer, 'bias', None) is not None:
>             delattr(layer, 'bias')
>             layer.register_parameter('bias', None)

i get confused, ...
thank you
",thank replay think normalization bias term affine true false spade project discriminator used several affine false removed bias code line function remove bias previous layer meaningless since effect normalization layer none none layer none get confused thank,issue,positive,negative,negative,negative,negative,negative
610057529,"1. You may want to add the flag `--no_droptout` when you use `--model test`. Just make sure your training and test flags are the same. 
2. If you use `--model cycle_gan` for testing, you should use `--dataset_mode unaligned`.",may want add flag use model test make sure training test use model testing use unaligned,issue,negative,positive,positive,positive,positive,positive
610053099,"You should be able to still see the plot after your training is finished, as long as you don't kill your visdom server. I haven't used `replay_log`. I am not sure if the format in `loss_log` will work for the function. ",able still see plot training finished long kill server used sure format work function,issue,negative,positive,positive,positive,positive,positive
610050275,"1) For CycleGAN, we followed the resnet-based architecture from prior [work](https://arxiv.org/abs/1603.08155). It has a large Conv layer (7x7) before the norm layer, which may be able to encode color information. Feel free to remove this layer and see how it works. @taesungp may know more details. 
2) It depends on the flag `affine`. If `affine=True`, the normalization layer already includes bias parameters and we do not need `use_bias` for the Conv layers before that normalization layer.  By default, we set `affine=False` for `nn.InstancNorm2d`. ",architecture prior work large layer norm layer may able encode color information feel free remove layer see work may know flag affine normalization layer already bias need normalization layer default set,issue,negative,positive,positive,positive,positive,positive
610042043,"Both model initialization and training are stochastic. Results might vary across different runs and checkpoints. If you have an evaluation metric for your tasks, you can compute the average metrics over 5 or 10 runs. You can also use the metric to choose the best model.",model training stochastic might vary across different evaluation metric compute average metric also use metric choose best model,issue,positive,positive,positive,positive,positive,positive
609578029,"Okay @junyanz I found out how it worked. When my program is finished running I want to visualize it. How can I visualize it? I can read that argument is replay_log and the filename. But how exactly does I run this? Do you have any guidelines?

I tried the following:
```
import visdom
vis = visdom.Visdom(server = 'http://localhost',port='8097')
vis.replay_log('checkpoints/cycada_svhn2mnist_noIdentity/loss_log.txt')
```

But it just gives me the following error: 
```
~/cycada_release/envs/project_env/lib/python3.7/site-packages/visdom/__init__.py in replay_log(self, log_filename)
    900             log_entries = f.readlines()
    901         for entry in log_entries:
--> 902             endpoint, msg = json.loads(entry)
    903             self._send(msg, endpoint, from_log=True)
    904 

~/cycada_release/envs/project_env/lib/python3.7/json/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
    346             parse_int is None and parse_float is None and
    347             parse_constant is None and object_pairs_hook is None and not kw):
--> 348         return _default_decoder.decode(s)
    349     if cls is None:
    350         cls = JSONDecoder

~/cycada_release/envs/project_env/lib/python3.7/json/decoder.py in decode(self, s, _w)
    335 
    336         """"""
--> 337         obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    338         end = _w(s, end).end()
    339         if end != len(s):

~/cycada_release/envs/project_env/lib/python3.7/json/decoder.py in raw_decode(self, s, idx)
    353             obj, end = self.scan_once(s, idx)
    354         except StopIteration as err:
--> 355             raise JSONDecodeError(""Expecting value"", s, err.value) from None
    356         return obj, end

JSONDecodeError: Expecting value: line 1 column 1 (char 0)
```
Is this happening because I am running the 'replay_log' incorrectly?",found worked program finished running want visualize visualize read argument exactly run tried following import vi server following error self entry entry none none none none return none decode self end end end end self end except err raise value none return end value line column char happening running incorrectly,issue,negative,positive,neutral,neutral,positive,positive
609004897,thanks for helping long way . got it :) training started. ,thanks helping long way got training,issue,positive,positive,neutral,neutral,positive,positive
608984781,my load size is 286 and crop size is 256,load size crop size,issue,negative,neutral,neutral,neutral,neutral,neutral
608983873,i could not solve . please tell where to add this flag?,could solve please tell add flag,issue,negative,neutral,neutral,neutral,neutral,neutral
608952500,"> > 嗨，@ junyanz， 
> > 我想确保在使用相同设置和相同配置的情况下，尝试使用WGAN-GP丢失的CycleGAN。
> > 这是我受**cycle_gan_model.py**上的[＃439](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/439)和[VON](https://github.com/junyanz/VON/blob/master/models/shape_gan_model.py#L80)启发的修改：****
> > ```
> > def backward_D_basic(self, netD, real, fake):
> >         """"""Calculate GAN loss for the discriminator
> > 
> >         Parameters:
> >             netD (network)      -- the discriminator D
> >             real (tensor array) -- real images
> >             fake (tensor array) -- images generated by a generator
> > 
> >         Return the discriminator loss.
> >         We also call loss_D.backward() to calculate the gradients.
> >         """"""
> >         # Real
> >         pred_real = netD(real)
> >         loss_D_real = self.criterionGAN(pred_real, True)
> >         # Fake
> >         pred_fake = netD(fake.detach())
> >         loss_D_fake = self.criterionGAN(pred_fake, False)
> >         # wgan-gp
> >         gradient_penalty, gradients = networks.cal_gradient_penalty(
> >             netD, real, fake, self.device,lambda_gp=10.0
> >         )
> >         gradient_penalty.backward(retain_graph=True)
> >         # Combined loss and calculate gradients
> >         loss_D = (loss_D_real + loss_D_fake) * 0.5
> >         loss_D.backward()
> >         return loss_D
> > ```
> > 
> > 
> > 并开始训练
> > ```
> > python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --gan_mode wgangp --norm instance 
> > ```
> > 
> > 
> > 这是全部吗？还是我应该照顾其他事情？
> > 这是您尝试过的相同设置吗？
> 
> 嗨@ moh3th1。
> 我不知道您的帮派是否真的有效。如果使用原始配置，我的网络将出现分歧。那么，您对wgangp的超参数有什么建议吗？
> 
> 但是，似乎需要为wgan模型做些什么？

Is there a problem with the way you write loss_d? Loss_d = (loss_D_real + loss_D_fake) * 0.5， 0.5 is your hyperparameter, but should it be in this form: loss_d = (loss_d_fake-loss_d_real) * 0.5？",self real fake calculate gan loss discriminator network discriminator real tensor array real fake tensor array generator return discriminator loss also call calculate real real true fake false real fake combined loss calculate return python name model norm instance problem way write form,issue,negative,negative,negative,negative,negative,negative
608581660,"When you mentioned ""it didn't work"", does it mean a) the program crashes or 2) the results look worse?  

1. For CycleGAN, Resnet-based generators often work much better than UNet.
2. UNet does support images whose width and height are divisable by 256. For example, 512x512. 
",work mean program look worse often work much better support whose width height example,issue,negative,negative,neutral,neutral,negative,negative
608034820,"It only supports one result now. You probably have to modify the `visualizer.py`. For example, this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/visualizer.py#L150).",one result probably modify example line,issue,negative,neutral,neutral,neutral,neutral,neutral
608029957,Not sure if I understand your question. We used Visdom to visualize results. Please see the visdom [repo](https://github.com/facebookresearch/visdom) for more details. ,sure understand question used visualize please see,issue,positive,positive,positive,positive,positive,positive
607420363,"@tinghuiz  See this [Q&A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#for-labels2photo-cityscapes-evaluation-why-does-the-pretrained-fcn-8s-model-not-work-well-on-the-original-cityscapes-input-images-150) and [docs](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#evaluating-labels2photos-on-cityscapes) for more details. 
> The pre-trained model is not supposed to work on Cityscapes in the original resolution (1024x2048) as it was trained on 256x256 images that are upsampled to 1024x2048. The purpose of the resizing was to 1) keep the label maps in the original high resolution untouched and 2) avoid the need of changing the standard FCN training code for Cityscapes. To get the ground-truth numbers in the paper, you need to resize the original Cityscapes images to 256x256 before running the evaluation code.
",see model supposed work original resolution trained purpose keep label original high resolution untouched avoid need standard training code get paper need resize original running evaluation code,issue,positive,positive,positive,positive,positive,positive
606842683,Which generator `--netG` are you using? `unet_256` only supports 256x256 inputs. You may want to use `resnet_9blocks`.,generator may want use,issue,negative,neutral,neutral,neutral,neutral,neutral
606546148,"> It depends on your data. Maybe you could try `--load_size 2000` and `--crop_size 400`. Please see our [docs](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images) for more details.


Thanks for your reply and sorry to ask again..I'm just trying to understand ..my image size is all 2000*2000 and as it is mentioned in ur doc, I did first try the load size of 1024 and the crop size of 360 ..it works till it reaches to the print_freq, where it wants to save one sample and it gives the error of  **ValueError: Cannot feed value of shape (1, 256, 256, 6) for Tensor 'real_A_and_B_images:0', which has shape '(?, 360, 360, 6)'**.. can you please kindly help me with that? 

",data maybe could try please see thanks reply sorry ask trying understand image size ur doc first try load size crop size work till save one sample error feed value shape tensor shape please kindly help,issue,positive,positive,positive,positive,positive,positive
606101475,The default parameters for different `--model` might be different. The default generator for `--model pix2pix` is `unet256`. It is overwritten in this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L32).,default different model might different default generator model line,issue,negative,neutral,neutral,neutral,neutral,neutral
606098076,There are probably new methods. You can check which paper cites CycADA in google scholar. ,probably new check paper scholar,issue,negative,positive,positive,positive,positive,positive
605899758,Thanks @junyanz. What about Domain Adaptation? I noticed you guys did an extension called CyCADA. Did you do any updates on this one? Or do you know of newer methods in domain adaptations with GAN?,thanks domain adaptation extension one know domain gan,issue,negative,positive,positive,positive,positive,positive
605716004,"In the BaseOptions the default for --netG is set to 'resnet_9blocks'. As I did not change this, I assumed this is what was used. Is this correct for pix2pix too? In fact, these parameters were used during training:  --model pix2pix --gan_mode lsgan --direction AtoB\n (and a custom name), the remaining parameters therefore should take the default values, is this correct?",default set change assumed used correct fact used training model direction custom name therefore take default correct,issue,negative,neutral,neutral,neutral,neutral,neutral
605402199,You need to create a paired dataset and name it as `./datasets/dataset/train`.  You can download pix2pix paired dataset using downloading script `./datasets/download_pix2pix_dataset.sh` or create your own paired dataset using the script `./datasets/combine_A_and_B.py`.,need create paired name paired script create paired script,issue,negative,neutral,neutral,neutral,neutral,neutral
605387649,"> I recommend that you use pix2pix for paired data, as Pix2pix should work better than CycleGAN in most of the paired cases. If you still want to use CycleGAN, you can add the flag `--dataset_mode aligned`.

When I add --dataset_mode aligned to the option with the same datasets used last time for Cycle GAN, but it came out an error meaasage: AssertionError: ./datasets/dataset/train is not a valid directory .  Should I change the dir trainA and trainB into train/A and train/B ?",recommend use paired data work better paired still want use add flag add option used last time cycle gan came error valid directory change,issue,negative,positive,positive,positive,positive,positive
605099777,Yes. You probably need to write some visualization code to plot the data stored in this text file.,yes probably need write visualization code plot data text file,issue,negative,neutral,neutral,neutral,neutral,neutral
605099390,You can implement your loss and add it to this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L111).,implement loss add line,issue,negative,neutral,neutral,neutral,neutral,neutral
605095207,You can use placeholder images. But your error might not be related to `--dataset_mode`. I wonder if you used the same flags `--netG resnet_9blocks --norm instance --no_dropout` during training.,use error might related wonder used norm instance training,issue,negative,neutral,neutral,neutral,neutral,neutral
605092826,"You may want to increase your `--load_size`. This error happened when the shorter side of your resized image is smaller than `--fine_size`.  We plan to improve the implementation in the future, by giving you a warning if it happens. ",may want increase error shorter side image smaller plan improve implementation future giving warning,issue,negative,neutral,neutral,neutral,neutral,neutral
605091077,"I recommend that you use pix2pix for paired data, as Pix2pix should work better than CycleGAN in most of the paired cases. If you still want to use CycleGAN, you can add the flag `--dataset_mode aligned`.",recommend use paired data work better paired still want use add flag,issue,positive,positive,positive,positive,positive,positive
605089916,"I cannot answer your first question as I haven't done training with VM. For your second question, you can delete previous checkpoints as long as you keep the latest one. ",answer first question done training second question delete previous long keep latest one,issue,negative,positive,positive,positive,positive,positive
605089340,I am not sure if colab supports visdom. It should still print training loss and epoch/iter count. You can check the results in the HTML file. ,sure still print training loss count check file,issue,negative,positive,positive,positive,positive,positive
605088215,It depends on your data. Maybe you could try `--load_size 2000` and `--crop_size 400`. Please see our [docs](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images) for more details. ,data maybe could try please see,issue,negative,neutral,neutral,neutral,neutral,neutral
604892742,"> > I have the same problem,have you fixed it ? bro
> > check the version of pytorch and torchvision ,the versions of pytorch and torchvision should correspond to each other.

thx！
",problem fixed check version correspond,issue,negative,positive,neutral,neutral,positive,positive
604597047,"Same problem here, I want to apply pix2pix in single image mode, i.e. translating an image from domain A to domain B.

` python test.py --dataroot ./datasets/custom_dataset --name custom_dataset --model test --direction AtoB --dataset_mode single --netG resnet_9blocks  --norm instance --no_dropout
`


`Traceback (most recent call last):
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
    model_pix2pix.setup(opt)  # regular setup: load and print networks; create schedulers
  File ""path/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 88, in setup
    self.load_networks(load_suffix)
  File ""path/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 197, in load_networks
    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))
  File ""path/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 173, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
  File ""path/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 173, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
  File ""somepath/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 585, in __getattr__
    type(self).__name__, name))
AttributeError: 'Sequential' object has no attribute 'model'`


I have seen https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/913 and https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/466 , may be I have overlooked something? Could I alternatively use pairwise data with a placeholder image to predict something?

Setting --model test seems to cause the problem.",problem want apply single image mode image domain domain python name model test direction single norm instance recent call last file frozen line file frozen line file frozen line file frozen line file frozen line opt regular setup load print create file line setup file line net file line module key file line module key file line type self name object attribute seen may something could alternatively use pairwise data image predict something setting model test cause problem,issue,negative,negative,neutral,neutral,negative,negative
604099995,"Duplicate of https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/28. 

I presume the way is to extract it from the `loss_log.txt`?",duplicate presume way extract,issue,negative,neutral,neutral,neutral,neutral,neutral
603953974,"> The names are recently changed to `load_size` and `crop_size`.
> 
> 1. `load_size`. The goal is to resize all the training images to the same size.
> 2. `crop_size`. Random cropping is a data argumentation that can create more training data points per input image.

I do have an image of size 2000*2000 ...and I would like to try with this size.. what do you recommend to consider for the load_size and fine_size??",recently goal resize training size random data argumentation create training data per input image image size would like try size recommend consider,issue,positive,negative,negative,negative,negative,negative
603931746,"I do have an image of size 2000*2000 ...and I would like to try with this size.. what do you recommend to consider for the load_size and fine_size??
",image size would like try size recommend consider,issue,positive,neutral,neutral,neutral,neutral,neutral
602757607,"How to change crop size in code?



> On 23-Mar-2020, at 22:55, Jun-Yan Zhu <notifications@github.com> wrote:
> 
> ﻿
> Your image size might be smaller than crop size.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",change crop size code wrote image size might smaller crop size thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
602742872,Your image size might be smaller than crop size. ,image size might smaller crop size,issue,negative,neutral,neutral,neutral,neutral,neutral
602742490,The program doesn't support this. Maybe you can copy and paste `train` and `val` to a new directory called `trainval` and use the flag `--phase trainval`.  ,program support maybe copy paste train new directory use flag phase,issue,negative,positive,positive,positive,positive,positive
602741765,It is possible. Feel free to try it.,possible feel free try,issue,positive,positive,positive,positive,positive,positive
602739454,It seems some other programs have used some portion of GPUs. You can do `nvidia-smi` and see which program is yours and can be killed. ,used portion see program,issue,negative,neutral,neutral,neutral,neutral,neutral
602155300,"Hi, I have a similar error.
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 2.42 GiB already allocated; 26.12 MiB free; 2.75 GiB reserved in total by PyTorch).
Is that GPU also too small? ",hi similar error memory tried allocate mib gib total capacity gib already mib free gib reserved total also small,issue,negative,positive,neutral,neutral,positive,positive
602049839,my image size is greater than crop size.,image size greater crop size,issue,negative,positive,positive,positive,positive,positive
602007495,i got it. Can you tell me where in the code (lines numbers) we have to make changes. Thanks ,got tell code make thanks,issue,negative,positive,positive,positive,positive,positive
600159760,"To report further, downsampling the images to 256x256 and using full-sized images without cropped patches has made a significant difference to the training, and as @junyanz observed above, made it faster as well.

<img width=""628"" alt=""PastedGraphic-1"" src=""https://user-images.githubusercontent.com/3166852/76876655-9aaa2f80-6848-11ea-99a7-f28c7e5d86b9.png"">

<img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/3166852/76876690-aa297880-6848-11ea-9654-8a2969ff26e5.png"">
",report without made significant difference training made faster well image,issue,negative,positive,positive,positive,positive,positive
599770022,"I think it is fine. If you are concerned, you can slightly increase the capacity of G or decrease the capacity of D. You can control it using `--ngf` and `--ndf` ",think fine concerned slightly increase capacity decrease capacity control,issue,negative,positive,positive,positive,positive,positive
599296714,"> You can potentially detect if the image has a white background, by counting the number of white pixels or using a color histogram.

Thanks for your advice! I will take it!",potentially detect image white background counting number white color histogram thanks advice take,issue,negative,positive,neutral,neutral,positive,positive
599164595,It is possible. But I would not recommend it as it involves 3D geometric changes. Pix2pix is not good at dealing with (3D) geometric changes. There is a standard [library](https://talhassner.github.io/home/publication/2015_CVPR_1) for this application. ,possible would recommend geometric good dealing geometric standard library application,issue,positive,positive,positive,positive,positive,positive
599143066,"sorry, was trying to pull request into my own branch.",sorry trying pull request branch,issue,negative,negative,negative,negative,negative,negative
598968966,"Hello @junyanz,

Thanks for your prompt answer. 
Another question that I missed:
 
What type of discriminator should I use giving that as input I have a 32x32 img? ",hello thanks prompt answer another question type discriminator use giving input,issue,positive,positive,positive,positive,positive,positive
598855495,"You can potentially detect if the image has a white background, by counting the number of white pixels or using a color histogram. ",potentially detect image white background counting number white color histogram,issue,negative,neutral,neutral,neutral,neutral,neutral
598854833,You probably need to modify the model for your application. See this [paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Deep_Photo_Enhancer_CVPR_2018_paper.pdf) for more details. ,probably need modify model application see paper,issue,negative,neutral,neutral,neutral,neutral,neutral
598854214,"1 & 2. You can change `num_downs`. No need to change `ndf` and `ngf`.
3.  G_GAN: GAN loss for the generator; G_L1: L1 loss for the generator; D_real: GAN loss for the discriminator on real images; D_fake: GAN loss for the discriminator on fake images. See the [code's](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py) comments for more details. ",change need change gan loss generator loss generator gan loss discriminator real gan loss discriminator fake see code,issue,negative,negative,negative,negative,negative,negative
598476673,I see. You can reuse the visualization function. Maybe add another flag to control the frequency of certain metrics. We are working on a few paper deadlines. We will add more modules later. ,see reuse visualization function maybe add another flag control frequency certain metric working paper add later,issue,negative,positive,positive,positive,positive,positive
598411747,"@junyanz Well visualization is already done nicely by your package (plots of losses, visualization of examples), but it seems for metrics I had to implement custom functionality (`compute_visuals` wouldn't work for metrics, no?).

So @taesungp is already working on making metric computation part of your package?",well visualization already done nicely package visualization metric implement custom functionality would work metric already working making metric computation part package,issue,positive,positive,positive,positive,positive,positive
598297216,"You can surely do that. But sometimes, you want to visualize results more often than evaluating metrics. ",surely sometimes want visualize often metric,issue,negative,positive,positive,positive,positive,positive
598296806,"Yes, it's correct. You need to find the min and max of the whole dataset. You can modify this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L106). ",yes correct need find min whole modify line,issue,negative,positive,positive,positive,positive,positive
598295896,"you can try a smaller `--crop_size`. Yes, there is a tradeoff between texture and distortion, as the program is not aware of the difference between necessary and unnecessary changes.",try smaller yes texture distortion program aware difference necessary unnecessary,issue,negative,negative,neutral,neutral,negative,negative
597888791,@junyanz would I have to implement this `compute_visuals` function? Isn't this limited to displaying images (it calls display_current_results)?,would implement function limited,issue,negative,negative,neutral,neutral,negative,negative
597834557,"Yes we've tried with default `--netG` and `--netD` but the problem with distorted/bubbly output is still quite common. We have also tried doubling the amount of training data in domain B and training with different `--load_size` and `--crop_size`. Any other setting you can recommend?

Is it possible that we're training it for too long or short? We've never trained beyond 200 epochs. In the first epochs there is minimal distortion but the texture is quite bad. It seems like there is a trade-off between texture quality and object distortion the longer you train. ",yes tried default problem output still quite common also tried doubling amount training data domain training different setting recommend possible training long short never trained beyond first minimal distortion texture quite bad like texture quality object distortion longer train,issue,negative,negative,negative,negative,negative,negative
597813632,"Have you tried the default `--netG` and `--netD`? The results can be improved using different generator/discriminator architecture, although it is hard to get 100% looking good. 
",tried default different architecture although hard get looking good,issue,negative,positive,positive,positive,positive,positive
597811328,"@taesungp is considering adding a general function. Currently, you can add your function in [train.py](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/train.py#L58) and a new flag to specify reporting frequency. ",considering general function currently add function new flag specify frequency,issue,negative,positive,neutral,neutral,positive,positive
597071987,"We discovered a mistake in our own data augmentation step causing some training images in domain B to become distorted which was the reason for the bad result shown in my latest comment. We fixed it and re-did the run. Around half of the outputs from CycleGAN are amazing while some are less good.

Input:
![train1041_real](https://user-images.githubusercontent.com/36590759/76313142-afc91080-62d4-11ea-8344-72339b956f22.png) ![train1030_real](https://user-images.githubusercontent.com/36590759/76313344-10f0e400-62d5-11ea-8b40-3f04a1a8700b.png)
Output:
![train1041_fake](https://user-images.githubusercontent.com/36590759/76313137-af307a00-62d4-11ea-8448-5e4f4a2d063f.png) ![train1030_fake](https://user-images.githubusercontent.com/36590759/76313358-177f5b80-62d5-11ea-9984-ad587a49b76d.png)

We want to train a pose estimation network with synthetic training data and want to investigate if the pose-results can be improved on if the training data is first passed through CycleGAN to make it more realistic. For this to succeed we need a more consistent output from CycleGAN.

Since you have more experience on training CycleGAN I would really like to know your thoughts on this. Is it inevitable to get some ""bad"" outputs from CycleGAN or can the consistency be improved upon with more training data and/or deeper generator/discriminator architecture? We're currently running `--netG resnet_6blocks` and `--netD n_layers` with `--n_layers_D 3` on 256x256 images. 

Thanks a lot for your help!",discovered mistake data augmentation step causing training domain become distorted reason bad result shown latest comment fixed run around half amazing le good input output want train pose estimation network synthetic training data want investigate training data first make realistic succeed need consistent output since experience training would really like know inevitable get bad consistency upon training data architecture currently running thanks lot help,issue,positive,positive,positive,positive,positive,positive
596667210,"Thank you @junyanz. I'll incorporate the suggestions wrt to the training. Happy to report that with decreasing the discriminator capacity I am seeing vast improvements. However, even there, the first training I ran with the same settings (generator with 64 layers and discriminator with 32 layers) did not work, whereas the second training I am running is giving better results.",thank incorporate training happy report decreasing discriminator capacity seeing vast however even first training ran generator discriminator work whereas second training running giving better,issue,positive,positive,positive,positive,positive,positive
596256171,"The distribution of object poses need to be similar across domains A and B. Check out the ""Viewpoint estimation"" in the [VON](https://papers.nips.cc/paper/7297-visual-object-networks-image-generation-with-disentangled-3d-representations.pdf) paper for more details. ",distribution object need similar across check viewpoint estimation paper,issue,negative,neutral,neutral,neutral,neutral,neutral
596254070,I am not familiar with the function. But this [post](#203) seems to be related. ,familiar function post related,issue,negative,positive,positive,positive,positive,positive
596253551,The performance will not be affected. The data loading time might be slightly increased. There is a related [issue](#203) that you may want to check out. ,performance affected data loading time might slightly related issue may want check,issue,negative,neutral,neutral,neutral,neutral,neutral
596253256,"I recommend that you start your first experiment at a lower resolution model (`--load_size 286 --crop_size 256`) (We changed the flag name in the latest commit). The training will be faster and easier to debug. 

To continue the training from previous sessions, you need to use the flag `--continue_train`. If you can use `--epoch_count` to avoid overwriting your old models. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#can-i-continueresume-my-training-350-275-234-87) for more details.

Does the model make any changes to the training images during training? You may want to make sure that happens first. 

",recommend start first experiment lower resolution model flag name latest commit training faster easier continue training previous session need use flag use avoid old see model make training training may want make sure first,issue,positive,positive,positive,positive,positive,positive
596243220,"You can add it to the generator. But for the paper ""Selt-Attention GANs"" (as mentioned by @c1a1o1), the authors only add it to the discriminator. ",add generator paper add discriminator,issue,negative,neutral,neutral,neutral,neutral,neutral
596242978,"You may want to try [StarGAN](https://github.com/clovaai/stargan-v2)(-v2), [MUNIT](https://github.com/NVlabs/MUNIT) and [DRIT](https://github.com/HsinYingLee/DRIT)(++). The performance of each model highly depends on your application. No single model works better than others in all the datasets. ",may want try performance model highly application single model work better,issue,negative,positive,positive,positive,positive,positive
595708025,"I followed your advice and get slightly better results, but still need improvements.

1. ""The object pose (angle) should be consistent between real and 3D objects"" - just to clarify; should **all** objects in domain A and domain B have the **same** pose? Or do you mean that all (most) poses should be present in the two domains?

2. In the first ~50 epochs the pose of real_A and fake_B is consistent but after a while pose can get lost or distorted as shown below. 

The loss-plot looks good overall, i.e. the generator/discriminator jumps around in the interval ~ 0.1-0.8. We still have roughly 1500 images in each domain with random poses.

Epoch 45
![epoch045_real_A](https://user-images.githubusercontent.com/36590759/76075065-a2dab300-5f9c-11ea-9a00-805a278da6b4.png)
![epoch045_fake_B](https://user-images.githubusercontent.com/36590759/76075067-a40be000-5f9c-11ea-938f-3642e767bee2.png)

Epoch 184
![epoch184_real_A](https://user-images.githubusercontent.com/36590759/76076327-f1894c80-5f9e-11ea-9103-b1e1259d0f49.png)
![image](https://user-images.githubusercontent.com/36590759/76075091-b0903880-5f9c-11ea-8cf1-c9a5979f935f.png)



Here is the train_opt.txt as well.
`----------------- Options ---------------`
 `              batch_size: 4                             	[default: 1]`
  `                  beta1: 0.5                           `
   `       checkpoints_dir: ./checkpoints                 `
  `         continue_train: True                          	[default: False]`
       `         crop_size: 200                           	[default: 256]`
      `           dataroot: ./datasets/tless1437          	[default: None]`
     `        dataset_mode: unaligned                     `
    `            direction: AtoB                          `
   `           display_env: main                          `
  `           display_freq: 400                           `
 `              display_id: 1                             `
`            display_ncols: 4                             `
`             display_port: 8097                          `
`           display_server: http://localhost              `
`          display_winsize: 256                           `
`                    epoch: latest                        `
`              epoch_count: 40                            	[default: 1]`
    `             gan_mode: lsgan                         `
    `              gpu_ids: 0                             `
    `            init_gain: 0.02                          `
    `            init_type: normal                        `
    `             input_nc: 3                             `
    `              isTrain: True                          	[default: None]`
    `             lambda_A: 10.0                          `
    `             lambda_B: 10.0                          `
    `      lambda_identity: 0.5                          ` 
    `            load_iter: 0                             	[default: 0]`
  `              load_size: 256                           	[default: 286]`
`                       lr: 0.0002                        `
`           lr_decay_iters: 50                            `
  `              lr_policy: linear                        `
  `       max_dataset_size: inf                      `     
  `                  model: cycle_gan                   `  
  `               n_epochs: 100                           `
  `         n_epochs_decay: 100                       `    
  `             n_layers_D: 2                             	[default: 3]`
`                     name: 1437A_1296B_small             	[default: experiment_name]`
`                      ndf: 64                            `
`                     netD: n_layers                      	[default: basic]`
`                     netG: resnet_6blocks                	[default: resnet_9blocks]`
  `                    ngf: 64                            `
  `             no_dropout: True                    `      
  `                no_flip: False                         `
  `                no_html: False                        ` 
  `                   norm: instance                      `
   `           num_threads: 4                             `
`                output_nc: 3                             `
  `                  phase: train                         `
  `              pool_size: 50                            `
    `           preprocess: resize_and_crop     `          
   `            print_freq: 100                           `
  `           save_by_iter: False                        ` 
 `         save_epoch_freq: 10                          `  
`         save_latest_freq: 5000                 `         
`           serial_batches: False                `         
`                   suffix:                               `
`         update_html_freq: 1000                    `      
`                  verbose: False                         `
`----------------- End -------------------`",advice get slightly better still need object pose angle consistent real clarify domain domain pose mean present two first pose consistent pose get lost distorted shown good overall around interval still roughly domain random epoch epoch image well default beta true default false default default none unaligned direction main epoch latest default normal true default none default default linear model default name default default basic default true false false norm instance phase train false false suffix verbose false end,issue,positive,positive,neutral,neutral,positive,positive
595424390,"> It is possible. I think the self-attention layer should be added to the discriminator. You don't need to change the generator. You can just take the D from self-att GANs.

@junyanz Can you please explain that why we do not need attention layer in generator?",possible think layer added discriminator need change generator take please explain need attention layer generator,issue,negative,neutral,neutral,neutral,neutral,neutral
594234866,"Per some readings, and after some conversations with folks who know a little more about machine learning than I do, I am trying to increase the generator capacity and decrease the discriminator capacity, by using `--ngf 64 and --ndf 32` in my training code. I am restarting the training, so my training command is: `python3 train.py --dataroot ./datasets/male2femalehd/ --name male2femalehd_cyclegan --model cycle_gan --resize_or_crop scale_width_and_crop --fineSize 360 --loadSize 1024 --batch_size 10 --gpu_ids 0,1,2,3 --norm instance --save_epoch_freq 1 --netG resnet_9blocks --ndf 32`",per know little machine learning trying increase generator capacity decrease discriminator capacity training code training training command python name model norm instance,issue,negative,negative,negative,negative,negative,negative
594233906,"Some related tickets for my reference as I research this:
* Information about loss curves: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/806
* Definition of generator and discriminator: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/941
* Learning rate and batch size: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/779
* Parameters common to train and test: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py
* Architectures for generator and discriminator when the dataset contains images of large sizes: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/760
* D loss drops but G loss does not drop: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/858 and https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/search?q=D+loss+0&type=Issues",related reference research information loss definition generator discriminator learning rate batch size common train test generator discriminator large size loss loss drop,issue,negative,negative,neutral,neutral,negative,negative
594154727,You need to crop the object before  running our program. Our program only does random cropping. ,need crop object running program program random,issue,negative,negative,negative,negative,negative,negative
594103172,"@junyanz sorry to ping you directly, but any advice would help. I am not sure where I am making a mistake in the training.",sorry ping directly advice would help sure making mistake training,issue,negative,positive,neutral,neutral,positive,positive
594100652,"I get the idea, thanks. Is it the `--crop_size` flag that I should use for this? My training images are 256x256 - what would be an appropriate crop size?",get idea thanks flag use training would appropriate crop size,issue,negative,positive,positive,positive,positive,positive
594074577,"The object pose (angle) should be consistent between real and 3D objects. For example, if your 3D objects are always frontal and your real objects always face to left, it is hard for CycleGAN to learn the mapping. Cropping will help the model focus on the object of interest, and also potentially increase the resolution of results. ",object pose angle consistent real example always frontal real always face left hard learn help model focus object interest also potentially increase resolution,issue,positive,positive,neutral,neutral,positive,positive
593803150,"Thanks for the reply! 

What do you mean when saying that I need the correct orientation for the 3D model? What will cropping the training data add when training the model?",thanks reply mean saying need correct orientation model training data add training model,issue,negative,negative,neutral,neutral,negative,negative
593642869,"## Fake image
![00010_fake_B](https://user-images.githubusercontent.com/3166852/75721456-678f6a00-5ca6-11ea-8980-31369d0d5552.png)

## Real image
![00010_real_A](https://user-images.githubusercontent.com/3166852/75721458-678f6a00-5ca6-11ea-8757-3d39115f5fc1.png)
",fake image real image,issue,negative,negative,negative,negative,negative,negative
593537572,See the definition of our [generator](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L315) and [discriminator](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L538).  ,see definition generator discriminator,issue,negative,neutral,neutral,neutral,neutral,neutral
593536843,You probably need to have the correct orientation for your 3D model. You should also train your model on cropped objects. ,probably need correct orientation model also train model,issue,negative,neutral,neutral,neutral,neutral,neutral
592987856,"@sanket-p 
First:
pip install Pillow

Then:
from PIL import Image",first pip install pillow import image,issue,negative,positive,positive,positive,positive,positive
592297586,"I got this error when I inadvertently downgraded pytorch to a CPU-only version (by conda installing some other packages).

Should be verifiable via:
```
>>> torch.cuda.is_available()
False
```

My fix was reinstalling pytorch/torchvision to these pinned versions:
```
conda install pytorch=1.3.1 torchvision=0.4.2 -c pytorch
```
, which showed the CPU-only version was in fact installed:
```
The following packages will be SUPERSEDED by a higher-priority channel:

  pytorch            pkgs/main::pytorch-1.3.1-cpu_py37h62f~ --> pytorch::pytorch-1.3.1-py3.7_cuda10.1.243_cudnn7.6.3_0
  torchvision        pkgs/main::torchvision-0.4.2-cpu_py37~ --> pytorch::torchvision-0.4.2-py37_cu101
```
",got error inadvertently version verifiable via false fix pinned install version fact following channel,issue,negative,negative,negative,negative,negative,negative
590966584,"I see, makes sense. Thanks for the response. I'm going to play around with different learning rates for each of the four networks (G_A, G_B, D_A, D_B). I have noticed one of the discriminators going to zero (< 0.09 loss) pretty quickly and staying there. ",see sense thanks response going play around different learning four one going zero loss pretty quickly,issue,positive,positive,positive,positive,positive,positive
590770203,See [README](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan). You can use the `--model test`. See an example script `./scripts/test_single.sh`.,see use model test see example script,issue,negative,neutral,neutral,neutral,neutral,neutral
590768922,"1. see this [Q & A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images)
2. If you have a paired training set, you should use pix2pix. Otherwise, use CycleGAN.",see paired training set use otherwise use,issue,negative,neutral,neutral,neutral,neutral,neutral
590767596,Not sure if it is related to our repo. Could you post the questions at PyTorch repo or ganomaly repo?,sure related could post,issue,negative,positive,positive,positive,positive,positive
590766235,You can filter them out if you don't expect to see them during test time. ,filter expect see test time,issue,negative,neutral,neutral,neutral,neutral,neutral
590745843,"
I solved it.
My local computer was set to the window version, and the server I learned was based on linux. Fixed the checkpoint version.",local computer set window version server learned based fixed version,issue,negative,positive,neutral,neutral,positive,positive
589936939,Thank you for the suggestion! I updated the Dockerfile following @arbrog 's suggestion. ,thank suggestion following suggestion,issue,negative,neutral,neutral,neutral,neutral,neutral
589910819,It could be. Sometimes the task of A->B might be harder than B->A or vice versa. It's hard to find two domains where both directions are equally hard/easy.,could sometimes task might harder vice hard find two equally,issue,negative,negative,negative,negative,negative,negative
589910697,Both ideas look good to me. You may want to run CycleGAN with default architectures and use it as a baseline model. ,look good may want run default use model,issue,negative,positive,positive,positive,positive,positive
588867419,"Should I convert model from pytorch to caffe and run evaluate.py in script folder?
The code use caffe in the evaluate.py.
Is it possible to add some arg like --eval ""fcn-score"" when I use the python train.py ?",convert model run script folder code use possible add like use python,issue,negative,neutral,neutral,neutral,neutral,neutral
588537984,"I was having issues with your Dockerfile which seems to specify [minconda 2 (ie python 2) ](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/8cda06f7c36b012769efac63adc1a68586b8fb85/docs/Dockerfile#L4)while the rest of your code uses [python 3](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/8cda06f7c36b012769efac63adc1a68586b8fb85/environment.yml#L6). I was having issues so I made a Dockerfile using miniconda3 (and CUDA 10.1). Seems like this issue was never fixed but the ticket was closed?

```
FROM nvidia/cuda:10.1-base

RUN apt update && apt install -y wget unzip curl bzip2 git
RUN curl -LO http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
RUN bash Miniconda3-latest-Linux-x86_64.sh -p /miniconda -b
RUN rm Miniconda3-latest-Linux-x86_64.sh
ENV PATH=/miniconda/bin:${PATH}
RUN conda update -y conda

RUN conda install -y pytorch torchvision -c pytorch
RUN mkdir /workspace/ && cd /workspace/ && git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git && cd pytorch-CycleGAN-and-pix2pix && pip install -r requirements.txt

WORKDIR /workspace
```",specify ie python rest code python made like issue never fixed ticket closed run apt update apt install curl git run curl run bash run path run update run install run git clone pip install,issue,negative,positive,positive,positive,positive,positive
587936979,"Have a similar issue, will try to modify the generator and discriminator layer.",similar issue try modify generator discriminator layer,issue,negative,neutral,neutral,neutral,neutral,neutral
587595358,Please see this [post](https://github.com/phillipi/pix2pix/issues/112) for my answers. The program will produce output images by default. It will be saved by different file names. ,please see post program produce output default saved different file,issue,positive,neutral,neutral,neutral,neutral,neutral
587587352,"You can add dropout to the generator by removing the flag `--no_dropout`. In the test time, it can produce some variations. To get more diverse outputs, you can have a look at some recent work such as [MUNIT](https://github.com/NVlabs/MUNIT), [DRIT](https://github.com/HsinYingLee/DRIT), and [Augmented CycleGAN](https://github.com/aalmah/augmented_cyclegan).",add dropout generator removing flag test time produce get diverse look recent work augmented,issue,negative,neutral,neutral,neutral,neutral,neutral
587369286,"I see the `evaluate.py` in the script folder, but I don't understand how to use it when I run the `train.py` and `test.py`.
The `evaluate.py` need the three folder, ""cityscapes_dir"", ""result_dir"" and ""output_dir"".
But after I run the `test.py`, the fake and real is group in the one folder since ""--direction AtoB"".
How to setup the arg that separate the generator image and ground truth image?
Thanks",see script folder understand use run need three folder run fake real group one folder since direction setup separate generator image ground truth image thanks,issue,negative,negative,neutral,neutral,negative,negative
587262879,"Sometimes, the artifacts will disappear with more training time. You also check the padding of your upsampling layer. The reflect padding is recommended compared to the zero padding.",sometimes disappear training time also check padding layer reflect padding zero padding,issue,negative,neutral,neutral,neutral,neutral,neutral
586165905,"Finally, after days of tries and errors, I am sure that the shift comes from the pretrained model, since I visualized every line of codes before the net.forward(), and there is no shift. So I suppose that it is the pretrained model which cause the shift.

`

    in_ = np.array(im, dtype=np.float32)
    # There are 11 mode:constant,reflect,edge and so on
    in_ = np.pad(in_, ((border, border), (border, border), (0, 0)), 'reflect')

    in_ = in_[:, :, 0:3]
    # switch to BGR
    in_ = in_[:, :, ::-1]
    # subtract mean
    in_ -= np.array((104.00698793, 116.66876762, 122.67891434))
    # make dims C x H x W for Caffe
    in_ = in_.transpose((2, 0, 1))

    # remove the following two lines if testing with cpu

    # shape for input (data blob is N x C x H x W), set data
    net.blobs['data'].reshape(1, *in_.shape)
    net.blobs['data'].data[...] = in_
    # run net and take argmax for prediction
    net.forward()
    fuse = net.blobs['sigmoid-fuse'].data[0][0, :, :]
`

================================================================

![out](https://user-images.githubusercontent.com/11862759/74516320-2891b280-4f4b-11ea-948c-1c0abbf40de7.png) 

![test2](https://user-images.githubusercontent.com/11862759/74516353-35aea180-4f4b-11ea-90c6-2074fd7781f4.png) 

![test3](https://user-images.githubusercontent.com/11862759/74516380-3d6e4600-4f4b-11ea-9c15-00579bf047df.png)

=================================================================

Done, with a little upset.
",finally day sure shift come model since every line shift suppose model cause shift mode constant reflect edge border border border border switch subtract mean make remove following two testing shape input data blob set data run net take prediction fuse test test done little upset,issue,negative,neutral,neutral,neutral,neutral,neutral
586096454,It can vary based on your GPU and hard drive The program will print timing per iteration on the console. You can calculate the training time. ,vary based hard drive program print timing per iteration console calculate training time,issue,negative,negative,negative,negative,negative,negative
586096107,"By default, your models will be saved every `--save_latest_freq` iterations (default 5000) or every `--save_epoch_freq` (default 5) epoches. In your case, as you only trained your model for 2 epoches, none of them would be saved. ",default saved every default every default case trained model none would saved,issue,positive,neutral,neutral,neutral,neutral,neutral
586045520,"Thanks!

In your tutorial ipynb, I ran the following command to train my model (I know n_epochs is too small, but this is more of a sanity check whether things work, as each epoch takes 400 s on my Google Colab single GPU).
 
`!python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan --gpu_ids 0 --n_epochs 1 --n_epochs_decay 1  --display_id 0`

This is your horse2zebra dataset, nothing new from my side.

But then I don't generate `./checkpoints/horse2zebra/latest_net_G_A.pth` as expected. In fact there is no .pth file generated. Whereas the pretrained folder `./checkpoints/horse2zebra_pretrained` has this pth file. ",thanks tutorial ran following command train model know small sanity check whether work epoch single python name model nothing new side generate fact file whereas folder file,issue,negative,positive,neutral,neutral,positive,positive
585883321,"It still uses GPU for training. However, since you call  vgg = VGGNet().cuda().eval() in the backward function, it takes a long time for initialization each time. Try put this in the initialization function. ",still training however since call backward function long time time try put function,issue,negative,negative,neutral,neutral,negative,negative
585271079,Glad that you figured it out. I closed this issue. ,glad figured closed issue,issue,negative,positive,positive,positive,positive,positive
585001119,"> When updating D, we don't need to compute gradients for G. That's why we used `detach` in `backward_D_basic`.

Thank you for your reply, that's helpful !!!
And after reading the below article, I finally understand the reason of using detach in above.
[https://blog.csdn.net/Hungryof/article/details/78035332](url)

",need compute used detach thank reply helpful reading article finally understand reason detach,issue,positive,neutral,neutral,neutral,neutral,neutral
584818657,You can use the flag `--continue_train`. It will load the weights from an existing network. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#fine-tuningresume-training) for more tips.,use flag load network see,issue,negative,neutral,neutral,neutral,neutral,neutral
584816535,"It depends on the type of GAN models. For CycleGAN, we don't need one-to-one correspondence. For pix2pix, we need. ",type gan need correspondence need,issue,negative,neutral,neutral,neutral,neutral,neutral
584815891,"When updating D, we don't need to compute gradients for G. That's why we used `detach` in `backward_D_basic`.",need compute used detach,issue,negative,neutral,neutral,neutral,neutral,neutral
584814907,"`--refresh` is currently not a flag. It is hard-coded. If you want to change, you can modify this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/dca9002f598fada785b92d496fcdc7b04528b393/util/visualizer.py#L165).",refresh currently flag want change modify line,issue,negative,neutral,neutral,neutral,neutral,neutral
584763505,"Hey guys, I have this problem too...

In the file ""pytorch-CycleGAN-and-pix2pix/util/html.py"" I have that line, as suggested by @mrgloom  but it looks just like a comment. Where the actual variable is defined?

I also tried editing the ""web/index.html"" file but I lost the edit everytime the training process updates it.

Where can I use the ""--refresh"" flag? I tried to add ""--refresh 0"" in the script that runs my train.py (together with --name, --model, --continue_train, --epoch_count and so on) but it's considered unknown.

edit: I used this workaround in ""pytorch-CycleGAN-and-pix2pix/util/html.py"", line 31:

```
        if refresh > 0:
            with self.doc.head:
                meta(http_equiv=""refresh"", content=""10000"")
```

I don't know if the ""refresh"" variable is defined elsewhere, so I used this solution to avoid possible conflicts.",hey problem file line like comment actual variable defined also tried file lost edit training process use refresh flag tried add refresh script together name model considered unknown edit used line refresh meta know refresh variable defined elsewhere used solution avoid possible,issue,negative,negative,neutral,neutral,negative,negative
584724273,"Hi, I just started playing with the horse2zebra dataset. I am a little new to GANs. In usual ML models, I am used to training data having a one-one correspondence. That is, if horse pics imageA001 to imageA009.jpg are in folder trainA, then imageB001 to imageB009.jpg should be the corresponding zebra images with the same background and so on, just the horse body replaced with a ditto zebra body. But then I don't see this kind of a one-one labeling in the train data folders. Is such a one-one labelling unimportant for GANs?",hi little new usual used training data correspondence horse folder corresponding zebra background horse body ditto zebra body see kind train data unimportant,issue,negative,negative,neutral,neutral,negative,negative
584501726,"Find an simple and crude solution: 

`# get rid of the border`
`#fuse = fuse[border:-border, border:-border]`
`# from 32+128 to 32+128+256,the 32 can be adjust! 34,35......`
`fuse = fuse[163:419, 163:419]`

And the result is below:

=============================

![3](https://user-images.githubusercontent.com/11862759/74216412-52df3800-4cdf-11ea-837a-c6e466b0f9b2.png)


=============================

However, this solution did not get the reason why that shift happen, so I will continue.",find simple crude solution get rid border fuse fuse border border adjust fuse fuse result however solution get reason shift happen continue,issue,negative,negative,negative,negative,negative,negative
584448114,"I used GIMP to measure the shift pix, it seems 32 pix. hum......",used gimp measure shift pix pix hum,issue,negative,neutral,neutral,neutral,neutral,neutral
584445224,"Continue. Go on locating the problem, I annotate the border cuting code to see the difference of images:

`# get rid of the border`
`#fuse = fuse[border:-border, border:-border]`

and the result is interesting, I got a 512*512 image, and it looks like:

=================================

![2](https://user-images.githubusercontent.com/11862759/74205589-fe25c800-4cb2-11ea-944b-44ea707546bd.png)

================================= 

So, I realized that **there must some code let the whole edges shift right and down**, I can get the right edges!",continue go problem annotate border code see difference get rid border fuse fuse border border result interesting got image like must code let whole shift right get right,issue,negative,positive,positive,positive,positive,positive
584395784,Hard to answer your question without knowing your problem and dataset. ,hard answer question without knowing problem,issue,negative,negative,negative,negative,negative,negative
583809188,"@junyanz 
Or more concretely, I changed the BCE loss to MSE loss (according to the LSGAN). The model just failed, d loss is close to 0 while the g loss keeping increasing",concretely loss loss according model loss close loss keeping increasing,issue,negative,positive,positive,positive,positive,positive
583808930,"@junyanz 
hey professor?
Could you tell me if there is any suggection to weaker the discriminator instead of changing LR?",hey professor could tell discriminator instead,issue,negative,neutral,neutral,neutral,neutral,neutral
583796582,"Continue. In order to **locate problem**, I add some code at the end of batch_hed.py, to **visualize the fuse ndarray, which is the output of hed model**. The codes are as follow:
` 
  
    # test code for fuse visualization
    # get to know fuse data structure 
    # print(type(fuse))
    # print(fuse.size)
    # print(fuse.shape)
    # clip float32 value range to [0.0-1.0]
    fuse = np.clip(fuse, 0.0, 1.0)
    # output image
    Image.fromarray(((fuse - 1.0) * (- 255.0)).astype(np.uint8)).save('./out.png')
`
Then I got the image, it shows that the part of the edges shifed and mirrored problem is still here, however, now I know the problem is in batch_hed.py. The image is bellow:

==================================

![out](https://user-images.githubusercontent.com/11862759/74094959-7ca02f80-4b24-11ea-93a6-7a3254c2a1a9.png)

==================================",continue order locate problem add code end visualize fuse output model follow test code fuse visualization get know fuse data structure print type fuse print print clip float value range fuse fuse output image fuse got image part mirrored problem still however know problem image bellow,issue,negative,neutral,neutral,neutral,neutral,neutral
583703370,"Thank you for your help, I have solved my problem, loss_D, loss_G is normal.But the quality of the generated picture is still a bit poor, and it does not meet my expectations,.How can I improve the quality of the generated picture.",thank help problem quality picture still bit poor meet improve quality picture,issue,negative,negative,negative,negative,negative,negative
583562213,Neither of those mentioned issues solve the problem.  #360 is in reference to something really old and #267 is a reference to configuring something to run on a mac cpu.,neither solve problem reference something really old reference something run mac,issue,negative,positive,neutral,neutral,positive,positive
583286834,"<img width=""1063"" alt=""截屏2020-02-07下午4 18 38"" src=""https://user-images.githubusercontent.com/11862759/74013292-105be980-49c7-11ea-8fac-2a9e0bf51de9.png"">

I **tried several border value** , from 128 to 8 (128, 64, 32, 16, 8), it seems that border is not the answer.",tried several border value border answer,issue,negative,neutral,neutral,neutral,neutral,neutral
583286106,"Very very long time. I advice you that only use pretrained model to predict on CPU, or training on a tiny dataset.",long time advice use model predict training tiny,issue,negative,negative,neutral,neutral,negative,negative
583248650,"Thank you so much for your time, it worked! ",thank much time worked,issue,negative,positive,positive,positive,positive,positive
583226161,You need to add `--norm batch`.,need add norm batch,issue,negative,neutral,neutral,neutral,neutral,neutral
583195176,"> I am not sure what happened. Maybe you want to check the flag `--border`. Here is the [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/edges/batch_hed.py#L28)

Thanks, I analyze some factors, which including it. I will try it today.",sure maybe want check flag border line thanks analyze try today,issue,positive,positive,positive,positive,positive,positive
583169430,"Thanks! I get the following error saying (RuntimeError: Error(s) in loading state_dict for ResnetGenerator ) and during my training, I used Resnet9block as my generator network and below are my training and test command. It would be great if you can tell me where I went wrong. Thank you so much @junyanz 

**Training command:** 

python train.py --dataroot ./thousandimages/dataset --name pix2pix1000Res --model pix2pix  --direction AtoB --no_flip --checkpoints_dir trained_model  --netG resnet_9blocks --display_freq 1 --num_threads 1 --save_epoch_freq 2 --save_latest_freq 1

**Test command:** 

python test.py --dataroot ./thousandimages/dataset/testA --model test --name pix2pix1000Res  --direction AtoB  --checkpoints_dir trained_model --netG resnet_9blocks 

**Error:** 

----------------- Options ---------------
             aspect_ratio: 1.0                           
             batch_size: 1                             
            checkpoints_dir: trained_model                 	[default: ./checkpoints]
            crop_size: 256                           
            dataroot: ./thousandimages/starcraft/testA	[default: None]
           dataset_mode: single                        
           direction: AtoB                          
          display_winsize: 256                           
                    epoch: latest                        
                     eval: False                         
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                isTrain: False                         	[default: None]
                load_iter: 0                             	[default: 0]
                load_size: 256                           
         max_dataset_size: inf                           
                    model: test                          
             model_suffix:                               
               n_layers_D: 3                             
                     name: pix2pix1000Restrans           	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: resnet_9blocks                
                      ngf: 64                            
               no_dropout: False                         
                  no_flip: True                          	[default: False]
                     norm: instance                      
                    ntest: inf                           
                 num_test: 50                            
              num_threads: 1                             	[default: 4]
                output_nc: 3                             
                    phase: test                          
               preprocess: resize_and_crop               
              results_dir: ./results/                    
           serial_batches: False                         
                   suffix:                               
                  verbose: False                         
----------------- End -------------------
dataset [SingleDataset] was created
initialize network with normal
model [TestModel] was created
loading the model from trained_model/expname/latest_net_G.pth
Traceback (most recent call last):
  File ""test.py"", line 47, in <module>
    model.setup(opt)               # regular setup: load and print networks; create schedulers
  File ""/content/gdrive/My Drive/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 88, in setup
    self.load_networks(load_suffix)
  File ""/content/gdrive/My Drive/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 198, in load_networks
    net.load_state_dict(state_dict)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 830, in load_state_dict
    self.__class__.__name__, ""\n\t"".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for ResnetGenerator:
	Missing key(s) in state_dict: ""model.1.bias"", ""model.4.bias"", ""model.7.bias"", ""model.10.conv_block.1.bias"", ""model.10.conv_block.6.bias"", ""model.11.conv_block.1.bias"", ""model.11.conv_block.6.bias"", ""model.12.conv_block.1.bias"", ""model.12.conv_block.6.bias"", ""model.13.conv_block.1.bias"", ""model.13.conv_block.6.bias"", ""model.14.conv_block.1.bias"", ""model.14.conv_block.6.bias"", ""model.15.conv_block.1.bias"", ""model.15.conv_block.6.bias"", ""model.16.conv_block.1.bias"", ""model.16.conv_block.6.bias"", ""model.17.conv_block.1.bias"", ""model.17.conv_block.6.bias"", ""model.18.conv_block.1.bias"", ""model.18.conv_block.6.bias"", ""model.19.bias"", ""model.22.bias"". 
	Unexpected key(s) in state_dict: ""model.2.weight"", ""model.2.bias"", ""model.5.weight"", ""model.5.bias"", ""model.8.weight"", ""model.8.bias"", ""model.10.conv_block.2.weight"", ""model.10.conv_block.2.bias"", ""model.10.conv_block.7.weight"", ""model.10.conv_block.7.bias"", ""model.11.conv_block.2.weight"", ""model.11.conv_block.2.bias"", ""model.11.conv_block.7.weight"", ""model.11.conv_block.7.bias"", ""model.12.conv_block.2.weight"", ""model.12.conv_block.2.bias"", ""model.12.conv_block.7.weight"", ""model.12.conv_block.7.bias"", ""model.13.conv_block.2.weight"", ""model.13.conv_block.2.bias"", ""model.13.conv_block.7.weight"", ""model.13.conv_block.7.bias"", ""model.14.conv_block.2.weight"", ""model.14.conv_block.2.bias"", ""model.14.conv_block.7.weight"", ""model.14.conv_block.7.bias"", ""model.15.conv_block.2.weight"", ""model.15.conv_block.2.bias"", ""model.15.conv_block.7.weight"", ""model.15.conv_block.7.bias"", ""model.16.conv_block.2.weight"", ""model.16.conv_block.2.bias"", ""model.16.conv_block.7.weight"", ""model.16.conv_block.7.bias"", ""model.17.conv_block.2.weight"", ""model.17.conv_block.2.bias"", ""model.17.conv_block.7.weight"", ""model.17.conv_block.7.bias"", ""model.18.conv_block.2.weight"", ""model.18.conv_block.2.bias"", ""model.18.conv_block.7.weight"", ""model.18.conv_block.7.bias"", ""model.20.weight"", ""model.20.bias"", ""model.23.weight"", ""model.23.bias"". ",thanks get following error saying error loading training used generator network training test command would great tell went wrong thank much training command python name model direction test command python model test name direction error default default none single direction epoch latest false normal false default none default model test name default basic false true default false norm instance default phase test false suffix verbose false end initialize network normal model loading model recent call last file line module opt regular setup load print create file line setup file line file line error loading missing key model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias model bias unexpected key model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias,issue,negative,negative,neutral,neutral,negative,negative
583096777,I am not sure what happened. Maybe you want to check the flag `--border`. Here is the [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/edges/batch_hed.py#L28),sure maybe want check flag border line,issue,negative,positive,positive,positive,positive,positive
583095510,"The `--model test` also works for the pix2pix model. You need to modify the model name using ` --name`. Also, you need to make sure the dropout and normalization flags are the same as those used during training. ",model test also work model need modify model name name also need make sure dropout normalization used training,issue,negative,positive,positive,positive,positive,positive
582999592,"You can increase or decrease the number of channels per layer and the number of layers. Here is a [post](https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/). In our code, for the generator, you can modify `--ngf` and use different generators (`--netG`). For the discrimnator, you can modify `--ndf`, `--netD`, and `--n_layers_D`. See the [base_options.py](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py) for more details.",increase decrease number per layer number post code generator modify use different modify see,issue,negative,neutral,neutral,neutral,neutral,neutral
582776422,Just add it after your training command. For example: python test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA --gpu_ids -1,add training command example python name model direction,issue,negative,neutral,neutral,neutral,neutral,neutral
582769050,"I m getting this error as i dont have gpu and cuda.
please help, thanks.
![SharedScreenshot](https://user-images.githubusercontent.com/54542400/73914060-6ef96880-48de-11ea-94e5-f70f602dbbf2.jpg)
",getting error dont please help thanks,issue,negative,positive,positive,positive,positive,positive
582765433,"> can i do training with gpu and cuda as i dont have these resources right now? i have only cpu. thanks

Yes you can, I do it all day.",training dont right thanks yes day,issue,negative,positive,positive,positive,positive,positive
582717397,What is the capacity of generator or discirminator?I don't quite understand.,capacity generator quite understand,issue,negative,neutral,neutral,neutral,neutral,neutral
582616115,"You can use `--save_epoch_freq` to specify how frequently you save models (see this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L24)). 

When you continue the previous training, the program will load the model specified by `--epoch` (see this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L53)). 

Regarding `--epoch_count`, it decides the starting epoch count. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L27). If you stopped at 50th epoch in your previous experiments, you can set `--epoch_count 50`.",use specify frequently save see line continue previous training program load model epoch see line regarding starting epoch count see line stopped th epoch previous set,issue,negative,negative,neutral,neutral,negative,negative
582611146,Maybe you want to increase the capacity of your generator and/or decrease the capacity of the discriminator.,maybe want increase capacity generator decrease capacity discriminator,issue,negative,neutral,neutral,neutral,neutral,neutral
582607003,You can use the flag `--direction`. (with options `AtoB` or `BtoA`). See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L42) for more details.,use flag direction see line,issue,negative,neutral,neutral,neutral,neutral,neutral
581709116,Ya i am using company cluster.....maybe killed by others I guess:(,ya company cluster maybe guess,issue,negative,neutral,neutral,neutral,neutral,neutral
581657609,Hard to know what has happened without further information. Are you using a cluster? It might be killed by other users. ,hard know without information cluster might,issue,negative,negative,negative,negative,negative,negative
581267586,Sorry it doesn't always end at one particular epoch. just around 25. Thank you!,sorry always end one particular epoch around thank,issue,negative,negative,negative,negative,negative,negative
581266716,"Hi there, my program is always killed with a single message ""Killed"" at one particular epoch, which is epoch 29. I really cannot figure out why without the exact error message. By the way, I am using 8G GPU, shouldn't be the memory issue.",hi program always single message one particular epoch epoch really figure without exact error message way memory issue,issue,negative,positive,positive,positive,positive,positive
581266067,"I found the problem. The epoch is too small ,I use 8,it worked",found problem epoch small use worked,issue,negative,negative,negative,negative,negative,negative
581129329,"My datasets are1008 other pictures different form the offcial photos. Besides,I didn't change anything",different form besides change anything,issue,negative,neutral,neutral,neutral,neutral,neutral
581129153,"I already uesd --checkpoints_dir checkpoints,My commnd is  `python train.py --dataroot ./datasets/makeup--name makeup_gan --model cycle_gan --display_id -1 --n_epochs 1 --n_epochs_decay 1  --checkpoints_dir checkpoints`。 but still no model appeard in this folder",already python name model still model folder,issue,negative,neutral,neutral,neutral,neutral,neutral
580662364,"> by default, 200 epochs are selected, first 100 with the same learning rate and then 100 more to get it to 0. you can see the parameters in https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/options
> 
> I just set --epoch_count 1 and done but i did a backup just in case.

@javergara Thanks for your reply! Would setting '--epoch_count 1' overwrite the existing training done?",default selected first learning rate get see set done backup case thanks reply would setting overwrite training done,issue,negative,positive,positive,positive,positive,positive
580495135,"> you need to set --netG option if you are not using the default netG

This works for me. Keep --netG the same as training",need set option default work keep training,issue,negative,neutral,neutral,neutral,neutral,neutral
580446325,"by default, 200 epochs are selected, first 100 with the same learning rate and then 100 more to get it to 0.  you can see the parameters in https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/options 

I just set --epoch_count 1  and done  but i did a backup just in case.",default selected first learning rate get see set done backup case,issue,negative,positive,positive,positive,positive,positive
579451140,"It is correct. It works in Lab space. It takes the L channel as input, and predict ab channel. Chromatic information has 2 channels. Intensity has been given by the grayscale input. ",correct work lab space channel input predict channel chromatic information intensity given input,issue,negative,neutral,neutral,neutral,neutral,neutral
578893542,"I recommend that you use the default code, before making modifications. For the generator, you probably should use some form of ReLU and normalization layer. ",recommend use default code making generator probably use form normalization layer,issue,negative,neutral,neutral,neutral,neutral,neutral
578625619,"Hey, I think there is a bug in your code, because:
- the GAN losses don't osciliate.
- D and G are supposed to be adversarials, but both D and G GAN losses decrease at the same time.
- all GAN losses stay at around 0.25, which is what you would expect for untrained networks in LSGAN (for D(x)=0.5, we get L=0.5**2=0.25). I.e. neither D or G seems to have learned from the GAN losses.

Currently, your network is probably just optimizing for L1 loss and somehow ignoring the GAN losses, which can still lead to visually acceptable results (it's probably just a bit blurry and not very colorful right?). 

I'd recommend to turn off the L1 loss and then train again. If your GAN losses are working, your network should still learn something. If not, it will just generate random noise.",hey think bug code gan supposed gan decrease time gan stay around would expect untrained get neither learned gan currently network probably loss somehow gan still lead visually acceptable probably bit blurry colorful right recommend turn loss train gan working network still learn something generate random noise,issue,negative,positive,neutral,neutral,positive,positive
577485585,"> Did you use the same model? Your final saved model might be different from the model during training.

Thanks for your reply",use model final saved model might different model training thanks reply,issue,positive,positive,neutral,neutral,positive,positive
577291688,Did you use the same model? Your final saved model might be different from the model during training. ,use model final saved model might different model training,issue,negative,neutral,neutral,neutral,neutral,neutral
576980328,"> During training, your results are produced by running mean/std. Therefore, it is not the same.

but I find whether use eval.() or not , the results can not be good as training's. So how can I reproduce the good results like training's ?  ",training produced running therefore find whether use good training reproduce good like training,issue,positive,positive,positive,positive,positive,positive
576843020,"We convert (1) any input image format to float, (2) do some neural network stuff, and (3) convert a float output back to the image format. You need to write your own code to convert it between 16bit to float and back.",convert input image format float neural network stuff convert float output back image format need write code convert bit float back,issue,negative,neutral,neutral,neutral,neutral,neutral
576641524,"thanks for your reponse and I will try it.
Another question, I think i can input the dicom image by transfer to TIFF format with 16bit RGB ou grayscale, does the core of GAN train the data with 16Bit? 
I see at the exit of GAN, to changer Tensor --> JPG, the value is multiple by 255, need I changer to 2**16-1? 
thanks à lot",thanks try another question think input image transfer tiff format bit core gan train data bit see exit gan changer tensor value multiple need changer thanks lot,issue,positive,positive,positive,positive,positive,positive
576448207,You may need to modify the data loader. It is easy to do If you know how to load and save a DICMO image using Python. You can change these two [lines](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L57).  Also see the template dataset [class](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) on how to implement a custom data loader. ,may need modify data loader easy know load save image python change two also see template class implement custom data loader,issue,positive,positive,positive,positive,positive,positive
576427605,The Dropout probably will not affect the training of the generator a lot. But feel free to experiment with it.,dropout probably affect training generator lot feel free experiment,issue,positive,positive,positive,positive,positive,positive
576381542,"During training, your results are produced by running mean/std. Therefore, it is not the same. ",training produced running therefore,issue,negative,neutral,neutral,neutral,neutral,neutral
575966664,"If I use net.eval(), the eval result is not same as the results saved in training",use result saved training,issue,negative,neutral,neutral,neutral,neutral,neutral
575966087,"> For IN, it is possible for any batchSize during test time. For BN, if you do not use running mean/std, it should be fine for any batchSize. Converting BN to IN is possible but it seems to be nontrivial.

how to eval Pix2pix with BN when batchsize=1, I find it will also run mean/std during inference",possible test time use running fine converting possible find also run inference,issue,negative,positive,positive,positive,positive,positive
575424263,"> I think the error is related to your Python version. This issue might be related.

I changed the python version to 3.7.0 and the program was ready to run! 
Thank you!!",think error related python version issue might related python version program ready run thank,issue,negative,positive,neutral,neutral,positive,positive
575421864,"
> I think the error is related to your Python version. This issue might be related.

My python version is 3.5.5.I found the python version in environment.yml was 3.5.5.",think error related python version issue might related python version found python version,issue,negative,neutral,neutral,neutral,neutral,neutral
575294309,I think the error is related to your Python version. This [issue](https://github.com/NVIDIA/flownet2-pytorch/issues/119) might be related.,think error related python version issue might related,issue,negative,neutral,neutral,neutral,neutral,neutral
575079611,"> 
> 
> Hi!
> I am in trouble because I got an error and could not solve it.
> Please tell me how to resolve.
> 
> The details of the error are described below.
> Error during test.
> 
> Traceback (most recent call last):
> File ""test.py"", line 60, in
> model.test() # run inference
> File ""C:\Users\migita\pytorch-CycleGAN-and-pix2pix\models\base_model.py"", line 105, in test
> self.forward()
> File ""C:\Users\migita\pytorch-CycleGAN-and-pix2pix\models\pix2pix_model.py"", line 88, in forward
> self.fake_B = self.netG(self.real_A) # G(A)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\parallel\data_parallel.py"", line 150, in forward
> return self.module(*inputs[0], **kwargs[0])
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\pytorch-CycleGAN-and-pix2pix\models\networks.py"", line 465, in forward
> return self.model(input)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\pytorch-CycleGAN-and-pix2pix\models\networks.py"", line 533, in forward
> return self.model(x)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\container.py"", line 92, in forward
> input = module(input)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\pytorch-CycleGAN-and-pix2pix\models\networks.py"", line 535, in forward
> return torch.cat([x, self.model(x)], 1)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\container.py"", line 92, in forward
> input = module(input)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\pytorch-CycleGAN-and-pix2pix\models\networks.py"", line 535, in forward
> return torch.cat([x, self.model(x)], 1)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\container.py"", line 92, in forward
> input = module(input)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\pytorch-CycleGAN-and-pix2pix\models\networks.py"", line 535, in forward
> return torch.cat([x, self.model(x)], 1)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\container.py"", line 92, in forward
> input = module(input)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\pytorch-CycleGAN-and-pix2pix\models\networks.py"", line 535, in forward
> return torch.cat([x, self.model(x)], 1)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\container.py"", line 92, in forward
> input = module(input)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\pytorch-CycleGAN-and-pix2pix\models\networks.py"", line 535, in forward
> return torch.cat([x, self.model(x)], 1)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\container.py"", line 92, in forward
> input = module(input)
> File ""C:\Users\migita\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 547, in **call**
> result = self.forward(*input, **kwargs)
> File ""C:\Users\migita\pytorch-CycleGAN-and-pix2pix\models\networks.py"", line 535, in forward
> return torch.cat([x, self.model(x)], 1)
> RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 14 and 15 in dimension 3 at C:/w/1/s/windows/pytorch/aten/src\THC/generic/THCTensorMath.cu:71

",hi trouble got error could solve please tell resolve error error test recent call last file line run inference file line test file line forward file line call result input file line forward return file line call result input file line forward return input file line call result input file line forward return file line call result input file line forward input module input file line call result input file line forward return file line call result input file line forward input module input file line call result input file line forward return file line call result input file line forward input module input file line call result input file line forward return file line call result input file line forward input module input file line call result input file line forward return file line call result input file line forward input module input file line call result input file line forward return file line call result input file line forward input module input file line call result input file line forward return invalid argument size must match except dimension got dimension,issue,negative,negative,neutral,neutral,negative,negative
574884702,"Sorry for the silence, @ibro45 and @jenspfau... I haven't been working with this code for a while, but looking back at the GAN work I did, it seems like I abandoned this repo and went with fastai's CycleGAN. I did just instantiate the generator itself when converting to ONNX and CoreML (as I was trying to do above). I also did get it working on-device, so it's definitely a viable path to an mlmodel-based implementation. Hope that helps somewhat.",sorry silence working code looking back gan work like abandoned went generator converting trying also get working definitely viable path implementation hope somewhat,issue,positive,negative,negative,negative,negative,negative
574878152,"@jbmaxwell I would also like to know how you solved the problem, if you did. Thanks.",would also like know problem thanks,issue,negative,positive,positive,positive,positive,positive
574842681,"Yes, I think it can work. Thank you very much.",yes think work thank much,issue,positive,positive,positive,positive,positive,positive
574824107,"@Scienceseb Hi, can you explain the role of this perceptual loss in your training ?, you use it to improve the generators ?",hi explain role perceptual loss training use improve,issue,negative,neutral,neutral,neutral,neutral,neutral
574798722,"then you can just use the original dataset, and reduce the number of epoches?",use original reduce number,issue,negative,positive,positive,positive,positive,positive
574798460,Glad that you figured it out. Cropping can prevent from memorizing training images from domain B. ,glad figured prevent training domain,issue,negative,positive,positive,positive,positive,positive
574581842,Do you think that it would be useful give the network a different subset each epoch? I think it can be good because it allows to have a bigger dataset without increasing the training time (not too big to allow the network to see the same image several times).,think would useful give network different subset epoch think good bigger without increasing training time big allow network see image several time,issue,positive,positive,positive,positive,positive,positive
574579242,"I mean that, if I translate from A to B, the result it's just an image from B. Anyway, it seems I almost solved the problem cropping the images (I didn't used crop before).",mean translate result image anyway almost problem used crop,issue,negative,negative,negative,negative,negative,negative
574346601,"For dataset B. 
(1) your 128x128 input image will be resized to 256x256
(2) the program will crop a 128x128 patch from your 256x256 upscaled image.",input image program crop patch image,issue,negative,neutral,neutral,neutral,neutral,neutral
574345538,"Not sure if I fully understand your question. What does it mean by ""sometimes the generator samples modified images from destination dataset.""? what is a modified image?",sure fully understand question mean sometimes generator destination image,issue,negative,positive,neutral,neutral,positive,positive
574339457,Thanks for clarifying it. It is definitely great to keep it in mind for future projects. ,thanks definitely great keep mind future,issue,positive,positive,positive,positive,positive,positive
574338473,"It will use the set of m images, as it was used to create the dataset during initialization. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L29).",use set used create see line,issue,negative,neutral,neutral,neutral,neutral,neutral
574167161,"@Hshihua  Hello, I have the same question as you. Have you solved it? How to convert those color label into gray label (or TrainId) easily? Thanks a lot. Looking forward to your reply.",hello question convert color label gray label easily thanks lot looking forward reply,issue,positive,positive,positive,positive,positive,positive
573700874,"@FishYuLi 
With your help, I have reproduced the evaluation results of label2image, and I am very grateful for this. Thank you very much! But I don't understand your above method of evaluating image2label. I have a few questions for this to ask you.
1. Did you use 'evaluate.py' when evaluating image2label?   I think you used it, did you?
2. If you used 'evaluate.py' to evaluate the generated results of image2label, is the ground truth '_gtFine_labelIds.png'?  The calculation of accuracy values during the evaluation process uses the 'trainIds' of the ground truth. (Use 'assign_trainIds' function in 'cityscape.py' to align).  But I don't know how to convert the generated color label map to trainIds value.  
3. ""Of couse you can use it to evaluate image2label. Just directly load your generated label, transfer the RGB label to H x W x C, and you can re-use the evaluation code in 'evaluate.py'."" --This is your reply. I don't know how to transfer the RGB label to H*W*C, and why you transfer it?
Looking forward to your reply. Thanks a lot!",help evaluation grateful thank much understand method ask use think used used evaluate ground truth calculation accuracy evaluation process ground truth use function align know convert color label map value use evaluate directly load label transfer label evaluation code reply know transfer label transfer looking forward reply thanks lot,issue,positive,positive,positive,positive,positive,positive
573688809,"Hi HeoJinLareine,

We tried to convert the CycleGAN model to ONNX but we got some problems:

First:
TestModel object has no attribute ‘state_dict’

After changing the net into this attribute, we got:
“forward() got an unexpected keyword argument: ‘keep_vars’”
This seems to be a generic function call to neural networks, so it is a bit fuzzy to me which of the actual implementations is really called there (I would assume TestModel). However, I added this argument to all of those function definitions, but still get this error. 

Did you try to do this? If so, can you share your solution?

Best regards,
César Coelho",hi tried convert model got first object attribute net attribute got forward got unexpected argument generic function call neural bit fuzzy actual really would assume however added argument function still get error try share solution best coelho,issue,negative,positive,positive,positive,positive,positive
573611741,"@ZhangCZhen 
In my case, I first got a standard color map for Cityscapes dataset. (I mean you should get what is the exact RGB color for a certain category in ground truth label map.) Then, for the generated label, calculate the L2 distance of each pixel with the RGB color of all categories, and asign that pixel to the category with the smallest L2 distance. Then you get the WxHxC prediction.",case first got standard color map mean get exact color certain category ground truth label map label calculate distance color category distance get prediction,issue,positive,positive,neutral,neutral,positive,positive
573296908,"Yeah, Johnson et al.'s design is based on the [finding](http://torch.ch/blog/2016/02/04/resnets.html) of Gross and Wilber that ReLU should not be applied to the identity path (as was originally proposed), so they removed the ReLU. However, after Gross and Wilber's finding, He et al. conducted [a more detailed study on residual block design](https://arxiv.org/abs/1603.05027), which found bn > relu > conv > bn > relu > conv > add to be the best performing block design.

However, all experiments were conducted for image classification, so that does not necessarily mean that the better performance translates to GANs. I agree that the qualitative results are not sufficient to tell which design is better. What quantitative comparison do you have in mind? FCN scores as used in the pix2pix paper? Or human perceptual studies?

For a proper comparison, we would also have to run experiments across different hyperparameter ranges, for both pix2pix and cyclegan. Since the differences in results seem very minor, I will not bother with this right now. I mainly wanted to share my findings here for other people that might be wondering about the residual block design.",yeah al design based finding gross applied identity path originally removed however gross finding al detailed study residual block design found add best block design however image classification necessarily mean better performance agree qualitative sufficient tell design better quantitative comparison mind used paper human perceptual proper comparison would also run across different since seem minor bother right mainly share people might wondering residual block design,issue,negative,positive,positive,positive,positive,positive
573222160,"We followed the work by Johnson et al. They have a detailed discussion about the design of the residual network in their paper's supplemental [material](https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf) (Sec 1). Here is the discussion. It might require a more careful quantitative benchmark.
> We use the residual block design of Gross and Wilber [2] (shown in Figure 1), which differs from that of He et al [3] in that the ReLU nonlinearity following the addition is removed; this modified design was found in [2] to perform slightly better for image classification
",work al detailed discussion design residual network paper supplemental material sec discussion might require careful quantitative use residual block design gross shown figure al following addition removed design found perform slightly better image classification,issue,negative,positive,positive,positive,positive,positive
572217908,"If your input has 3 channels, and the output has 1 channel, you can set `--input_nc 3` and `--output_nc 1`. Your loss curves look fine. I am not sure if they have converged. The default GAN losses only oscillate.  As you have 100K images, you can train the model for fewer epochs (e.g., 5-20 epochs)",input output channel set loss look fine sure default gan oscillate train model,issue,negative,positive,positive,positive,positive,positive
572080653,"@FishYuLi 
Sorry it's me again. When you evaluate the results of image2label, you need to convert the semantic segmentation results into Ids. Right? How do you convert the semantically segmented RGB map to Ids map?  I mean, the pixel values of the color label image you got do not exactly match the pixel values in 'labels.py'. How do you evaluate the semantic segmentation results?  Thanks a lot. 
   I will be very grateful if you could send me your evaluation code. My email: 825762985@qq.com",sorry evaluate need convert semantic segmentation right convert semantically segmented map map mean color label image got exactly match evaluate semantic segmentation thanks lot grateful could send evaluation code,issue,positive,negative,neutral,neutral,negative,negative
572035802,"@ZhangCZhen 
1. Yes, I got almost the same FCN score.
2. Yes, FCN model is used to transfer the generated real image to label again so that we can compare it with GT masks. Please refer to the paper of Pix2pix carefully to see how exactly the evaluation metric is designed.",yes got almost score yes model used transfer real image label compare please refer paper carefully see exactly evaluation metric designed,issue,positive,positive,positive,positive,positive,positive
571757573,"Resizing is not part of data augmentation. Sometimes, if your dataset has images with different sizes, you want to make sure they are the same size.  ",part data augmentation sometimes different size want make sure size,issue,negative,positive,positive,positive,positive,positive
571525012,"> For image-to-image translation, 1 still holds. For some datasets with only a few hundred images, it is quite easy for the generator to memorize the training set. But it will not generalize well for the test set. For some tasks such as edges2cats, you want to the input cat edge to appear in many locations, so that the model can work for user drawing at different locations at test time.
i wonder that  whether the role of resizing before random cropping is just for data augmentation ?
Could you tell me what does the operation resizing before random cropping  really do?
",translation still hundred quite easy generator memorize training set generalize well test set want input cat edge appear many model work user drawing different test time wonder whether role random data augmentation could tell operation random really,issue,positive,positive,neutral,neutral,positive,positive
571460202,"@FishYuLi    Thanks for your reply. 
1. When you use 'evaluate.py' to evaluate the label2RGB image results, did you get values similar to those mentioned in the paper? I used the RGB images in the dataset for testing, and the accuracy is still poor. 
2. You mean I use the 'evaluate.py' to evaluate image2label results, I don't need the FCN model? ",thanks reply use evaluate image get similar paper used testing accuracy still poor mean use evaluate need model,issue,negative,negative,negative,negative,negative,negative
571343023,"We used to assign each RGB pixel to its closest color, and then use the segmentation evaluation code. Maybe you can also just predict class one-hot vector map instead rather than 3-channel RGB images. ",used assign color use segmentation evaluation code maybe also predict class vector map instead rather,issue,negative,neutral,neutral,neutral,neutral,neutral
571342540,"For IN, it is possible for any batchSize during test time. For BN, if you do not use running mean/std, it should be fine for any batchSize. Converting BN to IN is possible but it seems to be nontrivial. ",possible test time use running fine converting possible,issue,negative,positive,positive,positive,positive,positive
571336032,"For image-to-image translation, 1 still holds. For some datasets with only a few hundred images, it is quite easy for the generator to memorize the training set. But it will not generalize well for the test set. For some tasks such as edges2cats, you want to the input cat edge to appear in many locations, so that the model can work for user drawing at different locations at test time.  ",translation still hundred quite easy generator memorize training set generalize well test set want input cat edge appear many model work user drawing different test time,issue,positive,positive,positive,positive,positive,positive
571145792,"> Hello, I evaluated the results after using the cityscape dataset, and the values obtained are far from those mentioned in the paper. So I would like to ask, is the code given by 'evaluate.py' only used to evaluate the results of label2image? Can I evaluate image2label? After I use RGB image to generate semantic segmentation results, how do I evaluate the segmentation results? Thanks a lot!

@ZhangCZhen 
1. Yes, the given 'evaluate.py' is for label2image. It sends the generated image to a FCN model to generate a mask, and evaluates the mask with ground truth mask.
2. Of couse you can use it to evaluate  image2label. Just directly load your generated label, transfer the RGB label to H x W x C, and you can re-use the evaluation code in 'evaluate.py'.",hello cityscape far paper would like ask code given used evaluate evaluate use image generate semantic segmentation evaluate segmentation thanks lot yes given image model generate mask mask ground truth mask use evaluate directly load label transfer label evaluation code,issue,positive,positive,positive,positive,positive,positive
571019955,"Hello, I evaluated the results after using the cityscape dataset, and the values obtained are far from those mentioned in the paper. So I would like to ask, is the code given by 'evaluate.py' only used to evaluate the results of label2image? Can I evaluate image2label? After I use RGB image to generate semantic segmentation results, how do I evaluate the segmentation results? Thanks a lot!",hello cityscape far paper would like ask code given used evaluate evaluate use image generate semantic segmentation evaluate segmentation thanks lot,issue,positive,positive,positive,positive,positive,positive
570903399,"> how to evaluate photo->labels then

Hello, I have the same question about the evaluation of image2label generated results. How do you ultimately evaluate the results of semantic segmentation?
Looking forward to your answer, thanks a lot!
",evaluate hello question evaluation ultimately evaluate semantic segmentation looking forward answer thanks lot,issue,negative,positive,neutral,neutral,positive,positive
570890267,"@junyanz 
Wow, thanks for your prompt reply :)

> But it is a common technique for many neural networks.

I don't mean to make this in to a prolonged QnA session, but I don't intuitively understand the benefit of cropping for generative tasks.

For classification, a few intuitive explanations are:
1. prevents overfitting to a set of specific features by randomly hiding some features in the image
2. Prevents the network from paying attention to noise (ex. background ) instead of the object of concern (ex. cat & dog)

But how does this generalize to generation tasks?

",wow thanks prompt reply common technique many neural mean make session intuitively understand benefit generative classification intuitive set specific randomly image network paying attention noise ex background instead object concern ex cat dog generalize generation,issue,positive,negative,neutral,neutral,negative,negative
570877915,"> I had the same problem (#890). Just create another folder and use it to save models with `--checkpoints_dir your_folder`.

Thanks for your help, it worked. ",problem create another folder use save thanks help worked,issue,positive,positive,positive,positive,positive,positive
570869864,"I don't have a paper in my mind. But it is a common technique for many neural networks. In our experiments, we observed that it could prevent overfitting, especially for small datasets.",paper mind common technique many neural could prevent especially small,issue,negative,negative,neutral,neutral,negative,negative
570819578,I had the same problem (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/890). Just create another folder and use it to save models with `--checkpoints_dir your_folder`.,problem create another folder use save,issue,negative,neutral,neutral,neutral,neutral,neutral
570745607,"The default folder `checkpoints` had the right permissions, but even applying `chmod 777` didn't solve the problem. Changing directoty with `--checkpoints_dir` solved instead the problem. Thanks.",default folder right even solve problem instead problem thanks,issue,negative,positive,positive,positive,positive,positive
570685728,Not sure if you have the write permission to the directory that saves models. You can specify it using `--checkpoints_dir`. ,sure write permission directory specify,issue,negative,positive,positive,positive,positive,positive
570098069,"> You can also set the --display_id 0 if you don't want to use visdom
It works!",also set want use work,issue,negative,neutral,neutral,neutral,neutral,neutral
569654952,"how to receive data in windows, because the bash cant use",receive data bash cant use,issue,negative,neutral,neutral,neutral,neutral,neutral
569583138,"> This seems to be related to PyTorch and CUDA. I recommend that you post the question on the PyTorch repo as well. I found a similar issue [here](https://github.com/pytorch/pytorch/issues/7548).

Thanks a lot.It's very interesting that I have already solved the problem with the help of the link you mentioned.  I forgot to update it here. Thank you again for your reply.",related recommend post question well found similar issue thanks interesting already problem help link forgot update thank reply,issue,positive,positive,positive,positive,positive,positive
569581136,This seems to be related to PyTorch and CUDA. I recommend that you post the question on the PyTorch repo as well. I found a similar issue [here](https://github.com/pytorch/pytorch/issues/7548).,related recommend post question well found similar issue,issue,positive,neutral,neutral,neutral,neutral,neutral
569580756,"I recommend that you directly use google street view images for training. The google street view images look quite different from Cityscapes datasets regarding the camera viewpoint and objects layout. Also,  two epochs might not be sufficient. ",recommend directly use street view training street view look quite different regarding camera viewpoint layout also two might sufficient,issue,negative,positive,neutral,neutral,positive,positive
569580371,"In the pix2pix setting, we assume that the input data contains both image_A and image_B. The program will split the input into two (see this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/aligned_dataset.py#L45)). In your case, your 400x600 image is split into 200x600 as input and 200x600 as output. We provided a script to prepare the paired dataset. (see [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#prepare-your-own-datasets-for-pix2pix))",setting assume input data program split input two see line case image split input output provided script prepare paired see,issue,negative,neutral,neutral,neutral,neutral,neutral
568844920,"> +1
> 
> ```
> WARNING:root:Setting up a new session...
> create web directory ./checkpoints/maps_cyclegan/web...
> Traceback (most recent call last):
>   File ""train.py"", line 51, in <module>
>     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
>   File ""/data/xiaoyubei/codes/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py"", line 183, in optimize_parameters
>     self.forward()      # compute fake images and reconstruction images.
>   File ""/data/xiaoyubei/codes/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py"", line 114, in forward
>     self.fake_B = self.netG_A(self.real_A)  # G_A(A)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
>     result = self.forward(*input, **kwargs)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py"", line 121, in forward
>     return self.module(*inputs[0], **kwargs[0])
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
>     result = self.forward(*input, **kwargs)
>   File ""/data/xiaoyubei/codes/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 373, in forward
>     return self.model(input)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
>     result = self.forward(*input, **kwargs)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/container.py"", line 91, in forward
>     input = module(input)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
>     result = self.forward(*input, **kwargs)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/conv.py"", line 691, in forward
>     output_padding, self.groups, self.dilation)
> RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1533672544752/work/aten/src/THC/THCBlas.cu:249
> ```
> 
> This is my environment:
> 
> ```
> >>> torch.__version__
> '0.4.1'
> >>> torch.version.cuda
> '9.0.176'
> 
> $ nvcc -V
> nvcc: NVIDIA (R) Cuda compiler driver
> Copyright (c) 2005-2018 NVIDIA Corporation
> Built on Sat_Aug_25_21:08:01_CDT_2018
> Cuda compilation tools, release 10.0, V10.0.130
> ```



> +1
> 
> ```
> WARNING:root:Setting up a new session...
> create web directory ./checkpoints/maps_cyclegan/web...
> Traceback (most recent call last):
>   File ""train.py"", line 51, in <module>
>     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
>   File ""/data/xiaoyubei/codes/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py"", line 183, in optimize_parameters
>     self.forward()      # compute fake images and reconstruction images.
>   File ""/data/xiaoyubei/codes/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py"", line 114, in forward
>     self.fake_B = self.netG_A(self.real_A)  # G_A(A)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
>     result = self.forward(*input, **kwargs)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py"", line 121, in forward
>     return self.module(*inputs[0], **kwargs[0])
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
>     result = self.forward(*input, **kwargs)
>   File ""/data/xiaoyubei/codes/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 373, in forward
>     return self.model(input)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
>     result = self.forward(*input, **kwargs)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/container.py"", line 91, in forward
>     input = module(input)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
>     result = self.forward(*input, **kwargs)
>   File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/conv.py"", line 691, in forward
>     output_padding, self.groups, self.dilation)
> RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1533672544752/work/aten/src/THC/THCBlas.cu:249
> ```
> 
> This is my environment:
> 
> ```
> >>> torch.__version__
> '0.4.1'
> >>> torch.version.cuda
> '9.0.176'
> 
> $ nvcc -V
> nvcc: NVIDIA (R) Cuda compiler driver
> Copyright (c) 2005-2018 NVIDIA Corporation
> Built on Sat_Aug_25_21:08:01_CDT_2018
> Cuda compilation tools, release 10.0, V10.0.130
> ```

I have the same environment with you. Have you solved it?
Looking forward to your reply. Thanks.",warning root setting new session create web directory recent call last file line module calculate loss get update network file line compute fake reconstruction file line forward file line result input file line forward return file line result input file line forward return input file line result input file line forward input module input file line result input file line forward error program execute environment compiler driver copyright corporation built compilation release warning root setting new session create web directory recent call last file line module calculate loss get update network file line compute fake reconstruction file line forward file line result input file line forward return file line result input file line forward return input file line result input file line forward input module input file line result input file line forward error program execute environment compiler driver copyright corporation built compilation release environment looking forward reply thanks,issue,negative,negative,neutral,neutral,negative,negative
568370632,"> We haven't tested it by ourselves. But several users have reported that they managed to use it on Windows (e.g. #845)

thanks for your reply : )",tested several use thanks reply,issue,negative,positive,neutral,neutral,positive,positive
568152005,"I believe in LSGAN the loss is squared distance from the labels. So if the output of D is very large, D will get a large penalty and it will learn to make a smaller output. Eventually, D should learn to output the correct labels, since those minimize the loss (and the loss is nice and smooth, just squared distance).",believe loss squared distance output large get large penalty learn make smaller output eventually learn output correct since minimize loss loss nice smooth squared distance,issue,negative,positive,positive,positive,positive,positive
568148104,"Thanks. Then  what is the difference of the output of D without sigmoid. For example, in LSGAN, if the output of D is very large (far from 1 or 0), can the loss function work?  since the real labels are still set to 1 and false labels set to 0.",thanks difference output without sigmoid example output large far loss function work since real still set false set,issue,negative,positive,neutral,neutral,positive,positive
567367540,"The sigmoid is contained in the loss function [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/2595573a7c9fd5bb389a949a3f1b2f06970a4eb8/models/networks.py#L234). But note that some variants of GAN discriminators don't use a sigmoid (e.g., see [LSGANs](https://arxiv.org/abs/1611.04076) or [WGANs](https://arxiv.org/abs/1701.07875)). ",sigmoid loss function note gan use sigmoid see,issue,negative,neutral,neutral,neutral,neutral,neutral
567361957,"Hi， I am wondering why sigmoid activation is not used for pathGAN, since the true patch should be close to 1, while the false should be close to 0.",wondering sigmoid activation used since true patch close false close,issue,negative,negative,neutral,neutral,negative,negative
567225828,"The data loader will load an image, and then apply scaling and random cropping during training time. The type of argumentation depends on the flag `--preprocess`. Rotation is not supported in this repo. ",data loader load image apply scaling random training time type argumentation flag rotation,issue,negative,negative,negative,negative,negative,negative
567225155,"You can revise the data loader code, and use YIQ, LAB, instead of RGB. In this case, your `input_nc` and `output_nc` can be set as 3.",revise data loader code use lab instead case set,issue,negative,neutral,neutral,neutral,neutral,neutral
567023755,"Thank you for the guidance and quick reply. I had actually read the code snippets, the reason I posted the query is that in options folder in base options there are arguments input_nc and output_nc which mention 3 for RGB and 1 for gray scale. But for example if we want to attempt Lab to Lab translation then I guess I need to explicitly 2 and also another doubt I had for Lab to Lab translation I have to combine both data sets. I was wondering as all colour spaces have mostly 3 channels, so instead of tampering input_nc or output_nc can we pass YIQ, LAB or HSV instead of RGB and make changes in aligned_dataset.py file The problem is what type of transforms need to be done for training in different spaces. ",thank guidance quick reply actually read code reason posted query folder base mention gray scale example want attempt lab lab translation guess need explicitly also another doubt lab lab translation combine data wondering colour mostly instead pas lab instead make file problem type need done training different,issue,negative,positive,neutral,neutral,positive,positive
566795462,"The file names will be preserved. Regarding test time flags, please refer to [README](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#pix2pix-traintest) and [Training & Test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images) for more details.",file regarding test time please refer training test,issue,negative,neutral,neutral,neutral,neutral,neutral
566733490,"The number of images in the two datasets can be different. For example, if we transform a Monet's painting to a landscape photo, we will only have a few hundreds of paintings. But we can use lots of natural photos. I recommend that you use the same or similar image sizes.",number two different example transform painting landscape photo use lot natural recommend use similar image size,issue,positive,positive,neutral,neutral,positive,positive
566730921,Two flags `----save_by_iter` and `--save_latest_freq` can do the job for you. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L23) and #683 for more details,two job see line,issue,negative,neutral,neutral,neutral,neutral,neutral
566727230,"You need to modify two things:  (1) the data loader and (2) visualization code. 
We have worked with Lab space for the colorization application. 
Step 1: When you load an image, you need to convert the color space. Here is an [example](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/colorization_dataset.py#L60).
Step 2: When you visualize the results, you need to convert the color space back to the original space. Here is an [example](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/colorization_model.py#L67).  ",need modify two data loader visualization code worked lab space colorization application step load image need convert color space example step visualize need convert color space back original space example,issue,negative,positive,positive,positive,positive,positive
566281926,I didn't remember the training details. But the pre-trained models might have been trained on 128x128. Please try to add `--load_size 143 --crop_size 128` in your test script. ,remember training might trained please try add test script,issue,negative,neutral,neutral,neutral,neutral,neutral
566281142,"Regarding zeros, it affects the cycle-consistency loss (in CycleGAN) and reconstruction loss (in pix2pix). If the target images have lots of zeros, and a generator learns to only predict zeros, both losses will be very small.",regarding loss reconstruction loss target lot generator predict small,issue,negative,negative,negative,negative,negative,negative
565963650,"Thanks for your reply again. My training script and test script are following:
training script: `python train.py - --dataroot ./datasets/cityscapes --name cityscapes_cyclegan --model cycle_gan 
`
test script:`python test.py --dataroot datasets/cityscapes/testA --name cityscapes_cyclegan --model test --no_dropout
`
Why did I get those results?",thanks reply training script test script following training script python name model test script python name model test get,issue,negative,positive,neutral,neutral,positive,positive
565830309,"Thanks for your reply @junyanz . I am normalizing to [-1,1] after that. Ok it makes sense I think.  

Is it because when the generator produce zeros, it looks like it does a good job? And then the discriminator will not be able to distinguish between the Generated and real image? And for this reason the discriminator returns small error to the generator?",thanks reply sense think generator produce like good job discriminator able distinguish real image reason discriminator small error generator,issue,positive,positive,positive,positive,positive,positive
565769420,"> 1. The  PIL image is [0,  255]. We convert it into [-1,  1] using `torchvision.transforms.Normalize`, as neural networks work better with zero-mean data. The input to the networks is [-1, 1] after this conversion.  See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/22fe4465c120ac92a2de09c3c2d50cbbe0e63f29/data/base_dataset.py#L111) for more details.
> 2. The range for both the original images and generated images is [-1, 1].

Ah, so that is what you meant! I was worried this whole time, images originally contained values ranging from [-1,1] instead of what I been telling people(i.e [0,255])

Also yes, that would do that. I got so used to using different precomputed means and stds which doesn't give [-1, 1] for new data, that I forgot you were using .5 for all. 

Does this mean that though, instead of input ranging from [0,1] and output ranging from [0,1] through Sigmoid, its better to normalize the [0,1] input to a [-1,1] input and output a [-1,1] output through Tanh, since the latter is normalized?",image convert neural work better data input conversion see line range original ah meant worried whole time originally ranging instead telling people also yes would got used different give new data forgot mean though instead input ranging output ranging sigmoid better normalize input input output output tanh since latter,issue,positive,positive,positive,positive,positive,positive
565759405,"You may want to normalize your images to [-1, 1], as the range of the generator's output is [-1, 1]. These methods do work better with more balanced data. If most of the pixels are 0, an easy solution for a generator is to produce zero everywhere, which can already achieve a very small training loss. ",may want normalize range generator output work better balanced data easy solution generator produce zero everywhere already achieve small training loss,issue,positive,positive,positive,positive,positive,positive
565758788,It looks worse than normal. Could you shared with us your training and test script? What is the size of your input image? ,worse normal could u training test script size input image,issue,negative,negative,negative,negative,negative,negative
565758731,"In some sense, CycleGAN is using the idea of self-supervised learning for image-to-image translation. If you have some paired data and still want to use unlabelled data, you can train models in a semi-supervised learning setting by mixing the objective of pix2pix and CycleGAN.  Here is a recent [work](https://arxiv.org/pdf/1901.08212.pdf) on this topic. ",sense idea learning translation paired data still want use data train learning setting objective recent work topic,issue,negative,neutral,neutral,neutral,neutral,neutral
565758424,Sure. Feel free to try it. ,sure feel free try,issue,positive,positive,positive,positive,positive,positive
565758335,Your image files might be corrupted. You can print the image path and see which image has caused the issue.,image might corrupted print image path see image issue,issue,negative,neutral,neutral,neutral,neutral,neutral
565758214,"1. The  PIL image is [0,  255]. We convert it into [-1,  1] using `torchvision.transforms.Normalize`, as neural networks work better with zero-mean data. The input to the networks is [-1, 1] after this conversion.  See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/22fe4465c120ac92a2de09c3c2d50cbbe0e63f29/data/base_dataset.py#L111) for more details.
2. The range for both the original images and generated images is [-1, 1].  ",image convert neural work better data input conversion see line range original,issue,positive,positive,positive,positive,positive,positive
565365609,"> The goal is to match the range. The range of real images is [-1, 1]. Tanh outputs a value between [-1, 1].

Yes, I have heard that the range of real images is [-1, 1] elsewhere as well
However, I have two sequential questions.

1. When I open an image using PIL, so PIL.Image('some_img.jpg')
Does PIL automatically convert the pixel values ranging from [-1, 1] to [0, 255]? Or did you mean something different when saying the range of real images is [-1, 1]? I guess I'm not totally sure if the actual pixel numerical values actually range from [-1, 1] originally due to my misunderstanding.

What I do know is that torchvision.transforms.ToTensor divides the values ranging from [0, 255] by 255, thus scaling them to [0, 1]

2. It was a bit odd to me that we usually first shift an image(if PIL does what question 1 says it does) to [0, 1] as original input AND THEN work with trying to output something from [-1, 1] again, then plot by shifting back to [0, 1].
Where I thought it might be better to just take as input the original values in between [-1, 1] and then output something from [-1, 1], then plot by shifting back to [0, 1]. 

But it's been my belief that this actually didn't matter too much because of the normalization layers. The normalization makes the activations have a mean of 0 and a std of 1, so it doesn't matter what range the original input is in, even though its been shifted to [0, 1]. Is that a bad statement or a bad conceived notion? What are your thoughts on that?",goal match range range real tanh value yes range real elsewhere well however two sequential open image automatically convert ranging mean something different saying range real guess totally sure actual numerical actually range originally due misunderstanding know ranging thus scaling bit odd usually first shift image question original input work trying output something plot shifting back thought might better take input original output something plot shifting back belief actually matter much normalization normalization mean matter range original input even though bad statement bad notion,issue,positive,positive,neutral,neutral,positive,positive
565328646,"Thanks for your reply. I have trained the model for 200 epochs. 
![2019-12-13 14-45-08屏幕截图](https://user-images.githubusercontent.com/34617934/70776153-3eea8900-1db7-11ea-9891-411a50c71633.png)
It's so weird.
And I have just downloaded pretrained_model to test the same dataset, the results are not good.
![2019-12-13 15-05-05屏幕截图](https://user-images.githubusercontent.com/34617934/70777198-1ca63a80-1dba-11ea-9ee5-0eb53ca37d7a.png)
![2019-12-13 15-05-20屏幕截图](https://user-images.githubusercontent.com/34617934/70777208-216aee80-1dba-11ea-9a9b-9132480957f1.png)
The above results are normal? Or am I doing something wrong?",thanks reply trained model weird test good normal something wrong,issue,negative,positive,neutral,neutral,positive,positive
565264650,"Thank you for your quick reply. I will try to modify --netD n_layers, should I change the number of resnet's or increase the learning rate of G?",thank quick reply try modify change number increase learning rate,issue,positive,positive,positive,positive,positive,positive
565203367,"The GAN losses often oscillate and may not be the best indicator for results. You need to check the quality of results by yourself. BTW, if you want to use a discriminator with different number of conv layers, you need to specify `--netD n_layers` in addition to using `--n_layers_D`.",gan often oscillate may best indicator need check quality want use discriminator different number need specify addition,issue,positive,positive,positive,positive,positive,positive
565200255,"latest_net_G_A is for A->B; while lateset_net_G_B is for B->A. It depends on your dataset A and B. If A contains photos, and B contains paintings, you should use latest_net_G_A. Otherwise, you need to use the latest_netG_B. ",use otherwise need use,issue,negative,neutral,neutral,neutral,neutral,neutral
564936852,"Ah, thanks for the clarification! Great idea! ",ah thanks clarification great idea,issue,positive,positive,positive,positive,positive,positive
564865135,"@junyanz 

Just a quick question, what is the difference between ""latest_net_G_A"" and ""latest_net_G_B""? What file should I rename into ""latest_net_G""?

I trained my model using `python train.py --dataroot datasets/monet2photo --name test_monet_cyclegan --model cycle_gan --direction BtoA` and only finished 1 epoch so far so I can't see the difference of results between the 2 files.

So far what I'm trying to do is convert a Real Photo to Monet Painting. Thanks. :)

Here is the result: https://imgur.com/a/3LpwZwx

From what I can see I should use ""latest_net_G_B"" but I just want to make sure.",quick question difference file rename trained model python name model direction finished epoch far ca see difference far trying convert real photo painting thanks result see use want make sure,issue,positive,positive,positive,positive,positive,positive
564672095,"Sorry for the confusion. I mean that we didn't use the normalization in the first layer of the discriminator. If the low-level statistics of output need to be related to those of the input, the discriminator has a chance to enforce it. ",sorry confusion mean use normalization first layer discriminator statistic output need related input discriminator chance enforce,issue,negative,negative,negative,negative,negative,negative
564670166,"The label is correct. ""Real A"" is the input image, and ""Fake B"" is the generated output result. The results are worse than normal. Have you trained the model for 200 epochs?",label correct real input image fake output result worse normal trained model,issue,negative,negative,negative,negative,negative,negative
564659407,"It sets the number of epochs. During the 1st and `--niter` epochs, it will use the initial learning rate. During the `--niter` and `--niter`+`--niter_decay`, it will use a decaying learning rate. ",number st niter use initial learning rate niter niter use learning rate,issue,negative,neutral,neutral,neutral,neutral,neutral
564466128,"--niter and --niter_decay ，The sum of these two figures is 200（default）,these are set learning rate but not epoch number in details.",niter sum two set learning rate epoch number,issue,negative,neutral,neutral,neutral,neutral,neutral
564430598,"@phillipi Thanks for your feedback! I suppose the details are not so so important to practitioners, as we just have to know that in most cases, normalization will help not hurt. By the way, for Batchnorm at least, the latest story/idea is that covariate shift is only a small chance part of it and it has more to do with making the optimization process easier by adding the gamma and beta parameters. They help be able to tune the activation fed into batchnorm based on just those two params in a linear like faction, w(x) + b, where w is gamma and b is beta, whereas the original activation might have a lot more params that work in more complex/intricate ways.  I mean, there is just a bunch of more theory that can be said about normalization, all of which you mentioned is plausible. I was just hoping someone knew the definite answer and can provide a concrete example for instancenorm in the case of why it specifically helps style gans, but intuitions are great!

Also for InstanceNorm, look at this quote from the paper https://arxiv.org/abs/1607.08022

It says about instancenorm that 

'This prevents instance-specific mean and covariance shift simplifying the learning process.'

just to reinforce your intuition.

Lastly, for batchnorm vs instancenorm, I'm sure you already know this but, the gamma and beta  I mentioned isn't the case when you are using the default params for InstanceNorm2d since affine=False means no learnable params. So most likely, it does help with all three others instead, covariate shift, making the cost function easier to optimize by reducing the elongation of the cost function and also adding some regularization.


@junyanz 
Now, Jun Yan, I believe that isn't the case for cycleGan. Take a look at your code here for the resnet9 option,

```
class ResnetGenerator(nn.Module):
    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):
        assert(n_blocks >= 0)
        super(ResnetGenerator, self).__init__()
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        model = [nn.ReflectionPad2d(3),
                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),
                 norm_layer(ngf),
                 nn.ReLU(True)]

```

you initialize your model, with instancenorm included in the first layer. Perhaps you were referring to a different model? 

However, I do appreciate extra feedback such as those. Those little details about knowing when to stay away from normalization is super nice.",thanks feedback suppose important know normalization help hurt way least latest shift small chance part making optimization process easier gamma beta help able tune activation fed based two linear like faction gamma beta whereas original activation might lot work way mean bunch theory said normalization plausible someone knew definite answer provide concrete example case specifically style great also look quote paper mean covariance shift learning process reinforce intuition lastly sure already know gamma beta case default since learnable likely help three instead shift making cost function easier optimize reducing elongation cost function also regularization yan believe case take look code option class self assert super self type else model true initialize model included first layer perhaps different model however appreciate extra feedback little knowing stay away normalization super nice,issue,positive,positive,positive,positive,positive,positive
564374208,"BTW, we don't use the normalization layer in the first layer, so that it can still preserve the brightness or other low-level statistics of the input image to some degree.",use normalization layer first layer still preserve brightness statistic input image degree,issue,negative,positive,positive,positive,positive,positive
564305873,"Good point, for style transfer you might not want to be invariant to things like overall brightness -- a bright input should generate a bright stylized output.

But most of the action of these normalization layers is happening deep in the networks, on feature maps. So it's hard to know exactly what invariances they are actually achieving -- the brightness example is just to provide an intuition of what it would be like to apply instance norm to the raw pxels themselves.

There are a variety of ideas for why normalization helps in general in deep nets. I'm not sure a clear story has emerged, but it may have to do with [covariate shift](https://arxiv.org/abs/1502.03167), or it may be more about [smoothing the optimization landscape](https://arxiv.org/abs/1805.11604), or perhaps it's more about regularization (limiting the capacty of networks by making them invariant to certain nuisances).",good point style transfer might want invariant like overall brightness bright input generate bright output action normalization happening deep feature hard know exactly actually brightness example provide intuition would like apply instance norm raw variety normalization general deep sure clear story may shift may smoothing optimization landscape perhaps regularization limiting making invariant certain,issue,positive,positive,positive,positive,positive,positive
563497909,Please refer to `--niter` and `--niter_decay`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L30) for more details. ,please refer niter see,issue,negative,neutral,neutral,neutral,neutral,neutral
562885511,"@ZzzackChen  Would you mind telling how do you implement evaluation in details since I used the same implementation as yours but all numbers are zero.

",would mind telling implement evaluation since used implementation zero,issue,negative,neutral,neutral,neutral,neutral,neutral
562729581,"The network is trained with 2D images only. You can still see flickering artifacts if you look at the results carefully. In a follow-up work [vid2vid](https://github.com/NVIDIA/vid2vid), we trained models on videos and enforced continuity during training. ",network trained still see flickering look carefully work trained enforced continuity training,issue,negative,negative,neutral,neutral,negative,negative
562262709,"If you want to generate 1024+ images, you may want to use HD. I guess pix2pix can handle 256. For 512, you can try both. I think pix2pixHD can support images with different sizes. If you have questions regarding this, please post your question on the pix2pixHD repo. ",want generate may want use guess handle try think support different size regarding please post question,issue,positive,neutral,neutral,neutral,neutral,neutral
562258998,GANs training often oscillates a lot due to the nature of minimax optimization. See this [tutorial](https://arxiv.org/pdf/1701.00160.pdf) for more details (Section 5).,training often lot due nature optimization see tutorial section,issue,negative,negative,negative,negative,negative,negative
562059986,"> If you have pairs, please refer to our recent work such as [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE). If you are using CycleGANs, I recommend that you train the models on cropped patches. Please refer to [training/test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images) for more details.


Is there a suggested threshold for using HD or just the normal version? In Pix2PixHD paper, the output for global network is 1024 * 512,  just a little smaller than 1024 * 1024 or,  in other words, has a same width compare with 1024 * 1024. However, in general application, a 256 * 256 size of patches would be a more common choice for common pix2pix model which serving as the global network in HD version.  I not sure if a  common pix2pix model could directly handle the 1024 * 1024 or 512 * 512 input size.

Further more, when using the HD version, how to design the output size for global network? Half of the input size of fix it to 1024 or 512 no matter how large the input is? Although this question depends heavily on different tasks but I still hope for a general answer.
",please refer recent work spade recommend train please refer threshold normal version paper output global network little smaller width compare however general application size would common choice common model serving global network version sure common model could directly handle input size version design output size global network half input size fix matter large input although question heavily different still hope general answer,issue,positive,negative,neutral,neutral,negative,negative
562034862,"Reducing the learning rate was not helpful, but large training sample size solved the strip problems.
Although, thank you for your suggestion and your wonderful package!",reducing learning rate helpful large training sample size strip although thank suggestion wonderful package,issue,positive,positive,positive,positive,positive,positive
561436019,"hello,gordonwisc.Have you solute this issue which is to apply the model to any size images?I meet the similar problem,but according to anthor's answer isn't enough,do you success?",hello solute issue apply model size meet similar problem according answer enough success,issue,negative,positive,neutral,neutral,positive,positive
561334566,"1. It's better to keep roughly the same resolution for training and test. 
2. You don't have to crop, although cropping is an effective data augmentation method. The get_transform function will do nothing with `--preprocess none`. See more options [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L49) and [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#preprocessing).
3. You can set it as `AtoB`. `BtoA` will flip the input and output images. ",better keep roughly resolution training test crop although effective data augmentation method function nothing none see set flip input output,issue,positive,positive,positive,positive,positive,positive
560557422,It might be helpful to post the question on the TensorFlow core repo or mailing list. ,might helpful post question core list,issue,negative,neutral,neutral,neutral,neutral,neutral
560485993,"I know that, sorry to ask so many silly questions",know sorry ask many silly,issue,negative,negative,negative,negative,negative,negative
560484462,"In other words, I want to know the meaning  of False : self.criterionGAN(pred_fake, False) ",want know meaning false false,issue,negative,negative,negative,negative,negative,negative
560478420,"Sorry, i have known that. But ,i have a new problem, when training D, why is loss_D_fake additive and loss_D_real average?",sorry known new problem training additive average,issue,negative,negative,negative,negative,negative,negative
560461860,"Now, I know the calculation formula of patch quantity. But what I don't understand is that the discriminator outputs a map, how do you compute the cross entropy?",know calculation formula patch quantity understand discriminator map compute cross entropy,issue,negative,neutral,neutral,neutral,neutral,neutral
560427531,"Hi. If I set the nlayer to 6, the output size is 2×2, does it mean that the image is divided into 2×2 patches for discrimination? If I set nlayer to 3, how many patches are separated?",hi set output size mean image divided discrimination set many,issue,negative,positive,neutral,neutral,positive,positive
560306551,"My fault!! It's seems like there are something wrong with my server, it only trained 3 epochs..Sorry~",fault like something wrong server trained,issue,negative,negative,negative,negative,negative,negative
559665418,"> It may potentially hurt the performance of G_A.
> Not sure what happened. For sanity check, you can run your test script and your model on your training images, and see if it looks as good as before. Make sure you use similar or compatible training and test parameters (e.g., --preprocess, --load_size, --crop_size, etc.)

Perhaps something wrong with my own data,now the model works.Thanks a lot!
",may potentially hurt performance sure sanity check run test script model training see good make sure use similar compatible training test perhaps something wrong data model lot,issue,negative,positive,positive,positive,positive,positive
559609504,Not sure what has happened. Maybe reduce the learning rate?,sure maybe reduce learning rate,issue,negative,positive,positive,positive,positive,positive
559435765,"> 1. The gradients of the discriminator are not calculated. Please see this [doc](https://pytorch.org/docs/stable/notes/autograd.html) for more details.
> 2. The order does not matter in practice.

Thank you very much for the prompt reply! I have read through the documentation which is helpful to clear my minds. All along my mistake (please advise if I still did not get this part right) was not realizing that it is the weights, instead of the gradients of the weights, of the discriminator that are required in order to backpropagate to the generator. By freezing the discriminator, anything that is dependent on the gradient of the weights of the discriminator will be cut off from the graph, this however does not obstruct the trace back to the weights of the generator. ",discriminator calculated please see doc order matter practice thank much prompt reply read documentation helpful clear along mistake please advise still get part right realizing instead discriminator order generator freezing discriminator anything dependent gradient discriminator cut graph however obstruct trace back generator,issue,positive,positive,positive,positive,positive,positive
559393095,"> They don't have to be square images. During training time, you can crop square patches using the flag `--preprocess crop`. During test time, you can apply the model to images with arbitrary sizes (rectangle or not) using the flag `--preprocess none`.

but out of memory
",square training time crop square flag crop test time apply model arbitrary size rectangle flag none memory,issue,negative,negative,neutral,neutral,negative,negative
559257566,That might work. You can also modify the first layer of your model (from 3 channel input to 6 channel input) and use the pre-trained weights for the rest of the network.  ,might work also modify first layer model channel input channel input use rest network,issue,negative,positive,positive,positive,positive,positive
559252437,"1. The gradients of the discriminator are not calculated. Please see this [doc](https://pytorch.org/docs/stable/notes/autograd.html) for more details. 
2. The order does not matter in practice. ",discriminator calculated please see doc order matter practice,issue,negative,neutral,neutral,neutral,neutral,neutral
559040047,"> It's a small speedup trick. set_requires_grad=False will stop calculating gradients for the discriminator while we update the generator. It can save some time and memory. Generators will still get the gradients.

Sorry to come back to this but it's still not clear to me. Does this mean that the gradients of the discriminator weights (which are required to backpropagate to the generator) will still be calculated but not stored in the memory? 
A second question, why optimize the generator before the discriminator (which opposes the optimization flow of the original GAN algorithm ?) ",small trick stop calculating discriminator update generator save time memory still get sorry come back still clear mean discriminator generator still calculated memory second question optimize generator discriminator optimization flow original gan algorithm,issue,positive,negative,neutral,neutral,negative,negative
558958950,"> Generators in this repo is designed for data in range [-1, 1], as you can see from the tanh at the end. So data with larger range will definitely fail. Moreover, it is generally a good idea to normalize data in DL, as it empirically stables training.

@SsnL How would you suggest normalizing the data to the range [-1, 1], and ""de-normalize"" the data back to the original values during testing? I'm currently doing normalized_data= (data - data_avg) / data_avg where data_avg = (data.min() + data.max()) / 2 to normalize the values to [-1, 1]. But because my datatype doesn't have a fixed minimum and maximum value, I just take the maximum and minimum of the training dataset to determine the average. Is this correct? Or should I be normalizing the data based on each individual image's minimum and maximum values? Also, how can I ""de-normalize"" the values back to the original?",designed data range see tanh end data range definitely fail moreover generally good idea normalize data training would suggest data range data back original testing currently data normalize fixed minimum maximum value take maximum minimum training determine average correct data based individual image minimum maximum also back original,issue,positive,positive,neutral,neutral,positive,positive
558915022,Do you mean that I can apply the transfer learning models to the source and target images separately as 3 channel images before concatenating them together?,mean apply transfer learning source target separately channel together,issue,negative,negative,negative,negative,negative,negative
558293178,"Not sure if ""set_requires_grad` will affect the parameter group.",sure affect parameter group,issue,negative,positive,positive,positive,positive,positive
558284810,You can use 6 channel discriminator as long as you do it for both source and target domains.,use channel discriminator long source target,issue,negative,negative,neutral,neutral,negative,negative
557967189,"@junyanz, but when i load the pretrained netD for resuming training,   error occurs:
loaded state dict contains a parameter group that doesn't match the size of optimizer's group.
when i do not set set_requires_grad, this error does not occur.  ",load training error loaded state parameter group match size group set error occur,issue,negative,neutral,neutral,neutral,neutral,neutral
557728724,Please use `--niter` and `--niter_decay` for training epochs. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L30)  for more details.,please use niter training see,issue,negative,neutral,neutral,neutral,neutral,neutral
557328118,"It does not matter a lot. You can use MSE, or other kinds of Lp norm loss, or a hybrid of L1/L2 loss. We found that L1 loss produces slightly sharper results while L2 loss tends to produce more blurry results. ",matter lot use norm loss hybrid loss found loss slightly sharper loss produce blurry,issue,negative,negative,negative,negative,negative,negative
556922006,"1. It may potentially hurt the performance of G_A.
2. Not sure what happened. For sanity check, you can run your test script and your model on your training images, and see if it looks as good as before. Make sure you use similar or compatible training and test parameters (e.g., `--preprocess`, `--load_size`, `--crop_size`, etc.)
",may potentially hurt performance sure sanity check run test script model training see good make sure use similar compatible training test,issue,negative,positive,positive,positive,positive,positive
556911158,You can convert the model with suffix `_net_G.pth`. We haven't converted it to ONNX by ourselves. You might find this [post](https://github.com/pytorch/pytorch/issues/4584) useful.,convert model suffix converted might find post useful,issue,negative,positive,positive,positive,positive,positive
555919297,"Setting some large number in `web/index.html` helps:

```
<!DOCTYPE html>
<html>
  <head>
    <title>Experiment name = maps_pix2pix</title>
    <meta content=""100000"" http-equiv=""refresh"">
  </head>
```",setting large number head title experiment name meta,issue,negative,positive,positive,positive,positive,positive
555809383,"> The model is symmetric. But datasets and tasks might be not. Sometimes, translating A to B is harder than the other direction.

Thanks! May i ask  another questions ?
1.In my task,Loss_D_B goes to zero after several epoch while loss_D_A looks good.Does the failure of discriminate D will have bad effect on the G_A and D_A？
2.I supervise the train process by watching the pictures generated by the G_A. And it seems look good.However,  when i try to test the G_A with 200 test pictures ,the result is so bad that i can not recognise what is . What problem may happen,could you give me some advices?",model symmetric might sometimes harder direction thanks may ask another task go zero several epoch failure discriminate bad effect supervise train process watching look try test test result bad problem may happen could give,issue,negative,negative,negative,negative,negative,negative
555805792,"thanks! 

> The model is symmetric. But datasets and tasks might be not. Sometimes, translating A to B is harder than the other direction.



> The model is symmetric. But datasets and tasks might be not. Sometimes, translating A to B is harder than the other direction.
",thanks model symmetric might sometimes harder direction model symmetric might sometimes harder direction,issue,negative,neutral,neutral,neutral,neutral,neutral
555702149,"Yes, you can use the `--refresh` flag. ",yes use refresh flag,issue,negative,neutral,neutral,neutral,neutral,neutral
555701940,"1. It depends on if you have paired data. In your case, if you have two photos taken from the same camera angle and position before and after. If you have paired data, use pix2pix. Otherwise, use CycleGAN
2. The correct flags are `--n_epochs` and `--n_epochs_decay`. See the [flag](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L30).
3. You need to merge them for pix2pix. ",paired data case two taken camera angle position paired data use otherwise use correct see flag need merge,issue,negative,neutral,neutral,neutral,neutral,neutral
555700391,We find that coarse-to-fine training sometimes helps. See [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) for an example. ,find training sometimes see example,issue,negative,neutral,neutral,neutral,neutral,neutral
555684475,"The model is symmetric. But datasets and tasks might be not. Sometimes, translating A to B is harder than the other direction.",model symmetric might sometimes harder direction,issue,negative,negative,neutral,neutral,negative,negative
555682777,"This is good to know. Maybe not in the introduction list, as we cannot provide full support now and we haven't fully tested the code.  (as we don't have a windows machine). ",good know maybe introduction list provide full support fully tested code machine,issue,positive,positive,positive,positive,positive,positive
555306753,"I wonder how to transfer binary img to rgb img? what is the input size? Is it concact binary and rgb or just only input rgb for training,binary for testing?",wonder transfer binary input size binary input training binary testing,issue,negative,neutral,neutral,neutral,neutral,neutral
554946584,"Seems nearest neighbour upsampling not solving all issues, but bring another one - random color edges:
https://distill.pub/2016/deconv-checkerboard/
https://twitter.com/ch402/status/793483428649766913?lang=en
https://distill.pub/2016/deconv-checkerboard/assets/style_clean.png
https://twitter.com/alexjc/status/788115081468997634",nearest bring another one random color,issue,negative,negative,negative,negative,negative,negative
554697755,"@junyanz Alright got it, thanks a lot for your help!",alright got thanks lot help,issue,positive,positive,positive,positive,positive,positive
554692379,We assume it is often an RGB image. But feel free to change it for your application.,assume often image feel free change application,issue,positive,positive,positive,positive,positive,positive
554419780,"@junyanz Ok I got that, thank you! I would like to clarify Is there any reason why there is a 3 in the 3rd dimension for image_numpy? I have set the input and output channels to 1, so I am not sure why there is a 3 at the end. Are there perhaps any other areas in the code which might be resulting in a 3? Thank you for your help!",got thank would like clarify reason dimension set input output sure end perhaps code might resulting thank help,issue,positive,positive,positive,positive,positive,positive
554364937,"Another problem is the `image_path` parameter in the `save_image` function – I can't save the image as tiff as the default is .png. When I tried to print `image_path`, I got the directory /results/model_name/test_latest/images/image1_real_A.png, meaning it is already appending the .png file format to the file and hence I cannot save the image as a tiff. Where can I change this? I've tried tracing back the code from `img_path = model.get_image_paths()` in the test.py file to the [create_model](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/1c733f5bd7a3aff886403a46baada1db62e2eca8/models/__init__.py#L54) function in models/__init__.py to the `get_image_paths(self)` function at [models/base_model.py](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/1c733f5bd7a3aff886403a46baada1db62e2eca8/models/base_model.py#L112) but so far no luck as the get_image_paths(self) function says:

> def get_image_paths(self):
        """""" Return image paths that are used to load current data""""""
        return self.image_paths

and I am loading the data in as tiff, so I'm not sure where to go on from there. Any ideas how I can change the .png to .tiff? Thank you for your help!",another problem parameter function ca save image tiff default tried print got directory meaning already file format file hence save image tiff change tried tracing back code file function self function far luck self function self return image used load current data return loading data tiff sure go change thank help,issue,positive,positive,positive,positive,positive,positive
554323881,"@junyanz Ok, I have printed out the shape of the image array and it says (256, 256, 3). Is there any reason why there is a 3 in the 3rd dimension? I have set the input and output channels to 1, so I am not sure why there is a 3 at the end. Are there perhaps any other areas in the code which might be resulting in a 3? Thank you for your help!",printed shape image array reason dimension set input output sure end perhaps code might resulting thank help,issue,positive,positive,positive,positive,positive,positive
554008478,"See this [Q & A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#can-i-continueresume-my-training-350-275-234-87) regarding continue training. For better results, you can try our recent work such as [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE).",see regarding continue training better try recent work spade,issue,negative,positive,positive,positive,positive,positive
553996333,We used a similar configuration for GTA5 to cityscapes. See @taesungp's reply at this [post](#586).,used similar configuration see reply post,issue,negative,neutral,neutral,neutral,neutral,neutral
553793672,"> If you have pairs, please refer to our recent work such as [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE). If you are using CycleGANs, I recommend that you train the models on cropped patches. Please refer to [training/test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images) for more details.

Thanks for your reply. I am using CycleGAN now. I use the default parameters except load_size=1024, crop_size=400(the maximal I can set) when training, load_size=crop_size=1024 when testing. But the result is not good.
I found that you apply the network in large image, can you tell me the parameters?",please refer recent work spade recommend train please refer thanks reply use default except maximal set training testing result good found apply network large image tell,issue,positive,positive,positive,positive,positive,positive
553671202,"@junyanz Thank you very much! I modified the line to say `image_pil = Image.fromarray(image_numpy, mode='F')`. However, I am now getting an error saying 

> ValueError: Too many dimensions: 3 > 2.

that the Image dimensions is 3 instead of 2. Was the image converted to 3D in during the testing phase?",thank much line say however getting error saying many image instead image converted testing phase,issue,negative,positive,positive,positive,positive,positive
553532324,"`--no_dropout` specifies the existence of the Dropout layer, and `--eval` specifies the behavior of the Dropout layer (if it exists) during evaluation mode. You will get stochastic output if (1) you train the model without `--no_dropout` and test the model without `--eval`.",existence dropout layer behavior dropout layer evaluation mode get stochastic output train model without test model without,issue,negative,neutral,neutral,neutral,neutral,neutral
553530944,"If you have pairs, please refer to our recent work such as [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE). If you are using CycleGANs, I recommend that you train the models on cropped patches. Please refer to [training/test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images) for more details.",please refer recent work spade recommend train please refer,issue,positive,neutral,neutral,neutral,neutral,neutral
553529024,It is mode collapse and sometimes it happens. Either retraining the model or using a small learning rate might mitigate the issue. ,mode collapse sometimes either model small learning rate might mitigate issue,issue,negative,negative,negative,negative,negative,negative
553528486,The error seems to be more relevant to the visdom package. Maybe you could post the question on visdom [repo](https://github.com/facebookresearch/visdom). ,error relevant package maybe could post question,issue,negative,positive,positive,positive,positive,positive
553527548,You need to rename the model file to `latest_net_G.pth`. Its original names come with suffix `_A` or `_B`.,need rename model file original come suffix,issue,negative,positive,positive,positive,positive,positive
553069021,Great. I will add the format to the latest commit.,great add format latest commit,issue,positive,positive,positive,positive,positive,positive
552406017,"After doing some tinkering around, I finally found the reason why I was getting the error! Turns out under data/image_folder.py there's a line
`IMG_EXTENSIONS = [
    '.jpg', '.JPG', '.jpeg', '.JPEG',
    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',
]`
and it doesn't contain '.tiff' and '.TIFF', so it wasn't reading the image files. So just add in the two lines to get
`IMG_EXTENSIONS = [
    '.jpg', '.JPG', '.jpeg', '.JPEG', 
    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tif', '.TIF', '.tiff', '.TIFF',
]`
and the error should go away!",around finally found reason getting error turn line contain reading image add two get error go away,issue,negative,neutral,neutral,neutral,neutral,neutral
552400281,"@ethanyanjiali 
thank you for the syntax which works but for me,
I am getting this error 
NameError: global name 'Image' is not defined
how can this be corrected
",thank syntax work getting error global name defined corrected,issue,negative,neutral,neutral,neutral,neutral,neutral
551246688,"I tried using WGAN-GP also on my own dataset. LSGAN does not produce good results so I tried using WGAN-GP, but not only are the results not better (I'd say worse), the losses seem to diverge to crazy values over time (1500+ at epoch 15). 

This work is fantastic, but I am somewhat surprised that there are no clear improvements to CycleGAN even after ~2.5 years? I'd love to try some techniques that are good candidates for improving upon CycleGAN. However, even bigger batchsize (cf. BigGAN) nor different image scales seem to help. Will continue investigating. But is anyone in this thread already looked at ways to improve? 

I'd be keen to explore e.g.:
https://github.com/akanimax/Variational_Discriminator_Bottleneck ",tried also produce good tried better say worse seem diverge crazy time epoch work fantastic somewhat clear even love try good improving upon however even bigger different image scale seem help continue investigating anyone thread already way improve keen explore,issue,positive,positive,positive,positive,positive,positive
551225142,"It's normal as the default pix2pix generator has Dropout, and by default, the test code still enables Dropout for randomness. If you don't like Dropout, you can disable it by setting `--no_dropout` for both training and test. ",normal default generator dropout default test code still dropout randomness like dropout disable setting training test,issue,negative,positive,positive,positive,positive,positive
550587733,yes sir I have enough paired visual-infrared images.  I will try pix2pix. Bundle of thanks. ,yes sir enough paired try bundle thanks,issue,positive,positive,neutral,neutral,positive,positive
550453418,"If you have paired visual-infrared images, you can try pix2pix. (Otherwise, you can try cyclegan) You may want to write your custom data loader for processing your images. See a template [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py). ",paired try otherwise try may want write custom data loader see template,issue,negative,neutral,neutral,neutral,neutral,neutral
550453181,"if your output image is a binary image, it may cause trouble for the default GAN loss. It is easy for the discriminator to distinguish ground truth binary data and generated results. You may want to try other GAN losses (e.g., `--gan_mode lsgan`).",output image binary image may cause trouble default gan loss easy discriminator distinguish ground truth binary data may want try gan,issue,negative,positive,positive,positive,positive,positive
550302221,"Yeah i set the params correctly. My sample images are something like this below....

![Image A](https://user-images.githubusercontent.com/49688647/68301032-61460400-009f-11ea-8605-59959dcba213.jpg)

![Image B](https://user-images.githubusercontent.com/49688647/68300697-9736b880-009e-11ea-8d12-c72b582e4d3b.jpg)",yeah set correctly sample something like image image,issue,positive,neutral,neutral,neutral,neutral,neutral
549992143,Not sure about it. You probably need to modify the script.,sure probably need modify script,issue,negative,positive,positive,positive,positive,positive
549991993,Did you set the correct `-input_nc` and `--output_nc` for your first experiment?,set correct first experiment,issue,negative,positive,positive,positive,positive,positive
549970596,You can fine-tune a classifier using pre-trained weights. See more sim2real experimental details in our [CycADA](https://arxiv.org/abs/1711.03213) paper. ,classifier see experimental paper,issue,negative,positive,neutral,neutral,positive,positive
549969724,"Dropout was not used in CycleGAN. But feel free to try it. If you implement your own version, maybe you want to test the code on our datasets first, and try to match the result to our version.",dropout used feel free try implement version maybe want test code first try match result version,issue,positive,positive,positive,positive,positive,positive
549265674,"@YilinLiu97  Hi,  i face a similar problem. Do you know why cyclegan will change shape? do you find way to avoid it? Thanks
",hi face similar problem know change shape find way avoid thanks,issue,negative,positive,neutral,neutral,positive,positive
549155550,"Hello Sir,
Firstly I am extremely sorry for the delayed response. I had thought I would reply after I figure out the next steps. But I haven't been able to do it. Sir, I have some trouble with using the transformation of images. I understand that I have to transform them to tensors and then return the paths to A and B. In my case I have 4-Channel stacked input and 3-Channel output. I tried using `# dataA = ToTensor()(A) # dataB = ToTensor()(B)` and also these - `params = get_params(self.opt, B.size)
dataA = get_transform(self.opt, params, method=Image.BILINEAR, normalize=False)
dataB = get_transform(self.opt, params)` and ` transform_params = get_params(self.opt, A.size)
        A_transform = get_transform(self.opt, transform_params, grayscale=(self.input_nc == 1))
        B_transform = get_transform(self.opt, transform_params, grayscale=(self.output_nc == 1))`. But I run into runtime errors. I was trying to apply the same transformations on both the images. Is my understanding correct?

Once again, I apologize for the delay. I am very thankful for your time and consideration so far.",hello sir firstly extremely sorry response thought would reply figure next able sir trouble transformation understand transform return case input output tried also run trying apply understanding correct apologize delay thankful time consideration far,issue,negative,negative,neutral,neutral,negative,negative
549041680,"Here's pretrained [checkpoint](http://efrosgans.eecs.berkeley.edu/cyclegta/gta2cityscapes_checkpoints.zip). Note that this model was trained on Lua Torch model using our original CycleGAN github repo. 

You can generate images by 

DATA_ROOT=[path to images] name=gta2cityscapes checkpoints_dir=[path to checkpoints] model=one_direction_test phase=train loadSize=1024 fineSize=1024 resize_or_crop=""scale_width"" th test.lua

Also note that this model is slightly different from [CyCADA's result](https://github.com/jhoffman/cycada_release) because CyCADA was trained longer (20 epochs) with larger crop size (400x400). Unfortunately I don't have that model on my side. 

You can train one for yourself too. In general, follow directions in https://github.com/junyanz/CycleGAN, but train with resize_or_crop=""scale_width_and_crop"" loadSize=1024 fineSize=400 lambda_identity=1.0.

You should also be able to generate all images using this pretrained model. Let me know if this worked for you!

",note model trained torch model original generate path path th also note model slightly different result trained longer crop size unfortunately model side train one general follow train also able generate model let know worked,issue,negative,positive,neutral,neutral,positive,positive
547975478,"If you use the `--model test` mode and only test one direction, you need to manually change the file name. ",use model test mode test one direction need manually change file name,issue,negative,neutral,neutral,neutral,neutral,neutral
547577322,"Thanks for your prompt reply and sorry I haven't seen you reply sooner.

I have found that the cycle-gan with extra paired supervision loss (L1) always produces better result than pix2pix on paired data... I will look into it again and hopefully find a solution.",thanks prompt reply sorry seen reply sooner found extra paired supervision loss always better result paired data look hopefully find solution,issue,positive,positive,neutral,neutral,positive,positive
547540610,"I appreciate your reply! The models saved in the checkpoint have direction specified. For example ""latest_net_G_A"" and when I try to test on the model using the same options as the horse&zebra, it fails since it needs a ""latest_net_G"". Where do I specify the direction?",appreciate reply saved direction example try test model horse zebra since need specify direction,issue,positive,neutral,neutral,neutral,neutral,neutral
547519288,"D_A and D_B are not available. Only G_A and G_B are provided. You can continue the training with G_A and G_B, and train D_A and D_B from scratch. ",available provided continue training train scratch,issue,negative,positive,positive,positive,positive,positive
547518285,The models are automatically saved to `./checkpoints/` directory.  You can control the auto-save frequency using `--save_epoch_freq`.,automatically saved directory control frequency,issue,negative,neutral,neutral,neutral,neutral,neutral
547433670,Make sure u send a tensor to the device,make sure send tensor device,issue,negative,positive,positive,positive,positive,positive
547285801,"Can we get latest_G_A, latest_G_B, latest_D_A, latest_D_B for continuing training on our's dataset ?
How can I get those dataset ? I can find horse2zebra and zebra2horse in [here](http://efrosgans.eecs.berkeley.edu/cyclegan/pretrained_models/), but still miss D_A and D_B",get training get find still miss,issue,negative,neutral,neutral,neutral,neutral,neutral
547041649,"1. It works `batch_size > 1`. The Pytorch built-in data loader will automatically handle it. 
2. The difference is whether index_B depends on index_A or not. In the first line, it depends. In the second line, it doesn't. 
3.  Nope. See 2. The index is a random integer assigned by the PyTorch data loader. 
4. PyTorch dataloader will shuffle the data for you (if setting `shuffle=True`). You don't need to shuffle it by yourself.

For more details, please refer to the [document](https://pytorch.org/docs/stable/data.html) for the PyTorch data loader. ",work data loader automatically handle difference whether first line second line nope see index random integer assigned data loader shuffle data setting need shuffle please refer document data loader,issue,negative,negative,neutral,neutral,negative,negative
546829359,"Thanks for your answer @junyanz. I have some followup questions based on that. And I hope you have time to answer me.

1) It seems like the code you are referring to only is used when the batch_size = 1? I base this on index_B is only one integer value? Where do you use the class UnalignedDataset (I wasn't able to locate it)? Is this called every time training is done? I'm curious because I would love to see what is done when batch_size > 1. 

1) I assume the **_index_** is a random integer for data indexing. So that means by default that the indexing start point is random, right? And if this is true, I don't see any difference between the following two lines:
The following index_B is random because **_index_** is random.:
`index_B = index % self.B_size`
The following indexing is random because of randint:
`index_B = random.randint(0, self.B_size - 1)`
Won't it just be random index by default anyway because of variable **_index_**?

3) So you are using the flag **_serial_batches_** and its _**False**_ by default. As I understand it this means that the indexing when training is random e.g. 1,6,100,2,600 etc. which correspond to shuffling the data? And if _**serial_batches**_ = True the order is 1,2,3,4,5 etc.? Right?

4) If the above is true: don't you shuffle the data before its used for training? Or is that what the flag serial_batches determines? By calling the function __get_item__ [code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L39) and determine if the dataset should be shuffled or not?

Thanks!",thanks answer based hope time answer like code used base one integer value use class able locate every time training done curious would love see done assume random integer data indexing default indexing start point random right true see difference following two following random random index following indexing random wo random index default anyway variable flag false default understand indexing training random correspond shuffling data true order right true shuffle data used training flag calling function code determine thanks,issue,positive,negative,neutral,neutral,negative,negative
546745045,Sorry about that. I will share the model with you ASAP. At the moment the Berkeley machines are down due to [california wildfire ](https://www.nytimes.com/2019/10/27/us/kincade-fire-california.html). I will share it once the machines are back up in a couple of days. ,sorry share model moment due wildfire share back couple day,issue,negative,negative,negative,negative,negative,negative
546738749,"Here is the [code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L51) for indexing.  Here is the [code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L65) that returns the length of the dataset.  We have two ways of indexing, which depends on the flag `--serial_batches`.",code indexing code length two way indexing flag,issue,negative,neutral,neutral,neutral,neutral,neutral
546593513,"> Because CycleGAN uses [LSGAN](https://arxiv.org/abs/1611.04076), which applies a MSE loss.

Thanks a lot!! Wondering about the same for a couple of days! ",loss thanks lot wondering couple day,issue,negative,positive,positive,positive,positive,positive
546581753,"> Does it work with default datasets and training script? Could you run the demo code of PyTorch?

I test some project.
Project1(Simple):
import torch
from torch.autograd import Variable

w1 = torch.Tensor([2]) #认为w1 与 w2 是函数f1 与 f2的参数
w1 = Variable(w1,requires_grad=True)
w2 = torch.Tensor([2])
w2 = Variable(w2,requires_grad=True)
x2 = torch.rand(1)
x2 = Variable(x2,requires_grad=True)
y2 = x2**w1            # f1 运算
z2 = w2*y2+1           # f2 运算
z2.backward()
print(x2.grad)
print(y2.grad)
print(w1.grad)
print(w2.grad)
I can get the result.
Project2:
A CNN experiment.
The address is https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/502_GPU.py.
The same problem occurs again.",work default training script could run code test project project simple import torch import variable variable variable variable print print print print get result project experiment address problem,issue,negative,neutral,neutral,neutral,neutral,neutral
546578451,"> Does it work with default datasets and training script? Could you run the demo code of PyTorch?

I find that problem is also in ssd which is implemented in Pytorch. Maybe the key point is Pytorch?",work default training script could run code find problem also maybe key point,issue,negative,neutral,neutral,neutral,neutral,neutral
546445114,"You can add a few more layers, or increase the learning rate of D.",add increase learning rate,issue,negative,neutral,neutral,neutral,neutral,neutral
546444789,Does it work with default datasets and training script? Could you run the demo code of PyTorch?,work default training script could run code,issue,negative,neutral,neutral,neutral,neutral,neutral
546183562,"@junyanz I found the issue. The issue is that the generator is too strong because I added a new loss to the generator, Hence, the D may easy to fool. Do you have any suggestion to solve it? Do I need to add more layer for D network (currently I used the patch of 7x7 as the output of the D network).",found issue issue generator strong added new loss generator hence may easy fool suggestion solve need add layer network currently used patch output network,issue,negative,positive,positive,positive,positive,positive
546059668,You probably need to retrain your models with `resnet_9blocks`. It depends on both `--netG` and `--preprocess`. See the [Training & Test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-rectangular-images) for more details.,probably need retrain see training test,issue,negative,neutral,neutral,neutral,neutral,neutral
546023922,"1. You probably need some training/test split, as you need to evaluate your classifier's performance. You can also just follow the training/test split for your classification task. 
2. I think it's fine to keep it as it.
3. They don't have to be the same.",probably need split need evaluate classifier performance also follow split classification task think fine keep,issue,negative,positive,positive,positive,positive,positive
545846909,"> Generators in this repo is designed for data in range [-1, 1], as you can see from the tanh at the end. So data with larger range will definitely fail. Moreover, it is generally a good idea to normalize data in DL, as it empirically stables training. In GAN training, D loss is often not a very good metric for quality. You should look at the results and see how they are, or evaluate them with domain specific metrics if you have any.
> […](#)
> On Mon, Oct 21, 2019 at 03:07 Skaudrey ***@***.***> wrote: Here are my data: 2D, not image, and each feature has different range. If I did not rescale my data, then G_loss is usually at 1000 or more. Aftre rescaled my data into [-1,1], G_loss went down to 0.8, and D_loss is mostly at 0.5. Here are my questions: 1. It seems the bias between generated data and ground truth is sensbile of the range. However, not like images which I know exactly the range is [0,255], the range of my data are done by sampling approximation. So, should I concentrate on getting a better approximation of range? 2. Is it a good result with D_loss mostly at 0.5? 3. Can you give me more advice for these kind of data? Thanks anyway. @SsnL <https://github.com/SsnL> — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#304?email_source=notifications&email_token=ABLJMZNTTQW7JMAA2ZCJN7DQPVIMFA5CNFSM4FGI3LV2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBZJDDY#issuecomment-544379279>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/ABLJMZM5VHKQKJ2KAETOIMTQPVIMFANCNFSM4FGI3LVQ> .

Thanks. I used CycleGAN, and choose to save the model with a smaller X2Y generator loss and Y2X generator loss, while
      generator loss = generative loss + cycle loss + identity loss.
However, I found that my identity loss always converges to 1 and so is cycle loss.
<img width=""255"" alt=""捕获"" src=""https://user-images.githubusercontent.com/26813208/67475494-e46f5f00-f688-11e9-9292-f3ebcddc667a.PNG"">

The shape of my X data is 1*3, and shape of Y is 20*3, to build generators easily, I expand my X by copying it 20 times, and thus the variety in X is smaller than Y. 

I tried to give a soft label for discriminators (recommended here: [https://github.com/soumith/ganhacks](url) at item 6), but the generators did not improve.

Any tips for me?
@SsnL ",designed data range see tanh end data range definitely fail moreover generally good idea normalize data training gan training loss often good metric quality look see evaluate domain specific metric mon wrote data image feature different range data usually data went mostly bias data ground truth range however like know exactly range range data done sampling approximation concentrate getting better approximation range good result mostly give advice kind data thanks anyway reply directly view thanks used choose save model smaller generator loss generator loss generator loss generative loss cycle loss identity loss however found identity loss always cycle loss shape data shape build easily expand time thus variety smaller tried give soft label item improve,issue,positive,positive,positive,positive,positive,positive
545784373,It was the `--no_dropout` missing from the command. I used the `test.py` file. I tried running the file from terminal not from PyCharm and it worked. Thank you !,missing command used file tried running file terminal worked thank,issue,negative,negative,negative,negative,negative,negative
545782281,"Changing `--input_nc`  and `--output_nc`removes the weight error.

```
 parser.add_argument('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')
        parser.add_argument('--output_nc', type=int, default=1, help='# of output image channels: 3 for RGB and 1 for grayscale')
```
But the missing keys error persists:
```
RuntimeError: Error(s) in loading state_dict for ResnetGenerator:
	Missing key(s) in state_dict: ""model.10.conv_block.6.weight"", ""model.10.conv_block.6.bias"", ""model.11.conv_block.6.weight"", ""model.11.conv_block.6.bias"", ""model.12.conv_block.6.weight"", ""model.12.conv_block.6.bias"", ""model.13.conv_block.6.weight"", ""model.13.conv_block.6.bias"", ""model.14.conv_block.6.weight"", ""model.14.conv_block.6.bias"", ""model.15.conv_block.6.weight"", ""model.15.conv_block.6.bias"", ""model.16.conv_block.6.weight"", ""model.16.conv_block.6.bias"", ""model.17.conv_block.6.weight"", ""model.17.conv_block.6.bias"", ""model.18.conv_block.6.weight"", ""model.18.conv_block.6.bias"". 
	Unexpected key(s) in state_dict: ""model.10.conv_block.5.weight"", ""model.10.conv_block.5.bias"", ""model.11.conv_block.5.weight"", ""model.11.conv_block.5.bias"", ""model.12.conv_block.5.weight"", ""model.12.conv_block.5.bias"", ""model.13.conv_block.5.weight"", ""model.13.conv_block.5.bias"", ""model.14.conv_block.5.weight"", ""model.14.conv_block.5.bias"", ""model.15.conv_block.5.weight"", ""model.15.conv_block.5.bias"", ""model.16.conv_block.5.weight"", ""model.16.conv_block.5.bias"", ""model.17.conv_block.5.weight"", ""model.17.conv_block.5.bias"", ""model.18.conv_block.5.weight"", ""model.18.conv_block.5.bias"". 

```",weight error input image output image missing error error loading missing key model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias unexpected key model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias model weight model bias,issue,negative,negative,neutral,neutral,negative,negative
545771588,You need to specify the `--input_nc` and `--output_nc` during test time. ,need specify test time,issue,negative,neutral,neutral,neutral,neutral,neutral
545768054,"I am using `--norm instance` for training and testing phase. It should be the same network, after I train I rename the `epoch_net_G_A.pth` to `latest_net_G.pth` and run the `test.py` with `--preprocess none`. As you can see in the error posted above the weight file has a `torch.Size([1, 64, 7, 7])` parameter, which is good because I trained it this way but he expects a `torch.Size([3, 64, 7, 7])` so my guess is that the model that is built on the testing phase has 3 channels as output.

**Here are the changes I made**: 
```
# GENERATOR A
        generator_a_channels_input = 3
        generator_a_channels_output = 1
        # GENERATOR B
        generator_b_channels_input = 1
        generator_b_channels_output = 3
        self.netG_A = networks.define_G(generator_a_channels_input, generator_a_channels_output, opt.ngf, opt.netG, opt.norm,
                                        not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)
        self.netG_B = networks.define_G(generator_b_channels_input, generator_b_channels_output, opt.ngf, opt.netG, opt.norm,
                                        not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)
```
Does this code influence the reconstruction of the model ?
```
parser.add_argument('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')
        parser.add_argument('--output_nc', type=int, default=1, help='# of output image channels: 3 for RGB and 1 for grayscale')
```

*Keep in mind that I commented the assert where the `in_channels == out_channels`


",norm instance training testing phase network train rename run none see error posted weight file parameter good trained way guess model built testing phase output made generator generator code influence reconstruction model input image output image keep mind assert,issue,positive,positive,positive,positive,positive,positive
545695496,"Ok, thanks! I downgraded visdom and the code has been running. I'll see.",thanks code running see,issue,negative,positive,positive,positive,positive,positive
545636951,Make sure that you use the same network and the same `--norm` flag for both training and test. ,make sure use network norm flag training test,issue,negative,positive,positive,positive,positive,positive
545636484,"They don't have to be square images. During training time, you can crop square patches using the flag `--preprocess crop`. During test time, you can apply the model to images with arbitrary sizes (rectangle or not) using the flag `--preprocess none`. ",square training time crop square flag crop test time apply model arbitrary size rectangle flag none,issue,negative,negative,neutral,neutral,negative,negative
545270008,Please also add `--norm instance` in your test code. The default one for pix2pix is `--norm batch`.,please also add norm instance test code default one norm batch,issue,negative,neutral,neutral,neutral,neutral,neutral
545259080,"Here are some of the options I used; may be relevant.

--model pix2pix --direction BtoA --batch_size 1 --norm instance --gpu_ids 0 --lr_policy linear --beta1 0.9 --lr 0.000001 --name test10 --gan_mode lsgan --init_type xavier",used may relevant model direction norm instance linear beta name test,issue,negative,positive,positive,positive,positive,positive
545232009,Have you installed visdom? Does visdom work for you? It seems to be related to visdom rather than our codebase. Maybe you can post the question in the visdom repo.,work related rather maybe post question,issue,negative,neutral,neutral,neutral,neutral,neutral
544579293,Use `--model test`. See this [script](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/test_single.sh) for an example.,use model test see script example,issue,negative,neutral,neutral,neutral,neutral,neutral
544578654,I am not sure if it will improve the results. But you could give a try.,sure improve could give try,issue,positive,positive,positive,positive,positive,positive
544531905,"Generators in this repo is designed for data in range [-1, 1], as you can
see from the tanh at the end. So data with larger range will definitely
fail. Moreover, it is generally a good idea to normalize data in DL, as it
empirically stables training.

In GAN training, D loss is often not a very good metric for quality. You
should look at the results and see how they are, or evaluate them with
domain specific metrics if you have any.

On Mon, Oct 21, 2019 at 03:07 Skaudrey <notifications@github.com> wrote:

> Here are my data: 2D, not image, and each feature has different range.
> If I did not rescale my data, then G_loss is usually at 1000 or more.
> Aftre rescaled my data into [-1,1], G_loss went down to 0.8, and D_loss is
> mostly at 0.5. Here are my questions:
>
>    1.
>
>    It seems the bias between generated data and ground truth is sensbile
>    of the range. However, not like images which I know exactly the range is
>    [0,255], the range of my data are done by sampling approximation. So,
>    should I concentrate on getting a better approximation of range?
>    2.
>
>    Is it a good result with D_loss mostly at 0.5?
>    3.
>
>    Can you give me more advice for these kind of data?
>
> Thanks anyway.
> @SsnL <https://github.com/SsnL>
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/304?email_source=notifications&email_token=ABLJMZNTTQW7JMAA2ZCJN7DQPVIMFA5CNFSM4FGI3LV2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBZJDDY#issuecomment-544379279>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABLJMZM5VHKQKJ2KAETOIMTQPVIMFANCNFSM4FGI3LVQ>
> .
>
",designed data range see tanh end data range definitely fail moreover generally good idea normalize data training gan training loss often good metric quality look see evaluate domain specific metric mon wrote data image feature different range data usually data went mostly bias data ground truth range however like know exactly range range data done sampling approximation concentrate getting better approximation range good result mostly give advice kind data thanks anyway reply directly view,issue,positive,positive,positive,positive,positive,positive
544379279,"Here are my data: 2D, not image, and each feature has different range.
If I did not rescale my data, then G_loss is usually at 1000 or more. Aftre rescaled my data into [-1,1], G_loss went down to 0.8, and D_loss is mostly at 0.5. Here are my questions:

1. It seems the bias between generated data and ground truth is sensbile of the range. However, not like images which I know exactly the range is [0,255], the range of my data are done by sampling approximation. So, should I concentrate on getting a better approximation of range?

2. Is it a good result with D_loss mostly at 0.5?

3. Can you give me more advice for these kind of data?

Thanks anyway.
@SsnL ",data image feature different range data usually data went mostly bias data ground truth range however like know exactly range range data done sampling approximation concentrate getting better approximation range good result mostly give advice kind data thanks anyway,issue,positive,positive,positive,positive,positive,positive
544297484,"Great suggestion. I will let you know the region. Actually, I met the issue when I train with multiple GPU. I increase the crop-size but the issue still happens. Let I debug it",great suggestion let know region actually met issue train multiple increase issue still let,issue,positive,positive,positive,positive,positive,positive
544295139,"See this [Q&A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#out-of-memory-174). You can crop square patches during training time. During test time, you can directly apply the model to your rectangular images. ",see crop square training time test time directly apply model rectangular,issue,negative,positive,neutral,neutral,positive,positive
544292650,"You can crop square patches during training time. During test time, you can apply the model to images with arbitrary sizes (rectangle or not).  ",crop square training time test time apply model arbitrary size rectangle,issue,negative,negative,neutral,neutral,negative,negative
544292549,"If you want to inspect it, you can add some debugging code. Each time, the loss is above some threshold, you save images to the disk.  ",want inspect add code time loss threshold save disk,issue,negative,neutral,neutral,neutral,neutral,neutral
544292362,Please use the valid flags. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/datasets/combine_A_and_B.py#L6). `--dataroot` does not exist for `combine_A_and_B.py`.,please use valid see exist,issue,negative,neutral,neutral,neutral,neutral,neutral
544251379,"I have the same problem and when I had `python3 datasets/combine_A_and_B.py --fold_A /path/to/data/A --fold_B /path/to/data/B --fold_AB /path/to/data --dataroot ./datasets`

I have this error : `create image pairs: error: unrecognized arguments: --dataroot ./datasets`

Sorry for the silly question but I don't get if I have to create a folder named `/path/to/data` into the `datasets` folder with A and B folder into it",problem python error create image error unrecognized sorry silly question get create folder folder folder,issue,negative,negative,negative,negative,negative,negative
544246763,Ok but how did you resize it to have rectangular output ? Thanks for your help !,resize rectangular output thanks help,issue,positive,positive,positive,positive,positive,positive
544246649,"With CycleGAN model I tried to add `--resize_or_crop none` for test.py to have a rectangular output but I have this error `test.py: error: unrecognized arguments: --resize_or_crop none` 
Should I do something different for Cycle GAN ? ",model tried add none rectangular output error error unrecognized none something different cycle gan,issue,positive,neutral,neutral,neutral,neutral,neutral
544218050,"So basically, for running the code in multiple GPU's, the one GPU has to has memory more than 12 GB. So the only option is to resize the images.",basically running code multiple one memory option resize,issue,negative,neutral,neutral,neutral,neutral,neutral
544212810,"You meant that sometimes the input network is noise images (i.e. many backgrounds with black regions and foreground is quite small). 
You are right. I use crop image, so maybe sometimes the cropped image contains only background region with very small foreground region.",meant sometimes input network noise many black foreground quite small right use crop image maybe sometimes image background region small foreground region,issue,negative,positive,neutral,neutral,positive,positive
544212251,"1. We haven't saved our training plots. But if you train a model with default datasets and parameters,  you should be able to see a typical cyclegan training loss curve. 

2. The GAN losses we used do not converge as it's minimax optimization problem. There are other types of GAN losses that converge (e.g., WGAN family)

3. If D's loss is almost 0 for a while, your training might have collapsed. You can inspect the visual results directly.

",saved training train model default able see typical training loss curve gan used converge optimization problem gan converge family loss almost training might inspect visual directly,issue,negative,positive,positive,positive,positive,positive
544211980,"Sometimes, it might be caused by an out-of-distribution training image: i.e., a usual training image that looks quite different from others. For example, most of your input has rich texture and structure, and suddenly you feed the network a flat input.",sometimes might training image usual training image quite different example input rich texture structure suddenly feed network flat input,issue,positive,positive,neutral,neutral,positive,positive
544211896,I am not sure what is best. Some folks suggest sqrt(K) or K when the batch size is K. Using the same learning rate is also fine.,sure best suggest batch size learning rate also fine,issue,positive,positive,positive,positive,positive,positive
544171787,@filmo : Have you find the reason why the spike in the loss and how to solve it?,find reason spike loss solve,issue,negative,neutral,neutral,neutral,neutral,neutral
544161622,Is it for CycleGAN ? What did you change to have this size of output ? I am interested to do the same output size !,change size output interested output size,issue,negative,positive,positive,positive,positive,positive
543964201,"@junyanz 
>Instancenorm should be fine. You can slightly increase the learning rate. Some folks suggest sqrt(K) or K when the batch size is K. Using the same learning rate is also fine.

If my learning rate for batch size of 1 is 0.0002 then the batch size of 16 should use the learning rate of 0.0002 * sprt(16) = 0.0008? Am I right?",fine slightly increase learning rate suggest batch size learning rate also fine learning rate batch size batch size use learning rate right,issue,positive,positive,positive,positive,positive,positive
543952824,"@kpagels : You have similar issue that I have before. Some comments may works
1. You can reduce number of layers D from 5 to 4 or 3. It may reduce the training loss D goes to zero
2. Using larger learning rate when use batch size bigger. I have not test but it may be work
3. Do not use the batch norm. Batch norm will make the style of image is not changed.",similar issue may work reduce number may reduce training loss go zero learning rate use batch size bigger test may work use batch norm batch norm make style image,issue,negative,neutral,neutral,neutral,neutral,neutral
543776040,The range of the loss is quite normal.,range loss quite normal,issue,negative,positive,positive,positive,positive,positive
543737017,"Could you git pull the latest code, and then make your commit?",could git pull latest code make commit,issue,negative,positive,positive,positive,positive,positive
543355156,"@junyanz Thanks for your info. It is agreeable for losses to oscillate. But my doubt is discriminator loss always between 0.001 to 1. I trained an model just with 100 images for 200 epochs. The result seems to be decent for 100 odd images. I have attached the screenshots logloss plot for reference. Is it normal behaviour for discrimator loss to be between 0 and 1?

![Screenshot from 2019-10-17 16-04-21](https://user-images.githubusercontent.com/49688647/67046111-e8066180-f12f-11e9-90ca-f0642d266021.png)
![Screenshot from 2019-10-17 16-04-41](https://user-images.githubusercontent.com/49688647/67046112-e8066180-f12f-11e9-9365-0ddcec9da4cb.png)
![Screenshot from 2019-10-17 16-04-26](https://user-images.githubusercontent.com/49688647/67046113-e8066180-f12f-11e9-908c-a35f1791348e.png)


",thanks agreeable oscillate doubt discriminator loss always trained model result decent odd attached plot reference normal behaviour loss,issue,negative,positive,neutral,neutral,positive,positive
543328578,"Thanks ill try that out! Just one last question :)

Is it correct that you don't have any activation function in the last layer of the discriminator? And if understood correctly, why not? I thought it should have sigmoid?",thanks ill try one last question correct activation function last layer discriminator understood correctly thought sigmoid,issue,negative,negative,neutral,neutral,negative,negative
543292059,The GAN loss often oscillates during the training. It's fine as long as your results get improved. See Sec 5.1 of this tutorial [paper](https://arxiv.org/pdf/1701.00160.pdf) for more details. ,gan loss often training fine long get see sec tutorial paper,issue,negative,positive,neutral,neutral,positive,positive
543289557,"The CycleGAN data loader takes the max(|testA_images|, |testB_images|). See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L71). You can use the flag `--model test` to only test the model in one direction. See the instruction [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan).",data loader see line use flag model test test model one direction see instruction,issue,negative,neutral,neutral,neutral,neutral,neutral
543288285,"If you use pix2pix, you need to use `aligned_dataset.py`. If you use CycleGAN, you need to use `unaligned_dataset.py`. If you want to implement a new data loading function (such as two inputs, and one output), you can look at the data loader [template](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py). The template has detailed instructions on how to implement your own function.  ",use need use use need use want implement new data loading function two one output look data loader template template detailed implement function,issue,negative,positive,positive,positive,positive,positive
543219575,"Hello Sir, Thank you very much for your response. 
So Sir, I went through your answers to figure out what you mean by modifying the files, but I am not able to understand it. What does it mean to modify the `aligned_dataset.py` code? In one of the other answers, I read that we would have to use the` unaligned_dataset.py`. If I have to write a new data loader, what exactly must I change?  I found a sample here - [(http://places.csail.mit.edu/deepscene/small-projects/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py )] 
I apologize for the barrage of questions. I tried to understand it, but I am still quite confused. 
Thank you for your time and consideration. 

",hello sir thank much response sir went figure mean able understand mean modify code one read would use write new data loader exactly must change found sample apologize barrage tried understand still quite confused thank time consideration,issue,negative,positive,neutral,neutral,positive,positive
543014315,"Thank you, I will try this branch with `Spectral normalization`.",thank try branch spectral normalization,issue,negative,neutral,neutral,neutral,neutral,neutral
543012939,"Yes. `opt.load_iter` is not used in the test code as it is an optional flag, which is not used very often. For completeness, I will add it to the test code. If `opt.load_iter > 0`, the test code will create a web directory using iteration for naming. `Spectral normalization` PR still needs some code review and clean up. But you are free to use this branch. ",yes used test code optional flag used often completeness add test code test code create web directory iteration naming spectral normalization still need code review clean free use branch,issue,positive,positive,positive,positive,positive,positive
542989364,`combine_A_and_B.py` may not work in this case. You need to modify the `aligned_dataset.py`. Here is a [template](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) on how to write your custom data loader. ,may work case need modify template write custom data loader,issue,negative,neutral,neutral,neutral,neutral,neutral
542950769,"Hi, I had a question, in this case(ie after stacking the input images along C dimension), should we still run the combine_A_and_B.py code? ",hi question case ie input along dimension still run code,issue,negative,neutral,neutral,neutral,neutral,neutral
542887873,"Good catch. I think we should probably move `visualizer.reset()` out of the inner loop. We just need to reset it once every epoch. The purpose of this condition (L156) is to make sure that we save results to HTML at least once for each epoch, when `display_current_results` is called, and even when `update_html_freq` is much larger than `display_freq`.",good catch think probably move inner loop need reset every epoch purpose condition make sure save least epoch even much,issue,positive,positive,positive,positive,positive,positive
542875440,"The current unet_256 is using deconvolution. For how to reduce checkboard artifacts, please see #190. For more advanced architectures, please refer to [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE).",current reduce please see advanced please refer spade,issue,positive,positive,positive,positive,positive,positive
542865887,"Your training collapsed. You can remove 1-2 layers from D, or increase the learning rate of G. ",training remove increase learning rate,issue,negative,neutral,neutral,neutral,neutral,neutral
542719518,"Thanks for your reply! Its really great that you are helping us out :)

I normalized my data to - 1,1 and set resnet to 6. I also tried to change real label to 0.9. But my discriminators loss is still going close to 0. Like 0.00011. Is this normal or did my system collapse?

Do you have any other implementation tips for the discriminator loss not going towards zero? Or some link to a place? Should i train generator more then discriminator? ",thanks reply really great helping u data set also tried change real label loss still going close like normal system collapse implementation discriminator loss going towards zero link place train generator discriminator,issue,positive,positive,positive,positive,positive,positive
542513318,"I'm very graceful for your help. I reinstalled firefox(a more modern version), it works! This appears to be a browser compatibility problem.  I'm sorry to bother you. ",graceful help modern version work browser compatibility problem sorry bother,issue,negative,negative,negative,negative,negative,negative
542392205,"1. Yes. 
2. You can reduce the number of resnet blocks. For example, you can use `--netG resnet_6blocks`.
3. Instancenorm should be fine. You can slightly increase the learning rate. Some folks suggest sqrt(K) or K when the batch size is K.  Using the same learning rate is also fine. ",yes reduce number example use fine slightly increase learning rate suggest batch size learning rate also fine,issue,positive,positive,positive,positive,positive,positive
542390155,"I am not sure what has caused the issue. If you have a paired dataset, I recommend that you use pix2pix. ",sure issue paired recommend use,issue,positive,positive,positive,positive,positive,positive
542089278,"ok, thanks a lot! Currently, I still can't solve this visdom problem. If I don't use visdom, will I miss the relevant training information? For example: loss curve, etc. Can I get training information in other ways?",thanks lot currently still ca solve problem use miss relevant training information example loss curve get training information way,issue,negative,positive,positive,positive,positive,positive
541799831,"I am not sure what has happened. Does it work for other visdom demos? Maybe you can run some example demos in the visdom [repo](https://github.com/facebookresearch/visdom). I am not sure if it is caused by our repo or just visdom issues. If you post your question on visdom repo, more people might be able to know what's going on. ",sure work demo maybe run example demo sure post question people might able know going,issue,positive,positive,positive,positive,positive,positive
541797483,"Yes, please use `--model test`. See this [section](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix) for more details.  Also, add the pix2pix generator flags `--netG unet --norm batch` or whatever you used. ",yes please use model test see section also add generator norm batch whatever used,issue,positive,neutral,neutral,neutral,neutral,neutral
541493921,Yes. The feature maching loss that is I meant. Thanks for your reference.,yes feature loss meant thanks reference,issue,positive,positive,positive,positive,positive,positive
541492414,"Thanks for your response! I reinstalled the related dependencies, but I still see a blue screen in localhost:8097 and no navigation bar, like this:
![1](https://user-images.githubusercontent.com/36001989/66629899-a4928b80-ec35-11e9-9c25-8a9f241285f0.png)
I tried a lot of methods, but it didn't work. I really need your help.
Could you give me some other advice? 
",thanks response related still see blue screen navigation bar like tried lot work really need help could give advice,issue,positive,positive,neutral,neutral,positive,positive
541468299,"I recommend that you use a smaller lambda for cycle consistency loss, remove identity loss, or increase the receptive field of the discriminator. You can also experiment with a bigger generator. ",recommend use smaller lambda cycle consistency loss remove identity loss increase receptive field discriminator also experiment bigger generator,issue,negative,neutral,neutral,neutral,neutral,neutral
541467566,"This [post](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/358) might be relevant. I recommend that you specify the netG and norm layers (e.g., `-netG unet --norm batch`).",post might relevant recommend specify norm norm batch,issue,negative,positive,positive,positive,positive,positive
541467421,Do you mean the feature matching loss (or perceptual loss)?  The matching loss requires paired data. You may want to check out a recent [work](http://openaccess.thecvf.com/content_ECCV_2018/papers/Diana_Sungatullina_Image_Manipulation_with_ECCV_2018_paper.pdf) that tries to use a hybrid of feature matching and GAN loss in unpaired cases.,mean feature matching loss perceptual loss matching loss paired data may want check recent work use hybrid feature matching gan loss unpaired,issue,negative,negative,negative,negative,negative,negative
541386442,"Same issue:
`AttributeError: 'Sequential' object has no attribute 'model'`

when setting `--model test`",issue object attribute setting model test,issue,negative,neutral,neutral,neutral,neutral,neutral
541296496,"I also use `conda env create -f environment.yml` but it causes another problem when building torchvision-0.5.0a0+7ae1b8c
Finally I use `pip install -r requirements.txt` and it works.",also use create another problem building finally use pip install work,issue,negative,neutral,neutral,neutral,neutral,neutral
541148741,i want to know how to save or create latest_net_G_A.pth file while training....?,want know save create file training,issue,positive,neutral,neutral,neutral,neutral,neutral
541104549,I cannot reproduce it with my environment. Could you start a new conda environment? ,reproduce environment could start new environment,issue,negative,positive,positive,positive,positive,positive
541102704,The Berkeley EECS server has been down temporarily due to fire danger. Please find alternative download links from Kaggle in #785.,server temporarily due fire danger please find alternative link,issue,negative,negative,negative,negative,negative,negative
540957093,another Issue may be help: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/785,another issue may help,issue,negative,neutral,neutral,neutral,neutral,neutral
540956305,"I found the server has been shutdown temporarily from Oct. 9 due to fire danger.
![image](https://user-images.githubusercontent.com/23192817/66633992-b4629d80-ec3e-11e9-88a9-296b1d0c5957.png)
![image](https://user-images.githubusercontent.com/23192817/66633964-9c8b1980-ec3e-11e9-827d-b127baf65c29.png)
",found server shutdown temporarily due fire danger image image,issue,negative,negative,negative,negative,negative,negative
540642497,"1. Your flags look good to me. It should use three GPUs. Maybe you also want to increase `--num_threads` to 12.
2. The current code only supports one generator. You need to modify the code by yourself. See this [template](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/template_model.py) on how to implement your own model. ",look good use three maybe also want increase current code one generator need modify code see template implement model,issue,positive,positive,positive,positive,positive,positive
540638411,"> It seems that the Berkeley EECS server is done. It might take a few hours to 1-2 days to get the server back.

Too bad, but thanks for the confirmation!",server done might take day get server back bad thanks confirmation,issue,negative,negative,negative,negative,negative,negative
540634525,It seems that the Berkeley EECS server is done. It might take a few hours to 1-2 days to get the server back.,server done might take day get server back,issue,negative,neutral,neutral,neutral,neutral,neutral
540633895,"I haven't encountered this issue before. It takes some time to get the first visualization results. Regarding `Mathzoom`, this [post](https://github.com/facebookresearch/visdom/issues/629) might be relevant. ",issue time get first visualization regarding post might relevant,issue,negative,positive,positive,positive,positive,positive
540619359,"


> You can doanload the dataset by searching in kaggle dataset.

Noted with many thanks! Is there also an alternative place to download the pre-trained models?",searching noted many thanks also alternative place,issue,negative,positive,positive,positive,positive,positive
540291318,"Hi, @junyanz . I follow your steps, start the visdom server before starting the training:
python -m visdom.server , and create a second terminal and type the training command. But  I still see a blue screen in localhost:8097, nothing in the screen, just an message ""file failed to load:/extension/MathZoom.js"" and  ""file failed to load:/extension/MathMenu.js"" in the lower left corner.  Could you give me some advice? thanks a lot!",hi follow start server starting training python create second terminal type training command still see blue screen nothing screen message file load file load lower left corner could give advice thanks lot,issue,negative,positive,neutral,neutral,positive,positive
539101991,(1) The batchnorm calculates the statistics over the entire dataset rather than an individual image. Therefore the color of each image is still preserved. (2) You can try it. We haven't tried it before. (3) I am not sure if it is related to normalization. Please see the [discussion](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/190) on checkboard artifacts for more details.,statistic entire rather individual image therefore color image still try tried sure related normalization please see discussion,issue,positive,positive,positive,positive,positive,positive
538902490,"@John1231983 , not really. I didn't do some quantitative evaluation (like inception score for example), but just visually looking at them, they are as good as the images trained with fp32. However, if the images become too big (thousands of pixels on both direction), then the results are not that good, but that is a matter of network architecture, not mixed precision. If you want big images, you should consider using something like progressive GAN types of architecture.

Also, I trained other nets for different problems with mixed precision (always using Apex), and it works like a charm.",really quantitative evaluation like inception score example visually looking good trained however become big direction good matter network architecture mixed precision want big consider something like progressive gan architecture also trained different mixed precision always apex work like charm,issue,positive,positive,positive,positive,positive,positive
538795848,"@TheRevanchist : Does it hurt performance when use mixed precision (using NVIDIA Apex)?
",hurt performance use mixed precision apex,issue,negative,neutral,neutral,neutral,neutral,neutral
538792482,GPUs will be much faster. The quality is not affected by the choice of GPU or CPU.,much faster quality affected choice,issue,negative,positive,positive,positive,positive,positive
538537477,"Great to know that. How about batch norm? Why I can still use batch norm in the first layer and the result does not reduce. 

Secondly, if I add 3x3 convolution (without instance norm) before the first layer, so Can I use instance norm in the first layer?

Thirdly, I used instance norm in cyclegan (the instance has been ignored in the first layer as original paper). I meet the rectangular artifact in the generated image (right side). Do you know how to solve my artifact? The artifact is disappeared when I used batchnorm but the whole performance of instance norm still better than batch norm
![Screenshot from 2019-10-04 15-59-45](https://user-images.githubusercontent.com/24875971/66236398-625ecc80-e6c0-11e9-9676-817e8c6b979c.png)
",great know batch norm still use batch norm first layer result reduce secondly add convolution without instance norm first layer use instance norm first layer thirdly used instance norm instance first layer original paper meet rectangular artifact image right side know solve artifact artifact used whole performance instance norm still better batch norm,issue,positive,positive,positive,positive,positive,positive
538497257,"If you use an instancenorm in the first layer, the color of the input image will be normalized and get ignored. For many applications, you may want to preserve the color of the input image. ",use first layer color input image get many may want preserve color input image,issue,negative,positive,positive,positive,positive,positive
537703366,"> @AllAwake Cool results!
> 
> Here is the implementation of resize-conv I used. It remove the checkerboard artifacts during early training. You may find it useful.
> 
> ```
>                           nn.Upsample(scale_factor = 2, mode='bilinear'),
>                           nn.ReflectionPad2d(1),
>                           nn.Conv2d(ngf * mult, int(ngf * mult / 2),
>                                              kernel_size=3, stride=1, padding=0),
> ```
> 
> It should replace the `ConvTranspose2d` in `ResnetGenerator`.

This method works pretty well, but it would also cause unstable training, therefore I applied an exponential learning rate decay and small initial learning rate to ensure that it works well enough. I am currently doing another training and see what will happen then",cool implementation used remove checkerboard early training may find useful mult mult replace method work pretty well would also cause unstable training therefore applied exponential learning rate decay small initial learning rate ensure work well enough currently another training see happen,issue,positive,positive,neutral,neutral,positive,positive
536759009,@muxgt Thanks dude. You saved me a lot of time.,thanks dude saved lot time,issue,positive,positive,positive,positive,positive,positive
536749352,"@kalai2033 you should add --continue_train flag. You can read it [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#fine-tuningresume-training)
I changed refreshing speed in html file [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/visualizer.py#L165). 1 means 1 second. And to see pictures you need to download them along with the file so they are in the same folder.

",add flag read refreshing speed file second see need along file folder,issue,negative,positive,positive,positive,positive,positive
536742856,@muxgt Thanks for a quick reply. Should i include --epoch_count flag for resuming the training. or shoud i just run the same train script. And one more doubt when i open the index.html file in my browser the page is always refreshing and it doesnt show any pics. Should i wait till the training is stopped and then open?,thanks quick reply include flag training run train script one doubt open file browser page always refreshing doesnt show wait till training stopped open,issue,negative,positive,positive,positive,positive,positive
536739043,"@kalai2033 to resume training you just need to copy checkpoint files to your google drive for example. And then upload them in the new Colab runtime in the same folder where they were. And regarding visualization I don't know how to do it with visdom, instead I just downloaded html file and the pictures from time to time and opened it on local disk.",resume training need copy drive example new folder regarding visualization know instead file time time local disk,issue,negative,positive,neutral,neutral,positive,positive
536733882,"@muxgt I was also not getting any files in the default checkpoint directory. But the files were created when i used a different path to checkpoint directory. Also, Could you please let me know how did you stop and resume training as colab workspace gets reset in 12 hours. And also how did you visualize the progress with visdom server. Thanks in advance.",also getting default directory used different path directory also could please let know stop resume training reset also visualize progress server thanks advance,issue,positive,positive,neutral,neutral,positive,positive
536286120,"> Sure. You might check out this recent [work](https://arxiv.org/pdf/1801.08624.pdf).

Thank you~^_^",sure might check recent work thank,issue,positive,positive,positive,positive,positive,positive
536262723,"How do you solve this question. I encounter the same question
",solve question encounter question,issue,negative,neutral,neutral,neutral,neutral,neutral
536020290,Sure. You might check out this recent [work](https://arxiv.org/pdf/1801.08624.pdf). ,sure might check recent work,issue,negative,positive,positive,positive,positive,positive
535644107,Is your output one-channel image? It should not have a chromatic color?,output image chromatic color,issue,negative,neutral,neutral,neutral,neutral,neutral
535566348,I also have this problem. There are multi-color spots in the generated images. Could you tell me the solution?,also problem could tell solution,issue,negative,neutral,neutral,neutral,neutral,neutral
535355639,"> Yes. But the result becomes blurring. It may be trade-off. Let me know if you also have same issue

Yes, I also used the nearest neighbor up-sampling + convolution to replace up-sampling. Although it can overcome the checkerboard problem, but the overall brightness of the image is not uniform, coutour will appear.",yes result becomes may let know also issue yes also used nearest neighbor convolution replace although overcome checkerboard problem overall brightness image uniform appear,issue,positive,neutral,neutral,neutral,neutral,neutral
535331597,Yes. But the result becomes blurring. It may be trade-off. Let me know if you also have same issue,yes result becomes may let know also issue,issue,negative,neutral,neutral,neutral,neutral,neutral
535305386,"> Thanks. I solved it by using NN upsampling. The deconv does not help when we train longer. When I train the network with more epoch and learning rate, I found that the checkboard artifact will appear more. Does it mean the model overfitting or collapse? How to know the model collapse? Thanks

Do you use the nearest neighbor up-sampling + convolution to replace deconv ?",thanks help train longer train network epoch learning rate found artifact appear mean model collapse know model collapse thanks use nearest neighbor convolution replace,issue,negative,positive,neutral,neutral,positive,positive
535155730,"Your FOR loop only works when A and B have the same number of images. Please look at unaligned_dataset [dataloader](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py). See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L52).

",loop work number please look see line,issue,negative,neutral,neutral,neutral,neutral,neutral
534848831,"@junyanz I try the same sum of dataa and datab ,it was no error. How i can fix it to use the different sum of dataa and datab.Do you have some advice? thank you",try sum error fix use different sum advice thank,issue,negative,neutral,neutral,neutral,neutral,neutral
534845802,"@junyanz Hi i found my data, the sum of domain A(after bs) and domain B(after bs)was different.
len(dataa)=128  len(datab)=172,i use blew.
        for data1,data2 in zip(dataa,datab):  # inner loop within one epoch
            iter_start_time = time.time()  # timer for computation per iteration
            if total_iters % opt.print_freq == 0:
                t_data = iter_start_time - iter_data_time
            visualizer.reset()
            total_iters += opt.batch_size
            epoch_iter += opt.batch_size
            # data1=next(a_iter)
            # data2=next(b_iter)
            model.set_input(data1,data2)   
Maybe is this problem.But How can i use different sum A and B to train.",hi found data sum domain domain different use data data zip inner loop within one epoch timer computation per iteration data data maybe use different sum train,issue,negative,neutral,neutral,neutral,neutral,neutral
534839217,"@junyanz I can finish training, I try epoch200.",finish training try epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
534823582,"@junyanz I use my own dataload.py, transfer them to set_input, my data was 256*256 without the path. Below was changed cycle_gan_model.py set_input:
    def set_input(self, inputa,inputb):
        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.
        Parameters:
            input (dict): include the data itself and its metadata information.
        The option 'direction' can be used to swap domain A and domain B.
        """"""
        AtoB = self.opt.direction == 'AtoB'
        # self.real_A = input['A' if AtoB else 'B'].to(self.device)
        # self.real_B = input['B' if AtoB else 'A'].to(self.device)
        # self.image_paths = input['A_paths' if AtoB else 'B_paths']
        self.real_A = inputa['image'].to(self.device)
        # print(inputa['image'].shape)
        self.real_B = inputb['image'].to(self.device)
",use transfer data without path self unpack input data perform necessary input include data information option used swap domain domain input else input else input else print,issue,negative,neutral,neutral,neutral,neutral,neutral
534621780,I am not sure. It might be caused (1) a corrupt image file (2) Your image size is not compatible with the network. Note that pix2pix only supports 256x256 input. ,sure might corrupt image file image size compatible network note input,issue,negative,neutral,neutral,neutral,neutral,neutral
534616481,You can use a higher learning rate with larger batch size. More recent [work](https://openreview.net/pdf?id=B1Yy1BxCZ) to read. ,use higher learning rate batch size recent work read,issue,negative,positive,positive,positive,positive,positive
533939431,Thank you very much for your professional answer. I think my problem has been solved with your help. ,thank much professional answer think problem help,issue,negative,positive,neutral,neutral,positive,positive
533921006,GANs evaluation is an open problem. You can either evaluate your model on your downstream task or application (recommended) or using some standard metrics like [FID](https://github.com/mseitzer/pytorch-fid). ,evaluation open problem either evaluate model downstream task application standard metric like fid,issue,negative,neutral,neutral,neutral,neutral,neutral
533919558,"Yes, I used the default setting. ",yes used default setting,issue,negative,neutral,neutral,neutral,neutral,neutral
533851455,"Looks like I was feeding it the wrong model instance, the issue has been fixed after inputting the generator network 

This worked for me:
```
dummy_input = torch.randn(1, 3, 256, 256)
torch.onnx.export(model.netG, dummy_input, ""./cycleGAN.onnx"")
```
I'll be testing the model now.
",like feeding wrong model instance issue fixed generator network worked testing model,issue,negative,negative,negative,negative,negative,negative
533841459,"Thanks for your answer. But I wonder without the loss curve as indicator, what can I refer to stop the training? ",thanks answer wonder without loss curve indicator refer stop training,issue,negative,positive,positive,positive,positive,positive
533836236,Hi @junyanz ! Changing the crop size to 256 solved the issue. Many thanks for your reply.,hi crop size issue many thanks reply,issue,negative,positive,positive,positive,positive,positive
533834734,"The default pix2pix generator only supports 256x256. However, your crop_size is 128. You can try three things (1)  increase your crop_size to 256, (2) remove 1-2 layers from the default pix2pix generator, (3) use a resnet generator such as resnet_9blocks or resnet_6blocks.",default generator however try three increase remove default generator use generator,issue,negative,neutral,neutral,neutral,neutral,neutral
533834620,"Your running time looks normal. Your loss curves look fine. The LSGAN/DCGAN losses often oscillate and will not converge.  To use multiple GPUs, you need to increase the batch size `--batch_size`.
As you have 22000 images, you probably only need 20 to 50 epochs. ",running time normal loss look fine often oscillate converge use multiple need increase batch size probably need,issue,negative,positive,positive,positive,positive,positive
533834400,"Both 1 and 3 might work better. Also, you don't need to train the model for 200 epochs. Maybe 10 or 50 epochs will be enough. ",might work better also need train model maybe enough,issue,negative,positive,positive,positive,positive,positive
533760070,"The loss hasn't converged, but the quality of output image is getting rising while training.",loss quality output image getting rising training,issue,negative,neutral,neutral,neutral,neutral,neutral
533759883,"Thanks a lot for your reply. The logging information is
epoch: 19, iters: 11932, time: 0.347, data: 0.001
epoch: 19, iters: 12032, time: 0.533, data: 0.001
...
I use 1080Ti GPU for training. And I tried to set _--gpu_ids '0,1'_. But it seems it still run only on gpu '0'. I cannot figure out the reason.
The loss curve is shown as follow:
![loss](https://user-images.githubusercontent.com/34597963/65366612-de84f900-dc58-11e9-96c0-4e6c17829736.png)
It seems the curve has no obviously changes.

",thanks lot reply logging information epoch time data epoch time data use ti training tried set still run figure reason loss curve shown follow loss curve obviously,issue,negative,positive,neutral,neutral,positive,positive
533615216,It is a little bit slow. Which GPU are you using? How fast is the data loading time (you can read it from the logging information). Consider using multiple faster GPUs or increase `--num_threads` (if data loading is the issue). ,little bit slow fast data loading time read logging information consider multiple faster increase data loading issue,issue,negative,negative,neutral,neutral,negative,negative
533611227,"It looks fine to me. As we are using LSGAN loss, the G and D losses are supposed to oscillate.  ",fine loss supposed oscillate,issue,negative,positive,positive,positive,positive,positive
533420343,It is fine as long as D doesn't go to 0. It's normal for  G and D to oscillate. ,fine long go normal oscillate,issue,negative,positive,positive,positive,positive,positive
533362961,"> @junyanz I waited 6 days using GPU. but the model epoch still in 6.
> What should i do?
> 
> (epoch: 6, iters: 78100, time: 0.524, data: 0.002) D_A: 0.000 G_A: 1.000 cycle_A: 6.530 idt_A: 4.588 D_B: 0.001 G_B: 1.060 cycle_B: 9.175 idt_B: 0.596
> (epoch: 6, iters: 78200, time: 0.899, data: 0.002) D_A: 0.000 G_A: 1.002 cycle_A: 5.349 idt_A: 3.873 D_B: 0.001 G_B: 1.003 cycle_B: 7.746 idt_B: 0.355
> (epoch: 6, iters: 78300, time: 0.530, data: 0.002) D_A: 0.000 G_A: 1.000 cycle_A: 4.084 idt_A: 4.294 D_B: 0.003 G_B: 0.941 cycle_B: 8.588 idt_B: 1.107
> (epoch: 6, iters: 78400, time: 0.513, data: 0.002) D_A: 0.000 G_A: 1.000 cycle_A: 7.437 idt_A: 4.732 D_B: 0.001 G_B: 0.956 cycle_B: 9.463 idt_B: 1.087
> (epoch: 6, iters: 78500, time: 0.534, data: 0.002) D_A: 0.000 G_A: 0.998 cycle_A: 5.723 idt_A: 5.124 D_B: 0.000 G_B: 0.982 cycle_B: 10.249 idt_B: 0.757
> (epoch: 6, iters: 78600, time: 0.937, data: 0.002) D_A: 0.000 G_A: 1.005 cycle_A: 6.363 idt_A: 5.203 D_B: 0.001 G_B: 0.971 cycle_B: 10.405 idt_B: 0.391
> (epoch: 6, iters: 78700, time: 0.521, data: 0.002) D_A: 0.000 G_A: 1.004 cycle_A: 3.957 idt_A: 5.325 D_B: 0.001 G_B: 0.994 cycle_B: 10.650 idt_B: 0.473
> (epoch: 6, iters: 78800, time: 0.543, data: 0.002) D_A: 0.000 G_A: 1.002 cycle_A: 6.084 idt_A: 4.655 D_B: 0.001 G_B: 1.057 cycle_B: 9.310 idt_B: 0.373
> (epoch: 6, iters: 78900, time: 0.523, data: 0.002) D_A: 0.000 G_A: 1.001 cycle_A: 4.752 idt_A: 5.088 D_B: 0.000 G_B: 0.986 cycle_B: 10.175 idt_B: 0.343
> (epoch: 6, iters: 79000, time: 0.924, data: 0.002) D_A: 0.000 G_A: 0.997 cycle_A: 5.851 idt_A: 5.049 D_B: 0.000 G_B: 1.012 cycle_B: 10.098 idt_B: 0.361
> (epoch: 6, iters: 79100, time: 0.513, data: 0.002) D_A: 0.000 G_A: 1.006 cycle_A: 5.204 idt_A: 5.140 D_B: 0.001 G_B: 1.000 cycle_B: 10.279 idt_B: 0.976
> 
> Training is not finished...

Have you solved the problem? My code also runs slowly. But I haven't figured out the reason.",day model epoch still epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data training finished problem code also slowly figured reason,issue,negative,negative,negative,negative,negative,negative
533266855,You should do early stopping or reduce the learning rate. ,early stopping reduce learning rate,issue,negative,positive,neutral,neutral,positive,positive
533245455,"@junyanz : it still worked but my problem is that when I train the model with more epochs, the performance reduces. Should I use the early stopping?  Note that, all loss do not increase if I train with more epoch, but the accuracy is reduced",still worked problem train model performance use early stopping note loss increase train epoch accuracy reduced,issue,negative,positive,neutral,neutral,positive,positive
533219702,"I am glad that you figured it out. You can evaluate your model on a hold-out validation set, and see if it still works.",glad figured evaluate model validation set see still work,issue,negative,positive,positive,positive,positive,positive
533178890,"> [Here](https://github.com/eriklindernoren/Keras-GAN/blob/44d3320e84ca00071de8a5c0fb4566d10486bb1d/cyclegan/cyclegan.py#L186), dB_loss_fake[1] contains the 'accuracy of prediction' for fake images.

Did you change the code in base keras '_train_on_batch_' to get the accuracy of prediction? Or just give it a parameter (_sample_weight=0.5_). I am not really understand how it operated.",prediction fake change code base get accuracy prediction give parameter really understand,issue,negative,negative,negative,negative,negative,negative
532930381,"Thanks. I solved it by using NN upsampling. The deconv does not help when we train longer.  When I train the network with more epoch and learning rate, I found that the checkboard artifact will appear more. Does it mean the model overfitting or collapse? How to know the model collapse? Thanks",thanks help train longer train network epoch learning rate found artifact appear mean model collapse know model collapse thanks,issue,negative,positive,neutral,neutral,positive,positive
532907656,"Another solution to this is to modify `net.load_state_dict(state_dict, strict=False)` where I added the `strict=False` option. This allows the network to load weights as long as the sizes and the number of parameters fit, even if the key-names aren't exact. ",another solution modify added option network load long size number fit even exact,issue,positive,positive,positive,positive,positive,positive
532511651,I cannot remember the parameters exactly. I will ask @taesungp to confirm it.,remember exactly ask confirm,issue,negative,positive,positive,positive,positive,positive
532279968,The generator and the discriminator used in super-resolution might be different from cyclegan networks. See some prior [work](http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w13/Yuan_Unsupervised_Image_Super-Resolution_CVPR_2018_paper.pdf) on using cyclegan for super-resolution tasks. I will start from that and then try to improve it. You may also borrow the network architecture from other recent super-resolution papers.,generator discriminator used might different see prior work start try improve may also borrow network architecture recent,issue,negative,neutral,neutral,neutral,neutral,neutral
532274858,The default of `max_dataset_size` is inf. But you can set a smaller number as you like.,default set smaller number like,issue,negative,neutral,neutral,neutral,neutral,neutral
532109984,in the code there is a use of opt.max_dataset_size that is set to Inf. So any number i use won't do any change i think,code use set number use wo change think,issue,negative,neutral,neutral,neutral,neutral,neutral
532109223,Thanks i will try it.you see i am thinking of a way instead of each epoch reload all 10GB if there is a way to have it loaded from the start and just accessing it.I believe after the first slow one epoch later on will be loading faster,thanks try see thinking way instead epoch reload way loaded start believe first slow one epoch later loading faster,issue,negative,positive,neutral,neutral,positive,positive
531910818,"It's possible. You can modify the initialization code of the data loader [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L29). You can load all the images in the beginning. I am not sure if it is necessary, as the data loading is already quite efficient. You can increase the `--num_threads`. You can see the time of data loading from the logging information. ",possible modify code data loader load beginning sure necessary data loading already quite efficient increase see time data loading logging information,issue,positive,positive,positive,positive,positive,positive
531908104,"I am not sure if pix2pix is the best option for you, as we assume that input images and output images are spatially aligned, which is not the case of your application. I recommend that you use other models such as [STN](https://arxiv.org/abs/1506.02025).",sure best option assume input output spatially case application recommend use,issue,positive,positive,positive,positive,positive,positive
531747500,can u show us your training dataset sample?,show u training sample,issue,negative,neutral,neutral,neutral,neutral,neutral
531745337,"please check   `project folder/results/""name of your experiment""/test_latest/ `, in this folder u will find images folder and index.html, u can find all the generated images and the real images in the images folder or open index.html for better visualization.
 ",please check project name experiment folder find folder find real folder open better visualization,issue,positive,positive,positive,positive,positive,positive
531662162,"how to overcome this error
runfile('C:/Users/AJAY/Desktop/AV MATCHING/lib tracking.py', wdir='C:/Users/AJAY/Desktop/AV MATCHING')
usage: lib tracking.py [-h] -i INPUT -o OUTPUT [-f FPS] [-c CODEC]
lib tracking.py: error: the following arguments are required: -i/--input, -o/--output
An exception has occurred, use %tb to see the full traceback.

SystemExit: 2",overcome error matching usage input output error following input output exception use see full,issue,negative,positive,positive,positive,positive,positive
531530625,"Thank you for replying.This is the trace.





------------------ 原始邮件 ------------------
发件人: ""Jun-Yan Zhu""<notifications@github.com>; 
发送时间: 2019年9月14日(星期六) 晚上11:10
收件人: ""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com>; 
抄送: ""tpy""<724754188@qq.com>; ""Author""<author@noreply.github.com>; 
主题: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] ValueError: num_samplesshould be a positive integeral value, but got num_samples=0 (#767)




We don't have num_samples in this repo. Would it be possible to share with us the trace of the program?
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.",thank trace author author positive value got would possible share u trace program thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
531487246,"We don't have `num_samples` in this repo. Would it be possible to share with us the trace of the program?
",would possible share u trace program,issue,negative,neutral,neutral,neutral,neutral,neutral
531268424,We haven't observed significant improvement. But you are free to try it by yourself.,significant improvement free try,issue,positive,positive,positive,positive,positive,positive
531073545,"alright, I get it. thanks very much.",alright get thanks much,issue,positive,positive,positive,positive,positive,positive
531072023,"let me tell you. The role of detach is to freeze the gradient drop. Whether it is for discriminating the network or generating the network, we update all about logD(G(z)). For the discriminant network, freezing G does not affect the overall gradient update (that is The inner function is considered to be a constant, which does not affect the outer function to find the gradient), but conversely, if D is frozen, there is no way to complete the gradient update. Therefore, we did not use the gradient of freezing D when training the generator. So, for the generator, we did calculate the gradient of D, but we didn't update the weight of D (only optimizer_g.step was written), so the discriminator will not be changed when the generator is trained. You may ask, that's why, when you train the discriminator, you need to add detach. Isn't this an extra move?
Because we freeze the gradient, we can speed up the training, so we can use it where it can be used. It is not an extra task. Then when we train the generator, because of logD(G(z)), there is no way to freeze the gradient of D, so we will not write detach here.",let tell role detach freeze gradient drop whether discriminating network generating network update discriminant network freezing affect overall gradient update inner function considered constant affect outer function find gradient conversely frozen way complete gradient update therefore use gradient freezing training generator generator calculate gradient update weight written discriminator generator trained may ask train discriminator need add detach extra move freeze gradient speed training use used extra task train generator way freeze gradient write detach,issue,negative,positive,neutral,neutral,positive,positive
530955366,"@junyanz : Thanks. Have you try Hinge loss in the cyclegan ?

https://github.com/NVlabs/SPADE/blob/0ff661e70131c9b85091d11a66e019c0f2062d4c/models/networks/loss.py#L66",thanks try hinge loss,issue,negative,positive,positive,positive,positive,positive
530953287,I don't know why. Did it also happen to the example training [script](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#pix2pix-traintest)?,know also happen example training script,issue,negative,neutral,neutral,neutral,neutral,neutral
530950174,We used the Hinge loss and spectral norm in our recent paired image-to-image translation project [GauGAN](https://github.com/NVlabs/SPADE).,used hinge loss spectral norm recent paired translation project,issue,negative,neutral,neutral,neutral,neutral,neutral
530793309,@wl082013: any improvement do you achieve using the new gan? My performance down when using it. ,improvement achieve new gan performance,issue,negative,positive,positive,positive,positive,positive
530775047,@jbmaxwell have you managed to resolve this issue? Interested to see how you did it if you're willing to share it.,resolve issue interested see willing share,issue,positive,positive,positive,positive,positive,positive
530654898,Never mind. I have increased the number of workers. Now it is taking around 1 hour for epoch. Closing the issue. Thanks for the reply junyanz.,never mind number taking around hour epoch issue thanks reply,issue,negative,positive,positive,positive,positive,positive
530609708,"Hi. Should I use patchgan for pix2pix? I have image size of 128x128, so it is clear that cyclegan will use nlayer as 5 (output is 4x4 patch). But for pix2pix, is nlayer 5 or 7?",hi use image size clear use output patch,issue,negative,positive,positive,positive,positive,positive
530443898,It is a little bit slow. What is your GPU usage percentage? Is it 100%?  What is the time for (1) data loading and (2) computation?,little bit slow usage percentage time data loading computation,issue,negative,negative,negative,negative,negative,negative
530443064,It's possible. You can try different GAN objective functions. It can be used for the unpaired case as the original unconditional GANs are trained without pairs.,possible try different gan objective used unpaired case original unconditional trained without,issue,negative,positive,neutral,neutral,positive,positive
530141271,The code is [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/visualizer.py#L156). Feel free to modify it.,code feel free modify,issue,positive,positive,positive,positive,positive,positive
530088031,I found a way to visualize the process but got another question. Is there a way to visualize more than one picture in each epoch? Where can I find this in the code?,found way visualize process got another question way visualize one picture epoch find code,issue,negative,neutral,neutral,neutral,neutral,neutral
530041346,I am not sure about your application and data. 90 images might be not enough. Consider applying some data augmentation and be cautious about overfitting.,sure application data might enough consider data augmentation cautious,issue,negative,positive,positive,positive,positive,positive
530039994,"During training time, we crop 256x256 patches from 600x600 images. This will allow us to run the model on 600x600 images during test time, as the network is fully convolutional. On the contrary, making the map image 256x256 during training time and test time will reduce the resolution of the results. See this [Q & A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#out-of-memory-174) for more details. ",training time crop allow u run model test time network fully convolutional contrary making map image training time test time reduce resolution see,issue,negative,neutral,neutral,neutral,neutral,neutral
529702392,"It is probably not possible to do it in such high resolution. You can separately translate cropped windows of the original image and combine them together, but then you will have to deal with boundaries. Plus the side length needs to be roughly a multiple of 2^(num_downsampling). Otherwise the output image is not exactly same size as original, but slightly different. But this may not be a big problem because you can resize the output by a little bit. ",probably possible high resolution separately translate original image combine together deal plus side length need roughly multiple otherwise output image exactly size original slightly different may big problem resize output little bit,issue,negative,positive,positive,positive,positive,positive
529679704,"The code can work on images with different sizes. You need to use a ResNet generator (either `--netG resnet_6blocks` or `--netG resnet_9blocks`), and specify a preprocessing option (either `--preprocess none` or `--preprocess  scale_width`). However, I am not sure if the current code can handle such high-resolution images due to the limitation of the GPU memory.",code work different size need use generator either specify option either none however sure current code handle due limitation memory,issue,negative,positive,neutral,neutral,positive,positive
529133476,"Thanks for your nice words. We have a section called [related projects](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#related-projects),  where you may find some relevant projects.",thanks nice section related may find relevant,issue,positive,positive,positive,positive,positive,positive
528930542,Image size is the same but not sure about network size. Where can I see it?,image size sure network size see,issue,negative,positive,positive,positive,positive,positive
528899242,I haven't used that TensorFlow version. Is the network's size and the image size the same? @SsnL ,used version network size image size,issue,negative,neutral,neutral,neutral,neutral,neutral
528898833,256x256 patches might be too small compared to the original images. You also may want to crop the center region from real_B. It's hard to synthesize head and limbs from real_A.,might small original also may want crop center region hard synthesize head,issue,negative,negative,neutral,neutral,negative,negative
528415362,"As a reference real_A looks like the following:

![Screenshot 2019-09-05 at 17 03 02](https://user-images.githubusercontent.com/6440096/64354246-57e8de80-cfff-11e9-8909-2348723ed1a2.png)

real_B:

![Screenshot 2019-09-05 at 17 04 01](https://user-images.githubusercontent.com/6440096/64354264-63d4a080-cfff-11e9-88ff-c667b9ee1ba9.png)

Those two examples are slices from the 3d volume, which are then processed into 256x256 image patches.",reference like following two volume image,issue,negative,neutral,neutral,neutral,neutral,neutral
528393562,"Could you share with us images for `real_B`? If you want to have bigger changes, you can try to set `--lambda_identity 0`.",could share u want bigger try set,issue,negative,neutral,neutral,neutral,neutral,neutral
528392001,"It depends on the SR generator. Some SR generators support images with different sizes. Regarding data loader, you probably need to write your own data loader. You can modify the aligned data [loader](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/aligned_dataset.py).",generator support different size regarding data loader probably need write data loader modify data loader,issue,negative,neutral,neutral,neutral,neutral,neutral
528275784,"It depends on the type of your dataset. For example in my experiment I used `--dataset_mode unaligned`, so I've changed the next few lines from [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L57) to something like:

```
        # A_img = Image.open(A_path).convert('RGB')
        # B_img = Image.open(B_path).convert('RGB')
        # apply image transformation
        # A = self.transform_A(A_img)
        # B = self.transform_B(B_img)
        A = np.array([np.load(A_path)['data']]).astype(np.float32)
        B = np.array([np.load(B_path)['data']]).astype(np.float32)	

        return {'A': A, 'B': B, 'A_paths': A_path, 'B_paths': B_path}
``` ",type example experiment used unaligned next something like apply image transformation return,issue,negative,neutral,neutral,neutral,neutral,neutral
528272488,"Thanks for your reply. My input is 512*512 in npy and what if I want to save output in npy file too? Which file shall I change?
",thanks reply input want save output file file shall change,issue,positive,positive,positive,positive,positive,positive
528271698,"I also have this problem
`RuntimeError: cuda runtime error (11) : invalid argument at /opt/conda/conda-bld/pytorch_1544084119927/work/aten/src/THC/THCGeneral.cpp:405`
",also problem error invalid argument,issue,negative,neutral,neutral,neutral,neutral,neutral
528265277,"I did it for my project, where my input data are npz files. You just have to change the data loader to use `np.load(PATH_TO_FILE)` instead of `Image.open(...)`.",project input data change data loader use instead,issue,negative,neutral,neutral,neutral,neutral,neutral
528149001,"Okay, how about the image size? since cyclegan fixed all image size, but  my SR dataset is with various image size, so can I just change the image size ? besides, is it possible to load trainA(LR) and trainB(HR) in a paired format using your unaligned dataloader? e.g. trainA: 1.png <-> trainB 1.png like this. 
Thanks for your kind help.
",image size since fixed image size various image size change image size besides possible load paired format unaligned like thanks kind help,issue,positive,positive,positive,positive,positive,positive
527940540,"It's possible. You can replace the current generator with an SR generator (e.g., this [one](https://github.com/pytorch/examples/blob/master/super_resolution/model.py#L6)).",possible replace current generator generator one,issue,negative,neutral,neutral,neutral,neutral,neutral
527939312,There is no specific reason. We mostly follow the [practice](https://github.com/pytorch/examples/blob/master/dcgan/main.py#L142) of prior work. ,specific reason mostly follow practice prior work,issue,negative,positive,positive,positive,positive,positive
527938667,The current repo doesn't support 3D input/output. You need to implement your own 3D generator and discriminator.,current support need implement generator discriminator,issue,negative,neutral,neutral,neutral,neutral,neutral
527611686,"Thanks. I tested and It also worked on [0,1]. Just one more question, why do you choose the range [-1,1] insted of [0,1]. Do you have any reason or benefit for the range?",thanks tested also worked one question choose range reason benefit range,issue,positive,positive,positive,positive,positive,positive
527610452,"The range of input/output in CycleGAN is also [-1, 1]. You can change tanh to sigmoid. But you also need to change the preprocessing code and post-processing code. They all assume that the range is [-1, 1].",range also change tanh sigmoid also need change code code assume range,issue,negative,neutral,neutral,neutral,neutral,neutral
527608884,"Yes. I forgot to mention that you need to add `--no_drouput` in your test script. It is added automatically in the CycleGAN training code [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/08af6e49389fe35a0d1c97f8e3b0b68df3535d58/models/cycle_gan_model.py#L39).
It is mentioned in the [README](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan). ",yes forgot mention need add test script added automatically training code,issue,negative,neutral,neutral,neutral,neutral,neutral
527599622,"I cannot find the bug. But again, if you first run `test.py` and then gradually change the code, it will be easier for debugging. I am not sure if your input is in the correct range or have gone through the correct preprocessing.",find bug first run gradually change code easier sure input correct range gone correct,issue,positive,positive,positive,positive,positive,positive
527598662,reset_9blocks might be enough. But you can try 12. I used 200 epochs for most of the experiments. But you can try different epochs.,might enough try used try different,issue,negative,neutral,neutral,neutral,neutral,neutral
527598126,The current generator and discriminator do not work for 3D data. You need to use 3D [convolutional](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv3d) layers for 3D data. I recommend that you implement your own networks.,current generator discriminator work data need use convolutional data recommend implement,issue,negative,neutral,neutral,neutral,neutral,neutral
527597418,I will leave @taesungp for code review. Would it be better to add spectral norm as an option for `--norm` rather than add new generators and discriminators?,leave code review would better add spectral norm option norm rather add new,issue,negative,positive,positive,positive,positive,positive
527550064,"Also according to this http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/
transposed convolution can be initialized with bilinear filter. ",also according convolution bilinear filter,issue,negative,neutral,neutral,neutral,neutral,neutral
527546060,"No problem, I realized that this is not the same as the cycle loss, I wasn't aware of an identity requirement, probably it's in the paper, will have a look.",problem cycle loss aware identity requirement probably paper look,issue,negative,positive,positive,positive,positive,positive
527384875,"Hey @junyanz,

Could you please specify where did you add noises? I cannot find it where is the noise in U-net? 

Thanks
",hey could please specify add find noise thanks,issue,positive,positive,positive,positive,positive,positive
526915062,"> I changed line 199 of base_model.py to:
> 
> `net.load_state_dict(state_dict, strict=False)`
> 
> Then it worked. But I am not sure whether this can lead to problems with other types of nets.

Ok, this was not the way to go. It works, but the generated images look nothing like the ones created during the training.

As it turns out, when the cycleGAN is created [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/08af6e49389fe35a0d1c97f8e3b0b68df3535d58/models/test_model.py#L45)  as testmodel object in testmodel.py, that there is a dropout layer generated because the opt.no_dropout flag is negated.
So even if you use the same switches for dropout in training and test, during loading one of the nets there will be errors.

I took away the negation. Then testing or ""applying"" the generator worked and the images looked like the ones during training.

[G_A_test.txt](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/files/3563452/G_A_test.txt)
[G_A_train.txt](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/files/3563453/G_A_train.txt)",line worked sure whether lead way go work look nothing like training turn object dropout layer flag even use dropout training test loading one took away negation testing generator worked like training,issue,negative,positive,positive,positive,positive,positive
526850344,"Thanks for your response. As you see if we compare synthetic image and real depth image the translation task is not just changing the color of the image there are also many small details (noise) in real image, which are missing in the synthetic image. And generally its shown that by increasing the number of resnet layers the performance can be increased. Therefore I am planning to extend the reset_9blocks to twelve. Would this be a good idea?
And I have 500images from domain A and 500 hundred from domain B. How much epochs should I train ? Generally you used 200 epochs should I train for more epochs ?

Thanks and best regards",thanks response see compare synthetic image real depth image translation task color image also many small noise real image missing synthetic image generally shown increasing number performance therefore extend twelve would good idea domain hundred domain much train generally used train thanks best,issue,positive,positive,positive,positive,positive,positive
526836895,"Ah yes, the dummy input (as the name suggests) is just a bogus input to let ONNX trace the network—it's a standard step in creating an ONNX version. I have a small set of test files that I ran through the ONNX model after conversion (and testing the mlmodel was done in the app itself, of course).
But I'm assuming then that there's nothing implicitly wrong, or missing, from my approach of just using `ResnetGenerator` directly?",ah yes dummy input name bogus input let trace standard step version small set test ran model conversion testing done course assuming nothing implicitly wrong missing approach directly,issue,positive,negative,negative,negative,negative,negative
526796016,"@junyanz : How about cyclegan range? Does it still work on [0,1] by replacing tanh() by sigmoid () function or even adding a scale (2*output-1) if you want to maintain the tanh()?",range still work tanh sigmoid function even scale want maintain tanh,issue,negative,neutral,neutral,neutral,neutral,neutral
526691310,"I haven't worked with ONNX. But since you are using a dummy input `torch.randn(1, 3, 64, 64)`, how do you know if the output is correct or not? I recommend that you start from the current test.py, change things gradually, one line each time, and see if the output is the same as the original `test.py`.",worked since dummy input know output correct recommend start current change gradually one line time see output original,issue,positive,positive,positive,positive,positive,positive
526599441,"I changed line 199 of base_model.py to:

`net.load_state_dict(state_dict, strict=False)`

Then it worked. But I am not sure whether this can lead to problems with other types of nets.",line worked sure whether lead,issue,negative,positive,positive,positive,positive,positive
526598855,"> Could you also make sure that you use the same normalization `--norm` during training and test?

I did not change this parameter. Should it be something in particular?",could also make sure use normalization norm training test change parameter something particular,issue,negative,positive,positive,positive,positive,positive
526597753,Could you also make sure that you use the same normalization `--norm` during training and test?,could also make sure use normalization norm training test,issue,negative,positive,positive,positive,positive,positive
526542499,"Hello,
same problem, same question... any development?",hello problem question development,issue,negative,neutral,neutral,neutral,neutral,neutral
526426644,Thanks a lot for your fast responses. I'll try it.,thanks lot fast try,issue,negative,positive,positive,positive,positive,positive
526274099,"See this [post](https://twitter.com/karpathy/status/720622989289644033?lang=en) on zero-padding for more details. Again, I think the network can be directly deployed on your test images without resizing and cropping. Please experiment with the flag `--preprocess none` or `--preprocess scale_width`.",see post think network directly test without please experiment flag none,issue,negative,positive,neutral,neutral,positive,positive
526190311,Somehow this issue disappears by itself the second day I tried it. ,somehow issue second day tried,issue,negative,neutral,neutral,neutral,neutral,neutral
526180311,"Thanks for your prompt response and cycleGAN is an excellent work. Hmm, in my work, I want to preserve the realness of test images (with different size) as possible, so I pad them to 512*512 and then crop the results according to real size instead of scaling or cropping before testing. Here are 4 simulation experiments during test. 

![result](https://user-images.githubusercontent.com/54628212/63942195-cb3e9d80-ca9f-11e9-8580-24e5ab64cf38.png)

In my opinion, EXP1,EXP3,EXP4 should have similar results because they have the same area.  I want to know your suggestions on this issue. I just saw in the comment that you said _it will not work_. Can you tell me the reason? 

Thanks a lot and look forward to your reply.",thanks prompt response excellent work work want preserve realness test different size possible pad crop according real size instead scaling testing simulation test result opinion similar area want know issue saw comment said tell reason thanks lot look forward reply,issue,positive,positive,positive,positive,positive,positive
525830748,You don't need to pad the test image. It will not work. You can just directly apply the trained models on test images. Set the flag `--preprocess none` or `--preprocess scale_width`(it should work if the width and height of your images are bigger than 256). See the [option](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L49) for more details.,need pad test image work directly apply trained test set flag none work width height bigger see option,issue,negative,positive,neutral,neutral,positive,positive
525569845,"之前epoch一直等于1，我就把trainA和trainB里面的图片减少了几百张（之前近万张），然后epoch就正常了。
对于训练数据集（trainA and trainB）,w我阅读了下面的、之前公布的
How can I make it work for my own data？ 
#309 320 202
The current code only supports RGB and grayscale images. If you would like to train the model on other data types, please follow the following steps:
change the parameters --input_nc and --output_nc to the number of channels in your input/output images.
Write your own custom data loader (It is easy as long as you know how to load your data with python).  If you write a new data loader class, you need to change the flag --dataset_mode accordingly.  
Alternatively, you can modify the existing data loader. For aligned datasets, change this line; For unaligned datasets, change these two lines.
If you use visdom and HTML to visualize the results, you may also need to change the visualization code.
trainA和trainB里面的数据图片一定要一样多？里面的数据图片有一定的数量规定吗？",make work current code would like train model data please follow following change number write custom data loader easy long know load data python write new data loader class need change flag accordingly alternatively modify data loader change line unaligned change two use visualize may also need change visualization code,issue,positive,positive,positive,positive,positive,positive
525340652,Here are several papers on multimodal unpaired image-to-image translation including [AugmentedCycleGAN](https://arxiv.org/abs/1802.10151) and [MUNIT](https://arxiv.org/abs/1804.04732). ,several multimodal unpaired translation,issue,negative,neutral,neutral,neutral,neutral,neutral
524950558,"Hi @cpretto, did you manage to get pix2pix converted? It would be great to have an example. I'm not sure how to do it with this library (I'm new to Python, and not used to dealing with argparse)... I'm not sure how to create a model and load the weights.",hi manage get converted would great example sure library new python used dealing sure create model load,issue,positive,positive,positive,positive,positive,positive
524893542,"I am not familiar with the codebase PyTorch-GAN. Maybe you can post the question in the original repo, or use this codebase.",familiar maybe post question original use,issue,negative,positive,positive,positive,positive,positive
524892676,Have you tried to run the model on the provided test set? Maybe you need to retrain the models on your own datasets. @taesungp ,tried run model provided test set maybe need retrain,issue,negative,neutral,neutral,neutral,neutral,neutral
524892059,"See our recent work such as [SPADE](https://arxiv.org/abs/1903.07291), where we used the spectral norm, the hinge loss, and different learning rates. See the `implementation details` section (Section 4) and appendix A of the paper.",see recent work spade used spectral norm hinge loss different learning see implementation section section appendix paper,issue,negative,neutral,neutral,neutral,neutral,neutral
524889972,I am not sure what happened. You can try to change the parameters such as `--iter` and `--iter_decay`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L30) for more details.,sure try change iter see,issue,negative,positive,positive,positive,positive,positive
524792186,Change `num_test` when running test script: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/test_options.py#L18,change running test script,issue,negative,neutral,neutral,neutral,neutral,neutral
524540984,"> Have you run `python -m visdom.server` before running the training script? You can also disable visdom by setting `--display_id 0`.

您好！我想问您一个问题，当我用自己的数据集A作为trainA数据集B作为trainB时，epoch一直等于1？",run python running training script also disable setting,issue,negative,neutral,neutral,neutral,neutral,neutral
524401672,"no, i just need a help and i couldn't think of anything better.",need help could think anything better,issue,positive,positive,positive,positive,positive,positive
524384924,"Just inspired, I translate the code to keras, but using the same archtecture and hyper-parameters.

Do you have some tip to improve the equilibrium in D and G loss?",inspired translate code tip improve equilibrium loss,issue,positive,neutral,neutral,neutral,neutral,neutral
524378728,"Did you use this repo? The logging style `[Epoch 604000] [D loss: 0.000411, acc: 100%] [G loss: 1381.413574]` does not look like this repo's style. It seems that your D is too strong and G is too weak. ",use logging style epoch loss loss look like style strong weak,issue,negative,positive,neutral,neutral,positive,positive
524236413,"Works great, I re-installed scipy 1.3 and started another training and everything seemed to work. Thanks!",work great another training everything work thanks,issue,positive,positive,positive,positive,positive,positive
524232151,"(epoch: 70, iters: 838, time: 0.313, data: 0.003) D_A: 0.298 G_A: 0.078 cycle_A: 0.483 idt_A: 0.126 D_B: 0.312 G_B: 0.256 cycle_B: 0.289 idt_B: 0.150
(epoch: 70, iters: 938, time: 0.323, data: 0.007) D_A: 0.307 G_A: 0.221 cycle_A: 0.312 idt_A: 0.174 D_B: 0.245 G_B: 0.262 cycle_B: 0.374 idt_B: 0.194
(epoch: 70, iters: 1038, time: 1.455, data: 0.002) D_A: 0.188 G_A: 0.177 cycle_A: 0.761 idt_A: 0.226 D_B: 0.061 G_B: 0.534 cycle_B: 0.487 idt_B: 0.241
saving the model at the end of epoch 70, iters 76860
End of epoch 70 / 200    Time Taken: 355 sec
learning rate = 0.0002000
(epoch: 71, iters: 40, time: 0.311, data: 0.003) D_A: 0.032 G_A: 0.719 cycle_A: 0.247 idt_A: 0.226 D_B: 0.338 G_B: 0.577 cycle_B: 0.532 idt_B: 0.179
(epoch: 71, iters: 140, time: 0.338, data: 0.007) D_A: 0.248 G_A: 0.932 cycle_A: 0.437 idt_A: 0.199 D_B: 0.105 G_B: 0.685 cycle_B: 0.593 idt_B: 0.213
(epoch: 71, iters: 240, time: 0.298, data: 0.003) D_A: 0.230 G_A: 0.958 cycle_A: 0.405 idt_A: 0.214 D_B: 0.260 G_B: 0.106 cycle_B: 0.477 idt_B: 0.178
(epoch: 71, iters: 340, time: 1.699, data: 0.001) D_A: 0.233 G_A: 0.502 cycle_A: 0.303 idt_A: 0.149 D_B: 0.179 G_B: 0.243 cycle_B: 0.299 idt_B: 0.138
(epoch: 71, iters: 440, time: 0.307, data: 0.003) D_A: 0.273 G_A: 0.463 cycle_A: 0.316 idt_A: 0.172 D_B: 0.227 G_B: 0.242 cycle_B: 0.329 idt_B: 0.139
(epoch: 71, iters: 540, time: 0.308, data: 0.002) D_A: 0.070 G_A: 0.477 cycle_A: 0.292 idt_A: 0.156 D_B: 0.219 G_B: 0.808 cycle_B: 0.344 idt_B: 0.136
(epoch: 71, iters: 640, time: 0.309, data: 0.001) D_A: 0.151 G_A: 0.409 cycle_A: 0.295 idt_A: 0.150 D_B: 0.224 G_B: 0.386 cycle_B: 0.366 idt_B: 0.153

在maps数据集上用cycle第一次训练，在这个地方停下不动了？能帮帮解决下吗？非常谢谢！",epoch time data epoch time data epoch time data saving model end epoch end epoch time taken sec learning rate epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data,issue,negative,negative,neutral,neutral,negative,negative
524199168,"Hey there, I have come across the same question when rendering a single picture.
I wonder whether you have solved it since the issue is closed, and it would be great help to me if you want to share the solution",hey come across question rendering single picture wonder whether since issue closed would great help want share solution,issue,positive,positive,positive,positive,positive,positive
524139858,"> I am sorry, I didn't download the dataset, I used my own dataset. The link (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md) may be useful to @2110317008

Wow, thank you very much.",sorry used link may useful wow thank much,issue,positive,positive,neutral,neutral,positive,positive
524138902,"I am sorry, I didn't download the dataset, I used my own dataset. The link (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md) may be useful to  @2110317008 
",sorry used link may useful,issue,negative,negative,neutral,neutral,negative,negative
523961000,CycleGAN might be enough. But additional paired data will always help. See this [work](https://arxiv.org/abs/1901.08212) for more details.,might enough additional paired data always help see work,issue,negative,neutral,neutral,neutral,neutral,neutral
523960291,`imresize` has been replaced in the latest [commit](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/dca9002f598fada785b92d496fcdc7b04528b393). Could you download the latest code and try it again?,latest commit could latest code try,issue,negative,positive,positive,positive,positive,positive
523932408,"> Hi ,I have a file like the one shown below:
> |——dataset
> |——|—trainA (1.jpg、2.jpg、3.jpg..........1000.jpg)
> |——|—trainB (1.jpg、2.jpg、3.jpg..........1000.jpg)
> |——|—valA (1.jpg、2.jpg、3.jpg..........50.jpg)
> |——|—valB (1.jpg、2.jpg、3.jpg..........50.jpg)
> The paired images in trainA and trainB have the same name(like 1.jpg)、but when I train CycleGan, I view the saved image ,the real_A and real_B are misplaced, just like the real_A is 1.jpg ,however the real_B is 20.jpg(I think the right situation : the real_A is 1.jpg and the real_B is 1.jpg)
> I don't know if my statement is clear. Looking forward for your reply, thank you!




The data set cannot be downloaded because of an error in the code running. Can you provide these data sets? I will be very grateful!
",hi file like one shown paired name like train view saved image like however think right situation know statement clear looking forward reply thank data set error code running provide data grateful,issue,positive,positive,positive,positive,positive,positive
523536143,"Just supervised training with cycleGAN wouldn't be enough?
I can't imagine where the pix2pix will act.
Image datasets with clean room and messy room. For example.",training would enough ca imagine act image clean room messy room example,issue,negative,positive,neutral,neutral,positive,positive
523503988,It might be possible and it depends o your datasets. Combining pix2pix and CycleGAN ins a semi-supervised fashion would be helpful.,might possible combining fashion would helpful,issue,negative,neutral,neutral,neutral,neutral,neutral
523503321,I am not sure why. Does the program freeze on the provided datasets?  Have you started visdom server?,sure program freeze provided server,issue,negative,positive,positive,positive,positive,positive
523425687,"Hello @riki7649255,

I think the best place to start is learning about generic artificial networks. I've found several courses on Udemy.com are good.  Specifically, the [OpenCV, SSD, and GANs](https://www.udemy.com/course/computer-vision-a-z/) would be helpful for learning how to use the CycleGAN base properly.

Good luck.
--Ladvien",hello think best place start learning generic artificial found several good specifically would helpful learning use base properly good luck,issue,positive,positive,positive,positive,positive,positive
523103975,Your task seems to be quite challenging. One way of doing it is to have a segmentation model for horse and zebra and force the discriminator to look at a particular region. See this [work](http://openaccess.thecvf.com/content_ECCV_2018/papers/Liang_Generative_Semantic_Manipulation_ECCV_2018_paper.pdf) for more details.,task quite one way segmentation model horse zebra force discriminator look particular region see work,issue,negative,positive,positive,positive,positive,positive
523084898,It is currently not available. Feel free to train a model with our code.,currently available feel free train model code,issue,positive,positive,positive,positive,positive,positive
522645327,"The zebra2horse's results are worse than horse2zebra's results, as sometimes, the generator has to preserve some zebra stripes in the generated horse images so that later it can reconstruct the original zebra images. This is one of the limitations of CycleGAN. See this [work](https://arxiv.org/abs/1712.02950) for more details.",worse sometimes generator preserve zebra horse later reconstruct original zebra one see work,issue,negative,negative,neutral,neutral,negative,negative
522643466,"Feel free to try CycleGAN.
- Crop_size: You can try 256 or 512.
- Use resnet_9blocks or resnet_6blocks. 
- I will use the default ones. 
",feel free try try use use default,issue,positive,positive,positive,positive,positive,positive
522416185,"Good morning Sir,
Referenced to previous conversation on horse2zebra conversion.
I have completed 200 epochs and attached the screen shot of conversion in both directions(2files)
It was learning and good experience .Forward conversion is good but the conversion from zebra2horse is not good in any  image .kindly guide what could be reason for that.
![horse2zebra](https://user-images.githubusercontent.com/51158296/63240706-7795ae00-c26e-11e9-9ac4-6dfe051f7008.png)
![zebra2horse](https://user-images.githubusercontent.com/51158296/63240713-80867f80-c26e-11e9-92d7-040006c09378.png)
Thanks
",good morning sir previous conversation conversion attached screen shot conversion learning good experience conversion good conversion good image guide could reason thanks,issue,positive,positive,positive,positive,positive,positive
522377153,"> In the prerequisites is saying CPU or NVIDIA GPU + CUDA CuDNN, is that means I can run without NVIDIA and CUDA but in CPU? I'm having the error: Found no NVIDIA driver on your system. Please check that you
> have an NVIDIA GPU and installed a driver from
> http://www.nvidia.com/Download/index.aspx
> I need to install NVIDIA driver even if I only use CPU?

base_options.py
('--gpu_ids', type=str, default='-1', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')",saying run without error found driver system please check driver need install driver even use use,issue,positive,neutral,neutral,neutral,neutral,neutral
521748172,"People just calculate FID between generated results (fake spring) and target real images (e.g., real spring). FID is not perfect. It doesn't capture the conditioning (i.e. alignment between output and input). But it captures the marginal distribution.",people calculate fid fake spring target real real spring fid perfect capture alignment output input marginal distribution,issue,negative,positive,positive,positive,positive,positive
521747276,"(1) Here is a PyTorch [tutorial](https://pytorch.org/tutorials/beginner/saving_loading_models.html) on how to load a model. 
(2) You can modify the code by removing the `isTrain` flag for your network.
Sorry that I don't have time to read your code. I recommend that you follow some PyTorch tutorials. ",tutorial load model modify code removing flag network sorry time read code recommend follow,issue,negative,negative,negative,negative,negative,negative
521575647,"Regading (1), I've defined my own architecture in a separate class file, I was wondering of how to load it during the training of another class/architecture. Could you elaborate on writing my own function for creating the object? We could talk via PM if this is easier so I can explain what I'm doing in my code.

Regarding (2), I've look thought that function before, it looks like the networks are being loaded only if the network is in test mode or if training continues from a previous point, unless I misunderstand something?

Thanks again for the response.",defined architecture separate class file wondering load training another could elaborate writing function object could talk via easier explain code regarding look thought function like loaded network test mode training previous point unless misunderstand something thanks response,issue,positive,positive,positive,positive,positive,positive
521500418,"Thanks for suggesting the FID. If two dataset are unpaired such as dataset 1 is synthetic of spring and dataset 2 is real spring. They are unpaired, could I use the FID as a performance metric. ",thanks suggesting fid two unpaired synthetic spring real spring unpaired could use fid performance metric,issue,negative,positive,positive,positive,positive,positive
521493918,"A few choices: (1) we often evaluate CycleGAN on paired datasets (e.g., cityscapes used in the paper) while the model is trained without using pairs. (2) some folks have used standard GAN metrics such as [FID](https://github.com/mseitzer/pytorch-fid) (3) As no metrics are perfect, a user study might be helpful. You can check out the details of the user study in the CycleGAN paper.  ",often evaluate paired used paper model trained without used standard gan metric fid metric perfect user study might helpful check user study paper,issue,positive,positive,positive,positive,positive,positive
521331176,"1. You need to define your network architecture, just like what we have done for [G](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L56) and for [D](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L60). You need to write your own function for creating a network object.  
2. It is called during [training](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L51) as well. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/base_model.py#L78).",need define network architecture like done need write function network object training well see line,issue,positive,neutral,neutral,neutral,neutral,neutral
521255383,"Thanks for the reply junyanz, I've had a look at the code but this seems to be a case where the models you would have in the model_names variable have to be of the same class, eg.  Pix2Pix. What if the Network B (like in the example) was another architecture, eg. a fully connected network, of another class FCN. Would you load it the same way?

Also I see that load_networks is only called in test mode, while I'd like to load the pre-trained model  B during the training of A as well, without saving it every time.",thanks reply look code case would variable class network like example another architecture fully connected network another class would load way also see test mode like load model training well without saving every time,issue,positive,positive,positive,positive,positive,positive
520964034,The default network for pix2pix is `unet256` which only works for `256x256` or `512x512`. There are a few options: (1) you can resize your image to `512x512`. See the option for `--preprocess`. (2) you can use a different network such as `resnet_9blocks` or your own network. ,default network work resize image see option use different network network,issue,negative,neutral,neutral,neutral,neutral,neutral
520950531,I see. I misunderstood your original question. I am not sure how to address the object boundary issue. I haven't seen that before. ,see misunderstood original question sure address object boundary issue seen,issue,negative,positive,positive,positive,positive,positive
520949932,"We use the function [load_networks](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/base_model.py#L175) to load all the pre-trained models. The variable [model_names](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L50) controls which models will be loaded.  If you want to write your custom model loading function, I recommend that you write a separate script to test the function before adding it to the training pipeline.",use function load variable loaded want write custom model loading function recommend write separate script test function training pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
520626377,"Actually, the boundary between objects. For example, the apple locates in the table. Then the boundary between the apple and table is so blur. I think it is not the issue of padding",actually boundary example apple table boundary apple table blur think issue padding,issue,negative,neutral,neutral,neutral,neutral,neutral
520581966,Not sure. Maybe you can try different padding types. ,sure maybe try different padding,issue,negative,positive,positive,positive,positive,positive
520379939,"@derekahuang No.. I did not.. if you find a solution, do post it here :)
Maybe you could use perceptual loss in addition?
Here is a paper which implements it for the unpaired dataset:
CartoonGAN: Generative Adversarial Networks for Photo Cartoonization [link](https://www.google.com/url?sa=t&source=web&rct=j&url=http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf&ved=2ahUKEwibwa63lP3jAhUDr6QKHZc9DosQFjAAegQIAxAB&usg=AOvVaw09VEqQyMBqY9xUMRQsf9Ea)",find solution post maybe could use perceptual loss addition paper unpaired generative photo link,issue,negative,neutral,neutral,neutral,neutral,neutral
520340436,"+1
```
WARNING:root:Setting up a new session...
create web directory ./checkpoints/maps_cyclegan/web...
Traceback (most recent call last):
  File ""train.py"", line 51, in <module>
    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
  File ""/data/xiaoyubei/codes/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py"", line 183, in optimize_parameters
    self.forward()      # compute fake images and reconstruction images.
  File ""/data/xiaoyubei/codes/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py"", line 114, in forward
    self.fake_B = self.netG_A(self.real_A)  # G_A(A)
  File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py"", line 121, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/data/xiaoyubei/codes/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 373, in forward
    return self.model(input)
  File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/gongke/miniconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/nn/modules/conv.py"", line 691, in forward
    output_padding, self.groups, self.dilation)
RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1533672544752/work/aten/src/THC/THCBlas.cu:249
```
This is my environment:
```
>>> torch.__version__
'0.4.1'
>>> torch.version.cuda
'9.0.176'

$ nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130

```",warning root setting new session create web directory recent call last file line module calculate loss get update network file line compute fake reconstruction file line forward file line result input file line forward return file line result input file line forward return input file line result input file line forward input module input file line result input file line forward error program execute environment compiler driver copyright corporation built compilation release,issue,negative,negative,neutral,neutral,negative,negative
520260685,"@vrao9 thanks for the tips. i'm training between two images that are pretty close, and the artifacts appear after just a few steps...did you solve it with methods besides decreasing the discriminator learning rate? ",thanks training two pretty close appear solve besides decreasing discriminator learning rate,issue,positive,positive,positive,positive,positive,positive
520187581,"@derekahuang I observed that for some epochs the corresponding images had large number of such patches, but for some epochs, those were comparatively less. Reducing the learning rate of the discriminator helped in a small way to reduce those artefacts. 
I think those artefacts occur when the discriminator gets too powerful, i.e., discriminator achieves approx 100% accuracy.",corresponding large number comparatively le reducing learning rate discriminator small way reduce think occur discriminator powerful discriminator accuracy,issue,negative,positive,neutral,neutral,positive,positive
519570396,"`--lambda_A` and `--lambda_B` control the magnitude of the cycle-consistency loss. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L41) for more details.

Not sure it black holes will make the file corrupt, as the generator's output is bounded by a TanH. It might happen if you have an inf or nan during training.",control magnitude loss see line sure black make file corrupt generator output bounded tanh might happen nan training,issue,negative,negative,neutral,neutral,negative,negative
519427624,"Thanks a lot for your fast responses. 
@vis-opt Maybe I am blind or whatever, but I do not find the `lambda_A ` and `lambda_B ` values in the option files. How is it possible to change the values? 

@junyanz Good point, I increased the learning rate and by `lr=0.0002` everything was fine, `lr=0.002` lead to these strange behaviours. So, I should test `lr=0.00002` to compare them. 

Do you have an idea if these black holes could make the png file ""corrupt""? Further evaluations I wanted to run were not possible because of some files which are obviously corrupt. Do you think these black holes could be the reason for that?


Thanks a lot in advance. I really appreciate your 
",thanks lot fast maybe blind whatever find option possible change good point learning rate everything fine lead strange test compare idea black could make file corrupt run possible obviously corrupt think black could reason thanks lot advance really appreciate,issue,positive,positive,neutral,neutral,positive,positive
519298124,"> Sorry for bother you, have you solved this error?

Alright I have fixed my error.... Pytorch does not support in-place operation...",sorry bother error alright fixed error support operation,issue,negative,negative,negative,negative,negative,negative
519173409,"Please post your question in the TensorFlow-pix2pix [repo](https://github.com/affinelayer/pix2pix-tensorflow). The web demo is not implemented by this repo. As far as I know, it is not supported by the TensorFlow-pix2pix.",please post question web far know,issue,negative,positive,neutral,neutral,positive,positive
519172403,A smaller learning rate will also make training more stable. ,smaller learning rate also make training stable,issue,negative,neutral,neutral,neutral,neutral,neutral
519063200,"I think they mean try boosting the `lambda_A, lambda_B` arguments in `cycle_gan_model.py`. Default is 10, you can try 25 or 50 for example. ",think mean try default try example,issue,negative,negative,negative,negative,negative,negative
518783897,"@junyanz  Thanks for your quick response! 

It is very interesting because I translated images from Cityscapes to Berkeley and vice versa. With the default learning rate (0.0002) everything looks fine but with the increased learning rate of 0.002 those strange artefacts show up. 

I am using unpaired data. How can I increase the loss? I am unsure what this means.. ",thanks quick response interesting vice default learning rate everything fine learning rate strange show unpaired data increase loss unsure,issue,positive,positive,positive,positive,positive,positive
518781352,"It sometimes happens for some applications. It's kind of mode collapse in the patch level and hard to fix directly. If you are using paired datasets, you can try more recent models such as [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE). If you are using unpaired datasets, you can try to increase identity loss and/or cycle consistency loss. ",sometimes kind mode collapse patch level hard fix directly paired try recent spade unpaired try increase identity loss cycle consistency loss,issue,negative,positive,positive,positive,positive,positive
518777315,You need to start the visdom server first. Type `python -m visdom.server` in a different console. You can also disable visdom server with `--display_id 0`.,need start server first type python different console also disable server,issue,negative,positive,positive,positive,positive,positive
518351070,"Please use the original (and the latest) code and train the models for 200 epochs. We haven't tested wgan-gp and different learning rates. If you want bigger changes, you can use `--lambda_identity 0` as used in the paper for horse2zebra. ",please use original latest code train tested different learning want bigger use used paper,issue,positive,positive,positive,positive,positive,positive
518118318,"Firstly, I tried without any modification in code but not complete till 200 epochs (it was till maximum 37epochs) .I tried facade dataset it works as shown in original paper.
secondly  tried with two modification as mentioned below:
1. Separate learning rate for G and D with below line by adding new parser.add_argument and lrD with 0.0001 and lr G with 0.0002(in train-options.py)
parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')
2. adding wgan-gp after loss_D_fake = self.criterionGAN(pred_fake, False) (in cyclegan.py)
completely 200 epochs image size 256*256
3. Yes , conversion did not take place in any single image.

",firstly tried without modification code complete till till maximum tried facade work shown original paper secondly tried two modification separate learning rate line new learning rate false completely image size yes conversion take place single image,issue,positive,positive,neutral,neutral,positive,positive
518097310,"Hi @nehaleosharma , did you modify any code, or is it directly running the training command from the unmodified code?

Also, do you get this kind of NO CONVERSION result for all input images? It's natural that CycleGAN cannot translate a few images when there is identity loss. ",hi modify code directly running training command unmodified code also get kind conversion result input natural translate identity loss,issue,positive,positive,positive,positive,positive,positive
518084103,"Good Morning Sir
As discussed I have trained the model with 200 epochs with the above written training script. But, did not get the result.I am astonished what I am doing wrong.Does this require any changes in code which i am missing. trainA, trainB are having 1064(horses) and 1334 (zebra) images respectively. testA having 120(horses) and testB 140(zebra)  images respectively.

1. [epoch200](https://user-images.githubusercontent.com/51158296/62439072-0d5a1500-b768-11e9-9773-837585ea2c85.png)
2. [NO CONVERSION](https://user-images.githubusercontent.com/51158296/62439081-1ba83100-b768-11e9-8a32-9c7bd9942e89.png)
Kindly guide me.what is wrong ?

and most importantly Thanks for replying every query so frequently.Looking forward for your reply.
Thanks
",good morning sir trained model written training script get require code missing zebra respectively testa zebra respectively epoch conversion kindly guide wrong importantly thanks every query forward reply thanks,issue,positive,positive,positive,positive,positive,positive
517846880,Could you share with us your training and test script? How many epochs have you trained?,could share u training test script many trained,issue,negative,positive,positive,positive,positive,positive
517543146,"If you can determine whether the generated image is correct, you could use it to evaluate the algorithm. See some recent denoising [work](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Image_Blind_Denoising_CVPR_2018_paper.pdf) for more details.

P.S: I closed the issue as the original visdom issue has been resolved. ",determine whether image correct could use evaluate algorithm see recent work closed issue original issue resolved,issue,negative,positive,neutral,neutral,positive,positive
517077937,"> Similar distortion will be ideal.

Thanks, I have one problem, if I want to generate the real distortion(the distortion types is not known), I don't easily judge the real distortios are transfered to the undistorted images. It is not like the simple distortion, such as gaussian blur, I can judge whether the generated images is correct. H Could you give me some advice?",similar distortion ideal thanks one problem want generate real distortion distortion known easily judge real undistorted like simple distortion blur judge whether correct could give advice,issue,positive,positive,positive,positive,positive,positive
516952808,"Having trouble implementing this.
I went into test.py and called visualizer11 instead of visualizer but when running I get:

Traceback (most recent call last):
  File ""test.py"", line 33, in <module>
    from util.visualizer11 import save_images
  File ""/Users/Magol02/pytorch-CycleGAN-and-pix2pix/util/visualizer11.py"", line 167
    import sys.path.join(self.img_dir, 'epoch%.3d_%s.png' % (epoch, label))
                                    ^
SyntaxError: invalid syntax
",trouble went visualizer instead visualizer running get recent call last file line module import file line import epoch label invalid syntax,issue,negative,negative,neutral,neutral,negative,negative
516700943,"I just trained CycleGAN on K80 with summer2winter_yosemite dataset, it took about 3 days. Hope it helps.",trained took day hope,issue,negative,neutral,neutral,neutral,neutral,neutral
516332938,"wow,tahks,but really what I want is a direct PDF file.




------------------ 原始邮件 ------------------
发件人: ""nehaleosharma""<notifications@github.com>;
发送时间: 2019年7月30日(星期二) 下午4:54
收件人: ""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com>;
抄送: ""Subscribed""<subscribed@noreply.github.com>;
主题: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] Loss D goes to zero whenadding a new loss to generator (#626)




How to print accuracy with the other details printed such as epoch and various losses.where to add metric=[accuracy]
 
—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub, or mute the thread.",wow really want direct file loss go zero new loss generator print accuracy printed epoch various add accuracy thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
516329333,How to print accuracy with the other details printed such as epoch and various losses.where to add metric=[accuracy],print accuracy printed epoch various add accuracy,issue,negative,neutral,neutral,neutral,neutral,neutral
516241476,Sorry for the noise. It works now and thank you so much!,sorry noise work thank much,issue,negative,negative,negative,negative,negative,negative
516236655,"> MeanIOU is only useful for the cityscape dataset and other segmentation datasets. It cannot be used in Day2night dataset. You need to design a new metric for your own dataset.

Yes, it depends on your specific task. You can find some paper in the area which your own data maybe related to, then you may find some metric，and could compare with Other one.",useful cityscape segmentation used need design new metric yes specific task find paper area data maybe related may find could compare one,issue,positive,positive,positive,positive,positive,positive
516079877,"When D is trained, we use an image [buffer](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/image_pool.py) that stores 50 previously generated samples from previous Gs. The idea of using non-optimal previously generated samples can slightly stabilize the training as discussed in this [paper](https://arxiv.org/pdf/1612.07828v1.pdf). Computing all the intermediate images (fake_A/B and rec_A/B) is expensive in CycleGAN. So we only computed once in our implementation. ",trained use image buffer previously previous idea previously slightly stabilize training paper intermediate expensive implementation,issue,negative,negative,negative,negative,negative,negative
516071445,"Have you installed the Python library scipy?  What is your version of your scipy? According to this [post](https://github.com/scipy/scipy/issues/6212),  the imresize function could have been removed in the latest version. If so, please use `skimage.transform.resize` instead.",python library version according post function could removed latest version please use instead,issue,negative,positive,positive,positive,positive,positive
515923201,"@gm039 @jialiang19 
Hi,I am having same problem.
I tried your approach,and tried CycleGAN train.

Traceback (most recent call last): 
File ""train.py"", line 25, in <module>
 from util.visualizer import Visualizer 
File ""/workspace/pytorch-CycleGAN-and-pix2pix/util/visualizer.py"", line 8, in <module>
 from scipy.misc import imresize 
ImportError: cannot import name 'imresize' from 'scipy.misc' (/miniconda/lib/python3.7/site-packages/scipy/misc/__init__.py)

I checked version of python , python3.7.3 .
Could you help.",hi problem tried approach tried train recent call last file line module import visualizer file line module import import name checked version python python could help,issue,negative,neutral,neutral,neutral,neutral,neutral
515779540,"Appreciate the quick response! I initially thought that the only difference is a one-step offset too but not performing a forward pass between `update G -> update D` would lead to optimizing the discriminator on a non-optimal generator's output (the generator might be producing better outputs).

I realize that these points might not be of practical consequence but wanted to know any reason/intuition for the current implementation.",appreciate quick response initially thought difference offset forward pas update update would lead discriminator generator output generator might better realize might practical consequence know current implementation,issue,positive,positive,positive,positive,positive,positive
515778334,"Yes, please follow this [instruction](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#evaluating-labels2photos-on-cityscapes).",yes please follow instruction,issue,positive,neutral,neutral,neutral,neutral,neutral
515778189,"1. I think the order is not that important. Either order should work in practice.
2. The current optimization sequence (forward -> update G -> update D -> forward -> update G -> update D ...) is consistently with the order you proposed. There is one step offset. (You start from update D- > forward -> update G ->update D...). But both are fine in practice.",think order important either order work practice current optimization sequence forward update update forward update update consistently order one step offset start update forward update update fine practice,issue,positive,positive,positive,positive,positive,positive
515741388,"My situation was similar to yours! That's how I solved it:
Clone this repo [evaluation of the Cityscapes dataset](https://github.com/mcordts/cityscapesScripts/tree/master/cityscapesscripts) and put the **cityscapesscripts** folder into your **cityscapes_dir** and rename it as **scripts**.",situation similar clone evaluation put folder rename,issue,negative,neutral,neutral,neutral,neutral,neutral
515619661,I don't know what has happened just based on the training loss curve. I recommend that you start with a good GAN [codebase](https://github.com/LMescheder/GAN_stability) and make changes gradually.,know based training loss curve recommend start good gan make gradually,issue,negative,positive,positive,positive,positive,positive
515619381,"I recommend that you read the [DCGAN](https://github.com/pytorch/examples/blob/master/dcgan/main.py) code and the original GAN paper. The objective for generator and discriminator is slightly different, and our code reflects the difference. ",recommend read code original gan paper objective generator discriminator slightly different code difference,issue,positive,positive,positive,positive,positive,positive
515618583,"I just tried it. It seems to work for me. Here is my training script. Are you using the latest code?
```
set -ex
GPU_ID=0
CLASS=cityscapes
SAVE_EPOCH=25

DISPLAY_ID=$((GPU_ID*10+1))
CUDA_VISIBLE_DEVICES=${GPU_ID} python train.py \
	--dataroot ./datasets/${CLASS} \
	--name ${CLASS}_pix2pix \
	--save_epoch_freq ${SAVE_EPOCH} \
	--model pix2pix \
	--display_id ${DISPLAY_ID} \
	--direction BtoA \
	--display_freq 4000 \
```",tried work training script latest code set python class name class model direction,issue,negative,positive,positive,positive,positive,positive
515530933,Could you share with us your training script?,could share u training script,issue,negative,neutral,neutral,neutral,neutral,neutral
515293518,"Thanks for your patience. I did train it for 200 epochs and that blurry picture is the final result. What should I set if I hope to reproduce the results in the paper. I don't think default setting also works well.

Thank you so much!  ",thanks patience train blurry picture final result set hope reproduce paper think default setting also work well thank much,issue,positive,positive,positive,positive,positive,positive
515192699,"The model was trained for 200 epochs in the paper and it might look blurry in the 25 epoch. Not sure about the D_loss. If you want to have even sharper results, you can use [pix2pixHD](https://github.com/NVIDIA/pix2pixHD).",model trained paper might look blurry epoch sure want even sharper use,issue,negative,positive,positive,positive,positive,positive
514929006,"Thank  you very much! Pretrained models work very well. By the way, I still have one more question.  When I use cityscapes dataset to train without any change of default setting, the output results are in dark gray , in other words , with no color. After about epoch 25, D_loss goes to zero and keeps zero till end. Could you tell me what's wrong with it ?

![13_fake_B](https://user-images.githubusercontent.com/52700830/61853597-e3465e80-aeee-11e9-9fa9-9ff7a719e9b6.png)
Thank you again!",thank much work well way still one question use train without change default setting output dark gray color epoch go zero zero till end could tell wrong thank,issue,negative,negative,negative,negative,negative,negative
514716910,"Yeah, U-Net only supports 256. You have to use `--crop_size 256` for U-Net. You can try to change the padding to 0 in this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L578). ALso check out the original 64x64 [imageGAN](https://github.com/pytorch/examples/blob/master/dcgan/main.py#L161) implemented in DCGAN.",yeah use try change padding line also check original,issue,positive,positive,positive,positive,positive,positive
514714493,MeanIOU is only useful for the cityscape dataset and other segmentation datasets. It cannot be used in Day2night dataset. You need to design a new metric for your own dataset.,useful cityscape segmentation used need design new metric,issue,negative,positive,positive,positive,positive,positive
514682917,"> Did you use `--display_id` or `--displat_id`? Your error seems not to be relevant to the visdom. Are you able to train models with provided datasets? I recommend that you first train a model on provided datasets before using it for your own datasets.

Thanks for your reply.I have solved it by using ""scale_width_and_crop"" not ""scale_and_width"". because my datasets have different sizes .
Thanks for your work again!",use error relevant able train provided recommend first train model provided thanks different size thanks work,issue,positive,positive,positive,positive,positive,positive
514646292,"Hi, I got the same error as @hpwarren in loading state_dict while using 9blocks ResNet, any suggestions?",hi got error loading,issue,negative,neutral,neutral,neutral,neutral,neutral
514500848,"I tried that, but it's causing problems with dimensions at the skip connections in Unet:

```
File ""/media/ssd1/.../workspace/GAN/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 535, in forward
    return torch.cat([x, self.model(x)], 1)
RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 16 and 17 in dimension 2
```",tried causing skip file line forward return invalid argument size must match except dimension got dimension,issue,negative,neutral,neutral,neutral,neutral,neutral
514461042,"Actually, I have trained the model on my own datasets and Day to night dataset. I want to evaluate it on Mean IOU, Mean pixel accuracy and Mean class accuracy. So, how to evaluate these on my datasets ?
",actually trained model day night want evaluate mean mean accuracy mean class accuracy evaluate,issue,negative,negative,negative,negative,negative,negative
514387374,The evaluation depends on your specific task and application. There is no single metric that works for all the datasets and tasks.,evaluation specific task application single metric work,issue,negative,negative,neutral,neutral,negative,negative
514327192,Did you use `--display_id` or `--displat_id`? Your error seems not to be relevant to the visdom. Are you able to train models with provided datasets? I recommend that you first train a model on provided datasets before using it for your own datasets.,use error relevant able train provided recommend first train model provided,issue,negative,positive,positive,positive,positive,positive
514326214,We calculate GAN training loss in this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L169). ,calculate gan training loss line,issue,negative,neutral,neutral,neutral,neutral,neutral
514325029,"In the original pix2pix paper, 286 × 286 discriminator:
C64-C128-C256-C512-C512-C512
Maybe you can change crop_size to 286.",original paper discriminator maybe change,issue,negative,positive,positive,positive,positive,positive
514323623,You are expected to get the last column of the results on this [webpage](https://phillipi.github.io/pix2pix/images/index_facades2_loss_variations.html).  Could you also try to [download](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix) pre-trained models and test the model and compare with your model? ,get last column could also try test model compare model,issue,negative,neutral,neutral,neutral,neutral,neutral
514200742,"> You can also add `--display_id 0` to disable visdom.

Thans for your reply! but when I add --displat_id 0,I got error like this:
`RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 10 and 11 in dimension 2 at /opt/conda/conda-bld/pytorch_1525909934016/work/aten/src/THC/generic/THCTensorMath.cu:111`

Do you know why?",also add disable reply add got error like invalid argument size must match except dimension got dimension know,issue,negative,neutral,neutral,neutral,neutral,neutral
514176779,"![18_fake_B](https://user-images.githubusercontent.com/52700830/61709726-6e561600-ad82-11e9-83ea-d0565ec0d3f3.png) Sorry to trouble you but I'm really new here. By using the default setting, the results I got are very blurry.

Here is the command line:python3 train.py --dataroot ./datasets/facades --name facades-o1 --model pix2pix --direction BtoA --gpu_ids 6 --gan_mode lsgan

Should I add something more or adjust some parameters? Thanks a lot! @ @junyanz ",sorry trouble really new default setting got blurry command line python name model direction add something adjust thanks lot,issue,negative,negative,neutral,neutral,negative,negative
514086511,"Thanks for your comment. In the standard pix2pix model the inputs to the Discriminator are 256x256, `--n_layers_D 6` then results in the size being 2x2, 1x1 is impossible, since it breaks at `--n_layers_D 7`",thanks comment standard model discriminator size impossible since,issue,negative,negative,negative,negative,negative,negative
514041310,"OK I'll try it and notice you
Thanks a lot!",try notice thanks lot,issue,negative,positive,positive,positive,positive,positive
514041273,"We found that normalization in the first layer removes too much information. Here's the reasoning. 

In case of image2image translation, information such as the brightness and contrast of the input image is valuable. If you normalize too early, this information disappears before the network has a chance to process.  We observed that in a couple of datasets, adding normalization in the first layer resulted in worse visual quality., ",found normalization first layer much information reasoning case translation information brightness contrast input image valuable normalize early information network chance process couple normalization first layer worse visual,issue,positive,positive,neutral,neutral,positive,positive
513927901," If you would like to reproduce the same results as included in the paper, check out the original CycleGAN Torch [code](https://github.com/junyanz/CycleGAN). These Pytorch models were retrained a few months after the paper publication, using the PyTorch code. There are a few subtle but important steps that might affect the performance: You need to save the images as PNG (the current default in this repo is JPG). You need to apply a proper resizing (You probably need to resize it to 2048x1024 directly in the code before you save it to PNG). ",would like reproduce included paper check original torch code paper publication code subtle important might affect performance need save current default need apply proper probably need resize directly code save,issue,positive,positive,neutral,neutral,positive,positive
513926416,A standalone ImageGAN has not been included in this repo. You can increase `--n_layers_D` so that the final output is 1x1.  I guess it's 5 or 6.,included increase final output guess,issue,negative,neutral,neutral,neutral,neutral,neutral
513922879,I suspect that visdom is not stable with Multi-GPUS but I haven't tested it. Could you disable visdom by `--display_id 0`?,suspect stable tested could disable,issue,negative,neutral,neutral,neutral,neutral,neutral
513878650,I updated the code. Just check out the latest code.,code check latest code,issue,negative,positive,positive,positive,positive,positive
513624352,"Hi,I have met the same problem,have you figured it out? THX.",hi met problem figured,issue,negative,neutral,neutral,neutral,neutral,neutral
513618743,Gosh... You're genius!! The code you wrote is awesome! ,gosh genius code wrote awesome,issue,positive,positive,positive,positive,positive,positive
513435639,"> It should be fixed with the latest commit. Please check out the code again.

But how should it be done?
",fixed latest commit please check code done,issue,positive,positive,positive,positive,positive,positive
513111037,"I used the default setting in both training and testing process. If so,the results are normal, right? ",used default setting training testing process normal right,issue,negative,positive,positive,positive,positive,positive
513097404,"Nope. I failed to fix it, and just run with single gpu.

And I think our issues are quite different..
In my case, literally whole system is frozen and crashed.
This is not a problem of speed.
But anyway learning on multi-gpu with this code seems not that stable.
",nope fix run single think quite different case literally whole system frozen problem speed anyway learning code stable,issue,negative,positive,neutral,neutral,positive,positive
513080336,"Thanks to reply.

As you advised ,I remove some layers and result is greater than I expected 
So, what caused the problem of my experiment is resolution : layers issue

Thanks you very much",thanks reply advised remove result greater problem experiment resolution issue thanks much,issue,positive,positive,positive,positive,positive,positive
512997769,"I have the same issue as you. When I try to use multi-gpu to train 2 models, everything is fine at the beginning, but after about 10 epochs,, the gpu-util is about 0, the training is really slow. Did you figure it out?",issue try use train everything fine beginning training really slow figure,issue,negative,positive,neutral,neutral,positive,positive
512890304,This is a good catch. I just updated the code (solution 2).,good catch code solution,issue,positive,positive,positive,positive,positive,positive
512880370,The issues are related to visdom. You can disable the visdom server by setting `--display_id 0`.,related disable server setting,issue,negative,neutral,neutral,neutral,neutral,neutral
512879049,"Random images of people and objects will not work. Real images should have similar objects. For example, if paintings are about flowers. Your real images should also be about flowers. If your real images have different content such as animals, CycleGAN will not work. ",random people work real similar example real also real different content work,issue,negative,negative,neutral,neutral,negative,negative
512877673,Test results are usually slightly worse than training results. Did you use the default setting?,test usually slightly worse training use default setting,issue,negative,negative,negative,negative,negative,negative
512583958,"> Real images need to share similar content with paintings.

This is what I cannot digest and understand. What does it mean similar content?  

Should I put in one folder 1000 images from Cointelegraph, and in second one like 1000 random images of people and objects? ",real need share similar content digest understand mean similar content put one folder second one like random people,issue,positive,negative,negative,negative,negative,negative
512521850,"@taesungp might be able to answer your question.
",might able answer question,issue,negative,positive,positive,positive,positive,positive
512521423,You can implement a custom loading function in your custom class and call the function during training.,implement custom loading function custom class call function training,issue,negative,neutral,neutral,neutral,neutral,neutral
512520369,You may want to use a smaller generator and a smaller discriminator for low-resolution images. You can remove a few layers from each model. ,may want use smaller generator smaller discriminator remove model,issue,negative,neutral,neutral,neutral,neutral,neutral
512519772,"As you don't have pairs, you need to use CycleGAN. Follow the [instruction](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#cyclegan-traintest). You need to prepare two folders. One contains real images. The other contains paintings. Real images need to share similar content with paintings. ",need use follow instruction need prepare two one real real need share similar content,issue,negative,positive,positive,positive,positive,positive
512222010,"It did work, thank you. I have previously checked for similar issues, I missed this one, though.",work thank previously checked similar one though,issue,negative,negative,neutral,neutral,negative,negative
512216159,"When I was training GAN without learning rate decay, I used to define it in this way:

```
checkpoint_name = 'checkpoint'
num_epoch_to_do = 15
# The total number of epochs done in previous sessions - 45 means 3 sessions x 15 epochs
previous_num_of_epochs = 45  

os.putenv(""CHECKPOINT"", str(checkpoint_name))
os.putenv(""EPOCH_COUNT"", str(previous_num_of_epochs + 1))
os.putenv(""NITER"", str(previous_num_of_epochs + num_epoch_to_do))
```
These environment variables would then be passed as arguments when calling the `train.py`

Unfortunately, I don't think it's possible to train with learning decay in such a fashion and I had to find another way. I eventually came up with the idea of using timeout of subprocess. The solution looks like this:
```
import subprocess

cmd = 'python train.py \
       --niter $NITER --niter_decay $NITERDECAY --dataset_mode aligned \
       --dataroot ../dataset/ --model pix2pix \
       --name $CHECKPOINT --continue_train --epoch_count $EPOCH_COUNT > ../out.txt'

seconds = 30000  # ~8.5 hours, 9 hours is kaggle's limit

try:
    subprocess.run(cmd, shell=True, timeout=seconds)
except subprocess.TimeoutExpired:
    print('Process stopped before the time limit')
```

The process will be run and it's output will be saved to out.txt. Once the process is terminated, I check what was the last epoch to be ran completely before the termination and use it to define the `epoch_count` of the next session. ",training gan without learning rate decay used define way total number done previous session session niter environment would calling unfortunately think possible train learning decay fashion find another way eventually came idea solution like import niter niter model name limit try except print stopped time limit process run output saved process check last epoch ran completely termination use define next session,issue,positive,negative,neutral,neutral,negative,negative
512085808,"thanks  a lot. I found it.  Is there any direct way to load data and set model? Since in my case, I need to load four directories and distribute them to generator and discriminators. 
Thanks a lot 
",thanks lot found direct way load data set model since case need load four distribute generator thanks lot,issue,positive,positive,positive,positive,positive,positive
511914404,"Yes, it can be considered as paired data. See some recent work on pose transfer [1](https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf), [2](https://arxiv.org/abs/1804.07739).",yes considered paired data see recent work pose transfer,issue,negative,neutral,neutral,neutral,neutral,neutral
511892339,It might be caused by visdom according to this [post](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/619). Could you disable visdom by setting `--display_id 0` and try it again?,might according post could disable setting try,issue,negative,neutral,neutral,neutral,neutral,neutral
511890889,"Please use `--niter` and `--niter_decay`. See the [code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L30) for more details. `--epoch_count` is for a different purpose (i.e., continue training).",please use niter see code different purpose continue training,issue,negative,neutral,neutral,neutral,neutral,neutral
511836976,"The create functions are defined in the `__init__.py` file for every directory, it's good to have a look in there.",create defined file every directory good look,issue,positive,positive,positive,positive,positive,positive
511814561,"> Could you try to remove `.DS_store` and use `python datasets/combine_A_and_B.py --fold_A ./datasets/combine/A --fold_B ./datasets/combine/B --fold_AB ./datasets/combine/AB`?

Thanks a lot. I searched the keyword '.DS_store' and got understand what happened.

### Solution

- Go to ./datasets/combine/A, use '`find . -name '*.DS_Store' -type f -delete`' to delete it.
- Go to ./datasets/combine/B, use the command above again.
- Back to /pytorch-CycleGAN-and-pix2pix/, run  `python datasets/combine_A_and_B.py --fold_A ./datasets/combine/A --fold_B ./datasets/combine/B --fold_AB ./datasets/combine/AB`

then I got what I want.",could try remove use python thanks lot got understand solution go use find delete go use command back run python got want,issue,positive,positive,neutral,neutral,positive,positive
511797645,"On my second trial, it stopped at 186th epoch.",second trial stopped th epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
511638242,"Thank you so much for your reply. After receiving your answer, I check my CycleGAN code in my computer and here. It's seem like I rewrote the input of D in CycleGAN using for a paired transfer task a long time ago and I forgot it. Sorry to bother you.
Still about question 2, is that the same person in same suit but different pose can be thought as a pair and use conditioned D? (Follow you kind advice I read the original pix2pix paper but I'm not sure is it the same condition with pix2pix.)",thank much reply answer check code computer seem like input paired transfer task long time ago forgot sorry bother still question person suit different pose thought pair use conditioned follow kind advice read original paper sure condition,issue,positive,positive,positive,positive,positive,positive
511478699,Could you try to remove `.DS_store` and use `python datasets/combine_A_and_B.py --fold_A ./datasets/combine/A --fold_B ./datasets/combine/B --fold_AB ./datasets/combine/AB`?,could try remove use python,issue,negative,neutral,neutral,neutral,neutral,neutral
511476923,"0 and 1: the code is consistent with the CycleGAN paper. We don't use conditional D for CycleGAN (as we don't have pairs). We use conditional D for pix2pix. Please refer to the original pix2pix paper for why conditional D is used in pix2pix.
2. It depends on your training data. If you don't have pairs, you cannot use conditional D. If you have pairs, you can use it and it may help.",code consistent paper use conditional use conditional please refer original paper conditional used training data use conditional use may help,issue,positive,positive,positive,positive,positive,positive
511475823,You should use the same setting. I am not sure what happened in your case.,use setting sure case,issue,negative,positive,positive,positive,positive,positive
511475469,"70 epochs might be enough. The metric is application-specific. As you have mentioned, there is no single metric that works for all the datasets and tasks.",might enough metric single metric work,issue,negative,negative,neutral,neutral,negative,negative
511375120,"@junyanz A validation set would be quiet perfect. However, I do not see how to validate these images, based on which attribute? The semantic preservation, the optical impression, based on pixel values (some kind of similarity measurement) or the ability of an object detector... There are many different possibilities and it is not clear which is the best point to stop. 

I am working with 17k images in domain A and 20k images in domain B. There are a lot of images. Until now, I trained over ~70 epochs and it took around 7 days.  ",validation set would quiet perfect however see validate based attribute semantic preservation optical impression based kind similarity measurement ability object detector many different clear best point stop working domain domain lot trained took around day,issue,positive,positive,positive,positive,positive,positive
511373985,"In both cases, I used the default settings **AtoB** Should I stop training and continue from the last checkpoint with the direction **BtoA**, or do I have to restart the training? 

I am confused about the results of the images, it just seems to be not correct and I do not get why. 
As you can see in the attached screenshot (night scenarios), the recovered image looks different than the translated image which is not reproducible for me.... 

@junyanz  ",used default stop training continue last direction restart training confused correct get see attached night image different image reproducible,issue,negative,negative,negative,negative,negative,negative
510935442,You can use a larger lambda for identity loss and cycle-consistenty loss.  Another trick is to use smaller patches for training. ,use lambda identity loss loss another trick use smaller training,issue,negative,neutral,neutral,neutral,neutral,neutral
510934607,I am not sure.  It might be possible that you accidentally flipped the directions of the models. Could you check the `--direction` flag and check if it is the same during training and test. ,sure might possible accidentally could check direction flag check training test,issue,negative,positive,positive,positive,positive,positive
510933308,"If you have a validation set and a task-specific metric, you can pick up the best epoch based on the performance on the validation set. Often times, we don't have that. For many of our applications in the paper and for a middle-size dataset (e.g., a few hundreds of images or 1000 images), we just fix the number of epochs as 200, which might not be the optimal epoch. But if you have a large dataset, you may want to reduce the number of epochs.",validation set metric pick best epoch based performance validation set often time many paper fix number might optimal epoch large may want reduce number,issue,positive,positive,positive,positive,positive,positive
510892876,"@junyanz Thanks for your fast response and your ideas! All in all, very helpful. 

However, I am still unsure how to interpret these results. I assume **real_A** and **real_B** are the original images, **fake_B** and **fake_A** tha translated images in the other domain and **rec_A** and **rec_B** the recovered images (back-translated from the translation), right? 

The translations look all in all very good, without errors (even for the night images). That's also a point I do not get - how can the translator generate night images if domain B does not contain night images? Furthermore, the original image seems to be very similar to the translated image, but the back translated image looks completely different.  

Earlier, I tried to use a tensorflow implementation and I got really poor results (the orignal night images were completely messed up with random artefacts) and as already mentioned your results look very similar (like nothing has changed). How can you explain it?  ",thanks fast response helpful however still unsure interpret assume original tha domain translation right look good without even night also point get translator generate night domain contain night furthermore original image similar image back image completely different tried use implementation got really poor orignal night completely random already look similar like nothing explain,issue,positive,positive,positive,positive,positive,positive
510884261,"@junyanz  I am facing the same issue, when is the right point to stop the training. When do you stop the CycleGAN training? Do you have any criteria? Do you know other approaches/papers when a CycleGAN training should be stopped? 

I trained your model for 45 epochs and I am still not sure if it is enough or not. Should I look at the loss curves and stop when the loss is (more or less) converged against a certain value? Should I focus more on Generator or Discriminator loss? 
There is a lot to consider and it is probably still a large research topic... 

",facing issue right point stop training stop training criterion know training stopped trained model still sure enough look loss stop loss le certain value focus generator discriminator loss lot consider probably still large research topic,issue,negative,positive,positive,positive,positive,positive
510721465,"Based what u discussed above, I run the commands as the following and it works to me:
         (1)run the python script in a new terminal:
` python -m visdom.server -port=15024`
             plues:if it prints error,you can look through all ports by running the command 'netstat -antu' and then make sure whether the port '15024' has been taken)
         (2)if u run the first command successfully , u can get the imformation just like this 
![Capture](https://user-images.githubusercontent.com/27773249/61097878-78793a00-a48f-11e9-8b72-205cc0ce0513.JPG)
              then u can see the result after copying the website in the broswer :
              http://30-0001-21910:8097/
![333](https://user-images.githubusercontent.com/27773249/61098117-6cda4300-a490-11e9-9113-475370d3e509.JPG)





",based run following work run python script new terminal python error look running command make sure whether port taken run first command successfully get like capture see result,issue,positive,positive,positive,positive,positive,positive
510210182,"0) Maybe you can modify this [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L62) in this line. You can rewrite the get_image_paths.
1) We have a recent [paper](https://arxiv.org/pdf/1711.03213.pdf) related to your application. See Sec 6.1.2 for the parameter configuration.
2) I am not sure why. 
3) You can evaluate your model using some (conditional) GANs metrics or downstream tasks. See the original cyclegan paper and this [paper](https://arxiv.org/pdf/1711.03213.pdf) for more details.",maybe modify function line rewrite recent paper related application see sec parameter sure evaluate model conditional metric downstream see original paper paper,issue,positive,positive,positive,positive,positive,positive
510005262,"@junyanz Ah I got confused because in the repo (here on Github) it is described as `preprocess` and in your docker image I use, there is still `resize_or_crop` (maybe you should update it). 
Now, the results have the desired resolution, I think I added the `--fineSize` and `--loadSize` parameters during testing by mistake. However, I am not sure how to interpret the results. 

I get the following html document: 
![image](https://user-images.githubusercontent.com/45428272/60961155-c9f8bc00-a30b-11e9-9d7d-81d0de13fa6b.png)

Left three images represent the domain A, the most right images represent domain B. 

I am only interested in the translation from **B to A**. Do you provide a way how to save the translated images into separate folders or (at least) that the image names are preserved? As you can see in the attached image, only the image names from domain B are set. Therefore, it is hard to separate the images (in my case, I translate 10k images).  

Another point particular on **this use case scenario**: 
**1)** Do you recommend a special **hyperparameter configuration** for the translation of **road scene images** (Cityscapes vs Berkeley Deep Drive)? Learning rate? Number of epochs if learning starts from scratch? Any experiences?
**2)** Image line 2 or 3: Why is the **night scenario** preserved within the translation from domain B (Berkeley contains night images) to domain A (Cityscapes does not contain night images) but the recovered image does not show a night scene? Any hints?
**3)** How to check when the model did converge (when the training is finished)?

A lot of open questions but maybe you have some experience and can give some remarks/hints. 
Thanks a lot in advance.",ah got confused docker image use still maybe update desired resolution think added testing mistake however sure interpret get following document image left three represent domain right represent domain interested translation provide way save separate least image see attached image image domain set therefore hard separate case translate another point particular use case scenario recommend special configuration translation road scene deep drive learning rate number learning scratch image line night scenario within translation domain night domain contain night image show night scene check model converge training finished lot open maybe experience give thanks lot advance,issue,positive,positive,neutral,neutral,positive,positive
509910517,"Yeah, its working now, even with `--load_size 1024`, I thought giving `--load_size` and `--crop_size` arguments would overwrite the `--preprocess` argument!
Thanks for the quick reply!",yeah working even thought giving would overwrite argument thanks quick reply,issue,positive,positive,positive,positive,positive,positive
509760675,It should be fine as long as you set the `--epoch_count` correctly. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L27) for more details. The learning decay will consider the `epoch_count`. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/41931e25c7d12e0ff2fcea4ee7ba2e597769e6f2/models/networks.py#L53).,fine long set correctly see learning decay consider see line,issue,negative,positive,positive,positive,positive,positive
509754304,Oh! I missed that parameter when looking through the fie. Thanks a lot @junyanz ,oh parameter looking fie thanks lot,issue,negative,positive,positive,positive,positive,positive
509750669,"Your current setting is different from what you have described. Given the logging information, your setting is `--preprocess none --load_size 1000 --crop_size 256`.

Try to change it to `--preprocess resize_and_crop --load_size 286 --crop_size 256`.",current setting different given logging information setting none try change,issue,negative,neutral,neutral,neutral,neutral,neutral
509744456,"Please see these [comments](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L54) for more details. There is an option called `--eval`. By default, the code will not use `--eval` so that the code only uses the statistics in the test batch. You can change the behavior by adding `--eval` in your command line.",please see option default code use code statistic test batch change behavior command line,issue,negative,neutral,neutral,neutral,neutral,neutral
509742412,Both UNet and ResNet should work. See some recent work such as [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE) for recent development in network architecture. ,work see recent work spade recent development network architecture,issue,negative,neutral,neutral,neutral,neutral,neutral
509741806,Did you use `--preprocess none` during test time? (Note: `--resize_or_crop` has been changed to `--preprocess`),use none test time note,issue,negative,neutral,neutral,neutral,neutral,neutral
509741274,Please use `--epoch`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L53) for more details.,please use epoch see,issue,negative,neutral,neutral,neutral,neutral,neutral
509641930,"Training Cycle Gans
Same thing, I am using unet_128, I tried using it with `256 crop size` and `128 crop size` both shows the error `RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 36 and 37 in dimension 2 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:71`

and with resnet_6blocks I get:
```
RuntimeError: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 11.75 GiB total capacity; 10.57 GiB already allocated; 34.50 MiB free; 71.22 MiB cached)
```

while using resnet_6blocks, I use the crop_size of, with my 12 GB GPU, it should work right?

Complete Traceback:

```
(base) tanay_curl@curlbeast128:~/pytorch-CycleGAN-and-pix2pix$ python train.py --dataroot ../data --crop_size 256 --netG resnet_6blocks --load_size 1000 --preprocess none --batch_size 1 --netD n_layers --n_layers_D 3
----------------- Options ---------------
               batch_size: 1                             
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 dataroot: ../data                       	[default: None]
             dataset_mode: unaligned                     
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: lsgan                         
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                 lambda_A: 10.0                          
                 lambda_B: 10.0                          
          lambda_identity: 0.5                           
                load_iter: 0                             	[default: 0]
                load_size: 1000                          	[default: 286]
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: cycle_gan                     
               n_layers_D: 3                             
                     name: experiment_name               
                      ndf: 64                            
                     netD: n_layers                      	[default: basic]
                     netG: resnet_6blocks                	[default: resnet_9blocks]
                      ngf: 64                            
                    niter: 100                           
              niter_decay: 100                           
               no_dropout: True                          
                  no_flip: False                         
                  no_html: False                         
                     norm: instance                      
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 50                            
               preprocess: none                          	[default: resize_and_crop]
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                  verbose: False                         
----------------- End -------------------
dataset [UnalignedDataset] was created
The number of training images = 352
initialize network with normal
initialize network with normal
initialize network with normal
initialize network with normal
model [CycleGANModel] was created
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 7.838 M
[Network G_B] Total number of parameters : 7.838 M
[Network D_A] Total number of parameters : 2.765 M
[Network D_B] Total number of parameters : 2.765 M
-----------------------------------------------
WARNING:root:Setting up a new session...
create web directory ./checkpoints/experiment_name/web...
The image size needs to be a multiple of 4. The loaded image size was (2550, 3301), so it was adjusted to (2552, 3300). This adjustment will be done to all images whose sizes are not multiples of 4
The image size needs to be a multiple of 4. The loaded image size was (2481, 3508), so it was adjusted to (2480, 3508). This adjustment will be done to all images whose sizes are not multiples of 4
The image size needs to be a multiple of 4. The loaded image size was (2481, 3509), so it was adjusted to (2480, 3508). This adjustment will be done to all images whose sizes are not multiples of 4
The image size needs to be a multiple of 4. The loaded image size was (2481, 3508), so it was adjusted to (2480, 3508). This adjustment will be done to all images whose sizes are not multiples of 4
Traceback (most recent call last):
  File ""train.py"", line 51, in <module>
    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
  File ""/home/tanay_curl/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py"", line 183, in optimize_parameters
    self.forward()      # compute fake images and reconstruction images.
  File ""/home/tanay_curl/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py"", line 115, in forward
    self.rec_A = self.netG_B(self.fake_B)   # G_B(G_A(A))
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 150, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/tanay_curl/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 373, in forward
    return self.model(input)
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 92, in forward
    input = module(input)
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/tanay_curl/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 432, in forward
    out = x + self.conv_block(x)  # add skip connections
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 92, in forward
    input = module(input)
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/modules/instancenorm.py"", line 55, in forward
    self.training or not self.track_running_stats, self.momentum, self.eps)
  File ""/home/tanay_curl/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py"", line 1713, in instance_norm
    use_input_stats, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 11.75 GiB total capacity; 10.57 GiB already allocated; 34.50 MiB free; 71.22 MiB cached)

```
",training cycle thing tried crop size crop size error invalid argument size must match except dimension got dimension get memory tried allocate mib gib total capacity gib already mib free mib use work right complete base python none beta false default none unaligned direction main epoch latest normal true default none default default linear model name default basic default niter true false false norm instance phase train none default false false suffix verbose false end number training initialize network normal initialize network normal initialize network normal initialize network normal model network total number network total number network total number network total number warning root setting new session create web directory image size need multiple loaded image size adjustment done whose size image size need multiple loaded image size adjustment done whose size image size need multiple loaded image size adjustment done whose size image size need multiple loaded image size adjustment done whose size recent call last file line module calculate loss get update network file line compute fake reconstruction file line forward file line result input file line forward return file line result input file line forward return input file line result input file line forward input module input file line result input file line forward add skip file line result input file line forward input module input file line result input file line forward file line momentum memory tried allocate mib gib total capacity gib already mib free mib,issue,positive,negative,neutral,neutral,negative,negative
509634880,"@junyanz @taesungp I tried it as described above. However, the output image are still quadratic. 
Any ideas how to solve it? The train and test images are size 512x256 and the translated images should be the same image size and not quadratic.. 
 ",tried however output image still quadratic solve train test size image size quadratic,issue,negative,neutral,neutral,neutral,neutral,neutral
509593043,Thanks a lot for the reply sir. :),thanks lot reply sir,issue,negative,positive,positive,positive,positive,positive
509474265,Two ways: (1) you can write a custom data loader. Please follow this [template](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py). (2) you can write a pre-processing script to crop objects using the bounding box offline before you run our training code.,two way write custom data loader please follow template write script crop bounding box run training code,issue,negative,neutral,neutral,neutral,neutral,neutral
509469822,Good catch. I just updated the code. Thank you.,good catch code thank,issue,positive,positive,positive,positive,positive,positive
509150529,Can you please explain me how we can add pedestrian bounding box location from xml along with the images used for training cyclegan?,please explain add pedestrian bounding box location along used training,issue,negative,neutral,neutral,neutral,neutral,neutral
509134351,"If your code is running in colab, and you are using visdom in local machine, it won't actually work!",code running local machine wo actually work,issue,negative,neutral,neutral,neutral,neutral,neutral
508999659,"> Hi @junyanz
> I want to make sure that I am trying CycleGAN with WGAN-GP loss in the same setup and with the same configuration you tried it.
> 
> This is my modification inspired by #439 and [VON](https://github.com/junyanz/VON/blob/master/models/shape_gan_model.py#L80) on **cycle_gan_model.py**:
> 
> ```
> def backward_D_basic(self, netD, real, fake):
>         """"""Calculate GAN loss for the discriminator
> 
>         Parameters:
>             netD (network)      -- the discriminator D
>             real (tensor array) -- real images
>             fake (tensor array) -- images generated by a generator
> 
>         Return the discriminator loss.
>         We also call loss_D.backward() to calculate the gradients.
>         """"""
>         # Real
>         pred_real = netD(real)
>         loss_D_real = self.criterionGAN(pred_real, True)
>         # Fake
>         pred_fake = netD(fake.detach())
>         loss_D_fake = self.criterionGAN(pred_fake, False)
>         # wgan-gp
>         gradient_penalty, gradients = networks.cal_gradient_penalty(
>             netD, real, fake, self.device,lambda_gp=10.0
>         )
>         gradient_penalty.backward(retain_graph=True)
>         # Combined loss and calculate gradients
>         loss_D = (loss_D_real + loss_D_fake) * 0.5
>         loss_D.backward()
>         return loss_D
> ```
> 
> and to start training
> 
> ```
> python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --gan_mode wgangp --norm instance 
> ```
> 
> Is this all? or should I take care of other things?
> Is this the same setup you tried ?

Hi @moh3th1 .
I wonder if your wgangp really works. If I use the original configuration, my networks just diverge. So, do you have some suggestions for hyperparameters of wgangp?

But, it seems that something needs to be done for the wgan model?",hi want make sure trying loss setup configuration tried modification inspired self real fake calculate gan loss discriminator network discriminator real tensor array real fake tensor array generator return discriminator loss also call calculate real real true fake false real fake combined loss calculate return start training python name model norm instance take care setup tried hi wonder really work use original configuration diverge something need done model,issue,negative,negative,negative,negative,negative,negative
508645846,"Aaah I see, so then I could to evaluation on those data generated using validation set.

Thank you for the advice",see could evaluation data validation set thank advice,issue,negative,neutral,neutral,neutral,neutral,neutral
508523929,"Validation set, as in 
splitting the data into train-val-test, then using the val set on certain iteration steps to see samples from the model?",validation set splitting data set certain iteration see model,issue,negative,positive,positive,positive,positive,positive
508521296,I am not aware. You can also use GPU cloud service if you don't want to buy a new GPU.,aware also use cloud service want buy new,issue,negative,positive,positive,positive,positive,positive
508520957,I am not sure why. You can look at existing CycleGAN Keras [implementation](https://github.com/eriklindernoren/Keras-GAN/blob/master/cyclegan/cyclegan.py) for a reference.,sure look implementation reference,issue,negative,positive,positive,positive,positive,positive
508519017,I ran it with load_size 128 - I know this will very less frame size. But a quick question is it worth to do that efforts.?,ran know le frame size quick question worth,issue,negative,positive,positive,positive,positive,positive
508518412,"The program requires more than 2 GB GPU. It seems that your GPU has 2 GB, which is not sufficient to run the program.",program sufficient run program,issue,negative,neutral,neutral,neutral,neutral,neutral
508411099,"I see, thank you very much for your advice",see thank much advice,issue,negative,positive,positive,positive,positive,positive
507493433,"> It seems that you haven't calculated loss_G_A. You can either remove 'G_A' from visual_names or calculate the loss_G_A.

Thans very much. I have a problem, the number of images under certain distortion type is relatively small. Can I mix images with some different distortion types to generate distortion images?  Do these distortion types need to be similar? such as global distortion or local distortion.",calculated either remove calculate much problem number certain distortion type relatively small mix different distortion generate distortion distortion need similar global distortion local distortion,issue,negative,positive,neutral,neutral,positive,positive
507348819,It seems that you haven't calculated loss_G_A. You can either remove 'G_A' from visual_names or calculate the loss_G_A.,calculated either remove calculate,issue,negative,neutral,neutral,neutral,neutral,neutral
506943449,"> It depends on the `crop_size`. If your `crop_size` is 128, the input to D is 128*128.

Thanks, could you help me? I want to add the perceptual loss of VGG to cyclegan method. However, I find this error is presented. I change the codes of network.py and cycle_gan.py.



 
![A](https://user-images.githubusercontent.com/32053722/60382252-1124b880-9a93-11e9-9f9b-f592dd56d2d0.png)
",input thanks could help want add perceptual loss method however find error change,issue,negative,positive,positive,positive,positive,positive
505714311,"> In your case, 360x360 might not be enough. Another idea would be to train a separate generator for each facial component. See this [paper](https://adoberesearch.ctlprojects.com/wp-content/uploads/2018/04/CVPR2018_Paper3623_Chang.pdf) for more details.

Amazing Idea!",case might enough another idea would train separate generator facial component see paper amazing idea,issue,positive,positive,positive,positive,positive,positive
505700491,"yes, i find this code, thank you very much, i will close this issue soon.   sorry to bother you.",yes find code thank much close issue soon sorry bother,issue,negative,negative,negative,negative,negative,negative
505583182,"The program will apply the same cropping to both input and output images. (x, y, width, height)",program apply input output width height,issue,negative,neutral,neutral,neutral,neutral,neutral
505582916,"In your case, 360x360 might not be enough. Another idea would be to train a separate generator for each facial component. See this [paper](https://adoberesearch.ctlprojects.com/wp-content/uploads/2018/04/CVPR2018_Paper3623_Chang.pdf) for more details.",case might enough another idea would train separate generator facial component see paper,issue,negative,neutral,neutral,neutral,neutral,neutral
505574954,"1. use `_B` models.
2. Yes. The code will save intermediate checkpoints every 5 epoch.",use yes code save intermediate every epoch,issue,positive,neutral,neutral,neutral,neutral,neutral
505573811,"We apply the same cropping (x, y, height, width) to the input and output images. They are still matched. ",apply height width input output still,issue,negative,neutral,neutral,neutral,neutral,neutral
505443433,"I have a question about the pix2pix, with the default set,  the preprocess is resize_and_crop with param of 286 and 256.    So after crop the paired images is not matched ,  is some thing wrong with my understanding?   Thank you very much.",question default set param crop paired thing wrong understanding thank much,issue,negative,negative,negative,negative,negative,negative
505356359,"Hi @junyanz  ,if the training data is human face with the size 1024*1024, is it okay to crop it with 360*360 ? What will happened if crops the part of one face(eg, one mouth or one eye)?",hi training data human face size crop part one face one mouth one eye,issue,negative,neutral,neutral,neutral,neutral,neutral
505014957,"@junyanz I have `latest_net_{D/G}_{A/B}.pth
`
If I want to translate images in testB folder (to domain A), which` .pth` file should be renamed to `latest_net_G.pth` ? 

Does latest mean the last epoch? an furthermore every 5th epoch another model is output? 

maybe @phyuphyuthaw or @omg777 figured out which model must be renamed? 

Thanks in advance and kind regards ",want translate folder domain file latest mean last epoch furthermore every th epoch another model output maybe figured model must thanks advance kind,issue,positive,positive,positive,positive,positive,positive
504720111,"Yeah, I think instance norm with a big batch size should be fine. Instance norm itself shouldn't be affected by batch size.",yeah think instance norm big batch size fine instance norm affected batch size,issue,negative,positive,positive,positive,positive,positive
504709730,"You don't need to modify the dataloader. You can just add a pre-processing function in the training code, for example, this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L94).",need modify add function training code example line,issue,negative,neutral,neutral,neutral,neutral,neutral
504709068,"Unfortunately, it is not available. But feel free to train these models with our code on the provided dataset. ",unfortunately available feel free train code provided,issue,negative,positive,positive,positive,positive,positive
504708989,It is not supported in the current codebase. You can modify the `test.py` accordingly. You can write code to read and save images using `--input` and `--output`.,current modify accordingly write code read save input output,issue,negative,neutral,neutral,neutral,neutral,neutral
504708843,"It's hard to tell. Could you try batch_size=1? Also, not sure if you can transfer a table into a randomly generated skeleton. The quality of the translation will depend on how you generate random skeletons.",hard tell could try also sure transfer table randomly skeleton quality translation depend generate random,issue,negative,negative,negative,negative,negative,negative
504704529,"@phillipi Instancenorm with a big batch size should be fine? Also, make sure that you use the same batch_size during training and test. By default, the test code will use batch_size=1 (hard-coded). You can change it in the code.",big batch size fine also make sure use training test default test code use change code,issue,negative,positive,positive,positive,positive,positive
504704441,I am not sure. Maybe you can trace the size of the tensor line-by-line.,sure maybe trace size tensor,issue,negative,positive,positive,positive,positive,positive
504704248,It seems to be not relevant to our code. Maybe you need to install additional cooling systems. See this [post](https://forums.fast.ai/t/gpu-cooling-solution/10110) for more details.,relevant code maybe need install additional cooling see post,issue,negative,positive,positive,positive,positive,positive
504704089,"The generator in the current codebase assumes that the input and output have the same size. You can modify the generator's code for your own purpose/sizes (e.g., adding or removing one downsampling/upsampling layer)",generator current input output size modify generator code removing one layer,issue,negative,neutral,neutral,neutral,neutral,neutral
504476850,"When downloading from [http://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh](http://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh), how it is specified in the [Dockerfile](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/Dockerfile), Miniconda with python2 automatically gets installed, which does not work with the needed dependencies.

I could solve this issue by replacing `Miniconda-latest-Linux-x86_64.sh` with `Miniconda3-latest-Linux-x86_64.sh` everywhere in the [Dockerfile](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/Dockerfile) and then building the image again. This downloads the latest Miniconda version with python3.",python automatically work could solve issue everywhere building image latest version python,issue,negative,positive,positive,positive,positive,positive
504440439,"Looks like it just changes the suffix of the latest cached model filename to indicate which iteration it was saved on: [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/656b9322f3db490149e9261b3742d6dbb8166c30/train.py#L67) (as opposed to just saying 'latest')
",like suffix latest model indicate iteration saved opposed saying,issue,positive,positive,positive,positive,positive,positive
504293561,"Yeah the dropout doesn't really matter much for performance. It has a very minor effect. One could argue about whether or not deterministic mappings count as ""generative models"". It's true that it does not model a _distribution_ of outputs, instead it just gives a single guess.

> And if pix2pix without dropout outperforms a U-Net based Autoencoder, can we think in this way: pix2pix without dropout is better because of the powerful discriminator loss. Is my understanding correct?

Yep, I think that's a good way to think about it.",yeah dropout really matter much performance minor effect one could argue whether deterministic count generative true model instead single guess without dropout based think way without dropout better powerful discriminator loss understanding correct yep think good way think,issue,negative,positive,positive,positive,positive,positive
504104363,"Hey @gm039. Could you please check the version of the python you are using? I had similar problem before, since I was using the python2. After the updating to python3, the problem was fixed. ",hey could please check version python similar problem since python python problem fixed,issue,negative,positive,neutral,neutral,positive,positive
504010803,"> Without noise, the mapping is deterministic, but that's often fine

Hi, thanks for the sharing but about the deterministic mapping, I have a doubt: 
I have tried to implement pix2pix on another dataset (where the input image is complex enough), so I do not apply dropout at all, and the result looks just as fine. But the problem is, without noise as input, can we even call it a generative model? It looks like a discriminative model (U-Net based Autoencoder) + the discriminator loss. 

And if pix2pix without dropout outperforms a U-Net based Autoencoder, can we think in this way: pix2pix without dropout is better because of the powerful discriminator loss. Is my understanding correct?

Thanks a lot!
",without noise deterministic often fine hi thanks deterministic doubt tried implement another input image complex enough apply dropout result fine problem without noise input even call generative model like discriminative model based discriminator loss without dropout based think way without dropout better powerful discriminator loss understanding correct thanks lot,issue,negative,positive,positive,positive,positive,positive
503993495,"@junyanz (and everybody who might have the same problem)
Start a docker container as described in the repo. In a second terminal window, you should run `docker exec -it <image name> bash`. There it is possible to run the `python -m visdom.server` command as described and it works as expected.  ",everybody might problem start docker container second terminal window run docker image name bash possible run python command work,issue,negative,neutral,neutral,neutral,neutral,neutral
503385275,"just like this:
python test.py --model ### \
                     --input input_sample.jpg \
                     --output output_sample.jpg \
                     --image_size 256",like python model input output,issue,negative,neutral,neutral,neutral,neutral,neutral
503070998,"@junyanz @Makhaon 
Could you give some explainment for this batch size issue.

I am facing same question in pix2pix algorithm. If I improve the batch_size up to 16 or 32 in facade training dataset, the results would be very blurring and the images' qualities would be bad, for example the window's glass would be not clear.

Same situation if I change the instance normalziation to batch normalization as well as the big batch size.

If I want to improve the batch size, what I need to do?

Thanks very much.",could give batch size issue facing question algorithm improve facade training would would bad example window glass would clear situation change instance batch normalization well big batch size want improve batch size need thanks much,issue,positive,negative,neutral,neutral,negative,negative
502996890,"I'm not sure, but I found something missing in the [Dockerfile](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/39e6060451c56f56578fd1fb2c3711435fe738a9/docs/Dockerfile).

First of all, [`Miniconda-latest-Linux-x86_64.sh`](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/656b9322f3db490149e9261b3742d6dbb8166c30/docs/Dockerfile#L4) should be replace to `Miniconda3-latest-Linux-x86_64.sh` based on the [Miniconda installer archive](https://repo.continuum.io/miniconda/)

Second, missed a `RUN conda env create -f environment.yml` in the Dockerfile and do more things, or just do it if you download `Miniconda3`.",sure found something missing first replace based installer archive second run create,issue,negative,positive,positive,positive,positive,positive
502990733,"@yinqk I'm also facing the same problem. How did you solve it?
",also facing problem solve,issue,negative,neutral,neutral,neutral,neutral,neutral
502832418,"> That will require tweaking the generator architecture a bit.
> 
> The simplest way would be creating the generator to work at the higher resolution, and use `torch.nn.funtional.interpolate` to the generated output to downsample the image to match the lower resolution.

What if resizing/downsampling is not an option? (in my case it is important to retain the exact values at the same size/resolution for the respective domain) Does the Generator inherently rely on the fact that the inputs/outputs are the same size? ",require generator architecture bit way would generator work higher resolution use output image match lower resolution option case important retain exact respective domain generator inherently rely fact size,issue,negative,positive,positive,positive,positive,positive
502579987,"When I also try to train using the docker image, I have the same problem as @Peetee06 .
He is right and I solved this problem to install python3 in the container.

Fix: a problem occurs like `from scipy.misc import imresize` if I install with requirements.txt, I reinstall scipy from `1.3.0` to `1.1.0`.",also try train docker image problem right problem install python container fix problem like import install reinstall,issue,negative,positive,positive,positive,positive,positive
502320917,"For both training and test modes I see this in the options output:
  netD: basic
  netG: resnet_9blocks",training test see output basic,issue,negative,neutral,neutral,neutral,neutral,neutral
502225670,"> As a side note, we did this because the network only takes the input that is divisible by 4.

I see, is there a particular the inputs must be divisible by 4? Would I easily be able to change this for arbitrary dimensions? For my project I can't afford to resize the arrays",side note network input divisible see particular must divisible would easily able change arbitrary project ca afford resize,issue,negative,positive,positive,positive,positive,positive
502160050,"Have you checked if the data loader reads these maps correctly? Another sanity check is to disable GAN loss and only use L1 loss, and see if you can reasonable results.",checked data loader correctly another sanity check disable gan loss use loss see reasonable,issue,negative,positive,positive,positive,positive,positive
502159047,"I think L1 tries to minimize the distortion of the output w.r.t ground truth. GAN loss tries to increase perceptual realism. This nice paper explains it in more depth: https://arxiv.org/abs/1711.06077
",think minimize distortion output ground truth gan loss increase perceptual realism nice paper depth,issue,positive,positive,positive,positive,positive,positive
502083561,"Thank you for your answer. Sure.
But I want to try the power of GANs for this task. 
I was just surprised that theses dots are exactly (255, 0, 0) , (0,255, 0) and (0, 0, 255). So I was wondering if it was possible to know from which layer it comes. This dots are only present on no data (ie. black). It is like the network cannot learn to map ""noise"" to ""black"" (no data).",thank answer sure want try power task thesis exactly wondering possible know layer come present data ie black like network learn map noise black data,issue,positive,positive,neutral,neutral,positive,positive
502036609,"> Reducing the number of Resnet blocks can make the network's capacity smaller. This might be helpful when your task only involved small and local changes. You don't need a big network for modeling small changes.

Thanks，when debuging the code, I  find the input of D network is 256*256.  Is this correct?",reducing number make network capacity smaller might helpful task involved small local need big network modeling small code find input network correct,issue,negative,negative,neutral,neutral,negative,negative
501985670,Hi @junyanz I thought the L1 loss was indeed responsible for small details since it's a pixel-wise loss. I thought the GAN loss was responsible for the global structure of the generated images. Do you have any reference?,hi thought loss indeed responsible small since loss thought gan loss responsible global structure reference,issue,negative,positive,neutral,neutral,positive,positive
501969733,"Hey, I'm closing this issue for now, for anyone encountering similar error with pix2pix, I resolved it by setting dropout as True. It works with PyTorch 1.1.0. ",hey issue anyone similar error resolved setting dropout true work,issue,negative,positive,positive,positive,positive,positive
501895965,"In Linux,
Env was ok but got additional issues.

```
  /tmp/pip-req-build-kkx3_6tp/torchvision/csrc/cpu/vision.h:2:29: fatal error: torch/extension.h: No such file or directory
  compilation terminated.
  error: command 'gcc' failed with exit status 1
  
  ----------------------------------------
  Failed building wheel for torchvision
  Running setup.py clean for torchvision
  Running setup.py bdist_wheel for tornado ... done
  Stored in directory: /home/ubuntu/.cache/pip/wheels/61/7e/7a/5e02e60dc329aef32ecf70e0425319ee7e2198c3a7cf98b4a2
  Running setup.py bdist_wheel for torchfile ... done
  Stored in directory: /home/ubuntu/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814
Successfully built dominate visdom tornado torchfile
Failed to build torchvision
mkl-random 1.0.1 requires cython, which is not installed.
Installing collected packages: dominate, Pillow, numpy, urllib3, idna, chardet, requests, tornado, pyzmq, six, torchfile, visdom, torchvision
  Found existing installation: numpy 1.15.2
    Uninstalling numpy-1.15.2:
      Successfully uninstalled numpy-1.15.2
  Running setup.py install for torchvision ... error
    Complete output from command /home/ubuntu/anaconda3/envs/pytorch-CycleGAN-and-pix2pix/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-req-build-kkx3_6tp/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-llhxtz8u/install-record.txt --single-version-externally-managed --compile:
    Building wheel torchvision-0.3.0a0+7693c89
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.5
    creating build/lib.linux-x86_64-3.5/torchvision
    copying torchvision/extension.py -> build/lib.linux-x86_64-3.5/torchvision
    copying torchvision/__init__.py -> build/lib.linux-x86_64-3.5/torchvision
    copying torchvision/version.py -> build/lib.linux-x86_64-3.5/torchvision
    copying torchvision/utils.py -> build/lib.linux-x86_64-3.5/torchvision
    creating build/lib.linux-x86_64-3.5/torchvision/transforms
    copying torchvision/transforms/transforms.py -> build/lib.linux-x86_64-3.5/torchvision/transforms
    copying torchvision/transforms/__init__.py -> build/lib.linux-x86_64-3.5/torchvision/transforms
    copying torchvision/transforms/functional.py -> build/lib.linux-x86_64-3.5/torchvision/transforms
    creating build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/fakedata.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/phototour.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/coco.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/cityscapes.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/omniglot.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/sbd.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/sbu.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/svhn.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/voc.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/caltech.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/semeion.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/stl10.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/__init__.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/celeba.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/vision.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/flickr.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/folder.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/lsun.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/usps.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/imagenet.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/mnist.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/cifar.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    copying torchvision/datasets/utils.py -> build/lib.linux-x86_64-3.5/torchvision/datasets
    creating build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/alexnet.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/googlenet.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/inception.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/resnet.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/mobilenet.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/__init__.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/vgg.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/_utils.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/shufflenetv2.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/densenet.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/squeezenet.py -> build/lib.linux-x86_64-3.5/torchvision/models
    copying torchvision/models/utils.py -> build/lib.linux-x86_64-3.5/torchvision/models
    creating build/lib.linux-x86_64-3.5/torchvision/ops
    copying torchvision/ops/roi_pool.py -> build/lib.linux-x86_64-3.5/torchvision/ops
    copying torchvision/ops/roi_align.py -> build/lib.linux-x86_64-3.5/torchvision/ops
    copying torchvision/ops/misc.py -> build/lib.linux-x86_64-3.5/torchvision/ops
    copying torchvision/ops/poolers.py -> build/lib.linux-x86_64-3.5/torchvision/ops
    copying torchvision/ops/feature_pyramid_network.py -> build/lib.linux-x86_64-3.5/torchvision/ops
    copying torchvision/ops/__init__.py -> build/lib.linux-x86_64-3.5/torchvision/ops
    copying torchvision/ops/_utils.py -> build/lib.linux-x86_64-3.5/torchvision/ops
    copying torchvision/ops/boxes.py -> build/lib.linux-x86_64-3.5/torchvision/ops
    creating build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/rpn.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/keypoint_rcnn.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/transform.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/faster_rcnn.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/roi_heads.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/image_list.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/backbone_utils.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/mask_rcnn.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/generalized_rcnn.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/__init__.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    copying torchvision/models/detection/_utils.py -> build/lib.linux-x86_64-3.5/torchvision/models/detection
    creating build/lib.linux-x86_64-3.5/torchvision/models/segmentation
    copying torchvision/models/segmentation/fcn.py -> build/lib.linux-x86_64-3.5/torchvision/models/segmentation
    copying torchvision/models/segmentation/segmentation.py -> build/lib.linux-x86_64-3.5/torchvision/models/segmentation
    copying torchvision/models/segmentation/__init__.py -> build/lib.linux-x86_64-3.5/torchvision/models/segmentation
    copying torchvision/models/segmentation/_utils.py -> build/lib.linux-x86_64-3.5/torchvision/models/segmentation
    copying torchvision/models/segmentation/deeplabv3.py -> build/lib.linux-x86_64-3.5/torchvision/models/segmentation
    running build_ext
    building 'torchvision._C' extension
    creating build/temp.linux-x86_64-3.5
    creating build/temp.linux-x86_64-3.5/tmp
    creating build/temp.linux-x86_64-3.5/tmp/pip-req-build-kkx3_6tp
    creating build/temp.linux-x86_64-3.5/tmp/pip-req-build-kkx3_6tp/torchvision
    creating build/temp.linux-x86_64-3.5/tmp/pip-req-build-kkx3_6tp/torchvision/csrc
    creating build/temp.linux-x86_64-3.5/tmp/pip-req-build-kkx3_6tp/torchvision/csrc/cpu
    creating build/temp.linux-x86_64-3.5/tmp/pip-req-build-kkx3_6tp/torchvision/csrc/cuda
    gcc -pthread -B /home/ubuntu/anaconda3/envs/pytorch-CycleGAN-and-pix2pix/compiler_compat -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA -I/tmp/pip-req-build-kkx3_6tp/torchvision/csrc -I/home/ubuntu/anaconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/lib/include -I/home/ubuntu/anaconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/lib/include/TH -I/home/ubuntu/anaconda3/envs/pytorch-CycleGAN-and-pix2pix/lib/python3.5/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/home/ubuntu/anaconda3/envs/pytorch-CycleGAN-and-pix2pix/include/python3.5m -c /tmp/pip-req-build-kkx3_6tp/torchvision/csrc/vision.cpp -o build/temp.linux-x86_64-3.5/tmp/pip-req-build-kkx3_6tp/torchvision/csrc/vision.o -O0 -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_EXTENSION_NAME=_C_tests -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11
    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
    <command-line>:0:0: warning: ""TORCH_EXTENSION_NAME"" redefined
    <command-line>:0:0: note: this is the location of the previous definition
    In file included from /tmp/pip-req-build-kkx3_6tp/torchvision/csrc/ROIAlign.h:3:0,
                     from /tmp/pip-req-build-kkx3_6tp/torchvision/csrc/vision.cpp:1:
    /tmp/pip-req-build-kkx3_6tp/torchvision/csrc/cpu/vision.h:2:29: fatal error: torch/extension.h: No such file or directory
    compilation terminated.
    error: command 'gcc' failed with exit status 1
    
    ----------------------------------------
Command ""/home/ubuntu/anaconda3/envs/pytorch-CycleGAN-and-pix2pix/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/pip-req-build-kkx3_6tp/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-llhxtz8u/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-req-build-kkx3_6tp/


```",got additional fatal error file directory compilation error command exit status building wheel running clean running tornado done directory running done directory successfully built dominate tornado build collected dominate pillow tornado six found installation successfully uninstalled running install error complete output command import open compile code install record compile building wheel running install running build running build running building extension warning command line option valid warning note location previous definition file included fatal error file directory compilation error command exit status command import open compile code install record compile error code,issue,negative,positive,positive,positive,positive,positive
501880794,@junyanz Thanks a lot for your help. Hope @taesungp could give me some hints for this.. ,thanks lot help hope could give,issue,positive,positive,positive,positive,positive,positive
501777638,"Alright, correct me if I am wrong. From what I understood, in the first solution given, we need to crop images from the real dataset from its bounding box so that the images are cropped up or (ii) as mentioned in the second solution, the size of the drones in the simulated environment must match the size of the real drone. 
Let me know if this is what you meant, thanks alot for your advice. ",alright correct wrong understood first solution given need crop real bounding box second solution size environment must match size real drone let know meant thanks advice,issue,positive,positive,neutral,neutral,positive,positive
501775189,You can write your own custom data loader. Here is a [template](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) with documentation. I recommend that you write a standalone python script to test your saving/loading/transformation function before adding them to the main program.,write custom data loader template documentation recommend write python script test function main program,issue,negative,positive,positive,positive,positive,positive
501773316,"Another thing that you can do is for your simulation environment, you adjust the camera parameters so that your CG drone has the same size as the real drone.",another thing simulation environment adjust camera drone size real drone,issue,negative,positive,positive,positive,positive,positive
501773155,"Thanku so much 

Sent from Yahoo Mail on Android 
 
  On Thu, 13 Jun 2019 at 9:46 pm, Jun-Yan Zhu<notifications@github.com> wrote:   
I haven't used the googlecolab so that I cannot give you too many suggestions. The program should tell you the training time per epoch, and you can calculate the total training time from that.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.
  
",much sent yahoo mail android wrote used give many program tell training time per epoch calculate total training time thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
501772902,"For ""cropped images"", I mean the output image from step (2). You crop the object and resize it to the same size.",mean output image step crop object resize size,issue,negative,negative,negative,negative,negative,negative
501772408,"I haven't used the googlecolab so that I cannot give you too many suggestions. The program should tell you the training time per epoch, and you can calculate the total training time from that.",used give many program tell training time per epoch calculate total training time,issue,negative,positive,positive,positive,positive,positive
501770908,I haven't used docker that much. I leave your question to @taesungp ,used docker much leave question,issue,negative,positive,positive,positive,positive,positive
501770607,"As a side note, we did this because the network only takes the input that is divisible by 4.",side note network input divisible,issue,negative,neutral,neutral,neutral,neutral,neutral
501568037,"Hello 
Thank you so much for the reply .earlier was doing program on anaconda commnad prompt and facing lot of problems but now have tried it on googlecolab it stop after 2nd epoch and ask to reconnect  .PFA in the last.
1. After reconnecting  the uploaded data disappear what is the solution to get results on googlecolab? 
2. Im using utkface right now with 128*128  image size how long will it take to complete execution .
3. Any further tips to complete the program 

Kindly guide me .As this is my first ever program that im trying to run for my work.
Please reply as soon as possible.
Thanks in advance  for ur guidance 
Regards Neha





Sent from Yahoo Mail on Android 
 
  On Thu, 13 Jun 2019 at 2:50 am, Jun-Yan Zhu<notifications@github.com> wrote:   
Did you run the code inside the main directory? See more information about this error here.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.
  
",hello thank much reply program anaconda prompt facing lot tried stop epoch ask reconnect last data disappear solution get right image size long take complete execution complete program kindly guide first ever program trying run work please reply soon possible thanks advance ur guidance sent yahoo mail android wrote run code inside main directory see information error thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
501556579,"@junyanz Thanks for your quick response. 

I am working within your docker container. Is it necessary to start the visdom.server command from inside the docker container or is it ok to run it from another terminal?

(If I should run it inside a docker container, how do I get start two docker instances with the same image?)
Thanks a lot in advance for your support and best regards",thanks quick response working within docker container necessary start command inside docker container run another terminal run inside docker container get start two docker image thanks lot advance support best,issue,positive,positive,positive,positive,positive,positive
501510887,"@junyanz Do you mind elaborating what the ""image_loader"" is referring to? I'm currently trying to add support for 4 channel data but have been running into a lot of troubles especially with saving/loading/transforms.",mind currently trying add support channel data running lot especially,issue,negative,neutral,neutral,neutral,neutral,neutral
501490442,"> @LilNader I'm curious, were you working with 3-channel dumpy data (3 layers) or 4? I am currently trying to directly pass 4 layer/channels arrays but am having lots of trouble modifying the code since the PIL library seems to bank on 3 channels

@patrick-han In my case I was passing single channel numpy data (binary images) directly to the Network without using PIL and I had to set --input_nc and --output_nc to 1
In your case, I guess you will need to indicate the number of input and output channels using --input_nc or --output_nc",curious working dumpy data currently trying directly pas lot trouble code since library bank case passing single channel data binary directly network without set case guess need indicate number input output,issue,negative,negative,neutral,neutral,negative,negative
501489033,"@LilNader I'm curious, were you working with 3-channel dumpy data (3 layers) or 4? I am currently trying to directly pass 4 layer/channels arrays but am having lots of trouble modifying the code since the PIL library seems to bank on 3 channels",curious working dumpy data currently trying directly pas lot trouble code since library bank,issue,negative,negative,neutral,neutral,negative,negative
501487903,"> I am not sure. Maybe due to this [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L115). The program, in general, assumes that the height and width can be divided by 4. (I am trying to directly load the arrays rather than generate an image)

I believe that was the issue, my numpy arrays needed to be divisible by 4. I had commented out [this](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L98) line and needed to manually resize the array. 

",sure maybe due function program general height width divided trying directly load rather generate image believe issue divisible line manually resize array,issue,negative,positive,positive,positive,positive,positive
501484983,"Ohh that seems to be a good solution, when you asked me to collect only ""cropped images"", did you mean to that for real images or images belonging to both the datasets ? The real images indeed are very far from the camera and not zoomed in. 

Please let me know, which type of images you were referring to. Thanks alot !",good solution collect mean real belonging real indeed far camera please let know type thanks,issue,positive,positive,positive,positive,positive,positive
501465837,"Could you share with us more details (e.g., training and test command)?",could share u training test command,issue,negative,neutral,neutral,neutral,neutral,neutral
501464996,Could you also make sure that you use the same `--netG` during training and test?,could also make sure use training test,issue,negative,positive,positive,positive,positive,positive
501464679,"It seems that drones have different sizes and locations in your source and target domains. One thing you can do is to (1) apply an object (drone) detection method to predict the bounding box of the drone, (2) crop the drone based on its bounding box and resize it to 256x256.  (3) apply Cyclegan on cropped images.",different size source target one thing apply object drone detection method predict bounding box drone crop drone based bounding box resize apply,issue,negative,neutral,neutral,neutral,neutral,neutral
501462283,Reducing the number of Resnet blocks can make the network's capacity smaller. This might be helpful when your task only involved small and local changes. You don't need a big network for modeling small changes.,reducing number make network capacity smaller might helpful task involved small local need big network modeling small,issue,negative,negative,neutral,neutral,negative,negative
501461599,"I am not sure. Maybe due to this [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L115). The program, in general, assumes that the height and width can be divided by 4.",sure maybe due function program general height width divided,issue,negative,positive,positive,positive,positive,positive
501459994,You need to create a second terminal and type the training command.,need create second terminal type training command,issue,negative,neutral,neutral,neutral,neutral,neutral
501459762,Did you run the code inside the main directory? See more information about this error [here](https://stackoverflow.com/questions/43728431/relative-imports-modulenotfounderror-no-module-named-x).,run code inside main directory see information error,issue,negative,positive,positive,positive,positive,positive
501456671,You need to run `python -m visdom.server` in a separate command panel. You can also run it as a background program.,need run python separate command panel also run background program,issue,negative,neutral,neutral,neutral,neutral,neutral
501404034,"@hengshanji @YushengZhang @Designbook1 @Matlmr @junyanz 

I am facing a similar issue. I do not know how to handle the visdom tool. 
Starting the server by `python -m visdom.server ` works (more or less) as expected - no error message, everything starts and I see a blue screen in `localhost:8097`
However, how should I go on? 

I can enter something in the same terminal window (where I started the server) but nothing happens. Should I close it before I can go on? 

I am using the docker image provided in the repo. 

Sorry for the confusions and thanks a lot in advance!!",facing similar issue know handle tool starting server python work le error message everything see blue screen however go enter something terminal window server nothing close go docker image provided sorry thanks lot advance,issue,negative,negative,neutral,neutral,negative,negative
501392149,"@frogprincel @guotong1988 @junyanz I have the same question. However, I am not sure how to handle the visdom server. 

I am using the docker container and if I execute 

> python -m visdom.server 

the server starts as expected however, I am not sure how to continue. I can either enter something in the command line but it is not working nor stopping the visdom server. Could you please provide the order in which I have to call the functions? 

Thanks a lot in advance!!!",question however sure handle server docker container execute python server however sure continue either enter something command line working stopping server could please provide order call thanks lot advance,issue,positive,positive,positive,positive,positive,positive
501004211,"That was actually the first thing I tried, but I got an error when reading the dictionary. Partial output is given below.

Also, I have to confess that I wasn't sure which of these files should be copied to latest_net_G.pth, so I tried both of them and got the same error

latest_net_G_A.pth
latest_net_G_B.pth

----------------- End -------------------
dataset [SingleDataset] was created
initialize network with normal
model [TestModel] was created
loading the model from ./checkpoints/my_project_cyclegan/latest_net_G.pth
Traceback (most recent call last):
  File ""test.py"", line 47, in <module>
    model.setup(opt)               # regular setup: load and print networks; create schedulers
 
RuntimeError: Error(s) in loading state_dict for ResnetGenerator:
	Missing key(s) in state_dict: ""model.10.conv_block.6.weight"", ""model.10.conv_block.6.bias"", . . .",actually first thing tried got error reading dictionary partial output given also confess sure copied tried got error end initialize network normal model loading model recent call last file line module opt regular setup load print create error loading missing key model weight model bias,issue,negative,positive,neutral,neutral,positive,positive
500674914,"> I think so. Both jpeg and gaussian blur are local effects.

Thanks, when I reduce the number of Resnet blocks in generator network and use the code of remove the checkerboard artificats, the effect is better. How to explain it? Is it because there are few samles in target domain?",think blur local effect thanks reduce number generator network use code remove checkerboard effect better explain target domain,issue,positive,positive,positive,positive,positive,positive
500607714,"I recommend that you use `--resize_or_crop crop` and set both load_size and fine_size as 256. During test time, you can use `--resize_or_crop none`.
 @taesungp what is your recommendation?",recommend use crop set test time use none recommendation,issue,negative,neutral,neutral,neutral,neutral,neutral
500605687,I got rid of `A` and `B` in test_model.py in the latest commit.,got rid latest commit,issue,negative,positive,positive,positive,positive,positive
500605075,"`real_A` means the input images (in your case, test images from domain B). The test code under `--model test` mode will always name the input images as `real_A`.",input case test domain test code model test mode always name input,issue,negative,neutral,neutral,neutral,neutral,neutral
500603609,"`num_threads` should help ideally. Maybe you want to time your data loading code. Also, check out your file system IO speed.",help ideally maybe want time data loading code also check file system io speed,issue,positive,positive,positive,positive,positive,positive
500603012,We implemented the network of Johnson et al.  You can find more details about the network in their [paper](https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf). You can also print the network.,network al find network paper also print network,issue,negative,neutral,neutral,neutral,neutral,neutral
500601551,You can write your own custom data loader. See this [template](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) class for more details.,write custom data loader see template class,issue,negative,neutral,neutral,neutral,neutral,neutral
500600765,You can add some visualization code to visualize `pred_fake` [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L134). It should be a 30x30x1 tensor.,add visualization code visualize tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
500600232,I think so. Both jpeg and gaussian blur are local effects.,think blur local effect,issue,negative,neutral,neutral,neutral,neutral,neutral
500600004,"I recommend that you use a semantic segmentation [network](https://github.com/CSAILVision/semantic-segmentation-pytorch) for your task. GANs sometimes struggle when the task's output is discrete,",recommend use semantic segmentation network task sometimes struggle task output discrete,issue,negative,neutral,neutral,neutral,neutral,neutral
500189028,"I got it to work.
Used the predefined Dockerfile to build a Docker container.
Then converted that container to a singularity sandbox directory (as the cluster I'm training the CycleGAN on uses singularity containers).
Then I ran the training code inside that container and whenever a module was missing or an error occured I installed/fixed that by shelling into the sandbox.
When the training started successfully I built an immutable image from the sandbox and uploaded it to the cluster filesystem.",got work used build docker container converted container singularity sandbox directory cluster training singularity ran training code inside container whenever module missing error shelling sandbox training successfully built immutable image sandbox cluster,issue,negative,positive,positive,positive,positive,positive
500161000,"To be more precise, what would be the number of filters used in the first layer and their size when 256x256 image is given as input. ",precise would number used first layer size image given input,issue,negative,positive,positive,positive,positive,positive
500138469,"I was wondering how to define the learning rate for G and D separately. And if the loss of only one of the generator blows up while the others performs well, how can we change the learning rate according to this problem.",wondering define learning rate separately loss one generator well change learning rate according problem,issue,negative,neutral,neutral,neutral,neutral,neutral
499916949,"I have the same patterns when training the CycleGAN. However, it only happens in one direction (translating StyleA to StyleB). Any idea what might cause this? My Style A images are in higher resolution and B is lower resolution. Thx!",training however one direction idea might cause style higher resolution lower resolution,issue,negative,positive,positive,positive,positive,positive
499844686,"@mhusseinsh @didirus @junyanz I am not sure if I got it correct. If I want to train the CycleGAN to translate e.g. images of `256x512` (in test mode). My input data is also `256x512`. 
So, I should train the model with, for example, `load_size=270x270` and `fine_size=256x256`. Furthermore, I have to add the `--resize_or_crop crop`? Is this correct? No other changes?
If this is the case, could you maybe add a short explanation how the crop parameters works in this case?

I appreciate any help or hints on this topic since I am also struggling to achieve good results. For the moment, I only get quadratic images. 

Thanks a lot in advance.  ",sure got correct want train translate test mode input data also train model example furthermore add crop correct case could maybe add short explanation crop work case appreciate help topic since also struggling achieve good moment get quadratic thanks lot advance,issue,positive,positive,positive,positive,positive,positive
499687733,"Hey, wanted to say thanks as well @dokasov. Your code helped a lot to figure out how to run inference on image feed with pix2pixhd! ",hey say thanks well code lot figure run inference image feed,issue,positive,positive,positive,positive,positive,positive
499215759,I run into the same issue. But I figured out a way by adding virtual memory to it. As long as you have an SSD on your computer. Free up some space if you don't have enough on your SSD (1.5* to 3* of actual RAM). According to this [virtual memory set up on Win10 tutorial](https://www.geeksinphoenix.com/blog/post/2016/05/10/how-to-manage-windows-10-virtual-memory.aspx),run issue figured way virtual memory long computer free space enough actual ram according virtual memory set win tutorial,issue,positive,positive,positive,positive,positive,positive
498513615,"> If you just deal with gaussian blur, you can use a much smaller network.

Thanks，but I need  to  generate some multiple distortion (such as jpeg and gaussian blur) or gaussian noise. Is it feasible to reduce network structure?",deal blur use much smaller network need generate multiple distortion blur noise feasible reduce network structure,issue,negative,neutral,neutral,neutral,neutral,neutral
498228996,"Thanks for remind, yes, actually I tried, but still failed. It might be because of some kind of firewall that can block the visdom. I don't know exactly why. But once turn off visdom it should work. ",thanks remind yes actually tried still might kind block know exactly turn work,issue,positive,positive,positive,positive,positive,positive
498086953,"Does the pix2pix (aligned dataset) model expect input in [-1, 1] or [0,1]? When passing input between [0,1] the saved images look washed out, and black is saved as grey (RGB value (127,127,127)), so I am guessing it expects [-1,1].",model expect input passing input saved look washed black saved grey value guessing,issue,positive,negative,negative,negative,negative,negative
498066220,"Thanks, I will try. I had some error but I guess it is sth wrong in my code",thanks try error guess wrong code,issue,negative,negative,negative,negative,negative,negative
497812362,"Update: resolved this. The error was in the dict returned by `__getitem__`. Instead of: 
```
return {'data_A': data_A, 'data_B': data_B, 'A_paths': A_paths, 'B_paths': B_path}
```
I now have: 
```
return {'A': data_A, 'B': data_B, 'A_paths': A_paths, 'B_paths': B_path}
```
---

Currently I'm able to load images properly using a custom data loader `__init__`. My `__getitem__` seems to be working as well. However, I'm getting errors training the pix2pix model. I receive the following error: 
```
Traceback (most recent call last):
  File ""train.py"", line 50, in <module>
    model.set_input(data)         # unpack data from dataset and apply preprocessing
  File ""/home/blancavillanueva/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py"", line 82, in set_input
    self.real_A = input['A' if AtoB else 'B'].to(self.device)
KeyError: 'A'
```

I have multiple paths for the A image, but only a single path for the B image. I assign them in `__getitem__` like this: 
```
return {'data_A': data_A, 'data_B': data_B, 'A_paths': A_paths, 'B_paths': B_path}
```


Is it possible to keep my current directory structure (which is different from the aligned dataset's expected `/path/to/data/A/train` and `/path/to/data/B/train`) or do I need to move my files into directories to match the expected directory structure? The files are quite large and moving them around will be expensive. ",update resolved error returned instead return return currently able load properly custom data loader working well however getting training model receive following error recent call last file line module data unpack data apply file line input else multiple image single path image assign like return possible keep current directory structure different need move match directory structure quite large moving around expensive,issue,negative,positive,neutral,neutral,positive,positive
497745996,Starting the visdom server by yourself before you run the training script might help.,starting server run training script might help,issue,negative,neutral,neutral,neutral,neutral,neutral
497198074,"Hi, I have the same problem. According to former answers, I use the command below. 
python train.py --dataroot ./datasets/cityscapes/ --name cityscapes_pix2pix --model pix2pix --direction BtoA --display_id -1 --continue_train
I think the thing is to disable visdom (does not work on my devices), and use the label ""--continue_train"", sometimes it will get stuck again, but I try to replay the command, and check saved files in /checkpoints, it seems that all the epochs are recorded. Hope this can help.  ",hi problem according former use command python name model direction think thing disable work use label sometimes get stuck try replay command check saved hope help,issue,positive,neutral,neutral,neutral,neutral,neutral
497114321,"Would you suggest stacking them along the C dimension, then? Since we're modifying `input_nc`. That is, for A1, A2, A3, where A1 has dim (H,W,C), A2 has dim (H,W,C), and A3 has dim (H,W,C), we stack these along the C dimension such that the input A123 has dim (H,W,3C). And the output B will be (H,W,C). Is this understanding correct? ",would suggest along dimension since dim dim dim stack along dimension input dim output understanding correct,issue,negative,positive,neutral,neutral,positive,positive
496624673,Ah ok.. Thanks very much for your reply. I plan to try to find augmentation methods will somehow probably let the training set of apples to have some distributions of bananas first. ,ah thanks much reply plan try find augmentation somehow probably let training set first,issue,negative,positive,positive,positive,positive,positive
496619799,I am not aware of it. You may need to invent some new methods. ,aware may need invent new,issue,negative,positive,positive,positive,positive,positive
496513191,"@vrao9 : Thanks. Which line to do ""accuracy of prediction by the discriminator for fake image is less than 50%: ""?",thanks line accuracy prediction discriminator fake image le,issue,negative,negative,negative,negative,negative,negative
496509223,"I used the keras implementation: 
https://github.com/eriklindernoren/Keras-GAN/tree/master/cyclegan
I don't know how it can be done in pytorch.",used implementation know done,issue,negative,neutral,neutral,neutral,neutral,neutral
496436480,"Thanks for your reply. If an extra cycle GAN would not do the job, can you recommend me an invertible deep learning/machine learning method to ""normalise"" the red_banana into red_apple which is expected to turn the output green_apple back into green banana?",thanks reply extra cycle gan would job recommend invertible deep learning method turn output back green banana,issue,positive,neutral,neutral,neutral,neutral,neutral
496342537,It's up to each user's personal preference. Some folks like to use CUDA_VISIBLE_DEVICE while others not. Feel free to modify the code as you like. ,user personal preference like use feel free modify code like,issue,positive,positive,positive,positive,positive,positive
496340885,Not sure if it is related to this repo. You may want to pot your question on the official Pytorch [repo](https://github.com/pytorch/pytorch). ,sure related may want pot question official,issue,negative,positive,positive,positive,positive,positive
496340427,"If you just deal with gaussian blur, you can use a much smaller network. ",deal blur use much smaller network,issue,negative,neutral,neutral,neutral,neutral,neutral
496340039,"This is an interesting idea. I am not sure if the red_apple to green_apple will work for red_banana. As 
 the color and texture statistics might be different for red_apple and red_banana. ",interesting idea sure work color texture statistic might different,issue,positive,positive,positive,positive,positive,positive
496338926,It's a small speedup trick. set_requires_grad=False will stop calculating gradients for the discriminator while we update the generator. It can save some time and memory. Generators will still get the gradients. ,small trick stop calculating discriminator update generator save time memory still get,issue,negative,negative,negative,negative,negative,negative
496331290,"
![1111111](https://user-images.githubusercontent.com/32053722/58444307-3b98f400-812a-11e9-8ea3-28b0b0dd72e7.png)


> You need to specify the display port when running train.py. See this [argument](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L18) for more details.
I want to generate two different levels' distortion images, the source domain consists of 4744 non-distortion images, when the 400 small gaussian blur images are included in target domain, the generation images is shown in the left. When the 400 severe gaussian blur images are included in target domain, the generation images is shown in the right.  In theory the quality of the slight distortion image should be better than that of severe distoriton, but I failed by using cyclegan. Do you know what could lead to this and what would be a good way to improve?

 ",need specify display port running see argument want generate two different distortion source domain small blur included target domain generation shown left severe blur included target domain generation shown right theory quality slight distortion image better severe know could lead would good way improve,issue,negative,positive,positive,positive,positive,positive
496323817,"And i feel like the identity loss won't address this issue effectively because it reconstructed image does erased the ghost in the background:

original:
![3_a2b_test_original_0](https://user-images.githubusercontent.com/8679679/58442695-bb2eb080-80a1-11e9-8741-7bb32bf9bf39.JPEG)

generated:
![b2a_0_original](https://user-images.githubusercontent.com/8679679/58442705-c97ccc80-80a1-11e9-890c-9542376777aa.JPEG)

reconstructed:
![b2a_0_generated](https://user-images.githubusercontent.com/8679679/58442707-cf72ad80-80a1-11e9-9886-99a6e2e6e28d.JPEG)

",feel like identity loss wo address issue effectively reconstructed image erased ghost background original reconstructed,issue,negative,positive,positive,positive,positive,positive
496051352,"""accuracy of prediction by the discriminator for fake image is less than 50%: "" how do you do it in pytorch? Thanks",accuracy prediction discriminator fake image le thanks,issue,negative,negative,negative,negative,negative,negative
496031454,"@John1231983 good to know that!
I tried 2 things:
1) reduce the learning rate: it did help in solving the problem
2) update the discriminator only when accuracy of prediction by the discriminator for fake image is less than 50%: there were less updates in the discriminator weights because of this new rule and discriminator loss did not go to zero.",good know tried reduce learning rate help problem update discriminator accuracy prediction discriminator fake image le le discriminator new rule discriminator loss go zero,issue,negative,positive,positive,positive,positive,positive
496029655,"the `im` in visualizer.py is a numpy array, so if you load that into a PIL image by `Image.fromarray(im)` and then call `.resize((h, int(w * aspect_ratio)), resample=PIL.Image.BICUBIC)` on it, it should work as before. I haven't try it tho because i just downgraded my scipy to 1.2.1...",array load image call work try tho,issue,negative,neutral,neutral,neutral,neutral,neutral
495971895,"@vrao9: do you find the good solution to solve the issue? I found that reduce number layer of D can solve it. I did not test your suggested solution: reduce lr, or increasing training G",find good solution solve issue found reduce number layer solve test solution reduce increasing training,issue,positive,positive,positive,positive,positive,positive
495917693,"maybe not,cos the error was occured at epoch 80, it has gone through a whole datasets. but when i change server ,it's gone!",maybe co error epoch gone whole change server gone,issue,negative,positive,positive,positive,positive,positive
495516644,"I think maybe it's because you have run ""python -m visdom.server"" before. All I do is just type localhost:8097 in web browser. And we can see the training results there.
![360截图16240201103544](https://user-images.githubusercontent.com/46192647/58312905-dfd71e00-7e3e-11e9-9a38-b84ffa91be32.png)
",think maybe run python type web browser see training,issue,negative,neutral,neutral,neutral,neutral,neutral
495019804,"> You need to specify the display port when running train.py. See this [argument](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L18) for more details.

Thank you very much.",need specify display port running see argument thank much,issue,negative,positive,positive,positive,positive,positive
494894386,You can set different learning rates for G and D. See this [paper](https://arxiv.org/abs/1706.08500) for more details. ,set different learning see paper,issue,negative,neutral,neutral,neutral,neutral,neutral
494893492,You need to specify the display port when running train.py. See this [argument](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L18) for more details. ,need specify display port running see argument,issue,negative,neutral,neutral,neutral,neutral,neutral
494812476,"> You or other people may have started a session. You can try (1) killing the existing visdom jobs, (2)restarting the machine, or (3) using a different display port `--display_port`

I meet the same problem. I use different display port, python -m visdom.server -port=15024, but the display screen shows no images, just blue.
![12345](https://user-images.githubusercontent.com/32053722/58180011-e7ca7d00-7cdb-11e9-9423-1227c296276b.png)
",people may session try killing machine different display port meet problem use different display port python display screen blue,issue,negative,neutral,neutral,neutral,neutral,neutral
494797320,"I'm also facing the same issue. I thought about increasing the number of times the generator gets updated when compared to the discriminator or decreasing the learning rate of the discriminator.
What are other ways in which generator can be made stronger / discriminator can be made weaker? @junyanz ",also facing issue thought increasing number time generator discriminator decreasing learning rate discriminator way generator made discriminator made,issue,negative,neutral,neutral,neutral,neutral,neutral
493839365,maybe making the discriminator weaker or the generator stronger. ,maybe making discriminator generator,issue,negative,neutral,neutral,neutral,neutral,neutral
493839264,I recommend that you try the new parameters/architectures on low-resolution images or small image datasets.,recommend try new small image,issue,negative,negative,neutral,neutral,negative,negative
493770406,"@junyanz : Thanks. I have removed the new loss and the issue still maintain. I guess that cycleGAN has the issue when I use it on the new dataset. For traditional GAN, they said that loss D goes to zero mean the model is a [failure (tip 10](https://github.com/soumith/ganhacks). Do you know how to handle with the issue",thanks removed new loss issue still maintain guess issue use new traditional gan said loss go zero mean model failure tip know handle issue,issue,negative,negative,neutral,neutral,negative,negative
493718827,Please discuss it in this [issue](https://github.com/phillipi/pix2pix/issues/174). I will close this one. ,please discus issue close one,issue,negative,neutral,neutral,neutral,neutral,neutral
493659913,"Ok for anybody else who had this problem here is what I used and it worked:
`python test.py --dataroot ./datasets/[your dataset] --name [whatever you named it]_pix2pix --netG unet_256 --norm batch --dataset_mode single --model test --gpu_ids 0  --preprocess none --epoch latest --no_dropout --load_size 512 --crop_size 512`

I ended up with a 512x512 output image.",anybody else problem used worked python name whatever norm batch single model test none epoch latest ended output image,issue,negative,positive,positive,positive,positive,positive
493659580,"Yes I already tried that as per that post and got 
`NotImplementedError: Generator model name [unet] is not recognized`
Then tried unet_256 instead which worked.",yes already tried per post got generator model name tried instead worked,issue,negative,neutral,neutral,neutral,neutral,neutral
493648642,Maybe adding `--netG unet --norm batch`. See this [post](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/358) for more details. ,maybe norm batch see post,issue,negative,neutral,neutral,neutral,neutral,neutral
493586394,"When I try to use model test I get 
`AttributeError:` 'Sequential' object has no attribute 'model'",try use model test get object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
493548699,"Could you share with us more details? Are you using pix2pix or CycleGAN? What is your test script command?  One possible scenario: you are testing pix2pix but your input data only contain 512x512 input image. If you use `--model pix2pix` during test time, the test script will divide this 512x512 image into two images. Each image is 256x512, because the input data should include both input and output images.  One workaround solution is to use `--model test`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) for more details. ",could share u test script command one possible scenario testing input data contain input image use model test time test script divide image two image input data include input output one solution use model test see,issue,positive,neutral,neutral,neutral,neutral,neutral
493547312,It seems that you have some issues when loading a TIff image. You may want to check whether your Tiff images are corrupted or if these files still exist. ,loading tiff image may want check whether tiff corrupted still exist,issue,negative,neutral,neutral,neutral,neutral,neutral
493546656,I am not sure. Maybe you can post the question at the visdom [repo](https://github.com/facebookresearch/visdom). ,sure maybe post question,issue,negative,positive,positive,positive,positive,positive
493546204,"You or other people may have started a session. You can try (1) killing the existing visdom jobs, (2)restarting the machine, or (3) using a different display port `--display_port`",people may session try killing machine different display port,issue,negative,neutral,neutral,neutral,neutral,neutral
493543147,"Your test script might have some issues.  In your case,  it would be better to use the flag `--model cycle_gan` during test time. ",test script might case would better use flag model test time,issue,negative,positive,positive,positive,positive,positive
493414274,"I might have a very similar problem, however removing the eval flag does not help there. Is there anything during training or testing that can be done to avoid this?

Just as reference these are my training/test commands
`python train.py --dataroot datasets/AachenDayNight/ --model cycle_gan --name aachendaynight2 --display_port 8834 --display_freq 200 --batch_size 1 --niter 10 --niter_decay 20 --netG unet_128 --no_dropout`
`python test.py --dataroot datasets/AachenDayNight --name aachendaynight2 --model test --no_dropout --netG unet_128 --phase train --num_test 100`
This was one of the last fake images generated during training stored in the web directory
![epoch029_fake_B](https://user-images.githubusercontent.com/15774442/57923504-c5cb9780-78a2-11e9-926f-77203fd7cc1c.png)
And here's the result of the test script (on images from the training set)
![1_fake_B](https://user-images.githubusercontent.com/15774442/57923594-00cdcb00-78a3-11e9-86af-b876e209cfa1.png)

Any advice for this?",might similar problem however removing flag help anything training testing done avoid reference python model name niter python name model test phase train one last fake training web directory result test script training set advice,issue,negative,negative,negative,negative,negative,negative
492235492,"Hi @junyanz 
I want to make sure that I am trying CycleGAN with WGAN-GP loss in the same setup and with the same configuration you tried it.

This is my modification inspired by #439 and [VON](https://github.com/junyanz/VON/blob/master/models/shape_gan_model.py#L80) on **cycle_gan_model.py**:
```
def backward_D_basic(self, netD, real, fake):
        """"""Calculate GAN loss for the discriminator

        Parameters:
            netD (network)      -- the discriminator D
            real (tensor array) -- real images
            fake (tensor array) -- images generated by a generator

        Return the discriminator loss.
        We also call loss_D.backward() to calculate the gradients.
        """"""
        # Real
        pred_real = netD(real)
        loss_D_real = self.criterionGAN(pred_real, True)
        # Fake
        pred_fake = netD(fake.detach())
        loss_D_fake = self.criterionGAN(pred_fake, False)
        # wgan-gp
        gradient_penalty, gradients = networks.cal_gradient_penalty(
            netD, real, fake, self.device,lambda_gp=10.0
        )
        gradient_penalty.backward(retain_graph=True)
        # Combined loss and calculate gradients
        loss_D = (loss_D_real + loss_D_fake) * 0.5
        loss_D.backward()
        return loss_D
```
and to start training  
```
python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --gan_mode wgangp --norm instance 
```
Is this all?  or should I take care of other things?
Is this the same setup you tried ?",hi want make sure trying loss setup configuration tried modification inspired self real fake calculate gan loss discriminator network discriminator real tensor array real fake tensor array generator return discriminator loss also call calculate real real true fake false real fake combined loss calculate return start training python name model norm instance take care setup tried,issue,negative,negative,negative,negative,negative,negative
492227428,still happening to me. @XPping how exactly did you make a fix for it? ,still happening exactly make fix,issue,negative,positive,positive,positive,positive,positive
492152777,"I add the parameter in Networks.py and in module called UnetSkipConnectionBlock.

I suspected there is an imcompatility in loss function or update process.",add parameter module suspected loss function update process,issue,negative,neutral,neutral,neutral,neutral,neutral
491640434,"> @taesungp which address does the docker container give to you?

Its actually the ID of the container, which is randomly combining numbers and letters and I donts think it has special meaning",address docker container give actually id container randomly combining think special meaning,issue,negative,negative,neutral,neutral,negative,negative
491446419,Maybe reducing the weight for your new loss.,maybe reducing weight new loss,issue,negative,positive,positive,positive,positive,positive
491446249,Ls-gan works better for cyclegan. Ls-GAN and vanilla gan loss work similarly for pix2pix.,work better vanilla gan loss work similarly,issue,negative,positive,positive,positive,positive,positive
491446117,"I don't think that you can. If you are using pix2pix, you need to train a new model from B->A. If you are using cyclegan, it will come up with models with both directions.",think need train new model come,issue,negative,positive,positive,positive,positive,positive
491445808,"I have tried the CycleGAN with WGAN-GP loss but didn't get significantly better performance. 
You may want to check out some recent work such as [1](https://arxiv.org/pdf/1808.04325.pdf).",tried loss get significantly better performance may want check recent work,issue,negative,positive,positive,positive,positive,positive
491445251,It's hard to know why. Could you give us more debugging information? @SsnL ,hard know could give u information,issue,negative,negative,negative,negative,negative,negative
491445063,"Yeah, it will change a bit. You can resize it back to the original size by yourself. ",yeah change bit resize back original size,issue,positive,positive,positive,positive,positive,positive
491444924,@taesungp which address does the docker container give to you?,address docker container give,issue,negative,neutral,neutral,neutral,neutral,neutral
491241743,"Hi junyanz, I found that, when I start the visdom in my local environment, the address It gives is http://localhost:8097, but When I run the my through a docker container in a server, it gives another address, and I cannot open it using my browser. ",hi found start local environment address run docker container server another address open browser,issue,negative,neutral,neutral,neutral,neutral,neutral
491125501,"Thank you. I think these will be my last question for now : Which loss (in your opinion) generates images with better quality
- LS-GAN (using MSE) and MAE (default config for pix2pix)
- LS-GAN (using MSE) and L1 loss (as used in pix2pix paper, by using `nn.L1Loss(reduction='mean')`)
- WGAN",thank think last question loss opinion better quality mae default loss used paper,issue,negative,positive,positive,positive,positive,positive
491123712,"So, could you suggest any solutiom to solve the issue?",could suggest solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
490750092,@junyanz  Thanks so much However the width size must be multiple of 4 hence the output width size change a bit to be 4 times multiplication ,thanks much however width size must multiple hence output width size change bit time multiplication,issue,negative,positive,positive,positive,positive,positive
490644192,"Thank you! However, we can theoretically implement CycleGAN using WGANGP loss, right?  
Btw. have you implemented CycleGAN with WGANGP loss?
If I want to improve CycleGAN, could you give me some advices what I can do ?",thank however theoretically implement loss right loss want improve could give,issue,negative,positive,positive,positive,positive,positive
490267362,"This model was trained on images alone, without temporal consistency. For video generation, please check out recent work [video-to-video synthesis](https://tcwang0509.github.io/vid2vid/).",model trained alone without temporal consistency video generation please check recent work synthesis,issue,negative,neutral,neutral,neutral,neutral,neutral
490266574,"CycleGAN is implemented using LSGAN loss. I recommend that you use LSGAN loss. WGANGP loss is included for completeness, in case people use the codebase for other purposes. ",loss recommend use loss loss included completeness case people use,issue,negative,neutral,neutral,neutral,neutral,neutral
490266155,You need to add it in the command line. ,need add command line,issue,negative,neutral,neutral,neutral,neutral,neutral
489753473,"Thank you for your comments @SsnL and @olivier-gillet .
This weekend I did 26 consecutive trainings without a hassle on the DGX server.
This time I used a [slightly newer version of the docker container](https://docs.nvidia.com/deeplearning/dgx/pytorch-release-notes/rel_19-04.html#rel_19-04) and used the `display_id=0` option with every training to disable visdom. 
I still have no clue what causes the issue.  Maybe the reason for the hang is indeed that I didn't start visdom beforehand manually as @olivier-gillet pointed out as well?
Because the last time that I did the training on the DGX I used [this container](https://docs.nvidia.com/deeplearning/dgx/pytorch-release-notes/rel_19-03.html#rel_19-03) which uses PyTorch commit [81e025d](https://github.com/pytorch/pytorch/commit/81e025d9ac9135acd991913715cc21a4497a4b79) (which is past version 1.0.0, if I am correct) and I had the same issue there.",thank weekend consecutive without hassle server time used slightly version docker container used option every training disable still clue issue maybe reason indeed start beforehand manually pointed well last time training used container commit past version correct issue,issue,positive,negative,negative,negative,negative,negative
489565784,"> I added `--gpu_ids -1` and this issue was resolved. Thanks.

where you added that",added issue resolved thanks added,issue,positive,positive,positive,positive,positive,positive
489373988,"Is this model was trained on images? i.e. no additional temporal consistency constrain that is special for video was not applied?

It's known that conditioned models trained on images are flickering when they applied to video frames
Examples:
Based on pix2pixHD, conditioned on pose:
https://github.com/nyoki-mtl/pytorch-EverybodyDanceNow/blob/master/output.gif
Based on pix2pixHD, conditioned on semantic label map:
https://youtu.be/5zlcXTCpQqM?t=140",model trained additional temporal consistency constrain special video applied known conditioned trained flickering applied video based conditioned pose based conditioned semantic label map,issue,negative,positive,positive,positive,positive,positive
489366940,"You can a different `preprocess` option. (e.g., `scale_width`) See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L49) for more details. ",different option see line,issue,negative,neutral,neutral,neutral,neutral,neutral
489332885,"What about the number of layers within it? Is there a reason behind it?
I've read the U-net paper, but I didn't see any explicit mention of the reason behind choice of network depth (or did I missed it?)",number within reason behind read paper see explicit mention reason behind choice network depth,issue,negative,negative,negative,negative,negative,negative
489306087,"Hi @junyanz  what if the image is not square? I am testing size of 78x128 then the output size become 128x128. the fake image is stretched ? what is the reason of this? and how to keep the original size after testing. 
Thanks in advance ",hi image square testing size output size become fake image reason keep original size testing thanks advance,issue,negative,positive,neutral,neutral,positive,positive
489290261,"In pix2pix work, we followed the architecture of U-Net [paper](https://arxiv.org/abs/1505.04597). For more recent architecture, maybe you would like to try [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE).",work architecture paper recent architecture maybe would like try spade,issue,negative,neutral,neutral,neutral,neutral,neutral
489286093,I have experimented with switching the side of dataset. And indeed it helps breaking the confusion. Thank you very much for the help,experimented switching side indeed breaking confusion thank much help,issue,negative,positive,positive,positive,positive,positive
489145624,@SsnL  You mentioned you have tested it on a test set but didn't work on a single image. What's the difference? Could you give us more details regarding the CUDA/Pytorch environment? ,tested test set work single image difference could give u regarding environment,issue,negative,negative,neutral,neutral,negative,negative
489042501,"I have the same issue as @azadyasar had:
`AttributeError: 'Sequential' object has no attribute 'model'`

I'm trying to apply my trained model in a production environment, where I just feed it a single image. I'm running the following command:
`python test.py --dataroot <data-root> --name aaai --model test --direction AtoB --dataset_mode single --norm batch --checkpoints_dir <data-root> --load_size 1024 --crop_size 1024 --no_dropout`

I've trained a pix2pix model succesfully, and testing it on a test set works properly.
Am using pytorch version 1.0.0 and the latest version of the repository. 

The full error is as follows:
```
loading the model from /var/scratch/rsa680/datasets/checkpoints/aaai/latest_net_G.pth
Traceback (most recent call last):
  File ""test.py"", line 47, in <module>
    model.setup(opt)               # regular setup: load and print networks; create schedulers
  File ""/home/rsa680/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 89, in setup
    self.load_networks(load_suffix)
  File ""/home/rsa680/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 198, in load_networks
    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))
  File ""/home/rsa680/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 174, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
  File ""/home/rsa680/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 174, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
  File ""/var/scratch/rsa680/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 535, in __getattr__
    type(self).__name__, name))
AttributeError: 'Sequential' object has no attribute 'model'
```",issue object attribute trying apply trained model production environment feed single image running following command python name model test direction single norm batch trained model testing test set work properly version latest version repository full error loading model recent call last file line module opt regular setup load print create file line setup file line net file line module key file line module key file line type self name object attribute,issue,negative,positive,neutral,neutral,positive,positive
488768577,"If you have a downstream task, you can evaluate the performance of your model regarding the task. Otherwise, it requires either (1) manual inspection to choose the best model, or (2) standard GAN metrics (e..g, [FID](https://github.com/mseitzer/pytorch-fid))",downstream task evaluate performance model regarding task otherwise either manual inspection choose best model standard gan metric fid,issue,positive,positive,positive,positive,positive,positive
488440467,"Thank you so much for the prompt response Professor. I just have another follow up question, when do you think I can stop the training process ? Is human involvement/ observation required  or are there any other methods to do it like keeping track of any particular loss function at their minimum, any other metrics etc ? 
Please let me know. Thank you . ",thank much prompt response professor another follow question think stop training process human observation like keeping track particular loss function minimum metric please let know thank,issue,positive,positive,positive,positive,positive,positive
488436039,"You are correct. The test data should be similar to training data. I recommend that you collect additional training data or apply additional data augmentation. (e.g., different kinds of cropping/scaling)",correct test data similar training data recommend collect additional training data apply additional data augmentation different,issue,negative,neutral,neutral,neutral,neutral,neutral
488406530,We didn't use weight decay. You can add it by yourself. ,use weight decay add,issue,negative,neutral,neutral,neutral,neutral,neutral
488216185,"@JosseVanDelm The pytorch code you linked is running in the worker process, and it is supposed to be an infinite loop until the main process sends a signal or dies. The hang could very well be in the main process.

That said, between 0.4.1 and 1.0.0, a lot of improvements are done to the data loader. If the hang is indeed related to the dataloader, upgrading may resolve it.",code linked running worker process supposed infinite loop main process signal could well main process said lot done data loader indeed related may resolve,issue,positive,positive,positive,positive,positive,positive
487728214,"I tried using imageio but that was a while ago... I lost track of how
things went. However, I am sure I changed the IMG_EXTENSION variable.

On Thu, Apr 25, 2019 at 10:44 PM Caelyn <notifications@github.com> wrote:

> Hi, first of all congratulations for your paper and for the code. It was a
> pleasure reading what you did and how you did it.
>
> I am running CycleGAN with different types of tiffs in trainA and trainB.
> The tiffs are 256x256 pixels in size and have 1 channel per pixel. I am
> using tiffs to have a wide range of values.
>
> I changed the code as you suggest (#320
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/320> and
> similar), but what I got out during the training in ./checkpoints are
> three-channels PNGs. Do you think it would be possible to change the code
> so that it goes from 1 channel tiff to 1 channel tiff with no information
> loss? As far as I understand, at present you are converting the imported
> files to PNGs along the way. In other words: I would like my tensors to be
> [256*256*int_range,1]. Thanks for the help!
>
> Hi, I am using the tiff data too. I was wondering what image loader you
> are using in open the tiff images to load 1-channel grayscale tiff images.
> Also, did you change the IMG_EXTENSION variable in the file
> ""image_folder.py""? Thx.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/565#issuecomment-486830789>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AACDP2Z6AE7F4PDCW7KDSLDPSIJZHANCNFSM4G4HKWQQ>
> .
>
",tried ago lost track went however sure variable wrote hi first paper code pleasure reading running different size channel per wide range code suggest similar got training think would possible change code go channel tiff channel tiff information loss far understand present converting along way would like thanks help hi tiff data wondering image loader open tiff load tiff also change variable file thread reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
487640985,"You need to collect the artwork by yourself using some online [tools](https://github.com/lucasdavid/wikiart) and train a model using our codebase.
For Cloude Monet to Tomas Kinkade, you could contact Dr. Fouhey.",need collect train model could contact,issue,negative,neutral,neutral,neutral,neutral,neutral
487416617,Maybe your newly added loss is too strong compared to existing CycleGAN losses. ,maybe newly added loss strong,issue,negative,positive,positive,positive,positive,positive
487319184,"You need to make two changes. 
1. add `idt_A` and `idt_B` to visual_names. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L60).
2. generate idt_A and idt_B in the forward [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L117).
```python
self.idt_A = self.netG_A(self.real_B)
self.idt_B = self.netG_B(self.real_A)
```",need make two add see line generate forward function python,issue,negative,neutral,neutral,neutral,neutral,neutral
487318929,We use a fully convolutional network (FCN) as our generator. One advantage of FCN is that you can train and test FCN on images with different sizes. See the original FCN [paper](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) for more details.  ,use fully convolutional network generator one advantage train test different size see original paper,issue,positive,positive,positive,positive,positive,positive
487318496,"You need to add `retain_graph=True` when you are calling `backward()` for gradient penalty loss. 
See [here](https://github.com/junyanz/VON/blob/master/models/shape_gan_model.py#L80) for an example (on generating 3D data with WGAN-GP)",need add calling backward gradient penalty loss see example generating data,issue,negative,neutral,neutral,neutral,neutral,neutral
486875679,"resolved this from README.md. I was missing on option when executing `test.py` This worked for me:
`python test.py --dataroot datasets/myown_test/testA --name maps_cyclegan --model test --no_dropout`",resolved missing option worked python name model test,issue,negative,negative,negative,negative,negative,negative
486830789,"> Hi, first of all congratulations for your paper and for the code. It was a pleasure reading what you did and how you did it.
> 
> I am running CycleGAN with different types of tiffs in trainA and trainB. The tiffs are 256x256 pixels in size and have 1 channel per pixel. I am using tiffs to have a wide range of values.
> 
> I changed the code as you suggest (#320 and similar), but what I got out during the training in `./checkpoints` are three-channels PNGs. Do you think it would be possible to change the code so that it goes from 1 channel tiff to 1 channel tiff with no information loss? As far as I understand, at present you are converting the imported files to PNGs along the way. In other words: I would like my tensors to be `[256*256*int_range,1]`. Thanks for the help!

Hi, I am using the tiff data too. I was wondering what image loader you are using in open the tiff images to load 1-channel grayscale tiff images. Also, did you change the IMG_EXTENSION variable in the file ""image_folder.py""? Thx.",hi first paper code pleasure reading running different size channel per wide range code suggest similar got training think would possible change code go channel tiff channel tiff information loss far understand present converting along way would like thanks help hi tiff data wondering image loader open tiff load tiff also change variable file,issue,positive,positive,neutral,neutral,positive,positive
486613550,"> Hi, I try to make work with pix2pix, so I made changes in pix2pix_model.py
> 
> ```
>     def backward_D(self):
>         """"""Calculate GAN loss for the discriminator""""""
>         # Fake; stop backprop to the generator by detaching fake_B
>         fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator
>         pred_fake = self.netD(fake_AB.detach())
>         self.loss_D_fake = self.criterionGAN(pred_fake, False)
>         # Real
>         real_AB = torch.cat((self.real_A, self.real_B), 1)
>         pred_real = self.netD(real_AB)
>         self.loss_D_real = self.criterionGAN(pred_real, True)
>         #wgan-gp
>         gradient_penalty, gradients = networks.cal_gradient_penalty(self.netD,real_AB,fake_AB,'cuda')
>         # Combined loss and calculate gradients
>         self.loss_D = (self.loss_D_real + self.loss_D_fake + gradient_penalty) * 0.5
>         self.loss_D.backward()
> ```
> And now I get error:
> 
> ```
> Traceback (most recent call last):
>   File ""train.py"", line 51, in <module>
>     model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
>   File ""/storage01/nikitam/pix2pix_wass/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py"", line 132, in optimize_parameters
>     self.backward_G()                   # calculate graidents for G
>   File ""/storage01/nikitam/pix2pix_wass/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py"", line 120, in backward_G
>     self.loss_G.backward()
>   File ""/storage01/nikitam/nikita3/lib/python3.6/site-packages/torch/tensor.py"", line 93, in backward
>     torch.autograd.backward(self, gradient, retain_graph, create_graph)
>   File ""/storage01/nikitam/nikita3/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward
>     allow_unreachable=True)  # allow_unreachable flag
> RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.
> ```
I am confused why the error occurs while you only call the backward() once.
",hi try make work made self calculate gan loss discriminator fake stop generator use conditional need feed input output discriminator false real true combined loss calculate get error recent call last file line module calculate loss get update network file line calculate file line file line backward self gradient file line backward flag trying backward graph second time already freed specify calling backward first time confused error call backward,issue,negative,negative,neutral,neutral,negative,negative
485802345,"I have the same issue, always stopping at 15 epochs.
Same I was not starting visdom manually.
So I disabled visdom and it is now working.
I use tensoborad instead.",issue always stopping starting manually disabled working use instead,issue,negative,negative,neutral,neutral,negative,negative
485633354,@nidetaoge : I also have same problem when add other loss. The D loss goes to zero and the generator loss goes up. How do you solve the issue?,also problem add loss loss go zero generator loss go solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
485206121,SSIM looks great. We don't have a plan to add it by ourselves. But feel free to do a PR request.  ,great plan add feel free request,issue,positive,positive,positive,positive,positive,positive
484731989,"If you use dropout during test time, you will get randomness. If you use some preprocessing options, which involve random cropping, you might get slightly different results. ",use dropout test time get randomness use involve random might get slightly different,issue,negative,negative,negative,negative,negative,negative
484728715,"I get reproducibility in both training and test with adding a fixed seed.  I understand that there are a number of factors during training (weight initialization, etc), but what is random in the test phase?",get reproducibility training test fixed seed understand number training weight random test phase,issue,negative,negative,negative,negative,negative,negative
484207417,"Hi there, after running the `train.py`-script code through the debugger (and being lucky enought that it got stuck again), I noticed that the program is not running past [this line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/22fe4465c120ac92a2de09c3c2d50cbbe0e63f29/train.py#L43) in the `train.py`script

It gets stuck in [these lines](https://github.com/pytorch/pytorch/blob/a24163a95edb193ff7b06e98cd69bf7cfd4c0d2f/torch/utils/data/dataloader.py#L94-L111) of pytorch code (comments added by myself):
```python
    while True:  # This loop takes forever
        try:
            r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL) # r: <class 'tuple'>: (709,[538])
        except queue.Empty:
            if watchdog.is_alive(): # and for some reason watchdog is always alive
                continue                  # so this loop keeps going forever :(
            else:
                break
        if r is None:
            break
        idx, batch_indices = r
        try:
            samples = collate_fn([dataset[i] for i in batch_indices])
        except Exception:
            data_queue.put((idx, ExceptionWrapper(sys.exc_info())))
        else:
            data_queue.put((idx, samples))
            del samples
```
This is the stacktrace I get:
```
_worker_loop, dataloader.py:97
run, process.py:93
_bootstrap, process.py:258
_launch, popen_fork.py:73
__init__, popen_fork.py:19
_Popen, context.py:277
_Popen, context.py:223
start, process.py:105
__init__, dataloader.py:289
__iter__, dataloader.py:501
__iter__, __init__.py:90
<module>, train.py:43
```
followed by the debugger that calls the train script.
```
execfile, _pydev_execfile.py:18
run, pydevd.py:1135
main, pydevd.py:1735
<module>, pydevd.py:1741
```
I still have no clue as to what makes this happen.
Any thoughts? Is it possible that this has something to do with the fact that I did not explicitly start the visdom server myself or something?
I'll try to keep looking whilst debugging, but this ""low-level"" code is way out of my comfort zone, so the help of anyone who knows more about this kind of issue is very much appreciated. Thanks!",hi running code lucky got stuck program running past line script stuck code added python true loop forever try class except reason watchdog always alive continue loop going forever else break none break try except exception else get run start module train script run main module still clue happen possible something fact explicitly start server something try keep looking whilst code way comfort zone help anyone kind issue much thanks,issue,positive,positive,positive,positive,positive,positive
483896612,"Hi, it seems not quite reasonable to combine these flags. I found that for this kind of U-Net structure, directly adding SN to G harms performance, while directly applying SN to D truly helps get better visually pleasant results. Besides, BN/IN should be always used for U-Net while SN is an additional stuff to get better results.
Maybe we can rename `use_spectral_norm_D` and `use_spectral_norm_G` to `use_spectral_norm` for `UNet` and `ResNet` constructors.",hi quite reasonable combine found kind structure directly performance directly truly get better visually pleasant besides always used additional stuff get better maybe rename,issue,positive,positive,positive,positive,positive,positive
483856533,"Great! Could you combine two flags (e.g., `use_spectral_norm_D` and `norm` and `use_spectral_norm_G` and `norm`)?
I would like to keep the same function interface if possible.
@taesungp ",great could combine two norm norm would like keep function interface possible,issue,positive,positive,positive,positive,positive,positive
483856001,"It is possible. K80 is 2.5 times slower than GTX 1080. Therefore, it might take 2-3 days. ",possible time therefore might take day,issue,negative,neutral,neutral,neutral,neutral,neutral
483359511,Not sure what is the reason. Maybe @SsnL @taesungp have a clue. ,sure reason maybe clue,issue,negative,positive,positive,positive,positive,positive
483156879,"End of epoch 113 / 200 	 Time Taken: 180 sec
learning rate = 0.0001743
(epoch: 114, iters: 50, time: 0.275, data: 0.002) D_A: 0.104 G_A: 0.423 cycle_A: 0.404 idt_A: 0.123 D_B: 0.262 G_B: 0.168 cycle_B: 0.489 idt_B: 0.114 
(epoch: 114, iters: 150, time: 1.337, data: 0.002) D_A: 0.031 G_A: 0.732 cycle_A: 0.323 idt_A: 0.125 D_B: 0.119 G_B: 0.549 cycle_B: 0.468 idt_B: 0.111 
(epoch: 114, iters: 250, time: 0.275, data: 0.002) D_A: 0.031 G_A: 0.829 cycle_A: 0.415 idt_A: 0.104 D_B: 0.160 G_B: 0.348 cycle_B: 0.404 idt_B: 0.118 

the command i run is this one 
python3 train.py --dataroot ./datasets/nicosia_data_cars --name nicosia_cycle_cars --model  cycle_gan --niter 100 --niter_decay 100 --num_threads 1

i use images 512*256
I use a 1080Ti
it is stacked there 3 days. If i stopped the training and continue it is stacked in the first iteration.If i start a new training even for --niter 1 --niter_decay 1 it is stacked again in the first iteration. The only way i can make it finish the train is to shut down the computer,start again and then continue the training.Some training needed 3 runs and others only two to complete 200 epochs",end epoch time taken sec learning rate epoch time data epoch time data epoch time data command run one python name model niter use use ti day stopped training continue first start new training even niter first iteration way make finish train shut computer start continue training two complete,issue,negative,positive,positive,positive,positive,positive
483056655,"Unfortunately, the demo is publicly available now.  But you can easily build one based on this codebase.",unfortunately publicly available easily build one based,issue,negative,positive,positive,positive,positive,positive
483056548,"Yes. To make the `latest_net_G.pth` the latest model, you need to save your model every iteration. ",yes make latest model need save model every iteration,issue,positive,positive,positive,positive,positive,positive
483056305,I see. You are translating images from B to A. I was confused before. You can just use the entire image as input. ,see confused use entire image input,issue,negative,negative,negative,negative,negative,negative
483056081,I added noises to the first half of the generator (encoder). ,added first half generator,issue,negative,positive,neutral,neutral,positive,positive
483054820,I don't know the name of this issue. Maybe try a smaller cycle loss.,know name issue maybe try smaller cycle loss,issue,negative,neutral,neutral,neutral,neutral,neutral
483041443,Thanks for your point. I tried both unet and resnet and result are the same. It does not solve my problem. The mapping from source to target are identical. It means the result of the synthetic image similar to the real image. Do you know what is the name of the issue?,thanks point tried result solve problem source target identical result synthetic image similar real image know name issue,issue,negative,positive,positive,positive,positive,positive
483032101,"I recommend that you use ResNet for CycleGAN, as U-Net doesn't work as well as ResNet in the CycleGAN model. You can use a smaller model `resnet_6blocks` is speed is a concern. It takes some time for a model to learn the mapping. If you like bigger changes, you can also decrease the cycle consistency loss. ",recommend use work well model use smaller model speed concern time model learn like bigger also decrease cycle consistency loss,issue,positive,neutral,neutral,neutral,neutral,neutral
482922613,Sorry I may write the wrong thing so you may misunderstand. I mean that the result of Unet gives the synthetic image is too similar to the input source image. I mean that the trained unet generator looks feeds directly the input to the output :(. so nothing to learning in down-sampling path. I tried the resnet before but training speed too slow although learning parameter less than Unet,sorry may write wrong thing may misunderstand mean result synthetic image similar input source image mean trained generator directly input output nothing learning path tried training speed slow although learning parameter le,issue,negative,negative,negative,negative,negative,negative
482920247,"I recommend that you use the Resnet generator for CycleGAN training. What's the problem? The more similar two domains are, the easier the model training will be.",recommend use generator training problem similar two easier model training,issue,negative,neutral,neutral,neutral,neutral,neutral
482911854,"Thank you for your response. Although the problem is theoretically possible, I just realize that it's actually due to a bug in my code, where I plot images coming out of the image buffers.",thank response although problem theoretically possible realize actually due bug code plot coming image,issue,negative,negative,neutral,neutral,negative,negative
482907721,"This is possible for some applications, as many permutations could satisfy cycleGAN loss, and it is an ill-posed problem. Two potential improvements could be (1): use a smaller generator (2) add an identity [loss](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#what-is-the-identity-loss-322-373-362) to further regularize the capacity of the network. ",possible many could satisfy loss problem two potential could use smaller generator add identity loss regularize capacity network,issue,negative,positive,positive,positive,positive,positive
482698789,Thanks. But I think checkbox artifact and the blur may different one. I meet the checkbox artifact and I just train longer and it is disappear :).,thanks think artifact blur may different one meet artifact train longer disappear,issue,negative,positive,neutral,neutral,positive,positive
482692802,Could you use batchsize=1? I haven't tested the batchsize=4 case. ,could use tested case,issue,negative,neutral,neutral,neutral,neutral,neutral
482692353,I don't have a method in my mind. VAE probably cannot reduce the blurness of the results. Maybe you can try different network architectures and upsampling [layers](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/217). ,method mind probably reduce maybe try different network,issue,negative,neutral,neutral,neutral,neutral,neutral
482648605,@junyanz : Cool. I expected VAE can reduce the blur in result but it does not. Could you suggest any paper/method to reduce the blur in the result of cycleGAN. I tried to reduce lamda to 5 but the result is still blurred. ,cool reduce blur result could suggest reduce blur result tried reduce result still blurred,issue,negative,positive,positive,positive,positive,positive
482637650,"We follow the common GAN practice. For example,  the [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) paper states that
```
• Use ReLU activation in generator for all layers except for the output, which uses Tanh.
• Use LeakyReLU activation in the discriminator for all layers.
```
We have explored this idea in follow-up [work](https://arxiv.org/pdf/1711.11586.pdf). It helps to produce different results given the same input. ",follow common gan practice example paper use activation generator except output tanh use activation discriminator idea work produce different given input,issue,negative,negative,negative,negative,negative,negative
482636061,"I recommend that you try a different `--preprocess` option (e.g., `resize_and_crop`)",recommend try different option,issue,negative,neutral,neutral,neutral,neutral,neutral
482635123,`resnet_6blocks` and `resnet_9blocks` support more input image sizes. ,support input image size,issue,negative,neutral,neutral,neutral,neutral,neutral
482634465,You could click the link `http://localhost:8097`. Or type it in your browser. ,could click link type browser,issue,negative,neutral,neutral,neutral,neutral,neutral
482600434,@ChristianEschen : The RELU. Check the relu operator. Let me know if it work,check operator let know work,issue,negative,neutral,neutral,neutral,neutral,neutral
482273909,@phillipi : Nice idea. Could you tell me what we expect when adding VAE in the generator? What is the purpose of the adding VAE? Thanks,nice idea could tell expect generator purpose thanks,issue,positive,positive,positive,positive,positive,positive
482127548,@phamnam95 : So do you use tanh in last layer? What kind of normalization do you prefer? I want to use ` (data-mean(data))/std(data)` but it seem that pytorch has no layer to normalize zero mean and unit variance,use tanh last layer kind normalization prefer want use data data seem layer normalize zero mean unit variance,issue,positive,positive,neutral,neutral,positive,positive
482056248,"obviously that was caused by the default model unet256, that supports images sized 256x256px (and multiples.).
fixed using --crop_size 256.
Probably an alternative model (different than '--netG unet256') can be used to avoid the problem.",obviously default model sized fixed probably alternative model different used avoid problem,issue,negative,positive,neutral,neutral,positive,positive
481932667,"Hi, I want to use the visdom so I change the display_id but it didn't show up. This is my current results, do you have any suggestion what to do next?
![image](https://user-images.githubusercontent.com/26785712/55924828-554bae00-5c46-11e9-8005-7db4631db49e.png)
",hi want use change show current suggestion next image,issue,negative,neutral,neutral,neutral,neutral,neutral
481916467,"I can help add this in a few days when I am free, if you don't mind.",help add day free mind,issue,positive,positive,positive,positive,positive,positive
481833103,@taesungp could you help add one? ,could help add one,issue,negative,neutral,neutral,neutral,neutral,neutral
481832860,"What might have happened is that: you have an image with (width, height) = (3000, 1500). 
After resizing using `--load_size 286 --preprocess scale_width_and_crop`. The image size will be (256, 128). Cropping it using `--crop_size 256` will produce a randrange error.  ",might image width height image size produce error,issue,negative,neutral,neutral,neutral,neutral,neutral
481608913,"> You would need to run an ssh tunnel for the used port.
> […](#)
> On Thu, Dec 6, 2018 at 20:31 HuiZHANG ***@***.***> wrote: @SsnL <https://github.com/SsnL> Thank you. But it said ""This site can’t be reached"" when I click the http link. I run cycleGAN with ssh to remote server. Can I open it in my personal computer? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#371 (comment)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/371#issuecomment-444855896)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFaWZXCwQaHcD-vT3nf08OdQigJhbIt5ks5u2Q4zgaJpZM4WeLLP> .

how to do that? would you share about it?
Thank you",would need run tunnel used port wrote thank said site click link run remote server open personal computer reply directly view comment mute thread would share thank,issue,positive,neutral,neutral,neutral,neutral,neutral
481307349,@taesungp : But he removed `--eval` and issue solved. It means that dropout and batch norm have behavior as training. Am I right? I think dropout must be zero and batchnorm must do as the evaluation,removed issue dropout batch norm behavior training right think dropout must zero must evaluation,issue,negative,positive,positive,positive,positive,positive
481213843,"I have more than a thousand images, but the training is particularly slow. I only have the following results：(epoch: 1, iters: 100, time: 2.498, data: 4.119) D_A: 0.426 G_A: 0.380 cycle_A: 4.048 idt_A: 1.767 D_B: 0.354 G_B: 0.448 cycle_B: 3.579 idt_B: 1.954
(epoch: 1, iters: 200, time: 1.088, data: 0.001) D_A: 0.316 G_A: 0.315 cycle_A: 2.628 idt_A: 1.365 D_B: 0.323 G_B: 0.333 cycle_B: 2.907 idt_B: 1.322
 What is the reason?",thousand training particularly slow following epoch time data epoch time data reason,issue,negative,negative,negative,negative,negative,negative
481153453,Alright. Where do I perform in-place operations in the code shown above?,alright perform code shown,issue,negative,neutral,neutral,neutral,neutral,neutral
481083372,Thank you very much!!! I have seen the link of 17 Oct 2018,thank much seen link,issue,negative,positive,positive,positive,positive,positive
480648403,"> Have you followed the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#evaluating-labels2photos-on-cityscapes)?

Thank you for your prompt reply Junyan! 
Yes, I have installed Caffe and downloaded the model(in Ubuntu, using CPU). I also ran the command to add to the python path. Then executed the following command to run the script:

python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir ./datasets/cityscapes/ --result_dir ./results/cityscapes_label2photo_pretrained/test_latest/images --output_dir ./eval/",thank prompt reply yes model also ran command add python path executed following command run script python,issue,positive,neutral,neutral,neutral,neutral,neutral
480614661,Details are slightly different. But the overall structure is similar. ,slightly different overall structure similar,issue,negative,neutral,neutral,neutral,neutral,neutral
480614108,I meet the problem before. It due to learning conv in upsampling path. You have two choices (1) training longer. (2) replace the conv2d transpose with upsampling followed by pad and conv3x3. Detail in https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/382,meet problem due learning path two training longer replace transpose pad detail,issue,negative,negative,negative,negative,negative,negative
480613900,"Thanks. I tried it and confirm that 4x4 bottleneck reduces the artifact boundary due to the conv2dtransposed operator. I also replace the conv2d transposed with upsampling -pad - conv3x3 but the artifact does not resolve. So, I guess the 4x4 bottleneck may good when we consider detail in the boundary. ",thanks tried confirm bottleneck artifact boundary due operator also replace artifact resolve guess bottleneck may good consider detail boundary,issue,positive,positive,positive,positive,positive,positive
480613868,"It might be caused by visdom. Could you activate the visdom server before starting the program?
```bash
python -m visdom.server
``` 
See this [post](https://github.com/facebookresearch/visdom/issues/549) for more details. ",might could activate server starting program bash python see post,issue,negative,neutral,neutral,neutral,neutral,neutral
480613681,"It should not make a big difference, as we will average the loss over all the output.  Feel free to try different paddings in your experiments.",make big difference average loss output feel free try different,issue,negative,positive,neutral,neutral,positive,positive
480613582,A few inferior results are expected. Reasons could be that your test input images are different from the training set images. (out of distribution),inferior could test input different training set distribution,issue,negative,neutral,neutral,neutral,neutral,neutral
480613452,"I don't know. There are three settings. 
Setting 1. Images A: 1000, Images B: 1000
Setting 2. Images A: 3000, Images B: 1000
Setting 3. Images A: 3000, Images B: 3000

Have you observed that Setting 2 works better than Setting 3? I  understand that Setting 2 works better than Setting 1 as there are more training data. 
",know three setting setting setting setting work better setting understand setting work better setting training data,issue,positive,positive,positive,positive,positive,positive
480613239,"@zhangyulee It looks fine in your case. I guess that your model is still at epoch 1? Epoch +=1 after the model is trained with all the images once.  It should be fine unless you have fewer than 200 images in the trainng set. 
@itsss  I haven't seen this error before. Did you still get this? Does it work for our training datasets?",fine case guess model still epoch epoch model trained fine unless set seen error still get work training,issue,negative,positive,positive,positive,positive,positive
480613026,"What does ""50-100 frames"" mean? Is it a video or an image? Training a CycleGAN on 256x256 images requires about 8 GB memory. ",mean video image training memory,issue,negative,negative,negative,negative,negative,negative
480612308,Feel free to try `1x1` or `4x4` bottlenecks. I don't think that `1x1` will lose spatial information as U-Net has other skip connections.,feel free try think lose spatial information skip,issue,negative,positive,positive,positive,positive,positive
480576423,"I am also trying to extract intermediate feature maps.
I have done as @happsky has suggested. 
However, I receive the following message:
""Exception has occurred: RuntimeError
a leaf Variable that requires grad has been used in an in-place operation.""

Does anyone have a suggestion to overcome this issue?",also trying extract intermediate feature done however receive following message exception leaf variable grad used operation anyone suggestion overcome issue,issue,negative,neutral,neutral,neutral,neutral,neutral
480551358,"> 300 is very likely too small a dataset.

However, the test results is not all the images are like  this, only a few are like this. ",likely small however test like like,issue,positive,negative,negative,negative,negative,negative
480414213,"No, `D_A` is responsible for discriminating in `B` domain. You can think of it as the paired discriminator for `G_A`.",responsible discriminating domain think paired discriminator,issue,negative,positive,positive,positive,positive,positive
480413955,You don't have enough memory. See the error.,enough memory see error,issue,negative,neutral,neutral,neutral,neutral,neutral
480413787,"The `Model` class used here includes multiple networks (`Module`s), optimizers for the networks, and learning rate schedulers for the optimizers. So it is really more than what `nn.Module` should do. It is more like an owner of the `Module`s, rather than is a `Module` itself.

In the current codebase, `initialize` could very well just be put in `__init__`. However, one could see the possibility of needing to reinitialize an existing model. In that case it will be helpful.",model class used multiple module learning rate really like owner module rather module current initialize could well put however one could see possibility needing model case helpful,issue,positive,positive,neutral,neutral,positive,positive
479707174,"Related question: why did you make a different `initialize` method for each model class instead of just using the default `__init__`?

@SsnL ",related question make different initialize method model class instead default,issue,negative,neutral,neutral,neutral,neutral,neutral
479321387,"To remove artifact, try the up-sampling without learning paramter. The layer is nn.Upsample in pytorch. Hope it help",remove artifact try without learning layer hope help,issue,negative,neutral,neutral,neutral,neutral,neutral
479319079,"> @sigmagod Hi, how did you deal with this issue ? Did changing the backgrounds help you in the case ?

@sudharavali Finally, I use style transfer and texturegan to rebuild CAD model texture, then synthesis with background. ",hi deal issue help case finally use style transfer rebuild cad model texture synthesis background,issue,negative,neutral,neutral,neutral,neutral,neutral
479290987,"I have the same problem, how do you solve it last?
(epoch: 1, iters: 100, time: 2.498, data: 4.119) D_A: 0.426 G_A: 0.380 cycle_A: 4.048 idt_A: 1.767 D_B: 0.354 G_B: 0.448 cycle_B: 3.579 idt_B: 1.954
(epoch: 1, iters: 200, time: 1.088, data: 0.001) D_A: 0.316 G_A: 0.315 cycle_A: 2.628 idt_A: 1.365 D_B: 0.323 G_B: 0.333 cycle_B: 2.907 idt_B: 1.322",problem solve last epoch time data epoch time data,issue,negative,neutral,neutral,neutral,neutral,neutral
478926689,"Thanks. I have a question. I found taht when the number of training sets in X and Y domains is approximately equal, the effect of training is better than that of three or four times the number of images in one domain. Why is that?

",thanks question found number training approximately equal effect training better three four time number one domain,issue,positive,positive,positive,positive,positive,positive
478827249,"@guxiao0822 Thank you very much for great suggestion!
I confirmed it was working fine after editing as you commented.

As far as my understanding, function `get_transform` in class `BaseDataset` (or subclasses of it) is called for building transformation procedure for loading images.
In this procedure, grayscale-transformation is done if `grayscale == True` (means `input_nc` or `output_nc` is 1).
So in case of grayscale, the shape of tensor is chenged from (3, height, width) to (1, height, width).
However, normalizing procedure is done for tensor with shape (3, height, width) for all cases (line 107--108) and it caused my error. 
I think better code from line 106 to line 108 of `base_dataset.py` is as follows:

```python:
if convert:
        transform_list.append(transforms.ToTensor())
        if grayscale:
            transform_list.append(transforms.Normalize([0.5], [0.5]))
        else:
            transform_list.append(transforms.Normalize((0.5, 0.5, 0.5), 
                                                        (0.5, 0.5, 0.5)))
```",thank much great suggestion confirmed working fine far understanding function class building transformation procedure loading procedure done true case shape tensor height width height width however procedure done tensor shape height width line error think better code line line python convert else,issue,positive,positive,positive,positive,positive,positive
478759431,"@sigmagod Hi, how did you deal with this issue ? Did changing the backgrounds help you in the case ?",hi deal issue help case,issue,negative,neutral,neutral,neutral,neutral,neutral
478714329,"Hi Dr.@junyanz ,
I want to use CycleGAN,for the same please let me know how of GPU Ram is required for train about 50-100 frames and how much of time it would be required for train any model?

Thanks in advance!!
Gourav G.",hi want use please let know ram train much time would train model thanks advance,issue,positive,positive,positive,positive,positive,positive
478692943,"Hi @giladdiv , I am currently doing something very similar to what you have done . Can you tell me how many iterations if you have tried this on, any other changes to have made to your code to get the best possible results ? ",hi currently something similar done tell many tried made code get best possible,issue,positive,positive,positive,positive,positive,positive
478599235,You are free to add them. We follow the U-net implementation in the original pix2pix paper. ,free add follow implementation original paper,issue,positive,positive,positive,positive,positive,positive
478568858,"Thanks for the reply! I've also seen, from [this blog post on Residual Net](http://torch.ch/blog/2016/02/04/resnets.html), that removing the ReLU layer at the end of residual block slightly improves performance.",thanks reply also seen post residual net removing layer end residual block slightly performance,issue,negative,positive,neutral,neutral,positive,positive
478536295,Right! My solution also lead to similar direction. Meanwhile that PR has some merge conflicts!,right solution also lead similar direction meanwhile merge,issue,negative,positive,positive,positive,positive,positive
478392865,Great. Why not add pad in the unet network? Thanks,great add pad network thanks,issue,positive,positive,positive,positive,positive,positive
478202801,"Could you verify that basic cuda comm primitives works on your machine? E.g., try `torch.cuda.broadcast`. Other primitives you can try can be found at https://pytorch.org/docs/stable/cuda.html#communication-collectives",could verify basic work machine try try found,issue,negative,neutral,neutral,neutral,neutral,neutral
478202618,I haven't been able to reproduce the error on my machine. @taesungp @SsnL ,able reproduce error machine,issue,negative,positive,positive,positive,positive,positive
478202551,"If you keep getting the checkboard artifacts, you can try different upsampling layers. See this post for more [details](https://distill.pub/2016/deconv-checkerboard/).  There is a [PR](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/382) related to this. ",keep getting try different see post related,issue,negative,neutral,neutral,neutral,neutral,neutral
478202267,It's a small speedup trick. `set_requires_grad=False` will stop calculating gradients for the discriminator while we update the generator. It can save some time and memory. ,small trick stop calculating discriminator update generator save time memory,issue,negative,negative,negative,negative,negative,negative
478202144,There is a ReLU layer in the [middle](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L407). We mostly follow the network implementation of [fast-neural-style-transfer](https://github.com/jcjohnson/fast-neural-style/blob/master/fast_neural_style/models.lua#L10). I haven't tested different implementations of resnet blocks. ,layer middle mostly follow network implementation tested different,issue,negative,positive,positive,positive,positive,positive
478125243,"> > I am having an issue with running CycleGAN on multiple GPUs. It works well when running on a single GPU (albeit very slowly, as expected) using
> > ```
> >  python3 train.py --dataroot ./datasets/cezanne2photo --name cezanne2photo_cyclegan --model cycle_gan
> > ```
> > Now when I try to train on multiple GPUs using
> > ```
> > python3 train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --gpu_ids 0,1,2,3 --batch_size 16 --norm instance
> > ```
> > I have also tried to run it with and without the `--norm instance` parameter and
> > also tried with `--batch_size 4`. This always leads to the same result:
> > The program stops at ""create web directory"" (I've let it run for a couple of days at this point, without any noticeable progress). It looks like a single python3 process is putting a single thread under full load, none of the other python3 processes get any CPU time. Trying to kill that process also seems impossible - I have had to restart the machine every time. None of the GPUs are ever under load and barely any of their memory is used.
> > I am using Python 3.5.2, CUDA 9.2, pytorch 1.0, cuDNN 7.4.1. The system has four 1080ti GPUs and an AMD Ryzen Threadripper 1950X.
> 
> It also happen to me. How to solve this? Guys, i need your help!

The same issue for me, any suggestion?",issue running multiple work well running single albeit slowly python name model try train multiple python name model norm instance also tried run without norm instance parameter also tried always result program create web directory let run couple day point without noticeable progress like single python process single thread full load none python get time trying kill process also impossible restart machine every time none ever load barely memory used python system four ti also happen solve need help issue suggestion,issue,negative,negative,neutral,neutral,negative,negative
478069843,"I get the same noise patterns in my checkpoint images when using 128x350 images, more epochs don't seem to fix the problem.",get noise seem fix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
477229446,I believe only the network weights are being saved in the current codebase. You can easily extend the code to save the state_dict of optimizers. See this [post](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for more details.  ,believe network saved current easily extend code save see post,issue,positive,positive,positive,positive,positive,positive
477227802,Interesting. I don't think the learning rate will affect GPU memory requirement. Maybe you can reset the GPU and train the model again. ,interesting think learning rate affect memory requirement maybe reset train model,issue,negative,positive,positive,positive,positive,positive
477013935,"Thanks for the reply
no other processes are running.
Even nvidia-smi shows no processes running on this gpu. 

I just changed initial learning rate from default value.. I also did not increase batch size from default value (1). I suppose that has nothing to do with this error ",thanks reply running even running initial learning rate default value also increase batch size default value suppose nothing error,issue,positive,positive,neutral,neutral,positive,positive
476904244,"We treat the label map as an RGB image in pix2pix and CycleGAN. This encoding will not work if you have too many categories. In more recent work pix2pixHD and [SPADE](https://github.com/NVlabs/SPADE), we treat it as a one-hot label map 30x2048x1024. ",treat label map image work many recent work spade treat label map,issue,positive,positive,positive,positive,positive,positive
476903809,We don't have a PyTorch pre-trained model. But feel free to train one by yourself. Please contact @phillipi for the Torch model. ,model feel free train one please contact torch model,issue,positive,positive,positive,positive,positive,positive
476903566,It seems that your program has used too much memory ( 11.93 GiB) or another process is running on the same GPU. Which parameters have you changed? ,program used much memory gib another process running,issue,negative,positive,positive,positive,positive,positive
476814768,I have fixed this problem with the latest commit. Please check out the code again. ,fixed problem latest commit please check code,issue,negative,positive,positive,positive,positive,positive
476789686,You cannot create a CycleGAN model like this. Please follow the README for more details. ,create model like please follow,issue,positive,neutral,neutral,neutral,neutral,neutral
476788928,It should be fixed with the latest commit. Please check out the code again. ,fixed latest commit please check code,issue,positive,positive,positive,positive,positive,positive
476771320,"I think the model aims to implement the formulation (e.g., pix2pix/cyclegan) rather than a network architecture. That's why we don't use `nn.module`. We implemented the code during the first release of PyTorch 0.1, when `nn.module` was not so popular.  What's your opinion (cons vs. pros)?  @SsnL ",think model implement formulation rather network architecture use code first release popular opinion,issue,negative,positive,positive,positive,positive,positive
476766695,"It's hard to get intermediate output using the current code. One possible way is to give different names to different modules.  
```python
self.unet_block1 =  UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)
self.unet_block2 =UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=self.unet_block1, norm_layer=norm_layer)
```
You need to change the forward function as well. 
",hard get intermediate output current code one possible way give different different python need change forward function well,issue,negative,negative,neutral,neutral,negative,negative
476692564,"Thanks. I am following your suggestion by changing the generator. I used Unet generator network. I want to get the intermediate output (likes yellow arrow). However, your coding style is sequence model. 

https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/18a40e606eb5ef5214db84b1bb24b9f0e3641371/models/networks.py#L459

![x2 png 750x0_q75_crop](https://user-images.githubusercontent.com/24875971/55006890-da7f6380-4fb4-11e9-82ff-57c60b07045b.png)

How can I get intermedidate output if I still want to use your code above?

",thanks following suggestion generator used generator network want get intermediate output yellow arrow however style sequence model get output still want use code,issue,positive,positive,neutral,neutral,positive,positive
476427356,"Thanks @junyanz . I confirmed that pix2pix worked much better than cycleGAN in case of paired data, although the paired data is not aligned well. Hope it can help other people

However, the synthetic image is still blurred. Do you think the reason comes from L1 loss? I am using L1 loss for Generation (with lambda=10) and MSE for Discrimination.

**Synthetic image**
![Screenshot from 2019-03-25 20-21-55](https://user-images.githubusercontent.com/24875971/54962573-c0ee0580-4f3b-11e9-890c-06386d7c8b14.png)

**Real image**
![Screenshot from 2019-03-25 20-26-05](https://user-images.githubusercontent.com/24875971/54962702-3c4fb700-4f3c-11e9-883d-bb7864a03f19.png)
",thanks confirmed worked much better case paired data although paired data well hope help people however synthetic image still blurred think reason come loss loss generation discrimination synthetic image real image,issue,positive,positive,positive,positive,positive,positive
476212211,2 or 3 pixels should be fine. You can also downsample both input and output images by 4x when you calculating the L1 loss. ,fine also input output calculating loss,issue,negative,positive,positive,positive,positive,positive
476199707,Thanks. But the problem is that the two images aligned not so well. Do you think pix2pix still work?,thanks problem two well think still work,issue,negative,positive,positive,positive,positive,positive
476061284,"@junyanz Thank you so much,
@taesungp Looking forward to your reply. Thanks",thank much looking forward reply thanks,issue,positive,positive,positive,positive,positive,positive
476050784,We do provide precomputed [images](http://efrosgans.eecs.berkeley.edu/cyclegta/cyclegta.zip) (18GB). @taesungp might be able to help you about pretrained models. ,provide might able help,issue,negative,positive,positive,positive,positive,positive
476050613,The current code works with Python 3. ,current code work python,issue,negative,neutral,neutral,neutral,neutral,neutral
476050546,"The pix2pix output is stochastic due to dropout. 
@yanjing1988  could you tell us your command and cuda version? ",output stochastic due dropout could tell u command version,issue,negative,negative,negative,negative,negative,negative
476048542,"The time depends on the size of the data (and the number of images). If you think the display takes too much time, you can reduce `display_freq` and `update_html_freq`.",time size data number think display much time reduce,issue,negative,positive,positive,positive,positive,positive
476046823,The error is not relevant to this repo. Maybe you want to post it on visdom [repo](https://github.com/facebookresearch/visdom).,error relevant maybe want post,issue,negative,positive,positive,positive,positive,positive
476000511,"I want to evaluate cityscapes datasets,because I try to reproduce the fcn score results from pip2pip by using https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix code.
I try many ways.I can not get any results when I run the scripts. All parameters are zero.
I still want to know how to configure the cityspaces dataset
There are three folders with gtFine, originals image, and predictions.
Are the gtFine and predictions color or grayscale? And what is the size of these three types of pictures?
I use python2,caffe.I Is that possible that it was because i was using python2.7 but not python3 ?
@FishYuLi ",want evaluate try reproduce score code try many get run zero still want know configure three image color size three use python possible python python,issue,negative,positive,positive,positive,positive,positive
475922738,You have too little memory and fork can't start a new subprocess.,little memory fork ca start new,issue,negative,negative,neutral,neutral,negative,negative
475897768,"had the same problem. i did not include any batch_size parameters,  but after I added --batch_size 4,  both GPUs started working at full speed.

```  
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 2070    Off  | 00000000:01:00.0  On |                  N/A |
| 66%   71C    P2   141W / 175W |   6723MiB /  7949MiB |     94%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 2070    Off  | 00000000:02:00.0 Off |                  N/A |
| 46%   59C    P2   140W / 175W |   5888MiB /  7952MiB |     96%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
```
",problem include added working full speed driver version version name volatile fan temp compute mib mib default mib mib default,issue,negative,positive,positive,positive,positive,positive
475537740,"@junyanz  hello, do you have pre-trained model that can transfer GTA to Cityscape? 
Many thanks",hello model transfer cityscape many thanks,issue,negative,positive,positive,positive,positive,positive
475367394,"> @LilNader May i know what did u change your lines to. ty! <

@joeljoeljo I used the template data class as mentioned by @junyanz then I changed the [two lines](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L57) to:
A_img = np.load(A_path)
B_img = np.load(B_path)

In my problem I set preprocess to none
I also made a small change in base_dataset.py
- I replaced this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/18a40e606eb5ef5214db84b1bb24b9f0e3641371/data/base_dataset.py#L98) by python statement `pass`
 ",may know change used template data class two problem set none also made small change line python statement pas,issue,negative,negative,negative,negative,negative,negative
475360082,"@yanjing1988 I left it as it was. I am not quite sure what the reason was actually. But I've realized that the generated image does depend on the input pair as it can be seen from the figures. One thing that helped me quiet a bit was that I have tried to use the marker as thin as possible within the drawing application. Try playing with that, it might have an affect. ",left quite sure reason actually image depend input pair seen one thing quiet bit tried use marker thin possible within drawing application try might affect,issue,negative,positive,neutral,neutral,positive,positive
475301203,"> ![horse2zebra_fake_1](https://user-images.githubusercontent.com/24612082/52906369-38bf6680-31ff-11e9-9829-f479a80e9383.png)
> Epoch 60 of a retrained model from my implementation.
> 
> Hi @junyanz, I fixed the dark output issue simply by normalizing the output, something I forgot to consider. My second question remain unsolved, if you can help. Thank you!

I guess I have the same issue of `normalizing the output`, could you give me some idea how to normalize the output? Thanks a lot!",epoch model implementation hi fixed dark output issue simply output something forgot consider second question remain unsolved help thank guess issue output could give idea normalize output thanks lot,issue,positive,positive,neutral,neutral,positive,positive
474662930,"Somehow the error doesn't appear after I stopped using docker (not from this repository, my own).
It seems that the cause of above error was my environment.
Thanks",somehow error appear stopped docker repository cause error environment thanks,issue,negative,positive,positive,positive,positive,positive
474294311,"I found that this error happens when I follow your tutorial, too.
I cloned latest version of this repository and tried `python train.py --display_id -1 --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA` and it worked fine.
But when I added `--input_nc 1` or `--output_nc 1` or both of them, following error happens:

```
$ python train.py --display_id -1  --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA --output_nc 1
----------------- Options ---------------
               batch_size: 1                             
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 dataroot: ./datasets/facades            	[default: None]
             dataset_mode: aligned                       
                direction: BtoA                          	[default: AtoB]
              display_env: main                          
             display_freq: 400                           
               display_id: -1                            	[default: 1]
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: vanilla                       
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                lambda_L1: 100.0                         
                load_iter: 0                             	[default: 0]
                load_size: 286                           
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: pix2pix                       	[default: cycle_gan]
               n_layers_D: 3                             
                     name: facades_pix2pix               	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: unet_256                      
                      ngf: 64                            
                    niter: 100                           
              niter_decay: 100                           
               no_dropout: False                         
                  no_flip: False                         
                  no_html: False                         
                     norm: batch                         
              num_threads: 4                             
                output_nc: 1                             	[default: 3]
                    phase: train                         
                pool_size: 0                             
               preprocess: resize_and_crop               
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                  verbose: False                         
----------------- End -------------------
dataset [AlignedDataset] was created
The number of training images = 400
initialize network with normal
initialize network with normal
model [Pix2PixModel] was created
---------- Networks initialized -------------
[Network G] Total number of parameters : 54.410 M
[Network D] Total number of parameters : 2.767 M
-----------------------------------------------
create web directory ./checkpoints/facades_pix2pix/web...
Traceback (most recent call last):
  File ""train.py"", line 43, in <module>
    for i, data in enumerate(dataset):  # inner loop within one epoch
  File ""/src/workspace/pytorch-CycleGAN-and-pix2pix/data/__init__.py"", line 90, in __iter__
    for i, data in enumerate(self.dataloader):
  File ""/opt/conda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 637, in __next__
    return self._process_next_batch(batch)
  File ""/opt/conda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 658, in _process_next_batch
    raise batch.exc_type(batch.exc_msg)
RuntimeError: Traceback (most recent call last):
  File ""/opt/conda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 138, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File ""/opt/conda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 138, in <listcomp>
    samples = collate_fn([dataset[i] for i in batch_indices])
  File ""/src/workspace/pytorch-CycleGAN-and-pix2pix/data/aligned_dataset.py"", line 55, in __getitem__
    A = A_transform(A)
  File ""/opt/conda/lib/python3.5/site-packages/torchvision/transforms/transforms.py"", line 60, in __call__
    img = t(img)
  File ""/opt/conda/lib/python3.5/site-packages/torchvision/transforms/transforms.py"", line 163, in __call__
    return F.normalize(tensor, self.mean, self.std, self.inplace)
  File ""/opt/conda/lib/python3.5/site-packages/torchvision/transforms/functional.py"", line 208, in normalize
    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])
RuntimeError: output with shape [1, 256, 256] doesn't match the broadcast shape [3, 256, 256]

```

Why do I take such error when setting input or output channels to 1 ?
Do you have any solutions ?",found error follow tutorial latest version repository tried python name model direction worked fine added following error python name model direction beta false default none direction default main default epoch latest vanilla normal true default none default linear model default name default basic niter false false false norm batch default phase train false false suffix verbose false end number training initialize network normal initialize network normal model network total number network total number create web directory recent call last file line module data enumerate inner loop within one epoch file line data enumerate file line return batch file line raise recent call last file line file line file line file line file line return tensor file line normalize mean none none none none output shape match broadcast shape take error setting input output,issue,negative,negative,neutral,neutral,negative,negative
474190895,"I first use python2 and find the problem with ABC, later I change ABC to ABCMeta and it comes the similar problems
Later, I use the python3 with ABCMeta, and get the same problem as you, after I change ABCMeta back to ABC, it works",first use python find problem later change come similar later use python get problem change back work,issue,negative,positive,neutral,neutral,positive,positive
473761091,"I think  i forget add ""--save_latest_freq"" and ""--save_epoch_freq"" so that I can continue to the latest training",think forget add continue latest training,issue,negative,positive,positive,positive,positive,positive
473755010,"I break down train by ""ctrL+c"".Would this operation save training?

thanks",break train operation save training thanks,issue,positive,positive,positive,positive,positive,positive
473753867,"my last train in epoch 3 ,but not finished. Then I continue""python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --gpu_ids -1 --epoch_count 3""

I try""python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --gpu_ids -1 --continue_train --epoch_count 3"" .But show""FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/maps_cyclegan/latest_net_G_A.pth'"" 

should I restart from epoch 1?


",last train epoch finished continue python name model try python name model show file directory restart epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
473752228,"Thanks for update.
I wish to express my appreciation for your help.",thanks update wish express appreciation help,issue,positive,positive,positive,positive,positive,positive
473678347,"You should set `--continue_train --epoch 3 --epoch_count 4` (3 is the current final epoch).
`--epoch` option means which weights is loaded for initialization of networks.
(Default value of this option is `latest`.)
`--epoch_count` option means the start number of epoch count.

In your case, when only `--continue_train` was set, latest weights were supposed to be loaded because you didn't specify `--epoch` option. 
Then ""no such file"" error occurred because latest weights were not saved yet. 

And when only `--epoch_count 3` was set,  the program started training from scratch with start epoch number 3, because you didn't set `--continue_train` option.
That's why your curve after epoch 3 had no relationship with epoch 1 and 2.",set epoch current final epoch epoch option loaded default value option latest option start number epoch count case set latest supposed loaded specify epoch option file error latest saved yet set program training scratch start epoch number set option curve epoch relationship epoch,issue,negative,positive,positive,positive,positive,positive
473676417,"when i break train (i.e epoch=3),and I add ""--continue_train"" in command.But it failed with ""no such file named lateset ...""So I just add ""--epoch_count 3"" 。It seems work,start from epoch 3 but the curve has break relationship of previous curve that epoch1 and 2 show .continue seem isolated and no relation with previous work.",break train add file add work start epoch curve break relationship previous curve epoch show seem isolated relation previous work,issue,negative,negative,negative,negative,negative,negative
471828827,"Besides, it is a segment task, and the target segment area are something like lines (about 4~10 pixel width) in the image, like roads on the map. i set the rgb value to 255 within roads area and set to 0 for other areas on label image. 

Should i use origin unet model other than pix2pix model for my task?
I noticed that origin unet model can input a pre-computed weight image, will that help for my task?",besides segment task target segment area something like width image like map set value within area set label image use origin model model task origin model input weight image help task,issue,positive,neutral,neutral,neutral,neutral,neutral
471828493," If it is over fitting, i think it will achieve more accurate result on train data. but it is not. The result on train dataset goes bad, and the faked image on train data get total lost.
So i am confused.

I have 1500 image pairs in my train dataset.
And i do some (flip, resize, rotate) to expand it to about 10000 image pairs
@grbagwe ",fitting think achieve accurate result train data result train go bad image train data get total lost confused image train flip resize rotate expand image,issue,negative,negative,neutral,neutral,negative,negative
471582983,"Correct me if I am wrong, 
 
The pixel values are initially in unsigned int 8 format meaning the max value they can store is 255. So if you do some operation where the value reaches 256 or more it stores 255, so now your information is lost. so preserve this information you would want to use float as you might deal with fractions as well. Now, you can just normalize the data so as to scale it along [0 and 1] or ([-1 to 1] in this case.

",correct wrong initially unsigned format meaning value store operation value information lost preserve information would want use float might deal well normalize data scale along case,issue,negative,negative,negative,negative,negative,negative
471578181,is it due to over fitting? ? how many images do you have in your train dataset ? ,due fitting many train,issue,negative,positive,positive,positive,positive,positive
471558883,"Is there a way to add Spatial Pyramid Pooling layer to work with images of different size and so as to not resize them ? 
Thanks
Gaurav ",way add spatial pyramid layer work different size resize thanks,issue,negative,positive,neutral,neutral,positive,positive
471495813,"Thank yo for you reply.
I am a project student and I dont have a gpu enabled system,So  how can i
do this in cpu enabled system.


On Mon, 11 Mar 2019 at 08:48, taesungp <notifications@github.com> wrote:

> Do you have GPUs to run it? If you don't have CUDA GPU installed, you
> should specify option --gpu_ids -1 to disable GPU mode.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/557#issuecomment-471391117>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AsEqlfrsWe-pbgDq9rvtmelG9UkFDa16ks5vVcrygaJpZM4bWBkp>
> .
>


-- 
Thanks and Regards,

Aswini Das M
",thank yo reply project student dont system system mon mar wrote run specify option disable mode thread reply directly view mute thread thanks da,issue,positive,positive,positive,positive,positive,positive
471495656,"Thank yo for you reply.
I am a project student and I dont have a gpu enabled system,So  how can i do this in cpu enabled system.
",thank yo reply project student dont system system,issue,negative,neutral,neutral,neutral,neutral,neutral
471477045,"> We do not have exact information and it is not part of this code repo. if you are referring to [CyCADA](https://github.com/jhoffman/cycada_release), I personally was able to compute FCN score using a 12GB GPU.

![捕获](https://user-images.githubusercontent.com/48146545/54115747-14cef980-4428-11e9-8007-2599f9a98e00.PNG)

Thank you for your reply. This is a strange problem. My model is pix2pix, using the same fcn8s with you. There may be something wrong with caffe and I have found that the memory is not full according to nvidia-smi; moreover when i use one GPU, the others' memory is also used a bit. Any help will be thanks.",exact information part code personally able compute score thank reply strange problem model may something wrong found memory full according moreover use one memory also used bit help thanks,issue,negative,positive,positive,positive,positive,positive
471449286,"Thanks for the tip! Have a great week

On Mon, Mar 11, 2019 at 4:06 AM taesungp <notifications@github.com> wrote:

> It should be possible, but you will need to change the codes that save the
> images into .png file, along with using the options --input_nc 1
> --output_nc 1.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/565#issuecomment-471389599>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAQ36_VyjgI87T6Bym1LUXEV2wm_86Diks5vVchKgaJpZM4bh1Wh>
> .
>
",thanks tip great week mon mar wrote possible need change save file along thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
471392592,"it should have 5 convolutional layers (note that [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L555) the range starts from 1, not 0. ) , and the last two layers don't have `stride=2`. In our code, `NLayerDiscriminator` with `n_layers_D==3` actually means the number of downsampling layers is 3. Sorry it's a bit misleading. ",convolutional note range last two code actually number sorry bit misleading,issue,negative,negative,negative,negative,negative,negative
471391664,"No one knows until you try and see how it looks. It will depend on many factors such as size of the dataset, resolution, diversity within dataset, and the quality and end task you'd like to achieve. ",one try see depend many size resolution diversity within quality end task like achieve,issue,negative,positive,positive,positive,positive,positive
471391487,"It's because dropout and batch norm have different behavior between train and test time, and it is a tricky issue. ",dropout batch norm different behavior train test time tricky issue,issue,negative,neutral,neutral,neutral,neutral,neutral
471391117,"Do you have GPUs to run it? If you don't have CUDA GPU installed, you should specify option `--gpu_ids -1` to disable GPU mode. ",run specify option disable mode,issue,negative,neutral,neutral,neutral,neutral,neutral
471390919,"As long as you use batch size 1, it should be able to do that with minor code revision. PatchGAN discriminator can work with arbitrary image sizes. The generator, on the other hand, may not directly work because the output size may not be the same as input size. ",long use batch size able minor code revision discriminator work arbitrary image size generator hand may directly work output size may input size,issue,negative,positive,neutral,neutral,positive,positive
471390580,"It is possible, but you should retrain the model at that resolution. You can use the option `--preprocess none` to disable image resizing. By the way, 1920x1080 is very high resolution for CycleGAN so it won't fit on most GPUs. We recommend that you train with something like `--preprocess crop --crop_size 512`, so that the model is trained with only cropped portions of the entire image. At test time, you can probably do `--preprocess none` because the memory requirement is significantly lower at test time. ",possible retrain model resolution use option none disable image way high resolution wo fit recommend train something like crop model trained entire image test time probably none memory requirement significantly lower test time,issue,positive,positive,positive,positive,positive,positive
471390035,"We do not have exact information and it is not part of this code repo. if you are referring to [CyCADA](https://github.com/jhoffman/cycada_release), I personally was able to compute FCN score using a 12GB GPU. ",exact information part code personally able compute score,issue,negative,positive,positive,positive,positive,positive
471389599,"It should be possible, but you will need to change the codes that save the images into .png file, along with using the options `--input_nc 1 --output_nc 1`. ",possible need change save file along,issue,negative,neutral,neutral,neutral,neutral,neutral
471389295,"unet_256 is only supposed to work with image size of 256, and possibly multiples of 256. Unet consists of series of downsampling and upsampling procedures, and there is size mismatch between the input and output image. 

In detail, the size of Conv2d(kernel_size=4, stride=2, padding=2) -> ConvTransposed2d(kernel_size=4, stride=2, padding=2) is not always equal to the original input image. Therefore, with UNet, you cannot use image size 360. ",supposed work image size possibly series size mismatch input output image detail size always equal original input image therefore use image size,issue,negative,positive,positive,positive,positive,positive
471388516,Not sure what went wrong here. The error is probably inside torchvision.transforms.RandomCrop. You might wanna print the image size and the intended cropping size right before the error occurs. ,sure went wrong error probably inside might wan na print image size intended size right error,issue,negative,positive,neutral,neutral,positive,positive
469737312,"Remove `--eval` solved my issue, but I do not know the reason.",remove issue know reason,issue,negative,neutral,neutral,neutral,neutral,neutral
469670906,"@junyanz  Let say if we parallelize the network across more gpus, will that be helpful?",let say parallelize network across helpful,issue,negative,neutral,neutral,neutral,neutral,neutral
469476628,"The quality of CycleGAN translation depends on many factors. If you found that L2 loss worked better for your task, feel free to use L2. Our motivation of using L1 is for sharper images as you pointed out. ",quality translation many found loss worked better task feel free use motivation sharper pointed,issue,positive,positive,positive,positive,positive,positive
469476344,"Have you set `--dataroot` option correctly? If you are using `unaligned_dataset`, the directory needs to be named `trainA` and `trainB`, not `train`. ",set option correctly directory need train,issue,negative,neutral,neutral,neutral,neutral,neutral
469476116,I think it's because GPU0 also needs to do the work of combining the gradients. You will probably need to modify the code inside pytorch itself to change it. This code repo does not plan to address this issue. The authors did not use multi-GPU training. ,think also need work combining probably need modify code inside change code plan address issue use training,issue,negative,neutral,neutral,neutral,neutral,neutral
469475602,It is overwritten [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L110). You can change this part to avoid overwriting. We intentionally made it overwritten so that the opt.txt file is always representing the newest options. ,change part avoid intentionally made file always,issue,negative,neutral,neutral,neutral,neutral,neutral
469474991,"`python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA` was used. The parameters not specified in this command line follows the default setting, which will be printed to console if you run this command. ",python name model direction used command line default setting printed console run command,issue,negative,neutral,neutral,neutral,neutral,neutral
469474659,"Now sure what you mean. Multi-color image as separate files? Do you mean the original image is 6-channel, and you would like to treat it as 6 different single-channel images? 

You can modify data/unaligned_loader.py to do that. For splitting the image into multiple images, you can convert the PIL Image to Numpy and slice the array. ",sure mean image separate mean original image would like treat different modify splitting image multiple convert image slice array,issue,positive,positive,neutral,neutral,positive,positive
469474051,"Usually GPU 0 does consume more memory than the others because it needs to do the work of combining the gradients from the other GPUs. I haven't thought deeply about how to remedy this. Maybe you can modify DataParallel so that GPU0 is only responsible for combining the gradients, without actually doing backprop? It would be appreciated if you can explore and let us know. ",usually consume memory need work combining thought deeply remedy maybe modify responsible combining without actually would explore let u know,issue,negative,negative,neutral,neutral,negative,negative
469473323,"In our memory, it didn't make a big difference, but feel free to try!",memory make big difference feel free try,issue,positive,positive,positive,positive,positive,positive
469473083,"That will require tweaking the generator architecture a bit. 

The simplest way would be creating the generator to work at the higher resolution, and use `torch.nn.funtional.interpolate` to the generated output to downsample the image to match the lower resolution. ",require generator architecture bit way would generator work higher resolution use output image match lower resolution,issue,negative,positive,positive,positive,positive,positive
469385251,"Pix2pix and CycleGAN are both networks for image to image translation between domains. Pix2pix works in a pairwise fashion in that it needs corresponding images from two different domains to learn to translate from one to the other. CycleGAN on the other hand, does not need corresponding images from both domains to learn. It learns a two way mapping between the domains simultaneously.",image image translation work pairwise fashion need corresponding two different learn translate one hand need corresponding learn two way simultaneously,issue,negative,neutral,neutral,neutral,neutral,neutral
469173100,"This is not the case, because this network, the image requirements you enter can only be like this, if you want to change the output image, you use the function to adjust",case network image enter like want change output image use function adjust,issue,negative,neutral,neutral,neutral,neutral,neutral
468871965,"Apparently, the error is caused by images that aren't in RGB color space.",apparently error color space,issue,negative,positive,neutral,neutral,positive,positive
468551667,Thank you for posting your final code @dokasov !! I'm doing something related and this saved me a lot of time :),thank posting final code something related saved lot time,issue,positive,neutral,neutral,neutral,neutral,neutral
468508385,"@mabdullahrafique I encountered the same problem as your's. Can u tackled the problem by reducing 
 the weight of L1 loss ? And which value of weight u choosed?
 THX!",problem tackled problem reducing weight loss value weight,issue,negative,neutral,neutral,neutral,neutral,neutral
468385434,"I've tried the following lines in `backward_D_basic`:
```python
    if self.opt.gan_mode == 'wgangp':
            # wgan-gp
            gradient_penalty, gradients = networks.cal_gradient_penalty(netD, real, fake, self.device)
            gradient_penalty.backward(retain_graph=True)
            loss_D = (loss_D_fake + loss_D_real) * 0.5
            loss_D.backward()
```
but, the losses exploded.",tried following python real fake exploded,issue,negative,negative,negative,negative,negative,negative
468294770,"I think your dataset probably has too much geometrical ambiguity (view point), which can be hard for gan to force a reasonable explanation. Maybe try using a larger network?",think probably much geometrical ambiguity view point hard gan force reasonable explanation maybe try network,issue,negative,positive,neutral,neutral,positive,positive
467925293,"I am doing the cross-view image translation, the input, fake_B, and real_B as follows:

![image](https://user-images.githubusercontent.com/5948971/53504504-b3964600-3a77-11e9-90df-4a8d3ccd0ffd.png) ![image](https://user-images.githubusercontent.com/5948971/53504540-c4df5280-3a77-11e9-9322-062e14a4d2a2.png)

//
![image](https://user-images.githubusercontent.com/5948971/53504674-fce69580-3a77-11e9-8f1d-d1d9c5e3d29f.png)
![image](https://user-images.githubusercontent.com/5948971/53504705-096aee00-3a78-11e9-80a7-0e4158f7d700.png)


//
![image](https://user-images.githubusercontent.com/5948971/53504736-1687dd00-3a78-11e9-9509-b2b9e68645f2.png)
![image](https://user-images.githubusercontent.com/5948971/53504753-21db0880-3a78-11e9-98ee-5ce6ac59680d.png)


//
![image](https://user-images.githubusercontent.com/5948971/53504785-30292480-3a78-11e9-8dc9-e5e2b9dd0b34.png)
![image](https://user-images.githubusercontent.com/5948971/53504806-3d461380-3a78-11e9-8f57-5a8f91c6aa7e.png)


",image translation input image image image image image image image image,issue,negative,neutral,neutral,neutral,neutral,neutral
467922128,You could mask the input image.,could mask input image,issue,negative,neutral,neutral,neutral,neutral,neutral
467293775,"Why images are treated as float type continuous value? and why the input should be zero-centered?
Besides, can you show me which line code do the normalization?",float type continuous value input besides show line code normalization,issue,negative,neutral,neutral,neutral,neutral,neutral
467215245,"It's probably no different from working with images in range [0, 255], but the images are treated as float type continuous value, so the number 255 becomes meaningless. Plus, you want the input values to be more or less zero-centered. ",probably different working range float type continuous value number becomes meaningless plus want input le,issue,negative,negative,negative,negative,negative,negative
467174975,"Resnet_6blocks and Resnet_9blocks produce similar results, but resnet_9blocks has larger receptive field, which means it should learn relationship between objects further apart. I would use resnet_6blocks for idea iteration and resnet_9blocks for the best result if your GPU allows it. 

Similarly for unet_128 and unet_256. They were designed to work with images of sizes 128x128 and 256x256, respectively, although the fully convolutional architecture makes them available for many other difference input sizes. ",produce similar receptive field learn relationship apart would use idea iteration best result similarly designed work size respectively although fully convolutional architecture available many difference input size,issue,positive,positive,positive,positive,positive,positive
466868956,"I have solved it, my bad, misspelling in the file. Thanks for your help.
I will close this thread.",bad misspelling file thanks help close thread,issue,negative,negative,negative,negative,negative,negative
466868273,"Yes, the batch size is 8.
The output of `AB_path` is,

> /data/TH/dataset/test/Case10_1_00000_1_00126.png
> /data/TH/dataset/test/Case10_1_00000__1_01469.png
> /data/TH/dataset/test/Case10_1_00000_1_01683.png
> /home/Special/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
>   warnings.warn(""nn.functional.upsample is deprecated. Use nn.functional.interpolate instead."")
> processing (0000)-th image... []
> Traceback (most recent call last):
>   File ""test.py"", line 39, in <module>
>     save_images(webpage, visuals, img_path, aspect_ratio=opt.aspect_ratio, width=opt.display_winsize)
>   File ""/home/Special/pytorch-CycleGAN-and-pix2pix_sg2_3_rmadv_changeGs_4_atte_feature10_rml1_attentionboth_12_tv_disloss4_pooling_feature_ego2top_uncertainty4/util/visualizer.py"", line 13, in save_images
>     short_path = ntpath.basename(image_path[0])
> IndexError: list index out of range",yes batch size output use instead use instead image recent call last file line module file line list index range,issue,negative,neutral,neutral,neutral,neutral,neutral
466866381,"@Ha0Tang are you the same person who started this thread? 

That is strange. What batch size are you using? What's the output of `AB_path` of `__getitem__` of `AlignedDataset`? Could you check this and share with me?",person thread strange batch size output could check share,issue,negative,negative,neutral,neutral,negative,negative
466861946,"the loaded images don't seem to have paths. Are you using your own dataset class?

By default, the name of the result image is used from `A_paths` field of the dict returned in `__getitem__` of `BaseDataset` class. It looks like `A_paths` returned an empty string. As reference, take a look at `__getitem__` function of `data/unaligned_dataset.py` and how it returns `A_paths`. ",loaded seem class default name result image used field returned class like returned empty string reference take look function,issue,negative,negative,neutral,neutral,negative,negative
466710708,"Question 1

According to WGAN-GP paper and [this](https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py#L219) implementation, loss should be ```D_fake - D_real + gradient_penalty```
Shouldn't it be `-` instead of `+` in         
`self.loss_D = (self.loss_D_real - self.loss_D_fake + gradient_penalty) `

Question 2

In your implementation of `backward_D_basic` function, you used `loss_D.backward()`, but I found this:
![image](https://user-images.githubusercontent.com/28640563/53292878-a18b7d80-37c1-11e9-9fbe-fadba2c22c6f.png)

Any suggestion?
",question according paper implementation loss instead question implementation function used found image suggestion,issue,negative,neutral,neutral,neutral,neutral,neutral
466602456,"Yes...? The cropped image can be just thought as a smaller uncropped image. You are just training with smaller images. 

Let's say you don't use cropping. What if the input image is

``
I = [[1,2,3,4,5,6,7,8,9],[11,12,13,14,15,16,17,18,19],
     [1,2,3,4,5,6,7,8,9],[11,12,13,14,15,16,17,18,19],
     [1,2,3,4,5,6,7,8,9],[11,12,13,14,15,16,17,18,19],
     [1,2,3,4,5,6,7,8,9],[11,12,13,14,15,16,17,18,19],
     [1,2,3,4,5,6,7,8,9],[11,12,13,14,15,16,17,18, 19]] + 100
``

so that all values are within [101, 119]? As such, cropping does not introduce any extra problem. If images are within range [-0.5, 0.5], the generator will learn to output [-arctanh(-0.5), arctanh(0.5)]. ",yes image thought smaller uncropped image training smaller let say use input image within introduce extra problem within range generator learn output,issue,negative,neutral,neutral,neutral,neutral,neutral
466601672,"I gave an example for that

```
import numpy as np

I = [[1,2,3,4,5,6,7,8,9],[11,12,13,14,15,16,17,18,255],
     [1,2,3,4,5,6,7,8,9],[11,12,13,14,15,16,17,18,255],
     [1,2,3,4,5,6,7,8,9],[11,12,13,14,15,16,17,18,255],
     [1,2,3,4,5,6,7,8,9],[11,12,13,14,15,16,17,18,255],
     [1,2,3,4,5,6,7,8,9],[11,12,13,14,15,16,17,18,255]]
I = np.asarray(I)
I = I/255
I= (I-0.5)/0.5
print (I.min(), I.max()) #-0.9921568627450981 1.0
I_crop= I[4:6, 4:6]
print(I_crop.min(),I_crop.max()) #-0.9607843137254902 -0.8745098039215686
```",gave example import print print,issue,negative,neutral,neutral,neutral,neutral,neutral
466601184,"- Even with tanh, if the ground-truth cropped image is in the range of [-.5, .5], the generator network will learn to output [-.5, .5]. In other words, tanh does not make all outputs to have max value 1. For example, if the generator outputs zero everywhere, the image will be also zero everywhere, not [-1, -1]. 
- You actually have exactly same situation with uncropped images. Some images are bright, so they will be in [0, 1] range, not [-1, 1]. Some images are greyish, so they will be within [-0.5, 0.5]. You have the same amount of problem with or without cropping. 
- Tanh merely constrains the minimum and maximum output of the generator to be -1 and 1. The network can probably do just as well with `.clamp(-1, 1)` instead of `Tanh()`. ",even tanh image range generator network learn output tanh make value example generator zero everywhere image also zero everywhere actually exactly situation uncropped bright range within amount problem without tanh merely minimum maximum output generator network probably well instead tanh,issue,positive,positive,positive,positive,positive,positive
466600309,"It is correct. But the problem here is that if an image size of WxH is normalized to [-1,1]. Then crop a region in the image, the region may not in range of [-1,1], it may be [-0.5 0.5]. Then the output of tanh is [-1,1], so it makes the inconsistent range between cropped input and output of the network.",correct problem image size crop region image region may range may output tanh inconsistent range input output network,issue,negative,neutral,neutral,neutral,neutral,neutral
466596944,"Yes. But after normalization, we will crop the image. I know that we should normalize after the crop image but in my case, I want to normalize before crop image. ",yes normalization crop image know normalize crop image case want normalize crop image,issue,negative,neutral,neutral,neutral,neutral,neutral
466592361,"In the first line you should do

`I = I/255.0` instead of `I = I/max(I)` so that it become independent of the values of the current cropped I. ",first line instead become independent current,issue,negative,positive,neutral,neutral,positive,positive
466586226,"@taesungp : No, I misunderstood my question. Let's `I` is an image with size of HxW. So the normalization will be

```
I=I/max(I)
I=(I-0.5)/0.5
```
Now, the image intensity will be in [-1,1]. If I randomly crop the image into [H/8 and W/8]. Do you think the crop image range still in [-1,1]. No. It will be in a different range.",misunderstood question let image size normalization image intensity randomly crop image think crop image range still different range,issue,negative,negative,negative,negative,negative,negative
466584888,"[-1, 1] is the range of the value each pixel (brightness / color of each pixel should be within -1 and 1), so it has nothing to do with the width and height of the image. ",range value brightness color within nothing width height image,issue,positive,neutral,neutral,neutral,neutral,neutral
466584535,"We don't have much experience in 3D. 

But in case of 2D images, reducing generator depth almost did no harm to the image quality, so I recommend trying the third approach first. You can use resnet_6blocks instead of resnet_9blocks. 

Regarding batch size, it's still an open problem. In many cases, large batch size helps, such as in BigGAN, but in CycleGAN we could not see improvement with larger batch size. But this was not fully investigated, and for some problems large batch size might help. ",much experience case reducing generator depth almost harm image quality recommend trying third approach first use instead regarding batch size still open problem many large batch size could see improvement batch size fully large batch size might help,issue,negative,positive,positive,positive,positive,positive
466457334,"Clear! 

If I normalize the whole image [HxW] to [-1,1] and then random crop to size of [H/8xW/8] and feed to the network. Clear that the range of [H/8xW/8] will not in the range [-1,1]. Should not use the tanh in the last layer? Which way do you prefer to handle it? I cannot feed the whole [HxW] due to the memory issue",clear normalize whole image random crop size feed network clear range range use tanh last layer way prefer handle feed whole due memory issue,issue,positive,negative,neutral,neutral,negative,negative
466271552,Thanks very much. I resolve the question by unsqueeze(0) adding the channel. ,thanks much resolve question channel,issue,positive,positive,positive,positive,positive,positive
466269493,"What kind of data are you using? 

The input to the deep neural generator network (`netG`) should be a 4-dimensional tensor, arranged in the order of (batch size, num channels, height, width). It looks like your input to the layer is not a 4 dimensional tensor, although it should be a 4 dimensional tensor with 64 channels",kind data input deep neural generator network tensor order batch size height width like input layer dimensional tensor although dimensional tensor,issue,positive,positive,positive,positive,positive,positive
466268799,"> @tjusxh which part of this thread are you referring to?

Expected 4-dimensional input for 4-dimensional weight [64, 3, 4, 4], but got input of size [4, 2048, 2048], I make sure my input is not [64,3,4,4]",part thread input weight got input size make sure input,issue,negative,positive,positive,positive,positive,positive
466267593,I also encounter the same question. Anyone resolve the question? ,also encounter question anyone resolve question,issue,negative,neutral,neutral,neutral,neutral,neutral
465870769,I see. Thanks for the quick response.,see thanks quick response,issue,negative,positive,positive,positive,positive,positive
465867397,"Sorry but we don't have the discriminators anymore. The only way is to retrain from scratch. Note that the pretrained models do not exactly replicate the results in the paper anyway, because the paper result was produced using the Torch repo.  ",sorry way retrain scratch note exactly replicate paper anyway paper result produced torch,issue,negative,negative,negative,negative,negative,negative
465867132,We do not have a definite answer for that. Resnet tends to make less changes and work better when two domains are similar. ,definite answer make le work better two similar,issue,positive,positive,positive,positive,positive,positive
465849325,"Thanks. It worked now. Just for your suggestion, unet has more parameters and larger checkpoints MB. What is the benefit of using resnet comparison with unet? When should I use resnet?",thanks worked suggestion benefit comparison use,issue,positive,positive,positive,positive,positive,positive
464822066,We wrote our own project template from scratch. It should be general enough for other deep learning projects. We used it for other research projects as well. ,wrote project template scratch general enough deep learning used research well,issue,negative,positive,neutral,neutral,positive,positive
464754673,"Thanks for the quick response! 

I think I must be a bit confused about how to evaluate photo->labels then.  Where are the labelled (ground truth and predicted) tensors located? Do you advise that we use the scripts provided in eval_cityscapes?  

Sorry if Im asking something stupid, it's just that we were originally going to compare generated .pngs with the ground truth .pngs (like you state above) found in the cityscapes dataset which you provide code to download (again thank you for this) but we couldn't find a way to turn the .png files into label masks because there seems to be at least 240 different colors in the .png files. Surely there should be only 30 colours in the .pngs if there are only 30 possible classes in the cityscapes dataset? or maybe we're missing something...

Thanks again",thanks quick response think must bit confused evaluate ground truth advise use provided sorry something stupid originally going compare ground truth like state found provide code thank could find way turn label least different color surely possible class maybe missing something thanks,issue,positive,negative,neutral,neutral,negative,negative
464510229,You don't need an FCN-8s for evaluating photo->labels direction. You can just compare it against ground truth human annotation. FCN-8s is used for evaluating labels-> photo direction.,need direction compare ground truth human annotation used photo direction,issue,negative,neutral,neutral,neutral,neutral,neutral
464456400,"Since I'm running this on a google colab instance, I only have 90 minutes of computation time.
Since running the train script on all of the  ~600 images in trainA and ~600 images in trainB will take more than 90 minutes, I randomly choose 20 images from each set to train on, this results in ~90 minutes of computation time, but I was under the impression that this won't be a problem since I'm using the unaligned_dataset option, which is for unpaired images.

My `batch_size` is 1.

I'll try training on the horse2zebra dataset and see if that works.",since running instance computation time since running train script take randomly choose set train computation time impression wo problem since option unpaired try training see work,issue,negative,negative,negative,negative,negative,negative
464448176,"When I delete self.metric from  update_learning_rate(self) function in base_model.py,  he learning rate change as what we expected.
I know that adding self.metric is to solve other problems. But after I deleted it, it works on my code.",delete self function learning rate change know solve work code,issue,negative,neutral,neutral,neutral,neutral,neutral
464442882,"I think the epoch in get_scheduler function for linear policy could not  change during training process.
        def lambda_rule(epoch):
            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)
since the epoch would not change, lr_l would keep the same value with the starting value",think epoch function linear policy could change training process epoch epoch float since epoch would change would keep value starting value,issue,positive,neutral,neutral,neutral,neutral,neutral
464439228,"I have checked the get_scheduler function in network.py. I knew lr would linearly decrease to 1 after first 100 epochs. But the training process is different from what was expected.
Again, I have not made any changes to lr_policy (ie set lr_policy as linear).",checked function knew would linearly decrease first training process different made ie set linear,issue,negative,positive,positive,positive,positive,positive
464404500,It's not possible with ResNet9. You probably need to write your own network or resize the 256x256 result to 256x512.,possible probably need write network resize result,issue,negative,neutral,neutral,neutral,neutral,neutral
464404164,"We use a [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L113) (base=4) to modify the image height/width  into a multiple of 4. The output will be slightly different if the input size is odd. You can manually resize it to original image size, after getting results.",use function modify image multiple output slightly different input size odd manually resize original image size getting,issue,negative,positive,neutral,neutral,positive,positive
464403892,I see. I must have missed something. Thank you :),see must something thank,issue,negative,neutral,neutral,neutral,neutral,neutral
464403842,"I haven't seen this error before. According to this [post](https://github.com/roytseng-tw/Detectron.pytorch/issues/107), maybe your file is corrupted. You can try to download the models again. (delete the old ones first)",seen error according post maybe file corrupted try delete old first,issue,negative,positive,positive,positive,positive,positive
464403782,"What do you mean by randomly choosing 20 images per run? What is your `batch_size`? CycleGAN should not be expected to work for any X, Y combinations. I will see if you can first reproduce horse<->zebra results.",mean randomly choosing per run work see first reproduce horse zebra,issue,negative,negative,negative,negative,negative,negative
464403675,"CycleGAN uses `--no_dropout` by default. (test.py will a CycleGAN class). `test_model.py` does not use `--no_dropout` by default. Therefore, you need to add `--no_dropout`. ",default class use default therefore need add,issue,negative,neutral,neutral,neutral,neutral,neutral
464403555,"Thanks. But what changes does '--no_dropout' option make for test.py or test_model.py ?

From the code it seems dropout is always false for cycle gan but we do get the error from original post if not including that option. 

Thanks!",thanks option make code dropout always false cycle gan get error original post option thanks,issue,positive,positive,neutral,neutral,positive,positive
464403278,Image pool is not related to `__patch_instance_norm_state_dict` and not being used during test time. It can improve training stability but it is optional.,image pool related used test time improve training stability optional,issue,negative,neutral,neutral,neutral,neutral,neutral
464399216,"![horse2zebra_fake_1](https://user-images.githubusercontent.com/24612082/52906369-38bf6680-31ff-11e9-9829-f479a80e9383.png)
Epoch 60 of a retrained model from my implementation. 

Hi @junyanz, I fixed the dark output issue simply by normalizing the output, something I forgot to consider. My second question remain unsolved, if you can help. Thank you!
",epoch model implementation hi fixed dark output issue simply output something forgot consider second question remain unsolved help thank,issue,positive,negative,neutral,neutral,negative,negative
464258590,"Sorry, the accurate value of the learning rate was 0.0002000. But this is not a big deal for my question.
I am wondering why the learning rate didn't change during training process.
Thanks again!",sorry accurate value learning rate big deal question wondering learning rate change training process thanks,issue,positive,positive,neutral,neutral,positive,positive
464257680,"Ｍany thanks for your kind reply which has helped me solve my problems.

After  finishing the training process, I found another question:
the learning rate was always the same with the starting value (0.002), even if I have not made any changes to lr_policy (ie set lr_policy as linear). 
Could you tell me why,please？
Thanks for your kind help in advance！",thanks kind reply solve finishing training process found another question learning rate always starting value even made ie set linear could tell thanks kind help,issue,positive,positive,positive,positive,positive,positive
464229571,"Hi Javis, 

you can use this file: 

[http://efrosgans.eecs.berkeley.edu/cyclegan/pretrained_models/style_monet.pth](http://efrosgans.eecs.berkeley.edu/cyclegan/pretrained_models/style_monet.pth)

It's probably a bug on our side that monet2photo.pth file exists under pix2pix. Painting <-> photo codes should exist in cyclegan directory of the web space. ",hi use file probably bug side file painting photo exist directory web space,issue,negative,neutral,neutral,neutral,neutral,neutral
464219908,"you are right, because I did the same with Van Gogh and the japanese painter and it all works perfect with them, and it only fails with Monet and it is what you say, I was using normal photos yes

In the Monet one, there  is no option we can put to make it go the other way? thank you again for your great creation
",right van painter work perfect say normal yes one option put make go way thank great creation,issue,positive,positive,positive,positive,positive,positive
464215105,"Hello, 

I suspect it's because you used actual photographs as test images?

The pretrained model `monet2photo.pth` changes Monet's paintings into photo-style. It should not change anything if you use photos as input. (Please refer to the _identity_ loss_ of the paper and `--identity` option in this code repo. `monet2photo` was trained with this option). I think you used actual photographs when you put the test photos. You may want to input Monet's paintings if you'd like to use this pretrained model. ",hello suspect used actual test model change anything use input please refer paper identity option code trained option think used actual put test may want input like use model,issue,negative,neutral,neutral,neutral,neutral,neutral
464181846,Maybe setting `--display_ncols -1` can work. It will display images in different panels.,maybe setting work display different,issue,negative,neutral,neutral,neutral,neutral,neutral
464156669,You can just have `train` and `test` directories.  Sometimes people use `val` for the validation set. But it is optional.,train test sometimes people use validation set optional,issue,negative,neutral,neutral,neutral,neutral,neutral
464156364,"I will recommend that you use pix2pix or pix2pixHD for paired cases. In the paper, we used some pix2pix datasets, but we didn't use the paired information. ",recommend use paired paper used use paired information,issue,negative,neutral,neutral,neutral,neutral,neutral
464078829,"Yes. If they are applying to paired data, is cycleGAN better than pix2pix? I think cycleGAN also can apply for paired data, as the paper mentioned ",yes paired data better think also apply paired data paper,issue,positive,positive,positive,positive,positive,positive
464069861,CycleGANs are for unpaired data while pix2pix are used for paired data,unpaired data used paired data,issue,negative,neutral,neutral,neutral,neutral,neutral
464068966,"Thanks. So if we have pair data, which one is better: CycleGan or pix2pix?",thanks pair data one better,issue,positive,positive,positive,positive,positive,positive
463505926,"@junyanz I think I have answered my first question. I copied your `__patch_instance_norm_state_dict` function and your pre-trained model now works with my implementation. However, the results look different... All output images from `my test.py` + `your pre-traied model` looks darker than your results... interesting... 

One thing I have not implement yet is the image pool, but my understanding is that it is not involved during testing process. Any suggestions? Could it caused by the `__patch_instance_norm_state_dict`?

![n02381460_1010_fake_b](https://user-images.githubusercontent.com/24612082/52766727-f1f22680-2fdc-11e9-8b65-ec47fc653180.png)
![horse2zebra_fake_1](https://user-images.githubusercontent.com/24612082/52766747-03d3c980-2fdd-11e9-8cf9-4a789e7749f9.png)

I'm training a new horse2zebra model tonight with my `train.py` and will keep you posted for results.",think first question copied function model work implementation however look different output model interesting one thing implement yet image pool understanding involved testing process could training new model tonight keep posted,issue,negative,positive,positive,positive,positive,positive
463259463,"well, since both are just flags which evaluate to 1, we could simply hardcode the integer. It's a little non-pythonic but I can make that change instead ",well since evaluate could simply integer little make change instead,issue,negative,negative,neutral,neutral,negative,negative
463242391,"@junyanz could you please elaborate a bit more? Why does it need to ""detect"" cars when there's already labels on the cars in the image B? By the way, I meant to do label-to-image (BtoA) translation.",could please elaborate bit need detect already image way meant translation,issue,negative,positive,positive,positive,positive,positive
463197052,I am wondering if there is a compatible version for both cv2 2.4 and 3.0. Many users are still cv2 2.4.,wondering compatible version many still,issue,negative,positive,positive,positive,positive,positive
463196609,You need surrounding pixels (context) to detect cars. I recommend that you use the original image as input. ,need surrounding context detect recommend use original image input,issue,positive,positive,positive,positive,positive,positive
462835117,Thanks for your input. Are you suggesting training with the full set of labels and taking only the cars in test time? Is what I am doing (learning just from cars) going to produce anything different from learning from everything?,thanks input suggesting training full set taking test time learning going produce anything different learning everything,issue,negative,positive,positive,positive,positive,positive
462809069,"Yes, this works. I didn't know that the size has to be 2^k.",yes work know size,issue,negative,neutral,neutral,neutral,neutral,neutral
462744859,You can set `--input_nc 1` and `--output_nc ` and leave opt.ngf as default. You may want to try Resnet architecture as well.,set leave default may want try architecture well,issue,negative,neutral,neutral,neutral,neutral,neutral
462741452,Could you use the latest code and the default parameters?,could use latest code default,issue,negative,positive,positive,positive,positive,positive
462740985,"You can also just predict a car segmentation `car_segment`. During the test time, you can compute original_image * (1-car_segment) + yellow_color * car_segment.",also predict car segmentation test time compute,issue,negative,neutral,neutral,neutral,neutral,neutral
462186436,"Thanks for the information!

To see what was going on, I trained 4 distinctly different styles (say one green/white, red/white, blue/white, black/white) to 1,200 different images. I let it run for 200 epochs.

But when I run the test on the Generator model, only one of the four distinct styles will be generated. If the last style image used for training was green/white, then only the green/white style will be generated.

After training for over a hundred epochs, I test a generator model by doing the following:

`cp ./checkpoints/fourStyles/latest_net_G_B.pth  ./checkpoints/fourStyles/latest_net_G.pth`
`python3 test.py --dataroot ./datasets/ae_photos/outA --name fourStyles --model test  --load_size 512  --crop_size 512  --preprocess scale_width --no_dropout`

Is that the correct way to test a model that has weights for all of the styles it is trained for? Thanks again!
",thanks information see going trained distinctly different say one different let run run test generator model one four distinct last style image used training style training hundred test generator model following python name model test correct way test model trained thanks,issue,positive,positive,neutral,neutral,positive,positive
462173414,"To use CycleGAN, you have to train 4 networks simultaneously. Removing one or two networks will hurt the performance of the training. If you have paired input-output data, you can consider using pix2pix which only needs two networks.",use train simultaneously removing one two hurt performance training paired data consider need two,issue,negative,neutral,neutral,neutral,neutral,neutral
462172879,CycleGAN should learn to use the style of all the images. The gradient you can get from one training sample during one iteration is small. It is possible that CycleGAN only learns one style (monkey or ape) as it is a one-to-one mapping. A side note: the visualization images saved in the visdom or HTML may not be the last training image unless you save images every iteration.,learn use style gradient get one training sample one iteration small possible one style monkey ape side note visualization saved may last training image unless save every iteration,issue,positive,negative,neutral,neutral,negative,negative
462171983,"The goal is to match the range. The range of real images is [-1, 1]. Tanh outputs a value between [-1, 1].",goal match range range real tanh value,issue,negative,positive,positive,positive,positive,positive
462171717,Could you try a size `512x512` instead?,could try size instead,issue,negative,neutral,neutral,neutral,neutral,neutral
462041732,1500. Every image size is around 390x190,every image size around,issue,negative,neutral,neutral,neutral,neutral,neutral
462014071,It usually improves after a number of epochs. How many epochs was that trained for?,usually number many trained,issue,negative,positive,positive,positive,positive,positive
461859309,"> 
> 
> You can use a big fineSize which is larger than 256. CycleGAN uses 2-4 times more memory than pix2pix. I will expect a bigger batch size than 1.

I tried 501x501 (finesize and loadsize) for pix2pix. The following error occured in line 310 of networks.py:
'RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 2 and 3 in dimension 2 at c:\new-builder_3\win-wheel\pytorch\aten\src\thc\generic/THCTensorMath.cu:87'",use big time memory expect bigger batch size tried following error line invalid argument size must match except dimension got dimension,issue,negative,neutral,neutral,neutral,neutral,neutral
461612579,Cool. I also updated the code accordingly.,cool also code accordingly,issue,negative,positive,positive,positive,positive,positive
461576285,"I did verify that your suggested fix does indeed switch the images, it also switches the labels, which isn't so bad.",verify fix indeed switch also bad,issue,negative,negative,negative,negative,negative,negative
461548188,"Let me include an image to make it a little more clear.

![visdom](https://user-images.githubusercontent.com/24667483/52434657-c0e99180-2ac4-11e9-8df0-fb4f87396c26.jpg)

Modifying those 2 lines will switch right most images? I thought those lines would only impact the labels.

I believe the label ordering makes sense, i think it makes more sense to change the image ordering as shown in in the picture so that all images derived from the same real image are on the same row.
",let include image make little clear switch right thought would impact believe label sense think sense change image shown picture derived real image row,issue,negative,positive,neutral,neutral,positive,positive
461543575,"Yes. I guess people may have different preferences regarding the ordering in visualization. You can change the order by modifying this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L60). 
",yes guess people may different regarding visualization change order line,issue,negative,neutral,neutral,neutral,neutral,neutral
461251502,"Thank you. After a bit of digging I managed to make it work. Here's the final test.py file in case anyone finds it useful. It continously grabs screenshots, converts them and displays them:

```
import os
import sys
from options.test_options import TestOptions
from data import create_dataset
from data.base_dataset import get_transform
from models import create_model
from util.visualizer import save_images
from util.util import tensor2im

# For live displaying
import numpy as np
from PIL import ImageGrab
import cv2
import time

if __name__ == '__main__':

    opt = TestOptions().parse()  # get test options
    # hard-code some parameters for test
    opt.num_threads = 0   # test code only supports num_threads = 1
    opt.batch_size = 1    # test code only supports batch_size = 1
    opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.
    opt.no_flip = True    # no flip; comment this line if results on flipped images are needed.
    opt.display_id = -1   # no visdom display; the test code saves the results to a HTML file.
    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options
    model = create_model(opt)      # create a model given opt.model and other options
    model.setup(opt)               # regular setup: load and print networks; create schedulers

    # test with eval mode. This only affects layers like batchnorm and dropout.
    # For [pix2pix]: we use batchnorm and dropout in the original pix2pix. You can experiment it with and without eval() mode.
    # For [CycleGAN]: It should not affect CycleGAN as CycleGAN uses instancenorm without dropout.
    if opt.eval:
        model.eval()

    transform = get_transform(opt)

    last_time = time.time()
    while(True):
        # 800x600 windowed mode
        printscreen = ImageGrab.grab(bbox=None) # grab screenshot
        data = {'A': transform(printscreen).unsqueeze(0), 'A_paths': ['doesnt_really_matter']}  # put image into data format for the library

        model.set_input(data)  # unpack data from data loader
        model.test()           # run inference
        visuals = model.get_current_visuals()  # get image results

        im_data = list(visuals.items())[1][1] # grabbing the important part of the result
        im = tensor2im(im_data)  # convert tensor to image
        
        print('loop took {} seconds'.format(time.time()-last_time))
        cv2.imshow('window', im)  # displaying

        last_time = time.time()
        if cv2.waitKey(25) & 0xFF == ord('q'):
            cv2.destroyAllWindows()
            break
```",thank bit digging make work final file case anyone useful import o import import data import import import import import live import import import import time opt get test test test code test code true disable data shuffling comment line randomly chosen true flip comment line display test code file opt create given model opt create model given opt regular setup load print create test mode like dropout use dropout original experiment without mode affect without dropout transform opt true mode grab data transform put image data format library data unpack data data loader run inference get image list important part result convert tensor image print took break,issue,positive,positive,positive,positive,positive,positive
460906570,i think the data should be 1xCxHxW. You need to add one dimension.,think data need add one dimension,issue,negative,neutral,neutral,neutral,neutral,neutral
460887527,"Thank you for the response. I did what you said but had a problem. For the moment I'm just trying to load an image and convert it with the method you suggested. For that I'm loading it and applying the transformations to it with get_transform, and then wrapping it up like so: `data = {'A': transform(img), 'A_paths': ['random_path']}`, to pass in get_input and then test. This gives me the error `AssertionError: 3D tensors expect 2 values for padding` in the `model.test()` line. transform is successfully creating the tensor but it doesn't move past that. Here's the current test.py, that has as a purpose simply loading and converting one image:
```
import os
from options.test_options import TestOptions
from data import create_dataset
from models import create_model
from util.visualizer import save_images

# Added
from util.util import tensor2im
from data.base_dataset import get_transform 
import numpy as np
from PIL import ImageGrab, Image
# -----

if __name__ == '__main__':

    opt = TestOptions().parse()
    opt.num_threads = 0   
    opt.batch_size = 1    
    opt.serial_batches = True 
    opt.no_flip = True    
    opt.display_id = -1
    dataset = create_dataset(opt)
    model = create_model(opt)
    model.setup(opt)

    if opt.eval:
        model.eval()

    transform = get_transform(opt) 

    printscreen = Image.open('datasets/custom/spaceship.jpg').convert('RGB')
    data = {'A': transform(printscreen), 'A_paths': ['datasets/custom\\spaceship.jpg']}

    model.set_input(data)  # unpack data from data loader
    model.test()           # run inference
    visuals = model.get_current_visuals()  # get image results

    im_data = list(visuals.items())[1][1] # Here I'm just getting the result to save it
    im = tensor2im(im_data)  # img already converted to show

    im_to_save = Image.fromarray(im)
    im_to_save.save(""results/style_vangogh_pretrained/test.jpeg"")
```

I think that should work, where could I be wrong? Thank you",thank response said problem moment trying load image convert method loading wrapping like data transform pas test error expect padding line transform successfully tensor move past current purpose simply loading converting one image import o import data import import import added import import import import image opt true true opt model opt opt transform opt data transform data unpack data data loader run inference get image list getting result save already converted show think work could wrong thank,issue,positive,positive,neutral,neutral,positive,positive
460880943,Not sure about the training/test difference. It is implemented in a custom [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L69). Maybe Taesung (@taesungp ) knows more details.,sure difference custom function maybe,issue,negative,positive,positive,positive,positive,positive
460859750,"Since this is aligned, these are two images, 640x380. From what I see
during training, they split the image scale and crop.

In test, if the same happens, then it should be 256 x 152. Now, based on
your explanation, this should throw an exception during training as well.
But, the training completes without an issue. Does my reasoning make any
sense?

On a side note, I do understand scaling and randomly cropping seems weird
for inference. Maybe, if it was scaled and cropped starting from (0, 0),
that makes more sense.

On Tue, Feb 5, 2019 at 5:33 PM Jun-Yan Zhu <notifications@github.com> wrote:

> Here is the issue. If you resize an image with 1280 x360 to an output
> image with width 256, it will become a 256 x 72 image. If you apply a
> 256x256 cropping, it may cause the error. You may want to do `--load_size
> 1280 --crop_size 256'. You can also use other preprocess options.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/516#issuecomment-460830785>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABsR4bxxVYa_uLMQe3z3g4yu-sqS1HkPks5vKgaygaJpZM4aiYLS>
> .
>


-- 
Regards,

Tharindu

blog: http://mackiemathew.com/
",since two see training split image scale crop test based explanation throw exception training well training without issue reasoning make sense side note understand scaling randomly weird inference maybe scaled starting sense tue wrote issue resize image output image width become image apply may cause error may want also use thread reply directly view mute thread,issue,negative,negative,negative,negative,negative,negative
460830785,"Here is the issue. If you resize an image with 1280 x360 to an output image with width 256, it will become a 256 x 72 image. If you apply a 256x256 cropping, it may cause the error. You may want to do `--load_size 1280 --crop_size 256'. You can also use other preprocess options. ",issue resize image output image width become image apply may cause error may want also use,issue,negative,neutral,neutral,neutral,neutral,neutral
460774422,"[updated]
I did not debug in the code, so I'm not sure.  From the config options it shouldn't be a problem?

Load size = 256, Cropsize = 256. Both my train and test images are larger, and aligned as shown below.

train$ file 0001.png
0001.png: PNG image data, 1280 x 360, 8-bit/color RGB, non-interlaced

test$ file 0300.png
0300.png: PNG image data, 1280 x 360, 8-bit/color RGB, non-interlaced",code sure problem load size train test shown train file image data test file image data,issue,negative,positive,positive,positive,positive,positive
460498236,Is the height of your test image (after resizing with load_size) smaller than crop_size?,height test image smaller,issue,negative,neutral,neutral,neutral,neutral,neutral
460478007,"You can convert a numpy array to a tensor. You can use [ToPILImage](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToPILImage) to convert a numpy array to a PIL image, and use [get_transform](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L36) to convert a PIL image to a tensor image.",convert array tensor use convert array image use convert image tensor image,issue,negative,neutral,neutral,neutral,neutral,neutral
460022829,"@junyanz I waited 6 days using GPU. but the model epoch still in 6.
What should i do?

(epoch: 6, iters: 78100, time: 0.524, data: 0.002) D_A: 0.000 G_A: 1.000 cycle_A: 6.530 idt_A: 4.588 D_B: 0.001 G_B: 1.060 cycle_B: 9.175 idt_B: 0.596 
(epoch: 6, iters: 78200, time: 0.899, data: 0.002) D_A: 0.000 G_A: 1.002 cycle_A: 5.349 idt_A: 3.873 D_B: 0.001 G_B: 1.003 cycle_B: 7.746 idt_B: 0.355 
(epoch: 6, iters: 78300, time: 0.530, data: 0.002) D_A: 0.000 G_A: 1.000 cycle_A: 4.084 idt_A: 4.294 D_B: 0.003 G_B: 0.941 cycle_B: 8.588 idt_B: 1.107 
(epoch: 6, iters: 78400, time: 0.513, data: 0.002) D_A: 0.000 G_A: 1.000 cycle_A: 7.437 idt_A: 4.732 D_B: 0.001 G_B: 0.956 cycle_B: 9.463 idt_B: 1.087 
(epoch: 6, iters: 78500, time: 0.534, data: 0.002) D_A: 0.000 G_A: 0.998 cycle_A: 5.723 idt_A: 5.124 D_B: 0.000 G_B: 0.982 cycle_B: 10.249 idt_B: 0.757 
(epoch: 6, iters: 78600, time: 0.937, data: 0.002) D_A: 0.000 G_A: 1.005 cycle_A: 6.363 idt_A: 5.203 D_B: 0.001 G_B: 0.971 cycle_B: 10.405 idt_B: 0.391 
(epoch: 6, iters: 78700, time: 0.521, data: 0.002) D_A: 0.000 G_A: 1.004 cycle_A: 3.957 idt_A: 5.325 D_B: 0.001 G_B: 0.994 cycle_B: 10.650 idt_B: 0.473 
(epoch: 6, iters: 78800, time: 0.543, data: 0.002) D_A: 0.000 G_A: 1.002 cycle_A: 6.084 idt_A: 4.655 D_B: 0.001 G_B: 1.057 cycle_B: 9.310 idt_B: 0.373 
(epoch: 6, iters: 78900, time: 0.523, data: 0.002) D_A: 0.000 G_A: 1.001 cycle_A: 4.752 idt_A: 5.088 D_B: 0.000 G_B: 0.986 cycle_B: 10.175 idt_B: 0.343 
(epoch: 6, iters: 79000, time: 0.924, data: 0.002) D_A: 0.000 G_A: 0.997 cycle_A: 5.851 idt_A: 5.049 D_B: 0.000 G_B: 1.012 cycle_B: 10.098 idt_B: 0.361 
(epoch: 6, iters: 79100, time: 0.513, data: 0.002) D_A: 0.000 G_A: 1.006 cycle_A: 5.204 idt_A: 5.140 D_B: 0.001 G_B: 1.000 cycle_B: 10.279 idt_B: 0.976 

Training is not finished...",day model epoch still epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data training finished,issue,negative,neutral,neutral,neutral,neutral,neutral
459888127,It depends on your input image size. U-Net is a relatively big model.,input image size relatively big model,issue,negative,neutral,neutral,neutral,neutral,neutral
459870820,"Is the pix2pix model memory intensive while testing?

I have logged some info before calling test method and after calling test
This is what i am observing for the first image which is 6.5 gig

Memory for process 99648 before test is 531.2109375
Memory for process 99648 after test is 6935.40234375

And for the subsequent images its little less but first one is taking up a lot of memory.

Is this expected?",model memory intensive testing logged calling test method calling test observing first image gig memory process test memory process test subsequent little le first one taking lot memory,issue,negative,positive,neutral,neutral,positive,positive
459471695,I cannot reproduce your issue on my computer. Maybe you want to see the difference between your computers. ,reproduce issue computer maybe want see difference,issue,negative,neutral,neutral,neutral,neutral,neutral
459421174,I think the batchnorm is not compatible with gradient penalty loss. Instance norm might be fine as it is a normalization for each instance.,think compatible gradient penalty loss instance norm might fine normalization instance,issue,negative,positive,positive,positive,positive,positive
458928826,I am not sure if `default_collate` is related. Maybe you want to make sure that your tensor size is correct. It should be a 4D tensor (batchsize x channels x height x width).,sure related maybe want make sure tensor size correct tensor height width,issue,positive,positive,positive,positive,positive,positive
458925827,Haven't done that. This [post](https://github.com/pytorch/pytorch/issues/4584) might help.,done post might help,issue,negative,neutral,neutral,neutral,neutral,neutral
458898633,@jacky841102 You could try nearest neighbor. I think that it didn’t make much difference in the dataset I tried.  ,could try nearest neighbor think make much difference tried,issue,negative,positive,positive,positive,positive,positive
458796357,"> @AllAwake Cool results!
> 
> Here is the implementation of resize-conv I used. It remove the checkerboard artifacts during early training. You may find it useful.
> 
> ```
>                           nn.Upsample(scale_factor = 2, mode='bilinear'),
>                           nn.ReflectionPad2d(1),
>                           nn.Conv2d(ngf * mult, int(ngf * mult / 2),
>                                              kernel_size=3, stride=1, padding=0),
> ```
> It should replace the `ConvTranspose2d` in `ResnetGenerator`.

Hi @SsnL 
may I ask why you choose bilinear upsampling instead of nearest-neighbor one?  The [distill paper](https://distill.pub/2016/deconv-checkerboard/) pointed out the result of nearest-neighbor interpolation should be better.",cool implementation used remove checkerboard early training may find useful mult mult replace hi may ask choose bilinear instead one distill paper pointed result interpolation better,issue,positive,positive,positive,positive,positive,positive
458719748,"(I found a method that works for me. Assuming image pool is only used by the discriminator in its backward, simply making sure that cloned -- in my case -- and detached images are placed on the pool already reduces the memory footprint significantly. This is only noticeable with large image inputs. By the way, this better fake pool management also significantly improves the speed of training. I've done this with respect to an April branch. Perhaps, the main branch already includes this fix.)",found method work assuming image pool used discriminator backward simply making sure case detached pool already memory footprint significantly noticeable large image way better fake pool management also significantly speed training done respect branch perhaps main branch already fix,issue,positive,positive,positive,positive,positive,positive
458355787,"Another question when i copied the code from aligned dataset __getitem__(self, index) method to my method, and passed that result to model.set_input() and model.test() it failed with this error message

 ****Expected 4-dimensional input for 4-dimensional weight [64, 3, 4, 4], but got input of size [4, 2048, 2048]****

When i digged though the dataset and dataloader  code i found somewhere **default_collate()** function is being called on the result from dataset's _getitem.

So i am calling that function at the end once i get the dictionary with {'A','B','A_paths','B_paths'}.

Do you know if **default_collate** function is needed at the end to convert it , i am suspecting if this is causing memory issue.I dont know what it does but that resolved the ""channels error above"" so i added that.If that is not needed how could i resolved the above error.

If my comments does not make any sense please check the code that is attached in my first comment, you might understand what i am talking about.

",another question copied code self index method method result error message input weight got input size though code found somewhere function result calling function end get dictionary know function end convert causing memory dont know resolved error added could resolved error make sense please check code attached first comment might understand talking,issue,negative,positive,positive,positive,positive,positive
458234639,"@junyanz epoch is not increasing, other computers change epochs, but my computer does not change at 1. dataset contains domain A(871) and domain B(1,364).

It is hard to see the intermediate result because the epoch does not change.",epoch increasing change computer change domain domain hard see intermediate result epoch change,issue,negative,negative,negative,negative,negative,negative
458214321,It looks fine to me as your `iters` is increasing. How many images are there in your training set?,fine increasing many training set,issue,negative,positive,positive,positive,positive,positive
457959164,"It is not implemented in this repo.  You can separate the D and G optimization into two functions, and add a for loop in the training code. 

People find that using different learning rates for G and D might be more effective than using a different number of iterations.  See this [paper](https://arxiv.org/abs/1805.08318) for more details.",separate optimization two add loop training code people find different learning might effective different number see paper,issue,positive,positive,positive,positive,positive,positive
457958791,"There might be a memory leak in your code. You should be able to load one image, apply pix2pix, clear both CPU and GPU memory, and process the next image.",might memory leak code able load one image apply clear memory process next image,issue,negative,positive,positive,positive,positive,positive
457958634,"Currently, we use it separately for other ongoing [projects](https://github.com/junyanz/VON/blob/master/models/shape_gan_model.py#L76). To use it inside GAN_Loss, we have to change the interface of GAN_loss, which might make things more complicated. I will treat it as a regularization. People have used gradient penalty loss for other GAN loss.  (LSGAN-gp, dcgan-gp)",currently use separately ongoing use inside change interface might make complicated treat regularization people used gradient penalty loss gan loss,issue,negative,negative,negative,negative,negative,negative
457905962,Also this function should be put inside `GAN_loss` class.,also function put inside class,issue,negative,neutral,neutral,neutral,neutral,neutral
457794929,"> What's the batch_size? If the batch_size is 1, then it cannot be deployed on multi-gpus.
> It's better to set batch_size to n*num_gpus, e.g., 2 as you want to use 2 gpus.

you are my sunshine^^",better set want use,issue,negative,positive,positive,positive,positive,positive
457794249,"> Do you use your custom model or the authors'?
> If you're using your own model, do as following:
> 
> ```python
> model = YourModel(params)
> # use one of the two lines below
> model = torch.nn.DataParallel(model, gpu_ids) # make sure gpu_ids is set properly
> model = init_net(model, init_type, init_gain, gpu_ids)
> ```
authors  I am Xiao Bai……
",use custom model model following python model use one two model model make sure set properly model model,issue,negative,positive,positive,positive,positive,positive
457793267,"> Have you processed your model with `networks.init_net()`? Or you can directly use `net = torch.nn.DataParallel(net, gpu_ids)` to make your model ready for multi-gpus.

this is source code
def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[]):
    if len(gpu_ids) > 0:
        assert(torch.cuda.is_available())
        net.to(gpu_ids[0])
        net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs
    init_weights(net, init_type, init_gain=init_gain)
    return net

but it dose not work",model directly use net net make model ready source code net assert net net net return net dose work,issue,negative,positive,neutral,neutral,positive,positive
457792810,"Have you processed your model with `networks.init_net()`? Or you can directly use `net = torch.nn.DataParallel(net, gpu_ids)` to make your model ready for multi-gpus.",model directly use net net make model ready,issue,negative,positive,neutral,neutral,positive,positive
457791660,"> By reading the source code, I found this code to set up gpu:
> 
> ```python
> str_ids = opt.gpu_ids.split(',')
> opt.gpu_ids = []
> for str_id in str_ids:
>     id = int(str_id)
>     if id >= 0:
>         opt.gpu_ids.append(id)
> if len(opt.gpu_ids) > 0:
>     torch.cuda.set_device(opt.gpu_ids[0])
> ```
> 
> If I set the parameters --gpu_ids 0,1,2,then using this code,it might be running:
> `torch.cuda.set_device(0)`,So, is the code here wrong?
> I hope that a friend can answer my doubts, I will be grateful.

哥们，我也发现这个问题，我设置gpu ids 0,1，但是通过nvidia-smi查看，还是只用了0号GPU，怎么才能用多GPU训练呢？",reading source code found code set python id id id set code might running code wrong hope friend answer grateful,issue,positive,negative,negative,negative,negative,negative
457783888,"If you check the custom_dataset.txt attached in previous comment its just a plain object with a new function called get_data.This function has the code from unaligned_dataset's _get_item which reads 
image from bytes and has code to convert into tensors.And when i test it i am actually using something like this

`custom_dataset = CustomAlignedDataset(opt)`
 `img_map= custom_dataset.get_data(resized_b)`
  `model.set_input(img_map)`
  `model.test()`

this is because test function of pix2pix model expects a map with {'A','B','A_paths','B_paths'}. When you mean modify the test function i hope that is models test() method, what i need to modify i don't think it takes raw bytes it has to be converted in some form.

And the bigger question is, i somehow got it working for smaller data like if i have 5 to 6 pdfs which would have around 150 pages, so i am trying too debug the issues with more data,
if the pix2pix code is memory intensive or 
i am screwing something integrating this into spark etl code which is making memory intensive.",check attached previous comment plain object new function function code image code convert test actually something like opt test function model map mean modify test function hope test method need modify think raw converted form bigger question somehow got working smaller data like would around trying data code memory intensive screwing something spark code making memory intensive,issue,positive,negative,neutral,neutral,negative,negative
457718370,You don't have to use any dataset class. You can replace this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py#L56) with your own data pipeline. You can modify the `test` function and feed your data as input.,use class replace line data pipeline modify test function feed data input,issue,negative,neutral,neutral,neutral,neutral,neutral
457332193,"If it is the case, you can crop smaller patches from the original images. ",case crop smaller original,issue,negative,positive,positive,positive,positive,positive
457152242,"> Could you add `-preprocess none`? It should work if all of the images are at 720x480.

It seems my memory is not enough",could add none work memory enough,issue,negative,neutral,neutral,neutral,neutral,neutral
457036341,"Hi, I want it work on 480 * 36 images,  can anyone tell me how to change the code?",hi want work anyone tell change code,issue,negative,neutral,neutral,neutral,neutral,neutral
456875218,"I think that the generatored image (fake_A，fake_B) also need to be preprocessed, and then be added to the original dataset before sent to Discriminator. Because the original dataset is not incomplete (the lack of generated data from generator). I really want to know which line of code implements this above operation.

If I have any mistake, please point out directly. Thanks so much and your suggestion would be helpful for me. ",think image also need added original sent discriminator original incomplete lack data generator really want know line code operation mistake please point directly thanks much suggestion would helpful,issue,positive,positive,positive,positive,positive,positive
456803319,"I am not sure. You may want to monitor loss_gp as well. You can add 'gp' to self.loss_names.
I will record both loss_gp and loss_D.
```python
self.loss_gp, gradients = cal_gradient_penalty()...
self.loss_gp.backward(retain_graph=True)
self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5
self.loss_D.backward()
```",sure may want monitor well add record python,issue,positive,positive,positive,positive,positive,positive
456801542,Could you tell if the code at least looks correct? ,could tell code least correct,issue,negative,negative,negative,negative,negative,negative
456801302,You can use `--preprocess none` if you have enough GPU memory.,use none enough memory,issue,negative,neutral,neutral,neutral,neutral,neutral
456801081,Could you add `-preprocess none`? It should work if all of the images are at 720x480.,could add none work,issue,negative,neutral,neutral,neutral,neutral,neutral
456800625,We haven't tested the wgan-gp loss. The results in the paper were obtained with LSGAN loss. ,tested loss paper loss,issue,negative,neutral,neutral,neutral,neutral,neutral
456653753,"> I am having an issue with running CycleGAN on multiple GPUs. It works well when running on a single GPU (albeit very slowly, as expected) using
> 
> ```
>  python3 train.py --dataroot ./datasets/cezanne2photo --name cezanne2photo_cyclegan --model cycle_gan
> ```
> Now when I try to train on multiple GPUs using
> 
> ```
> python3 train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --gpu_ids 0,1,2,3 --batch_size 16 --norm instance
> ```
> I have also tried to run it with and without the `--norm instance` parameter and
> also tried with `--batch_size 4`. This always leads to the same result:
> 
> The program stops at ""create web directory"" (I've let it run for a couple of days at this point, without any noticeable progress). It looks like a single python3 process is putting a single thread under full load, none of the other python3 processes get any CPU time. Trying to kill that process also seems impossible - I have had to restart the machine every time. None of the GPUs are ever under load and barely any of their memory is used.
> 
> I am using Python 3.5.2, CUDA 9.2, pytorch 1.0, cuDNN 7.4.1. The system has four 1080ti GPUs and an AMD Ryzen Threadripper 1950X.

It also happen to me. How to solve this? Guys, i need your help!",issue running multiple work well running single albeit slowly python name model try train multiple python name model norm instance also tried run without norm instance parameter also tried always result program create web directory let run couple day point without noticeable progress like single python process single thread full load none python get time trying kill process also impossible restart machine every time none ever load barely memory used python system four ti also happen solve need help,issue,negative,negative,neutral,neutral,negative,negative
456632417,"> modified the so

Can you help me about  how to modified the code to retrained the model with resize or crop with 720×480 images? I 'am so helpless.",help code model resize crop helpless,issue,negative,neutral,neutral,neutral,neutral,neutral
456589828,"I changed the last line to:
`self.loss_D.backward(retain_graph=True)`
Now it starts, but the result is strange: I use it for mask-to-image translation and images are very blurry (like they are out of focus) in the background, but the objects look fine. These images are from training-time (from visdom server). Trained for 400 epochs so far.

The loss is: 
`G_GAN: -0.503 G_L1: 5.096 D_real: -0.501 D_fake:0.501`
Does it look correct? I was thinking that loss, in this case, should look like for usual training (decrease over time in correct case).
Thank you!",last line result strange use translation blurry like focus background look fine server trained far loss look correct thinking loss case look like usual training decrease time correct case thank,issue,negative,positive,neutral,neutral,positive,positive
456441843,You need to call `gradient_penalty.backward(retain_graph=True)`. See [here](https://github.com/junyanz/VON/blob/master/models/shape_gan_model.py#L80) for an example.,need call see example,issue,negative,neutral,neutral,neutral,neutral,neutral
456383505,"Hi, I try to make work with pix2pix, so I made changes in pix2pix_model.py
```
    def backward_D(self):
        """"""Calculate GAN loss for the discriminator""""""
        # Fake; stop backprop to the generator by detaching fake_B
        fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator
        pred_fake = self.netD(fake_AB.detach())
        self.loss_D_fake = self.criterionGAN(pred_fake, False)
        # Real
        real_AB = torch.cat((self.real_A, self.real_B), 1)
        pred_real = self.netD(real_AB)
        self.loss_D_real = self.criterionGAN(pred_real, True)
        #wgan-gp
        gradient_penalty, gradients = networks.cal_gradient_penalty(self.netD,real_AB,fake_AB,'cuda')
        # Combined loss and calculate gradients
        self.loss_D = (self.loss_D_real + self.loss_D_fake + gradient_penalty) * 0.5
        self.loss_D.backward()
```

And now I get error:

```
Traceback (most recent call last):
  File ""train.py"", line 51, in <module>
    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
  File ""/storage01/nikitam/pix2pix_wass/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py"", line 132, in optimize_parameters
    self.backward_G()                   # calculate graidents for G
  File ""/storage01/nikitam/pix2pix_wass/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py"", line 120, in backward_G
    self.loss_G.backward()
  File ""/storage01/nikitam/nikita3/lib/python3.6/site-packages/torch/tensor.py"", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""/storage01/nikitam/nikita3/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.
```",hi try make work made self calculate gan loss discriminator fake stop generator use conditional need feed input output discriminator false real true combined loss calculate get error recent call last file line module calculate loss get update network file line calculate file line file line backward self gradient file line backward flag trying backward graph second time already freed specify calling backward first time,issue,negative,negative,neutral,neutral,negative,negative
456323031," @ taozhuang123  hi  ，I got the trick， IN 'test_option.py' Line20  ,the model should set as:  model='pix2pix'.....hhha~~~~ ",hi got line model set,issue,negative,neutral,neutral,neutral,neutral,neutral
456226590,"You can crop patches during training and run the model on full images during test time. We have precomputed Cityscape->GTA results. See our [webpage](https://junyanz.github.io/CycleGAN/) ""Driving Applications"" for more details.  Also, see our recent work [CycADA](https://arxiv.org/abs/1711.03213). ",crop training run model full test time see driving also see recent work,issue,negative,positive,positive,positive,positive,positive
456202932,"Yes. You can write a custom data loader. In this data loader, you need to load three images from the disk separately,  concatenate them, and feed them as input to the generator. You need to also change `--input_nc`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) on how to implement your own data loader. Also, see the [overview](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/overview.md) of this repo. ",yes write custom data loader data loader need load three disk separately concatenate feed input generator need also change see implement data loader also see overview,issue,negative,neutral,neutral,neutral,neutral,neutral
456190963,The error is caused by cropping when crop_size is larger than the height or width of the image. Maybe you height (after rescaling) is smaller than crop_size. This Q & A might be [relevant](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#valueerror-empty-range-for-randrange-390-376-194). Training and test preprocess options do not have to be the same.,error height width image maybe height smaller might relevant training test,issue,negative,positive,positive,positive,positive,positive
455891202,"Thanks, If I understand correctly in the BicycleGAN project `self.nz` is noises which added to layers. Are you adding noises to the whole Generator layers (Encoder and Decoder)? If I want to add only to the output of the encoder should I edit the lines 743 till 761? ",thanks understand correctly project added whole generator want add output edit till,issue,negative,positive,positive,positive,positive,positive
455804744,"We had added noises to the generator in our project [BicycleGAN](https://github.com/junyanz/BicycleGAN/). See the network [definition](https://github.com/junyanz/BicycleGAN/blob/master/models/networks.py#L660) for more details.  As a side note, CycleGAN uses ResNet by default. Pix2pix uses Unet by default.",added generator project see network definition side note default default,issue,negative,neutral,neutral,neutral,neutral,neutral
455622317,"> You can change these two [lines](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L57). Also see the template dataset [class](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) on how to implement a custom class.

Thanks very much :)",change two also see template class implement custom class thanks much,issue,negative,positive,positive,positive,positive,positive
455412250,Thanks. It worked by passing val loss in self.metric. ,thanks worked passing loss,issue,negative,positive,positive,positive,positive,positive
455385177,You can change these two [lines](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L57).  Also see the template dataset [class](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) on how to implement a custom class. ,change two also see template class implement custom class,issue,negative,neutral,neutral,neutral,neutral,neutral
454707362,"Will you try it on pix2pix model on testing? It can run well with cyclegan model, but not pix2pix on my machine.
@junyanz @taesungp ",try model testing run well model machine,issue,negative,neutral,neutral,neutral,neutral,neutral
454399800,It looks fine. Maybe you want to use `self.device`rather than `'cuda'`. You probably don't need to multiply gp loss by 0.5.,fine maybe want use rather probably need multiply loss,issue,negative,positive,positive,positive,positive,positive
454309734,"How to call the  the function cal_gradient_penalty ?

def backward_D_basic(self, netD, real, fake):
        """"""Calculate GAN loss for the discriminator

        Parameters:
            netD (network)      -- the discriminator D
        """"""Calculate GAN loss for the discriminator

        Parameters:
            netD (network)      -- the discriminator D
            real (tensor array) -- real images
            fake (tensor array) -- images generated by a generator

        Return the discriminator loss.
        We also call loss_D.backward() to calculate the gradients.
        """"""
        # Real
        pred_real = netD(real)
        loss_D_real = self.criterionGAN(pred_real, True)
        # Fake
        pred_fake = netD(fake.detach())
        loss_D_fake = self.criterionGAN(pred_fake, False)
        #wgan-gp
        gradient_penalty, gradients = networks.cal_gradient_penalty(netD,real,fake,'cuda')
        # Combined loss and calculate gradients
        loss_D = (loss_D_real + loss_D_fake + gradient_penalty) * 0.5
        loss_D.backward()
        return loss_D
",call function self real fake calculate gan loss discriminator network discriminator calculate gan loss discriminator network discriminator real tensor array real fake tensor array generator return discriminator loss also call calculate real real true fake false real fake combined loss calculate return,issue,negative,negative,negative,negative,negative,negative
454268784,"Do you want to generate idt images at test time? The code doesn't have an option to do that (we removed such functionality for simplicity), but it should be fairly easy to add it. Basically, use `--model cycle_gan` and modify lines like [this](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L59) and `def forward` to generate G(B). ",want generate test time code option removed functionality simplicity fairly easy add basically use model modify like forward generate,issue,positive,positive,positive,positive,positive,positive
454250098,It is quite detailed! I think this would help a lot of people  get deep into your codebase. ,quite detailed think would help lot people get deep,issue,negative,positive,positive,positive,positive,positive
454205918,I mean that one can reduce the weight for L1 loss through `--lambda_L1`. This will encourage the model to generate small details. ,mean one reduce weight loss encourage model generate small,issue,negative,negative,negative,negative,negative,negative
454205743,"The names are recently changed to `load_size` and `crop_size`. 
1. `load_size`. The goal is to resize all the training images to the same size. 
2. `crop_size`. Random cropping is a data argumentation that can create more training data points per input image. ",recently goal resize training size random data argumentation create training data per input image,issue,negative,negative,negative,negative,negative,negative
453965376,"> Maybe your L1 loss is too big. You can try to use a smaller L1 loss.

Hi,

Could you elaborate on that pls?",maybe loss big try use smaller loss hi could elaborate,issue,negative,positive,positive,positive,positive,positive
453922906,"Hi, I'm confused with ""**We use --loadSize 1024 --fineSize 360 during training, and --loadSize 1024 --fineSize 1024 during test.**"" I see the arguments, _loadSize_ & _fineSize_, will be input to function 'load_train_data', and the image will be resize to _loadSize_ at first, and then it will be cropped according to _fineSize_. I cannot understand the meaning that training with loadSize=1024 & fineSize=360 because it only use the 360x360 parts inside the 1024x1024 image during training. Is it the case for some special data? ",hi confused use training test see input function image resize first according understand meaning training use inside image training case special data,issue,negative,positive,neutral,neutral,positive,positive
453861095,"In wgan-gp, there are two loss functions: GAN loss (you can calculate it with GANLoss class with `--gan_mode wgangp`), and gradient penalty loss (you can use the function cal_gradient_panalty to calculate it). Please see the [wgan-gp](https://arxiv.org/pdf/1704.00028.pdf) paper for more details.",two loss gan loss calculate class gradient penalty loss use function calculate please see paper,issue,negative,neutral,neutral,neutral,neutral,neutral
453798161,"Maybe you want to give us more details. I just tried the following command. 
```bash
python train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --preprocess none
```
It works on my machine. ",maybe want give u tried following command bash python name model none work machine,issue,negative,neutral,neutral,neutral,neutral,neutral
453797325,It is possible if your GPU has enough memory. You can use preprocessing flag `--preprocess none`.,possible enough memory use flag none,issue,negative,neutral,neutral,neutral,neutral,neutral
453726641,"I tested the '--preprocess none' option. it encounters  'CuDNN error: CUDNN_STATUS_BAD_PARAM' error. However, it can run well without the '--preprocess none', so i think its not caused by my cudnn enviroment.

The error:
```
---------- Networks initialized -------------
[Network G] Total number of parameters : 54.414 M
-----------------------------------------------
The image size needs to be a multiple of 4. The loaded image size was (237, 631), so it was adjusted to (236, 632). This adjustment will be done to all images whose sizes are not multiples of 4
Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fa1660d7e48>>
Traceback (most recent call last):
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 399, in __del__
    self._shutdown_workers()
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 378, in _shutdown_workers
    self.worker_result_queue.get()
  File ""/home/universe/miniconda3/lib/python3.6/multiprocessing/queues.py"", line 337, in get
    return _ForkingPickler.loads(res)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py"", line 151, in rebuild_storage_fd
    fd = df.detach()
  File ""/home/universe/miniconda3/lib/python3.6/multiprocessing/resource_sharer.py"", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File ""/home/universe/miniconda3/lib/python3.6/multiprocessing/resource_sharer.py"", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File ""/home/universe/miniconda3/lib/python3.6/multiprocessing/connection.py"", line 494, in Client
    deliver_challenge(c, authkey)
  File ""/home/universe/miniconda3/lib/python3.6/multiprocessing/connection.py"", line 722, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File ""/home/universe/miniconda3/lib/python3.6/multiprocessing/connection.py"", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File ""/home/universe/miniconda3/lib/python3.6/multiprocessing/connection.py"", line 407, in _recv_bytes
    buf = self._recv(4)
  File ""/home/universe/miniconda3/lib/python3.6/multiprocessing/connection.py"", line 383, in _recv
    raise EOFError
EOFError:
Traceback (most recent call last):
  File ""test.py"", line 61, in <module>
    model.test()           # run inference
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 105, in test
    self.forward()
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py"", line 88, in forward
    self.fake_B = self.netG(self.real_A)  # G(A)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 121, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 459, in forward
    return self.model(input)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 527, in forward
    return self.model(x)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 529, in forward
    return torch.cat([x, self.model(x)], 1)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 529, in forward
    return torch.cat([x, self.model(x)], 1)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 529, in forward
    return torch.cat([x, self.model(x)], 1)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 529, in forward
    return torch.cat([x, self.model(x)], 1)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 529, in forward
    return torch.cat([x, self.model(x)], 1)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 529, in forward
    return torch.cat([x, self.model(x)], 1)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/jupyter/gxl/project/digitalprint/deep/test/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 529, in forward
    return torch.cat([x, self.model(x)], 1)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/universe/miniconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: CuDNN error: CUDNN_STATUS_BAD_PARAM
```

@taesungp ",tested none option error error however run well without none think error network total number image size need multiple loaded image size adjustment done whose size exception bound method object recent call last file line file line file line get return file line file line detach conn file line client address file line client file line response reject large message file line file line file line raise recent call last file line module run inference file line test file line forward file line result input file line forward return file line result input file line forward return input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward error,issue,positive,positive,neutral,neutral,positive,positive
453630181,I pushed the code to support `--preprocess none` option. Could you check and let us know? Thanks!,code support none option could check let u know thanks,issue,positive,positive,positive,positive,positive,positive
453433735,Can I train and test at the same time? I have got the same error as omg777.  I am a little confused how to rename manually.,train test time got error little confused rename manually,issue,negative,negative,negative,negative,negative,negative
453383980,I just added `self.metric` to the code. It was initialized as 0  [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/base_model.py#L44). and used in [step](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/base_model.py#L119)(). You need to assign your loss value to self.metric. ,added code used step need assign loss value,issue,negative,neutral,neutral,neutral,neutral,neutral
453382047,You need to specify `--gan_mode wgangp` and also call the function [cal_gradient_penalty](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L271) by yourself. ,need specify also call function,issue,negative,neutral,neutral,neutral,neutral,neutral
453017406,"I then set the preprocess parameter to none, but still get resize and cropped results. here is my command:
```
python3 test.py --dataroot /path/pix2pix_data --name my_pix2pix --model pix2pix --direction AtoB --preprocess none
```

I add print code to test.py to print opt, here is the the print result on terminal:
```
Namespace(aspect_ratio=1.0, batch_size=1, checkpoints_dir='./checkpoints', crop_size=256, dataroot='/path/pix2pix_data', dataset_mode='aligned', direction='AtoB', display_id=-1, display_winsize=256, epoch='latest', eval=False, gpu_ids=[0], init_gain=0.02, init_type='normal', input_nc=3, isTrain=False, load_iter=0, load_size=256, max_dataset_size=inf, model='pix2pix', n_layers_D=3, name='my_pix2pix', ndf=64, netD='basic', netG='unet_256', ngf=64, no_dropout=False, no_flip=True, norm='batch', ntest=inf, num_test=50, num_threads=1, output_nc=3, phase='test', preprocess='none', results_dir='./results/', serial_batches=True, suffix='', verbose=False)
```

@junyanz ",set parameter none still get resize command python name model direction none add print code print opt print result terminal,issue,negative,neutral,neutral,neutral,neutral,neutral
452960849,"Sorry for the late response. I pull the code, and run command like this:
```
python3 test.py --dataroot /path/pix2pix_data --name my_pix2pix --model pix2pix --direction AtoB --resize_or_crop none
```

The error says:
```
usage: test.py [-h] --dataroot DATAROOT [--name NAME] [--gpu_ids GPU_IDS]
               [--checkpoints_dir CHECKPOINTS_DIR] [--model MODEL]
               [--input_nc INPUT_NC] [--output_nc OUTPUT_NC] [--ngf NGF]
               [--ndf NDF] [--netD NETD] [--netG NETG]
               [--n_layers_D N_LAYERS_D] [--norm NORM] [--init_type INIT_TYPE]
               [--init_gain INIT_GAIN] [--no_dropout]
               [--dataset_mode DATASET_MODE] [--direction DIRECTION]
               [--serial_batches] [--num_threads NUM_THREADS]
               [--batch_size BATCH_SIZE] [--load_size LOAD_SIZE]
               [--crop_size CROP_SIZE] [--max_dataset_size MAX_DATASET_SIZE]
               [--preprocess PREPROCESS] [--no_flip]
               [--display_winsize DISPLAY_WINSIZE] [--epoch EPOCH]
               [--load_iter LOAD_ITER] [--verbose] [--suffix SUFFIX]
               [--ntest NTEST] [--results_dir RESULTS_DIR]
               [--aspect_ratio ASPECT_RATIO] [--phase PHASE] [--eval]
               [--num_test NUM_TEST]
test.py: error: unrecognized arguments: --resize_or_crop none
```

It seems that the resize_or_crop parameter is removed?


@junyanz ",sorry late response pull code run command like python name model direction none error usage name name model model norm norm direction direction epoch epoch verbose suffix suffix phase phase error unrecognized none parameter removed,issue,positive,negative,negative,negative,negative,negative
452736930,"hello! 
I also have same error with them. I run the command python -m visdom.server and it still You can navigate to http://localhost.localdomain:8097 like that. How long does it take to wait? What can I do after waiting to finish?",hello also error run command python still navigate like long take wait waiting finish,issue,negative,negative,neutral,neutral,negative,negative
452400306,"It works for most of the applications reported in the paper. The parameters depend on your task and the number of training images. Let's say you have 1 million images, you probably only need a few epochs. I recommend that you save checkpoints frequently and pick up the best one after training. ",work paper depend task number training let say million probably need recommend save frequently pick best one training,issue,positive,positive,positive,positive,positive,positive
452315796,"So why did you fix the default opt.niter=100 and opt.niter_decay=100 ? 

I mean, did you find out that this setting is suitable for the majority of tasks or you just set these numbers randomly ? In your opinion, should I pay attention to change this setting when I train my own dataset ?",fix default mean find setting suitable majority set randomly opinion pay attention change setting train,issue,negative,negative,neutral,neutral,negative,negative
452172024,"1. Colorization might be different from artist style transfer. From my experience, 25 images might not be enough. 
2. I don't have a good answer. To prevent overfitting, you may want to apply some random cropping and flipping. (e.g,. for 512x512 images, use `--load_size 600 --crop_size 512`. )
3. For GANs training, Section 3.4 in [improved-GANs](https://arxiv.org/pdf/1606.03498.pdf) does tell some logic behind the trick. I would also recommend that you read Ian's [tutorial](https://arxiv.org/pdf/1701.00160.pdf) on GANs. ",colorization might different artist style transfer experience might enough good answer prevent may want apply random use training section tell logic behind trick would also recommend read tutorial,issue,positive,negative,neutral,neutral,negative,negative
452151028,"Also, I am learning to train GANS  (training deep networks for that matter) via DIY. I would like to know how to go about training GANs. Are these tweaking parameters, loss functions and then checking the output completely based on trial-and-error or does there exist any logic behind trying every change? Please help me with some tips on how to go about training GANs (or any deep networks).",also learning train training deep matter via would like know go training loss output completely based exist logic behind trying every change please help go training deep,issue,positive,negative,neutral,neutral,negative,negative
452149852,"1. Thanks for your patient reply. But then, there was a case where the model was trained to watercolor the black and white image (a church kind of an input image..an issue was raised here about that. [Issue)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/416) which used just 2 training images but after training, it was found to work. Can I know why then its not working in my case?

2. Also, for me, 256x256 input images are not overfitting whereas for 512x512 images are found to overfit (they are found to be colored exactly like the training image)..can I conclude that this is because there are too many small features in the image which the 3x3 filters used in the generator cannot recognize when 256x256 image is used while they can recognize the features when 512x512 images are used?",thanks patient reply case model trained black white image church kind input image issue raised issue used training training found work know working case also input whereas found overfit found colored exactly like training image conclude many small image used generator recognize image used recognize used,issue,positive,positive,positive,positive,positive,positive
452139514,"1. For your own task, you probably need more images. 25 might not be enough.  You probably need  thousands or even tens of thousands of training images.  You may also want to check out [colorization-PyTorch](https://github.com/richzhang/colorization-pytorch) repo. You can fine-tine a pre-trained colorization model on your training images. 
2. 0.7-1.2 might help stabilize GANs training. You are free to modify our code. You can also try other GAN loss (e.g. LSGAN) within this repo. Please see Section 3.4 in [improved-GAN](https://arxiv.org/pdf/1606.03498.pdf) paper for more details. 
3. For edges2hangbags, we used batchSize =4 in the paper with [Torch](https://github.com/phillipi/pix2pix) code. Also, see Figure 21 in the [paper](https://arxiv.org/pdf/1611.07004.pdf) for typical failure cases of pix2pix. 
",task probably need might enough probably need even training may also want check colorization model training might help stabilize training free modify code also try gan loss within please see section paper used paper torch code also see figure paper typical failure,issue,negative,negative,neutral,neutral,negative,negative
452137178,"Also, inorder to understand the concept indepth, I tried to train the pix2pix model on edges2handbags dataset - 138k images (downloaded using your code)..after training for 15 epochs with a batch size of 32, the generated image is like this:
![116_ab_fake_b](https://user-images.githubusercontent.com/29298860/50802980-0da64500-132c-11e9-8f49-b7734fd3059a.png)
Can you tell me whats going wrong? How did the paper get so good images? I am just running the same code.
Thanks in advance.",also understand concept tried train model code training batch size image like tell whats going wrong paper get good running code thanks advance,issue,positive,positive,positive,positive,positive,positive
452136933,"`--epoch` is not. We have `--niter` and `--niter_decay`. If the learning rate policy `linear` is used, the number of epochs = `--niter` + `--niter_decay`.  See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L31) for more details. Also check out the training option [class](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py). ",epoch niter learning rate policy linear used number niter see also check training option class,issue,negative,neutral,neutral,neutral,neutral,neutral
451838670,"1. Please help me with this: I have trained the model on 25 odd images for converting binary manga images to colored images (I know the dataset size is very small but atleast I expect the model to overfit for 25 odd images first with the unet_256 architecture which is not happening). The loss curves are like this:
![loss_curves](https://user-images.githubusercontent.com/29298860/50753426-34656c80-1295-11e9-986d-73c77366e4c4.png)

When the binary input image **used for training** is sent to the generator (after training for 500 epochs), the result  is like this:
*Input Binary Image*
![Input Binary Image](https://user-images.githubusercontent.com/29298860/50753216-6924f400-1294-11e9-9e14-76e111aacb07.png)
*Real colored image*
![Real colored image](https://user-images.githubusercontent.com/29298860/50753241-7e9a1e00-1294-11e9-831f-5bef5576751d.png)
*Generated color image*
![Generated image](https://user-images.githubusercontent.com/29298860/50753251-878aef80-1294-11e9-92e9-671e5619ab92.png)

And for **test set**, the output is like this:
*Input binary image*
![0002-011 png_real_a](https://user-images.githubusercontent.com/29298860/50753329-e18bb500-1294-11e9-9c89-5ba4671c25c2.png)
*Generated color image*
![0002-011 png_fake_b](https://user-images.githubusercontent.com/29298860/50753341-e8b2c300-1294-11e9-9936-5a83b95ea494.png)

As it can be seen, the generated image for both train and test set are worse. The loss seems to be normal but the generator still didnt learn anything useful (and it didnt overfit the training data even after 500 epochs as can be seen from the generated image for the training input image). Can you suggest anything for this purpose? I feel a single input image has too many features for the generator to learn while training. So, is it better to use a much complex architecture like inception module (GoogLeNet architecture)? Please suggest some other alternatives to try if any. 

I feel this task is an alternative to edges2shoes conversion except the fact that the input image has too many features to learn at a time. Please tell me whether I am right and also suggest something to try.

2. Also, can you tell me how to find whether the vanishing gradient is the reason for network performing poorly?
3. ganhacks says `D loss goes to 0: failure mode` Is it failure mode for my loss curve for the discriminator?
4. Also, ganhacks says real label has to be given a number between 0.7 and 1.2 randomly but in the implementation, the real label is fixed as 1..right? Can you tell me whats the intuitive impact of giving a random number between 0.7 and 1.2?",please help trained model odd converting binary manga colored know size small expect model overfit odd first architecture happening loss like binary input image used training sent generator training result like input binary image input binary image real colored image real colored image color image image test set output like input binary image color image seen image train test set worse loss normal generator still didnt learn anything useful didnt overfit training data even seen image training input image suggest anything purpose feel single input image many generator learn training better use much complex architecture like inception module architecture please suggest try feel task alternative conversion except fact input image many learn time please tell whether right also suggest something try also tell find whether vanishing gradient reason network poorly loss go failure mode failure mode loss curve discriminator also real label given number randomly implementation real label fixed right tell whats intuitive impact giving random number,issue,negative,positive,neutral,neutral,positive,positive
451776270,"Yes, some flags are changed to match the Python style. ",yes match python style,issue,negative,neutral,neutral,neutral,neutral,neutral
451679380,There are two parameters; <niter> and <niter_decay>. we keep the same learning rate for the first <opt.niter> epochs and linearly decay the rate to zero over the next <opt.niter_decay> epochs.,two niter keep learning rate first linearly decay rate zero next,issue,negative,positive,positive,positive,positive,positive
451537310,I added a Wasserstein loss and gradient penalty loss in our codebase. Feel free to try it. ,added loss gradient penalty loss feel free try,issue,negative,positive,positive,positive,positive,positive
451537055,1/2 or 1/3 of image full size is quite a big context for many tasks. But It depends on how much context you need in your application. You are free to try different configurations.,image full size quite big context many much context need application free try different,issue,positive,positive,positive,positive,positive,positive
451536120,Thanks for pointing out the issues. It should be fixed with the latest commit.,thanks pointing fixed latest commit,issue,positive,positive,positive,positive,positive,positive
451275050,"Case 1: Using separate optimizers for D_A and D_B
Case 2: Using the same optimizer for D_A and D_B

> Using one for two Ds is equivalent to the current scheme.
@SsnL By the above statement, do you mean Case 1 and Case 2 will give the same results?

Thank you
",case separate case one two equivalent current scheme statement mean case case give thank,issue,negative,negative,negative,negative,negative,negative
451271691,Thanks for your contributions. This looks great. It might be better hosted as a separate project/repo/paper as it was not discussed in the original papers. ,thanks great might better separate original,issue,positive,positive,positive,positive,positive,positive
451238766,Good call. I fixed it with the latest commit. Could you try it again?,good call fixed latest commit could try,issue,positive,positive,positive,positive,positive,positive
451143851,Cool. You can use a smaller network `--netG resnet_6blocks` with lower image size `--loadSize 143` and `--fineSize 128`.,cool use smaller network lower image size,issue,negative,positive,positive,positive,positive,positive
451024456,Please open an issue on [pytorch issue tracker](https://github.com/pytorch/pytorch/issues). Make sure to follow the bug report template.,please open issue issue tracker make sure follow bug report template,issue,positive,positive,positive,positive,positive,positive
451017963,"Thanks for your prompt reply.

Actually it works for me when I train it on an VM instance with GPU. I had a CPU memory less than 8GB and I didn't test the code in PyTorch library. If anyone has a similar problem, he/she could try to increase CPU memory or use a GPU instance. Anyway, it is strange that a problem of type ""segmentation fault"" could occur in this context.",thanks prompt reply actually work train instance memory le test code library anyone similar problem could try increase memory use instance anyway strange problem type segmentation fault could occur context,issue,negative,positive,neutral,neutral,positive,positive
451012774,Also added an [overview](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/overview.md) of our codebase. Feel free to edit or add stuff.,also added overview feel free edit add stuff,issue,positive,positive,positive,positive,positive,positive
450991824,"I encountered the same problem recently, while testing on 1 CPU. However I still haven't tried increasing my CPU memory on Google cloud platform. Hope this solves the problem. ",problem recently testing however still tried increasing memory cloud platform hope problem,issue,negative,neutral,neutral,neutral,neutral,neutral
450988813,Just saw that. Added a small PR that might clarify the steps for others. Thanks!,saw added small might clarify thanks,issue,negative,negative,neutral,neutral,negative,negative
450955590,"The code works for my CPU, but it is very slow (15 seconds per iteration). Do you have enough CPU memory (~8GB). You can also add `--display_id -1 --print_freq 1` to turn off the visdom display and print training loss every single iteration. ",code work slow per iteration enough memory also add turn display print training loss every single iteration,issue,negative,negative,negative,negative,negative,negative
450954056,Could you run other PyTorch code with your CPU? Like the code in PyTorch library?,could run code like code library,issue,negative,neutral,neutral,neutral,neutral,neutral
450946449,"<img width=""1115"" alt=""1"" src=""https://user-images.githubusercontent.com/26926814/50606404-ee3f9000-0ec5-11e9-8dd6-a255188295cf.png"">
<img width=""1115"" alt=""2"" src=""https://user-images.githubusercontent.com/26926814/50606411-f4357100-0ec5-11e9-991b-0c4d9326408c.png"">
<img width=""1120"" alt=""3"" src=""https://user-images.githubusercontent.com/26926814/50606414-f992bb80-0ec5-11e9-819b-ae9cdd4df667.png"">
<img width=""1113"" alt=""4"" src=""https://user-images.githubusercontent.com/26926814/50606423-00b9c980-0ec6-11e9-973f-0bbe562f5ef5.png"">

This is what I got when I tap:
gdb python
r train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --gpu_ids -1
where
",got tap python name model,issue,negative,neutral,neutral,neutral,neutral,neutral
450896436,"Your command with multi-GPU training works for me. I am using Python 3.6.4, PyTorch 0.4.1, CUDA 9.0, cudnn 7.0.5. @taesungp ",command training work python,issue,negative,neutral,neutral,neutral,neutral,neutral
450692904,"Thanks again for your valuable tips. Taesung (@taesungp) and I spent some time writing documentation and guide during winter break. Taesung provided a docker [file](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/docker.md).  I just wrote two templates: dataset [template](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/template_dataset.py) and model [template](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/template_model.py). We are writing an overview of our code structure, and adding more documentation to some key functions. Let me know if you have any questions or comments.  ",thanks valuable spent time writing documentation guide winter break provided docker file wrote two template model template writing overview code structure documentation key let know,issue,positive,positive,neutral,neutral,positive,positive
450420588,It might be OK. Could you share us with your plot?,might could share u plot,issue,negative,neutral,neutral,neutral,neutral,neutral
450291585,"Loss function can be used to identify failure mode as suggested by [ganhacks](https://github.com/soumith/ganhacks#10-track-failures-early). For example, If D loss is always 0, maybe D is too strong. You can increase the capacity of G.",loss function used identify failure mode example loss always maybe strong increase capacity,issue,negative,positive,neutral,neutral,positive,positive
450256961,"I am new to GANs. I have this query: If we cannot interpret anything from loss functions and suppose the generated images are not good, how can we understand what parameters (or model architecture) to change and check?",new query interpret anything loss suppose good understand model architecture change check,issue,negative,positive,positive,positive,positive,positive
449998478,"1. I fixed the issue with the latest commit. 
2. Yeah, add your images to `train` and `test` directory. I also added a new script `scripts/test_colorization.sh`. 
3. I am not sure about overfitting. You can try both `resnet_9blocks` and `unet_256` architecture. ",fixed issue latest commit yeah add train test directory also added new script sure try architecture,issue,positive,positive,positive,positive,positive,positive
449993046,"Sometimes GAN loss might explode if you have an unusual training input (e.g., a white image). But it should be fine as long as the loss is reasonable for most of the times. ",sometimes gan loss might explode unusual training input white image fine long loss reasonable time,issue,negative,positive,positive,positive,positive,positive
449932605,Can you tell me what does GAN loss exploding signify and when does that happen?,tell gan loss signify happen,issue,negative,neutral,neutral,neutral,neutral,neutral
449761224,"Hi @Paylet, were you able to solve your problem?  Also, were you training with the default parameters? or did you change them (e.g. learning rate, batch_size, etc)?   I'm trying to start something similar and I'm also having a similar issue as described by you.  Any help is appreciated. Thank you. ",hi able solve problem also training default change learning rate trying start something similar also similar issue help thank,issue,positive,positive,positive,positive,positive,positive
449513290,Try to add ``--lambda_identity 0`. The identity loss assumes that the input and output images have the same dimensions. I just added a new assertion. ,try add identity loss input output added new assertion,issue,negative,positive,positive,positive,positive,positive
448823581,"I have a similar problem and I'm wondering if you can help me.  I am trying to use some `np.int16` images.  I assume that I could somehow rescale each pixel to be in the range [0,255] but not sure if there is a better way of dealing with my data.  I feel that going from `np.int16` to `np.int8` is losing some information.  Do you think I can just train using the np.int16 data? or somehow float data that is not in the range [0,255]?  Is this range a requirement?  

Any help is appreciated.  Thank you in advance. ",similar problem wondering help trying use assume could somehow range sure better way dealing data feel going losing information think train data somehow float data range range requirement help thank advance,issue,positive,positive,positive,positive,positive,positive
448774054,See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L21). You can use `--iter` and `--iter_decay`.,see line use iter,issue,negative,neutral,neutral,neutral,neutral,neutral
448662415,"any tips on the best way to go about setting up a colab for cyclegan, mishurov? Looked through your repos, figured you would know lol. ",best way go setting figured would know,issue,positive,positive,positive,positive,positive,positive
447719162,"You can download a pix2pix dataset and see an example. Follow the download [instruction](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md).  Each image has a pair of input and output images. 
|——dataset
|——|—train (1.jpg、2.jpg、3.jpg..........1000.jpg)
|——|—test (1.jpg、2.jpg、3.jpg..........50.jpg)",see example follow instruction image pair input output,issue,negative,neutral,neutral,neutral,neutral,neutral
447706554,"I think I know the reason for my question. I  made the 'pahse = unaligned', but if I want to make the  'pahse = aligned',I don't know how to store my data set A, B.",think know reason question made unaligned want make know store data set,issue,negative,neutral,neutral,neutral,neutral,neutral
447651274,"@azadyasar could you attach the command line options and also the list of options you are using? 
I did a clean install of Pytorch 1.0.0 on CUDA 9.2 and it seems to work on both GPU and CPU mode. 

",could attach command line also list clean install work mode,issue,negative,positive,positive,positive,positive,positive
447577676,"It's synchronized in pix2pix. It's not synchronized in CycleGAN.  For unpaired case, we can assume that data are not aligned.",synchronized synchronized unpaired case assume data,issue,negative,neutral,neutral,neutral,neutral,neutral
447392505,Could you try to run our code on the default datasets first? ,could try run code default first,issue,negative,positive,positive,positive,positive,positive
447321383,"Thank you for your answer @junyanz . As I said in the post, I've actually tried to run and test the pre-trained pix2pix model (edges2handbags) with the `--model test` option. But unfortunately I'm getting the following error.

`loading the model from ./checkpoints/edges2handbags/latest_net_G.pth
Traceback (most recent call last):
  File ""test.py"", line 20, in <module>
    model.setup(opt)
  File ""/home/ay/Documents/workspaces/py_ws/.virtualenvs/pytorch/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 43, in setup
    self.load_networks(opt.epoch)
  File ""/home/ay/Documents/workspaces/py_ws/.virtualenvs/pytorch/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 135, in load_networks
    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))
  File ""/home/ay/Documents/workspaces/py_ws/.virtualenvs/pytorch/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 115, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
  File ""/home/ay/Documents/workspaces/py_ws/.virtualenvs/pytorch/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 115, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
  File ""/home/ay/Documents/workspaces/py_ws/.virtualenvs/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 518, in __getattr__
    type(self).__name__, name))
AttributeError: 'Sequential' object has no attribute 'model'
`

I have not thought clearly. You are right the reason for getting different output images might be that instead of the change in real image pairings.",thank answer said post actually tried run test model model test option unfortunately getting following error loading model recent call last file line module opt file line setup file line net file line module key file line module key file line type self name object attribute thought clearly right reason getting different output might instead change real image,issue,negative,negative,neutral,neutral,negative,negative
446842799,"If you have ground truth (real B), you should then use pix2pix rather than CycleGAN. Otherwise, you don't need the corresponding real B.",ground truth real use rather otherwise need corresponding real,issue,negative,positive,positive,positive,positive,positive
446832988,"thanks for the advice, I will try your suggestion
But not sure I get it. 
Unfortunately, I don't have ground truth of my inference (real B). I only have the inference  real A images. The objective of this work is to use cyclegan to synthesize the inference Fake A close to ground truth. If I only mix the inference real A with training real A, without providing the inference real B. Does it help? 
",thanks advice try suggestion sure get unfortunately ground truth inference real inference real objective work use synthesize inference fake close ground truth mix inference real training real without providing inference real help,issue,positive,positive,neutral,neutral,positive,positive
446780545,"There is no explicit way of doing that. Maybe you can add your test images to your training set. In your case, it is fine to train and test on the dataset.",explicit way maybe add test training set case fine train test,issue,negative,positive,positive,positive,positive,positive
446764868,"I added one conv layer to the discriminator using n_layer = 4. The training images looks great and capture the long range pattern. However, the inference image (Fake A) clarity got compromised. It is hard to see if it captures the long range textile pattern. Is there a way, Cyclegan can tune the sharpness of the images, _**particularly at inference phase**_.  

![image](https://user-images.githubusercontent.com/39716577/49902377-c3679c00-fe31-11e8-9d6c-64bb91e29f62.png)
",added one layer discriminator training great capture long range pattern however inference image fake clarity got hard see long range textile pattern way tune sharpness particularly inference phase image,issue,positive,positive,neutral,neutral,positive,positive
446322911,"If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use `--model test` option. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix) for more details.
pix2pix with dropout will give you different results during test time. But the results should not depend on the real images. Also, for edges2photos application, you may want to preprocess the input before you feed it to the network. See [here](https://github.com/phillipi/pix2pix#extracting-edges) for more details.",would like apply model collection input rather image please use model test option see dropout give different test time depend real also application may want input feed network see,issue,positive,positive,neutral,neutral,positive,positive
446318668,"If you train a CycleGAN model, you don't need to run the script `combine_A_and_B`. (It is only for pix2pix). The structure for CycleGAN should be: 
├── apple2orange
│   ├── testA
│   └── trainA
│   ├── testB
│   └── trainB

",train model need run script structure testa,issue,negative,neutral,neutral,neutral,neutral,neutral
446315597,I will keep the PatchGAN consistent with the Torch [code](https://github.com/phillipi/pix2pix/blob/master/models.lua#L206). But you are free to use other discriminators in your experiments. ,keep consistent torch code free use,issue,positive,positive,positive,positive,positive,positive
446075529,"Sorry, we don't have that model... Please train a new model. Thanks!",sorry model please train new model thanks,issue,positive,negative,neutral,neutral,negative,negative
446005658,Could you add `--no_dropout`? Make sure that you use the same `--netG` and `--norm` option during training and test. ,could add make sure use norm option training test,issue,negative,positive,positive,positive,positive,positive
446003702,"You can use Van Gogh's artworks. A few hundred should be OK. If your photo is a portrait, you can also train a model on Van Gogh's portrait paintings.",use van hundred photo portrait also train model van portrait,issue,negative,neutral,neutral,neutral,neutral,neutral
445540523,"please tell me , i am beginner,i meet same question , i just change the netG, what can i do? thank you every much.
",please tell beginner meet question change thank every much,issue,positive,positive,positive,positive,positive,positive
445446835,"To address that issue, I have trained my model to learn how to map the same image to same image. Once it is trained I used its weights to perform my operation so L1 loss is low ( as the entire image is same except some pixels ), but in this way it still does not look at small differences in images. ",address issue trained model learn map image image trained used perform operation loss low entire image except way still look small,issue,negative,negative,neutral,neutral,negative,negative
445446661,Maybe your L1 loss is too big. You can try to use a smaller L1 loss.,maybe loss big try use smaller loss,issue,negative,neutral,neutral,neutral,neutral,neutral
445314152,"Your implementation with CycleGAN looks good. Probably need to tweak some small details. 
In their paper, Figure 15 shows some texture transfer results with pix2pix. ",implementation good probably need tweak small paper figure texture transfer,issue,negative,positive,positive,positive,positive,positive
445312475,"thank you. 
The reference is exactly what I want. Though I haven't wrap up my mind on how to train it with cyclegan, as I need to train A-> B generator to generate texture right. Any suggestions on how it can be implement with cyclegan?",thank reference exactly want though wrap mind train need train generator generate texture right implement,issue,negative,positive,positive,positive,positive,positive
445268271,"real_A: real images in domain A. 
fake_B: G(real_A); generated images from real_A
rec_A: F(G(real_A)): reconstructed images 
The same thing applies to real_B, fake_A, and rec_B.
You may want to view these images in the webpage `index.html`.",real domain reconstructed thing may want view,issue,negative,positive,positive,positive,positive,positive
444863641,"You would need to run an ssh tunnel for the used port.

On Thu, Dec 6, 2018 at 20:31 HuiZHANG <notifications@github.com> wrote:

> @SsnL <https://github.com/SsnL> Thank you. But it said ""This site can’t
> be reached"" when I click the http link.
> I run cycleGAN with ssh to remote server. Can I open it in my personal
> computer?
>
> —
> You are receiving this because you were mentioned.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/371#issuecomment-444855896>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFaWZXCwQaHcD-vT3nf08OdQigJhbIt5ks5u2Q4zgaJpZM4WeLLP>
> .
>
",would need run tunnel used port wrote thank said site click link run remote server open personal computer reply directly view mute thread,issue,negative,neutral,neutral,neutral,neutral,neutral
444855896,"@SsnL Thank you. But it said ""This site can’t be reached"" when I click the http link. 
I run cycleGAN with ssh to remote server. Can I open it in my personal computer?",thank said site click link run remote server open personal computer,issue,negative,negative,neutral,neutral,negative,negative
444841555,"@junyanz ,hello,after I run the 'python -m visdom.server',there is no response.

the details:
Downloading scripts. It might take a while.
It's Alive!
INFO:root:Application Started
You can navigate to http://localhost.localdomain:8097
",hello run response might take alive root application navigate,issue,negative,positive,neutral,neutral,positive,positive
444554796,You can choose to add your additional input to D as well. You can modify the D similarly. ,choose add additional input well modify similarly,issue,negative,neutral,neutral,neutral,neutral,neutral
444433109,"And D is not affected?
I mean... Image A plus input I1 should result in image B1 and image A plus input I2 should result in image B2. Therefore the comparison of D between A und B1 is different to the comparison between A und B2.",affected mean image plus input result image image plus input result image therefore comparison different comparison,issue,negative,negative,negative,negative,negative,negative
444350651,"Yes, you can test your model only in one direction. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) for more details. ",yes test model one direction see,issue,negative,neutral,neutral,neutral,neutral,neutral
444289094,Maybe you can increase the receptive field of your discriminator. (add one conv layer for example). There is a texture synthesis [work](https://arxiv.org/abs/1805.04487) related to your task.  You may want to have a look. ,maybe increase receptive field discriminator add one layer example texture synthesis work related task may want look,issue,negative,neutral,neutral,neutral,neutral,neutral
444288407,"The training set size depends on your tasks and domains. The more complicated task/domain is, the more images you meed. In your case, applying some cropping as data argumentation should not decrease the quality of images. ",training set size complicated meed case data argumentation decrease quality,issue,negative,negative,negative,negative,negative,negative
444148843,"It's possible. You can modify the G to take two inputs. In a follow-up [project](https://github.com/junyanz/BicycleGAN/blob/master/models/networks.py#L393), our network can take both image and additional input. ",possible modify take two project network take image additional input,issue,negative,neutral,neutral,neutral,neutral,neutral
443983940,"Replying @taesung89, no, the dataset setting is exactly like what I mentioned. The A domain is the sketch domain, while the B domain is photo domain

Here's the proof for domain A set
![image](https://user-images.githubusercontent.com/12664445/49422223-9548e300-f7c5-11e8-93a0-124164445a93.png)

And here's for domain B set
![image](https://user-images.githubusercontent.com/12664445/49422236-a691ef80-f7c5-11e8-870f-4044197190d3.png)

I suppose, judging from the header of each table in the HTML, it reads data in domain B as input, thus making it `real_A`. I'll try experimenting it on switched sets (A being photo and B being sketched)",setting exactly like domain sketch domain domain photo domain proof domain set image domain set image suppose header table data domain input thus making try switched photo,issue,negative,positive,positive,positive,positive,positive
443918353,Sounds great! Thanks for quick response. ,great thanks quick response,issue,positive,positive,positive,positive,positive,positive
443917937,"@taesung89 Thanks for responding.

facebookresearch/visdom#450 does look like the problem I am seeing.

I guess this issue can be closed as a known problem with visdom?  It doesn't appear to affect anything that I can tell, so closing is probably fine.",thanks look like problem seeing guess issue closed known problem appear affect anything tell probably fine,issue,negative,positive,positive,positive,positive,positive
443915278,This could be some version problem with visdom or its dependencies. Could you look at this post ([https://github.com/facebookresearch/visdom/issues/450](https://github.com/facebookresearch/visdom/issues/450))?,could version problem could look post,issue,negative,neutral,neutral,neutral,neutral,neutral
443911858,"By the way, looking at your results, it looks like the dataset A and B were flipped. Shouldn't real_A be sketch images according to your description?",way looking like sketch according description,issue,negative,neutral,neutral,neutral,neutral,neutral
443559816,"@junyanz 

> Are you using the latest code?

I'm using the latest checkout from `master`.

I'm not sure why you're not able to reproduce this.  Maybe it has to do with different `python` versions?  I'm using Python 3.7.1.

> file ./checkpoints/maps_cyclegan/web/index.html is not created within the first few minutes of running the above command, but maybe it takes a while before it is created?

After running `train.py` for a while using the above command, I have confirmed that this `index.html` is created.

> If I open up the Visdom server at http://localhost:8097, I just see the default blue screen, but I don't see anything that appears to be output from train.py. Maybe it takes a while before something appears?

I have confirmed that this actually does work as well.  I was correct in my guess that it takes a while before something appears.

> `'>' not supported between instances of 'float' and 'NoneType'`

This error still repeatedly shows up.

------------------------------------------------------------------------

To sum up, I'm still seeing the error `Visdom python client failed to establish socket to get messages from the server`.  I'm also seeing the error `'>' not supported between instances of 'float' and 'NoneType'`.

However, it doesn't appear that either of these are actually causing problems, as `train.py` appears to be working.",latest code latest master sure able reproduce maybe different python python file within first running command maybe running command confirmed open server see default blue screen see anything output maybe something confirmed actually work well correct guess something error still repeatedly sum still seeing error python client establish socket get server also seeing error however appear either actually causing working,issue,negative,positive,positive,positive,positive,positive
443557532,I cannot reproduce your bug. Are you using the latest code?,reproduce bug latest code,issue,negative,positive,positive,positive,positive,positive
443496325,"Also, it looks like the `'>' not supported between instances of 'float' and 'NoneType'` error may have been introduced by the fix to the issue below?

https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/440",also like error may fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
443437664,"For pix2pix, you can modify this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/pix2pix_model.py#L72). For CycleGAN, you can modify this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L92). ",modify line modify line,issue,negative,neutral,neutral,neutral,neutral,neutral
443413483,"> Maybe [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) will help you.

Ok cool...i'll check it out. Thanks",maybe help cool check thanks,issue,positive,positive,positive,positive,positive,positive
443308763,You can use a big fineSize which is larger than 256. CycleGAN uses 2-4 times more memory than pix2pix. I will expect a bigger batch size than 1.,use big time memory expect bigger batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
443247341,"What will hapen, if one takes images (finesize) being larger than 256x256?

I noticed that pix2pix can run even for batchsize=16 on my GPU, but cycle_gan is not able to use batchsize>1 without CUDA error ""out of memory"". Is this normal?",one run even able use without error memory normal,issue,negative,positive,positive,positive,positive,positive
442893380,"I've had similar issue while trying to test CycleGAN (`--model test`) on my own dataset.
Default `--norm instance` was used for both training and testing. 
Deleting the root project directory and cloning this repository again did not help.

I've noticed that the problem is wrong keys in the `state_dict` dictionary.
E.g. model tries to load missing key ""model.10.conv_block.6.weight"", however there is unexpected key ""model.10.conv_block.5.weight"". So, I decided to fix these wrong keys. 

Based on my error message I have created two lists (missing_list and expected_list). Afterwards I've replaced wrong keys with corresponding correct ones.

Example of my snippet is [here](https://gist.github.com/dovletov/8c7b742e4e19b8acf3188b6e1bc97dd1). I've inserted it after line 135 [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/0442670604839f5a5d3e1570be563a27568c16e4/models/base_model.py#L135).
",similar issue trying test model test default norm instance used training testing root project directory repository help problem wrong dictionary model load missing key model weight however unexpected key model weight decided fix wrong based error message two afterwards wrong corresponding correct example snippet inserted line,issue,negative,negative,negative,negative,negative,negative
442214058,It is called mode collapse. It sometimes happened when your dataset is too small. Try using some data argumentation or use a larger dataset. ,mode collapse sometimes small try data argumentation use,issue,negative,negative,negative,negative,negative,negative
442213603,See the training details in the appendix of cyclegan [paper](https://arxiv.org/abs/1703.10593). ,see training appendix paper,issue,negative,neutral,neutral,neutral,neutral,neutral
442213310,"Yes, you can use 3x3. We follow the architecture from some early work such as [DCGAN](https://github.com/pytorch/examples/tree/master/dcgan), and [style-transfer.](https://github.com/jcjohnson/fast-neural-style) ",yes use follow architecture early work,issue,negative,positive,neutral,neutral,positive,positive
440895552,You can change your `fineSize` to match the `loadSize`. You can also crop the center part of the images and feed them to the model. You can write a new transform function [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L26). ,change match also crop center part feed model write new transform function,issue,negative,positive,neutral,neutral,positive,positive
440728043,"Each time the test is conducted, the input image seems to be randomly cropped and resized? How to fix that to use the original input image? Thanks. @junyanz ",time test input image randomly fix use original input image thanks,issue,positive,positive,neutral,neutral,positive,positive
440633615,"Hello phillipi,
thanks for you explaination and for sharing your implementation!
I'm also trying to better understand PatchGAN Discriminator, and I understand that is equivalent to a convnet from a design point of view. In other words, if I have to implement a patchgan discriminator, I should do as you did. 
But what happens if I already got a (pre-trained) neural network which accepts as input the receptive-field (in this case 70x70 images) of a bigger image (e.g. 1024x1024)? I couldn't figure out how the network should be integrated efficiently or rewritten using convolutional layers without modifying the architecture of the pre-trained network.
P.S. I'm trying to implement this in tensorflow, but I don't think it's a platform-related issue.

Thank you!",hello thanks implementation also trying better understand discriminator understand equivalent design point view implement discriminator already got neural network input case bigger image could figure network efficiently convolutional without architecture network trying implement think issue thank,issue,positive,positive,positive,positive,positive,positive
440510009,We provide a single direction test script. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) for more details.,provide single direction test script see,issue,negative,negative,neutral,neutral,negative,negative
440509798,"You probably need to set `--niter` and `--niter_decay`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L21),",probably need set niter see,issue,negative,neutral,neutral,neutral,neutral,neutral
439629491,Thanks for your contribution. We will try to add more documentation later.,thanks contribution try add documentation later,issue,negative,positive,neutral,neutral,positive,positive
439421016,We just applied our method frame by frame. You can see flickering artifacts in our video as well. See recent [work](https://github.com/NVIDIA/vid2vid) on video synthesis.,applied method frame frame see flickering video well see recent work video synthesis,issue,negative,neutral,neutral,neutral,neutral,neutral
439041519,It should be 64. We will update the appendix in a future version. ,update appendix future version,issue,negative,neutral,neutral,neutral,neutral,neutral
439038753,"Yeah, I guess you also need to add a `downsample` layer. You can look at Table 2 in the progressive gans [paper](https://arxiv.org/pdf/1710.10196.pdf). ",yeah guess also need add layer look table progressive paper,issue,negative,neutral,neutral,neutral,neutral,neutral
438936708,"> I haven't tried the distill tricks by myself. For discriminators, they mentioned that you can replace stride 2 [conv](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L325) with a regular 3x3 conv.

Does Regular 3x3 conv mean that we just need to change stride =1 in this case ?",tried distill replace stride regular regular mean need change stride case,issue,negative,negative,negative,negative,negative,negative
438929964,"sir how about Q1
Thank you sir.
Q1: In the paper, you write c7s1-32, but in your code , ngf=64, maybe somewhere I have confused?

    parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in first conv layer')
    model = [nn.ReflectionPad2d(3),
    nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias)
    norm_layer(ngf),
    nn.ReLU(inplace=True)]
",sir thank sir paper write code maybe somewhere confused first layer model,issue,negative,negative,neutral,neutral,negative,negative
438785085,We didn't use it in the original paper. It might help. Feel free to try it.,use original paper might help feel free try,issue,positive,positive,positive,positive,positive,positive
438784372,not sure. The cycle consistency loss might affect the use_bias for the G's last layer. I don't think we use the bias for the G's last layer anyway. use_bias could affect both G and D.,sure cycle consistency loss might affect last layer think use bias last layer anyway could affect,issue,negative,positive,positive,positive,positive,positive
438737660,"I think the following paper proposes a solution to that issue:

https://arxiv.org/pdf/1810.06758.pdf",think following paper solution issue,issue,negative,neutral,neutral,neutral,neutral,neutral
438698114,"Q2: In the d64 and d128, you don't use ReflectionPad, right? As I know, in the neural_style, it use ReflectionPad in the downsampling? or maybe the difference don't influence the results?",use right know use maybe difference influence,issue,negative,positive,positive,positive,positive,positive
438694262,"Thank you sir.
Q1: In the paper, you write c7s1-32, but in your code , ngf=64, maybe somewhere I have confused?
> parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in first conv layer')
model = [nn.ReflectionPad2d(3),
                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias)
                 norm_layer(ngf),
                 nn.ReLU(inplace=True)]
",thank sir paper write code maybe somewhere confused first layer model,issue,negative,negative,neutral,neutral,negative,negative
438681223,"Even if InstanceNorm doesn't have affine parameters, under the calculation x-E(x), it also remove the influence of use_bias. Maybe somewhere I'm wrong, please check me out if you're free.",even affine calculation also remove influence maybe somewhere wrong please check free,issue,negative,negative,neutral,neutral,negative,negative
438649746,"I haven't tried the distill tricks by myself. For discriminators, they mentioned that you can replace stride 2 [conv](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L325) with a regular 3x3 conv.  ",tried distill replace stride regular,issue,negative,neutral,neutral,neutral,neutral,neutral
438646852,"Their method has a few post-processing steps (as detailed in Sec 3.5). You can potentially apply their steps to remove noise in your results. I think pix2pix can overfit to 1-2 images as shown [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/416). 

I guess that the generalization ability does not only depend on the number of images,  but also on (1) the complexity of your dataset, and (2) if your test images look like your training images. ",method detailed sec potentially apply remove noise think overfit shown guess generalization ability depend number also complexity test look like training,issue,positive,positive,positive,positive,positive,positive
438642348,You can ignore it. It is used to load models trained with older PyTorch versions. InstanceNorm can have running_mean and running_var when `track_running_stats=False`.,ignore used load trained older,issue,negative,positive,positive,positive,positive,positive
438641495,"I will say that 90 is not enough for day2night. Sometimes the model doesn't work for new types of natural scenes, which are not present in the training set. For your case, you may want to see if the model works during training vs. during the test. The model should perform well in the training test at least before we are talking out test time performance. You should do augmentation (flip, cropping, jitter the color, [etc](https://github.com/mdbloice/Augmentor).). You might want to use dropout. (I assume that you have used it)",say enough sometimes model work new natural present training set case may want see model work training test model perform well training test least talking test time performance augmentation flip jitter color might want use dropout assume used,issue,positive,negative,neutral,neutral,negative,negative
438596698,"Hi @SsnL and @phillipi 
I currently research about `lower-to-high resolution `image. My `Generato`r and `Discriminator` artichecture are same as your cycleGan. I followed your guide to replace the `ConvTranspose2d` but it seems the checkerboard artifacts still appear in my result. Follow the paper [distill](https://distill.pub/2016/deconv-checkerboard/), they mentioned about `resize the image` (using nearest-neighbor interpolation or bilinear interpolation) and also changing something in `Discriminato`r. Could you please tell me how we implement it to remove the checkerboard artifacts?. This is my result
Input 
![116_real_a](https://user-images.githubusercontent.com/28720676/48473070-a63fad80-e832-11e8-977c-4c98fbf8aa12.png)
result
![116_fake_b2](https://user-images.githubusercontent.com/28720676/48473092-afc91580-e832-11e8-9e46-8aac541d2c46.png)
",hi currently research resolution image discriminator guide replace checkerboard still appear result follow paper distill resize image interpolation bilinear interpolation also something could please tell implement remove checkerboard result input result,issue,negative,neutral,neutral,neutral,neutral,neutral
438512133,"So, for day2night, was ~90 different natural scenes (which maybe like 1000s of images obtained from every frame) sufficient for the model to learn? I have this query because even in the colorization case, I just have 170 images (which are then augmented to create 10k images). But then model is not learning in colorization.

If day2night is able to learn from just 90 different scenes and colorization is not able to learn from 170 images (10k after augmentation), then is it like the learning that colorization model has to do is much more than that for day2night? (I mean is it like the colorization network has to learn about various features like face and their color as opposed to day2night case where that much learning is not required?)

And by more data, do you mean different dataset (not the ones obtained from augmenting)? Is there anything I could do to train the model using these small set of images?

Thanks.",different natural maybe like every frame sufficient model learn query even colorization case augmented create model learning colorization able learn different colorization able learn augmentation like learning colorization model much mean like colorization network learn various like face color opposed case much learning data mean different anything could train model small set thanks,issue,positive,positive,neutral,neutral,positive,positive
438435850,Thanks a lot!! Your immediate responses helped a lot!!,thanks lot immediate lot,issue,negative,positive,positive,positive,positive,positive
438289495,Thanks. Should be fixed by the latest commit. ,thanks fixed latest commit,issue,positive,positive,positive,positive,positive,positive
438283370,I see. I guess you can also train a pix2pix for the masking. ,see guess also train,issue,negative,neutral,neutral,neutral,neutral,neutral
438282065,"Thank you!
From this page https://phillipi.github.io/pix2pix/,""Background masking"".
I want to know how to do.Thanks!",thank page want know,issue,negative,neutral,neutral,neutral,neutral,neutral
438264251,You can set fine_size between 1/3 to 1/2 of the load_size. load_size depends on the GPU memory.  The code will first load the image as `load_size x load_size` and then crop `fine_size x fine_size` patces. ,set memory code first load image crop,issue,negative,positive,positive,positive,positive,positive
438260710,"It seems what you have pointed about the dataset is true. The resolution of my input images is very high(1920 × 1080). What would be preferable values for load and finesize for good results? Also, would it be a good idea to use `--no_flip` during training? ",pointed true resolution input high would preferable load good also would good idea use training,issue,positive,positive,positive,positive,positive,positive
438255562,"I think day2night is slightly different from your colorization case. There are 91 webcam videos, but each video has many frames. For each webcam, we created all day photos (""night"" attribute < 0.1) to all night photos. (""night"" attribute > 0.8).Unfortunately, you still only have ~90 different natural scenes no matter what you do. Therefore, a larger dataset might be better. 

I think in general more data should be helpful. People usually train colorization models on millions of images. See [colorization-pytorch](https://github.com/richzhang/colorization-pytorch) for more details.


",think slightly different colorization case video many day night attribute night night attribute still different natural matter therefore might better think general data helpful people usually train colorization million see,issue,positive,positive,positive,positive,positive,positive
438251440,I think it is possible. I think you need to rewrite the `test.py` and add some flags to `test_options`. You don't need to use visualizer. You can write your own IO code.,think possible think need rewrite add need use visualizer write io code,issue,negative,neutral,neutral,neutral,neutral,neutral
438250868,"I guess somehow, there might be more trees in snowy days than in clear days. Maybe you can try to do cyclegan in the cropped patches (e.g., if your original image size is 256, you can set `loadSize 256 fineSize 128` or `loadSize 256 fineSize 96`). You can also use the identity [loss](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#what-is-the-identity-loss-322-373-362). ",guess somehow might snowy day clear day maybe try original image size set also use identity loss,issue,negative,positive,positive,positive,positive,positive
438249943,It seems that the question is not related to the repo. See [this](http://xiaoyongshen.me/webpage_portrait/index.html) for portrait segmentation,question related see portrait segmentation,issue,negative,neutral,neutral,neutral,neutral,neutral
438208168,"Thank you both for the help. 

I've got this mostly working, and I can test it using the script provided on this Git. 

Question: Is there a way to run this from an image coming from OpenCV webcam? Currently the test needs to be run from .sh with a number of arguments / parameters that are embedded in a variety of different files (test, test_options, base_options, visualizer, etc.) and I'm not quite sure how to pull all of what is required out to run .pth model that has been created on a real-time feed. 

I assume this is possible, just not sure how. ",thank help got mostly working test script provided git question way run image coming currently test need run number variety different test visualizer quite sure pull run model feed assume possible sure,issue,positive,positive,positive,positive,positive,positive
438130140,"I was able to figure it out now. Indeed, it was just that I didn't understand how pytorch worked. 
The answer is that if you do them separately, then once you call loss.backward() for one generator's loss, then both generators will be removed from memory unless you use the pytorch `retain_graph=True` argument to backward. 
Thus, if we do them in the same function and call .backward on the summed loss for both, we won't have to deal with using `retain_graph`",able figure indeed understand worked answer separately call one generator loss removed memory unless use argument backward thus function call summed loss wo deal,issue,negative,positive,positive,positive,positive,positive
438097756,"> Could you check if you have used the same normalization (`batchnorm`, `instancenorm`) during training and test?

Thank you @junyanz This resolved it for me.",could check used normalization training test thank resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
438095071,"That would be excellent Ismail!

Running nvidia-smi shows that while I’m using the GPU at 80+%, the
effective use hovers around 3/24gb. Not sure what is reserving the rest.

In any case, I downsampled everything to 256x256 and it worked. Now around
Epoch 50 so will let you know how it goes when done (30 mins/epoch).

On Mon, Nov 12, 2018 at 10:56 PM Ismail Elezi <notifications@github.com>
wrote:

> It seems that 24GB can fit 480x360 images. Maybe you can further reduce
> the size of training images (to 256x256).
>
> This is correct. Even in a NVIDIA® Tesla® V100 32GB, it is hard to work
> with images which are larger than 700 by 700. I converted the code to half
> precision which allows training on 1200 x 1200 images, and am working on
> gradient checkpointing and possibly model parallelism, with the goal of
> reaching 2000 x 2000 training (training on small resolution and generating
> large images seems to not work well).
>
> When everything is tested and working, I can make a pull request if you
> think that might be helpful.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/422#issuecomment-437910825>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/Aqq449_8uJe0dvlT2A9zubrm4FIP1i8zks5uuYwagaJpZM4YNMwr>
> .
>
",would excellent running effective use around sure rest case everything worked around epoch let know go done mon wrote fit maybe reduce size training correct even registered registered hard work converted code half precision training working gradient possibly model parallelism goal reaching training training small resolution generating large work well everything tested working make pull request think might helpful thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
437910825,"> It seems that 24GB can fit `480x360` images. Maybe you can further reduce the size of training images (to `256x256`).

This is correct. Even in a NVIDIA® Tesla® V100 32GB, it is hard to work with images which are larger than 700 by 700. I converted the code to mixed half precision (using NVIDIA Apex) which allows training on 1200 x 1200 images, and am working on gradient checkpointing and possibly model parallelism, with the goal of reaching 2000 x 2000 training (training on small resolution and generating large images seems to not work well).

When everything is tested and working, I can make a pull request if you think that might be helpful.",fit maybe reduce size training correct even registered registered hard work converted code mixed half precision apex training working gradient possibly model parallelism goal reaching training training small resolution generating large work well everything tested working make pull request think might helpful,issue,positive,negative,neutral,neutral,negative,negative
437736067,I fixed some small issues and removed 'load_by_iter' flag. I reused the 'load_iter'. See this [commit](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/c31852967bf9f5b9a45b3e655e0d41fa8e75aa6f) for more details.,fixed small removed flag see commit,issue,negative,negative,neutral,neutral,negative,negative
437732066,I am not sure whether the changes meet your expectation.,sure whether meet expectation,issue,negative,positive,positive,positive,positive,positive
437716251,"netG can still get the gradients. See this PyTorch [blog](https://pytorch.org/docs/stable/notes/autograd.html) for more details. According to the blog, ""This (requires_grad) is especially useful when you want to freeze part of your model, or you know in advance that you’re not going to use gradients w.r.t. some parameters.""",still get see according especially useful want freeze part model know advance going use,issue,positive,positive,positive,positive,positive,positive
437394935,"Thanks for the PR. I have a few comments: 
- can we merge `save_iter_freq` and `save_latest_freq`? Maybe we can reuse `save_latest_freq`. Just different behavious when `save_by_iter` is True. 
- The `--iter` flag might be a little confusing. Maybe change it to `--load_iter`.",thanks merge maybe reuse different true iter flag might little maybe change,issue,positive,positive,neutral,neutral,positive,positive
437356262,"Update:
I trained my own image pairs and the image pairs align perfectly. ",update trained image image align perfectly,issue,positive,positive,positive,positive,positive,positive
437251571,I see. You can also specify `--netG unet --norm batch` for `--model test`.,see also specify norm batch model test,issue,negative,neutral,neutral,neutral,neutral,neutral
437244076,"![test_fake_b](https://user-images.githubusercontent.com/44742883/48242864-1bb51380-e418-11e8-9fcf-507a56413a14.png)

This was the output from edges to image. I only trained it on one image for the base settings, so I would have imagined it would be fairly accurate. Any sense of why the BtoA didn't generate the image exactly?

Would I see more fidelity in terms of painting in colors. Canny generates black background with white edges, but I notice Edges2Cats is black edges on white. Should I invert from that perspective? 
",output image trained one image base would would fairly accurate sense generate image exactly would see fidelity painting color canny black background white notice black white invert perspective,issue,negative,negative,neutral,neutral,negative,negative
437240659,Actually I want to use --dataset_mode single then it seems I must use --model test to go with it,actually want use single must use model test go,issue,negative,negative,neutral,neutral,negative,negative
437234929,"For pix2pix, you just need to use `--model pix2pix` for testing. See the [instruction](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix). `--model test` is useful when you want to apply cyclegan in one direction. ",need use model testing see instruction model test useful want apply one direction,issue,negative,positive,positive,positive,positive,positive
437221462,"Hi, I have encountered the same error. I found this error occurred due to running a pix2pix  model uisng --model test . I have changed line 14 in models/test_model.py from parser = CycleGANModel.modify_commandline_options(parser, is_train=False) to parser = Pix2PixModel.modify_commandline_options(parser, is_train = False) .  Also I added from .pix2pix_model import Pix2PixModel at the beginning of the file. Then it runs well. Hope this helps.",hi error found error due running model model test line parser parser parser parser false also added import beginning file well hope,issue,negative,negative,negative,negative,negative,negative
437073495,"You converted images into a GIF with Adobe products (Premiere and Photoshop). If you want to a free option, you can search `images to GIF` on the web.",converted gif adobe premiere want free option search gif web,issue,positive,positive,positive,positive,positive,positive
436825690,"I am actually implementing this paper [cGAN-based Manga Colorization](https://arxiv.org/pdf/1706.06918.pdf) which uses cgan but the author uses few training images for the purpose of colorization. 

Yes, I have just around 150 training images (very small dataset). But the author of the above-mentioned paper mentions that a single image is sufficient for the cgan model to learn a character (with crops though which I am doing using `resize_and_crop` parameter). That paper seems to be contradicting the general cGAN training procedure. So, can you please explain where I am going wrong.

Also, if cgan requires lots of images, can you suggest me a method to color a picture when I have a very small dataset? (I will checkout the suggested argumentation framework).",actually paper manga colorization author training purpose colorization yes around training small author paper single image sufficient model learn character though parameter paper general training procedure please explain going wrong also lot suggest method color picture small argumentation framework,issue,negative,negative,negative,negative,negative,negative
436758627,"Change '8' to '4'  in `net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)` works for me when input 64*64 images.",change net work input,issue,negative,neutral,neutral,neutral,neutral,neutral
436652409,"Thanks for you reply!
I somehow skipped the line in download_cyclegan_model.sh and did not see the inverse. Thanks again.

I did not specify the resize_or_crop when testing. The dataset I used are the Yosemite dataset. I took the dataset and pre-trained model all off-the-shelf.
I tried again setting resize_or_crop to none and still find the slight alignment issue.


",thanks reply somehow line see inverse thanks specify testing used took model tried setting none still find slight alignment issue,issue,positive,positive,neutral,neutral,positive,positive
436627419,"Thank you.

So would this imply 512x256? Given that A and B should be collated in one
image?

Or should I have A and B in two separate images?


On Wed, Nov 7, 2018 at 9:14 PM Jun-Yan Zhu <notifications@github.com> wrote:

> It seems that 24GB can fit 480x360 images. Maybe you can further reduce
> the size of training images (to 256x256).
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/422#issuecomment-436618581>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/Aqq44xlciK8yUYrkZdSKF7unjh-jNkumks5ustzLgaJpZM4YNMwr>
> .
>
",thank would imply given one image two separate wed wrote fit maybe reduce size training thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
436624324,Aha. Then it was bias = False for batch normalization and bias = True for instance normalization.,aha bias false batch normalization bias true instance normalization,issue,negative,negative,neutral,neutral,negative,negative
436622815,"If you only have one GPU, you should set `gpu_ids=0`. You may want to train a model with low-res images. (`loadSize=143 fineSize=128`).
See [Q & A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#out-of-memory-174) for `out of memory` error.",one set may want train model see memory error,issue,negative,neutral,neutral,neutral,neutral,neutral
436621196,"Models: The pretrained model only has one direction. But you can download different directions using different names `horse2zebra` / `zebra2horse`.

summer2winter: which `resize_or_crop` are you using? If you use cropping, your output image might not align well with the original input. ",model one direction different different use output image might align well original input,issue,positive,positive,positive,positive,positive,positive
436620375,"I don't see a problem here. Maybe you can try to train a model with `--gpu_ids 0` first, and then test it. Then you train the second model with `--gpu_ids` and then test it. See if it works. I don't think two separate jobs will affect each other.  ",see problem maybe try train model first test train second model test see work think two separate affect,issue,negative,positive,positive,positive,positive,positive
436619123,"You might be overfitting your training set. How many training images do you have? For colorization, it at least needs tens of thousands of images. If you don't have many training images, you should apply data argumentation (e.g., like cropping with `loadSize 286` and `fineSize 256`).",might training set many training colorization least need many training apply data argumentation like,issue,negative,positive,positive,positive,positive,positive
436618581,It seems that 24GB can fit `480x360` images. Maybe you can further reduce the size of training images (to `256x256`).,fit maybe reduce size training,issue,negative,positive,positive,positive,positive,positive
436618241,"`InstanceNorm` doesn't have affine parameters (by default). So we add a bias term. 
`BatchNorm` has affine parameters so there is no need for the bias term.",affine default add bias term affine need bias term,issue,negative,neutral,neutral,neutral,neutral,neutral
436566008,"Hi JunYanz, 

Thanks for the note. :)

The images are coming out of the webcam at 1920 x 1080, and I'm saving them as 480x360 sets (1/4 scale). I'm then joining these together to form 960x360 images with an A/B pair. 

Brian",hi thanks note coming saving scale joining together form pair,issue,negative,positive,positive,positive,positive,positive
436558444,Probably because link to pix2pix is the second one when search for 'visdom multiple environment'.,probably link second one search multiple environment,issue,negative,neutral,neutral,neutral,neutral,neutral
436556529,"Sorry, I made issue in wrong repository mistakenly",sorry made issue wrong repository mistakenly,issue,negative,negative,negative,negative,negative,negative
436505104,"Sorry for not conveying the info about my dataset earlier. I used custom training and test set. I used combine_A_and_B.py to create the dataset in the desired format. When I just train one model at a time on a GPU, the test.py seems to work well. But when I train two models at the same time (on two different GPUs in the same PC by giving different GPU ids..like giving `--gpu_ids 0` for the first model and `--gpu_ids 1` for the second model), the training seems to complete. But when I run test.py, I get the above error. 
<br>
So, I would like to know whether the model is getting messed up because of some global variables sharing by the two models (or any similar kind of issue)? And is it right to train two models on a single GPU at the same time (Specifically I would like know whether GPU visualizes memory for every single process)?  I am using NVIDIA GeForce GTX 1080 btw.",sorry used custom training test set used create desired format train one model time work well train two time two different giving different like giving first model second model training complete run get error would like know whether model getting global two similar kind issue right train two single time specifically would like know whether memory every single process,issue,positive,positive,neutral,neutral,positive,positive
436488195,"Does it work for the training and test script on provided datasets (e.g., facades, maps)? It's hard to tell without knowing more details about your training and test. ",work training test script provided hard tell without knowing training test,issue,negative,negative,negative,negative,negative,negative
436488036,CycleGAN assumes that images in different domains share similar content but with different styles. The white background has zero information and does not share content with the target domain. ,different share similar content different white background zero information share content target domain,issue,positive,neutral,neutral,neutral,neutral,neutral
436487558,"What does ""multiple environments"" mean? You mean the visdom environments? You can use the display port but different display ids.",multiple mean mean use display port different display,issue,negative,negative,negative,negative,negative,negative
436470211,"Hi, can you explain why a white background its hard to synthesize, so if I can synthesis CAD model below with some different picture, does it possibly will working?
![plane_0006](https://user-images.githubusercontent.com/21048834/48104551-f933c680-e26e-11e8-8438-92d0348ba104.png)
![plane_0020](https://user-images.githubusercontent.com/21048834/48104559-0355c500-e26f-11e8-8a5d-673ce5c0e795.png)
",hi explain white background hard synthesize synthesis cad model different picture possibly working,issue,negative,negative,neutral,neutral,negative,negative
436298932,"It looks normal, slightly worse than the numbers in the paper (based on Torch implementation). 
When you train the cityscape label-> image, sometimes the tree and building labels will be flipped, which hurts the numbers thought images look great. ",normal slightly worse paper based torch implementation train cityscape image sometimes tree building thought look great,issue,negative,positive,positive,positive,positive,positive
436277266,"Yes, you can use `test.py`. Please see the [instruction](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#cyclegan-traintest) 
It might be challenging to translate a CAD model to an entire image directly as it's hard to synthesize a background from a white background. You might need to segment the airplane and only run CycleGAN between segmented CAD model and segmented real images. ",yes use please see instruction might translate cad model entire image directly hard synthesize background white background might need segment airplane run segmented cad model segmented real,issue,positive,negative,neutral,neutral,negative,negative
436151660,"Sorry, I follow the step from this repo and try to train a module from my datasets, its look like 
trainA
![blender_008](https://user-images.githubusercontent.com/21048834/48047204-4a41ad00-e1d2-11e8-9228-216c2e27b4b9.png)  and
trainB
![test](https://user-images.githubusercontent.com/21048834/48048643-22a11380-e1d7-11e8-9da1-226d35e6d3c6.jpg)
to compression 256*256
So far, after trained complete how can I generate my customized pre-training(airplane2Eva.pb) to let the module transfer all the image what I want
Or I need to follow this command below
python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test
--name can change my dataset and names what I want?
And I also follow the repo below, but the result is not very well, so I want to compare what the difference between
https://github.com/vanhuyz/CycleGAN-TensorFlow
By the way, for the dataset, the airplane position will be the key to determine the result? Or what should I notice?",sorry follow step try train module look like test compression far trained complete generate let module transfer image want need follow command python name model test name change want also follow result well want compare difference way airplane position key determine result notice,issue,positive,negative,neutral,neutral,negative,negative
435516162,@junyanz Would it be possible to freeze these trained features and add new layers to train another task? How do we load the generator model and extract intermediate layers for other training objectives? Thanks. ,would possible freeze trained add new train another task load generator model extract intermediate training thanks,issue,negative,positive,positive,positive,positive,positive
435403970,probably mode collapse. You may want to increase the training set size or add more data augmentation. ,probably mode collapse may want increase training set size add data augmentation,issue,negative,neutral,neutral,neutral,neutral,neutral
434960062,"1 GPU， but i don't know which memory you point, so i upload a picture.",know memory point picture,issue,negative,neutral,neutral,neutral,neutral,neutral
434933486,"Yeah, please ask in the BicycleGAN page.",yeah please ask page,issue,positive,neutral,neutral,neutral,neutral,neutral
434933416,You can open one visdom and use different display_ids.,open one use different,issue,negative,neutral,neutral,neutral,neutral,neutral
434933325,@taesung89  Have you trained models with `scale_width`? Maybe you have the code to fix it.,trained maybe code fix,issue,negative,neutral,neutral,neutral,neutral,neutral
434933181,I think it's too ideal. It rarely happens in practice.,think ideal rarely practice,issue,positive,positive,positive,positive,positive,positive
434926491,"@junyanz  For example, if I want to run pix2pix and CycleGAN codes at the same time, could I open two visdom windows each for pix2pix and CyclGAN separately?",example want run time could open two separately,issue,negative,neutral,neutral,neutral,neutral,neutral
434841878,"Thanks @junyanz . Found the answer there. BicycleGAN is exactly what I’m looking for. 
I want a pix2pix model which can produce different watercolor paintings from the same pencil sketch.
If I have some followup questions, should I ask in the BicycleGAN page?",thanks found answer exactly looking want model produce different pencil sketch ask page,issue,negative,positive,positive,positive,positive,positive
434834134,It's to do with visdom's visualization expecting images of the same size. For now I'm using --display_id 0 to bypass the problem.,visualization size bypass problem,issue,negative,neutral,neutral,neutral,neutral,neutral
434764563,"Hi @junyanz, can we know that by seeing if D outputs 0.5 or is that too ideal in practice? Thanks",hi know seeing ideal practice thanks,issue,positive,positive,positive,positive,positive,positive
434601825,Thanks for your reply. I'll check the paper and try their tricks.,thanks reply check paper try,issue,negative,positive,positive,positive,positive,positive
434600051,"These papers papers really helps, thanks a lot.
Besides, can you provide paired data for reference?
@junyanz ",really thanks lot besides provide paired data reference,issue,negative,positive,positive,positive,positive,positive
434593474,"I also cannot find the implementation 70x70 PatchGAN loss in this code. @liuwei16 if u manage to locate the implementation of patchGAN in this code, please kindly tell me, thx.",also find implementation loss code manage locate implementation code please kindly tell,issue,negative,positive,positive,positive,positive,positive
434556754,Not sure what you are looking for. Could you run visdom in the background?,sure looking could run background,issue,negative,positive,positive,positive,positive,positive
434553915,Your results look really cool. Data augmentation might help! See this paper for training a pix2pix [model](https://arxiv.org/abs/1805.04487) with only one image. They have some tricks!,look really cool data augmentation might help see paper training model one image,issue,positive,positive,positive,positive,positive,positive
434553853,Thank you too much. Sorry that I missed this point.,thank much sorry point,issue,negative,negative,negative,negative,negative,negative
434553626,"I guess so. It can help if you don't have paired data. See these prior work [1](http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w13/Yuan_Unsupervised_Image_Super-Resolution_CVPR_2018_paper.pdf), [2](https://arxiv.org/pdf/1806.09748.pdf). ",guess help paired data see prior work,issue,negative,neutral,neutral,neutral,neutral,neutral
434527433,"I have same problem, and I found my softmax_output=7, but my real_label=8, so I fixed it. May be you should check it out",problem found fixed may check,issue,negative,positive,neutral,neutral,positive,positive
434035477,"There is definitely a dependence in the forward pass which is what you linked to

But I'm not sure how that would affect the order in which the backward pass is done 

I don't see how the current way would produce a different result from just doing it separately
```
func
        # GAN loss D_A(G_A(A))
        self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)
        # GAN loss D_B(G_B(B))
        self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)
        # Forward cycle loss
        self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A
        # Backward cycle loss
        self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B
```
vs 
```
func1
        # GAN loss D_A(G_A(A))
        self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)
        # Forward cycle loss
        self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A

func2
        # GAN loss D_B(G_B(B))
        self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)
        # Backward cycle loss
        self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B
```

The losses are just accumulated, so the order shouldn't matter right? Unless I'm just missing something about how pytorch works.
``` 
self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B
self.loss_G.backward()
```",definitely dependence forward pas linked sure would affect order backward pas done see current way would produce different result separately gan loss true gan loss true forward cycle loss backward cycle loss gan loss true forward cycle loss gan loss true backward cycle loss order matter right unless missing something work,issue,negative,positive,positive,positive,positive,positive
434007074,"The code updates them at the same time. See these [lines](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L82).
```python
self.fake_B = self.netG_A(self.real_A)
self.rec_A = self.netG_B(self.fake_B)
```",code time see python,issue,negative,neutral,neutral,neutral,neutral,neutral
433772545,"On (2), it doesn't seem like the code needs to update G_A and G_B in the same function?
```
 # GAN loss D_A(G_A(A))
 self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)
 # GAN loss D_B(G_B(B))
 self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)
 # Forward cycle loss
 self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A
 # Backward cycle loss
 self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B
```

Where is the dependence in the cycle consistency loss?",seem like code need update function gan loss true gan loss true forward cycle loss backward cycle loss dependence cycle consistency loss,issue,negative,positive,positive,positive,positive,positive
433569454,You can install the `dominate` package. You can also modify the `visualizer.py` to remove dominate. I think visdom is using a remote environment.,install dominate package also modify remove dominate think remote environment,issue,negative,negative,neutral,neutral,negative,negative
433268337,"It only initializes them to zero, which is a common practice in training conv nets. In training, they will become nonzero.",zero common practice training training become nonzero,issue,negative,negative,negative,negative,negative,negative
433241552,"Thank you for your reply, I have solved this problem and wish you a smooth job!",thank reply problem wish smooth job,issue,negative,positive,positive,positive,positive,positive
432519805,some people said it related to NCCL of Nvidia? Is it right?,people said related right,issue,negative,positive,positive,positive,positive,positive
432477138,"Thank you for the reply. I also considered that it might be because of GAN and tried training without GAN and only with L1 loss. However I noticed that the noise still exists. Can you also guess why this might have happened? 
I uploaded some pictures trained only by L1 loss but had noise in test output.
Thank you.

![08932_015 jpg_enhanced_fake_b](https://user-images.githubusercontent.com/44370759/47399826-717d9080-d775-11e8-86a1-688b6bb9d076.png)

![07841_006 jpg_enhanced_fake_b](https://user-images.githubusercontent.com/44370759/47399828-76424480-d775-11e8-8bab-7fb496f0f0bb.png)
",thank reply also considered might gan tried training without gan loss however noise still also guess might trained loss noise test output thank,issue,positive,neutral,neutral,neutral,neutral,neutral
432425183,"Interestingly, Cyclegan also learns to change the shape. If you want to preserve the shape, there might be two things that you can try (1) use a smaller fineSize (e.g., fineSize=128 rather than 256), the model tends to only make smaller shape change when the input patch size is small (2) you can apply a binary mask on top of your generated results: final result = mask * G(input). ",interestingly also change shape want preserve shape might two try use smaller rather model make smaller shape change input patch size small apply binary mask top final result mask input,issue,positive,positive,positive,positive,positive,positive
432250545,These are typical GAN artifacts. Training longer might help. You can also add more training images and apply more data augmentation.,typical gan training longer might help also add training apply data augmentation,issue,negative,negative,negative,negative,negative,negative
432124611,"It's informal Victorian English for wonderful or marvellous. 
For example 
""How's the tea?""
""Absolutely smashing thank you!""

And as for the 0GB error, just interpret it as an out of memory error and that should put you along the right path",informal wonderful example tea absolutely smashing thank error interpret memory error put along right path,issue,negative,positive,positive,positive,positive,positive
432090116,"@CodeMaterial Excuse me, I have a similar problem, what do u mean by smashing?",excuse similar problem mean smashing,issue,negative,negative,negative,negative,negative,negative
431838872,You can resize your images to `200x200` by setting `--display_winsize 200` during test time.,resize setting test time,issue,negative,neutral,neutral,neutral,neutral,neutral
430506821,i found the problem was caused by the incorrect input of --model.  thank you for your reply,found problem incorrect input model thank reply,issue,negative,neutral,neutral,neutral,neutral,neutral
429698479,"You might want to check out some recent keypoints based synthesis methods such as [[1](https://tcwang0509.github.io/vid2vid/)],[[2](https://arxiv.org/abs/1808.07371)],[[3](https://arxiv.org/abs/1808.06847)]. 
Similarly, you can train a model from face [keypoints](https://github.com/1adrianb/face-alignment) to face. 
During the test time, you can apply face keypoint detection on the cloud.",might want check recent based synthesis similarly train model face face test time apply face detection cloud,issue,negative,neutral,neutral,neutral,neutral,neutral
429619670,"Smashing, that did the job. Thanks for the help!",smashing job thanks help,issue,positive,positive,positive,positive,positive,positive
429581427,"what do you mean by ""from face keypoints to face?"" could you please be a bit more specific? I did not quite get the picture..",mean face face could please bit specific quite get picture,issue,negative,negative,negative,negative,negative,negative
429516523,"You can either use (1) cyclegan or (2) train a pix2pix model from face keypoints to face. During test time, you can apply the face keypoint model on the cloud. ",either use train model face face test time apply face model cloud,issue,negative,neutral,neutral,neutral,neutral,neutral
429396231,Maybe try smaller image sizes `--loadSize 143 --fineSize 128`.,maybe try smaller image size,issue,negative,neutral,neutral,neutral,neutral,neutral
429376590,"I currently have 15gb installed. What is the best way to reduce ram consumption or how much do I need to increase my memory by for the maps dataset? Could I reduce the dataset size?
Thank you for the quick response",currently best way reduce ram consumption much need increase memory could reduce size thank quick response,issue,positive,positive,positive,positive,positive,positive
429370824,`UnetSkipConnectionBlock` defines a U-Net module with skip connection. inner_nc is the number of channels in the inner layer of this module. Please check the original U-Net [paper](https://arxiv.org/pdf/1505.04597.pdf) for more details. ,module skip connection number inner layer module please check original paper,issue,positive,positive,positive,positive,positive,positive
429369396,It seems that you are using the CPU mode. But your RAM might be too small for the memory usage of CycleGAN.,mode ram might small memory usage,issue,negative,negative,negative,negative,negative,negative
429302238,"I found the problem was converting from np array to PIL image - it's important that the np array is type uint8 before conversion.  So code to read in the image works fine as follows:
```
A_np = sitk.GetArrayFromImage(sitk.ReadImage(A_path))
B_np = sitk.GetArrayFromImage(sitk.ReadImage(B_path)
A_pil = Image.fromarray(A_np.astype(np.uint8))
B_pil = Image.fromarray(B_np.astype(np.uint8))
```",found problem converting array image important array type conversion code read image work fine,issue,negative,positive,positive,positive,positive,positive
429040322,"This is the preprocessing [code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L24). The preprocessing will do scaling, cropping, toTensor (to float), Normalize to [-1, 1]. Here is the [code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/util.py#L10) to convert tensor to image again. I will recommend that you write a python script to convert it to tensor, and then convert it back, and see if it is the same.",code scaling float normalize code convert tensor image recommend write python script convert tensor convert back see,issue,negative,neutral,neutral,neutral,neutral,neutral
429035269,"Hi,
I was looking through the closed issues and found the ""manually rename to latest_net_G.pth"" advice, that helpled, thanks",hi looking closed found manually rename advice thanks,issue,negative,positive,neutral,neutral,positive,positive
428918290,"To clarify,  it is not that I am worried about the visualization, but I would like to be sure that the images are being represented correctly internally in the model ..... ",clarify worried visualization would like sure correctly internally model,issue,negative,positive,positive,positive,positive,positive
428730323,"@junyanz I changed the calling function as follows,
```
    def forward(self):
        combine=torch.cat((self.real_A, self.real_B), 1)
        self.fake_B, self.f1, self.f2, self.f3 = self.netG(combine)
```",calling function forward self combine,issue,negative,neutral,neutral,neutral,neutral,neutral
428637098,"The logic and structures of your code. How the code runs and what should we change if we want to process our own operations.
 For example, change 'data/aligned_dataset' to process your own preprocessing if your model reads aligned pairs of data, 'models/networks.py' aims at building networks and initialization of models. 'models/pix2pix_model.py' handles the main logical procedures.
It would be nice if it is detailedly illustrated by  a toy task.",logic code code change want process example change process model data building main logical would nice detailedly toy task,issue,negative,positive,positive,positive,positive,positive
428609093,"I think you also need to change the code when calling the function. 
out, out1, out2, out3 = self.netG()...",think also need change code calling function,issue,negative,neutral,neutral,neutral,neutral,neutral
428608187,Thanks for the great suggestions. We plan to add more detailed comments as well as toy model and dataset classes. Still working on it.  I also wonder which kinds of information will be helpful for you and your friends.  ,thanks great plan add detailed well toy model class still working also wonder information helpful,issue,positive,positive,positive,positive,positive,positive
428414353,"@junyanz but I got the following error:
>  File ""/home/Special/h_106/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [512, 256, 4, 4], expected input[4, 6, 256, 256] to have 256 channels, but got 6 channels instead

I have set `input_nc=6`, `output_nc=3` and `batch_size=4`.
Any suggestion?


",got following error file line forward given weight size input got instead set suggestion,issue,negative,neutral,neutral,neutral,neutral,neutral
428409774,you need to set --netG option if you are not using the default netG,need set option default,issue,negative,neutral,neutral,neutral,neutral,neutral
428329693,"
![image](https://user-images.githubusercontent.com/5948971/46694892-9dc0db00-cbd3-11e8-9c3c-bcaac384c8a8.png)


@junyanz is this the right way? Thanks a lot.
",image right way thanks lot,issue,negative,positive,positive,positive,positive,positive
428229167,It has been added. You can use `--model test`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) for more details.,added use model test see,issue,negative,neutral,neutral,neutral,neutral,neutral
428016509,You can modify the return values of the forward [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L253). This [post](https://discuss.pytorch.org/t/error-while-returning-multiple-variable-from-forward-function/8983) is related. ,modify return forward function post related,issue,negative,neutral,neutral,neutral,neutral,neutral
427949109,Cool. I am glad that you figured it out. It seems that you are using an old version. There is no tensor argument in the current [version](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L117). ,cool glad figured old version tensor argument current version,issue,negative,positive,positive,positive,positive,positive
427750296,"Hello,

I have found where the error was coming from.

class GANLoss(nn.Module):
    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,
                 tensor=torch.FloatTensor (I replaced it by torch.cuda.FloatTensor)):",hello found error coming class self,issue,negative,neutral,neutral,neutral,neutral,neutral
427741007,"  File ""/home/{}/.pyenv/versions/3.5.0/envs/Net_Trim/lib/python3.5/site-packages/torch/nn/functional.py"", line 1788, in mse_loss
    return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss, input, target, reduction)
  File ""/home/{}/.pyenv/versions/3.5.0/envs/Net_Trim/lib/python3.5/site-packages/torch/nn/functional.py"", line 1750, in _pointwise_loss
    return lambd_optimized(input, target, _Reduction.get_enum(reduction))
RuntimeError: Expected object of backend CUDA but got backend CPU for argument #2 'target'",file line return lambda input target reduction file line return input target reduction object got argument,issue,negative,neutral,neutral,neutral,neutral,neutral
427736134,"Python 3.5.0

>>> import torch
>>> print(torch.__version__)
0.5.0a0+0a8c8c1

But it works fine on CPU. It just takes ages ahahah :)",python import torch print work fine,issue,negative,positive,positive,positive,positive,positive
427448429,Are you using the latest code? Which pytorch version are you using? display_winsize should have been defined in the base_options.py,latest code version defined,issue,negative,positive,positive,positive,positive,positive
426483102,"I think loading D is not so important, as you can train D given a G, not vice versa. ",think loading important train given vice,issue,negative,positive,positive,positive,positive,positive
426477360,"@junyanz Yes, that is the purpose. For instance, one dataset has a large number of pairs and another dataset has a small number of pairs. In this case, how can we fine-tune the model trained on a large dataset to get good results for the small dataset?",yes purpose instance one large number another small number case model trained large get good small,issue,positive,positive,positive,positive,positive,positive
426473049,"What is the purpose of fine-tuning? Fine-tuning on a different dataset? I think the released models are already well-trained on the original dataset, and fine-tuning might not help.",purpose different think already original might help,issue,positive,positive,positive,positive,positive,positive
426430711,I agree. Let's set it to false. @HectorAnadon thanks for pointing this out! We really appreciate it. ,agree let set false thanks pointing really appreciate,issue,positive,negative,neutral,neutral,negative,negative
426397056,"> The current CycleGAN fine-tuning assumes that you have G_A, G_B, D_A, and D_B. Unfortunately, we currently only provide the generators as they can be used during test time. One quick hack that you can do is to only load the weights for G_A and G_B, but train D_A and D_B from scratch. We haven't tried that by ourselves and not sure about the results. You probably can do it by removing `D_A` and `D_B` from this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L39).

@junyanz Regarding the fine-tuning, I have loaded the G and retrain the D. However, the performance does not make much difference. The starting learning rate is the same. I also tried to load both G and D and retrained them, no big difference as well. Any suggestions? Thanks.",current unfortunately currently provide used test time one quick hack load train scratch tried sure probably removing line regarding loaded retrain however performance make much difference starting learning rate also tried load big difference well thanks,issue,positive,positive,positive,positive,positive,positive
426362020,This is a good point. This is a new flag that we haven't tested. Maybe we should set the default value as Fasle so that we can match the original code. @SsnL @taesung89 ,good point new flag tested maybe set default value match original code,issue,positive,positive,positive,positive,positive,positive
426332084,"I'm also curious about this. When I visit this repository a year ago, there was no `track_running_stats` in PyTorch. Does `track_running_stats` affect the performance?",also curious visit repository year ago affect performance,issue,negative,negative,neutral,neutral,negative,negative
426163470,@junyanz Thanks man for your hard work!,thanks man hard work,issue,negative,negative,neutral,neutral,negative,negative
426130987,"Yes, `-1`. If you want the visualization, you should start the visdom server. Otherwise, you can set `--display_id 0`.",yes want visualization start server otherwise set,issue,negative,neutral,neutral,neutral,neutral,neutral
426074356,"@junyanz you mean `gpu_ids -1`?
also before training we should start the vserver? `python -m visdom.server`",mean also training start python,issue,negative,negative,negative,negative,negative,negative
425404082,"It's just some coding trick. The difference is that (1) we can update D_A and D_B independently and we can write a `backward_D_basic` and reuse it to make the code more compact. (2) we have to update G_A and G_B at the same time due to the cycle consistency loss.

",trick difference update independently write reuse make code compact update time due cycle consistency loss,issue,negative,negative,neutral,neutral,negative,negative
425402886,"detach() stops the gradient. fake.detach() will make sure that G does not get gradients. Real does not need to be detached as real is a constant, not a variable. ",detach gradient make sure get real need detached real constant variable,issue,negative,positive,positive,positive,positive,positive
424973247,"@maxdel 

For your second question, I feel that setting 'requires_grad' to False for Ds is done for speed, not correctness. Here is a link of a similar question that may be helpful ([https://github.com/pytorch/examples/issues/116](url)).

And for the first question, my guess is YES, but I am just wondering is there any particular reason they do not write the code in this way.",second question feel setting false done speed correctness link similar question may helpful first question guess yes wondering particular reason write code way,issue,positive,positive,neutral,neutral,positive,positive
424961504,"I would like to add to the question above:
1) Can we thus also simply do more like this: 
```
self.loss_D = self.loss_D_A + self.loss_D_B
self.loss_D.backward()
```
2) Why do we set requires_grad to False for Ds (line below) if we use a separate optimizers for Ds and G s anyway?
https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/fdc7fcd1421ce386c57c6bdb9db09fcb0d22221a/models/cycle_gan_model.py#L140

Thanks in advance!",would like add question thus also simply like set false line use separate anyway thanks advance,issue,positive,negative,neutral,neutral,negative,negative
424357840,"I wonder if you've solved this issue. 
I met the same problem when testing a cyclegan model, but it didn't happen in my test of pix2pix model  before.",wonder issue met problem testing model happen test model,issue,negative,neutral,neutral,neutral,neutral,neutral
423377751,"The PyTorch model does not work well for this particular site as it looks quite different from training sites. It works reasonably well for other test sites. For example, this one. 
![example](https://user-images.githubusercontent.com/1924757/45853811-4d153b00-bd15-11e8-8d73-447c75b056e3.png)

The model is trained with only 90 sites. I think adding more sites might help improve the results. 

Also, the PyTorch was recently trained with the current PyTorch code. The results might be different from the Torch model. Please use the Torch model if you would like to reproduce the results in the paper. 
",model work well particular site quite different training work reasonably well test example one example model trained think might help improve also recently trained current code might different torch model please use torch model would like reproduce paper,issue,positive,positive,neutral,neutral,positive,positive
423185104,"`train.py` is a python script, not a jupyter notebook.  You probably have to modify the code. For example, removing `if __name__ == '__main__`.",python script notebook probably modify code example removing,issue,negative,neutral,neutral,neutral,neutral,neutral
423004578,"Thanks for fixing it =)

> I just fixed it. Thanks.

",thanks fixing fixed thanks,issue,positive,positive,positive,positive,positive,positive
422881660,See  [Traing/test tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#trainingtesting-with-high-res-images)  and this [Q & A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#out-of-memory-174) for how to reduce memory when training with high-res images.  ,see reduce memory training,issue,negative,neutral,neutral,neutral,neutral,neutral
422665130,"> Would you be able to get good results with the original parameters in the paper? (loadSize=286, fineSize=256, netG=resnet_9blocks, netD=default)

As I'm working on loadSize and fineSize 640，due to limiation of GPU mem of 1080ti, I can use no more than 7 residual block😂",would able get good original paper working mem ti use residual block,issue,negative,positive,positive,positive,positive,positive
422658308,"Well, I found them in sec 5.2 now, sorry for spam.",well found sec sorry,issue,negative,negative,negative,negative,negative,negative
422634168,Glad that you like the repo. :) Have fun with it. I'm going to close this one for now. Feel free to reopen if you have further questions.,glad like fun going close one feel free reopen,issue,positive,positive,positive,positive,positive,positive
422633657,"> Maybe you can run the code with multiple GPUs and see how it works.

I have done map and horse experiments on multiple gpus, because during the experiment, I feel that your code is very elegant, so I decided to intensively read your code line by line. Thank you for writing such a good code.",maybe run code multiple see work done map horse multiple experiment feel code elegant decided intensively read code line line thank writing good code,issue,positive,positive,positive,positive,positive,positive
422633321,"> It just setting the default cuda device to gpu_ids[0], e.g., if you crest a cuda tensor without specifying device it will be on that device. Nothing prevents using other devices later. We use DataParallel with **all** devices.
> […](#)
> On Mon, Sep 17, 2018 at 03:48 sunshine ***@***.***> wrote: No it's not wrong. What's the problem? But in this code,it seems that I just used the 0th gpu? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#387 (comment)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/387#issuecomment-421917812)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFaWZfZbcDaRnF4dgRPBaXQjrFA1qTuLks5ub1O7gaJpZM4WrNwi> .

ok, I think I should get it.Thank you very much about this wonderful code!",setting default device crest tensor without device device nothing later use mon sunshine wrote wrong problem code used th reply directly view comment mute thread think get much wonderful code,issue,negative,positive,positive,positive,positive,positive
422616590,"@junyanz Thanks very much, @taesung89 yes, I modified the source code earlier, and it performs well right now. Thanks!",thanks much yes source code well right thanks,issue,positive,positive,positive,positive,positive,positive
422433798,"It should stop at 200, as niter=200, niter_decay=200, and you start from epoch_count=200. If you want to train it longer, you can increase niter and/or niter_decay . See more details about these flags [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L20).",stop start want train longer increase niter see,issue,negative,neutral,neutral,neutral,neutral,neutral
422376878,"Hi,
I tried to continue training like that:
`-dataroot ./datasets/opaGan --name opa_cyclegan --model cycle_gan --epoch_count 200 --continue_train` but it just did the 200th epoch and then stopped. Is this the max training size or can I go up to epoch 400?",hi tried continue training like name model th epoch stopped training size go epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
422260300,It is not related to the visdom. It might be caused by data augmentation. I am not 100% sure. But your image sizes might be smaller than the fineSize (cropping size). ,related might data augmentation sure image size might smaller size,issue,negative,positive,positive,positive,positive,positive
422259802,"Would you be able to get good results with the original parameters in the paper? (loadSize=286, fineSize=256, netG=resnet_9blocks, netD=default)",would able get good original paper,issue,positive,positive,positive,positive,positive,positive
422251798,"@junyanz  Hi, I'm also getting this issue, using ngf=64 with 7 residual block, udf=64 with 2 layers,  I keep lr for the initial 20 epochs and decay lr to 0 in the following 20 epochs. After about 25 epochs, the result seems good, but after I trained all the 40 epochs, the result is a little blurred.
Could this be caused by overfitting? And can dropout help it?",hi also getting issue residual block keep initial decay following result good trained result little blurred could dropout help,issue,negative,positive,positive,positive,positive,positive
422185823,Yes that would be a better explanation! And thanks for your response to this.,yes would better explanation thanks response,issue,positive,positive,positive,positive,positive,positive
422095046,I am not sure. But It might be related to padding. Maybe you want to try different kinds of padding [methods](https://pytorch.org/docs/stable/_modules/torch/nn/modules/padding.html). ,sure might related padding maybe want try different padding,issue,negative,positive,positive,positive,positive,positive
422090439,Maybe you can run the code with multiple GPUs and see how it works. ,maybe run code multiple see work,issue,negative,neutral,neutral,neutral,neutral,neutral
422047951,"It just setting the default cuda device to gpu_ids[0], e.g., if you crest a
cuda tensor without specifying device it will be on that device. Nothing
prevents using other devices later. We use DataParallel with **all**
devices.

On Mon, Sep 17, 2018 at 03:48 sunshine <notifications@github.com> wrote:

> No it's not wrong. What's the problem?
>
> But in this code,it seems that I just used the 0th gpu?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/387#issuecomment-421917812>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFaWZfZbcDaRnF4dgRPBaXQjrFA1qTuLks5ub1O7gaJpZM4WrNwi>
> .
>
",setting default device crest tensor without device device nothing later use mon sunshine wrote wrong problem code used th reply directly view mute thread,issue,negative,negative,negative,negative,negative,negative
421917812,"> No it's not wrong. What's the problem?

But in this code,it seems that I just used the 0th gpu?",wrong problem code used th,issue,negative,negative,negative,negative,negative,negative
421886804,"The plot looks normal to me. The loss curve doesn't usually reveal much information, so I cannot tell whether the training was successful or not just based on the loss curve. ",plot normal loss curve usually reveal much information tell whether training successful based loss curve,issue,negative,positive,positive,positive,positive,positive
421803868,"That's a good point! Batchnorm does have this property. So to be precise we should say the PatchGAN architecture is equivalent to chopping up the image into 70x70 patches, making a big batch out of these patches, and running a discriminator on each patch, with batchnorm applied across the batch, then averaging the results.",good point property precise say architecture equivalent chopping image making big batch running discriminator patch applied across batch,issue,negative,positive,positive,positive,positive,positive
421660748,"- GPU: We can train the CycleGAN on a GPU (e.g., GTX 1080) with 8GB memory. I haven't tested the CPU training performance. 
- the number of epochs (training details): See this [Q & A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#experiment-details-eg-bw-color-306).
- loss function: see this [Q & A.](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#why-does-my-training-loss-not-converge-335-164-30) ",train memory tested training performance number training see loss function see,issue,negative,neutral,neutral,neutral,neutral,neutral
421530918,"Hi @phillipi @junyanz , 
I understood how patch sizes are calculated implicitly by tracing back the receptive field sizes of successive convolutional layers. But don't you think batch normalization sort of harms the overall idea of patch-gan discriminator? I mean theoretically each member X_ij of the final NxN output should just be dependent on some 70x70 patch in the original image. And that any changes beyond that 70x70 patch should not result in change in the value of X_ij. But if we use batch normalization then that won't necessarily be true right?",hi understood patch size calculated implicitly tracing back receptive field size successive convolutional think batch normalization sort overall idea discriminator mean theoretically member final output dependent patch original image beyond patch result change value use batch normalization wo necessarily true right,issue,positive,positive,neutral,neutral,positive,positive
421480489,"For the eval mode, I added a [flag](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/test_options.py#L12) that allows you to use eval mode. In the original pix2pix paper (@phillipi ), we don't use eval mode during the test, as we often use batchSize=1, and we would like to get per-image statistics. We often get better results without eval mode. Here is a comparison of label-> facades with and without eval mode. 

No eval mode
![no_eval](https://user-images.githubusercontent.com/1924757/45574442-0c618180-b83e-11e8-837a-85ed7cebd655.png)

Eval mode
![eval](https://user-images.githubusercontent.com/1924757/45574448-11becc00-b83e-11e8-98d6-ba877ff15696.png)


But in your case, you have a big batchSize during training (32) and you use a small batchSize(=1) during test (Note: we hard-coded the batchSize=1 in our test [code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/test.py). I will try to relax it later). 
In general, I will recommend that users use instance norm for both pix2pix and CycleGAN, which guarantees training/test behavior and also get per-image statistics.",mode added flag use mode original paper use mode test often use would like get statistic often get better without mode comparison without mode mode mode case big training use small test note test code try relax later general recommend use instance norm behavior also get statistic,issue,positive,positive,positive,positive,positive,positive
421477316,"Got this to run in Windows properly now!

I changed the directories in image_folder.py, and downgraded the version of visdom.

Thanks again ! :)",got run properly version thanks,issue,negative,positive,neutral,neutral,positive,positive
421453785,There is random flipping in the current data augmentation. You can add `no_flip`. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L40) for more details.,random current data augmentation add see,issue,negative,negative,negative,negative,negative,negative
421449928,"@junyanz @SsnL As I posted before,
![image](https://user-images.githubusercontent.com/5948971/45568889-87696e80-b823-11e8-8d05-8fba93c73178.png)
Why the fake_B and the real_B flipped left and right?",posted image left right,issue,negative,positive,positive,positive,positive,positive
421446925,@SsnL Thank you so much and I got quite better results now!,thank much got quite better,issue,positive,positive,positive,positive,positive,positive
421441885,"@happsky @mhusseinsh This should be fixed in https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/7dfdd06d8f7ca41735c06ea67ffbebd222a4d65e ! Because the pix2pix model uses batch norm, if we don't set it to eval mode, the running stats are not used, and result will look quite bad because batch size is 1 in test time. Sorry about it. Could you try to pull the repo and test again? There is no need to re-train. Just running test.py again should be fine.",fixed model batch norm set mode running used result look quite bad batch size test time sorry could try pull test need running fine,issue,negative,negative,negative,negative,negative,negative
421423735,"To avoid overfitting: (1) increase training set (2) dropout? (3) more data argumentation. 
We haven't used a big batchSize before. We use to use batchSize=1. 
You can try to make the model as eval mode. Call this [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/base_model.py#L47) in the test code. ",avoid increase training set dropout data argumentation used big use use try make model mode call function test code,issue,negative,neutral,neutral,neutral,neutral,neutral
421423062,You can increase the [num_threads](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L33). You can store your data on an SSD drive.,increase store data drive,issue,negative,neutral,neutral,neutral,neutral,neutral
421404802,"@junyanz I got the similar bad results after setting loadSize=286 during test time. For avoiding overfitting during training time, do you have any suggestions?",got similar bad setting test time training time,issue,negative,negative,negative,negative,negative,negative
421393498,"Great thanks a lot. I will give it a shot. One last question, do I need to have same number of examples in my trainA and trainB sets? 

",great thanks lot give shot one last question need number,issue,positive,positive,positive,positive,positive,positive
421289585,"I will ask a ""stupid"" question, but how do you improve the IO operate on a framework such as pytorch. ",ask stupid question improve io operate framework,issue,negative,negative,negative,negative,negative,negative
421248950,@happsky  maybe you also want to set the loadSize=286 during test time. I also think that there is severe overfitting during training. This is an ill-posed problem. But you training set results look identical to the ground truth images.,maybe also want set test time also think severe training problem training set look identical ground truth,issue,negative,neutral,neutral,neutral,neutral,neutral
421247683,"The current CycleGAN fine-tuning assumes that you have G_A, G_B, D_A, and D_B. Unfortunately, we currently only provide the generators as they can be used during test time. One quick hack that you can do is to only load the weights for G_A and G_B, but train D_A and D_B from scratch. We haven't tried that by ourselves and not sure about the results. You probably can do it by removing `D_A` and `D_B` from this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L39).",current unfortunately currently provide used test time one quick hack load train scratch tried sure probably removing line,issue,negative,positive,positive,positive,positive,positive
421244696,"I think you can leave `self.set_requires_grad(self.netD, False)` outside the for loop.",think leave false outside loop,issue,negative,negative,negative,negative,negative,negative
421211423,"thanks for your reply. I have another question should I set the `retain_graph=True` in the backward() when I want to train G more times than training D?  I found that when I just set as follows in `pix2pix_model.py`  (it can not work):
`    def optimize_parameters(self):


        self.forward()
        # update D
        self.set_requires_grad(self.netD, True)
        self.optimizer_D.zero_grad()
        self.backward_D()
        self.optimizer_D.step()
        for i in range(20):
            # update G
            self.set_requires_grad(self.netD, False)
            self.optimizer_G.zero_grad()
            self.backward_G()
            self.optimizer_G.step()`",thanks reply another question set backward want train time training found set work self update true range update false,issue,positive,positive,neutral,neutral,positive,positive
421121367,"@junyanz @taesung89 @SsnL I got the same problem when I trian pix2pix on cross-view image translation task.
During training time, the results are quite good as I wanted,
![image](https://user-images.githubusercontent.com/5948971/45509752-06946f00-b75e-11e8-9b57-85a8ce5d231b.png)
However, when I use the same images for testing and I got very bad results,
![image](https://user-images.githubusercontent.com/5948971/45509804-275cc480-b75e-11e8-9ee3-e89468c0e480.png)


My commads are:
python train.py --dataroot ./data --name setting_1 --model pix2pix --which_model_netG unet_256 --which_direction AtoB --dataset_mode aligned --norm batch --pool_size 50 --gpu_ids 0 --batch 32 --loadSize 286 --fineSize 256;


python test.py --dataroot ./data  --name setting_1 --model pix2pix --which_model_netG unet_256 --which_direction AtoB --dataset_mode aligned --norm batch --gpu_ids 0 --batchSize 32 --loadSize 256 --fineSize 256;

Any suggestion.

",got problem image translation task training time quite good image however use testing got bad image python name model norm batch batch python name model norm batch suggestion,issue,negative,positive,neutral,neutral,positive,positive
421120629,"Yes. 

We have datasets in only one direction, because the opposite direction can be simply used by using `--which_direction BtoA` in training and testing. 

For the pretrained model, there's still a subtle difference between the Torch and PyTorch implementation of CycleGAN, although it's hard to say which one is better. If you'd like results that are exactly same as the ones appearing in the paper, please use our Torch implementation. ",yes one direction opposite direction simply used training testing model still subtle difference torch implementation although hard say one better like exactly paper please use torch implementation,issue,positive,positive,neutral,neutral,positive,positive
421094560,I fixed it by replacing the opt.display_winsize into 256. the reason is that the display_winsize is part of the train options but  it is not part of test options. so the script cannot find it. The default value in train options is 256.,fixed reason part train part test script find default value train,issue,negative,positive,neutral,neutral,positive,positive
421086791,"not sure I m doing the right thing with git :-/ well, this pull request adds resnet_9blocks+ and resnet_6blocks+ models so that one can try the ""resize-convolution instead transposed-convolution to avoid checkerboard artifacts #64""

python train.py --dataroot ./datasets/onoff/ --name onoff --model cycle_gan --display_env onoff-dev --netG **resnet_6blocks+** 

",sure right thing git well pull request one try instead avoid checkerboard python name model,issue,negative,positive,positive,positive,positive,positive
421048672,It seems that you haven't downloaded the dataset. Please run `bash ./datasets/download_cyclegan_dataset.sh maps` to get the dataset. Please ask the question in English so that others can also answer it.,please run bash get please ask question also answer,issue,positive,neutral,neutral,neutral,neutral,neutral
420953219,"Okay thanks for looking into it 👍 

Maybe I should try docker.",thanks looking maybe try docker,issue,negative,positive,positive,positive,positive,positive
420841703,I don't know the solution. The command `python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan` works for me on my Linux. The code has not been tested on Windows. ,know solution command python name model work code tested,issue,negative,neutral,neutral,neutral,neutral,neutral
420841421,"I think you are using Python 2, which `ConnectionError` is not defined. Could you either run the visdom server or set `display_id -1`?  See the [Q & A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#connection-errorhttpconnectionpool-230-24-38) for more details.",think python defined could either run server set see,issue,negative,neutral,neutral,neutral,neutral,neutral
420713061,"I tried adding `--loadSize 143 --fineSize 128`

and I still get  the same error;

```
    index_B = random.randint(0, self.B_size - 1)
  File ""C:\Users\tyk12\Anaconda3\lib\random.py"", line 221, in randint
    return self.randrange(a, b+1)
  File ""C:\Users\tyk12\Anaconda3\lib\random.py"", line 199, in randrange
    raise ValueError(""empty range for randrange() (%d,%d, %d)"" % (istart, istop, width))
ValueError: empty range for randrange() (0,0, 0)
```

Thank you for helping me along the way :)
I got to learn a lot.
I think I am almost there..!",tried still get error file line return file line raise empty range width empty range thank helping along way got learn lot think almost,issue,negative,negative,neutral,neutral,negative,negative
420710099,"Oooooh... I never thought about having 2 anaconda prompt windows open!
one for Visdom and one for training... Sorry and thanks!

Now I ran into a new error;
`ValueError: empty range for randrange() (0,0, 0)`

I am referring to issue #194 ",never thought anaconda prompt open one one training sorry thanks ran new error empty range issue,issue,negative,negative,neutral,neutral,negative,negative
420694547,"Also, I tested your winter2summer pretrained model for some random images, it seems that it works nicely only if there is snow in winter. when I dont have snow, then it produces bluish color instead of green to change the scene. I glanced your dataset and it seems to be very diverse so I was expecting a result similar to what you have posted on the sample images. I am going to finetune your model on a more diverse data that I have but is there anything that I am missing before i start doing it ? ",also tested model random work nicely snow winter dont snow bluish color instead green change scene diverse result similar posted sample going model diverse data anything missing start,issue,negative,negative,neutral,neutral,negative,negative
420673382,"Thank you ! After sorting out the file directory like you mentioned, it moved on :)

The training process started, but then I ran into 4 exceptions and 1 connection error -

1. `ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it`

2. `urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001B7122DDDD8>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it`

3. `urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B7122DDDD8>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))`

4. ` File ""C:\Users\tyk12\Anaconda3\lib\site-packages\requests\adapters.py"", line 508, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B7122DDDD8>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it',))`

5. `ConnectionError: Error connecting to Visdom server`

When I trigger `visdom` command, the Visdom browser window pops up fine.

I am going through Q&A, and google-searching  to solve the issues one by one :)",thank file directory like training process ran connection error connection could made target machine actively object establish new connection connection could made target machine actively object establish new connection connection could made target machine actively file line send raise object establish new connection connection could made target machine actively error server trigger command browser window fine going solve one one,issue,positive,positive,neutral,neutral,positive,positive
420636989,"> See these two posts [1](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/322) and [2](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/362).

I generally understand your thoughts, thank you very much!",see two generally understand thank much,issue,negative,positive,positive,positive,positive,positive
420567514,"so ,like this...what should do to solve this trouble?




------------------ 原始邮件 ------------------
发件人: ""Jun-Yan Zhu""<notifications@github.com>;
发送时间: 2018年9月12日(星期三) 上午6:20
收件人: ""junyanz/pytorch-CycleGAN-and-pix2pix""<pytorch-CycleGAN-and-pix2pix@noreply.github.com>;
抄送: ""向元吉""<739203572@qq.com>; ""Comment""<comment@noreply.github.com>; 
主题: Re: [junyanz/pytorch-CycleGAN-and-pix2pix] Error while downloadingday2night model  (#374)




What is your error?
 
—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub, or mute the thread.",like solve trouble comment comment error model error reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
420445562,Two possible reasons: (1) Maybe you haven't placed the dataset in the path ` ./datasets/maps/trainA`. (2) I haven't used Windows for a long time. Maybe you want to replace the `/` by `\` in the path.,two possible maybe path used long time maybe want replace path,issue,negative,negative,neutral,neutral,negative,negative
420243868,"I made a small mistake, and now I could load the model by using load_network function. And my pytorch version is 4.0. Thanks for your help.",made small mistake could load model function version thanks help,issue,negative,negative,neutral,neutral,negative,negative
420104624,"`net.load_state_dict(state_dict)` returns None, as expected. We are not using the return value anywhere. From the trace you posted, it seems that your `self.netD_A` or `self.netD_B` is None, unless you are not posting the full trace.",none return value anywhere trace posted none unless posting full trace,issue,negative,positive,positive,positive,positive,positive
419880781,Could you check out the latest commit? We moved the models to a new place.,could check latest commit new place,issue,negative,positive,positive,positive,positive,positive
419773330,"Hi, I got error during downloading this model:

--2018-09-09 18:32:50--  https://people.eecs.berkeley.edu/~taesung_park/pytorch-CycleGAN-and-pix2pix/pix2pix_models/day2night.pth
Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.189.73
Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.189.73|:443... connected.
HTTP request sent, awaiting response... 404 Not Found
2018-09-09 18:32:51 ERROR 404: Not Found.
",hi got error model connected request sent response found error found,issue,negative,neutral,neutral,neutral,neutral,neutral
419733696,"This [post](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/322) should help you understand the intuition of identity loss. 
Sorry for the naming confusion. It might be better to look at the code. 
```python 
self.fake_B = self.netG_A(self.real_A)
self.rec_A = self.netG_B(self.fake_B)
self.fake_A = self.netG_B(self.real_B)
self.rec_B = self.netG_A(self.fake_A)
self.idt_A = self.netG_A(self.real_B)
self.idt_B = self.netG_B(self.real_A)
```",post help understand intuition identity loss sorry naming confusion might better look code python,issue,negative,neutral,neutral,neutral,neutral,neutral
419727215,"Interesting. I am able to load the D networks by adding a few lines (by simply (1) define D, and (2) add D to the self.model_names). Could you check out the latest commit and tell us which PyTorch version you are using. ",interesting able load simply define add could check latest commit tell u version,issue,positive,positive,positive,positive,positive,positive
419695217,"yes, I have defined D .When I loaded the model from checkpoint, I got above error.
It seems that **net.load_state_dict(state_dict)** doesn't work.",yes defined loaded model got error work,issue,negative,neutral,neutral,neutral,neutral,neutral
419684098,"You need to define the D before you load it from the disk. I am wondering if you have defined D. 
```python 
self.netD_A = networks.define_D(opt.output_nc, opt.ndf, opt.netD,
                                            opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)
self.netD_B = networks.define_D(opt.input_nc, opt.ndf, opt.netD,
                                            opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)
```",need define load disk wondering defined python,issue,negative,neutral,neutral,neutral,neutral,neutral
419654396,"From the debugging information, it seems that the network `D_A` and `D_B` has been loaded. And the code failed when you tried to load them the second time. Maybe you want to check out which code produces the first two lines. 
```
loading the model from ./checkpoints\try_cyclegan\100_net_D_A.pth
loading the model from ./checkpoints\try_cyclegan\100_net_D_B.pth
```",information network loaded code tried load second time maybe want check code first two loading model loading model,issue,negative,positive,positive,positive,positive,positive
419356250,"And forget to add, I would be happy to get correct interpretation of results from author of the paper in order continue to explore further GANs with correct foundation information from the author :) ",forget add would happy get correct interpretation author paper order continue explore correct foundation information author,issue,negative,positive,positive,positive,positive,positive
419355725,"![maps](https://user-images.githubusercontent.com/42893908/45204891-e86fd000-b280-11e8-97ed-e888181d15df.png)
Dear Jun-Yan Zhu, 

I am writing to ask why as shown in the attached image, idt_A looks like as real_B (which starts in 2nd row from left) and idt_B looks like as real_A? As far as I understood from paper,  idt is identity mapping loss function to preserve color of the input image, right? 

Here I formulated my interpretation of result figure: Could you please have look and let me know if I am in right direction or not. I am trying to understand it more than one week so far, but I have a lot of confusion, I tried to follow line by line in code, but then decided to ask author if I interpret final figure correctly: 

1st row from left to right: We give input **map** image which is real_A, and then second image is generated from Generator A trained on satellite images and produces A image but in B style (i.e. as satellite) and marked as fake_B. After, by difference of real_A and fake_B, we get reconstructed A, rec_A. And then we add identity mapping loss on on A image and as a result of it we transform our initial input map image into aerial (satellite) image??? I have big confusion here, the image idt_A does not look as A at all. 

2nd row from left to right: similar logic: we have input aerial image as real_B, and from Generator B trained on maps images, we produce aerial image but in maps style representation and mark it as fake_A, and goes reconstruction and again idt_B??? 

Could you please explain in details: 

1. Am I thinking in right direction by above interpretation of final figure? Especially when we get fake_B is it from Generator B trained on A images? Similarly   for fake_A, it is from Generator A trained on B images?

2. the reconstructed image: rec_A = realA - fake_B? Am I thinking correctly here?

3. I thought that  1st row from left to right = forward cycle and 2nd row = from left to right backward cycle. Is it true?

4. we start from 1st row  with map image and at the end of 1st row we get transformed aerial image, then from 2nd row we start from transformed aerial image and transform back into map image, correct? But then name labels for idt_A and idt_B is confusing, do not they be inverse?

I would be grateful if You can have time and answer to my questions in details because I am new in GANs. 

Thanks

Looking forward to hear from You soon 

Kind regards
Altynay  ",dear writing ask shown attached image like row left like far understood paper identity loss function preserve color input image right interpretation result figure could please look let know right direction trying understand one week far lot confusion tried follow line line code decided ask author interpret final figure correctly st row left right give input map image second image generator trained satellite image style satellite marked difference get reconstructed add identity loss image result transform initial input map image aerial satellite image big confusion image look row left right similar logic input aerial image generator trained produce aerial image style representation mark go reconstruction could please explain thinking right direction interpretation final figure especially get generator trained similarly generator trained reconstructed image thinking correctly thought st row left right forward cycle row left right backward cycle true start st row map image end st row get aerial image row start aerial image transform back map image correct name inverse would grateful time answer new thanks looking forward hear soon kind,issue,positive,positive,positive,positive,positive,positive
419341247,"Thanks for your reply. According to your instruction, I used the  load_networks function to load my network, but I got the following error:

#training images = 120
initialize network with normal
initialize network with normal
initialize network with normal
initialize network with normal
loading the model from ./checkpoints\try_cyclegan\100_net_D_A.pth
loading the model from ./checkpoints\try_cyclegan\100_net_D_B.pth
loading the model from ./checkpoints\try_cyclegan\100_net_D_A.pth
Traceback (most recent call last):
  File ""train.py"", line 17, in <module>
    model = create_model(opt)
  File ""F:\ylp\pytorch-CycleGAN-and-pix2pix\models\__init__.py"", line 37, in create_model
    instance.initialize(opt)
  File ""F:\ylp\pytorch-CycleGAN-and-pix2pix\models\cycle_gan_model.py"", line 64, in initialize
    self.netD_B = BaseModel.load_disnetworks(self,opt.which_epoch)
  File ""F:\ylp\pytorch-CycleGAN-and-pix2pix\models\base_model.py"", line 158, in load_disnetworks
    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))
  File ""F:\ylp\pytorch-CycleGAN-and-pix2pix\models\base_model.py"", line 115, in __patch_instance_norm_state_dict
    self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)
AttributeError: 'NoneType' object has no attribute 'model'

",thanks reply according instruction used function load network got following error training initialize network normal initialize network normal initialize network normal initialize network normal loading model loading model loading model recent call last file line module model opt file line opt file line initialize self file line net file line module key object attribute,issue,negative,positive,neutral,neutral,positive,positive
419265011,Maybe you can use the `load_networks` [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/base_model.py#L118) to load the network. You can add `D_A` to `self.model_names`. You can change this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L38). ,maybe use function load network add change line,issue,negative,neutral,neutral,neutral,neutral,neutral
419264235,CycleGAN can recover images from the generated images as it is trained with this learning objective. See this [paper](https://arxiv.org/abs/1712.02950) for more details. ,recover trained learning objective see paper,issue,negative,neutral,neutral,neutral,neutral,neutral
418754258,"This looks great. 
Do you have instructions/tutorial for that? 

I checked the website mentioned in the YouTube video description does not have introduction. 
",great checked video description introduction,issue,positive,positive,positive,positive,positive,positive
418597244,"@junyanz 

Yeah. I already install visdom and type ""python -m visdom.server"". It worked well but suddenly it doesn't work. I may need to install visdom again.",yeah already install type python worked well suddenly work may need install,issue,positive,neutral,neutral,neutral,neutral,neutral
418596992,"See this [Q & A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#connection-errorhttpconnectionpool-230-24-38) for more details. You need to install `visdom` and type `python -m visdom.server`. 
@taesung89  Adding `--display_id 0` should be able to perform training without visualization function. We will have a look at that. ",see need install type python able perform training without visualization function look,issue,negative,positive,positive,positive,positive,positive
418595635,"I have same problem now. If I use --display ""non-zero"" value, it shows 
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8889): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1ca74fb208>: Failed to establish a new connection: [Errno 111] Connection refused',)). Basically, visdom error. It seems visdom error is from bug.

So, I would set --display_id 0 up so that it turned off display. Then, no html and ""checkpoints"" folder and also no training is done. 


",problem use display value object establish new connection connection basically error error bug would set turned display folder also training done,issue,negative,positive,positive,positive,positive,positive
418580233,"Yeah, we added it in [Q & A](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md). Will add it in [training/testing tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md) soon.",yeah added add soon,issue,negative,neutral,neutral,neutral,neutral,neutral
418503849,"Thanks for this question.
I suggest to put some reminder in ReadMe in case people like us do not notice the multi-GPU and batch size problem.",thanks question suggest put reminder case people like u notice batch size problem,issue,negative,positive,positive,positive,positive,positive
418372724,"Add/remove a few layers or increase/decrease the number of features per layer. For example, you can modify the value of `--ngf` and `--ndf` first. See the [options](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L20) for more details.",number per layer example modify value first see,issue,negative,positive,positive,positive,positive,positive
418303872,@junyanz thanks for your reply. Would you mind telling me some specific tricks to deal with the problem? (e.g. how to increase the capacity of G or decrease the capacity of D?) I am freshman on GAN field. Thanks again for your kind reply and your nice project.,thanks reply would mind telling specific deal problem increase capacity decrease capacity freshman gan field thanks kind reply nice project,issue,positive,positive,positive,positive,positive,positive
418162497,Thank you very much for your answer. I will think about the way to overcome that. ,thank much answer think way overcome,issue,negative,positive,positive,positive,positive,positive
418155234,It seems that the D is too strong (or the G is too weak). D dominates the two-player game from the beginning. I will try to increase the capacity of G or decrease the capacity of D.,strong weak game beginning try increase capacity decrease capacity,issue,negative,negative,negative,negative,negative,negative
418155023,"Yes, 3D volumetric data is quite memory-intensive.",yes volumetric data quite,issue,negative,neutral,neutral,neutral,neutral,neutral
418153531,"Okay. I did load 3D volumetric data and change some part of resnetGenerator() but need to modify entire network. Due to out of memory problem, larger volume cannot be trained though.  ",load volumetric data change part need modify entire network due memory problem volume trained though,issue,negative,negative,neutral,neutral,negative,negative
418151755,The G is a fully convolutional network (FCN). It does not require the same image size for training and test. See the original FCN [paper](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) and [slides](http://www.micc.unifi.it/bagdanov/pdfs/FCN-presentation.pdf) for more details.,fully convolutional network require image size training test see original paper,issue,negative,positive,positive,positive,positive,positive
418151351,The current code does not support 3D input. You probably need to write your own data loader (as well as generator and discriminator). See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#how-can-i-make-it-work-for-my-own-data-eg-16-bit-png-tiff-hyperspectral-images-309--320-202) for how to write the data loader. ,current code support input probably need write data loader well generator discriminator see write data loader,issue,positive,neutral,neutral,neutral,neutral,neutral
418113823,@junyanz Can you answer me the question? thank you very much.,answer question thank much,issue,negative,positive,positive,positive,positive,positive
418005174,"Hello @taesung89,

I also have similar problem since not enough GPU memory for my 512 x 512 images. I did exact same ways you suggested for the training (loadSize = 512, fineSize = 256) and the test (loadSize = 512, fineSize = 512). My question is a little bit different from above. How does G work for larger test image (512 x 512) even though it was trained with small (in this case, 256 x 256). 

When testing, is there anyways of upsampling technique involved? I may miss some parts for now. For now, the results after G seems visually ok. 

Thank you in advance.",hello also similar problem since enough memory exact way training test question little bit different work test image even though trained small case testing anyways technique involved may miss visually thank advance,issue,negative,negative,neutral,neutral,negative,negative
417931361,The current code is correct as fake_B is generated from G(real_A). Sorry for the naming confusion. ,current code correct sorry naming confusion,issue,negative,negative,negative,negative,negative,negative
417826056,"Oh I see! Thank you for the feedback. 
I’m not so sure how I can build the support for CUDA - I will also try the same setup with python 3.7 on a linux environment using docker on a Windows laptop.",oh see thank feedback sure build support also try setup python environment docker,issue,positive,positive,positive,positive,positive,positive
417825750,"The PyTorch binary for osx does not support CUDA. So if you run with gpu_ids not -1, you will see such error. You can build from pytorch github source to support CUDA.",binary support run see error build source support,issue,negative,neutral,neutral,neutral,neutral,neutral
417817013,okay thanks! I will try the examples and come back :),thanks try come back,issue,negative,positive,neutral,neutral,positive,positive
417808892,"add `--gpu_ids -1` to your python command. 
I am not sure if your issue is caused by this repo. You may want to run some PyTorch official [examples](https://github.com/pytorch/examples) and see if your PyTorch is installed correctly. ",add python command sure issue may want run official see correctly,issue,negative,positive,positive,positive,positive,positive
417798488,"yes I have an NVIDIA GPU. 
I would like to use the GPU, since it is much faster..

I tried the CPU mode command anyway, but it won't work.
Am I meant to insert `--gpu_ids -1` line into  .profile ?

Thank you for answering my questions! :)

<img width=""476"" alt=""screen shot 2018-08-31 at 22 54 47"" src=""https://user-images.githubusercontent.com/16857323/44937522-9525eb00-ad71-11e8-9697-304da5a3ea84.png"">
",yes would like use since much faster tried mode command anyway wo work meant insert line thank screen shot,issue,positive,positive,positive,positive,positive,positive
417794131,"Hmm, do you have an NVIDIA GPU on your Mac? If not, you can set `--gpu_ids -1` to use CPU mode.",mac set use mode,issue,negative,neutral,neutral,neutral,neutral,neutral
417787491,yup I tried the latest versions (0.4+) first. Same error.,tried latest first error,issue,negative,positive,positive,positive,positive,positive
417542465,"@junyanz thanks for reply. And I use your pix2pix model to train my own datasets, I use resnet-9blocks generator, but I get poor result. and my G-loss always large than D-loss.  there is my screenshot of training log. **would you mind telling me some tricks for deal with this situation?** thanks a lot :)

`(epoch: 14, iters: 58, time: 0.069, data: 0.001) G_GAN: 7.849 G_L1: 11.972 D_real: 0.006 D_fake: 0.001
(epoch: 14, iters: 158, time: 0.114, data: 0.001) G_GAN: 12.429 G_L1: 10.823 D_real: 0.001 D_fake: 0.000
(epoch: 14, iters: 258, time: 0.084, data: 0.001) G_GAN: 13.875 G_L1: 19.624 D_real: 0.000 D_fake: 0.000
(epoch: 14, iters: 358, time: 0.114, data: 0.001) G_GAN: 9.193 G_L1: 13.014 D_real: 0.000 D_fake: 0.000
(epoch: 14, iters: 458, time: 0.126, data: 0.001) G_GAN: 14.895 G_L1: 16.324 D_real: 0.000 D_fake: 0.000
(epoch: 14, iters: 558, time: 0.091, data: 0.001) G_GAN: 12.620 G_L1: 18.035 D_real: 0.000 D_fake: 0.000
`
",thanks reply use model train use generator get poor result always large training log would mind telling deal situation thanks lot epoch time data epoch time data epoch time data epoch time data epoch time data epoch time data,issue,negative,positive,neutral,neutral,positive,positive
417541602,"Regarding the second issue about the attribute error, have you trained and tested using the same git revision? I am suspecting this may happen if you train with one version of our codebase, and train with a newer version....",regarding second issue attribute error trained tested git revision may happen train one version train version,issue,negative,neutral,neutral,neutral,neutral,neutral
417541192,"hmmmm..... are you using the latest code and the model? Could you git pull the latest code, and also download the latest pretrained model, and try it again? I think I vaguely remember our code had this kind of bug a while ago...",latest code model could git pull latest code also latest model try think vaguely remember code kind bug ago,issue,positive,positive,positive,positive,positive,positive
417505067,"For test, could you try `--resize_or_crop none --loadSize 720`? @taesung89 ",test could try none,issue,negative,neutral,neutral,neutral,neutral,neutral
417177364,"The current code does not support different training times for G and D.  But it should be straightforward to add. Regarding loss function, we support the normal GAN loss and [LSGAN](https://arxiv.org/abs/1611.04076) loss. ",current code support different training time straightforward add regarding loss function support normal gan loss loss,issue,negative,positive,positive,positive,positive,positive
416789650,"@junyanz  on question one, I mean how can I set times of training G and D alternately(e.g. I want to train G2 2 times then train D 1 time). I am looking forward your reply. Thanks a lot.",question one mean set time training alternately want train time train time looking forward reply thanks lot,issue,negative,negative,neutral,neutral,negative,negative
416789257,"@junyanz thanks for quick reply. And can I set the times of training G and D manually? if yes ,how can I set the parameters on your codabase? **And another question**  Are your Cycle GAN and Pix2pix model based on by WGAN or the normal GAN, I am a freshman, would you mind telling me whether the WGAN will be better?",thanks quick reply set time training manually yes set another question cycle gan model based normal gan freshman would mind telling whether better,issue,positive,positive,positive,positive,positive,positive
416310252,"The size of the dataset depends on your task. If your task is easy, a few hundreds of pairs are enough. But if your task is complicated, you may need more training data.  Also, the L1 loss may not be a good indicator of the quality. ",size task task easy enough task complicated may need training data also loss may good indicator quality,issue,positive,positive,positive,positive,positive,positive
416307298,"@junyanz thanks for the reply. I have tried with increasing the lambda, but with no observed benefits. The L1 loss still fluctuates and the synthesized results contain artifacts. I have used around 500 pairs of samples, comprising of 30 subjects. Do you think the number of subjects would be the problem? I have added some data augmentations in this case. ",thanks reply tried increasing lambda loss still contain used around think number would problem added data case,issue,negative,positive,positive,positive,positive,positive
416118497,You can probably increase the lambda for L1 loss if your main concern is about the convergence of L1 loss.,probably increase lambda loss main concern convergence loss,issue,negative,positive,positive,positive,positive,positive
416118382,I think you can use both (either one optimizer for two Ds or two optimizers for two Ds). It is not a big difference. One optimizer for two Ds can save a few lines of code. ,think use either one two two two big difference one two save code,issue,negative,neutral,neutral,neutral,neutral,neutral
415562907,It seems as though it was an issue with pixel density and image scaling. The results are now printing to visdom.,though issue density image scaling printing,issue,negative,neutral,neutral,neutral,neutral,neutral
415495349,"Wait. I got an error that time at epoch 195.

Traceback (most recent call last):
  File ""train.py"", line 55, in <module>
    model.save_networks(epoch)
  File ""/home/ubuntu/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 99, in save_networks
    torch.save(net.module.cpu().state_dict(), save_path)
  File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py"", line 161, in save
    return _with_file_like(f, ""wb"", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py"", line 118, in _with_file_like
    return body(f)
  File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py"", line 161, in <lambda>
    return _with_file_like(f, ""wb"", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File ""/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py"", line 238, in _save
    serialized_storages[key]._write_file(f, _is_real_file(f))
RuntimeError: Unknown error -1",wait got error time epoch recent call last file line module epoch file line file line save return lambda file line return body file line lambda return lambda file line key unknown error,issue,negative,negative,neutral,neutral,negative,negative
415494849,"I feel like there must be some simple data formatting step that I have missed. Nothing changed with those tags. This is the typical output I get while the program is running: 

End of epoch 165 / 200 	 Time Taken: 0 sec
learning rate = 0.0000693
End of epoch 166 / 200 	 Time Taken: 0 sec
learning rate = 0.0000673
End of epoch 167 / 200 	 Time Taken: 0 sec
learning rate = 0.0000653
End of epoch 168 / 200 	 Time Taken: 0 sec
learning rate = 0.0000634
End of epoch 169 / 200 	 Time Taken: 0 sec
learning rate = 0.0000614

The learning rate starts at 0.02 and decreases as the number of epochs gets near 200.",feel like must simple data step nothing typical output get program running end epoch time taken sec learning rate end epoch time taken sec learning rate end epoch time taken sec learning rate end epoch time taken sec learning rate end epoch time taken sec learning rate learning rate number near,issue,negative,negative,neutral,neutral,negative,negative
415247288,"Hi, Taesug,
I want to use one optimizer for four different discriminators in CycleGAN, and it occurs above error.
Why are you use one optimizer for two different discriminators rather than two different optimizers for two different discriminators?",hi want use one four different error use one two different rather two different two different,issue,negative,neutral,neutral,neutral,neutral,neutral
415223569,Could you try running it with `--display_freq 1 --print_freq 1` and see what happens? This should display the result and print the error at every iteration. ,could try running see display result print error every iteration,issue,negative,neutral,neutral,neutral,neutral,neutral
415223259,"Hi,

could you post the entire call stack? This can be resolved by specifying `retain_graph=True` in the call to `backward` of autograd. (https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward) However, I would like to know why this happened in your case. 

-Taesung",hi could post entire call stack resolved call backward however would like know case,issue,negative,neutral,neutral,neutral,neutral,neutral
414785333,@AllAwake It is not impossible that something in PyTorch changed that impacted. Please let me know if you found anything wrong in that regard.,impossible something impacted please let know found anything wrong regard,issue,negative,negative,negative,negative,negative,negative
414753318,You need to write your own data loader. See #320 for more details.,need write data loader see,issue,negative,neutral,neutral,neutral,neutral,neutral
414715328,The aspect ratio for 32*280 is too extreme. Maybe you would like to train a model on 32x32 patches or 32x64 patches. You can try `--resize_or_crop crop --fineSize 32`. See more details in the training/test [practice](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md).,aspect ratio extreme maybe would like train model try crop see practice,issue,negative,negative,negative,negative,negative,negative
414702541,"thank you guys, as long as there are no changes on your side, i'll continue to tweak my own code - basically i'm trying to use cycleGAN for super-resolution so upscaling result in the weird texture... ",thank long side continue tweak code basically trying use result weird texture,issue,negative,negative,negative,negative,negative,negative
414599846,"Thanks for your answer, my train dataset size is 20000 and my train script now is : python -u train.py --dataroot ./datasets/rgb2black/classAB/ --name rgb2black_pix2pix --model pix2pix --which_model_netG resnet_6blocks --display_id 0 --which_direction AtoB --continue_train --resize_or_crop none --no_flip. 
the size of 
![image](https://user-images.githubusercontent.com/19235054/44391215-d5ab8a80-a561-11e8-9902-6983c6742118.png)
is 32*280，how should I change the script?
@junyanz 
",thanks answer train size train script python name model none size image change script,issue,negative,positive,positive,positive,positive,positive
414580851,"I have solved this problem. Thanks a lot. The reason is that I open the HTML file in local IE. I tried to connect the server using jupyter notebook in local browser, and it worked. Another question is that I want to know how should I change the code to process this situation: training two inputs images to generate one output image?",problem thanks lot reason open file local ie tried connect server notebook local browser worked another question want know change code process situation training two generate one output image,issue,negative,positive,neutral,neutral,positive,positive
414572215,Cool. Sorry for the confusion. You are correct!,cool sorry confusion correct,issue,negative,negative,neutral,neutral,negative,negative
414560854,"@junyanz thanks for your kind reply, I currently got to understand how the iter=200 is calculated, the total iter=niter+niter_decay.  Before that, I thought they work separately.",thanks kind reply currently got understand calculated total thought work separately,issue,positive,positive,positive,positive,positive,positive
414555085,"From this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/visualizer.py#L50), `self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, 'web')`. If you set different names, the code should write HTML files to different directories. Could you do a test with different opt.name?",line set different code write different could test different,issue,negative,neutral,neutral,neutral,neutral,neutral
414554271,See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/87) on how to resume your previous training. ,see resume previous training,issue,negative,negative,negative,negative,negative,negative
414553651,"Interesting. Are there images in the `images` directory? Does the HTML visualization work for the provided datasets (e.g., facades)?",interesting directory visualization work provided,issue,negative,positive,positive,positive,positive,positive
414552768,"Also, see this [paper](https://arxiv.org/pdf/1808.04325.pdf) ""Improving Shape Deformation in Unsupervised Image-to-Image Translation"". ",also see paper improving shape deformation unsupervised translation,issue,negative,neutral,neutral,neutral,neutral,neutral
414552544,Some recent work like [RecycleGAN](https://arxiv.org/abs/1808.05174) can actually produce big geometric changes. Maybe you want to check it out. ,recent work like actually produce big geometric maybe want check,issue,negative,neutral,neutral,neutral,neutral,neutral
414552309,"Not sure about that. I think we only updated the PyTorch 0.4 syntax. 
Maybe @SsnL can have a look.",sure think syntax maybe look,issue,negative,positive,positive,positive,positive,positive
414552218,I am not sure if this is related to this repo. Would you be able to run some PyTorch [examples](https://github.com/pytorch/examples) on your machine? ,sure related would able run machine,issue,negative,positive,positive,positive,positive,positive
414550870,"We don't own the edges2cats model. The model and data belong to Chris, who developed the [pix2pix-tensorflow ](https://github.com/affinelayer/pix2pix-tensorflow). You can leave an issue there or email him. ",model model data belong leave issue,issue,negative,neutral,neutral,neutral,neutral,neutral
414550540,"If you want to get the output of a particular layer, you can modify the `forward` function of the generator and also ask it to return that layer. ",want get output particular layer modify forward function generator also ask return layer,issue,negative,positive,positive,positive,positive,positive
414550288,"It looks normal to me. The training often takes a few days. In your case, you may not need to train the model for 200 epochs. Sometimes, 50 will be enough if you have 15k images. You may want to check if your GPU usage is 100%. If not, you may want to improve the IO.",normal training often day case may need train model sometimes enough may want check usage may want improve io,issue,positive,positive,neutral,neutral,positive,positive
414549016,"Yes, people typically convert an RGB image to Lab. Many libraries can do the job for you with one line of code.  Once you have a Lab image, you can train a network to predict ab from L. ",yes people typically convert image lab many job one line code lab image train network predict,issue,negative,positive,positive,positive,positive,positive
414548708,Strided-convolution will increase the spatial dimension. Convolution will decrease the dimension. ResNet block will keep the dimension. See this [post](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d) and ResNet paper for more details.,increase spatial dimension convolution decrease dimension block keep dimension see post paper,issue,negative,neutral,neutral,neutral,neutral,neutral
414548335,"Sorry. I was traveling for the past two weeks. You can produce results with the test script on the training images, and see if the results are the same as your ""saved"" training image results. 

There is often a gap between training and test due to overfitting. So it's quite common that the test images look worse compared to the training images. But to make sure that your test script is correct, you can do a sanity check using training images as described above.",sorry traveling past two produce test script training see saved training image often gap training test due quite common test look worse training make sure test script correct sanity check training,issue,negative,negative,negative,negative,negative,negative
414547616,"It might be great to mention your (1) dataset size (2) training data details (e.g., image resolution (height x width), if input and output images are aligned). It is hard to give any advice without knowing more details. 

I think you should crop square patches and train with patches during training. During the test time, you can run the model in a fully convolutional fashion. You can also use different networks (such as ResNet Generator). ",might great mention size training data image resolution height width input output hard give advice without knowing think crop square train training test time run model fully convolutional fashion also use different generator,issue,positive,positive,positive,positive,positive,positive
414536897,"Now result 
![image](https://user-images.githubusercontent.com/19235054/44378436-e6dda280-a533-11e8-9821-11702e28622d.png)
generate some string not the right a-z
",result image generate string right,issue,negative,positive,positive,positive,positive,positive
414090638,@junyanz Which parameters did you use for training FCN-8s on cityscape dataset? I what to reproduce this result on pytorch. Thank you.,use training cityscape reproduce result thank,issue,negative,neutral,neutral,neutral,neutral,neutral
414035710,I think Image2image translation can deal with this problem,think translation deal problem,issue,negative,neutral,neutral,neutral,neutral,neutral
413723670,checkpoints will be retained using epoch_count option - as you might see the default is 1 but you could set it to say 201 and then your checkpoints will be numbered accordingly,option might see default could set say accordingly,issue,negative,neutral,neutral,neutral,neutral,neutral
413436008,"Hi, I also want to visualize the results by index.html file, could you tell me how should I do to make it ?  I opened the file, but the pictures failed  to load.",hi also want visualize file could tell make file load,issue,negative,neutral,neutral,neutral,neutral,neutral
412868934,"Sure.
Can you please be more specific?
Getting the encoder output involves tweaking the function ResnetGenerator() (assuming your configuration sets this to be used).",sure please specific getting output function assuming configuration used,issue,positive,positive,positive,positive,positive,positive
412547376,"@didirus The original resolution will be preserved.
During training, you can train do augmentation by resizing and random cropping. Since the network is fully convolutional, this means that you can test on any size of the image, and it won't be affected.

To elaborate, assume that you have an image of 800x600, you can train on square patches of 256x26, but you set the` --resize_or_crop none`, during testing, so the full image (original resolution) will be translated normally based on your saved weights/model",original resolution training train augmentation random since network fully convolutional test size image wo affected elaborate assume image train square set none testing full image original resolution normally based saved,issue,positive,positive,positive,positive,positive,positive
412507108,"@junyanz, what do you mean with 

> During the test time, you can apply the model to the full image.

How would you do this if you want the original resolution of the image?",mean test time apply model full image would want original resolution image,issue,negative,positive,positive,positive,positive,positive
411019579,@PhoenixFeifei @junyanz May i ask how to use the Imagenet data for training colorization task. Do I need to convert the data into gray images myself?,may ask use data training colorization task need convert data gray,issue,negative,neutral,neutral,neutral,neutral,neutral
410536683,"""The ResnetBlock will keep the spatial resolution""

This means that it is 'same convolution'  ?

 ""(2) can be treated as the bottleneck""

Isn't it (3) ?? as the strided convolution should decrease the spatial dimension till reaching the bottleneck. 

Best ",keep spatial resolution convolution bottleneck convolution decrease spatial dimension till reaching bottleneck best,issue,positive,positive,positive,positive,positive,positive
410532631,"ResnetGenerator has (1) K convolutional layers (2) N ResnetBlock (3) K strided convolutional layers. 
The ResnetBlock will keep the spatial resolution. (2) can be treated as the bottleneck. ",convolutional convolutional keep spatial resolution bottleneck,issue,negative,neutral,neutral,neutral,neutral,neutral
410531477,The results look like the saved one without `--resize_or_crop none`？,look like saved one without none,issue,positive,neutral,neutral,neutral,neutral,neutral
410379069,"""InstanceNorm with batch size > 1"" and ""BatchNorm with batch size > 1"" are different things. Instance norm computes activation statistics per sample, while batch norm computes it over all samples in the minibatch. ""InstanceNorm with batch size > 1"" and ""InstanceNorm with batch size == 1"" are also different, because in the former case, we average the gradients from samples. 

You can try any of these in this code repo. They tend to produce different results. The default setting is Instance Norm with batch size == 1. ",batch size batch size different instance norm activation statistic per sample batch norm batch size batch size also different former case average try code tend produce different default setting instance norm batch size,issue,negative,negative,neutral,neutral,negative,negative
409064722,"In my opinion, you may forget to add ""--no_dropout"".",opinion may forget add,issue,negative,neutral,neutral,neutral,neutral,neutral
408710829,Fixed the problem! Simple mistake,fixed problem simple mistake,issue,negative,positive,neutral,neutral,positive,positive
408517112,"@junyanz So yes, this is the main point.
In your experiments with CycleGAN, it really achieved good results for domain adaptation because it was working on low semantic images (poor classes). But in a case from simulation to real-world for driving scenarios task (GTA5 <-> cityscape), having square patches won't be that good I think.
The idea, I don't only need to change colors, but I want to adapt features also as well. So you think in this case, training on a full image is better ? or what ?",yes main point really good domain adaptation working low semantic poor class case simulation driving task cityscape square wo good think idea need change color want adapt also well think case training full image better,issue,positive,positive,positive,positive,positive,positive
408515933,"I see. In general, bigger patch size will produce more dramatic changes. In the case of GTA5<->cityscapes, if you use a small window size (e.g., `360` out of 1k image), you can only change the local color and texture. But if you use a big window size (e.g., full image size), the model will learn to add or remove trees as there are more trees in cityscapes dataset then the GTA5.",see general bigger patch size produce dramatic case use small window size image change local color texture use big window size full image size model learn add remove,issue,negative,negative,neutral,neutral,negative,negative
408312546,"@junyanz  Yes I understand.
I know that the same settings has to be set for training and testing, in order not to create a big gap between them.

But this is not my question, my question is according to your knowledge, why do you think that this hyperparameter is effective in achieving different results ? What is its importance on the dataset ?",yes understand know set training testing order create big gap question question according knowledge think effective different importance,issue,positive,positive,positive,positive,positive,positive
408307075,"@davidwessman 

The quickest and easiest solution i found is set different display_port for each train so each job's trainign result can be displayed on different visdom sever.

For example when you want to train 4 jobs at once. At first start 4 instances of visdom server at 4 different ports. Then for each job, set the display_port to corresponding port so the training result would appear on separated visdom server

1. start 4 different visdom server instances

```bash
python -m visdom.server -port 8096
python -m visdom.server -port 8097
python -m visdom.server -port 8098
python -m visdom.server -port 8099
```
2. start training jobs and mount on desired visdom server
```bash
job1: python train.py ..... --display_port 8096
job2: python train.py ..... --display_port 8097
job3: python train.py ..... --display_port 8098
job4: python train.py ..... --display_port 8099
```
Hope that help
",easiest solution found set different train job result displayed different sever example want train first start server different job set corresponding port training result would appear server start different server bash python python python python start training mount desired server bash job python job python job python job python hope help,issue,positive,positive,neutral,neutral,positive,positive
408281717,"I think you need to make sure the training and test images have the same scale. 
For example, consider this parameter setting: 
`--fineSize 256 --loadSize 360` for training; 
`--resize_or_crop none` for test.
If your original images are `360x360`, this parameter setting will work. 
But if your original images are `720x720`, the scales are different across training and test. 
In this case, you might want to use  `--fineSize 256 --loadSize 720` for training and `--resize_or_crop none` for test. 
You can also use  `--fineSize 256 --loadSize 360` for training and `--fineSize 360 --loadSize 360` for test.",think need make sure training test scale example consider parameter setting training none test original parameter setting work original scale different across training test case might want use training none test also use training test,issue,negative,positive,positive,positive,positive,positive
408280900,You can run the model on the training images and see if it is the same as the saved results during training. ,run model training see saved training,issue,negative,neutral,neutral,neutral,neutral,neutral
408183535,@junyanz  Sorry I don't get your point. What do you mean by reproducing the training set results with the test script ?,sorry get point mean training set test script,issue,negative,negative,negative,negative,negative,negative
408179404,There is always a training/test gap in any ML system. The key is to reproduce the training set results with the test script.,always gap system key reproduce training set test script,issue,negative,neutral,neutral,neutral,neutral,neutral
408035148,"@taesung89 maybe it is a little bit better
`--resize_or_crop none`
![image](https://user-images.githubusercontent.com/33177438/43253826-28db1136-90c6-11e8-8658-2b44eefc9d93.png)

without `--resize_or_crop none`
![image](https://user-images.githubusercontent.com/33177438/43253846-30a164b0-90c6-11e8-8989-b94319f4bc9e.png)


But this is for an image in the test set, in general, it is not performing good on the test set compared to what was saved during training 
",maybe little bit better none image without none image image test set general good test set saved training,issue,positive,positive,positive,positive,positive,positive
408033660,"**how about running a test with cyclegan model directly?**
@junyanz same results with cyclegan model directly

**I recommend you first test without --resize_or_crop none to see this is the real problem.**
@taesung89 also same problem
![image](https://user-images.githubusercontent.com/33177438/43253543-886f784a-90c5-11e8-9e0f-1f916579ddd2.png)

",running test model directly model directly recommend first test without none see real problem also problem image,issue,positive,positive,positive,positive,positive,positive
407869063,"Travis-ci might be an overkill. Instead, we wrote a lightweight [script](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/test_before_push.py) for testing. ",might instead wrote lightweight script testing,issue,negative,neutral,neutral,neutral,neutral,neutral
407863120,You're right! I feel so stupid right now lol,right feel stupid right,issue,negative,negative,neutral,neutral,negative,negative
407843792,"I think the problem might be that you scaled the image to 256px x 256px at training time, but at test time, you used the 800px x 600px original resolution. This is a pretty big gap. 

I recommend you first test without --resize_or_crop none to see this is the real problem. 
Then I recommend training and test at the same scale. You can do this by 

`--resize_or_crop crop --fineSize 360`

which loads the image at the original resolution of 800x600 and then making a square crop of 360x360. Please change the number 360 to something that fits on your GPU. 

This method does not change the scale of the image, so you can use `--resize_or_crop none` option at test time. ",think problem might scaled image training time test time used original resolution pretty big gap recommend first test without none see real problem recommend training test scale crop image original resolution making square crop please change number something method change scale image use none option test time,issue,positive,positive,positive,positive,positive,positive
407839545,"1. It could be slow for each GPU to only process 1.  You may want to feed 4 images per GPU. 
2. You may want to use `instance_normalization`. Multi-GPU[ synchronized batchnorm](https://github.com/pytorch/pytorch/issues/2584) has not been implemented in this repo. Using batch_norm with multiple GPUs  might casue issues.",could slow process may want feed per may want use synchronized multiple might,issue,negative,negative,negative,negative,negative,negative
407838434,"Interesting. Does it work without `--resize_or_crop none`?  @taesung89 @SsnL 

how about running a test with cyclegan model directly?
```bash
python test.py --dataroot ./datasets/kitti--name kitti_cyclegan --model cycle_gan --gpu_ids=6
```",interesting work without none running test model directly bash python name model,issue,negative,positive,positive,positive,positive,positive
407835954,"You need to also tell the network which color you would like to generate during training/test. 
See a relevant work on user-guided colorization: [paper](https://arxiv.org/abs/1705.02999), [code](https://github.com/junyanz/interactive-deep-colorization)",need also tell network color would like generate see relevant work colorization paper code,issue,negative,positive,positive,positive,positive,positive
407834858,"If you detach the output of D, G cannot get gradients. We did calculate the gradients for D, but we didn't update the D's weights.",detach output get calculate update,issue,negative,neutral,neutral,neutral,neutral,neutral
407741149,"But for example, in this line: `self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)` you don't detach the output of the discriminator and by that the loss can back-propagate to it. Isn't it?",example line true detach output discriminator loss,issue,negative,positive,positive,positive,positive,positive
407646955,"The images saved in the checkpoints during training are much much better, and this is so weird for me
Have a look
![image](https://user-images.githubusercontent.com/33177438/43183083-129a1616-8fe4-11e8-9fed-0104e8be9b7f.png)
![image](https://user-images.githubusercontent.com/33177438/43183104-28d5b75a-8fe4-11e8-80ae-eba40ab8e703.png)

I think for sure it has to be something wrong in testing, I really don't know what, but it is very weird.
Even on training data, they look so bad when testing them, however, during training, they look really nice and as I want.",saved training much much better weird look image image think sure something wrong testing really know weird even training data look bad testing however training look really nice want,issue,negative,negative,neutral,neutral,negative,negative
407640943,"but which size do you recommend ? because I have read some issues about batch size, and most of the people said that` --batchSize=1` works best, and you already mentioned here #137  that for you batch size of 1 on a single GPU, gave the best results.

So this means that I choose my batch size according the #GPUs ? eg: If I am using 2 GPUs, then `batchSize=2`, and if 3 GPUs then `batchSize=3` and so ? so it becomes that each GPU will process 1 batch ?

And also I read something about `instance_normalization` and `batch_normalization`, when changing the batchSize

what's your opinion in general @junyanz ?",size recommend read batch size people said work best already batch size single gave best choose batch size according becomes process batch also read something opinion general,issue,positive,positive,positive,positive,positive,positive
407635538,"@junyanz  This is how I run and test
`python train.py --dataroot ./datasets/kitti--name kitti_cyclegan --model cycle_gan --gpu_ids=6 --display_id -1`

`python test.py --dataroot datasets/kitti/testA --name kitti_cyclegan --checkpoints_dir ./checkpoints/ --model test --gpu_ids=7 --resize_or_crop none`",run test python name model python name model test none,issue,negative,neutral,neutral,neutral,neutral,neutral
407627168,"No worries. Have fun with CycleGAN. :)

On Tue, Jul 24, 2018 at 22:54 Helena <notifications@github.com> wrote:

> Closed #334
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/334>.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/334#event-1751598728>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFaWZcZPmwamywS7pt054BHF_Nd8LJNEks5uJ93ugaJpZM4VaxkP>
> .
>
",fun tue wrote closed reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
407617140,and of course you're right  - double checked the training set - some of the images coming my  phone were rotated and i totally missed - so sorry :woman_facepalming: ,course right double checked training set coming phone rotated totally sorry,issue,negative,negative,neutral,neutral,negative,negative
407614643,Do you have a concrete example? From the code it shouldn't have such behavior...,concrete example code behavior,issue,negative,positive,positive,positive,positive,positive
407613332,"i don't set the first two so they are defaulted , and yes - no_flip

and it seems like only the rectangular images get rotated - i'm currently training with square ones, and there is no rotation as far as i can see",set first two yes like rectangular get rotated currently training square rotation far see,issue,positive,positive,positive,positive,positive,positive
407607461,Many GAN losses do not converge at all (exception: WGAN) due to the nature of minimax optimization. Your loss curves look quite normal.  ,many gan converge exception due nature optimization loss look quite normal,issue,negative,positive,positive,positive,positive,positive
407604964,"Yes. You can crop the patches (e.g., `360x360`) without resizing the original image. You can use `--resize_or_crop crop`. During the test time, you can apply the model to the full image. Please check out the Preprocessing instruction in the training/test [details](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#trainingtest-details).",yes crop without original image use crop test time apply model full image please check instruction,issue,negative,positive,positive,positive,positive,positive
407588424,"It was a mistake on me. Sorry about that. I fixed it. Could you check if it works now?

",mistake sorry fixed could check work,issue,negative,negative,negative,negative,negative,negative
407584848,"@devraj89 This is a great question. For your questions: 
1. You are right. This loss can regularize the generator to be near an identity mapping when real samples of the target domain are provided. If something already looks like from the target domain, you should not map it into a different image.
2. Yes. The model will be more conservative for unknown content.
3. It was described in Sec 5.2 ""Photo generation from paintings (Figure 12) "" in the CycleGAN paper.  The idea was first proposed by Taigman et al's [paper](https://arxiv.org/pdf/1611.02200.pdf). See Eqn (6) in their paper.
4. It depends on your goal and it is quite subjecive. We don't have a benchmark yet. But Fig 
9 in the paper illustrate the difference. In general, it can help bette preserve the content if that is your priority. ",great question right loss regularize generator near identity real target domain provided something already like target domain map different image yes model conservative unknown content sec photo generation figure paper idea first al paper see paper goal quite yet fig paper illustrate difference general help preserve content priority,issue,positive,positive,positive,positive,positive,positive
407582624,We did freeze the D's weights when calculating the G loss. Only G's weights got updated. See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L143).,freeze calculating loss got see line,issue,negative,neutral,neutral,neutral,neutral,neutral
407580073,You need to increase your batchSize. Try `--batchSize 4` or even a larger batchSize. Each GPU will process batchSize/#GPUs,need increase try even process,issue,negative,neutral,neutral,neutral,neutral,neutral
407510378,"Could you share with your training and test script? 
Could you also try to run your saved model on the training set images and see if it can match the saved results?",could share training test script could also try run saved model training set see match saved,issue,positive,neutral,neutral,neutral,neutral,neutral
406379207,You can reduce the number of iterations of training. This can be done by changing `--niter` and `--niter_decay`. The total number of iterations is `niter + niter_decay`,reduce number training done niter total number niter,issue,negative,neutral,neutral,neutral,neutral,neutral
406341532,"I'm not sure that you can specify the .ssh/config file on windows in the same way. 

Alternatively you could use the windows powershell.

You first need to allow ssh to be used in powershell,[ this tutorial](https://www.howtogeek.com/336775/how-to-enable-and-use-windows-10s-built-in-ssh-commands/) can help you with that. 

Then when you ssh you need to specify that you want to local port forward. 
For example run `ssh -L 8097:127.0.0.1:8097 username@address`

After you do that refer to step 4 above. 

Let me know if that works out! ",sure specify file way alternatively could use first need allow used tutorial help need specify want local port forward example run address refer step let know work,issue,positive,positive,positive,positive,positive,positive
406249680,"I also think so. However, it seems that the style_ukiyoe_pretrained is stored in style_cezanne_pretrained. Therefore, style_cezanne_pretraind is missing. It takes me a long time to train. I would be very grateful if I could receive style_cezanne_pretrained data.",also think however therefore missing long time train would grateful could receive data,issue,negative,negative,negative,negative,negative,negative
406158745,"@calebescobedo thanks a lot for your help.
I am running on a local windows machine, do you have any idea if that is applicable too ?",thanks lot help running local machine idea applicable,issue,positive,positive,neutral,neutral,positive,positive
406157714,"hello @taesung89 
Thanks for your helpful reply

### Did you try to train or test?
I was training

### 800x600 is more than 7 times larger than 256x256 image (800x600/(256x256) = 7.32), so the memory requirement will be very high.
Yes, this another problem I have. I am already working on a server which has lots of GPUs, each one is 16 GB. When I choose a single GPU, it is allocated but not fully utilized, only 4GB out of the 16 are utilized. Is there an idea of how to fully use the GPU ?? and accordingly, if I choose multiple GPUs, only one of them is allocated. you can refer to my issue here [#327](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/327)

### One approach to save memory is to train on cropped images using --resize_or_crop resize_and_crop, and then generate the images at test time by loading only one generator network using --model test --resize_or_crop none. I think 800x600 can be dealt this way.
Exactly, this is what I did. I did a resize and crop (as the original implementation), and then during testing, I tested on the full image, and it worked


One last question, what is your opinion concerning my case of training on rectangle images 800x600. Do you think resizing to 286x286 then cropping to 256x256 is a good idea ?, or shall I skip the resizing and only crop square patches, or shall I resize to smaller rectangle images instead of square, and then do the random cropping. 
what can you recommend me to do ?

",hello thanks helpful reply try train test training time image memory requirement high yes another problem already working server lot one choose single fully idea fully use accordingly choose multiple one refer issue one approach save memory train generate test time loading one generator network model test none think dealt way exactly resize crop original implementation testing tested full image worked one last question opinion concerning case training rectangle think good idea shall skip crop square shall resize smaller rectangle instead square random recommend,issue,positive,positive,positive,positive,positive,positive
406144776,"Yes. @calebescobedo  is correct. 
If we call G the generator from A to B, and F from B to A, 

rec_A = F(G(A))
rec_B = G(F(B))
idt_A = G(B)
idt_B = F(A)",yes correct call generator,issue,negative,neutral,neutral,neutral,neutral,neutral
406144401,"Could you share the options you used for training and testing? There can be many reasons, but I personally sometimes flip A and B of the datasets. ",could share used training testing many personally sometimes flip,issue,negative,positive,positive,positive,positive,positive
406144023,"Did you try to train or test? 800x600 is more than 7 times larger than 256x256 image (800x600/(256x256) = 7.32), so the memory requirement will be very high. 

One approach to save memory is to train on cropped images using `--resize_or_crop resize_and_crop`, and then generate the images at test time by loading only one generator network using `--model test --resize_or_crop none`. I think 800x600 can be dealt this way. 

If it still run into out-of-memory error, you can try reducing the network size. ",try train test time image memory requirement high one approach save memory train generate test time loading one generator network model test none think dealt way still run error try reducing network size,issue,negative,positive,positive,positive,positive,positive
406085465,"I believe rec means reconstruction and idt means identity. 

Some sections of the [Cycle-GAN paper](https://arxiv.org/pdf/1703.10593.pdf) offer good examples. 
Figure 4 has a good example of reconstruction. 

Section 5.2 under the header **Photo generation from paintings** explains what identity mapping loss does. It helps preserve color. 
Figure 9 shows an example of using identity along with CycleGAN. 

",believe reconstruction identity paper offer good figure good example reconstruction section header photo generation identity loss preserve color figure example identity along,issue,positive,positive,positive,positive,positive,positive
406051370,"Hey mhusseinsh, I had this exact same problem! 

I'm gonna write out what I did and hopefully it will help you.  

1) On your own computer run `touch ~/.ssh/config ` to create a ssh configuration file where you can specify some setting for when you ssh. 

2) In that ssh config file you can create a new host to specific settings for this remote server 
Example:
normally you would run something like
`ssh username@address` 
In the config file add this as a new entry
```
Host nameForSSHing
    HostName address
    User username 
    LocalForward 127.0.0.1:8097 127.0.0.1:8097
```
3) now you can use `ssh nameForSSHing`
this website helped me: https://www.digitalocean.com/community/tutorials/how-to-configure-custom-connection-options-for-your-ssh-client

4) When I ran the actual code on the test server I first started the visdom server using 
`visdom` and it gave me a link so that I could view the results as they came in. 

5) Finally when running train.py on my remote server I added the flag `--display_server ""http://address""` where address is the name of the ssh address mentioned earlier. 

Let me know if any of these parts are unneeded to run the code! 
",hey exact problem gon na write hopefully help computer run touch create configuration file specify setting file create new host specific remote server example normally would run something like address file add new entry host address user use ran actual code test server first server gave link could view came finally running remote server added flag address name address let know unneeded run code,issue,positive,positive,neutral,neutral,positive,positive
405546440,"even with a single GPU, it allocates the selected one, but doesn't fully utilize it
it only uses `4021MiB / 16276MiB`",even single selected one fully utilize mib mib,issue,negative,negative,neutral,neutral,negative,negative
405504527,"So if you could please tell me what is your opinion about training rectangle images ? Should I feed the network without resizing or cropping ? or train on cropped square patches and then test on the whole image ? 
what is your idea about such operation ?",could please tell opinion training rectangle feed network without train square test whole image idea operation,issue,negative,positive,positive,positive,positive,positive
405329825,I think the identity loss is used to preserve the color and prevent reverse color in the result. ,think identity loss used preserve color prevent reverse color result,issue,negative,neutral,neutral,neutral,neutral,neutral
405239212,"@IsDling  thank you,I try to padding my images because the largest size of my images is 1000,and the loss of texture information is  unacceptable now

",thank try padding size loss texture information unacceptable,issue,negative,neutral,neutral,neutral,neutral,neutral
405177573,Thank you very much. I downloaded the new version of the code and fixed the problem.,thank much new version code fixed problem,issue,negative,positive,positive,positive,positive,positive
405139216,"@liang233 firstly,you must make your data set aligned, and then you can crop your original images to some images which have smaller size. For example,if we have some a image which size is 400*400,then you can  crop it to 4 images with size 200*200.Train the model with smaller images and when test,you can use model to generate some images with size 200*200,and then merging them in order,so you can get a result with size 400*400.But it have some disadvantage that the result may lost some texture information. You can try this method with your data set.Good luck!",liang firstly must make data set crop original smaller size example image size crop size model smaller test use model generate size order get result size disadvantage result may lost texture information try method data luck,issue,negative,positive,positive,positive,positive,positive
405102378,"@IsDling hello,I now meet this problem,
![default](https://user-images.githubusercontent.com/8869756/42735889-d290ee02-888e-11e8-8dbe-9cb40fc528b2.png),I want to know how to combine? thanks
",hello meet problem default want know combine thanks,issue,negative,positive,positive,positive,positive,positive
405059296,Thank you！But I can't open the web-link of the post. Can you check it? ,thank ca open post check,issue,negative,neutral,neutral,neutral,neutral,neutral
405012290,"I'm using TFGAN API for TensorFlow
https://ai.googleblog.com/2017/12/tfgan-lightweight-library-for.html

It's quite possible that the issue is implementation specific however the information may be useful.

I train CycleGAN on horse2zebra dataset, I augment it with random left-right flips and random crop after resizing an image to 286x286 and shuffling the dataset.

I've noticed that the ""inverse colour"" effect appears after 100 steps with batch 1 and it depends on the first images drawn from the dataset. I've solved it by manually tuning random seed for flips, crops and shuffle. If the network doesn't start to invert colours after first 100 steps, it's going to be OK for the rest of training. With image summaries in TensorBoard it's quite easy to tune and restart training couple of times until you get a desirable result.

I also started to train 256x256 images with 6 resnet blocks simply because I couldn't cram a deeper network into 2gb memory of my laptop's GeForce. Apart from horses it started also to recolour green grass into brown soil. The network with 9 resnet blocks on Google Colab does nice though. It could be because of the shallow architecture but it could be because of ""unfortunate"" combination of images from dataset at the start of training as well.

Thus it could make sense to make random seed a hyperparameter and a cmd line option for datasets.",quite possible issue implementation specific however information may useful train augment random random crop image shuffling inverse colour effect batch first drawn manually tuning random seed shuffle network start invert first going rest training image quite easy tune restart training couple time get desirable result also train simply could cram network memory apart also green grass brown soil network nice though could shallow architecture could unfortunate combination start training well thus could make sense make random seed line option,issue,positive,negative,neutral,neutral,negative,negative
404993095,"You need to modify the data loader.  See #320 for more details.
",need modify data loader see,issue,negative,neutral,neutral,neutral,neutral,neutral
404981893,"The issue with `unexpected key: num_batches_tracked` should be fixed by the latest commit. 
Regarding the first error on this thread, I believe it's because PyTorch's default setting has changed. Could you get the latest pytorch and try again?",issue unexpected key fixed latest commit regarding first error thread believe default setting could get latest try,issue,negative,positive,positive,positive,positive,positive
404981340,"Please pull the latest commit of this repo and try again. I added some code to get around the issue. 

The problem was due to the following situation on PyTorch. 
The conda version uses version 0.4, which does not have num_batches_tracked field. 
The version built from source is 0.5, which introduced `num_batches_tracked`. 
For forward compatibility, I disabled `num_batches_tracked`. ",please pull latest commit try added code get around issue problem due following situation version version field version built source forward compatibility disabled,issue,negative,positive,neutral,neutral,positive,positive
404903160,"The current code does not support tiff images. You can change the data loader accordingly.
For aligned dataset, change this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/aligned_dataset.py#L24); For unaligned dataset, change this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L36)",current code support tiff change data loader accordingly change line unaligned change line,issue,negative,neutral,neutral,neutral,neutral,neutral
404826474,"Sorry, I solved this problem by correct docker setting.
When I used `pytorch-nightly` instead of `pytorch`, I got good results.
This is my dockerfile.(forgive my dockerfile that is dirty)
```
FROM nvidia/cuda:9.0-cudnn7-devel

RUN apt-get update && apt-get install -y \
   build-essential curl wget git cmake vim pkg-config unzip libgtk2.0-dev python3 python3-pip \
   imagemagick graphviz > /dev/null

# Miniconda3
ENV PATH /opt/conda/bin:$PATH
ENV LB_LIBRARY_PATH /opt/conda/lib:$LB_LIBRARY_PATH
RUN curl -Ls https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -o /tmp/install-miniconda.sh && \
   /bin/bash /tmp/install-miniconda.sh -b -p /opt/conda && \
   conda update -n base conda && \
   conda update --all -y

# Basic dependencies
RUN conda config --add channels conda-forge
RUN conda install -y readline mkl openblas numpy scipy hdf5 \
   pillow matplotlib cython pandas gensim protobuf \
   lmdb leveldb boost jupyterlab
RUN pip install pydot_ng nnpack h5py scikit-learn scikit-image hyperdash backports.ssl_match_hostname

# OpenCV
RUN conda install opencv3 -c menpo -y
RUN conda install dominate bz2file visdom

# PyTorch
RUN conda install pytorch-nightly torchvision cuda90 -c pytorch -y

# For CycleGAN and pix2pix
ADD ./vision /vision
WORKDIR /vision
RUN python3 setup.py install
WORKDIR /
```",sorry problem correct docker setting used instead got good forgive dirty run update install curl git vim python path path run curl update base update basic run add run install pillow boost run pip install run install run install dominate run install add run python install,issue,negative,negative,negative,negative,negative,negative
404812089,"I faced same error on applying a pre-train model (cyclegan) in the newest version.
Is normalization(instancenorm) also related to this case?",faced error model version normalization also related case,issue,negative,neutral,neutral,neutral,neutral,neutral
404807625,"I used the default normalization (`instancenorm`) during training and test. 
I was able to solve the problem downloading the newest version of the code and training the model again.",used default normalization training test able solve problem version code training model,issue,negative,positive,positive,positive,positive,positive
404731634,"Faced the same error.

<img width=""1913"" alt=""screen shot 2018-07-12 at 10 42 24 pm"" src=""https://user-images.githubusercontent.com/6956728/42674507-df3d5724-8624-11e8-9deb-667a93578313.png"">
",faced error screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
404700790,@shirishr how to clean up? could you please tell me?,clean could please tell,issue,positive,positive,positive,positive,positive,positive
404670134,"Could you check if you have used the same normalization (`batchnorm`, `instancenorm`) during training and test?",could check used normalization training test,issue,negative,neutral,neutral,neutral,neutral,neutral
404669871,The generator's last layer is TanH(). (see this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L181) for an example.) The images are normalized [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/aligned_dataset.py#L37) for aligned datasets and [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py#L49) for unaligned datasets.  You are free to change the range of input/output. But you may also want to change the last layer of G accordingly.,generator last layer tanh see line example unaligned free change range may also want change last layer accordingly,issue,positive,positive,positive,positive,positive,positive
404669243,It is possible. I think the self-attention layer should be added to the discriminator. You don't need to change the generator. You can just take the D from self-att GANs.,possible think layer added discriminator need change generator take,issue,negative,neutral,neutral,neutral,neutral,neutral
404498542,"I face the same issue. I've trained pix2pix model with the previous version of the code and tried to test it using the older and the latest commit and got the same ""missing keys in state_dict"" error in both.",face issue trained model previous version code tried test older latest commit got missing error,issue,negative,positive,neutral,neutral,positive,positive
404087761,"I face the same problem.
I'd like to run a CycleGAN pre-trained model.
However, "" RuntimeError : Unexpected key (s) in state_dict "" occurs. Please help me.",face problem like run model however unexpected key please help,issue,positive,positive,neutral,neutral,positive,positive
403685218,Added! Please use the script `pretrained_models/download_cyclegan_model.sh` to download the pretrained models. ,added please use script,issue,negative,neutral,neutral,neutral,neutral,neutral
403288398,"Similar question...
Do you think whether it's possible to add self-attention layer into pix2pix?
If possible, how to do it? add it to decoder part?",similar question think whether possible add layer possible add part,issue,negative,neutral,neutral,neutral,neutral,neutral
402774979,"I mean when I train, I guess I cannot feed two input images A and B to the input tensor. I need to stack them to create one image for input.",mean train guess feed two input input tensor need stack create one image input,issue,negative,negative,negative,negative,negative,negative
402774164,"If you write your own data loader, you can load each image separately by the name `image0000_A`, `image0000_B`, `image0000_C`. You don't need to stack them.",write data loader load image separately name need stack,issue,negative,neutral,neutral,neutral,neutral,neutral
402585125,"I have faced the same phenomenon. When discriminator loss goes down to 0 and repeats periodically, the results of the generator look terrible. And in this case L1 loss doesn't go down all the time. I faced this when I added another branch or another loss, like perceptural loss or adding another branch to make FC and regression loss. Though this added loss looks normal, it seems D easily judges the generated outputs when loss goes to 0, no matter what namda I give it to FC_weight to match l1_weight and gan_weight.
Look like this, the first is the normal loss, the second is when I add FC loss.
![1](https://user-images.githubusercontent.com/12149807/42298692-df241bc4-8039-11e8-98c2-947deb8d047c.png)
Fig1.

![2](https://user-images.githubusercontent.com/12149807/42298702-e5784cfc-8039-11e8-8276-0e699dd8084c.png)
Fig2.
",faced phenomenon discriminator loss go periodically generator look terrible case loss go time faced added another branch another loss like loss another branch make regression loss though added loss normal easily loss go matter namda give match look like first normal loss second add loss fig fig,issue,negative,negative,neutral,neutral,negative,negative
402543773,Should I stack along the channel dimension for A and B?,stack along channel dimension,issue,negative,neutral,neutral,neutral,neutral,neutral
402543014,This is a great catch. I just added it through this [commit](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/2ecf15f8a7f87fa56e784e0504136e9daf6b93d6). Let me know if it works for you. ,great catch added commit let know work,issue,positive,positive,positive,positive,positive,positive
402542198,It's possible that images in your output space are more complicated than images in your input space. This will cause the different sizes of images. The current code will save everything to a 3-channel image. You can change the behavior [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/util/visualizer.py#L20).,possible output space complicated input space cause different size current code save everything image change behavior,issue,negative,negative,negative,negative,negative,negative
402541548,"I see. In this special case, you may want to write your own data loader. It should only take 1 hour. ",see special case may want write data loader take hour,issue,negative,positive,positive,positive,positive,positive
402541300,"You can tweak the kernel size or padding of some conv layers. But you can resize your 80x80 to 128x128, and feed 128x128 to the network. ",tweak kernel size padding resize feed network,issue,negative,neutral,neutral,neutral,neutral,neutral
402541288,"I am a little confused because my input has 2 image A, B; and my output is only image C. Thanks",little confused input image output image thanks,issue,negative,negative,negative,negative,negative,negative
402541049,"If you stack your inputs, the image will be (200, 400, 1). 
The `aligned_dataset` will load the (200, 400, 1) and split it into two: one for input, and one for output. 
See this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/aligned_dataset.py#L27) for more details.",stack image load split two one input one output see line,issue,negative,neutral,neutral,neutral,neutral,neutral
402498748,"Hello,
thank you for replying
Here the shape of  resulting and training images
>>> im = Image.open('epoch143_real_B.png')
>>> import numpy
>>> numpy.array(im).shape
(256, 256, 3)
>>> im = Image.open('epoch143_real_A.png')
>>> numpy.array(im).shape
(256, 256, 3)
>>> im = Image.open('epoch143_fake_B.png')
>>> numpy.array(im).shape
(256, 256, 3)
Training image example
>>> im = Image.open('/gpfs/scratch/userinternal/gpedrazz/fs/train/1330.png')
>>> numpy.array(im).shape
(256, 514)

Giorgio",hello thank shape resulting training import training image example,issue,negative,neutral,neutral,neutral,neutral,neutral
402356409,"So if I stack the dataset like you suggested, the dimension of input and output will be different. For example, I have two sets of images with dimension (200,200,1) and (200,200,1). I want to create the output of dimension (200,200,1). How can I stack inputs to feed in training? If I stack along width dimension, it will be (200,400,1) for input and (200,200,1) for output?",stack like dimension input output different example two dimension want create output dimension stack feed training stack along width dimension input output,issue,positive,neutral,neutral,neutral,neutral,neutral
402355747,"It's not supported by the `aligned_dataset.py`. Also, the code assumes that H and W are the same for both input and output. ",also code input output,issue,negative,neutral,neutral,neutral,neutral,neutral
402350932,"Is it possible to have the input image and output image with different dimensions? If we stack along the width dimension, the dimension of input image is (H,W+W,C) and the dimension of output image is (H,W,C)?",possible input image output image different stack along width dimension dimension input image dimension output image,issue,negative,neutral,neutral,neutral,neutral,neutral
402350366,@phamnam95  it is the current design of default data loader for pix2pix. You can run this [script](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/datasets/combine_A_and_B.py) to concatenate input and output images. It works fine if C1=C2=3 or C1=C2=1. It might not be the best way for your datasets. Feel free to write your own data loader. ,current design default data loader run script concatenate input output work fine might best way feel free write data loader,issue,positive,positive,positive,positive,positive,positive
402350080,"Could you print the shape of real_A, fake_B, and real_B? You can either get the information from the saved images or add a few lines to the code.",could print shape either get information saved add code,issue,negative,neutral,neutral,neutral,neutral,neutral
402349457,Why do we need to stack along the width dimension?,need stack along width dimension,issue,negative,neutral,neutral,neutral,neutral,neutral
402349364,I think it's normal. `G_GAN` is the GAN loss for the generator. `D_real` and `D_fake` are the losses for Ds regarding real and fake inputs.,think normal gan loss generator regarding real fake,issue,negative,negative,negative,negative,negative,negative
402348833,"You should stack two images as (H1, W1+W1, C1) where we assume C1=C2. For other types of data, you may consider writing your own data loader inherited from the base_dataset [model](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/base_dataset.py).",stack two assume data may consider writing data loader model,issue,negative,neutral,neutral,neutral,neutral,neutral
402016532,"Yes, please check out the latest commit.",yes please check latest commit,issue,positive,positive,positive,positive,positive,positive
402016209,"Unfortunately, UNet does not work with arbitrary input sizes. Try ResNet instead.",unfortunately work arbitrary input size try instead,issue,negative,negative,negative,negative,negative,negative
402015909,"I am not familiar with torch->pytorch conversion. You can also use the old Torch test code for your torch model, or retrain a new PyTorch model with the PyTorch code. ",familiar conversion also use old torch test code torch model retrain new model code,issue,negative,positive,positive,positive,positive,positive
402008687,"Hmm... I just clicked the [link](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/) and it still works for me. 
![image](https://user-images.githubusercontent.com/1924757/42198563-b1a21fb4-7e56-11e8-9416-efbebd383f49.png)
",link still work image,issue,negative,neutral,neutral,neutral,neutral,neutral
401839436,"@tangjilin I am guessing it's the right loss plot from previous issues. Something different between our plots is your G_L1 loss is higher, and my G_GAN loss is higher. May I ask you what kind of operation did you apply to your input images?",guessing right loss plot previous something different loss higher loss higher may ask kind operation apply input,issue,negative,positive,positive,positive,positive,positive
401512348,"Ok, good luck! If you have any useful idea,please tell me!",good luck useful idea please tell,issue,positive,positive,positive,positive,positive,positive
401512228,I am doing similar problem. But my A and B have different range. And I am having same problem with you. I am looking for the solution as well.,similar problem different range problem looking solution well,issue,negative,neutral,neutral,neutral,neutral,neutral
401511930,"Yes, I stack A and B along the channel dimension as you said. My segmentation label is also a RGB image.  I think their range is similar.Do you have any advice? Thanks.",yes stack along channel dimension said segmentation label also image think range advice thanks,issue,positive,positive,positive,positive,positive,positive
401445643,"How did you stack A and B as a pair? Did you stack along the channel dimension. For example A has dimension (H1,W1,C1) and B is (H1,W1,C2). Did you stack along the C dimension to have (H1,W1,C1+C2)? Does your segmentation label have only label 1 and 0, or the label is between 0 and 1? How you normalize the data after stacking (I guess the range of your RGB image and segmentation result is different)?",stack pair stack along channel dimension example dimension stack along dimension segmentation label label label normalize data guess range image segmentation result different,issue,negative,neutral,neutral,neutral,neutral,neutral
401362056,"Could you please add a pre-trained model for apple2orange.

Regards,
Mukesh Chandak",could please add model,issue,negative,neutral,neutral,neutral,neutral,neutral
401041736,"@BlackDeal Check your training options in the dir ./checkpoints/expriment_name/opt.txt , follow the parameters in your testing options  --norm batch/instance
![image](https://user-images.githubusercontent.com/14801873/42038301-359d4f56-7b1d-11e8-8161-6910a0d931d5.png)
",check training follow testing norm image,issue,negative,neutral,neutral,neutral,neutral,neutral
401002288,"I have improved the accuracy of the input, and the LI_loss reduced to 4. But I can't find a good way about the validation in the tain. ",accuracy input reduced ca find good way validation tain,issue,negative,positive,positive,positive,positive,positive
400928125,"Hello, I have the similar situation with you.Have you solved it? I have 10000 images to train my model.
My loss plot is like this:
![my](https://user-images.githubusercontent.com/35884085/42017108-80022cca-7ae0-11e8-83d1-dfeb51a46ce7.png)



",hello similar situation train model loss plot like,issue,negative,neutral,neutral,neutral,neutral,neutral
400810332,"> Thank u so much. I didn’t notice there’s a so important parameter to me ! 😅

We got hit by it too! @bill0812 

Btw: Close the issue if it helped!",thank much notice important parameter got hit bill close issue,issue,positive,positive,positive,positive,positive,positive
400803278,Thank u so much. I didn’t notice there’s a so important parameter to me ! 😅,thank much notice important parameter,issue,positive,positive,positive,positive,positive,positive
400772294,tanh makes the result bounded. relu can potentially give you unbounded ones.,tanh result bounded potentially give unbounded,issue,negative,neutral,neutral,neutral,neutral,neutral
400530115,"Is this neccesary to put tanh as activation function in last layer? Can I use relu, lrelu or not using activation function at all? I tried normalizing the input to [-1,1] but the result is not as good as normalizing using mean, std, and not using tanh as activation function.
",put tanh activation function last layer use activation function tried input result good mean tanh activation function,issue,negative,positive,positive,positive,positive,positive
400529272,"Range is bounded it seems? Then you can just similarly normalize to [-1, +1] range, without using dataset mean and std.",range bounded similarly normalize range without mean,issue,negative,negative,negative,negative,negative,negative
400525247,Thanks. I will try stack along the image_channel dimension.,thanks try stack along dimension,issue,negative,positive,positive,positive,positive,positive
400508018,Can you suggest any way to structure the input if I have two input for set 1 (A and B) and an output for set 2 (C)? I want to generate C from two input A and B.,suggest way structure input two input set output set want generate two input,issue,negative,neutral,neutral,neutral,neutral,neutral
400420925,"I am using seismic images. After normalizing the data by using (data-mean(data))/std(data), the range is from -10 to 10.",seismic data data data range,issue,negative,neutral,neutral,neutral,neutral,neutral
400419262,"It depends on how your dataset is structured. But you should change it so that `real_A` is a 6 channel tensor, and `real_B` is a 3 channel. Then set `input_nc = 6` and `output_nc = 3`. `6` is from assuming that your label image is also 3-channeled. It should be 3 + #channel of the label image.",structured change channel tensor channel set assuming label image also channel label image,issue,negative,neutral,neutral,neutral,neutral,neutral
400382214,Great idea! That would make things look much nicer. I'll look into this.,great idea would make look much look,issue,positive,positive,positive,positive,positive,positive
400207279,"@phillipi  I mean that the idea of CycleGAN is more or less similar to image-to-image translation except that in CycleGAN the mapping is bidirectional ... so if we are interested in only unidirectional mapping then both are similar (according to my understanding). Is there any difference between them in terms of latent space mapping ?

 ",mean idea le similar translation except bidirectional interested unidirectional similar according understanding difference latent space,issue,negative,negative,neutral,neutral,negative,negative
400140107,"@ahmed-fau Yeah you only need z if you want the translation function to output multiple possibilities for each input.

I'm not sure I understand the question about CycleGAN. In CycleGAN, we don't use z.

",yeah need want translation function output multiple input sure understand question use,issue,positive,positive,positive,positive,positive,positive
400127397,"@phillipi excuse me I have one question regarding this issue:

Based on your statement: ""Without noise, the mapping is deterministic, but that's often fine."", what I understood is that the prior Z is only useful if we need to to have some sort of variety in the generated samples. However, if we need to just learn direct mapping between two paired domains (e.g. image and its semantic label map), then it is sufficient to ignore Z ... is this correct ?

If this intuition is true, why isn't it sufficient for the CycleGAN and you enforced the cycle consistency for the generated signals ?

Many thanks in advance ",excuse one question regarding issue based statement without noise deterministic often fine understood prior useful need sort variety however need learn direct two paired image semantic label map sufficient ignore correct intuition true sufficient enforced cycle consistency many thanks advance,issue,positive,positive,positive,positive,positive,positive
399977752,"This might be because the framework uses Data augmentation by default, input the option `--no_flip`.
https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/base_options.py#L43",might framework data augmentation default input option,issue,negative,neutral,neutral,neutral,neutral,neutral
399976673,"Try settings `--display_id -1`, this is a failing call to Visdom.

Check the `Visualization` part [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#trainingtest-details).

**Edit**:

Did you install all the dependencies?
https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#installation
You need Visdom and Dominate",try failing call check visualization part edit install need dominate,issue,negative,neutral,neutral,neutral,neutral,neutral
399691337,"@ShaniGam Sorry, I tried cycle gan with resnet version, instead of the unet version. ",sorry tried cycle gan version instead version,issue,negative,negative,negative,negative,negative,negative
399669772,"Unfortunately, it doesn't work. When I use unet_128 or unet_256 I get the error:
`raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))
ValueError: Expected more than 1 value per channel when training, got input size [1L, 512L, 1L, 1L]`
It works perfectly when I use one of the resnets.",unfortunately work use get error raise value per channel training got input size size value per channel training got input size work perfectly use one,issue,positive,positive,positive,positive,positive,positive
399033333,"I deleted the root project directory and clone this repository again. Then, it works. I don't know the reason. This is just a fast method to solve this issue for me.",root project directory clone repository work know reason fast method solve issue,issue,negative,positive,positive,positive,positive,positive
398657973,"@jiangwei221 do you know if it works when the training data is not eaqual in height and width? 
maybe the pictures are 600 * 500 and not 600 * 600 ",know work training data height width maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
398649806,"i just confused by the height and width of the training pictures, i found many data sets have the same height and width ,and i what to know  its reflection in a model",confused height width training found many data height width know reflection model,issue,negative,positive,neutral,neutral,positive,positive
398638014,"I tried maps dataset(600x600) and horse2zebra dataset(256x256), they both directly works using `python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan` (change maps to horse2zebra)
I think you do not need to do any changes to use it for 80x80 images.
",tried directly work python name model change think need use,issue,negative,positive,neutral,neutral,positive,positive
397976239,"@junyanz so does this mean, that I can skip resizing and cropping for my dataset ? and just feed them as they are, and train them without any random cropping ?

And also, what is your opinion towards rectangle images ? I mean concerning resizing and cropping ? shall they be resized or not ? and if yes, to what extent or portion can they be resized too ? and accordingly, with the random cropping, shall I do rectangle crop patches or square ones ?",mean skip feed train without random also opinion towards rectangle mean concerning shall yes extent portion accordingly random shall rectangle crop square,issue,negative,negative,negative,negative,negative,negative
397802691,"Indeed, that did the trick :) Many Thanks!",indeed trick many thanks,issue,negative,positive,positive,positive,positive,positive
397794196,"It might be caused by instancenorm/batchnorm. Sometimes, you use a different normalization layer during test compared to the training.  ",might sometimes use different normalization layer test training,issue,negative,neutral,neutral,neutral,neutral,neutral
397794028,"hmm, it's hard to tell without seeing the data and tasks. Could you try batchSize 1 with 1 GPU?",hard tell without seeing data could try,issue,negative,negative,negative,negative,negative,negative
397793953,"CycleGAN can get reasonable results on relatively small datasets (e.g., 1000, 2000 images). It also depends on your task (e.g., the complexity of your transformation, the diversity of your datasets). Also, we apply data augmentation during the training.",get reasonable relatively small also task complexity transformation diversity also apply data augmentation training,issue,negative,negative,neutral,neutral,negative,negative
397793785,"There are no particular reasons. You can also crop smaller patches from 256x256 images. Also, images in some datasets are larger than 286x286.",particular also crop smaller also,issue,negative,positive,neutral,neutral,positive,positive
397740979,"It's an interesting idea to try an L1 loss in the unconditional case, and related to some popular methods! As you suggest, you could create a randomly paired dataset of {z, image} pairs, and then train min_f |f(z) - image|. However, this may require f to be a very convoluted function, since nearby z's might be forced to map to very different images. So we probably don't want to match z's with completely randomly chosen images. 

Instead you could try to match z's to the ""right"" images according to the current f, e.g., try to find a function g(image) --> z that minimizes the above L1 loss: min_g |f(g(image)) - image|. Putting it all together we get an autoencoder: min_{f,g} |f(g(image)) - image|, and, with a few small modifications to make it probabilistic, we get a VAE, which is an unconditional generative model trained with an L1 (or, more often, L2) loss. You can also add an additional GAN loss, giving a [VAE-GAN](https://arxiv.org/abs/1512.09300?context=cs).

This is just one way to train an unconditional model with an L1 loss between generated samples and 
""randomly"" (but intelligently) selected images. There may be some other interesting ways to do it waiting to be discovered. Here is one paper more related paper, which goes the other way around, learning to map images to ""random"" noise vectors and getting good image features out of the process: https://arxiv.org/pdf/1704.05310.pdf",interesting idea try loss unconditional case related popular suggest could create randomly paired image train however may require convoluted function since nearby might forced map different probably want match completely randomly chosen instead could try match right according current try find function image loss image together get image small make probabilistic get unconditional generative model trained often loss also add additional gan loss giving one way train unconditional model loss randomly intelligently selected may interesting way waiting discovered one paper related paper go way around learning map random noise getting good image process,issue,positive,positive,neutral,neutral,positive,positive
397703821,"I am not really sure what you mean, but if you train CycleGAN on two identical datasets, it will learn to not change anything. We observe this as long as the dataset size is reasonably big. This will hold true on the test dataset as well, so you are right that the outputs are unchanged. It's possible to run it on high res images as long as your GPU memory is enough. ",really sure mean train two identical learn change anything observe long size reasonably big hold true test well right unchanged possible run high long memory enough,issue,positive,positive,neutral,neutral,positive,positive
396831412,"@junyanz  so why do we resize images to a bigger size before cropping ? they are already 256x256, why are they resized to 286x286 ?",resize bigger size already,issue,negative,neutral,neutral,neutral,neutral,neutral
396748656,"You can disable the cropping during the test time. Check out options such as `resize_or_crop`, `loadSize`, `fineSize`. See training/test [details](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#trainingtest-details).",disable test time check see,issue,negative,neutral,neutral,neutral,neutral,neutral
396479504,"Thank you,
But I find that in your github project the datasets provided are not very large like 'summer2winter_yosemite' and 'horse2zebra', the images in trainA and trainB are not more than 2000. The dataset are not very large.  And can I easy to achieve the transform from night picture to day picture? I saw your experiment about the transforming from night picture to day picture, can you give me some  suggestions?
Thank again.",thank find project provided large like large easy achieve transform night picture day picture saw experiment transforming night picture day picture give thank,issue,positive,positive,positive,positive,positive,positive
396445570,"Have anyone come accross this phenomenon?
size of my training datasets is 20000 pairs 
my configuration with params:
 --batchSize 64 --no_html  --nThreads 16 --niter 800 --niter_decay 8000 --epoch_count 729 --gpu_ids 0,1,3 --continue_train

loss like this ,and the generated images sometimes looks terrible 
![image](https://user-images.githubusercontent.com/35061171/41266485-a38eed4a-6e29-11e8-92fb-6aaa85ec438f.png)


How can I adjust some parameters to repair these problems?many thanks ",anyone come phenomenon size training configuration niter loss like sometimes terrible image adjust repair many thanks,issue,negative,negative,neutral,neutral,negative,negative
395977790,"The code assumes that you have images in both test directory. If you only want to test one direction,  please use ‘--dataset_mode single’ and ‘--model test’ options.  You can find an example script in readme.",code test directory want test one direction please use single model test find example script,issue,negative,negative,neutral,neutral,negative,negative
395938108,I have never seen it before. Maybe other people are running jobs on the same GPU. ,never seen maybe people running,issue,negative,neutral,neutral,neutral,neutral,neutral
395938081,"Have you fixed the issue? If not, you can also download it manually. The datasets are [here](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/).",fixed issue also manually,issue,negative,positive,neutral,neutral,positive,positive
395937541,"For your questions: 
1. Pytorch should work in train mode by default. 
2. Currently `.eval()` is not being used. I added it just in case others would like to use it. It should not affect CycleGAN model at all as CycleGAN has no dropout and batchnorm. It might affect pix2pix model. But we found that pix2pix can produce more diverse results when using dropout during the test time. You are free to try it for your own test code.",work train mode default currently used added case would like use affect model dropout might affect model found produce diverse dropout test time free try test code,issue,positive,positive,positive,positive,positive,positive
395937281,I am not sure if this issue is related to our repo. Are you able to run some simple PyTorch [demos](https://github.com/pytorch/examples)? ,sure issue related able run simple demo,issue,negative,positive,positive,positive,positive,positive
395936826,"Overfitting will happen if the size of the dataset is small. We usually train 200K iterations for most of our experiments. You may want to apply more data [argumentation](https://github.com/mdbloice/Augmentor) (e.g., cropping, changing the color, etc.) if your training set is small. ",happen size small usually train may want apply data argumentation color training set small,issue,negative,negative,negative,negative,negative,negative
394091814,"""--gpu_ids -1"" doesn't work on mac sadly...

I tried --gpu -1 but I still get the same error. 
",work mac sadly tried still get error,issue,negative,negative,negative,negative,negative,negative
393410098,I think CycleGAN takes more than 4G for 256x256 images. Try 128x128 instead. Use `--which_model_netG resnet_128 --loadSize 143 --fineSize 128`,think try instead use,issue,negative,neutral,neutral,neutral,neutral,neutral
393372071,run python -m visdom.server，then run your code,run python run code,issue,negative,neutral,neutral,neutral,neutral,neutral
393032223,It can be real-time during the test time. It depends on the input image size and your GPUs.,test time input image size,issue,negative,neutral,neutral,neutral,neutral,neutral
392923538,"Hi, 
Since there's no pretrained model for sat2map, I'm using default params to train sat2map by running 
`python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --no_dropout`
as suggested in README.

The generated images don't seem to have the quality it should at epoch 100+. Here's a generated image at epoch 151. Am I missing something? Thank you!

(epoch151_fake_A.png, epoch151_fake_B.png)
![epoch151_fake_a](https://user-images.githubusercontent.com/3687000/40682134-d9eb8c58-633f-11e8-86d0-ca4897b7cadd.png)
![epoch151_fake_b](https://user-images.githubusercontent.com/3687000/40682156-e5a7d588-633f-11e8-9a78-d86be5b60527.png)


",hi since model default train running python name model seem quality epoch image epoch missing something thank,issue,negative,negative,negative,negative,negative,negative
391740007,"I have run some independent example for the visdom server, the problem comes from using a http proxy with visdom. Sorry for the noise.",run independent example server problem come proxy sorry noise,issue,negative,negative,negative,negative,negative,negative
391694026,"I tried ssh tunnelling to avoid possible firewall problems, but still encounter the same problem:

<img width=""1435"" alt=""screen shot 2018-05-24 at 14 18 38"" src=""https://user-images.githubusercontent.com/8866751/40484968-806fc63e-5f5d-11e8-8c3b-e76a7eec2ef6.png"">

Here is the initial output I get when I start the visdom server with python -m visdom.server

```
Downloading scripts. It might take a while.
It's Alive!
INFO:root:Application Started
You can navigate to http://localhost:8097
INFO:tornado.access:200 GET /vis_socket (127.0.0.1) 78.57ms
```

Do you have any other ideas what the problem could be?",tried avoid possible still encounter problem screen shot initial output get start server python might take alive root application navigate get problem could,issue,negative,positive,neutral,neutral,positive,positive
391231288,`Variable` is checked 3 lines right above your change. I don't think it is needed.,variable checked right change think,issue,negative,positive,positive,positive,positive,positive
391208417,It would be useful if you can tell us the specs of your computer.,would useful tell u spec computer,issue,negative,positive,positive,positive,positive,positive
391208051,Haven't got this error before. Let us know if you figure it out.,got error let u know figure,issue,negative,neutral,neutral,neutral,neutral,neutral
391207952,"Also, use `epoch_count`. See this [issue](#85) for more details.",also use see issue,issue,negative,neutral,neutral,neutral,neutral,neutral
391206767,I don't see a big difference regarding the order of G and D updates. ,see big difference regarding order,issue,negative,neutral,neutral,neutral,neutral,neutral
391206684,This is not normal. Could you first try to train a pix2pix model on a given dataset (e.g. facades) and then try to adapt it to your own dataset? Also try batchSize=1 first.  ,normal could first try train model given try adapt also try first,issue,negative,positive,positive,positive,positive,positive
391186252,"No. I don’t think there is particular reason as both approaches are
implemented in practice. Note that in both cases D is updated with same
gradients. Perhaps by updating G first it is less likely to be dominated by
D. But likely You can use either.

On Tue, May 22, 2018 at 20:48 Weini <notifications@github.com> wrote:

> Thank you!
>
> On point 2 - is there a reason why cycle GAN updates generator first while
> pix2pix updates discriminator first? - Is this also answered by LSGAN? I
> skimmed through the paper and did not find the updating strategy mentioned.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/269#issuecomment-391185668>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFaWZdRwBQr_QGmHf2GZg2YRjlHwq_wzks5t1LH4gaJpZM4T9KOd>
> .
>
",think particular reason practice note perhaps first le likely dominated likely use either tue may wrote thank point reason cycle gan generator first discriminator first also skimmed paper find strategy reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
391185668,"Thank you!

On point 2 - is there a reason why cycle GAN updates generator first while pix2pix updates discriminator first? - Is this also answered by LSGAN? I skimmed through the paper and did not find the updating strategy mentioned.
",thank point reason cycle gan generator first discriminator first also skimmed paper find strategy,issue,negative,positive,positive,positive,positive,positive
391110945,"Also might be a problem if you have old GPU, or GPU with suboptimal CUDA version, or PyTorch install with wrong CUDA version.",also might problem old suboptimal version install wrong version,issue,negative,negative,negative,negative,negative,negative
391110805,after that it enters the training loop. maybe it's just training too slow for you.,training loop maybe training slow,issue,negative,negative,negative,negative,negative,negative
391110326,"did you start the visdom server on your remote machine, and ssh tunnel to it?",start server remote machine tunnel,issue,negative,negative,neutral,neutral,negative,negative
390520331,"You can measure L1 loss for both training and validation data, and see if there is a big gap.",measure loss training validation data see big gap,issue,negative,neutral,neutral,neutral,neutral,neutral
390520292,Cool. The current version only works with PyTorch <=0.3.1. We will make it compatible with PyTorch 0.4 soon.,cool current version work make compatible soon,issue,negative,positive,positive,positive,positive,positive
389789648,Sorry if this is a dumb question but what values would you expect to see for L1 loss if the model is overfitting? ,sorry dumb question would expect see loss model,issue,negative,negative,negative,negative,negative,negative
389504544,"Thanks for your quick reply @phillipi and I have another question that when I training on pix2pix, the loss_D_fake, loss_D_real and loss_G_GAN are equal to 0, is this a normal phenomenon and why does this happen?",thanks quick reply another question training equal normal phenomenon happen,issue,negative,positive,positive,positive,positive,positive
389359518,"In my experience the important thing is to have the test setting exactly match the train setting. One way to do this is to use the same batch size at both train and test. Another way would be instance norm. Or “virtual batch norm”. Or anneal the contribution of the current iter's stats to zero, etc. As long as train and test conditions are iid, you will likely be in good shape!",experience important thing test setting exactly match train setting one way use batch size train test another way would instance norm virtual batch norm anneal contribution current iter zero long train test likely good shape,issue,positive,positive,positive,positive,positive,positive
387223502,"It is currently used during the test phase. Sorry for the naming. For pix2pix, L1 loss can be used to detect overfitting.",currently used test phase sorry naming loss used detect,issue,negative,negative,negative,negative,negative,negative
386262751,"go test_options.py and set
 self.parser.add_argument('--how_many', type=int, default=200, help='how many test images to run')",go set many test run,issue,negative,positive,positive,positive,positive,positive
385842927,"Dear @junyanz ,Now it is working by following issue #254: 
Weights are in CPU format",dear working following issue format,issue,negative,neutral,neutral,neutral,neutral,neutral
385399587,"@HugoGranstrom ,  Dear, Thank you so much for your help.  I get it now!",dear thank much help get,issue,positive,positive,positive,positive,positive,positive
385391572,"@yifanw90 I got them mixed up above as well, sorry. I have edited my answer now.
D_A tells you how likely it is for an image to be a zebra and D_B tells you how likely it is that an image is a horse. Not the other way around as I first wrote above. So the code in ""cycle_gan_model.py"" is correct. 
Here is the code for D_A:
`def backward_D_A(self):`
`    fake_B = self.fake_B_pool.query(self.fake_B)`
`    self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)`
It compares the real B and the fake B images.

G_A(A) -> fake B image -> D_A(fake B image)  -> does it look like B? Yes/No.

It comes down to a matter of taste which Discriminator you name what. It could have been the other way around. ",got mixed well sorry answer likely image zebra likely image horse way around first wrote code correct code self real fake fake image fake image look like come matter taste discriminator name could way around,issue,negative,negative,negative,negative,negative,negative
385386784,"@HugoGranstrom   Dear, Sincerely thank you for your detailed explanation.  :)
So  I think the original code in Line 135 and 139 in the""cycle_gan_model.py"" should be modified as  ""self.loss_G_A = self.criterionGAN(self.netD_B(self.fake_B), True)"" and  ""self.loss_G_B = self.criterionGAN(self.netD_A(self.fake_A), True)"", respectively.  Am I right?",dear sincerely thank detailed explanation think original code line true true respectively right,issue,positive,positive,positive,positive,positive,positive
385356373,"I think it has to do with that Generator A takes an image from A as input and outputs an image that is supposed to look like the images from B. Discriminator A is trained to evaluate how much an image looks like an image from B.
Ex horse2zebra: 
A are images of horses and B are images of zebras. D_A trains on the zebra images to be able to determine if a picture is a horse, D_B train on horse pictures. G_A takes horse images as input and output an image that is supposed to look like a zebra image. So G_A is trying to convert an image from A to B (horse to zebra). To see how much the generated ""zebra image"" looks like a zebra we let D_A evaluate how much it looks like a zebra. We can't let D_B determine that because it just tells you how likely it is for an image to be a horse which we aren't interested in because we generated a zebra image. 

I hope this cleared up some things at least. If I was unclear about something or this didn't make sense tell me and I will try another way of saying it :)",think generator image input image supposed look like discriminator trained evaluate much image like image ex zebra able determine picture horse train horse horse input output image supposed look like zebra image trying convert image horse zebra see much zebra image like zebra let evaluate much like zebra ca let determine likely image horse interested zebra image hope least unclear something make sense tell try another way saying,issue,positive,positive,positive,positive,positive,positive
385304520,"@junyanz ,
I try with torch version=0.3.1.post2 and Anaconda3(python 3.6.4).I am looking forward to get next instruction.
",try torch post anaconda python looking forward get next instruction,issue,negative,neutral,neutral,neutral,neutral,neutral
385259139,Which PyTorch version are you using? The current one only works with PyTorch 0.3. The 0.4 support will be added soon.,version current one work support added soon,issue,negative,neutral,neutral,neutral,neutral,neutral
385175196,"""Note: The current software works well with PyTorch 0.1-0.3. PyTorch 0.4 support will be added by the end of May.""
It currently does not work with Pytorch 0.4, you need to install Pytorch 0.3.1, preferably in a virtual environment so that you can have both versions installed at the same time. ",note current work well support added end may currently work need install preferably virtual environment time,issue,positive,neutral,neutral,neutral,neutral,neutral
384731673,"The difference is pretty small in our experiments. With dropout, the output is stochastic, and you will get slightly different results each time you run the code. ",difference pretty small dropout output stochastic get slightly different time run code,issue,negative,neutral,neutral,neutral,neutral,neutral
384527712,"I cannot reproduce your issue. The following command for horse2zebra works for me: 
```bash
python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan --no_dropout --display_port 2005 --gpu_ids 0,1 --batchSize 4
```
You may have accidentally removed this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L63). If I remove  `.cuda(gpu_ids[0]`, I can get errors you got.  ",reproduce issue following command work bash python name model may accidentally removed line remove get got,issue,negative,neutral,neutral,neutral,neutral,neutral
384524840,I think we just assigned it to its nearest neighbor in color space. @tinghuiz @phillipi ,think assigned nearest neighbor color space,issue,negative,neutral,neutral,neutral,neutral,neutral
384524651,"Also,  what is your Pytorch version? The current code only works with PyTorch 0.1-0.3, not 0.4",also version current code work,issue,negative,neutral,neutral,neutral,neutral,neutral
384524175,"Not sure if you are referring to the right equation. This is the equation 3 in pix2pix paper. 
![image](https://user-images.githubusercontent.com/1924757/39288525-57f78e12-48f6-11e8-9963-bf5878199d28.png)
For your questions, I think it is an interesting idea and you are free to try it. It depends on your particular setup. If two tasks are similar to each other, it might help. 
",sure right equation equation paper image think interesting idea free try particular setup two similar might help,issue,positive,positive,positive,positive,positive,positive
384520050,"I just ran CycleGAN with a different dataset. I figured out the solution.. stopping and rebooting the EC2 instance solved it. 

Thanks :)",ran different figured solution stopping instance thanks,issue,positive,positive,neutral,neutral,positive,positive
384513809,"Did you use a custom architecture by any chance, or just run CycleGAN with a different dataset? Does it happen for only a particular image?

Good luck with CS280! ;-)",use custom architecture chance run different happen particular image good luck,issue,positive,positive,positive,positive,positive,positive
384513511,We fixed the issue(ea3b646) following @Naruto-Sasuke 's suggestion. ,fixed issue following suggestion,issue,negative,positive,neutral,neutral,positive,positive
384513425,"We fixed the issue (ea3b646) following your suggestion. We will investigate the multigpu case as well. 
",fixed issue following suggestion investigate case well,issue,negative,positive,neutral,neutral,positive,positive
384511015,It looks like a cuda backend issue to me. I haven't had this problem. Have you tried reinstalling cuda and pytorch?,like issue problem tried,issue,negative,neutral,neutral,neutral,neutral,neutral
384510742,"It does not result in a big difference. 

With higher identity loss, the image translation becomes more conservative, so it makes less changes. I tried using values like 0.1, 0.5, 1 and 10, and the result is not so different. I believe this is because the generator figures out essentially two different translation functions by detecting whether the input image is from the source dataset or the target dataset. ",result big difference higher identity loss image translation becomes conservative le tried like result different believe generator essentially two different translation whether input image source target,issue,negative,positive,neutral,neutral,positive,positive
384510381,"What kind of bad results did you get? Did you see flipping of color? I sometimes see that the color flips depending on initialization. To resolve this, you can 

1. just start it multiple times until you get lucky and there is no color flipping. 
2. reduce `--init_variance`. 
3. add identity loss, at least in the first epoch of the training. ",kind bad get see color sometimes see color depending resolve start multiple time get lucky color reduce add identity loss least first epoch training,issue,negative,positive,neutral,neutral,positive,positive
384509967,"You can specify `--resize_or_crop none`. However, you will get an error if an input image has size that is not a multiple of 4, because the default resnet generator changes the image shape after downsamplings and upsamplings. To avoid that, you can add your custom if clause in `get_transform` function of `base_dataset.py` to resize the image to be multiple of 4. ",specify none however get error input image size multiple default generator image shape avoid add custom clause function resize image multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
384155320,I just fixed the model saving/loading issue with CPU mode. Let me know if it works for you.,fixed model issue mode let know work,issue,negative,positive,neutral,neutral,positive,positive
384154739,"@junyanz  Sorry, I forgot to mention it, I used CPU using `--gpu_ids -1` ",sorry forgot mention used,issue,negative,negative,negative,negative,negative,negative
384153280,"It works with GPUs. I find it failed with CPU `--gpu_ids - 1`. I wonder if you are using GPUs or CPUs. If possible, could you paste the exact command line you are using?",work find wonder possible could paste exact command line,issue,negative,positive,positive,positive,positive,positive
384152414,It was fixed by the latest [commit](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/d7558eb269efdf181411e0b856df59647d916424). `git pull` and see if it works.,fixed latest commit git pull see work,issue,negative,positive,positive,positive,positive,positive
383790072,we don't have these models available. You can improve the results with additional identity loss.,available improve additional identity loss,issue,negative,positive,positive,positive,positive,positive
383436582,"python -m visdom.server python train.py
then I start python train.py
It start works!",python python start python start work,issue,negative,neutral,neutral,neutral,neutral,neutral
383358822,"It is possible. In the Monet's case, it should be fine. But I guess the performance might drop as you are using two completely different datasets (e.g. horse->zebra, and apple->orange) during one training session.",possible case fine guess performance might drop two completely different zebra orange one training session,issue,negative,positive,positive,positive,positive,positive
383358642,"This repo is for PyTorch implementations. Please post your questions on the Torch [pix2pix](https://github.com/phillipi/pix2pix/) repo. For your question, you need to set `which_model_netG= unet_128` for 128x128 inputs. You probably want to change the number of layers in the D as well. (`n_layers_D=2` )",please post torch question need set probably want change number well,issue,positive,neutral,neutral,neutral,neutral,neutral
382956011,"0.5 applies to both LSGAN and DCGAN. criterionGAN is a class for both objectives. 0.5 should make no big difference. In practice, we divide the objective by 2 while optimizing D, which slows down the rate at which D learns relatively to G. ",class make big difference practice divide objective slows rate relatively,issue,negative,neutral,neutral,neutral,neutral,neutral
382892762,@Hunterwolf88  this seems to be strange. Could you post both your command line and the errors?,strange could post command line,issue,negative,negative,neutral,neutral,negative,negative
382612801,"Thanks for the reply.
 Yes, I have read those discussions and it is very helpful for my realization of FCN-score, however I encounter problem with Classification Performance on  generated color segmentation labels that is how to convert those color label into gray label (or TrainId) easily.  The evaluation code provided in pix2pix repository  excludes the implementation of converting color segmentation label to gray label. And I am wondering if you could share some important details about the implementation and the related code would be better, for which  I will be very appreciated. 
Best, ",thanks reply yes read helpful realization however encounter problem classification performance color segmentation convert color label gray label easily evaluation code provided repository implementation converting color segmentation label gray label wondering could share important implementation related code would better best,issue,positive,positive,positive,positive,positive,positive
382568475,"added ""--display_id 0"" running train.py but I still get this error",added running still get error,issue,negative,neutral,neutral,neutral,neutral,neutral
381855878,"I installed PyTorch from source, then I met the same problem. I uninstalled the PyTorch, and I installed Pytorch using conda **conda install pytorch torchvision -c pytorch**, I ran it successfully.
",source met problem uninstalled install ran successfully,issue,negative,positive,positive,positive,positive,positive
381604457,"Thanks for the reply!
I ended up modifying the get_transform function so that it doesn't do the cropping, and resizes the images to the nearest multiple of 4.",thanks reply ended function nearest multiple,issue,negative,positive,neutral,neutral,positive,positive
381380521,Did you reduce the learning rate in the later stage of the training?,reduce learning rate later stage training,issue,negative,neutral,neutral,neutral,neutral,neutral
381380478,"I guess the padding/conv layers in the G will make height and width as multiples of 4. 
One thing you can do: 
- record the original image size 362x529
- resize your input to 364x332 by yourself. 
- feed it to the G 
- resize the output by 362x529. 
",guess make height width one thing record original image size resize input feed resize output,issue,negative,positive,positive,positive,positive,positive
381379988,I just updated the `test_model.py`. It should fix the memory issue. Let me know if it works for you. ,fix memory issue let know work,issue,negative,neutral,neutral,neutral,neutral,neutral
380275568,"@junyanz @avijit9 @IsDling 

You can test a 2820x1880 image on a 8GB GPU so I'm confident you could do that on a 12GB GPU. I have gotten resolutions of at least 4800x3,200 on a P6000 with 24gb of ram

the trick is, for test.py use ```--model cycle_gan --dataset_mode unaligned``` instead of ```--model test --dataset_mode single```

using ```--model test``` appears to take more VRAM and thus can only run at a lower resolution

see issue: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/236",test image confident could gotten least ram trick use model unaligned instead model test single model test take thus run lower resolution see issue,issue,negative,positive,neutral,neutral,positive,positive
380273123,"@lxj616 when you use CPU mode, it is unable to perform as many calculations in parallel compared with a GPU and thus has to hold more in memory",use mode unable perform many parallel thus hold memory,issue,negative,neutral,neutral,neutral,neutral,neutral
380271132,"@charan223 @Matlmr 

As @junyanz said, you can increase the batch size to a higher number and find the limits of how high you can go to start a test.

Alternatively, I have typically trained at higher resolutions for a better result and more RAM utilization. at some point though, increasing resolution doesn't necessarily gain that much, and I have actually gotten better results with ```batchSize=1```",said increase batch size higher number find high go start test alternatively typically trained higher better result ram utilization point though increasing resolution necessarily gain much actually gotten better,issue,positive,positive,positive,positive,positive,positive
380270292,"@botaichang  @junyanz 

I have been able to generate test images at 2800x1880 using a 1070 (8gb VRAM) using --model cycle_gan --dataset_mode unaligned instead of --model test --dataset_mode single

using --model test appears to take more VRAM and thus can only run at a lower resolution

see issue: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/236",able generate test model unaligned instead model test single model test take thus run lower resolution see issue,issue,negative,positive,positive,positive,positive,positive
380269734,"@u10116032 @junyanz 

I have been able to generate test images at 2800x1880 using a 1070 (8gb VRAM) for test.py use ```--model cycle_gan --dataset_mode unaligned``` instead of ```--model test --dataset_mode single```

using ```--model test``` appears to take more VRAM and thus can only run at a lower resolution

see issue: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/236",able generate test use model unaligned instead model test single model test take thus run lower resolution see issue,issue,negative,positive,positive,positive,positive,positive
379624171,"See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/87) for how to resume your previous training. To see the results, you can either see the saved results in the HTML file or run `test.py` code.",see resume previous training see either see saved file run code,issue,negative,negative,negative,negative,negative,negative
379539815,"@icbcbicc @botcs  ur answer works, thank you very much! 
  `pip uninstall torchvision` 
  `git clone https://github.com/pytorch/vision`
  `cd vision`
 `python setup.py install`",ur answer work thank much pip git clone vision python install,issue,positive,positive,positive,positive,positive,positive
379134078,You need to rename it to `latest_net_G.pth` manually. I am thinking about adding a new flag `checkpoint_path` so that people can specify the path directly. I will add it once I have time. ,need rename manually thinking new flag people specify path directly add time,issue,negative,positive,positive,positive,positive,positive
378852779,You can also set the --display_id 0 if you don't want to use visdom,also set want use,issue,negative,neutral,neutral,neutral,neutral,neutral
378799012,"@junyanz 
In case ll, I tried below command,
`python test.py --dataroot database/trainA --checkpoints_dir ./checkpoints/ --name AtoB --no_dropout --model test --dataset_mode single --loadSize 256
`
then,
`FileNotFoundError : : [Errno 2] No such file or directory: './checkpoints/AtoB/latest_net_G.pth'`

because I don`t have latest_net_G.pth file.
I have only **latest_net_D_A.pth , latest_net_D_B.pth
latest_net_G_A.pth and latest_net_G_B.pth** files.
",case tried command python name model test single file directory file,issue,negative,negative,neutral,neutral,negative,negative
378783982,"It depends. 
- case I (apply both G_A and G_B). See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#cyclegan-traintest).  Assume that you have training data under `../dataset/`, you can add your new dataset as a subdirectory (e.g.,  `../dataset/valA`  and `../dataset/valB`)  and set the phase as `--phase val`.
- case II (only apply one model G_A). See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) for more details about how to apply a single pretrained model.  
",case apply see assume training data add new set phase phase case apply one model see apply single model,issue,negative,positive,neutral,neutral,positive,positive
378144857,"@YushengZhang
 start the visdom server before starting the training:
`python -m visdom.server` 
It works! Thanks a lot!",start server starting training python work thanks lot,issue,negative,positive,neutral,neutral,positive,positive
378112128,I am not sure if our program can run on a CPU with 2G memory. I recommend a GPU with more than 6 GB memory.  ,sure program run memory recommend memory,issue,positive,positive,positive,positive,positive,positive
377854703,"To me, it looks like you are using the wrong model. You appear to be testing the photo generator on *paintings* (the 'normal' use case), so it is generating photos (if your paintings are A and your photos are B, G<sub>B</sub>(a) ≈ b). It's perfectly acceptable to see some minor colour differences between paintings and photos.

However, the identity loss is the loss between the result of the photo generator on *photos* and the identity function (G<sub>B</sub>(b) ≈ b). So, unless I've misunderstood and you're already doing so, try passing photos through the photo generator - the thing that comes out should be more or less exactly the thing that comes in, which is the definition of the identity function.

Also, it's entirely possible I misunderstood the problem. May I see the code you used to get those images?",like wrong model appear testing photo generator use case generating sub perfectly acceptable see minor colour however identity loss loss result photo generator identity function sub unless misunderstood already try passing photo generator thing come le exactly thing come definition identity function also entirely possible misunderstood problem may see code used get,issue,negative,positive,positive,positive,positive,positive
377784632,"Finally I have found the answer!
""start the visdom server before starting the training

python -m visdom.server
""
And the code can run.",finally found answer start server starting training python code run,issue,negative,neutral,neutral,neutral,neutral,neutral
377781415,"Not yet,I'm going to python another CycleGAN code",yet going python another code,issue,negative,neutral,neutral,neutral,neutral,neutral
377766163,I also meet this problem . Do you solve it ? ,also meet problem solve,issue,negative,neutral,neutral,neutral,neutral,neutral
377478616,"@junyanz Is that because of overfitting? The generator network overfit the distribution of target domain, so that  the result become blurry? thanks!",generator network overfit distribution target domain result become blurry thanks,issue,negative,positive,positive,positive,positive,positive
377358074,"I think the issue occurred because of attempted installations of pytorch via conda as well as via via pip from a wheel. I cleaned everything up and got it working again. Thanks. 
I am now on I am on Ubuntu 16.04, pytorch 3.1, cuda 9.0 cuDnn 7.0.4",think issue via well via via pip wheel everything got working thanks,issue,positive,positive,positive,positive,positive,positive
376764727,"thanks for your reply, I can reach good result in my project as well
![14](https://user-images.githubusercontent.com/12381290/38010082-fc799904-3289-11e8-9e9c-d5d8017c8818.PNG)
The result is training after 83000 steps, and it pretty good I think, but when it reach to 130000 steps, the result is become  blurry and twisty, is that normal or just in my case?",thanks reply reach good result project well result training pretty good think reach result become blurry twisty normal case,issue,positive,positive,positive,positive,positive,positive
376727258,Will try...it could also be shared memory issue. I had made this work in my windows machine a few months ago.,try could also memory issue made work machine ago,issue,negative,neutral,neutral,neutral,neutral,neutral
376711612,Not sure if this is related to our code. This [post](https://github.com/pytorch/pytorch/issues/2714) might be related. You may also want to try to run some PyTorch [examples](https://github.com/pytorch/examples/tree/master/dcgan).,sure related code post might related may also want try run,issue,negative,positive,positive,positive,positive,positive
376688744,"For grayscale images, you may want to set `--input_nc 1 --output_nc 1`. For the intermediate features, you probably need to implement register_forward_hook function and add names to each module in the Unet. ",may want set intermediate probably need implement function add module,issue,negative,neutral,neutral,neutral,neutral,neutral
376686995,"I think it is due to weights_init [function](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/0ae4f0500e415a6a67689ef9356e8e4779ae5833/models/networks.py#L27). We searched for keywords ""Conv"". You may want to modify the weights_init functions or change the name of your layer. ",think due function may want modify change name layer,issue,negative,negative,negative,negative,negative,negative
376686347,It's hard to tell without reading your code line by line. Harry provided a good Tensorflow [implementation](https://github.com/leehomyc/cyclegan-1). Maybe you want to compare the difference between his and yours. ,hard tell without reading code line line harry provided good implementation maybe want compare difference,issue,negative,positive,positive,positive,positive,positive
375975250,"Hi,
I wondered if there was any progress on adding pretrainined models for photo 2 monet/cezanne/vangogh.  I have trained cyclegan myself with the default parameters, but wasn't 100% happy with the results and would like to see the results of your models for comparison!
Regards,
Alex",hi progress photo trained default happy would like see comparison,issue,positive,positive,positive,positive,positive,positive
374946056,"Hi @visonpon,  Thank you for trying it out!!   It's great to be able to check my code with someone!

I think what might be wrong is the `vmin=-25, vmax=25` in the `plt.imshow()` on the third line from the bottom. I shouldn't have included those limits, since they are specific to the facades network I trained.  I wanted to standardize the colour map range to be the same for all of the filters. 

For getting the correct max and min of the 0th channel, I think this works (though there may be a shorter command):
```python
np.min(np.array(np.floor(net[""model.model.0.weight""][:,0,:,:]*255)))
np.max(np.array(np.floor(net[""model.model.0.weight""][:,0,:,:]*255)))
```
If you adjust the `vmax` and `vmin`, can you see non-white images?

",hi thank trying great able check code someone think might wrong third line bottom included since specific network trained standardize colour map range getting correct min th channel think work though may shorter command python net weight net weight adjust see,issue,positive,positive,positive,positive,positive,positive
374861610,"Hey, 

in networks.py you're initializing all the weights this way:
```
def weights_init_normal(m):
    classname = m.__class__.__name__
    # print(classname)
    if classname.find('Conv') != -1:
        init.normal(m.weight.data, 0.0, 0.02)
    elif classname.find('Linear') != -1:
        init.normal(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm2d') != -1:
        init.normal(m.weight.data, 1.0, 0.02)
        init.constant(m.bias.data, 0.0)
```

Thus you're looking for all entries with ""conv"" .. you could rename `UpsampleConvLayer` to `UpsampleConLayer`. Should work :) - Hope that helps! ",hey way print thus looking could rename work hope,issue,negative,neutral,neutral,neutral,neutral,neutral
374825247,"@canghel  I have tried your script , but the images are all white and have nothing,  hope there are some process in your script~ thanks~",tried script white nothing hope process,issue,negative,neutral,neutral,neutral,neutral,neutral
374399433,@javl I'm actually not sure - I've never used `loadSize` and `fineSize` while training; only while generating the images. I just wrote a super quick python/cv2 script to chunk up the images into random 256/256 crops for me beforehand.,actually sure never used training generating wrote super quick script chunk random beforehand,issue,positive,positive,positive,positive,positive,positive
374398239,"@robbiebarrat Thank you for your clarification. I've tried all kinds of data sets and though some have given interesting results, I keep struggling  to fine tune and direct them towards the output I have in mind. Might be time to start working on some sort of cycleGAN/pix2pix tips and tricks cheatsheet.

Quick related question: I understand using `fineSize` during training crops your image. Does this always results in the same crop or is the position randomized? If I have an image that is 512x512 for instance, and I use `loadSize=512 fineSize=128` does the network always only gets to see the center part? (meaning I should split the image into something like 4 256x256 images beforehand if I want the network to see more parts of the image).",thank clarification tried data though given interesting keep struggling fine tune direct towards output mind might time start working sort quick related question understand training image always crop position image instance use network always see center part meaning split image something like beforehand want network see image,issue,positive,positive,positive,positive,positive,positive
374394265,"@javl wait - sorry if I wasn't clear; I have used tiling in combination with changing the `loadSize` and `fineSize` in the past to get alright results; just train the network to generate 256x256 unresized chunks of your images, and then after training ask it to generate a larger image (with `loadSize`/`fineSize`), and since it's learned larger features from the chunks, it should be able to generalize to a much higher resolution image.",wait sorry clear used tiling combination past get alright train network generate training ask generate image since learned able generalize much higher resolution image,issue,positive,positive,neutral,neutral,positive,positive
374391613,"@robbiebarrat Sorry, thought you might have a different way (like how some use tiling). Thanks.",sorry thought might different way like use tiling thanks,issue,positive,negative,neutral,neutral,negative,negative
374390901,"@javl just add the arguments `--loadSize 1024 --fineSize 1024` to the command you use to generate images. (replacing 1024 with your desired resolution, of course)

I got mediocre results this way, though.",add command use generate desired resolution course got mediocre way though,issue,negative,negative,negative,negative,negative,negative
374388009,"@robbiebarrat Do you have an example of the workflow you use for the second part? I can imagine the first part using some random crops from the source images, but what would be a good way to generate an HD image after?",example use second part imagine first part random source would good way generate image,issue,negative,positive,positive,positive,positive,positive
373974390,That makes sense. Let me know if this [commit](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/6718e13d5ded5f630cba3a8eab7e29fc7a7455e2) will fix the issue. @hazirbas @SsnL ,sense let know commit fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
373951790,"Hi there,
Thanks for sharing the code, when I try to run it an error message is displayed as follows
AttributeError: 'UpsampleConvLayer' object has no attribute 'weight'

not sure how to resolve that",hi thanks code try run error message displayed object attribute sure resolve,issue,positive,positive,positive,positive,positive,positive
373882149,"Yeah, that’s possible. Maybe we can switch to first splitting and them
upsampling.

On Fri, Mar 16, 2018 at 20:29 Jun-Yan Zhu <notifications@github.com> wrote:

> Interesting. What is the size of your original image? What is the loadSize
> in your experiment? @SsnL <https://github.com/ssnl> might know the
> solution.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/222#issuecomment-373878964>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFaWZejqy8HsiF0-RHLYdwoQV9IFDNaAks5tfFjqgaJpZM4SpxFo>
> .
>
",yeah possible maybe switch first splitting mar wrote interesting size original image experiment might know solution reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
373878964,Interesting. What is the size of your original image? What is the loadSize in your experiment? @SsnL might know the solution.,interesting size original image experiment might know solution,issue,positive,positive,positive,positive,positive,positive
373878870,"I am not sure if a 12GB GPU can fit 3000x2300 image during the test time. If you just want to work on 1k image, you can set loadSize=1024, fineSize=256 during training, and use loadSize=1024, fineSize=1024 during test.",sure fit image test time want work image set training use test,issue,positive,positive,positive,positive,positive,positive
373878607,"Yes, we first load an image with loadSize and then crop a patch with fineSize. You can modify the data loader to support multiple patches cropping.  ",yes first load image crop patch modify data loader support multiple,issue,positive,positive,positive,positive,positive,positive
373878473,"We currently use `batchSize=1`. You can use a larger batchSize, which also depends on the size of your GPU memory. ",currently use use also size memory,issue,negative,neutral,neutral,neutral,neutral,neutral
373371221,I think my problem was different filetypes. Hope I'm right. ,think problem different hope right,issue,negative,positive,positive,positive,positive,positive
373349533,"I think the folder structure is unclear, ""dataset"" and ""datasets"". I'm trying to figure this out. @SimpleXP  can you mention the issue you encountered?
",think folder structure unclear trying figure mention issue,issue,negative,neutral,neutral,neutral,neutral,neutral
372711881,I have the same problem of memory consumption. Did you figure anything out?,problem memory consumption figure anything,issue,negative,neutral,neutral,neutral,neutral,neutral
372161890,"@FilippoGrazioli you could also try training using small patches of images, but then when you're testing it generate an HD image. That's been working for me so far.",could also try training small testing generate image working far,issue,negative,negative,neutral,neutral,negative,negative
372092557,"@avijit9 I solve that with that add padding to make the image to 3504*3504(not resize!) And then, when train, set   loadSize 3504 and fineSize 256, when test ,crop the image in order to several 256*256 small image ,set loadSize 256 and fineSize 256,then combine the result in order. The **import** is your dataset must be aligned, result will not be  ideal if not. May help you!",solve add padding make image resize train set test crop image order several small image set combine result order import must result ideal may help,issue,positive,positive,positive,positive,positive,positive
371814605,thanks for help.And Can different loadSize and fineSIze change the number of patches from per image?It's that one patch from per image however the setting of loadSize and fineSize?,thanks different change number per image one patch per image however setting,issue,negative,positive,neutral,neutral,positive,positive
371643538,One patch per image. ,one patch per image,issue,negative,neutral,neutral,neutral,neutral,neutral
371007381,"thanks for your help. loadSize 286 and fineSize 256,how can I change to get the result 3504*2336",thanks help change get result,issue,positive,positive,positive,positive,positive,positive
370404108,"Ok. Found my error. Had to change the filter dimensions:

```
class UnetSkipConnectionBlock(nn.Module):
    def __init__(self, outer_nc, inner_nc, input_nc=None,
                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):
        super(UnetSkipConnectionBlock, self).__init__()
        self.outermost = outermost
        self.innermost = innermost
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d
        if input_nc is None:
            input_nc = outer_nc
        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,
                             stride=2, padding=1, bias=use_bias)
        downrelu = nn.LeakyReLU(0.2, True)
        downnorm = norm_layer(inner_nc)
        uprelu = nn.ReLU(True)
        upnorm = norm_layer(outer_nc)

        if outermost:
            upconv = UpsampleConLayer(inner_nc * 2, outer_nc, kernel_size=3, stride=1, upsample=2)      
            #upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2,padding=1, bias=use_bias)
            down = [downconv]
            up = [uprelu, upconv, nn.Tanh()]
            model = down + [submodule] + up
        elif innermost:
            upconv = UpsampleConLayer(inner_nc, outer_nc, kernel_size=3, stride=1, upsample=2)
            #upconv = nn.ConvTranspose2d(inner_nc, outer_nc,kernel_size=4, stride=2,padding=1, bias=use_bias)
            down = [downrelu, downconv]
            up = [uprelu, upconv, upnorm]
            model = down + up
        else:
            upconv = UpsampleConLayer(inner_nc * 2, outer_nc, kernel_size=3, stride=1, upsample=2)
            #upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,kernel_size=4, stride=2,padding=1, bias=use_bias)
            down = [downrelu, downconv, downnorm]
            up = [uprelu, upconv, upnorm]

            if use_dropout:
                model = down + [submodule] + up + [nn.Dropout(0.5)]
            else:
                model = down + [submodule] + up

        self.model = nn.Sequential(*model)

    def forward(self, x):
        if self.outermost:
            return self.model(x)
        else:
            return torch.cat([x, self.model(x)], 1)
```",found error change filter class self super self outermost innermost type else none true true outermost model innermost model else model else model model forward self return else return,issue,negative,positive,positive,positive,positive,positive
370400185,"For anyone who searched 'Out of memory' to get here
1. At 256x256 resolution it needs 4010 MB exactly on GTX 745 (grow from 3800 MB to 4010 MB for one epoch, then stay still), if your only have 4GB gpu, try shutdown your x-server to reserve exactly 4GB gpu memory
2. A weird thing is that if you use cpu instead of gpu, it took 20GB+ memory to run on 256x256, I don't know what's wrong

So... I shutdown my x-server then provide full gtx745 4G gpu to train on 256x256, can not go further without a better gpu : (",anyone memory get resolution need exactly grow one epoch stay still try shutdown reserve exactly memory weird thing use instead took memory run know wrong shutdown provide full train go without better,issue,negative,positive,neutral,neutral,positive,positive
370350909,"Never mind, after re-reading the paper I think I misunderstood how simple PatchGAN is. I'll just alter PixelDiscriminator to use rectangular kernels, thanks.",never mind paper think misunderstood simple alter use rectangular thanks,issue,negative,positive,neutral,neutral,positive,positive
369952379,"Nice point! I missed that the generator is just a model with fully convolutional layers.
Thanks for your kindly help!!!!!",nice point generator model fully convolutional thanks kindly help,issue,positive,positive,positive,positive,positive,positive
368344839,It achieves similar results for CycleGAN. I would recommend that you use Dropout fox pix2pix to prevent overfitting. ,similar would recommend use dropout fox prevent,issue,negative,neutral,neutral,neutral,neutral,neutral
368278175,@IsDling just warp the train or test code in: if __name__=='__main__': will do the work.,warp train test code work,issue,negative,neutral,neutral,neutral,neutral,neutral
368239697,"Here is a visual receptive field calculator: https://fomoro.com/tools/receptive-fields/#

I converted the math into python to make it easier to understand: 
```python
def f(output_size, ksize, stride):
    return (output_size - 1) * stride + ksize

last_layer = f(output_size=1, ksize=4, stride=1)
# Receptive field: 4
fourth_layer = f(output_size=last_layer, ksize=4, stride=1)
# Receptive field: 7
third_layer = f(output_size=fourth_layer, ksize=4, stride=2)
# Receptive field: 16
second_layer = f(output_size=third_layer, ksize=4, stride=2)
# Receptive field: 34
first_layer = f(output_size=second_layer, ksize=4, stride=2)
# Receptive field: 70

print(first_layer)
```",visual receptive field calculator converted math python make easier understand python stride return stride receptive field receptive field receptive field receptive field receptive field print,issue,negative,neutral,neutral,neutral,neutral,neutral
368206274,@CaptainEven How do you solve this problem? Can you give me the code that have been changed and could run in windows. Thank you,solve problem give code could run thank,issue,negative,neutral,neutral,neutral,neutral,neutral
367731420,We haven't tried that. I guess you can move this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/train.py#L57) into the inner [forloop](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/train.py#L22). You can implement your new learning rate policy with lambda function [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L88).,tried guess move line inner implement new learning rate policy lambda function,issue,negative,positive,neutral,neutral,positive,positive
367729393,It helps reduce the boundary artifacts in the cyclegan training. See this twitter [discussion](https://twitter.com/karpathy/status/720642926070530049) for more details. ,reduce boundary training see twitter discussion,issue,negative,neutral,neutral,neutral,neutral,neutral
366908273,"Hi, Thanks for the response. Yes I am trying to implement a new one. I am not executing your code, rather I am writing my own based on your work. I am only focusing on CycleGAN. The dataset I am using is czenne2pic. My more direct question from you is, if it is possible to give a dictionary {'A': A, 'B': B} of datasets to pytorch dataloader function?  
Thank you once again!",hi thanks response yes trying implement new one code rather writing based work direct question possible give dictionary function thank,issue,positive,positive,positive,positive,positive,positive
366791909,"Hello!  I had the same question and I am not sure if this is right... I was trying to visualize the weights for the first layer of the G network.  For the facades dataset, the 4x4 weights do not seem to capture features, but for my dataset they seem to (e.g. show a gradient across the square or a smaller more intense middle square) .  I was having trouble following the UNet architecture in the code.  I'm not sure if I even have the correct layer. Does anybody know?

```
### PREAMBLE ##################################################################
import torch
import torch.nn as nn
import os

import numpy as np
import matplotlib.pyplot as plt
import cv2

# the path for the network to load
pathNets = '/home/Documents/placenta/pytorch-CycleGAN-and-pix2pix/checkpoints/facades_pix2pix'
fileToLoad = 'latest_net_G.pth'

### VISUALIZE FILTERS #########################################################
# load the network into the variable 'net'
net = torch.load(os.path.join(pathNets, fileToLoad))

## figuring out dimensions of filters, layers, etc.
for k, v in net.items():
	print(k)

# the filters are of size 3x4x4, and there are 64 of them
# I think this is for the first conv. layer?
print(net[""model.model.0.weight""].size())

# plot just one channel of every filter
for jj in range(64):
	# get jj-th filter, which is 3x4x4
	temp = np.floor((net[""model.model.0.weight""][jj,:,:,:])*255);
	# save the first (0th) channel in the variable 'img'
	img = np.zeros((4,4))
	img = temp[0,:,:].numpy()
	# plot grayscale image of that channel in the jj-th filter
	plt.figure() 
	plt.imshow(img, vmin=-25, vmax=25, cmap='gray')
	fig = plt.gcf()
	fig.savefig(""./filter""+str(jj)+"".png"")
```",hello question sure right trying visualize first layer network seem capture seem show gradient across square smaller intense middle square trouble following architecture code sure even correct layer anybody know preamble import torch import import o import import import path network load visualize load network variable net print size think first layer print net weight plot one channel every filter range get filter temp net weight save first th channel variable temp plot image channel filter fig,issue,positive,positive,positive,positive,positive,positive
366581964,My bad. It should be fixed by the latest commit. ,bad fixed latest commit,issue,negative,negative,neutral,neutral,negative,negative
366434369,Thanks that works. But then it's not using the GPU of course. Is there a way to do that? ,thanks work course way,issue,negative,positive,positive,positive,positive,positive
366085216,"It's possible to run CycleGAN on 1024p images. The model is fully convolutional. This means that even if you train a model on 360x360 images, you can apply the model directly (without patch by patch) on 1024 images. We use `--loadSize 1024 --fineSize 360` during training, and `--loadSize 1024 --fineSize 1024` during test. Not sure about 4096.",possible run model fully convolutional even train model apply model directly without patch patch use training test sure,issue,negative,positive,positive,positive,positive,positive
366084767,"Could you give us more details? Do you have issues with the existing data_loader, or are you trying to implement a new one?",could give u trying implement new one,issue,negative,positive,positive,positive,positive,positive
366083958,It seems that the issue is related to WIndows+PyTorch. This [post](https://medium.com/@mackie__m/running-a-cifar-10-image-classifier-on-windows-with-pytorch-9094e29089cd) might help you.,issue related post might help,issue,negative,neutral,neutral,neutral,neutral,neutral
364741141,"Thanks for getting back! Would you suggest to exchange the convTranspose2D
or do I need to do anything else? I'm not that familiar with the coding in
pytorch though. I'll have a try! Thanks!
",thanks getting back would suggest exchange need anything else familiar though try thanks,issue,positive,positive,positive,positive,positive,positive
364683512,"Here are three steps to load your own data. 
- change the parameters `--input_nc` and `--output_nc` to the number of channels in your input/output images.
- Write a custom data loader (It is easy as long as you know how to load your data with python). Here is [one](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/single_dataset.py) we wrote. 
- Add your data_loader to the file [custom_dataset_data_loader.py](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/custom_dataset_data_loader.py#L7)",three load data change number write custom data loader easy long know load data python one wrote add file,issue,negative,positive,positive,positive,positive,positive
364681266,"@beniroquai Maybe, you can try adept the code to unet. It’d be cool if it
also helps unet. If you do some experiment, please let me know the results.
:)

On Sat, Feb 10, 2018 at 12:28 beniroquai <notifications@github.com> wrote:

> Thanks! It's working way better now? Maybe stupid question, but Would it
> make sense to do the same in the unet?
>
> On 9 Feb 2018 20:35, ""Tongzhou Wang"" <notifications@github.com> wrote:
>
> > @beniroquai <https://github.com/beniroquai> that's right. Hope that it
> > helps! :)
> >
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <
> https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/64#issuecomment-364538852
> >,
> > or mute the thread
> > <
> https://github.com/notifications/unsubscribe-auth/AEJOuEo4EI6GOHR3zXebIt8U4ggeGOx3ks5tTJ36gaJpZM4OctGj
> >
> > .
> >
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/64#issuecomment-364673987>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFaWZV0w1lXfzsGgZrlgJBapM_yQ--KOks5tTdGogaJpZM4OctGj>
> .
>
",maybe try adept code cool also experiment please let know sat wrote thanks working way better maybe stupid question would make sense wang wrote right hope reply directly view mute thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
364673987,"Thanks!  It's working way better now? Maybe stupid question, but Would it
make sense to do the same in the unet?

On 9 Feb 2018 20:35, ""Tongzhou Wang"" <notifications@github.com> wrote:

> @beniroquai <https://github.com/beniroquai> that's right. Hope that it
> helps! :)
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/64#issuecomment-364538852>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AEJOuEo4EI6GOHR3zXebIt8U4ggeGOx3ks5tTJ36gaJpZM4OctGj>
> .
>
",thanks working way better maybe stupid question would make sense wang wrote right hope reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
364538240,"Wow that was quick! Thanks a lot. Did you plan to make a PR in this repo as well? So the only thing is to exchange the resnet code part, right? 
Will try it! Eager to see if this is solving my issues ;) ",wow quick thanks lot plan make well thing exchange code part right try eager see,issue,positive,positive,positive,positive,positive,positive
364536691,Is there any activity on this topic? I'm also interested to implement this feature - at least to get it work ;-) ,activity topic also interested implement feature least get work,issue,negative,negative,neutral,neutral,negative,negative
362451210,"It's not about single GPU... A tensor lives on either CPU memory or a GPU's memory. There is no such thing of having a tensor on multiple GPUs. You can always copy a tensor from one GPU to another though. Here we just put the tensor onto one of the GPUs. MultiGPU training is done via DataParallel.

Since our dataloader uses pinned memory, we can load asynchronously. Setting async to true avoids cuda synchronization. So it's mainly for the slight speed gain.",single tensor either memory memory thing tensor multiple always copy tensor one another though put tensor onto one training done via since pinned memory load setting true synchronization mainly slight speed gain,issue,positive,positive,neutral,neutral,positive,positive
362130620,"Thanks a lot. 

I'm hoping to see your upcoming works~!",thanks lot see upcoming,issue,negative,positive,positive,positive,positive,positive
362086636,We use `--batchSize 1` mainly due to GPU memory concern. You can train a model at a higher resolution. ,use mainly due memory concern train model higher resolution,issue,negative,positive,neutral,neutral,positive,positive
360783829,"If you want to use the visdom server, you should change the display_id to a non-zero value. To view training results and loss plots, run `python -m visdom.server` and click the URL http://localhost:8097. ",want use server change value view training loss run python click,issue,negative,neutral,neutral,neutral,neutral,neutral
360697388,"@junyanz Thanks for your reply, to have a stronger D I have set ndf to be 64, 128, 256, 512,but the problem still remains, are there any other skills I can try to make a stronger D?",thanks reply set problem still remains try make,issue,negative,positive,positive,positive,positive,positive
360694719,"@junyanz , did change the display_id to 0 but whenever i start the visdom server,it shows nothing,is there any specific folder i should run it into?should i do it after training?",change whenever start server nothing specific folder run training,issue,negative,neutral,neutral,neutral,neutral,neutral
360610851,Your lossG is a little bit small. Maybe you want to have a stronger D (you can set ndf). ,little bit small maybe want set,issue,negative,negative,negative,negative,negative,negative
360437878,"Maybe you should change you D structure to make it stronger, like deeper net",maybe change structure make like net,issue,negative,neutral,neutral,neutral,neutral,neutral
360338350,This is related to visdom. See this [post](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/24) for the fix. ,related see post fix,issue,negative,neutral,neutral,neutral,neutral,neutral
360336129,"Thanks,But i meet another problem..
<pre>
Traceback (most recent call last):
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/visdom/__init__.py"", line 261, in _send
    data=json.dumps(msg),
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/api.py"", line 112, in post
    return request('post', url, data=data, json=json, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/adapters.py"", line 508, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /events (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fdf70809cd0>: Failed to establish a new connection: [Errno 111] Connection refused',))
Exception in user code:
------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/visdom/__init__.py"", line 261, in _send
    data=json.dumps(msg),
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/api.py"", line 112, in post
    return request('post', url, data=data, json=json, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/adapters.py"", line 508, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /events (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fdf70809e10>: Failed to establish a new connection: [Errno 111] Connection refused',))
Exception in user code:
------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/visdom/__init__.py"", line 261, in _send
    data=json.dumps(msg),
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/api.py"", line 112, in post
    return request('post', url, data=data, json=json, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/adapters.py"", line 508, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /events (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fdf70809cd0>: Failed to establish a new connection: [Errno 111] Connection refused',))
(epoch: 1, iters: 200, time: 0.231) D_A: 0.278 G_A: 0.306 Cyc_A: 1.762 D_B: 0.362 G_B: 0.131 Cyc_B: 2.103 idt_A: 1.058 idt_B: 0.807 
Exception in user code:
------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/visdom/__init__.py"", line 261, in _send
    data=json.dumps(msg),
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/api.py"", line 112, in post
    return request('post', url, data=data, json=json, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""/home/moon/miniconda2/lib/python2.7/site-packages/requests/adapters.py"", line 508, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /events (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fdf70809850>: Failed to establish a new connection: [Errno 111] Connection refused',))
</pre>",thanks meet another problem recent call last file line file line post return request file line request return file line request resp prep file line send request file line send raise object establish new connection connection exception user code recent call last file line file line post return request file line request return file line request resp prep file line send request file line send raise object establish new connection connection exception user code recent call last file line file line post return request file line request return file line request resp prep file line send request file line send raise object establish new connection connection epoch time exception user code recent call last file line file line post return request file line request return file line request resp prep file line send request file line send raise object establish new connection connection,issue,negative,positive,neutral,neutral,positive,positive
360227926,"I am running into the same problem.

So are you saying that images with low-variance may lead to exploding gradients? Could some type of gradient clipping be used to avoid that? 

I've been trying to debug this, but it is hard to isolate the problem =/",running problem saying may lead could type gradient clipping used avoid trying hard isolate problem,issue,negative,negative,negative,negative,negative,negative
360163687,It might be related to the data loader. Could you try `--loadSize 143 --fineSize 128 --which_model_G resnet_6blocks`?,might related data loader could try,issue,negative,neutral,neutral,neutral,neutral,neutral
360112635,"I think your GPU doesn't have enough memory for the model training. Try to reduce the image size of training images (e.g., `--loadSize 128`, `--fineSize 128`)",think enough memory model training try reduce image size training,issue,negative,neutral,neutral,neutral,neutral,neutral
359552077,I'll try to come up with a PR to update that code when I have time :),try come update code time,issue,negative,neutral,neutral,neutral,neutral,neutral
359551337,"Found it! The problem was that each time it called display_visuals it loaded the entire html file and as it got bigger, this operation took more and more time. Solve the problem by disabling it (just need the images, not the html file).
Thank you!",found problem time loaded entire file got bigger operation took time solve problem need file thank,issue,negative,neutral,neutral,neutral,neutral,neutral
359257913,"Log the data loading time and display_visuals time in train.py. Let’s
identiy the issue first before changing anything.

On Sun, Jan 21, 2018 at 10:47 ShaniGam <notifications@github.com> wrote:

> Thank you @SsnL <https://github.com/ssnl>!
> Regarding the 0% - It seems like it does use the GPU but there are long
> pauses between each time. I guess there are some other operations that take
> time (saving images?). Do you have any ideas on how to make speed up the
> process?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/168#issuecomment-359257760>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFaWZVFyqcjldFh_34ST-E3GJ5swTzsuks5tM1v0gaJpZM4RDGqk>
> .
>
",log data loading time time let issue first anything sun wrote thank regarding like use long time guess take time saving make speed process reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
359257760,"Thank you @SsnL!
Regarding the 0% - It seems like it does use the GPU but there are long pauses between each time. I guess there are some other operations that take time (saving images?). Do you have any ideas on how to make speed up the process?",thank regarding like use long time guess take time saving make speed process,issue,positive,negative,neutral,neutral,negative,negative
359115974,"It should still show the first image of the batch. Could you check again? Also, visdom doesn't show anything for test.py.",still show first image batch could check also show anything,issue,negative,positive,positive,positive,positive,positive
359115739,"@oldgil  @LambdaWill  IIRC, there was a related DataParallel bug that is fixed between 0.2 and 0.3. Could you try to upgrade pytorch to 0.3?",related bug fixed could try upgrade,issue,negative,positive,neutral,neutral,positive,positive
359115432,"@ShaniGam `[python] <defunct>` are likely to be data loading processes. They are re-created at beginning of each epoch.

Also, data loading might become the bottleneck and the reason why you see 0% gpu-util. Try to increase `nThreads`.",python defunct likely data loading beginning epoch also data loading might become bottleneck reason see try increase,issue,negative,neutral,neutral,neutral,neutral,neutral
359115302,"@Naruto-Sasuke Also, since D is patchGAN, depending on the dataset, you may have patches that are just as likely to occur in both datasets.",also since depending may likely occur,issue,negative,neutral,neutral,neutral,neutral,neutral
359114802,"@qqqq7411 if you look at the PBT paper, they use Inception score to do param search for GANs. With CycleGAN / pix2pix, it is not necessarily true that one side is realistic photos.",look paper use inception score param search necessarily true one side realistic,issue,negative,positive,positive,positive,positive,positive
358855066,"Solving environment: failed

PackagesNotFoundError: The following packages are not available from current channels:

  - pytorch-gpu

Current channels:

  - https://conda.anaconda.org/anaconda/osx-64
  - https://conda.anaconda.org/anaconda/noarch
  - https://repo.continuum.io/pkgs/main/osx-64
  - https://repo.continuum.io/pkgs/main/noarch
  - https://repo.continuum.io/pkgs/free/osx-64
  - https://repo.continuum.io/pkgs/free/noarch
  - https://repo.continuum.io/pkgs/r/osx-64
  - https://repo.continuum.io/pkgs/r/noarch
  - https://repo.continuum.io/pkgs/pro/osx-64
  - https://repo.continuum.io/pkgs/pro/noarch
",environment following available current current,issue,negative,positive,neutral,neutral,positive,positive
358834544,"I see. It might be hard to apply grid search to GANs models as often times, there is no standard metric to look at it. See this [post](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/166) for more discussion on evaluation metrics. ",see might hard apply grid search often time standard metric look see post discussion evaluation metric,issue,negative,negative,negative,negative,negative,negative
358829234,@junyanz  Thanks for your reply.I want to use grid search to optimize the hyper-parameters.But I don't know if this will work in pix2pix.,thanks want use grid search optimize know work,issue,positive,positive,positive,positive,positive,positive
358663235,"If you install the pytorch with conda, you should use `conda install -c anaconda pytorch-gpu`.",install use install anaconda,issue,negative,neutral,neutral,neutral,neutral,neutral
358662410,"What do you mean ""grid search""? It is not included in this repo.",mean grid search included,issue,negative,negative,negative,negative,negative,negative
358546675,"@AllAwake Cool results! 

Here is the implementation of resize-conv I used. It remove the checkerboard artifacts during early training. You may find it useful.
```
                          nn.Upsample(scale_factor = 2, mode='bilinear'),
                          nn.ReflectionPad2d(1),
                          nn.Conv2d(ngf * mult, int(ngf * mult / 2),
                                             kernel_size=3, stride=1, padding=0),
```
It should replace the `ConvTranspose2d` in `ResnetGenerator`. ",cool implementation used remove checkerboard early training may find useful mult mult replace,issue,positive,positive,positive,positive,positive,positive
358528453,Sorry my bad. I forget to turn on visdom.server. The problem is now solved.,sorry bad forget turn problem,issue,negative,negative,negative,negative,negative,negative
358523365,"thank you Phillip, great to have the path forward - checking the links!
the original images in my training sets are pretty high res but i'm bounded by GTX 1080 -  both A and B DSs are around 1k each
i trained on loadSize=384, loadSize=1024 and fineSize=384, while testing on a higher res and was not that happy with the results, the original image being too pronounced...
so that's where i an now, training with loadSize=768, fineSize=384

",thank great path forward link original training pretty high bounded around trained testing higher happy original image pronounced training,issue,positive,positive,positive,positive,positive,positive
358505314,"Very cool! Care to share what the inputs look like too?

This [distill paper](https://distill.pub/2016/deconv-checkerboard/) talks about one of the causes of the checkerboard artifacts. You can fix that issue by switching from ""deconvolution"" to nearest-neighbor upsampling followed by regular convolution. I think @SsnL may have implemented this at some point.

We've also noticed that sometimes the checkboard artifacts go away if you simply train long enough. Maybe try training a bit longer.

Another cause of repetitive artifacts can be that the discriminator's receptive field is too small. For some discussion on this, please see Section 4.4 and Figure 6 of the [pix2pix paper](https://arxiv.org/pdf/1611.07004.pdf). The issue is that if the discriminator looks at too myopic a region, it won't notice that textures are repeating. I think this is probably not the case in your results, but it's something to keep in mind.",cool care share look like distill paper one checkerboard fix issue switching regular convolution think may point also sometimes go away simply train long enough maybe try training bit longer another cause repetitive discriminator receptive field small discussion please see section figure paper issue discriminator myopic region wo notice think probably case something keep mind,issue,positive,negative,neutral,neutral,negative,negative
358202363,"
My computer system is Mac OS 10.13.2 , I install pytorch by conda, if I want to use gpu to work，may be I should use the source to install the pytorch? did I have to install NVIDIA CUDA  + CUDA CuDNN ? 
how can I use cpu to work, did I need to edit the source?

",computer system mac o install want use use source install install use work need edit source,issue,negative,neutral,neutral,neutral,neutral,neutral
358124945,I wonder if you have installed PyTorch (GPU) correctly. Could you run PyTorch demo [code](https://github.com/pytorch/examples)?,wonder correctly could run code,issue,negative,neutral,neutral,neutral,neutral,neutral
357881306,"I print out the values of the output of D and find that no matter whether the input is fake or real, it always outputs a value around 0.5.   Then what can I do ?  Complicates the model by stacking more convs or simplify it. Or just update D more often?",print output find matter whether input fake real always value around model simplify update often,issue,negative,negative,negative,negative,negative,negative
357838273,Maybe you want to update D more often?,maybe want update often,issue,negative,neutral,neutral,neutral,neutral,neutral
357716293,"Thanks for reporting the experiments. I guess other generators (e.g., resnet_6blocks or resnet_9blocks) may support more image sizes. ",thanks guess may support image size,issue,positive,positive,positive,positive,positive,positive
357668451,"Thanks @junyanz and @phillipi  for your quick replies.
I have tested various combinations of test image sizes: 
1> While test images of size 512 X 512 ,1024 X 1024 and 1024 X 2048 works fine
2> Test images of size 640 X 640 or 640 X 1152 don't work and pop up with the underlined error :
[https://gist.github.com/PraveerSINGH/4dd6d113f1a86fbbf376cefef6147d9e](url)
3> Thus only test images of height and width which are multiples of 256 work with the current architecture.
",thanks quick tested various test image size test size work fine test size work pop error thus test height width work current architecture,issue,negative,positive,positive,positive,positive,positive
357630270,"Add a hint for those who searched 'time' to get here:
'It takes 3-4 hours for pix2pix training on facades datasets with a Nvidia GTX 1080 GPU.' 

which means that if you use a desktop nvidia gpu, like GTX 745 I am using, It has a 4GB vram (50% of the GTX1080), then it shall use 6-8 hours IF ONLY CONSIDERS THE BATCH SIZE LOSS

And If you are using cpu, consider a desktop i7 with enough RAM , it is about 48-64 hours in total (800% time consuming)

for a laptop 4GB RAM, you need to reduce the batch size, which is ... even slower, and if you are using i5-laptop-version, please save your time and don't even try",add hint get training use like shall use batch size loss consider enough ram total time consuming ram need reduce batch size even please save time even try,issue,positive,neutral,neutral,neutral,neutral,neutral
357563094,We just implemented `aspect_ratio` in the test code. Let us know if it works for you.,test code let u know work,issue,negative,neutral,neutral,neutral,neutral,neutral
357553636,It takes 3.8 GB on my GTX 1080. You probably need a larger GPU or train models at a lower resolution (128p). ,probably need train lower resolution,issue,negative,neutral,neutral,neutral,neutral,neutral
357552090,"If I remember correctly, the architecture only works when each downsampling/upsampling layer operates on images/feature maps whose height and width are even numbers. For example, it will work if the dimensions of the input are powers of 2. Rectangular images should be fine. There are a few options for testing on 700x1100 images:
1. Resize to a power of 2
2. Pad or crop to a power of 2
3. Modify the architecture to handle downsampling/upsampling odd numbered dimensions",remember correctly architecture work layer whose height width even example work input rectangular fine testing resize power pad crop power modify architecture handle odd,issue,negative,positive,positive,positive,positive,positive
357551933,"For cityscapes, we downsampled images to 256p or 512p. The model should work for rectangular images during the test time. Could you tell us the errors you got?
- try to run the model on 512x512 images.  
- try something like 640x1152 (both can be divided by 64)
- not sure if you have enough memory for the large test image. ",model work rectangular test time could tell u got try run model try something like divided sure enough memory large test image,issue,positive,positive,positive,positive,positive,positive
357541397,You should check if you set `--gpu_ids 0`. I also wonder if you are able to run PyTorch [examples](https://github.com/pytorch/examples) on a GPU.  ,check set also wonder able run,issue,negative,positive,positive,positive,positive,positive
357540831,"I will argue that the test code (only 33 lines) is relatively simple. You can remove the visualization part if you do not need them. Here a few steps to apply a single inference. 
- create a model [Line 15 - Line 17]
- get the data from the data loader. 
- `model.set_input(data)`
- `mode.test()`",argue test code relatively simple remove visualization part need apply single inference create model line line get data data loader data,issue,negative,negative,neutral,neutral,negative,negative
357540544,"Yes, we have it in the PyTorch version. It is called [test_model](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/test_model.py) and you can use the flag `--model test` during test time.  See the [instructions](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) on how to apply a pretrained cyclegan model for more details. ",yes version use flag model test test time see apply model,issue,negative,neutral,neutral,neutral,neutral,neutral
357540196,I think it is possible. You can have L1+  GAN loss for the paired data and cycle-consistency loss + GAN loss for the unpaired data. Looking forward to seeing your results. ,think possible gan loss paired data loss gan loss unpaired data looking forward seeing,issue,negative,neutral,neutral,neutral,neutral,neutral
357358273,"@srinlee-kyra: Hey, How do you test for a rectangular image (lets say of size 480 X 640) for a Unet pre-trained with lets say images of size 512 X 512 ? Thanks in advance.",hey test rectangular image say size say size thanks advance,issue,negative,positive,positive,positive,positive,positive
357355375,"@visonpon : Hey, How did you solve this issue ? Like testing a rectangular image (say size 480 X 640) using a Unet pre-trained on 512 X 512. Thanks in advance ",hey solve issue like testing rectangular image say size thanks advance,issue,positive,positive,positive,positive,positive,positive
357082276,"That's right, it's a way to make the discriminator conditional on A. Giving only B as input is indeed the unconditional way.",right way make discriminator conditional giving input indeed unconditional way,issue,negative,positive,positive,positive,positive,positive
356396756,"Hi @junyanz  I'm trying to load the pretrained cyclegan (horse2zebra) model as below code template,

model = XXXModel()
model.load_state_dict(torch.load('latest_net_G.pth'))

dumb question please. which XXXModel should I use in your code base please? I'm assuming that it should be a child class of torch.nn.Module.

or could you save and share the whole model for reference please? :)

Thank you very much!
  
  Forget about it. Figured out from the test_model.py. :)",hi trying load model code template model dumb question please use code base please assuming child class could save share whole model reference please thank much forget figured,issue,positive,negative,negative,negative,negative,negative
356060086,Cool. The reason nb is forced as 1 is that the image output code doesn't loop over the batch. You can pretty easily change that to support it :),cool reason forced image output code loop batch pretty easily change support,issue,positive,positive,positive,positive,positive,positive
356057895,"Thanks for the reply, @SsnL.

Yes, we are training using batch norm, i.e. --norm batch, so it could make a substantial difference.",thanks reply yes training batch norm norm batch could make substantial difference,issue,positive,positive,positive,positive,positive,positive
355775885,"unless you train with batch norm, it won't make a difference.",unless train batch norm wo make difference,issue,negative,neutral,neutral,neutral,neutral,neutral
355599099,Using one for two Ds is equivalent to the current scheme.,one two equivalent current scheme,issue,negative,neutral,neutral,neutral,neutral,neutral
355128699,windows builds may have some weird issues as it is not as stable. your pytorch is probably not properly installed. see https://github.com/pytorch/pytorch/issues/3865,may weird stable probably properly see,issue,negative,negative,negative,negative,negative,negative
354841108,"Hmmm, I'm not sure how well pytorch runs on windows 10, especially wrt the multiprocessing dataloaders, because the mp mechanism is different on win and posix. Which pytorch version are you using? Could you try `--nThreads 0` option?",sure well especially mechanism different win version could try option,issue,positive,positive,positive,positive,positive,positive
354840128,"Unfortunately, your GPU runs out of memory. IIRC, the cyclegan with batch size 1 and 256x256 resolution needs 3G-5G GPU memory with cuDNN. Please check that you have cuDNN installed and maybe reduce batch size and/or resolution.

Btw, which pytorch version are you using?",unfortunately memory batch size resolution need memory please check maybe reduce batch size resolution version,issue,negative,negative,negative,negative,negative,negative
354839372,"@electop That's great!

@junyanz If you don't mind, I can submit the above patch as a PR. :)",great mind submit patch,issue,positive,positive,positive,positive,positive,positive
354839029,"@SsnL Thanks for your effort. I solved the problem with this patch, too. (e.g : $patch -p1 < resize_.patch)",thanks effort problem patch patch,issue,negative,positive,positive,positive,positive,positive
354794032,"@electop It could be an issue with the tensor somehow becomes non-resizable. The way this repo does this part is not super efficient anyways. So could you try this patch? It should work.

https://gist.github.com/SsnL/351720fb0fd0a43c6fdc370be402cff3",could issue tensor somehow becomes way part super efficient anyways could try patch work,issue,positive,positive,positive,positive,positive,positive
354779878,"Assuming you're talking about pix2pix, the first one (G_GAN) is related to the adversarial training of the autoencoder part of the network (meaning we want the autoencoder to fool the discriminator with the produced images), while the latter (G_L1) is related to the quality of the produced image itself. The reason for combining them (clearly explained in subsection 4.2 of https://arxiv.org/pdf/1611.07004.pdf) is to obtain sharp images (G_GAN) without artifacts (G_L1).",assuming talking first one related training part network meaning want fool discriminator produced latter related quality produced image reason combining clearly subsection obtain sharp without,issue,negative,positive,neutral,neutral,positive,positive
354729015,"@SsnL Thanks for your attention. This problem continued to occur when I used PyTorch 0.3.0 packaged for Anaconda. Accoring to the opinion of Mr. nabihach, I will try it with PyTorch 0.40 and share the result with you.",thanks attention problem continued occur used anaconda opinion try share result,issue,negative,positive,positive,positive,positive,positive
354635359,"@nabihach That is good to hear! Btw, if you see any issues, please try our 0.3 release, as that is more stable.",good hear see please try release stable,issue,positive,positive,positive,positive,positive,positive
354521879,"@electop @keineahnung2345 Hi all, PyTorch dev here. The default code should work just fine. What are the PyTorch versions you are using?",hi dev default code work fine,issue,negative,positive,positive,positive,positive,positive
354521799,@ElijahLai Sorry for missing your response. That is interesting. Could you try uploading to pytorch 0.3 and cuda 9? A number of bugs are fixed in these two releases.,sorry missing response interesting could try number fixed two,issue,negative,negative,neutral,neutral,negative,negative
354513359,"Usually, cyc losses should go down gradually. D and G losses sometimes just go up and down. It is fine as long as they do not explode. ",usually go gradually sometimes go fine long explode,issue,negative,positive,neutral,neutral,positive,positive
354513070,@hanrelan We also fixed a few initialization issues. And it should work even without identity loss now. ,also fixed work even without identity loss,issue,negative,positive,neutral,neutral,positive,positive
354513021,I guess this cuda runtime error might not be related to this repo. Are you able to run some basic examples from pytorch examples [repo](https://github.com/pytorch/examples)?,guess error might related able run basic,issue,negative,positive,positive,positive,positive,positive
354512829,"@mrluker  It depends on which generator you are using, for the resnet generator, you can change the network architectures [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L207); for the u-net generator, you can edit [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L333). You can change the kernel_size in each function. ",generator generator change network generator edit change function,issue,negative,neutral,neutral,neutral,neutral,neutral
354512322,"It only depends on the generator. You are free to use 'resnet_6blocks' or 'resnet_9blocks' which should support some rectangular inputs. I think 'unet_128' and 'unet_256' only support square input images. There are other workaround solutions. (1) You can load the image as rectangular images, and crop square patches for example. Or (2) resize the image to a square image, run the training/test,  and resize the result back to the original rectangular image during the test time. ",generator free use support rectangular think support square input load image rectangular crop square example resize image square image run resize result back original rectangular image test time,issue,positive,positive,positive,positive,positive,positive
354511949,"We have a few pre-trained models available. More are coming. 
pix2pix: edges2shoes, sat2map, and facades_label2photo
CycleGAN: horse2zebra and zebra2horse

Please check out the [download_scripts](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/pretrained_models),  [instructions(pix2pix)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-pix2pix) and  [instructions(CyclceGAN)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#apply-a-pre-trained-model-cyclegan) for more details. ",available coming please check,issue,negative,positive,positive,positive,positive,positive
354511534,"How to automatically evaluate the results is an open research problem for GANs research. In the case of CycleGAN, we may be able to evaluate the results with validation data when a few paired ground truth data are available.  See the FCN score evaluation for cityscape label -> photo in the paper (Section 5.1.1) for an example. But for many creative applications, the results are subjective and hard to evaluate without humans in the loop.",automatically evaluate open research problem research case may able evaluate validation data paired ground truth data available see score evaluation cityscape label photo paper section example many creative subjective hard evaluate without loop,issue,negative,positive,positive,positive,positive,positive
354511124,"It is quite normal. It is fine as long as the losses do not explode. We are using LSGAN and there is no guarantee that the GAN loss will converge. You can get a convergent loss if you are using [WGAN](https://arxiv.org/abs/1701.07875
)(or [WGAN-GP](https://arxiv.org/abs/1704.00028)).",quite normal fine long explode guarantee gan loss converge get convergent loss,issue,negative,positive,positive,positive,positive,positive
354510691,"Please see the notes from the original pix2pix torch repo (copied below): 
The pre-trained model does not work well on Cityscapes in the original resolution (1024x2048) as it was trained on 256x256 images that are resized to 1024x2048. The purpose of the resizing was to 1) keep the label maps in the original high resolution untouched and 2) avoid the need of changing the standard FCN training code for Cityscapes. To get the ground-truth numbers in the paper, you need to resize the original Cityscapes images to 256x256 before running the evaluation code.",please see original torch copied model work well original resolution trained purpose keep label original high resolution untouched avoid need standard training code get paper need resize original running evaluation code,issue,positive,positive,positive,positive,positive,positive
354508145,"@isalirezag  We first (1) read the image, (2) resize it to (LoadSize, LoadSize)  (3) crop random patches of (fineSize, fineSize). It is a common data argumentation. ",first read image resize crop random common data argumentation,issue,negative,negative,negative,negative,negative,negative
353979671,@blitu12345  Sorry I don't really remember but I try reduce the batch size.,sorry really remember try reduce batch size,issue,negative,negative,negative,negative,negative,negative
353499877,Thank you so much for the kind reply!,thank much kind reply,issue,positive,positive,positive,positive,positive,positive
353475015,"1. I think there are a bunch of applications, like an artist could sketch a shoe and then the bicyclegan would present several possible colorizations and the artist could choose the one they like the most (in pix2pix, you only get a single choice). But I also think it's definitely a project full of fantasy and imagination :)
2. Yeah it's all paired training data. The name is a bit confusing since it actually is applied to the pix2pix setting, not the cyclegan setting.",think bunch like artist could sketch shoe would present several possible artist could choose one like get single choice also think definitely project full fantasy imagination yeah paired training data name bit since actually applied setting setting,issue,positive,positive,neutral,neutral,positive,positive
353293428,"@phillipi 
Thanks, phillipi. I have tried on the link you sent, and it seems it does have no big differences...
What's more, I have read Bicyclegan about which there are two questions confusing: 
1、Does Bicyclegan  have some reality application?Or just a image transfer project full of fantasy and imagenation?
2、Are all images used to train paired? e.g. Domain A contains an image of a building, so it's paired image in domain B must be the same building shot in the same angle? So I think it is time-consuming to choose training set,right?
",thanks tried link sent big read two reality application image transfer project full fantasy used train paired domain image building paired image domain must building shot angle think choose training set right,issue,negative,positive,positive,positive,positive,positive
353236610,"You're welcome. To the problem of pytorch, I think I'm not very familiar with it, so maybe I will leave the problem to the others.",welcome problem think familiar maybe leave problem,issue,negative,positive,positive,positive,positive,positive
353231363,"We tried a few ways of adding z to the nets, e.g., adding z to a latent state, concatenating with a latent state, applying dropout, etc. The output tended not to vary much as a function of z. You can see the effect of random dropout here: https://affinelayer.com/pixsrv/

Click the ""pix2pix"" button multiple times to see different random samples. In this implementation, the only noise is dropout (as in the pix2pix paper). Some minor details vary from click to click but overall not much changes.

Conditional GANs don't really need noise as long as the input you are conditioning on is sufficiently complex, so that it can kind of play the role of noise. Without noise, the mapping is deterministic, but that's often fine.

Here's a follow up paper that shows one way of getting z to actually have a substantial effect: https://junyanz.github.io/BicycleGAN/",tried way latent state latent state dropout output vary much function see effect random dropout click button multiple time see different random implementation noise dropout paper minor vary click click overall much conditional really need noise long input sufficiently complex kind play role noise without noise deterministic often fine follow paper one way getting actually substantial effect,issue,positive,positive,neutral,neutral,positive,positive
352996617,"@junyanz Could you please explain how did you find z get ignored in detail? What did the result show?
Since the noise may be ignored by G, why does original Conditional Gan perform well?
Thank you very much!",could please explain find get detail result show since noise may original conditional gan perform well thank much,issue,positive,positive,positive,positive,positive,positive
352636037,"when I made a change in network.py ,as following:
if len(gpu_ids) > 0:
print(gpu_ids[0])
netG.cuda(gpu_ids[0]) ########################device_id=gpu_ids[0]

Errors are like this:
THCudaCheck FAIL file=d:\pytorch\pytorch\torch\lib\thc\generic/THCTensorMath.cu line=15 error=48 : no kernel image is available
for execution on the device
Traceback (most recent call last):
File ""test.py"", line 22, in 
model = create_model(opt)
File ""D:\cv2\DeblurGAN-master\models\models.py"", line 11, in create_model
model.initialize(opt)
File ""D:\cv2\DeblurGAN-master\models\test_model.py"", line 19, in initialize
opt.learn_residual)
File ""D:\cv2\DeblurGAN-master\models\networks.py"", line 54, in define_G
netG.apply(weights_init)
File ""E:\programs\Anaconda\envs\python\lib\site-packages\torch\nn\modules\module.py"", line 198, in apply
module.apply(fn)
File ""E:\programs\Anaconda\envs\python\lib\site-packages\torch\nn\modules\module.py"", line 198, in apply
module.apply(fn)
File ""E:\programs\Anaconda\envs\python\lib\site-packages\torch\nn\modules\module.py"", line 199, in apply
fn(self)
File ""D:\cv2\DeblurGAN-master\models\networks.py"", line 17, in weights_init
m.bias.data.fill_(0)
RuntimeError: cuda runtime error (48) : no kernel image is available for execution on the device at d:\pytorch\pytorch\torch\lib
\thc\generic/THCTensorMath.cu:15

So I really dont't know how to run this code properly on Windows 10 X64 with cuda 8.0,cudnn6, python 3.6.1,torch0.3.0 after many times' attempts",made change following print like fail kernel image available execution device recent call last file line model opt file line opt file line initialize file line file line apply file line apply file line apply self file line error kernel image available execution device really know run code properly python torch many time,issue,negative,positive,positive,positive,positive,positive
352632434,"Hello, keineahnung2345
Thank you so much for your reply. Thanks to you, I solved the problem perfectly.
I think this is a problem with the torch code, but it still does not resolve the problem.
For reference, the link below is about the issue of torch code similar to my issue.
https://github.com/pytorch/pytorch/pull/4084",hello thank much reply thanks problem perfectly think problem torch code still resolve problem reference link issue torch code similar issue,issue,positive,positive,positive,positive,positive,positive
352609628,"Hello, I'm from KupynOrest/DeblurGAN and the same problem just occurred.
I've found a quick hack to the problem of resize_().
Just replace
`self.input_A.resize_(input_A.size()).copy_(input_A)`
into
```
temp = self.input_A.clone()
temp.resize_(input_A.size())
temp.copy_(input_A)
self.input_A = temp
```
",hello problem found quick hack problem replace temp temp,issue,negative,positive,positive,positive,positive,positive
352604596,has this been implemented already ? currently manually dividing height during inference and thought you lot might have perhaps worked out something more elegant  ...,already currently manually dividing height inference thought lot might perhaps worked something elegant,issue,negative,positive,positive,positive,positive,positive
351744253,"People have been throwing a term called ""Nash equilibrium"" around, which sounds really fancy but I haven't seen a quantitative definition. ",people throwing term nash equilibrium around really fancy seen quantitative definition,issue,negative,positive,positive,positive,positive,positive
350931685,"Hi 
I have trained cycleGan on my own dataset. During the training, I found that the GAN loss of Generator A cannot be convergent and becomes larger. At the same time, the GAN loss of Generator B, Discriminator A and B all go convergent. Have anyone met this problem before? Thank you.",hi trained training found gan loss generator convergent becomes time gan loss generator discriminator go convergent anyone met problem thank,issue,negative,neutral,neutral,neutral,neutral,neutral
350861294,"I think you could apply the pix2pixHD architecture to unpaired translation, but yeah, it may take some modification to make it work well. I haven't yet seen an HD CycleGAN. 

In general, I would try the coarse-to-fine approach where you first learn the translation between low frequency images, then add additional layers to learn the residual high frequencies. This idea, applied to GANs, first appeared in [LAPGAN](https://arxiv.org/abs/1506.05751), and it underlies a lot of recent success, including pix2pixHD, progressive GANs, and [StackedGAN++](https://github.com/hanzhanggit/StackGAN-v2).",think could apply architecture unpaired translation yeah may take modification make work well yet seen general would try approach first learn translation low frequency add additional learn residual high idea applied first lot recent success progressive,issue,positive,positive,positive,positive,positive,positive
350857200,"Thanks a lot, Phillip.

I am struggling with an unpaired samples problem. I'd like to be able to translate a datasat A into a dataset B. I guess the pix2pixHD framework won't help me. Do you know a ""HD version"" of the Jun-Yan's unpaired-sample GAN?",thanks lot struggling unpaired problem like able translate guess framework wo help know version gan,issue,negative,positive,positive,positive,positive,positive
350834540,"Sure, you can apply the architecture to any size images, as long as they fit in memory. You'll probably hit memory limits pretty quickly though. You can modify `loadSize` and `fineSize` to change the image resolution. Upon loading, images are first scaled to `loadSize` then cropped to `fineSize`.

You might also want to try some of the newer methods for making really HD images, like [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) or [progressive GANs](https://github.com/tkarras/progressive_growing_of_gans).",sure apply architecture size long fit memory probably hit memory pretty quickly though modify change image resolution upon loading first scaled might also want try making really like progressive,issue,positive,positive,positive,positive,positive,positive
350653069,"Yes, the problem is exact what you said, I only applied `--no_drop` in training phase, and did not use it for testing @junyanz .  Problem solved after I added `--no_drop` flag to the testing script. And I think you are facing the same issue. @ivjia ",yes problem exact said applied training phase use testing problem added flag testing script think facing issue,issue,negative,positive,positive,positive,positive,positive
350651987,Ok I see where's the problem and it works now. Thank you for your kind reply! @junyanz ,see problem work thank kind reply,issue,negative,positive,positive,positive,positive,positive
350637673,"I met the same problem as @ivjia , I got the same output:

File ""test.py"", line 17, in <module>
    model = create_model(opt)
  File ""/media/data1/CycleGAN/new/models/models.py"", line 19, in create_model
    model.initialize(opt)
  File ""/media/data1/CycleGAN/new/models/test_model.py"", line 23, in initialize
    self.load_network(self.netG, 'G', which_epoch)
  File ""/media/data1/CycleGAN/new/models/base_model.py"", line 53, in load_network
    network.load_state_dict(torch.load(save_path))
  File ""/home/tan/anaconda2/envs/cyclegan/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 490, in load_state_dict
    .format(name))
KeyError: 'unexpected key ""model.10.conv_block.5.weight"" in state_dict'

after I renamed the ""latest_net_G.pth"" manually. Also, all the parameters used for training and testing were default values. ",met problem got output file line module model opt file line opt file line initialize file line file line name key model weight manually also used training testing default,issue,negative,neutral,neutral,neutral,neutral,neutral
350581476,"Hi @onursertkaya,

Sorry it's been hard to follow. You are right that the last two layers both have stride 1. Reading over the appendix of the [pix2pix paper](https://arxiv.org/abs/1611.07004) it looks like we indeed failed to mention this. I'll update it in the next arxiv draft.

As you continue working on your reimplementation, I would suggest looking at our code, rather than trying to reimplement directly from the papers. There are probably going to be more details that are in the code but not mentioned in the paper (although we tried to minimize this). My own perspective is that the ""scientific publication"" should not be thought of as just the paper, but the paper+code+data. For learning about the basic idea and math, the paper is the place to look. For reimplementing the exact method, I would say the code is the primary place to look.",hi sorry hard follow right last two stride reading appendix paper like indeed mention update next draft continue working would suggest looking code rather trying directly probably going code paper although tried minimize perspective scientific publication thought paper learning basic idea math paper place look exact method would say code primary place look,issue,negative,positive,neutral,neutral,positive,positive
349942060,"Is there any pretrained model? I want to implement the testing directly, but came across the error:
no such file latest_net_G.pth'
 ",model want implement testing directly came across error file,issue,negative,positive,neutral,neutral,positive,positive
349841375,"We are doing conversion between PyTorch Tensors and Numpy-formatted images.

Transpose was needed because in pytorch, channel dimension is the first dim. 

(X+1)/2.0 was needed because the Tensor values are between [-1,1], not [0,1]. ",conversion transpose channel dimension first dim tensor,issue,negative,positive,positive,positive,positive,positive
349331996,"I have the same problem here. I first trained my own model and tried to apply it to one image, the error above reported. Then I directly trained a map model exactly follow your README file, then it shows ""
 
IOError: [Errno 2] No such file or directory: './checkpoints/map_cyclegan/latest_net_G.pth' 

""
after I manually change the name of ""latest_net_G_A.pth"" to ""latest_net_G.pth"", it shows ""

KeyError: 'unexpected key ""model.10.conv_block.5.weight"" in state_dict'. 

It looks like a issue with the state_dict from the trained model. Please help me with it.

""

",problem first trained model tried apply one image error directly trained map model exactly follow file file directory manually change name key model weight like issue trained model please help,issue,negative,positive,positive,positive,positive,positive
348168606,"`       

        # encoder_outputs  seq x hidden_size
        # enc_contri  1 x seq x hidden_size
        # first
        # 
        encout = nn.Parameter(torch.randn(self.hidden_size, self.hidden_size))

        enc_contri = torch.bmm(encoder_outputs.unsqueeze(0),encout.unsqueeze(0))  

        # second

        self.enc = nn.Linear(self.hidden_size,self.hidden_size) 

        enc_contri = self.enc(encoder_outputs).unsqueeze(0)  
`

Hi , my problem is here, I can run the second code but the first code will show that issue.
my torch version is 0.2.0_3, cuda version is 8.0, V8.0.61   
@SsnL ",first second hi problem run second code first code show issue torch version version,issue,negative,positive,positive,positive,positive,positive
347577480,"Hi all, PyTorch developer here. Are all of you seeing this issue using Pascal GPUs? If so, you should make sure that the CUDA version is at least 8.0. 

Also, it would be very helpful if you can provide your PyTorch version (`import torch; print(torch.__Version__)`, CUDA version (`nvcc -V`), and cuDNN version (`grep CUDNN_MAJOR /path/to/cudnn.h`).",hi developer seeing issue make sure version least also would helpful provide version import torch print version version,issue,positive,positive,neutral,neutral,positive,positive
346770933,I think you need to first install the pytorch with cuda/gpu mode and make sure you can run the pytorch test code. ,think need first install mode make sure run test code,issue,negative,positive,positive,positive,positive,positive
346758888,"Sure, you need to train a model from blurred images to clear images. ",sure need train model blurred clear,issue,positive,positive,positive,positive,positive,positive
346758597,"I took a closer look at the pictures [here](https://junyanz.github.io/CycleGAN/images/photo_enhancement.jpg)


It looks as if he's blurring the picture, not making it clear！",took closer look picture making,issue,negative,neutral,neutral,neutral,neutral,neutral
346754663,Possibly. We have used CycleGAN to create shallow depth-of-field [effect](https://junyanz.github.io/CycleGAN/images/photo_enhancement.jpg). But I will not expect it to work better than depth-aware methods.  Additional 3D (depth) informaiton always helps.,possibly used create shallow effect expect work better additional depth always,issue,positive,positive,neutral,neutral,positive,positive
346704553,"Essentially we [divide](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/cycle_gan_model.py#L115) the objective by 2 while optimizing D, which slows down the rate at which D learns relative to G. We will clarify it in the arxiv v3. ",essentially divide objective slows rate relative clarify,issue,negative,neutral,neutral,neutral,neutral,neutral
346495489,"Thanks! Also note that batchsize=1 is instance norm (aka contrast normalization), which has qualitatively different properties from batchnorm. Batchnorm achieves invariance to mean and variance of features across a bunch of images. Instance norm achieves invariance to mean and variance of features in a single image. As a result, instance norm will be (nearly) invariant to image-level operations like changing the exposure or contrast of a photo, whereas batchnorm will not. Batchnorm is only invariant to batch-level operations.

*caveat, these statements are only strictly true if the momentum parameter is set to zero, which we don't do in practice",thanks also note instance norm aka contrast normalization qualitatively different invariance mean variance across bunch instance norm invariance mean variance single image result instance norm nearly invariant like exposure contrast photo whereas invariant caveat strictly true momentum parameter set zero practice,issue,positive,negative,neutral,neutral,negative,negative
346107348,Try running on CPU by adding the following option: `--gpu_ids -1`,try running following option,issue,negative,neutral,neutral,neutral,neutral,neutral
345941282,"Pix2pix training on convergence curves on Facades dataset using batch sizes of 1, 16, 32, respectively.
 
![pix2pix_facades_batch_size_1](https://user-images.githubusercontent.com/13028330/33059888-628596f2-ceea-11e7-93b4-1740780b8920.png)
![pix2pix_facades_batch_size_16](https://user-images.githubusercontent.com/13028330/33059889-62ae0ff6-ceea-11e7-8ed4-1ae08e6c2030.png)
![pix2pix_facades_batch_size_32](https://user-images.githubusercontent.com/13028330/33059890-62d617c6-ceea-11e7-997f-11aae418320d.png)
",training convergence batch size respectively,issue,negative,neutral,neutral,neutral,neutral,neutral
345907486,"The current model does not take `z` as input. In both pix2pix and CycleGAN, we tried to add `z` to the generator but often found that `z` got ignored. So we decided to only take `real_A` as input. ",current model take input tried add generator often found got decided take input,issue,negative,neutral,neutral,neutral,neutral,neutral
344932358,"@junyanz  Sorry it's me again. I tried your evaluation [code](https://github.com/phillipi/pix2pix/tree/master/scripts/eval_cityscapes). It looks normal when I take the fake image as input, but it's so weird that when the input is the true image, the segmentation results become so bad. My questions are,
1. what iou does this of this caffe model get on cityscapes?
2. is there any special configs that we need to change when running normal images?
![results](https://user-images.githubusercontent.com/17564896/32892903-ac99332e-cb12-11e7-9023-a3b9d51c0615.png)
The numbers are (pixel acc., cls acc., iou).
(Do I need to new an issue in pix2pix?)",sorry tried evaluation code normal take fake image input weird input true image segmentation become bad model get special need change running normal need new issue,issue,negative,negative,negative,negative,negative,negative
344793048,"Sorry for the delayed response (my daughter was born early like 12 hours after you graciously responded) but thanks so much.  The load and finesize adjustments worked to get rid of the blobs! I think I need to do some tweaking to the datasets, but this is one problem solved.",sorry response daughter born early like graciously thanks much load worked get rid think need one problem,issue,positive,neutral,neutral,neutral,neutral,neutral
344510699,"@junyanz Ok, I see. Thanks a lot! I use `batchSize=20` on 4 gpus to accelerate training. I will train the model again and try your evaluation code in pix2pix.",see thanks lot use accelerate training train model try evaluation code,issue,negative,positive,positive,positive,positive,positive
344506650,"Thanks for your nice words. We use `batchSize=1` and we got worse results with a larger batchSize.  I remembered a few details (e.g.  saving the output as png images, `loadSize=143`, `fineSize=128`). We use the evaluation [code](https://github.com/phillipi/pix2pix/tree/master/scripts/eval_cityscapes) in pix2pix. See more discussion [here](https://github.com/phillipi/pix2pix/issues/112).",thanks nice use got worse saving output use evaluation code see discussion,issue,negative,positive,positive,positive,positive,positive
344086549,"thanks guys. @junyanz answer fixed my problem. Just putting the following just before building the model:
```python
torch.set_default_tensor_type('torch.cuda.FloatTensor')
```

PS: now I've realized that I should have used something like:
```python
if torch.cuda.is_available():
    torch.set_default_tensor_type('torch.cuda.FloatTensor')
```",thanks answer fixed problem following building model python used something like python,issue,negative,positive,positive,positive,positive,positive
344002303,I wonder if your gaussian_model is on GPU. This [issue](https://github.com/amdegroot/ssd.pytorch/issues/32)  might be related. You can use `get_device()` for debugging. ,wonder issue might related use,issue,negative,neutral,neutral,neutral,neutral,neutral
344001859,What does your `gaussian_model.SimpleGaussian` look like? Which line did you get the error in?,look like line get error,issue,negative,neutral,neutral,neutral,neutral,neutral
343368717,"it should be fine. You can see which norm you are using, from the output log.",fine see norm output log,issue,negative,positive,positive,positive,positive,positive
343368148,"@junyanz  Yes,I append the args --gpu_ids=0,1,2 and did not change the type of ""norm"".
will there be some trouble with pytorch version or something else?",yes append change type norm trouble version something else,issue,negative,negative,negative,negative,negative,negative
343364162,"You can set the gpu_ids as 0,1,2. len(self.opt.gpu_ids) > 0 
checks if there are more than one gpu ids. 
torch.cuda.set_device(self.opt.gpu_ids[0]) sets the main device as the first gpu. 
See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/137) for more discussion. The multi-GPU code should work with instancenorm.  We haven't implemented the synchronized batchnorm as mentioned [here](http://hangzh.com/SynchronizeBN/)
",set one main device first see discussion code work synchronized,issue,negative,positive,positive,positive,positive,positive
343295263,@myt00seven What is your environ setting? I am running cyclegan with latest PyTorch and it seems fine.,environ setting running latest fine,issue,negative,positive,positive,positive,positive,positive
342981585,"With Instance Normalization, since we are using affine=False, there exist no weights. Thank you for the pull request though!",instance normalization since exist thank pull request though,issue,negative,neutral,neutral,neutral,neutral,neutral
342756422,Currently the CycleGAN model still has the memory leaking problem on GPU. I'm using the latest pytorch docker from [pytorch/pytorch](https://github.com/pytorch/pytorch).,currently model still memory problem latest docker,issue,negative,positive,positive,positive,positive,positive
342731566,"@hanrelan Thanks for your nice words. We also observed the color inversion in the PyTorch version, and we are still investigating it. For now, you can add a small [identity](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/options/train_options.py#L28) loss to address it. ",thanks nice also color inversion version still investigating add small identity loss address,issue,positive,positive,positive,positive,positive,positive
342730698,"I used a recent version of the code (a couple days old) with two face datasets (lfw -> anime) and saw the same issue

Just want to add that CycleGAN is one of my favorite ML projects. So well documented and easy-to-use, definitely kickstarted my interest in GANs. Thanks!",used recent version code couple day old two face anime saw issue want add one favorite well definitely interest thanks,issue,positive,positive,positive,positive,positive,positive
342238732,"We recently updated the code to fix the gpu memory leak. There were small issues as you reported. 
I just removed the self in this [line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/a24e24d67d88f75869f447690f7d994fe7d42e2d). ",recently code fix memory leak small removed self line,issue,negative,negative,negative,negative,negative,negative
342053426,My bad. It should be fixed now. Could you try it again?,bad fixed could try,issue,negative,negative,negative,negative,negative,negative
342032528,"I did change both ndf/ngf and also made sure that it is the same at testing stage.
I dont see why it can be an issue though :)",change also made sure testing stage dont see issue though,issue,negative,positive,positive,positive,positive,positive
342027555,"It looks fine to me. We have 
Conv2d(1, 100, kernel_size=(7, 7), stride=(1, 1))
 Conv2d(100, 1, kernel_size=(7, 7), stride=(1, 1))
which means it takes an 1-channel image as input, and produces an 1-channel image. 
I wonder if you have changed the ndf/ngf during training/test. 
It would be great if can also print the network for your training script. ",fine image input image wonder would great also print network training script,issue,positive,positive,positive,positive,positive,positive
342020626,"Here is the output:

 cycle_gan
initialization method [xavier]
initialization method [xavier]
ResnetGenerator (
  (model): Sequential (
    (0): ReflectionPad2d (3, 3, 3, 3)
    (1): Conv2d(1, 100, kernel_size=(7, 7), stride=(1, 1))
    (2): InstanceNorm2d(100, eps=1e-05, momentum=0.1, affine=False)
    (3): ReLU (inplace)
    (4): Conv2d(100, 200, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (5): InstanceNorm2d(200, eps=1e-05, momentum=0.1, affine=False)
    (6): ReLU (inplace)
    (7): Conv2d(200, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (8): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
    (9): ReLU (inplace)
    (10): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (11): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (12): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (13): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (14): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (15): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (16): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (17): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (18): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (19): ConvTranspose2d(400, 200, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (20): InstanceNorm2d(200, eps=1e-05, momentum=0.1, affine=False)
    (21): ReLU (inplace)
    (22): ConvTranspose2d(200, 100, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (23): InstanceNorm2d(100, eps=1e-05, momentum=0.1, affine=False)
    (24): ReLU (inplace)
    (25): ReflectionPad2d (3, 3, 3, 3)
    (26): Conv2d(100, 1, kernel_size=(7, 7), stride=(1, 1))
    (27): Tanh ()
  )
)
Total number of parameters: 27738001
ResnetGenerator (
  (model): Sequential (
    (0): ReflectionPad2d (3, 3, 3, 3)
    (1): Conv2d(1, 100, kernel_size=(7, 7), stride=(1, 1))
    (2): InstanceNorm2d(100, eps=1e-05, momentum=0.1, affine=False)
    (3): ReLU (inplace)
    (4): Conv2d(100, 200, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (5): InstanceNorm2d(200, eps=1e-05, momentum=0.1, affine=False)
    (6): ReLU (inplace)
    (7): Conv2d(200, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (8): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
    (9): ReLU (inplace)
    (10): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (11): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (12): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (13): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (14): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (15): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (16): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (17): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (18): ResnetBlock (
      (conv_block): Sequential (
        (0): ReflectionPad2d (1, 1, 1, 1)
        (1): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (2): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
        (3): ReLU (inplace)
        (4): ReflectionPad2d (1, 1, 1, 1)
        (5): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1))
        (6): InstanceNorm2d(400, eps=1e-05, momentum=0.1, affine=False)
      )
    )
    (19): ConvTranspose2d(400, 200, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (20): InstanceNorm2d(200, eps=1e-05, momentum=0.1, affine=False)
    (21): ReLU (inplace)
    (22): ConvTranspose2d(200, 100, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    (23): InstanceNorm2d(100, eps=1e-05, momentum=0.1, affine=False)
    (24): ReLU (inplace)
    (25): ReflectionPad2d (3, 3, 3, 3)
    (26): Conv2d(100, 1, kernel_size=(7, 7), stride=(1, 1))
    (27): Tanh ()
  )
)
Total number of parameters: 27738001
initialization method [xavier]
initialization method [xavier]
While copying the parameter named model.1.weight, whose dimensions in the model are torch.Size([100, 1, 7, 7]) and whose dimensions in the checkpoint are torch.Size([1, 3, 7, 7]), ...
Traceback (most recent call last):
  File ""test.py"", line 28, in <module>
    model = create_model(opt)
  File ""/home/sgolest1/Desktop/AAA/JPEG/vision/pytorch-CycleGAN-and-pix2pix/models/models.py"", line 19, in create_model
    model.initialize(opt)
  File ""/home/sgolest1/Desktop/AAA/JPEG/vision/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py"", line 47, in initialize
    self.load_network(self.netG_A, 'G_A', which_epoch)
  File ""/home/sgolest1/Desktop/AAA/JPEG/vision/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 53, in load_network
    network.load_state_dict(torch.load(save_path))
  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 360, in load_state_dict
    own_state[name].copy_(param)
RuntimeError: invalid argument 2: sizes do not match at /pytorch/torch/lib/THC/generic/THCTensorCopy.c:48",output method method model sequential sequential sequential sequential sequential sequential sequential sequential sequential sequential tanh total number model sequential sequential sequential sequential sequential sequential sequential sequential sequential sequential tanh total number method method parameter model weight whose model whose recent call last file line module model opt file line opt file line initialize file line file line name param invalid argument size match,issue,negative,neutral,neutral,neutral,neutral,neutral
342017663,"The training did finished successfully, the problem appears for testing.
if I put 
```
networks.print_network(self.netG_A)
networks.print_network(self.netG_B) 
```
before 
`model = create_model(opt)`
that wont work because it says 

> NameError: name 'networks' is not defined

",training finished successfully problem testing put model opt wont work name defined,issue,negative,positive,positive,positive,positive,positive
342013720,"Did you get good results during training? Your input and output images are both grayscale images?
From the error information, It seems that you can load G_A but not G_B.
Could you try to print the network architecture with the following command during both training and test. 
You can add this command before loading the models. 
```python
networks.print_network(self.netG_A)
networks.print_network(self.netG_B)
```",get good training input output error information load could try print network architecture following command training test add command loading python,issue,negative,positive,positive,positive,positive,positive
342009729,"yeap (python test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --phase test  --no_dropout --input_nc 1 --output_nc 1
)
, it didnot help ",python name model phase test help,issue,negative,neutral,neutral,neutral,neutral,neutral
342002828,Did you add `--input_nc 1` flag in your test script,add flag test script,issue,negative,neutral,neutral,neutral,neutral,neutral
342001746,"I dont think it is supporting grayscale images correctly yet.
It did train based on grayscale but when I tried to test it on gray scale images it gave me an error:
```
While copying the parameter named model.1.weight, whose dimensions in the model are torch.Size([100, 1, 7, 7]) and whose dimensions in the checkpoint are torch.Size([1, 3, 7, 7]), ...
Traceback (most recent call last):
  File ""test.py"", line 27, in <module>
    model = create_model(opt)
  File ""/home/sgolest1/Desktop/AAA/JPEG/vision/pytorch-CycleGAN-and-pix2pix/models/models.py"", line 19, in create_model
    model.initialize(opt)
  File ""/home/sgolest1/Desktop/AAA/JPEG/vision/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py"", line 46, in initialize
    self.load_network(self.netG_A, 'G_A', which_epoch)
  File ""/home/sgolest1/Desktop/AAA/JPEG/vision/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 53, in load_network
    network.load_state_dict(torch.load(save_path))
  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 360, in load_state_dict
    own_state[name].copy_(param)
RuntimeError: invalid argument 2: sizes do not match at /pytorch/torch/lib/THC/generic/THCTensorCopy.c:48

```",dont think supporting correctly yet train based tried test gray scale gave error parameter model weight whose model whose recent call last file line module model opt file line opt file line initialize file line file line name param invalid argument size match,issue,negative,positive,neutral,neutral,positive,positive
341946749,"The model may try to hallucinate highlights similar to the ones in the output image domain but doesn't succeed to produce the realistic ones. For your applications, I will try to train a model with small window size. (you can set loadSize=800, fineSize=256 or 360). A model with smaller window size may tend to be conservative.",model may try hallucinate similar output image domain succeed produce realistic try train model small window size set model smaller window size may tend conservative,issue,negative,negative,neutral,neutral,negative,negative
341942629,"Sorry should have put this as well: I changed the load size to 800, fine size to 780.  Trained on a Quadro P6000.",sorry put well load size fine size trained,issue,negative,negative,neutral,neutral,negative,negative
341942541,"Thank you for the response! I am using cyclegan for this experiment (I may try pix2pix later) for an advanced media studies project about machine editing of photos.  I provided about 2000 interior jpg images straight from a camera and about 2000 jpg images of edited interiors to try and make a model for automatic editing of building interiors.  I didn't change the options except for load and fine size.

The model is very good aside from the weird blobs/blooms. Neither the input or output images have these blobs in the highlight areas and so I'm not sure where it is coming from.

Here are a few of the input images for training:
![4g1a0498](https://user-images.githubusercontent.com/4666339/32411061-4de70e9a-c19f-11e7-98db-c044e3dc8b1f.jpg)
![4t8a1538](https://user-images.githubusercontent.com/4666339/32411062-4df26f88-c19f-11e7-9fe1-08a53948b0bd.jpg)
![4t8a9457](https://user-images.githubusercontent.com/4666339/32411063-4dfe5488-c19f-11e7-8870-e56143997e64.jpg)
![5o6a7955](https://user-images.githubusercontent.com/4666339/32411064-4e0b431e-c19f-11e7-98fd-6473fbe2a3f3.jpg)

And a few of the output images for training:
![003_7l9a1741](https://user-images.githubusercontent.com/4666339/32411065-6893f816-c19f-11e7-939a-3af5c07e8315.jpg)
![03_7l9a7351](https://user-images.githubusercontent.com/4666339/32411066-68a09e40-c19f-11e7-9974-01d4ba832edb.jpg)
![04_7l5a8682](https://user-images.githubusercontent.com/4666339/32411067-68acc8dc-c19f-11e7-91a6-a9e2bc08666a.jpg)
![4g1a1407](https://user-images.githubusercontent.com/4666339/32411068-68b8ad46-c19f-11e7-99ed-e81552696211.jpg)",thank response experiment may try later advanced medium project machine provided interior straight camera try make model automatic building change except load fine size model good aside weird neither input output highlight sure coming input training ga ta ta output training ga,issue,positive,positive,positive,positive,positive,positive
341939097,"It's hard to tell without further information. 
Which methods are you using? pix2pix or cyclegan? 
What does input/output data look like? ",hard tell without information data look like,issue,negative,negative,negative,negative,negative,negative
341884615,What is your training script? I guess that you might have used `batchnorm` during the training. ,training script guess might used training,issue,negative,neutral,neutral,neutral,neutral,neutral
341557840,Cool. That is a great catch. Could you send us a PR request to fix the issue?,cool great catch could send u request fix issue,issue,positive,positive,positive,positive,positive,positive
341110016,Same problem here -- anyone found a solution?,problem anyone found solution,issue,negative,neutral,neutral,neutral,neutral,neutral
340970783,"Thanks for the update. You are right. The ""scale_width_and_crop"" assumes that the height of the resized image is larger than the `fineSize`.",thanks update right height image,issue,negative,positive,positive,positive,positive,positive
340770944,"If it is batchSize/#gpus, then norm still need to be ""instance"" for successful training. I have tested this.",norm still need instance successful training tested,issue,positive,positive,positive,positive,positive,positive
340710778,"Yes.  with --norm instance, it worked.

Specify the number of images per GPU? Is there an option or is it simple for changing your code?",yes norm instance worked specify number per option simple code,issue,negative,neutral,neutral,neutral,neutral,neutral
340684383,"We observe that batchSize=1 with single gpu gives us the best results so far. 
According to [this post](https://github.com/tensorflow/tensorflow/issues/7439), It seems that pytorch calculates mean/var statistics for each gpu. 
So how many images do you have per GPU? 1 per GPU might cause some problems for batchnorm. 
Have you tried instancenorm with multi-gpu setting and batchSize>1?
 ",observe single u best far according post statistic many per per might cause tried setting,issue,positive,positive,positive,positive,positive,positive
340661050,"Interesting. I can run the `--loadSize 256 --fineSize 256` on the public maps dataset. Here is my command. I wonder if you can run the same command on the maps dataset. There might be something special about your own dataset. 
```bash
python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --no_dropout --loadSize 256 --fineSize 256
```",interesting run public command wonder run command might something special bash python name model,issue,positive,positive,positive,positive,positive,positive
340660646, The problems are often caused by inconsistent flags used in the training/test.  Which kinda of `norm` parameters are you using for training and test?,often inconsistent used norm training test,issue,negative,neutral,neutral,neutral,neutral,neutral
339639551,"Ah okay, so the reason the OP in that discussion was failing when performing his change must have been because the original version improved the discriminator each time on the real input before forwarding the fake input.

Thanks for the rapid support, guys!",ah reason discussion failing change must original version discriminator time real input forwarding fake input thanks rapid support,issue,negative,positive,neutral,neutral,positive,positive
339372973,"@AAnoosheh fwd pass over the same network doesn't overwrite the saved variable values. Calling the same module twice will be two links in the dynamic graph. The saved variables for bwd will be stored differently without overwriting each other.

See my reply at https://discuss.pytorch.org/t/how-to-use-the-backward-functions-for-multiple-losses/1826/7?u=simonw",pas network overwrite saved variable calling module twice two link dynamic graph saved differently without see reply,issue,positive,neutral,neutral,neutral,neutral,neutral
339363922,"But do the forward passes over the same network not overwrite the intermediate activation values?  I don't know if autograd keeps references to the historic values of a variable or if it has copies. If the former, then it may very well be overwritten with each forward pass, making the gradients wrong.",forward network overwrite intermediate activation know historic variable former may well forward pas making wrong,issue,negative,negative,negative,negative,negative,negative
339360532,I think this [post](https://discuss.pytorch.org/t/how-to-combine-multiple-criterions-to-a-loss-function/348/8)  helps clarify the issue.,think post clarify issue,issue,negative,neutral,neutral,neutral,neutral,neutral
339330869,"Is this really true that Pytorch autograd handles this?

According to this post, it says otherwise:  https://discuss.pytorch.org/t/how-to-use-the-backward-functions-for-multiple-losses/1826/5",really true according post otherwise,issue,negative,positive,positive,positive,positive,positive
339199921,It is hard to tell unless you send more information regarding your code and error messages. ,hard tell unless send information regarding code error,issue,negative,negative,negative,negative,negative,negative
339199704,"You can just set loadSize=256, fineSize=256 during the test. If you want to keep the same resolution between training and test, you can center-crop the test images in advance by yourself. ",set test want keep resolution training test test advance,issue,negative,neutral,neutral,neutral,neutral,neutral
339199474,"This is adapted from the original [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) paper. According to DCGAN, ""Directly applying batchnorm to all layers however, resulted in sample oscillation and model instability. This was avoided by not applying batchnorm to the generator output layer and the discriminator input layer.""",original paper according directly however sample oscillation model instability generator output layer discriminator input layer,issue,negative,positive,positive,positive,positive,positive
338910848,The code only supports 1-channel and 3-channel images. You need to modify the image_loader to support 4-channel data. Here is a [modification](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L43) for supporting the 1-channel image. ,code need modify support data modification supporting image,issue,positive,positive,positive,positive,positive,positive
338728307,We haven't tried it by ourselves. Maybe you can simply try both pix2pix objective (conditional GANs and L1) and a small cycle consistency loss. ,tried maybe simply try objective conditional small cycle consistency loss,issue,negative,negative,neutral,neutral,negative,negative
338377800,I think it is hard...Or just remind users of this change in Readme.,think hard remind change,issue,negative,negative,negative,negative,negative,negative
338376371,Thanks for the PR. I wonder if it is possible to add a VERSION flag. ,thanks wonder possible add version flag,issue,negative,positive,neutral,neutral,positive,positive
338366475,"Hello,
I have forgotten to mention that I am using CycleGan and not pix2pix. I'm using medical images of different modalities, however, they cannot be registered sufficiently, so I thought I should give CycleGan a chance.

When training the network I experienced strong grid-like artifacts as already mentioned in issue 78. I thought this might stem from preprocessing such as resizing operations. As far as I understand your frameworks loads an image (in my case 256x256 resolution) resizes it to 286 and subsequently cuts it to randomly to 256x256.

However, I don't want this to happen. I want CycleGan to directly take my images as input and don't do any further preprocessing.

Unfortunately, I don't have my own machine with a GPU. I run all the code on a cloud server, so debugging into code is actually pretty hard for me at this point ...

Best,
Florian",hello forgotten mention medical different however registered sufficiently thought give chance training network experienced strong already issue thought might stem far understand image case resolution subsequently randomly however want happen want directly take input unfortunately machine run code cloud server code actually pretty hard point best,issue,positive,positive,positive,positive,positive,positive
338365285,"I confirm this.
```bash
  File ""train.py"", line 48, in <module>
    model.optimize_parameters() # update parameters
  File ""/home/clp001/pt-lr/models/pix2pix_model.py"", line 229, in optimize_parameters
    self.backward_D()
  File ""/home/clp001/pt-lr/models/pix2pix_model.py"", line 189, in backward_D
    self.loss_D.backward()
  File ""/home/clp001/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py"", line 156, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File ""/home/clp001/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 98, in backward
    variables, grad_variables, retain_graph)
  File ""/home/clp001/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py"", line 25, in backward
    return comm.reduce_add_coalesced(grad_outputs, self.input_device)
  File ""/home/clp001/anaconda3/lib/python3.6/site-packages/torch/cuda/comm.py"", line 122, in reduce_add_coalesced
    result = reduce_add(flattened, destination)
  File ""/home/clp001/anaconda3/lib/python3.6/site-packages/torch/cuda/comm.py"", line 92, in reduce_add
    nccl.reduce(inputs, outputs, root=destination)
  File ""/home/clp001/anaconda3/lib/python3.6/site-packages/torch/cuda/nccl.py"", line 161, in reduce
    assert(root >= 0 and root < len(inputs))
AssertionError
```",confirm bash file line module update file line file line file line backward self gradient file line backward file line backward return file line result destination file line file line reduce assert root root,issue,negative,neutral,neutral,neutral,neutral,neutral
338229022,"Any reason for random jitter in pix2pix test?
Is there an option to reproduce the same result in test consecutive runs?

Awesome code sharing. Very inspirational.",reason random jitter test option reproduce result test consecutive awesome code inspirational,issue,positive,positive,positive,positive,positive,positive
338106146,"Hi,

The followingg does not work
python train.py --dataroot /data/CycleGan --name shanghai_test_2 --model cycle_gan --no_dropout --display_id 0  --loadSize 256 --fineSize 256

The following does work:

python train.py --dataroot /data/CycleGan --name shanghai_test_2 --model cycle_gan --no_dropout --display_id 0 

This is the error I receive

> model [CycleGANModel] was created
create web directory ./checkpoints/test/web...
Traceback (most recent call last):
  File ""train.py"", line 21, in <module>
    for i, data in enumerate(dataset):
  File ""anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 201, in __next__
    return self._process_next_batch(batch)
  File ""envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 221, in _process_next_batch
    raise batch.exc_type(batch.exc_msg)
TypeError: Traceback (most recent call last):
  File ""anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 40, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File ""anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 40, in <listcomp>
    samples = collate_fn([dataset[i] for i in batch_indices])
  File ""PKU/pytorch-CycleGAN-and-pix2pix/data/unaligned_dataset.py"", line 34, in __getitem__
    A = self.transform(A_img)
  File ""anaconda3/envs/pytorch/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/transforms.py"", line 369, in __call__
    img = t(img)
  File ""anaconda3/envs/pytorch/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/transforms.py"", line 611, in __call__
    i, j, h, w = self.get_params(img, self.size)
TypeError: 'Image' object is not iterable
",hi work python name model following work python name model error receive model create web directory recent call last file line module data enumerate file line return batch file line raise recent call last file line file line file line file line file line object iterable,issue,negative,neutral,neutral,neutral,neutral,neutral
338094114,"I just trained a pix2pix facades model with `--loadSize 256 --fineSize 256`, and it worked. Could you post more information on the errors?",trained model worked could post information,issue,negative,neutral,neutral,neutral,neutral,neutral
338087238,Cropping and random jittering is a commonly used data augmentation technique to prevent overfitting. There is no specific reason for 286. You can try different sizes for your own applications,random commonly used data augmentation technique prevent specific reason try different size,issue,negative,negative,negative,negative,negative,negative
338087072,You are right. We just run our model on a horse sequence frame by frame. We first convert video to still images. ,right run model horse sequence frame frame first convert video still,issue,negative,positive,positive,positive,positive,positive
337581386,yes you are right. and i want to know why use random jitter? i mean you resize the input image to 286*286(why specific 286? reciptive field?) then crop 256*256 ？(for data augumentation?)  thanks a lot,yes right want know use random jitter mean resize input image specific field crop data thanks lot,issue,positive,negative,neutral,neutral,negative,negative
337550902,"You probably need to output the intermediate layer's activation, and then add L1 norm on that. So you should modify your code to have `y, middle_layer=G(x)`",probably need output intermediate layer activation add norm modify code,issue,negative,neutral,neutral,neutral,neutral,neutral
337350528,Did someone train the network for segmentation?,someone train network segmentation,issue,negative,neutral,neutral,neutral,neutral,neutral
336737526,"1. The ""70"" is implicit, it's not written anywhere in the code but instead emerges as a mathematical consequence of the network architecture.

2. The math is here: https://github.com/phillipi/pix2pix/blob/master/scripts/receptive_field_sizes.m",implicit written anywhere code instead mathematical consequence network architecture math,issue,negative,neutral,neutral,neutral,neutral,neutral
336692351,"I have a question.
1. I saw the code(class NLayerDiscriminator(nn.Module)), but I do not see the number 70 anywhere.
So why is it called 70x70 patchGAN?
that is, Why is it the number ***70***?

2. the output of the code is 30x30x1. (X_ij)
The patch of patchGAN was called 70x70. (ij)
You said, you traceback and found that patch ij is 70x70, how did you do it?
",question saw code class see number anywhere number output code patch said found patch,issue,negative,neutral,neutral,neutral,neutral,neutral
336689175,"Yes I get the same on a monet2photo that I trained myself. 

```
[CustomDatasetDataLoader
dataset [SingleImageDataset] was created
test
initialization method [xavier]
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    model = create_model(opt)
  File ""/home/isaac/Desktop/pytorch-CycleGAN-and-pix2pix/models/models.py"", line 19, in create_model
    model.initialize(opt)
  File ""/home/isaac/Desktop/pytorch-CycleGAN-and-pix2pix/models/test_model.py"", line 23, in initialize
    self.load_network(self.netG, 'G', which_epoch)
  File ""/home/isaac/Desktop/pytorch-CycleGAN-and-pix2pix/models/base_model.py"", line 53, in load_network
    network.load_state_dict(torch.load(save_path))
  File ""/home/isaac/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 355, in load_state_dict
    .format(name))
KeyError: 'unexpected key ""model.1.weight"" in state_dict]
```


This is the command I'm using

`python test.py --dataroot ./datasets/monet2photo/testA/ --name monet2photo --model test --which_model_netG unet_256 --which_direction AtoB --dataset_mode single`
`",yes get trained test method recent call last file line module model opt file line opt file line initialize file line file line name key model weight command python name model test single,issue,negative,negative,neutral,neutral,negative,negative
336154703,"This might be caused by different flags used in training and test scripts. For example, if one might use batchnorm for training, but have instancenorm for test. ",might different used training test example one might use training test,issue,negative,neutral,neutral,neutral,neutral,neutral
335383731,Could you try the latest commit? I merged the patch made by @AndersAsa ,could try latest commit patch made,issue,negative,positive,positive,positive,positive,positive
335383612,Sorry about that. I just merged the [patch](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/709d6153efdb4e8fe00c1285a3b55763946b32dc) made by @AndersAsa. and It should work. ,sorry patch made work,issue,negative,negative,negative,negative,negative,negative
335383084,"My bad. I only added init_type to G, and this commit added init_type to D as well. ",bad added commit added well,issue,negative,negative,negative,negative,negative,negative
335381904,"Hi!
Yes, I have tried it with the latest commit and I just tried again.
The pix2pix code works but the cycle gan code throws the error above also observed by soumyadeepg.
The cycle gan also throws the same error on the reference horse2zebra dataset and input_nc=3.
Before the latest commits the code ran smoothly.

If you do not mind, I will open a new issue, as, at least for me, the error is not related to the number of input color channels.


 ",hi yes tried latest commit tried code work cycle gan code error also cycle gan also error reference latest code ran smoothly mind open new issue least error related number input color,issue,negative,positive,positive,positive,positive,positive
335357603,"It should not matter so much. Currently, we use the max(#trainA, #trainB) for the dataset size. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/data/unaligned_dataset.py#L54) for more details. ",matter much currently use size see,issue,negative,neutral,neutral,neutral,neutral,neutral
335357493,"If you train a cyclegan model, you can either use `cyclegan` or `test` for `--model` flags. The difference is that the flag `cyclegan` takes image pairs (A,B) as inputs while the `test` takes only input images A.  ",train model either use test model difference flag image test input,issue,negative,neutral,neutral,neutral,neutral,neutral
335357304,It should be fixed with the latest commit. Could you have a try again?,fixed latest commit could try,issue,negative,positive,positive,positive,positive,positive
335357229,Thanks for the PR. But it was already reflected in the latest [commit](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/fe8ea339e8f8a9dac5d202ebe29f0a578f876b86),thanks already reflected latest commit,issue,positive,positive,positive,positive,positive,positive
335357121,"UNet gives slightly better results than Resnet in some of the pix2pix applications. We haven't  varied the depth of the UNet model, but it might be worth trying. ",slightly better varied depth model might worth trying,issue,positive,positive,positive,positive,positive,positive
335356966,You can definitely use a different batchSize. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/27) for more details. ,definitely use different see,issue,negative,neutral,neutral,neutral,neutral,neutral
335119291,"Hi!
Thanks for implementing input_nc=1!
While the pix2pix implementation still works nicely,  for the Cycle Gan I am getting the same error as soumyadeepg above. (fresh git pull about an hour ago.) 
The error occurs also for input_nc=3, so maybe a new issue should be filed?
to be precise the error is:
(...)
cycle_gan
initialization method [xavier]
initialization method [xavier]
initialization method [[0]]
Traceback (most recent call last):
  File ""train3.py"", line 15, in <module>
    model = create_model(opt)
  File ""C:\Users\Eulsen\Documents\Programs\Python\pytorch-CycleGAN-and-pix2pix\models\models.py"", line 19, in create_model
    model.initialize(opt)
  File ""C:\Users\Eulsen\Documents\Programs\Python\pytorch-CycleGAN-and-pix2pix\models\cycle_gan_model.py"", line 39, in initialize
    opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids)
  File ""C:\Users\Eulsen\Documents\Programs\Python\pytorch-CycleGAN-and-pix2pix\models\networks.py"", line 144, in define_D
    init_weights(netD, init_type=init_type)
  File ""C:\Users\Eulsen\Documents\Programs\Python\pytorch-CycleGAN-and-pix2pix\models\networks.py"", line 73, in init_weights
    raise NotImplementedError('initialization method [%s] is not implemented' % init_type)
NotImplementedError: initialization method [[0]] is not implemented

To me, it seems, as if something goes wrong with the initalization of the discriminator networks. However, I was not able to fix it myself.

Help would be much appreciated!!
",hi thanks implementation still work nicely cycle gan getting error fresh git pull hour ago error also maybe new issue precise error method method method recent call last file line module model opt file line opt file line initialize file line file line raise method method something go wrong discriminator however able fix help would much,issue,negative,positive,positive,positive,positive,positive
334978812,It should be fixed. Let me know if it works for you. ,fixed let know work,issue,negative,positive,neutral,neutral,positive,positive
334841045,"Also I am not being able to understand what is the difference between testing the cyclegan model and applying a pretrained cyclegan model to generate data from either A to B or B to A ??
In the readme file these two things are mentioned separately. 

I tried to test the model by the following command:
 
python test.py --dataroot ./datasets/casia_full_vis_hr_nir_hr/ --name load150fine128inp1out1_visHR_visLR_FULL_CASIA_new_atob_350iter --model cycle_gan --phase test --no_dropout

I got this error:

Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    model = create_model(opt)
  File ""/home/iiitd/soumyadeep/pytorch-CycleGAN-and-pix2pix-master/models/models.py"", line 19, in create_model
    model.initialize(opt)
  File ""/home/iiitd/soumyadeep/pytorch-CycleGAN-and-pix2pix-master/models/cycle_gan_model.py"", line 45, in initialize
    self.load_network(self.netG_A, 'G_A', which_epoch)
  File ""/home/iiitd/soumyadeep/pytorch-CycleGAN-and-pix2pix-master/models/base_model.py"", line 53, in load_network
    network.load_state_dict(torch.load(save_path))
  File ""/home/iiitd/.local/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 355, in load_state_dict
    .format(name))
KeyError: 'unexpected key ""model.2.weight"" in state_dict'


somebody please help !!!

Thanks in advance :)

@junyanz Can you please help ?
",also able understand difference testing model model generate data either file two separately tried test model following command python name model phase test got error recent call last file line module model opt file line opt file line initialize file line file line name key model weight somebody please help thanks advance please help,issue,positive,positive,positive,positive,positive,positive
334824923,We include a few popular initialization methods. Please use `--init_type` to choose the method. ,include popular please use choose method,issue,positive,positive,positive,positive,positive,positive
334807444,"[index.zip](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/files/1363332/index.zip)
@junyanz My training script is as follows:


import os
import time
from options.train_options import TrainOptions
from data.data_loader import CreateDataLoader
from models.models import create_model
from util.visualizer import Visualizer

os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""   
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""

opt = TrainOptions().parse()
data_loader = CreateDataLoader(opt)
dataset = data_loader.load_data()
dataset_size = len(data_loader)
print('#training images = %d' % dataset_size)

model = create_model(opt)
visualizer = Visualizer(opt)
total_steps = 0

for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):
    epoch_start_time = time.time()
    epoch_iter = 0
    for i, data in enumerate(dataset):
        iter_start_time = time.time()
        total_steps += opt.batchSize
        epoch_iter += opt.batchSize
        model.set_input(data)
        model.optimize_parameters()

        if total_steps % opt.display_freq == 0:
            visualizer.display_current_results(model.get_current_visuals(), epoch)

        if total_steps % opt.print_freq == 0:
            errors = model.get_current_errors()
            t = (time.time() - iter_start_time) / opt.batchSize
            visualizer.print_current_errors(epoch, epoch_iter, errors, t)
            if opt.display_id > 0:
                visualizer.plot_current_errors(epoch, float(epoch_iter)/dataset_size, opt, errors)

        if total_steps % opt.save_latest_freq == 0:
            print('saving the latest model (epoch %d, total_steps %d)' %
                  (epoch, total_steps))
            model.save('latest')

    if epoch % opt.save_epoch_freq == 0:
        print('saving the model at the end of epoch %d, iters %d' %
              (epoch, total_steps))
        model.save('latest')
        model.save(epoch)

    print('End of epoch %d / %d \t Time Taken: %d sec' %
          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))

    if epoch > opt.niter:
        model.update_learning_rate()


###----------------THE END OF SCRIPT---------------------------############
****

When I renamed *_G_A.pth to *_G.pth I got output images in the **result** folder like these. (unzip the index.zip file)







",training script import o import time import import import import visualizer opt opt print training model opt visualizer visualizer opt epoch range data enumerate data epoch epoch epoch float opt print latest model epoch epoch epoch print model end epoch epoch epoch print epoch time taken sec epoch epoch end script got output result folder like file,issue,negative,positive,positive,positive,positive,positive
334801736,"[As you can see in this line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L314), the first half is the skip connection, and the later half comes from the bottleneck. ",see line first half skip connection later half come bottleneck,issue,negative,negative,neutral,neutral,negative,negative
334791994,[DCGAN](https://arxiv.org/pdf/1511.06434.pdf) paper (Figure 5) used guided backpropagation to visualize the filters in discriminators. Not sure about visualization for generators. ,paper figure used visualize sure visualization,issue,negative,positive,positive,positive,positive,positive
334791471,I haven't tested the multi-gpu with the latest version. Could you give more details on your errors?,tested latest version could give,issue,negative,positive,positive,positive,positive,positive
334791153,"@soumyadeepg What's your training script? Your errors sometimes happen when you have different architectures/normalization configuration between training and test scripts. Also the test script with `--model test` will load the model `*_G.pth`. If you train a cyclegan model, you model will be named as `*_G_A.pth` and `*_G_A.pth`. So you have to change the name manually.  ",training script sometimes happen different configuration training test also test script model test load model train model model change name manually,issue,negative,neutral,neutral,neutral,neutral,neutral
334789142,@blindfish You need to use `--model test --dataset_mode single`. You got the error as you used `--model cycle_gan` with `--dataset_mode single`. ,blindfish need use model test single got error used model single,issue,negative,negative,neutral,neutral,negative,negative
334716134,"I am having the same problem. I used the following command....

python test.py --dataroot ./datasets/full_vis_hr/testB/ --name load150fine128inp1out1_new_btoa_350iter --model test --which_model_netG unet_256 --which_direction AtoB --dataset_mode single --norm batch

The error i am getting is this...

File ""test.py"", line 17, in <module>
    model = create_model(opt)
  File ""/home/iiitd/soumyadeep/pytorch-CycleGAN-and-pix2pix-master/models/models.py"", line 19, in create_model
    model.initialize(opt)
  File ""/home/iiitd/soumyadeep/pytorch-CycleGAN-and-pix2pix-master/models/test_model.py"", line 22, in initialize
    self.load_network(self.netG, 'G', which_epoch)
  File ""/home/iiitd/soumyadeep/pytorch-CycleGAN-and-pix2pix-master/models/base_model.py"", line 53, in load_network
    network.load_state_dict(torch.load(save_path))
  File ""/home/iiitd/.local/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 355, in load_state_dict
    .format(name))
KeyError: 'unexpected key ""model.1.weight"" in state_dict'



Also when I remove --which_model_netG unet_256 from the command I am getting some outputs but all the images have grey horizontal stripes and these outputs are not desirable. Please let me know if someone has successfully cracked this testing thing. ",problem used following command python name model test single norm batch error getting file line module model opt file line opt file line initialize file line file line name key model weight also remove command getting grey horizontal desirable please let know someone successfully cracked testing thing,issue,negative,positive,positive,positive,positive,positive
334180665,"Open base_options.py, and change loadSize and fineSize.
Save and then run test. 
After the model is built, you can test at higher resolutions than you can train.",open change save run test model built test higher train,issue,negative,positive,positive,positive,positive,positive
333983919,"I've run into problems changing the sampling to kernel size 4 for the sampling. @phillipi 

How did you go about changing that setting in the networks.py file?",run sampling kernel size sampling go setting file,issue,negative,neutral,neutral,neutral,neutral,neutral
332706827,"You probably can do the following: 
```python
opt.dataroot = your_sub_folder_name
data_loader = CreateDataLoader(opt)
dataset = data_loader.load_data()
```",probably following python opt,issue,negative,neutral,neutral,neutral,neutral,neutral
330552651,The problem starts from [commit ee5d38e287](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/ee5d38e287e3dda25fa159baa7232d6f6588267a). You can copy and paste changes from file options/train_options.py to options/test_options.py. This workaround fix problem for me.,problem commit copy paste file fix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
330395085,"@PraveerSINGH 
Sorry, i couldn't. 
I didn't find the reason, so I just commented out the code that cause the error.  ",sorry could find reason code cause error,issue,negative,negative,negative,negative,negative,negative
330176894,@wlwsea: Were you able to resolve this error ? I am having similar issue.,able resolve error similar issue,issue,negative,positive,positive,positive,positive,positive
330112092,"Hi  @khryang ,
It really work !
Thank you !",hi really work thank,issue,negative,positive,positive,positive,positive,positive
330111585,"@wlwsea  I changed the code how data insert in 'unaligned_dataset.py'.
look at the part of 'def __getitem__' .  the index of B is random.
At the test mode, I use "" index_B = index_A"" instead of random code.",code data insert look part index random test mode use instead random code,issue,negative,negative,negative,negative,negative,negative
330110804,"Hello @khryang ,  
I have the same problem with you.
I just used the command as follows, but the fakeA data i got is randomly.
`python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --no_dropout`
I have the file 'unaligned dataset.py' , but i don't know how to use it.
Could you tell me some details about how you did it ?
Thanks a ton !",hello problem used command data got randomly python name model file know use could tell thanks ton,issue,negative,negative,negative,negative,negative,negative
329679794,"@isalirezag  when you test the trained model, the latest trained network which named 'latest_net*.pth' will be loaded and used for test!
",test trained model latest trained network loaded used test,issue,negative,positive,positive,positive,positive,positive
329634343,"@PraveerSINGH : 
I just re-install PyTorch in my computer, and it didn't show that error again.
Reference:  [http://pytorch.org/](url)",computer show error reference,issue,negative,neutral,neutral,neutral,neutral,neutral
329500993,@wlwsea : I am having similar issue. How did you manage to solve this ?,similar issue manage solve,issue,negative,neutral,neutral,neutral,neutral,neutral
329230405,@khryang Soo I can reload this *.pth file letter if I want to reload the trained model?,reload file letter want reload trained model,issue,negative,neutral,neutral,neutral,neutral,neutral
329069900,"@isalirezag In checkpoints directory, you can see the file of *.pth. I know that this file is the trained network. every 5 epoch, it will be saved.

is this answer what you mean?",directory see file know file trained network every epoch saved answer mean,issue,negative,negative,negative,negative,negative,negative
328926733,"@khryang So it will not save the trained model, right? it is just saving the images in checkpoints",save trained model right saving,issue,negative,positive,positive,positive,positive,positive
328795087,"There is in the directory of 'checkpoints'
when you run the code with train.py, the 'checkpoints' directory will be created in the directory where you run the 'train.py' code.

And the results of test are in the 'results' directory. It is also created when you run the 'test.py' code.",directory run code directory directory run code test directory also run code,issue,negative,neutral,neutral,neutral,neutral,neutral
328526395,"Yeah, sorry for the inconvenience I will clean the code up in two weeks, and make another PR, with all the listed details - if that is acceptable for you",yeah sorry inconvenience clean code two make another listed acceptable,issue,positive,negative,neutral,neutral,negative,negative
326850577,"I have solved this problem.
Since I use opencv2 to handle image, the color was change.
Your project  processes image by PIL. I get it. THKS!",problem since use handle image color change project image get,issue,negative,neutral,neutral,neutral,neutral,neutral
326848903,"What will happen if you save the image again, and open it? Could you show a side-by-side example?",happen save image open could show example,issue,negative,neutral,neutral,neutral,neutral,neutral
326848736,"It's possible. But it is not implemented in the current repo. You can copy and paste the `backward_` functions, and comment `loss_*.backward()` in each function.",possible current copy paste comment function,issue,negative,neutral,neutral,neutral,neutral,neutral
326848454,"@junyanz Sorry but I am a little confused.
Can we even have Loss for the testing procedure? If so what loss would it be? because I do not see any loss in the testing codes...",sorry little confused even loss testing procedure loss would see loss testing,issue,negative,negative,negative,negative,negative,negative
326508232,Just notice that this will also turn dropout off. I will push a better version which only changes the BN mode.,notice also turn dropout push better version mode,issue,negative,positive,positive,positive,positive,positive
326432919,You can call the same function in the test mode. ,call function test mode,issue,negative,neutral,neutral,neutral,neutral,neutral
326318440,"That would be great!! It would allow me to process larger pictures.
Thank you!!!

Btw. it seems that that pytorch implementations in general need a lot more gpu ram than 
for example Tensorflow implementations of the same problem.
Is this a special feature of pytorch or is this generic for torch implementations?
",would great would allow process thank general need lot ram example problem special feature generic torch,issue,positive,positive,positive,positive,positive,positive
326122836,`nc=1` is currently not well supported. I will look at it later if you I have time in the following week. ,currently well look later time following week,issue,negative,neutral,neutral,neutral,neutral,neutral
326122377,"Yeah, it is still an ongoing project. We recently fixed a few issues/differences between PyTorch and Torch, and the results look better. I wonder when you downloaded the code, and if you have tried the latest one. ",yeah still ongoing project recently fixed torch look better wonder code tried latest one,issue,positive,positive,positive,positive,positive,positive
326121978,"See the argparse [document](https://docs.python.org/2/library/argparse.html) for more details.
By default, `opt.serial_batches=False`. If you added the flag `--serial_batches` in the command line, it will make `opt.serial_batches=True`, and it will not shuffle the input data. This is helpful if you want to fix the order of your test images.",see document default added flag command line make shuffle input data helpful want fix order test,issue,negative,neutral,neutral,neutral,neutral,neutral
325556322,"@junyanz  ah.. but in the previous version of cycle-gan code, it used the default setting of dataset as 'aligned data' ----> (at here, in base_option) https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/commit/e6858e35f0a08c6139c133122d222d0d85e8005d

that time I trained the network(cycle-gan), but during training the data was not imported with paired.
Anyway, it showed good result. ",ah previous version code used default setting data time trained network training data paired anyway good result,issue,negative,positive,positive,positive,positive,positive
325466617,"I guess you need to add `--norm batch` to the test script. By default, it will use `--norm instance`.",guess need add norm batch test script default use norm instance,issue,negative,neutral,neutral,neutral,neutral,neutral
325328862,"Great news, thank you for the update!",great news thank update,issue,positive,positive,positive,positive,positive,positive
325262751,"Yes, `aligned` means that the input and output images are aligned/paired (e.g. in pix2pix). `Unaligned` means that the input/output images are unpaired (in cyclegan), and `single` means that only input images are needed/used.  I will probably change the naming to paired/unpaired later.",yes input output unaligned unpaired single input probably change naming later,issue,negative,negative,neutral,neutral,negative,negative
325254304,"@junyanz  thanks for your answer.
where can I find the comment about 'epoch_count' in your code?
I can't find it in the code of 'train_option' and 'base_option'
",thanks answer find comment code ca find code,issue,negative,positive,positive,positive,positive,positive
325215198,"Hey,
do you have any training curves? I tried your code and the results do not look too good ... would be great if you can provide the training curves for a comparison, thanks a lot!
",hey training tried code look good would great provide training comparison thanks lot,issue,positive,positive,positive,positive,positive,positive
325171517,"I just added the `--epoch_count` flag. By default, the program will initialize the epoch count as 1. Set `--epoch_count` to specify a different starting epoch count.",added flag default program initialize epoch count set specify different starting epoch count,issue,negative,neutral,neutral,neutral,neutral,neutral
325171489,"To fine-tune a pre-trained model, or resume the previous training, use the `--continue_train` flag. The program will then load the model based on `which_epoch`. By default, the program will initialize the epoch count as 1. Set `--epoch_count <int>` to specify a different starting epoch count.
",model resume previous training use flag program load model based default program initialize epoch count set specify different starting epoch count,issue,negative,negative,neutral,neutral,negative,negative
325107015,"Thanks a lot, @hazirbas. This is a great catch. ",thanks lot great catch,issue,positive,positive,positive,positive,positive,positive
324858729,"@SsnL Hey, thanks a lot for your comments. My solution to transforms was to send Lambda function which uses numpy.random. Then it was consistent for me.",hey thanks lot solution send lambda function consistent,issue,positive,positive,positive,positive,positive,positive
324830871,"Okay, so the reason that `torchvision.transforms` can give different results is the multithread data loading. There is no guarantee on the order that workers finish. I tried setting seed basing on index, which seemed giving consistent data across training sessions. But it will crop the same patch for each image, which is not ideal. Seed option seem impossible to implement without doing some serious change to the code in either this repo or the pytorch repo. So I will close and abandon. ",reason give different data loading guarantee order finish tried setting seed index giving consistent data across training session crop patch image ideal seed option seem impossible implement without serious change code either close abandon,issue,positive,positive,neutral,neutral,positive,positive
324825125,"@hazirbas I tried initialization with same seed, the weights are identical. This is likely from a recent pytorch change that added setting GPU seeds in `manual_seed`. I will change the code so it explicitly set GPU seeds in order to not depend on most recent pytorch.

About `torchvision.transforms`, I realize that they do give different results even if we set same seed. Thanks for helping me finding out this bug. I'll research and see if there are any solutions.",tried seed identical likely recent change added setting change code explicitly set order depend recent realize give different even set seed thanks helping finding bug research see,issue,positive,positive,neutral,neutral,positive,positive
324733474,"Hi @junyanz and @khryang ,
Since batchnorm mode is not set to evaluation, batches are normalized by their mean and variance.",hi since mode set evaluation mean variance,issue,negative,negative,negative,negative,negative,negative
324576966,"Did you have any problem regarding the initialization of the weights and biases for normalization? I tried this for pix2pix and setting the seed did not make it reproducable due to using different mean and variance in the ..data.weight.normal_() function:
networks.py, line 15 and 17:
`m.weight.data.normal_(0.0, 0.02)
m.weight.data.normal_(1.0, 0.02)`

Moreover, random.seed setting did not set the seed and torchvision.transforms were different at each run.",problem regarding normalization tried setting seed make due different mean variance function line moreover setting set seed different run,issue,negative,negative,negative,negative,negative,negative
324515281,"I have the same bug,,when I setting `--nThreads 0 and --display_id `，，，Do` I need to run the program after the file changes, such that `python base_options.py` ? I am note sure the run sequence .",bug setting need run program file python note sure run sequence,issue,negative,positive,positive,positive,positive,positive
324197285,"@junyanz  oh! I see, thank you for nice and fast answering ",oh see thank nice fast,issue,positive,positive,positive,positive,positive,positive
323971271,"@khryang  Thanks for your nice words. By default, the code should use the instancenorm. When batchSize=1, instancenorm=batchnorm, and, you are right, the normalization applies to each image. ",thanks nice default code use right normalization image,issue,positive,positive,positive,positive,positive,positive
323970697,@AliceMa-thss15  you can use cpu by setting `--gpu_ids -1`. See [training details](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#trainingtest-details) for more information.,use setting see training information,issue,negative,neutral,neutral,neutral,neutral,neutral
323944522,@junyanz I got stuck in exactly the same step. My pytorch has been installed correctly but I cannot use gpu  on my own laptop. Can I run this project without gpu?,got stuck exactly step correctly use run project without,issue,negative,positive,positive,positive,positive,positive
323485210,"Thanks for your PR, @botcs !

I took a brief look at the code. This PR contains much more than what you have described. You should definitely clean up stuff like normalization, video, printing, etc. 

Some questions:
1. Why [change strided Conv to Conv+MaxPooling in downsampling](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/64/files#diff-a56d77751e639c0dd4a453fc554f55dbR167)?
2. Why use bilinear interpolation? The post authors suggest nearest neighbor gives them best results. What's the reasoning behind your choice? And maybe make it an option?

By the way, nn.Upsample is available now. You may also want to uncomment [this line](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/pull/64/files#diff-a56d77751e639c0dd4a453fc554f55dbR186) and remove the now deprecated nn.UpsamplingBilinear2d? :)",thanks took brief look code much definitely clean stuff like normalization video printing change use bilinear interpolation post suggest nearest neighbor best reasoning behind choice maybe make option way available may also want line remove,issue,positive,positive,positive,positive,positive,positive
323218104,"For running torch.cat((self.real_A, self.fake_B), 1), we just compare the current fake_B and current real_A, however we could use history image pool data to help the discriminator doesn't forget what it has done wrong before. You can refer to SimGAN, and the following image
![screen shot 2017-08-17 at 7 08 49 pm](https://user-images.githubusercontent.com/11957155/29437617-a1dbfb04-837f-11e7-91c0-55b237c2e619.png)
 ",running compare current current however could use history image pool data help discriminator forget done wrong refer following image screen shot,issue,negative,negative,negative,negative,negative,negative
323066942,"What's the benefit compared to just feed torch.cat((self.real_A, self.fake_B), 1) directly every time?",benefit feed directly every time,issue,negative,positive,neutral,neutral,positive,positive
322914950,"nThreads: the data loader is [multithreaded](https://en.wikipedia.org/wiki/Multithreading_(computer_architecture)) so that it can use multiple CPU cores to load images.

resize_or_crop: yeah, I think 'crop' will do what you want.

pool_size: the discriminator is trained against the current batch of generated images as well as images generated on previous iterations. Essentially, we remember the last pool_size generated images then randomly sample from this pool to create a batch_size batch of images to do one iteration of backprop on. This helps to stabilize training, kind of like experience replay.",data loader multithreaded use multiple load yeah think want discriminator trained current batch well previous essentially remember last randomly sample pool create batch one iteration stabilize training kind like experience replay,issue,positive,negative,neutral,neutral,negative,negative
322908732,"Could be related to this issue: https://distill.pub/2016/deconv-checkerboard/

The solution proposed there is to replace deconvolution with upsampling followed by stride 1 convolution. The cost is a naive implementation will be ~4 times as slow (but you could make it almost as fast with a smarter implementation, see the matrices in the distill post). 

Another partial solution is to use a combination of kernel sizes and strides that minimize the unequal sampling problem (e.g., use kernel size 4 and stride 2; you can see the effect in the interactive widget on the distill post).

In my experience, the artifacts usually go away with enough training, as the adversary can notice these artifacts and eventually the generator will adjust its weights to avoid them, even with a naive architecture. The architecture tricks above could help get to good solutions faster, but in general we found them to be unnecessary if you just train long enough.",could related issue solution replace stride convolution cost naive implementation time slow could make almost fast implementation see matrix distill post another partial solution use combination kernel size minimize unequal sampling problem use kernel size stride see effect interactive distill post experience usually go away enough training adversary notice eventually generator adjust avoid even naive architecture architecture could help get good faster general found unnecessary train long enough,issue,negative,negative,neutral,neutral,negative,negative
322847917,"To be simple, we want the discriminator to remember what it has did wrong/right before, as it doesn't have the ability to memorize history information.",simple want discriminator remember ability memorize history information,issue,negative,neutral,neutral,neutral,neutral,neutral
322677138,"@lyhangustc when you change pool_size, loadSize,fineSize  in training, should you change it in testing as well?
Also, can you please what is the difference between loadSize  and fineSize ? and what does pool_size do?

Thanks",change training change testing well also please difference thanks,issue,positive,positive,positive,positive,positive,positive
322031974,"You probably didn't install the cuda, gpu driver and/or pytorch correctly. Are you able to run some basic pytorch [examples](https://github.com/jcjohnson/pytorch-examples)? ",probably install driver correctly able run basic,issue,negative,positive,positive,positive,positive,positive
321814098,"Thanks,  I use your ways .  And I have solved the problem. Thank you very much.",thanks use way problem thank much,issue,negative,positive,positive,positive,positive,positive
320394466,It can run with multi threads (e.g. `opt.nThreads = 2`). We set it as 1 as testing is quite fast by itself.,run set testing quite fast,issue,negative,positive,positive,positive,positive,positive
319816557,"Here's my result of test images on Torch/Pytorch. They look similar. 

https://ibb.co/jYm3AQ

@petergerten, I had similar result as yours when we had affine=True as the default option in our Instance Normalization code. Setting affine=False seemed to improve the result. Our Torch implementation uses affine=False. 

The image below shows the difference between affine=True and False. It is subtle but noticeable. 

From left, columns are input horse, affine=True, affine=False, input zebra, affine=True, affine=False. 

https://ibb.co/cRUjH5

Does this look similar to your situation? We updated the codebase on Jul 3rd (233630e79d79901faff420eb0ae481b35d952f9). Have you run your code after this commit?",result test look similar similar result default option instance normalization code setting improve result torch implementation image difference false subtle noticeable left input horse input zebra look similar situation run code commit,issue,positive,negative,negative,negative,negative,negative
319580338,"Did you test the horse2zebra dataset yet? Also do you intend to share your pretrained model? I would really like to compare as my results do not look good at all.

http://imgur.com/a/yWuGF",test yet also intend share model would really like compare look good,issue,positive,positive,positive,positive,positive,positive
318819314,I would also be very interested? Anybody has a pretrained model?,would also interested anybody model,issue,negative,positive,positive,positive,positive,positive
318798840,"Yes, they work together to reduce the cycle loss. ",yes work together reduce cycle loss,issue,negative,neutral,neutral,neutral,neutral,neutral
318345819,"I am now testing a transformer-net alternative with PixelShuffle using Fast-Neural-Style loss, CycleGAN still have some convergence issues, and the problem seems to be invariant of the capacity",testing alternative loss still convergence problem invariant capacity,issue,negative,neutral,neutral,neutral,neutral,neutral
317626782,"After I check again, seems I put the images into wrong folder, so it's not a bug after all. 
Issue closed.",check put wrong folder bug issue closed,issue,negative,negative,negative,negative,negative,negative
316936218,"Yes, it will restart the epoch count, and rewrite the results webpage. Maybe one can add a  '--epoch_count' flag to address this issue. Feel free to send a pull request if you can fix it.  Otherwise, we will look at it later. ",yes restart epoch count rewrite maybe one add flag address issue feel free send pull request fix otherwise look later,issue,positive,positive,positive,positive,positive,positive
316382197,"I work with virtualenvs, and when I installed from package I believe that there were conflicts, and after I uninstalled torchvision from pip, the installation from source started to work, and it resolved this issue",work package believe uninstalled pip installation source work resolved issue,issue,negative,neutral,neutral,neutral,neutral,neutral
315347376,I was running with CPU and had the same problem. Setting --nThreads 0 and --display_id 0 solved the problem for me.,running problem setting problem,issue,negative,neutral,neutral,neutral,neutral,neutral
315331217,"@AlexanLee 
May be you should install the torchvision package from the source:

`git clone https://github.com/pytorch/vision`
`cd vision`
`python setup.py install`

related issue: #42 ",may install package source git clone vision python install related issue,issue,negative,neutral,neutral,neutral,neutral,neutral
315114070,"@yeguixin yes you are right. At the time being, the code for pix2pix still requires square image.",yes right time code still square image,issue,negative,positive,positive,positive,positive,positive
315066056,"When I use the unaligned dataset. I encounter the error as follows:
------------ Options -------------
batchSize: 1
beta1: 0.5
checkpoints_dir: ./checkpoints
continue_train: False
dataroot: ./datasets/captcha/
dataset_mode: unaligned
display_freq: 100
display_id: 1
display_port: 8097
display_single_pane_ncols: 0
display_winsize: 256
fineSize: 100
gpu_ids: [0]
identity: 0.0
input_nc: 3
isTrain: True
lambda_A: 100.0
lambda_B: 10.0
loadSize: 100
lr: 0.0002
max_dataset_size: inf
model: pix2pix
nThreads: 2
n_layers_D: 3
name: captcha_model
ndf: 64
ngf: 64
niter: 100
niter_decay: 100
no_flip: False
no_html: False
no_lsgan: True
norm: batch
output_nc: 3
phase: train
pool_size: 50
print_freq: 100
resize_or_crop: scale_width
save_epoch_freq: 5
save_latest_freq: 5000
serial_batches: False
use_dropout: False
which_direction: AtoB
which_epoch: latest
which_model_netD: basic
which_model_netG: unet_256
-------------- End ----------------
CustomDatasetDataLoader
dataset [UnalignedDataset] was created
#training images = 20000
pix2pix
Traceback (most recent call last):
  File ""train.py"", line 13, in <module>
    model = create_model(opt)
  File ""/home/nisl/gxye/program/pytorch-CycleGAN-and-pix2pix_rectangle/models/models.py"", line 10, in create_model
    assert(opt.dataset_mode == 'aligned')
AssertionError

Where the command line as follows:
python train.py --dataroot ./datasets/captcha/ --name captcha_model --model pix2pix --which_model_netG unet_256 --which_direction AtoB --lambda_A 100 --dataset_mode unaligned --no_lsgan --norm batch --loadSize 100 --fineSize 100 --resize_or_crop scale_width

I read the code in details to analyze the reason and found that the pix2pix model has to use the aligned dataset. That is the training images must to be square using pix2pix model. Isn't it?  Sorry for do not clear which model that I used before. Nevertheless, Thank you very much.",use unaligned encounter error beta false unaligned identity true model name niter false false true norm batch phase train false false latest basic end training recent call last file line module model opt file line assert command line python name model unaligned norm batch read code analyze reason found model use training must square model sorry clear model used nevertheless thank much,issue,positive,negative,neutral,neutral,negative,negative
314657991,"yeah you get this error because you're using the ""aligned"" mode, but the code in my fork is only modified to work on rectangular images with the ""unaligned"" mode.",yeah get error mode code fork work rectangular unaligned mode,issue,negative,neutral,neutral,neutral,neutral,neutral
314633001,"I tried the `torch.cuda.set_device(opt.gpu_ids[0])` but it didn't work for me. Let me know if you can make it work, and I can merge it through pull request. ",tried work let know make work merge pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
314631953,"@junyanz thx～～
We also can set `torch.cuda.set_device(opt.gpu_ids[0])` before the model's initialization. See [here](https://github.com/pytorch/pytorch/issues/1150) for more details.",also set model see,issue,negative,neutral,neutral,neutral,neutral,neutral
314629535,"I am aware of the bug but I haven't found a fix. For the current code, you always need to use gpu0. However, you can add flag ""CUDA_VISIBLE_DEVICES=1"" to specify the GPU when running the python script while setting the `gpu_ids 0`. See [here](https://discuss.pytorch.org/t/select-gpu-device-through-env-vars/318)  for more details.  ",aware bug found fix current code always need use however add flag specify running python script setting see,issue,negative,positive,positive,positive,positive,positive
314620706,"Got it. Thanks for your timely replay. Suppose the size of the image is 160*70pix. I can set in the command line as ""--loadSize 160 --loadHeight 70 --resize_or_crop no_resize no_flip"". Isn't is?
When I train the mode as follows, the error occurs:
--------------------------------------------------------------------------------------------------------------------
python train.py --dataroot ./datasets/captcha/ --name captcha_mdoel --model pix2pix --which_model_netG unet_256 --which_direction AtoB --lambda_A 100 --dataset_mode aligned --use_dropout --no_lsgan --norm batch --loadSize 100 --loadHeight 70 --resize_or_crop no_resize --no_flip 
--------------------------------------------------------------------------------------------------------------------
errors:
--------------------------------------------------------------------------------------------------------------------
CustomDatasetDataLoader
dataset [AlignedDataset] was created
Traceback (most recent call last):
  File ""train.py"", line 8, in <module>
    data_loader = CreateDataLoader(opt)
  File ""/home/gxy/kx/program/pytorch-CycleGAN-and-pix2pix/data/data_loader.py"", line 6, in CreateDataLoader
    data_loader.initialize(opt)
  File ""/home/gxy/kx/program/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py"", line 30, in initialize
    self.dataset = CreateDataset(opt)
  File ""/home/gxy/kx/program/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py"", line 20, in CreateDataset
    dataset.initialize(opt)
  File ""/home/gxy/kx/program/pytorch-CycleGAN-and-pix2pix/data/aligned_dataset.py"", line 18, in initialize
    assert(opt.resize_or_crop == 'resize_and_crop')
AssertionError",got thanks timely replay suppose size image pix set command line train mode error python name model norm batch recent call last file line module opt file line opt file line initialize opt file line opt file line initialize assert,issue,negative,positive,neutral,neutral,positive,positive
314488253,"You need to install the latest torch vision library from the source. 
```bash
git clone https://github.com/pytorch/vision
cd vision
python setup.py install
```",need install latest torch vision library source bash git clone vision python install,issue,negative,positive,positive,positive,positive,positive
314463241,"I have the same problem but i got this message when I tried to download pyTorch:

```
pip install http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp27-none-linux_x86_64.whl --user
Requirement already satisfied: torch==0.1.12.post2 from http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp27-none-linux_x86_64.whl in /usr/local/lib/python2.7/dist-packages
Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from torch==0.1.12.post2)
```
It looks like I have the last pyTorch version but Is till get the same error. Did you guys soleved just by installing the last pyTorch version?
",problem got message tried pip install user requirement already satisfied post requirement already satisfied post like last version till get error last version,issue,negative,positive,positive,positive,positive,positive
314461547,"I'm very sorry. I got confused in my previous post, as I have two related projects with similar issues.
The height has to be passed as a parameter in your command line, see the file base_options.py in the options folder.
Also make sure that --resize_or_crop is set to no-resize
and the no-flip switch is set in your command line.
",sorry got confused previous post two related similar height parameter command line see file folder also make sure set switch set command line,issue,negative,negative,neutral,neutral,negative,negative
314455654,Thanks. But I didn't find the config.py and songToData.py in the project. Could you give me a link? Thanks again.,thanks find project could give link thanks,issue,positive,positive,positive,positive,positive,positive
314436698,"you have to set the desired width and height in config.py and songToData.py 
[EDIT: >THIS IS WRONG!]",set desired width height edit wrong,issue,negative,negative,negative,negative,negative,negative
314435156,"Can the forking project  train the rectangle images? If it can, how to set the options? @junyanz",project train rectangle set,issue,negative,neutral,neutral,neutral,neutral,neutral
313872444,"At the innermost layer of UNet, the tensor is of size 1x1. If you normalize it, the value becomes 0, so we did not add normalization layer at the innermost bottleneck. ",innermost layer tensor size normalize value becomes add normalization layer innermost bottleneck,issue,negative,neutral,neutral,neutral,neutral,neutral
313581092,"Got it. I increased the number of training sets to 50,000 and the result become better than previous. Thank you very much. ",got number training result become better previous thank much,issue,positive,positive,positive,positive,positive,positive
313486823,Thank you!  I make the samples square but thought to tell you.  Thanks again for the great library.,thank make square thought tell thanks great library,issue,positive,positive,positive,positive,positive,positive
313390589,I think it is possible as people have used pix2pix for generating Chinese Calligraphy. See [zi2zi](https://github.com/kaonashi-tyc/zi2zi) project for more details. I will recommend the default learning rate and batchSize as a starting point. ,think possible people used generating calligraphy see project recommend default learning rate starting point,issue,negative,neutral,neutral,neutral,neutral,neutral
313389949,"Yes, they are horizontally flipped and no other directions. Thank you very much.
In addition, I want to use pix2pix to train a model which aims to transform the the image shown on real_A (epoch 17)to the image shown on real_B (epoch 17). Do you think pix2pix model can do it according to your rich experience? In recent days, I trained some models using different options such as different batchSize and learning rate. But the result is much worse than expected. It seems that the model does not converge because the value of G_GAN has been fluctuating. Thanks for your advise.",yes horizontally thank much addition want use train model transform image shown epoch image shown epoch think model according rich experience recent day trained different different learning rate result much worse model converge value thanks advise,issue,positive,positive,neutral,neutral,positive,positive
313279001,"The training images are horizontally flipped with 50% probability. If you want to prevent this, please specify --no_flip as one of the options. 

Did you observe flipping or rotation in any other direction?",training horizontally probability want prevent please specify one observe rotation direction,issue,negative,neutral,neutral,neutral,neutral,neutral
313151297,"Thank you very much for your kind response. I made a fork of the project, where I'm playing with the code. I hope you don't mind. If I find anything useful, I'll let you know.
Currently my main change actually implements the change in the size of the slices, as described in the changelog.",thank much kind response made fork project code hope mind find anything useful let know currently main change actually change size,issue,positive,positive,positive,positive,positive,positive
312923437,The images should not be rotated. Did you try the code on our datasets or your data? Could you share a screenshot?,rotated try code data could share,issue,negative,neutral,neutral,neutral,neutral,neutral
312922682,It might be related to the data_loader. Could you try set `--nThreads 1` and see what will happen?,might related could try set see happen,issue,negative,neutral,neutral,neutral,neutral,neutral
312921106,Thanks a lot. This is quite helpful!,thanks lot quite helpful,issue,positive,positive,positive,positive,positive,positive
312759617,"![screenshot from 2017-07-04 09_23_36](https://user-images.githubusercontent.com/28495909/27811625-02482010-609b-11e7-84b6-ca04e37de4c2.png)
thank you very much. But i have not solve the problem, i have run the command python -m visdom.server.  I set the --display_id = 1.  I open the http://localhost:8097, nothing in the html.
So i need your help.
Thank you very much!  And what should i do next?
",thank much solve problem run command python set open nothing need help thank much next,issue,negative,positive,neutral,neutral,positive,positive
312755314,"Dear junyanz,

Thanks for your quick reply.",dear thanks quick reply,issue,positive,positive,positive,positive,positive,positive
312700329,"We haven't tried the dropout for CycleGAN. I guess it might make it more difficult to reconstruct the original input image. Currently, the option is off and will produce the same output. ",tried dropout guess might make difficult reconstruct original input image currently option produce output,issue,negative,negative,neutral,neutral,negative,negative
312544285,"The current code assumes `width==height`.  However, it is possible to modify the code to work on the rectangular cases.  You need to modify the dataset class under `/data` subdirectory. ",current code however possible modify code work rectangular need modify class,issue,negative,neutral,neutral,neutral,neutral,neutral
312543968,"Please install the `visdom` library, and run the following command `python -m visdom.server`. Please see more details in [training/test details](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#trainingtest-details).",please install library run following command python please see,issue,positive,neutral,neutral,neutral,neutral,neutral
312405997,"I try to modify the aligned_dataset.py as follows:
---------------------------------------------------------------
        AB = AB.resize((160 * 2, 70), Image.BICUBIC)
        AB = self.transform(AB)

        self.opt.loadSize = 160
        self.opt.fineSize = 160
----------------------------------------------------------------
Here the size of my image is 160*70pix，but error occurs when runing train.py. The erroe messagge is:
RuntimeError: CUDNN_STATUS_BAD_PARAM
I don't know where is wrong. Did you solved it? @ianni67 
In addition, can the program only to train square images? @junyanz 
Thank you.",try modify size image error know wrong addition program train square thank,issue,negative,negative,negative,negative,negative,negative
312186259,"Hi @yeguixin, 

unfortunately, for aligned_dataset.py, we did not implement modes other than 'resize_and_crop'. Feel free to implement your own. ;-)",hi unfortunately implement feel free implement,issue,negative,negative,neutral,neutral,negative,negative
312149832,"I also encounter the similar error ""RuntimError: SUDNN_STATUS_BAD_PARAM"". When I set --loadSize 160 and --fineSize 160 during training pix2pix model. Both the input and output images are 3-channel.
Also, When adding the parameters --resize_or_crop scale_width (python train.py --dataroot ./datasets/captcha --name captcha_model --model pix2pix --which_model_netG unet_256 --which_direction AtoB --lambda_A 100 --dataset_mode aligned --use_dropout --no_lsgan --loadSize 160 --fineSize 160 --resize_or_crop scale_width), the error as follows: 
-------------------------------------------------------------------------------
CustomDatasetDataLoader
dataset [AlignedDataset] was created
Traceback (most recent call last):
  File ""test.py"", line 16, in <module>
    data_loader = CreateDataLoader(opt)
  File ""/home/gxy/program/pytorch-CycleGAN-and-pix2pix/data/data_loader.py"", line 6, in CreateDataLoader
    data_loader.initialize(opt)
  File ""/home/gxy/program/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py"", line 30, in initialize
    self.dataset = CreateDataset(opt)
  File ""/home/gxy/program/pytorch-CycleGAN-and-pix2pix/data/custom_dataset_data_loader.py"", line 20, in CreateDataset
    dataset.initialize(opt)
  File ""/home/gxy/program/pytorch-CycleGAN-and-pix2pix/data/aligned_dataset.py"", line 18, in initialize
    assert(opt.resize_or_crop == 'resize_and_crop')
AssertionError
-------------------------------------------------------------------------------------------------------------------
Note that the size of train images is 160*70pix",also encounter similar error set training model input output also python name model error recent call last file line module opt file line opt file line initialize opt file line opt file line initialize assert note size train pix,issue,negative,neutral,neutral,neutral,neutral,neutral
311964965,"According to the [traininng/test details](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix#trainingtest-details), you can set `--gpu_ids -1` to use CPU mode. ",according set use mode,issue,negative,neutral,neutral,neutral,neutral,neutral
311950319,"Also wondering this.  Readme states prerequesits as ""CPU or NVIDIA GPU + CUDA CuDNN."", so I assumed that meant CPU only supported.",also wondering assumed meant,issue,negative,neutral,neutral,neutral,neutral,neutral
311924083,"It seems that no optional arguments  for work with cpu only

```
optional arguments:
  -h, --help            show this help message and exit
  --dataroot DATAROOT   path to images (should have subfolders trainA, trainB,
                        valA, valB, etc)
  --batchSize BATCHSIZE
                        input batch size
  --loadSize LOADSIZE   scale images to this size
  --fineSize FINESIZE   then crop to this size
  --input_nc INPUT_NC   # of input image channels
  --output_nc OUTPUT_NC
                        # of output image channels
  --ngf NGF             # of gen filters in first conv layer
  --ndf NDF             # of discrim filters in first conv layer
  --which_model_netD WHICH_MODEL_NETD
                        selects model to use for netD
  --which_model_netG WHICH_MODEL_NETG
                        selects model to use for netG
  --n_layers_D N_LAYERS_D
                        only used if which_model_netD==n_layers
  --gpu_ids GPU_IDS     gpu ids: e.g. 0 0,1,2, 0,2
  --name NAME           name of the experiment. It decides where to store
                        samples and models
  --dataset_mode DATASET_MODE
                        chooses how datasets are loaded. [unaligned | aligned
                        | single]
  --model MODEL         chooses which model to use. cycle_gan, pix2pix, test
  --which_direction WHICH_DIRECTION
                        AtoB or BtoA
  --nThreads NTHREADS   # threads for loading data
  --checkpoints_dir CHECKPOINTS_DIR
                        models are saved here
  --norm NORM           instance normalization or batch normalization
  --serial_batches      if true, takes images in order to make batches,
                        otherwise takes them randomly
  --display_winsize DISPLAY_WINSIZE
                        display window size
  --display_id DISPLAY_ID
                        window id of the web display
  --display_port DISPLAY_PORT
                        visdom port of the web display
  --display_single_pane_ncols DISPLAY_SINGLE_PANE_NCOLS
                        if positive, display all images in a single visdom web
                        panel with certain number of images per row.
  --identity IDENTITY   use identity mapping. Setting identity other than 1
                        has an effect of scaling the weight of the identity
                        mapping loss. For example, if the weight of the
                        identity loss should be 10 times smaller than the
                        weight of the reconstruction loss, please set
                        optidentity = 0.1
  --use_dropout         use dropout for the generator
  --max_dataset_size MAX_DATASET_SIZE
                        Maximum number of samples allowed per dataset. If the
                        dataset directory contains more than max_dataset_size,
                        only a subset is loaded.
  --resize_or_crop RESIZE_OR_CROP
                        scaling and cropping of images at load time
                        [resize_and_crop|crop|scale_width]
  --no_flip             if specified, do not flip the images for data
                        argumentation
  --display_freq DISPLAY_FREQ
                        frequency of showing training results on screen
  --print_freq PRINT_FREQ
                        frequency of showing training results on console
  --save_latest_freq SAVE_LATEST_FREQ
                        frequency of saving the latest results
  --save_epoch_freq SAVE_EPOCH_FREQ
                        frequency of saving checkpoints at the end of epochs
  --continue_train      continue training: load the latest model
  --phase PHASE         train, val, test, etc
  --which_epoch WHICH_EPOCH
                        which epoch to load? set to latest to use latest
                        cached model
  --niter NITER         # of iter at starting learning rate
  --niter_decay NITER_DECAY
                        # of iter to linearly decay learning rate to zero
  --beta1 BETA1         momentum term of adam
  --lr LR               initial learning rate for adam
  --no_lsgan            do *not* use least square GAN, if false, use vanilla
                        GAN
  --lambda_A LAMBDA_A   weight for cycle loss (A -> B -> A)
  --lambda_B LAMBDA_B   weight for cycle loss (B -> A -> B)
  --pool_size POOL_SIZE
                        the size of image buffer that stores previously
                        generated images
  --no_html             do not save intermediate training results to
                        [opt.checkpoints_dir]/[opt.name]/web/  
```",optional work optional help show help message exit path input batch size scale size crop size input image output image gen first layer first layer model use model use used name name name experiment store loaded unaligned single model model model use test loading data saved norm norm instance normalization batch normalization true order make otherwise randomly display window size window id web display port web display positive display single web panel certain number per row identity identity use identity setting identity effect scaling weight identity loss example weight identity loss time smaller weight reconstruction loss please set use dropout generator maximum number per directory subset loaded scaling load time flip data argumentation frequency showing training screen frequency showing training console frequency saving latest frequency saving end continue training load latest model phase phase train test epoch load set latest use latest model niter niter iter starting learning rate iter linearly decay learning rate zero beta beta momentum term initial learning rate use least square gan false use vanilla gan weight cycle loss weight cycle loss size image buffer previously save intermediate training,issue,positive,positive,neutral,neutral,positive,positive
310178135,Could you give more details? How much memory does your GPU/CPU have?,could give much memory,issue,negative,positive,positive,positive,positive,positive
309866415,"@lyhangustc  On my GTX 1080 GPU, it takes 2.8 GB to train a horse2zebra model on 256x256 images. I think K80 or Titan X should have enough memory for 256x256 models. I wonder if is is related to your GPU settings. (e.g. [ECC](http://tutorial.caffe.berkeleyvision.org/performance_hardware.html) on/off) ",train model think enough memory wonder related,issue,negative,neutral,neutral,neutral,neutral,neutral
309793269,"I solved it by running with smaller loadSize and fineSize：
`python train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan --pool_size 50 --loadSize 128 --fineSize 128 --batchSize 1`
And the memory used is 4441MB on a Tesla K80 GPU.

I also ran on four Tesla K80 GPUs：
`python train.py --dataroot ./datasets/horse2zebra --name horse2zebra_Q_cyclegan --model cycle_gan --pool_size 50 --loadSize 128 --fineSize 128 --batchSize 16 --gpu_ids=0,1,2,3`
And the memory used is about 4972MB on each GPU.",running smaller python name model memory used also ran four python name model memory used,issue,negative,neutral,neutral,neutral,neutral,neutral
309594568,"You need to upgrade your torch vision library for the current codebase: 
``` bash
git clone https://github.com/pytorch/vision
cd vision
python setup.py install
```
Scale(opt.loadSize) might not work if the input image is not square. 
From pytorch [doc](http://pytorch.org/docs/torchvision/transforms.html#torchvision.transforms.Scale): 
Rescales the input PIL.Image to the given ‘size’. If ‘size’ is a 2-element tuple or list in the order of (width, height), it will be the exactly size to scale. If ‘size’ is a number, it will indicate the size of the smaller edge. For example, if height > width, then image will be rescaled to (size * height / width, size) size: size of the exactly size or the smaller edge interpolation: Default: PIL.Image.BILINEAR",need upgrade torch vision library current bash git clone vision python install scale might work input image square doc input given size size list order width height exactly size scale size number indicate size smaller edge example height width image size height width size size size exactly size smaller edge interpolation default,issue,positive,positive,neutral,neutral,positive,positive
309555314,Nope. transform_list is a list that stores all the transformation. Here we appended more transformation operations to the list.  ,nope list transformation transformation list,issue,negative,neutral,neutral,neutral,neutral,neutral
309231744,"Thanks @kabrio. It should work.  
To use our latest dataset loader, you need to install the latest PyTorch vision library. 
 ```bash
git clone https://github.com/pytorch/vision
cd vision
python setup.py install
```",thanks work use latest loader need install latest vision library bash git clone vision python install,issue,positive,positive,positive,positive,positive,positive
309221639,"maxbe you are not using the right version of torchvision?

try `pip install https://github.com/pytorch/vision/archive/master.zip --upgrade`",right version try pip install upgrade,issue,negative,positive,positive,positive,positive,positive
308051134,Haven't you saved the model? so that we can use it without training?,saved model use without training,issue,negative,neutral,neutral,neutral,neutral,neutral
308029671,"Thanks, @hiepph for sharing the helpful script. Sorry for the delay. I just uploaded a new [script](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/test_single.sh) for applying a pre-trained model to a collection of input images. Please use `--dataset_mode single` and `--model test` options. ",thanks helpful script sorry delay new script model collection input please use single model test,issue,positive,negative,neutral,neutral,negative,negative
308013000,How much time does it take on your computer?,much time take computer,issue,negative,positive,positive,positive,positive,positive
307988331,"Hi, with the latest update, now the algorithm will save the training losses to a text file called ""loss_log.txt"" under opt.checkpoints_dir/opt.name directory.",hi latest update algorithm save training text file directory,issue,negative,positive,positive,positive,positive,positive
307988171,"I am not sure if PyTorch supports AMD GPUs. You can train your model on the Intel CPUs, But it will be quite slow. I haven't trained any model on an everyday use laptop by myself.  ",sure train model quite slow trained model everyday use,issue,negative,negative,neutral,neutral,negative,negative
307298806,"I am aware of the problem. I often just use `CUDA_VISIBLE_DEVICES=1,2` to avoid it. As you suggested, I tried to add `torch.cuda.device(self.opt.gpu_ids[0])` to the `base_options.py`. But I still got the same errors. Have you resolved it?",aware problem often use avoid tried add still got resolved,issue,negative,positive,positive,positive,positive,positive
307297606,The losses are not so interpretable as G and D are optimizing a minimax game.  The plots you posted here looks quite typical to me (except the spike). I will mainly focus on the quality of images. ,interpretable game posted quite typical except spike mainly focus quality,issue,negative,negative,negative,negative,negative,negative
307295436,"Thanks for the PR. However, I will let the users set the `loadSize` and `fineSize` by themselves, rather than hard-coding it.",thanks however let set rather,issue,negative,positive,positive,positive,positive,positive
307294495,`--continue_train` should work. It will initialize the network with the latest_epoch_netG.pth by default. See #33 for more discussion ,work initialize network default see discussion,issue,negative,neutral,neutral,neutral,neutral,neutral
307294355,"Currently, the program does not save the loss to the disk so that there is no way to retrieve the loss plot. If you would like to keep the loss or other logging information, you can redirect the program output to a file. See [here](https://stackoverflow.com/questions/418896/how-to-redirect-output-to-a-file-and-stdout) for more details. ",currently program save loss disk way retrieve loss plot would like keep loss logging information redirect program output file see,issue,negative,neutral,neutral,neutral,neutral,neutral
307080427,"Yes, the default output_nc = 3, you need to set `--output_nc 1` for 1 channel images. I think you probably also have to modify set_input for grayscale images.",yes default need set channel think probably also modify,issue,negative,neutral,neutral,neutral,neutral,neutral
307079665,"Yeah,  epoch/iter count is reset to 0. Otherwise, it should be fine. I used to save the old model somewhere else. ",yeah count reset otherwise fine used save old model somewhere else,issue,positive,positive,positive,positive,positive,positive
307078984,The `visdom` display functionality is turned on by default. To avoid the extra overhead of communicating with `visdom` set `--display_id 0`. ,display functionality turned default avoid extra overhead communicating set,issue,negative,neutral,neutral,neutral,neutral,neutral
307078620,could you post it here? Other people might have the same problem. Which language/platform do you use?,could post people might problem use,issue,negative,neutral,neutral,neutral,neutral,neutral
305575964,"In fact, a ""PatchGAN"" is just a convnet! Or you could say all convnets are patchnets: the power of convnets is that they process each image patch identically and independently, which makes things very cheap (# params, time, memory), and, amazingly, turns out to work.

The difference between a PatchGAN and regular GAN discriminator is that rather the regular GAN maps from a 256x256 image to a single scalar output, which signifies ""real"" or ""fake"", whereas the PatchGAN maps from 256x256 to an NxN array of outputs X, where each X_ij signifies whether the patch ij in the image is real or fake. Which is patch ij in the input? Well, output X_ij is just a neuron in a convnet, and we can trace back its receptive field to see which input pixels it is sensitive to. In the CycleGAN architecture, the receptive fields of the discriminator turn out to be 70x70 patches in the input image! 

This is all mathematically equivalent to if we had manually chopped up the image into 70x70 overlapping patches, run a regular discriminator over each patch, and averaged the results.

Maybe it would have been better if we called it a ""Fully Convolutional GAN"" like in FCNs... it's the same idea :)",fact could say power process image patch identically independently cheap time memory amazingly turn work difference regular gan discriminator rather regular gan image single scalar output real fake whereas array whether patch image real fake patch input well output neuron trace back receptive field see input sensitive architecture receptive discriminator turn input image mathematically equivalent manually chopped image run regular discriminator patch maybe would better fully convolutional gan like idea,issue,negative,positive,neutral,neutral,positive,positive
305229414,"I was tired of waiting so I decided writing my own. It takes `real_A.jpg` as input and generates `fake_B.png`. You can parse images location as arguments as you need:

```
    # Load model
    model = create_model(opt)

    # Load image
    real = Image.open('./real_A.jpg')
    preprocess = transforms.Compose([
        transforms.Scale(opt.loadSize),
        transforms.RandomCrop(opt.fineSize),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5),
                             (0.5, 0.5, 0.5)),
    ])  

    # Load input
    input_A = preprocess(real).unsqueeze_(0)
    model.input_A.resize_(input_A.size()).copy_(input_A)
    # Forward (model.real_A) through G and produce output (model.fake_B)
    model.test()

    # Convert image to numpy array
    fake = util.tensor2im(model.fake_B.data)
    # Save image
    util.save_image(fake, './fake_B.png')
```

You can check full example here: https://gist.github.com/hiepph/125ebe0795ca9f5bfac3328b4d604928

And you run it with, for example:

```
python gen.py --dataroot datasets --name edges2shoes_pix2pix
```

`--dataroot` is a must option of base options so I just add a random value here. It matters at `--name` ---> the name of the trained model.",tired waiting decided writing input parse location need load model model opt load image real load input real forward produce output convert image array fake save image fake check full example run example python name must option base add random value name name trained model,issue,negative,negative,negative,negative,negative,negative
304953963,"In base_options, isn't the default for output_nc 3? 

I had a similar error but it was fixed with --output_nc 1. 

I'm working with color/grayscale images and the grayscale images were converted to 1 channel in set_input and converted back in get_current_visuals for display purposes.",default similar error fixed working converted channel converted back display,issue,negative,positive,neutral,neutral,positive,positive
304925232,"I have the same error!
Please tell me how to solve it.",error please tell solve,issue,negative,neutral,neutral,neutral,neutral,neutral
303923256,"I""m not sure how representative this is, but here's my final loss for the discriminators and generators.
There's visible oscillation from about 20 epochs to 100 epochs for Generator B as well as D_B.

Once the learning rate started to get lowered at epoch 100, G_B loss slowly increased and G_A seemed to converge in the .35 to .40 range. Both Discriminators stopped oscillating and gradually got lower.

Perhaps this will be useful to someone doing the same. I used instance norm. Perhaps I should have used batch norm since I was running batch_size = 8 

![2017-05-24 monet2photo g d](https://cloud.githubusercontent.com/assets/112011/26436578/57ea9988-40cc-11e7-977e-ead3357cf456.png)
",sure representative final loss visible oscillation generator well learning rate get epoch loss slowly converge range stopped oscillating gradually got lower perhaps useful someone used instance norm perhaps used batch norm since running,issue,negative,positive,positive,positive,positive,positive
303596233,"Hi XavierLinNow, 

For GANs, monitoring the loss is not very informative, and I recommend looking at the generated images. If the resulting images are not satisfactory, try --use_dropout option, or --which_model_netD=n_layers with --n_layers_D=4 ",hi loss informative recommend looking resulting satisfactory try option,issue,negative,neutral,neutral,neutral,neutral,neutral
303594113,"Hi revilokeb, 

I believe it is due to repeatedly applying normalization with images of low variance. 

When applying normalization like [InstanceNorm](http://pytorch.org/docs/nn.html#torch.nn.InstanceNorm2d), the gradients tend to blow up fast if the image has low variance. This becomes even worse because we are going through multiple normalizations in the deep network. This problem is more frequent in the CycleGAN architecture because we use InstanceNorm and deep network with many normalization layers. 

I personally ran into this issue when one image in the dataset was uniformly black due to corrupt image. I think this problem can be alleviated by increasing the value of epsilon, or removing the few images that cause the problem. 

",hi believe due repeatedly normalization low variance normalization like tend blow fast image low variance becomes even worse going multiple deep network problem frequent architecture use deep network many normalization personally ran issue one image uniformly black due corrupt image think problem increasing value epsilon removing cause problem,issue,negative,negative,neutral,neutral,negative,negative
303177916,"Hi @junyanz, It's already May 23. Did you make any progress for this issue? Maybe a script, if I input an A image, it will produce a B generated image with trained model?",hi already may make progress issue maybe script input image produce image trained model,issue,negative,neutral,neutral,neutral,neutral,neutral
302655828,"Oh, I realized that I used default batchSize which is 1. Now I increased it up to 100 and both GPUs are working very well.",oh used default working well,issue,negative,neutral,neutral,neutral,neutral,neutral
302354288,"Work,
some options like epoch/iter count, image size or loss values can be reconfigured, that is why epoch start from 0. It mean ""I won't more 100 epoch with lambda_A = 100"" for example

And some options like model type or model settings can`t change, if you create pix2pix model, continue to train pix2pix model",work like count image size loss epoch start mean wo epoch example like model type model change create model continue train model,issue,positive,negative,negative,negative,negative,negative
302137276,When using batch size = 8 did you use batch norm or instance norm?,batch size use batch norm instance norm,issue,negative,neutral,neutral,neutral,neutral,neutral
302034837,@filmo I have tried using batch size = 8. But the result is worse than batch size = 1 just based on my feeling. So I doubt if it has not converged. But there is no a good way to monitor it.,tried batch size result worse batch size based feeling doubt good way monitor,issue,negative,positive,positive,positive,positive,positive
302026228,"Thanks. monet2pic is 1831 seconds per epoch 6287 images.  2 x GTX-1070s. Batch size = 8 with identity set to 0.50. Iteration speed is ~320 ms per image

Have you tried using a larger batch size?

Irrespective of the loss, did the end results look like they should have?",thanks per epoch batch size identity set iteration speed per image tried batch size irrespective loss end look like,issue,negative,positive,positive,positive,positive,positive
301994107,"@filmo, training horse2zebra which contains 2400 pictures totally took about 1300 sec per epoch using TitanX. And the batch size is 1.",training totally took sec per epoch batch size,issue,negative,negative,neutral,neutral,negative,negative
301990716,"Out of curiosity, how long did it take you do do 200 epochs? I'm currently training monet2photo and it's taking about 30 minutes per epoch using two GTX-1070s.",curiosity long take currently training taking per epoch two,issue,negative,negative,neutral,neutral,negative,negative
301396514,We haven't compared the quality of results with different batch sizes. It would be great if someone can look at it. We use batchSize=1 mainly because we would like to train a model on images with higher resolution. ,quality different batch size would great someone look use mainly would like train model higher resolution,issue,positive,positive,positive,positive,positive,positive
301228033,"We will provide a test code for single image after NIPS deadline (May 19). For now, you can create a placeholder image B (e.g. white image). ",provide test code single image deadline may create image white image,issue,negative,negative,neutral,neutral,negative,negative
301227510,"i also don't know how to do, did you make any progress? thks @jinfagang 
and if there is any advises you can provide? @junyanz @phillipi ",also know make progress provide,issue,negative,neutral,neutral,neutral,neutral,neutral
300411187,"This problem is related to visdom. You can start the visdom visualizatoin server by running `python -m visdom.server`. You can disable the visdom visualization by adding `--display_id 0`
",problem related start server running python disable visualization,issue,negative,neutral,neutral,neutral,neutral,neutral
300407980,"Hello, I have the same problem, may I ask if you have solved?",hello problem may ask,issue,negative,neutral,neutral,neutral,neutral,neutral
298820859,"I am not sure why. I used 1300 MB GPU memory for 128/128, and 2218 MB GPU memory for 286/256. ",sure used memory memory,issue,negative,positive,positive,positive,positive,positive
298812113,"@junyanz Thanks for your help that the second problem was solved. 
I'm using pytorch that depend on torch.\__version\__=0.1.11+2b56711, the pytorch was installed a week ago so that it should be new. Now, I found when I killed all python process with command `killall python`, all the   scripts aforementioned run successfully with loadSize=128 and fineSize=128, and the command `nvidia-smi` shows that 5033MiB/8110MiB for Memory-Usage.

But out of memory still raised with the default loadSize=286 and fineSize=256, scripts as follow:
```bash
python train.py --dataroot ./../datasets/facades --name facades_cyclegan --model cycle_gan --pool_size 50 --loadSize 286 --fineSize 256
```",thanks help second problem depend week ago new found python process command python run successfully command memory still raised default follow bash python name model,issue,positive,positive,positive,positive,positive,positive
298770113,"1. GPU memory: Interesting. I ran the command on my machine, and only used 1294MB on my GTX 1080. Which pytorch version are you using now?
```
python train.py --dataroot ./../datasets/facades --name facades_cyclegan --model cycle_gan --pool_size 50 --loadSize 128 --fineSize 128
```
2. Error messages: Error message is related to visdom rather than GPU memory. You can start the visdom visualizatoin server by running `python -m visdom.server`. You can disable the visdom visualization by adding `--display_id 0`
",memory interesting ran command machine used version python name model error error message related rather memory start server running python disable visualization,issue,negative,positive,positive,positive,positive,positive
298527179,"When I try to use your Torch version [CycleGAN](https://github.com/junyanz/CycleGAN), the program runs well during training

My own dataset (4821 training image pairs, 512×512):
`DATA_ROOT=./../datasets/modcn name=modcn_model th train.lua loadSize=512 fineSize=512`

I don't know why PyTorch and Torch have such great difference for CycleGAN.......",try use torch version program well training training image th know torch great difference,issue,positive,positive,positive,positive,positive,positive
298519395,"@junyanz I have tried the following training command with your provided dataset:

facades dataset (400 training image pairs, 256×256):
`python train.py --dataroot ./../datasets/facades --name facades_cyclegan --model cycle_gan --pool_size 50 --loadSize 128 --fineSize 128`
Out of memory, when the loadSize and fineSize were further reduced to 64, out of memory disappeared 

maps dataset (1096 training image pairs, 600×600):
`python train.py --dataroot ./../datasets/maps --name maps_cyclegan --model cycle_gan --pool_size 50 --loadSize 128 --fineSize 128`
Out of memory, when the loadSize and fineSize were further reduced to 64, out of memory disappeared 

My own dataset (4821 training image pairs, 512×512):
`python train.py --dataroot ./../datasets/modcn --name modcn_cyclegan --model cycle_gan --pool_size 50 --loadSize 128 --fineSize 128`
Out of memory, when the loadSize and fineSize were further reduced to 64, out of memory disappeared 


Besides, **new errors appeared when loadSize and fineSize are set to 64**. The error messages are repeated recurring during training without terminate the program...

`Exception in user code:    
`------------------------------------------------------------
`Traceback (most recent call last):
  File ""/home/bwt/anaconda2/lib/python2.7/site-packages/visdom/__init__.py"", line 228, in _send
    data=json.dumps(msg),
  File ""/home/bwt/anaconda2/lib/python2.7/site-packages/requests/api.py"", line 110, in post
    return request('post', url, data=data, json=json, **kwargs)
  File ""/home/bwt/anaconda2/lib/python2.7/site-packages/requests/api.py"", line 56, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/home/bwt/anaconda2/lib/python2.7/site-packages/requests/sessions.py"", line 488, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/bwt/anaconda2/lib/python2.7/site-packages/requests/sessions.py"", line 609, in send
    r = adapter.send(request, **kwargs)
  File ""/home/bwt/anaconda2/lib/python2.7/site-packages/requests/adapters.py"", line 487, in send
    raise ConnectionError(e, request=request)
ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /events (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f62f5560c50>: Failed to establish a new connection: [Errno 111] Connection refused',))
` 


",tried following training command provided training image python name model memory reduced memory training image python name model memory reduced memory training image python name model memory reduced memory besides new set error repeated recurring training without terminate program exception user code recent call last file line file line post return request file line request return file line request resp prep file line send request file line send raise object establish new connection connection,issue,negative,positive,neutral,neutral,positive,positive
298490374,Could you share with us your training command? Could you train a pix2pix/cyclegan model on our provided dataset?,could share u training command could train model provided,issue,negative,neutral,neutral,neutral,neutral,neutral
297771189,"hi, thank u for your reply and your new code,
i retrain with the your new code  using the following cmd
training cmd:

`python train.py --dataroot ./datasets/faceid/folderAB --name facaid_pix2pix --model pix2pix --which_model_netG unet_256 --which_direction BtoA --lambda_A 100 --align_data --use_dropout --no_lsgan --niter 3000 --continue_train `

and the test cmd I use is this:
` python test.py --dataroot datasets/9  --name  facaid_pix2pix  --model one_direction_test  --which_direction BtoA --align_data --use_dropout`
is this cmd right？

And errors occured :

- model.test()
- File ""/pytorch/GAN_NEW/pytorch-CycleGAN-and-pix2pix/models/one_direction_test_model.py"", line 41, in test
- self.fake_B = self.netG_A.forward(self.real_A)
- File ""/home//GAN_NEW/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 225, in forward
- return nn.parallel.data_parallel(self.model, input, self.gpu_ids)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/parallel/data_parallel.py"", line 100, in data_parallel
- return module(*inputs[0], **module_kwargs[0])
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/home/public/pytorch/GAN_NEW/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 276, in forward
- return self.model(x)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/container.py"", line 64, in forward
- input = module(input)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/home/public/itorch/zsl_exercise/pytorch/GAN_NEW/pytorch-CycleGAN-and-
- input = module(input)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/home/public/itorch/zsl_exercise/pytorch/GAN_NEW/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 278, in forward
- return torch.cat([self.model(x), x], 1)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/container.py"", line 64, in forward
- input = module(input)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/home/public/itorch/zsl_exercise/pytorch/GAN_NEW/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 278, in forward
- return torch.cat([self.model(x), x], 1)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/container.py"", line 64, in forward
- input = module(input)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/home/public/itorch/zsl_exercise/pytorch/GAN_NEW/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 278, in forward
- return torch.cat([self.model(x), x], 1)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/container.py"", line 64, in forward
- input = module(input)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/home/public/itorch/zsl_exercise/pytorch/GAN_NEW/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 278, in forward
- return torch.cat([self.model(x), x], 1)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/container.py"", line 64, in forward
- input = module(input)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/home/public/itorch/zsl_exercise/pytorch/GAN_NEW/pytorch-CycleGAN-and-pix2pix/models/networks.py"", line 278, in forward
- return torch.cat([self.model(x), x], 1)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/container.py"", line 64, in forward
- input = module(input)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 206, in __call__
- result = self.forward(*input, **kwargs)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/conv.py"", line 237, in forward
- self.padding, self.dilation, self.groups)
- File ""/usr/local/lib/python2.7/dist-packages/torch/nn/functional.py"", line 39, in conv2d
- return f(input, weight, bias)
- RuntimeError: CUDNN_STATUS_BAD_PARAM



the  A and B image size is 128*128
in AB folder images are  256*128
training is ok。
but  A Image  in  my testset  is 128*128

  my test folder is datasets/9  A image is dataset/9/test is this right？
thank u so much  


",hi thank reply new code retrain new code following training python name model niter test use python name model file line test file line forward return input file line return module file line result input file line forward return file line result input file line forward input module input file line result input file input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward return file line result input file line forward input module input file line result input file line forward file line return input weight bias image size folder training image test folder image thank much,issue,positive,positive,positive,positive,positive,positive
297649494,"I just committed the new code for doing that. When generating images, please use

`--model=one_direction_test --max_dataset_size=20`

This will load [...]_net_G.pth and generate 20 samples without loading the model of the other direction. ",new code generating please use load generate without loading model direction,issue,negative,positive,positive,positive,positive,positive
297234923,"This multi-gpu support would be a great feature.   
With the current implementation, you can also crop patches from high-res images (e.g. crop 256x256 patches from 512x512 images. Set  loadSize=512, and fineSize=256 for both training and test). This will save you gpu memory during training, while still allows you to test your model on high-res images. ",support would great feature current implementation also crop crop set training test save memory training still test model,issue,positive,positive,positive,positive,positive,positive
296437412,"Thanks for the quick fix. Confirmed to be working now for me.

![atobfixed](https://cloud.githubusercontent.com/assets/945979/25313203/5f855dc4-287e-11e7-8f8d-2a0080d26b4a.png)
",thanks quick fix confirmed working,issue,negative,positive,positive,positive,positive,positive
296436837,"Thanks for pointing out the issue. I just fixed the bug. It should support both directions now.

",thanks pointing issue fixed bug support,issue,positive,positive,positive,positive,positive,positive
296427069,It works for me with CPU mode. This [issue](https://github.com/pytorch/pytorch/issues/1154) might be related. ,work mode issue might related,issue,negative,neutral,neutral,neutral,neutral,neutral
296426952,We fixed a few small issues. Now the cyclegan results on maps and pix2pix results on facades should be comparable to those generated by Torch implementation. We are testing more datasets now. ,fixed small comparable torch implementation testing,issue,negative,negative,neutral,neutral,negative,negative
296417861,"Thanks @hubertlee915! I just add the comment ""pip install future --upgrade"" before the ""from builtins import object"".",thanks add comment pip install future upgrade import object,issue,negative,positive,neutral,neutral,positive,positive
296405714,"I still get this error
`RuntimeError: cuda runtime error (35) : CUDA driver version is insufficient for CUDA runtime version at torch/csrc/autograd/engine.cpp:353
`",still get error error driver version insufficient version,issue,negative,neutral,neutral,neutral,neutral,neutral
296254755,"I can close the server killing it with the pid (not sure if the cleanest way) when running in bg.
Thanks!",close server killing sure way running thanks,issue,negative,positive,positive,positive,positive,positive
296246420,"Yes that's what I do right now open another screen, if  run it in the background is there a way to close the visdom server? ",yes right open another screen run background way close server,issue,negative,positive,positive,positive,positive,positive
296244201,Open another screen? or run the server in background?,open another screen run server background,issue,negative,neutral,neutral,neutral,neutral,neutral
296242112,"Thanks @ruotianluo , I hadn't done that. Currently I am opening an additional connection and opening the visdom server. Is there a way I can do that using only one connection?
",thanks done currently opening additional connection opening server way one connection,issue,negative,positive,neutral,neutral,positive,positive
296168684,"Please try this:
`pip install future --upgrade`",please try pip install future upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
296108448,"Thank you so much @griegler! This is definitely a bug that should hurt the performance. 

We will test if this bug fixes the quality and close the issue. ",thank much definitely bug hurt performance test bug quality close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
296087389,"Hi,
You call `NLayerDiscriminator` with `use_sigmoid` in networks.py, but it is never used in the constructor of the class. Hence, you always add a sigmoid output unit. Might this be the problem?
Best, 
Gernot",hi call never used constructor class hence always add sigmoid output unit might problem best,issue,negative,positive,positive,positive,positive,positive
295665132,"I just merged your code. This is very nice, and also thank you for implementing the plotting code! 

For plotting, on my environment (Python 3.6) there was a small bug where I had to explicitly convert `opt['legend']` to `list`. I suppose this is a python version issue. ",code nice also thank plotting code plotting environment python small bug explicitly convert opt list suppose python version issue,issue,positive,positive,positive,positive,positive,positive
295630197,"Cool! I guess this is CycleGAN, not pix2pix? 2000 images is often enough (e.g., horses2zebras is ~1000 images), and more data does not always help. The distribution of data definitely matters though. Ideally you want the two domains to be as similar as possible -- same kinds of planes, same number of each kind, etc.

I'd definitely let it run a bit longer just to see if it keeps getting better. But this level of blur is pretty common in our results.",cool guess often enough data always help distribution data definitely though ideally want two similar possible number kind definitely let run bit longer see getting better level blur pretty common,issue,positive,positive,positive,positive,positive,positive
295549134,"Hi Ruotian, 

Thanks for reporting it. We are looking at it as well. I tested the model on maps before, and results are just slightly worse than those generated by Torch version. The default setting also doesn't use the reply image buffer, which might make a difference. This is a work in progress, and we are working on improving PyTorch results.   Let us know if you find any issues in the code.

Best,
Jun-Yan  ",hi thanks looking well tested model slightly worse torch version default setting also use reply image buffer might make difference work progress working improving let u know find code best,issue,positive,positive,positive,positive,positive,positive
295043391,"Ruotianluo, 

We were not aware of visdom! Thank you for letting us know. We will check it out. ",aware thank u know check,issue,negative,positive,positive,positive,positive,positive
295029167,I added `--gpu_ids -1` and this issue was resolved. Thanks.,added issue resolved thanks,issue,positive,positive,positive,positive,positive,positive
