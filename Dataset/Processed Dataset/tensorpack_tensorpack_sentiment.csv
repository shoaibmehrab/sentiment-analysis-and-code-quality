id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
1596280952,"hello @ wangg12
I have this problem now
can you help me please",hello problem help please,issue,negative,neutral,neutral,neutral,neutral,neutral
1243998160,"Thanks! Now it works if I change CNAME to `models.tensorpack.com.s3-website-us-east-1.amazonaws.com.` Is that the expected solution?
If I change it to `models.tensorpack.com.s3.amazonaws.com.` or `s3.amazonaws.com.`, then I still cannot access without `index.html`. ",thanks work change solution change still access without,issue,positive,positive,positive,positive,positive,positive
1243629203,"Added static website hosting with default index.html document

http://models.tensorpack.com.s3-website-us-east-1.amazonaws.com/

works now with `index.html`. Can you try something [along these lines](https://gist.github.com/ogt/5294121)?",added static hosting default document work try something along,issue,negative,positive,positive,positive,positive,positive
1243488605,"I configured the cname record. Now `http://models.tensorpack.com/index.html` works.
However, `http://models.tensorpack.com/` does not work. Is there an AWS configuration for it?",record work however work configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
1242958368,"I am hosting these models now (still uploading the files) under

http://models.tensorpack.com.s3.amazonaws.com/index.html

You probably need to change the CNAME entry.",hosting still probably need change entry,issue,negative,neutral,neutral,neutral,neutral,neutral
1242804851,"I would like to transfer those models to another place as I gonna shutdown the server (which randomly restarts).
If there is no alternative I would upload them to S3.",would like transfer another place gon na shutdown server randomly alternative would,issue,negative,negative,negative,negative,negative,negative
1242750768,@PatWie would you mind taking a look?,would mind taking look,issue,negative,neutral,neutral,neutral,neutral,neutral
1226581272,"Using the `ProcessTensors` callback https://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.ProcessTensors  with the right tensor name, user can specify an arbitrary function that will be called with the value of the tensor.",right tensor name user specify arbitrary function value tensor,issue,negative,positive,neutral,neutral,positive,positive
1226577379,"the `pkl` files were created by `torch.load` the pth file and dumping the dict of weights with `pkl.dump`. You can open the files in both format to see their format, and write a simple conversion script with no more than 10 lines of code.",file dumping open format see format write simple conversion script code,issue,negative,neutral,neutral,neutral,neutral,neutral
1226573567,"> Also shouldn't the remap variables be used to quantize weights

Yes it should, and the current DoReFaNet example does that.",also remap used quantize yes current example,issue,negative,neutral,neutral,neutral,neutral,neutral
1226568076,The code uses NCHW format and requires GPU. The failure is expected. Therefore closing.,code format failure therefore,issue,negative,negative,negative,negative,negative,negative
1190669171,"To add to this discussion, I was able to convert the pretrained mask rcnn R50 FPN 3x to a .pd file using the code in this repository. However, is there a way to also convert custom weights after I used transfer learning with the backbone on a custom dataset?",add discussion able convert mask file code repository however way also convert custom used transfer learning backbone custom,issue,negative,positive,positive,positive,positive,positive
1060421054,"> 

Hi, solarflarefx did you try it using the pretrained file from the modelzoo: https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl. This file.",hi try file file,issue,negative,neutral,neutral,neutral,neutral,neutral
1034315263,"OK, I won't notify you about tensorflow again, unless you re-open this PR or update it yourself. 😢",wo notify unless update,issue,negative,neutral,neutral,neutral,neutral,neutral
1024961753,"Okey, I think I solve it myself. The problem was in 
```
def __len__(self):
        return self.num_caps
```
Which I thought was not that important so I didnt pay enough attention when working on self.num_caps and all of my annotations where list of lists so the len was 1. Therfore it always processed only one input. Now it is running and I can see the progress x out of 113287 inputs which is more likely when working with MS-COCO.",think solve problem self return thought important didnt pay enough attention working list always one input running see progress likely working,issue,negative,positive,positive,positive,positive,positive
932834683,"I am sorry, the question seems to be unrelated to tensorpack specifically but instead to tensorflow. I will close this ticket. ",sorry question unrelated specifically instead close ticket,issue,negative,negative,negative,negative,negative,negative
926874584,it seems that the issue was because I was not using the batch normalization layer on the fc0 layer,issue batch normalization layer layer,issue,negative,neutral,neutral,neutral,neutral,neutral
923297325,There is no specific reason. In fact the `nonlin` is pretty much just a relu,specific reason fact pretty much,issue,negative,positive,positive,positive,positive,positive
901641058,It will not affect performance because the mapper that's referred to at this line is not a compute-bound function.,affect performance mapper line function,issue,negative,neutral,neutral,neutral,neutral,neutral
896152144,"Hi thanks that looks good. Could you make sure `flake8 .` checks can pass?

(there is also an unrelated unittest failure, please disregard)",hi thanks good could make sure flake pas also unrelated failure please disregard,issue,positive,positive,positive,positive,positive,positive
895569871,Please `from tensorpack.compat import tfv1` and then use `tfv1.xxx` so that the same code runs in both TFv1 and TFv2.,please import use code,issue,negative,neutral,neutral,neutral,neutral,neutral
888079168,"This only happens on non-linux system and is expected, because only linux has anonymous pipe.

Automatic clean up can be implemented as an atexit handler when the pipe is created in the dataflow library.",system anonymous pipe automatic clean handler pipe library,issue,negative,positive,positive,positive,positive,positive
883620209,@ppwwyyxx  Thank you very much for the reply. I added the dimension and the training went normally. ,thank much reply added dimension training went normally,issue,negative,positive,positive,positive,positive,positive
883610790,"According to [documentation](https://tensorpack.readthedocs.io/en/latest/modules/dataflow.dataset.html#tensorpack.dataflow.dataset.Mnist), the mnist data loader already removes the extra dimension. It's done at here:

https://github.com/tensorpack/tensorpack/blob/ff7de87f2013783641352a3786d931f9bd300ad4/tensorpack/dataflow/dataset/mnist.py#L108-L111",according documentation data loader already extra dimension done,issue,negative,neutral,neutral,neutral,neutral,neutral
878600599,"@ppwwyyxx , thank you for the reply. I will try inference with the steps suggested. I close this since the steps I followed were not recommend in the guide.",thank reply try inference close since recommend guide,issue,positive,neutral,neutral,neutral,neutral,neutral
877718297,">  running inference in the model restored from metafile 

That's not a proper way to run inference as we said in https://tensorpack.readthedocs.io/en/latest/tutorial/inference.html#step-1-build-the-model-graph, so I'm not sure what will happen.",running inference model proper way run inference said sure happen,issue,negative,positive,positive,positive,positive,positive
877718109,"> ran following after cloning
pip3 install -r requirements.txt

This project doesn't contain `requirements.txt` and our `setup.py` doesn't contain pycocotools.

>   #error architecture not supported

See https://github.com/facebookresearch/detectron2/issues/2288 for solutions.
",ran following pip install project contain contain error architecture see,issue,negative,neutral,neutral,neutral,neutral,neutral
873445525,"Tensorpack is a training library. It produces standard tensorflow model as the outcome of training, and anything after that is not its responsibility.",training library standard model outcome training anything responsibility,issue,negative,neutral,neutral,neutral,neutral,neutral
873413354,"@ppwwyyxx Has this been solved? If not, What is the right procedure to export the model into TfLite format?

Will the way: (PyTorch -> Onnx -> TensorFlow -> TfLite) is a suitable for this conversion?  
",right procedure export model format way suitable conversion,issue,negative,positive,positive,positive,positive,positive
870910774,"This made it work. Thank you, I really appreciate it.",made work thank really appreciate,issue,positive,positive,positive,positive,positive,positive
870887054,"Hi @ppwwyyxx,

I am sorry I write again. As per the suggestions in the old tickets, I loaded the checkpoint for post-processing. I can see quantized variables now. But I get the below error while saving the checkpoint. Creating the dictionary of updated variables, however, does not give any error and provided quantized variables as output.


![error_dorefa](https://user-images.githubusercontent.com/33551684/123861550-9a438400-d927-11eb-98ce-f8fabeea7f3b.png)

The code snippet I used is:



``` 
BITW = 1
BITA = 2
BITG = 4

def getBinarizedWights():

    fw, fa, fg = get_dorefa(BITW, BITA, BITG)

    dict = {}
    def get_weights():           
        if not name.endswith('W') or 'conv0' in name or 'fc' in name:
            return reader[name]
        else:
            return fw(reader[name])

    reader = tfutils.varmanip.load_checkpoint_vars(r'C:\Users\sab\Downloads\AI Testing\Source\Dorefanet\tensorpack\examples\DoReFa-Net\train_log\svhn-dorefa-1,2,4\model-4721')
    for name in reader:
        val = get_weights()
        if(not ""/Adam"" in name and not ""bn"" in name):
            #print(keys, "":"", val, ""\n"")
            dict[name] = val
    return dict

dict = getBinarizedWights()
tfutils.varmanip.save_checkpoint_vars(dict, r""C:\Users\sab\Downloads\AI Testing\Source\Dorefanet\tensorpack\examples\DoReFa-Net\train_log\svhn-dorefa-1,2,4\model-55"")
```

I could not find any fix for this issue on the internet. I tried disabling the eager execution and also tried to use the tf.compat.v1 for eager execution but it does not work.",hi sorry write per old loaded see get error saving dictionary however give error provided output code snippet used fa name name return reader name else return reader name reader name reader name name print name return could find fix issue tried eager execution also tried use eager execution work,issue,negative,negative,negative,negative,negative,negative
870464774,"Thank you very much for the prompt response. I am trying to use DoReFa for quantization for my research project and the closed issues contain a ton of information.
Thanks again for your prompt response.",thank much prompt response trying use quantization research project closed contain ton information thanks prompt response,issue,positive,positive,positive,positive,positive,positive
854600272,"There was an error elsewhere in the code. I am able to train it correctly with passing roidb['boxes] = np.zeros((0,4) and setting the DATA.FILTER_EMPTY_ANNOTATIONS=False in the config argument",error elsewhere code able train correctly passing setting argument,issue,negative,positive,positive,positive,positive,positive
853785662,"What changes are required exactly? 
In my case, I want to have a custom dataset, so I modified the balloons.py file. For the images with no targets, I returned a roidb dict with roidb[boxes']=[]. 
But this gives me an error like:
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Input to reshape is a tensor with 0 values, but the requested shape has 4
	 [[{{node tower0/encode_bbox_target/Reshape_2}}]]
	 [[tower0/fastrcnn/outputs/class/BiasAdd/_945]]
  (1) Invalid argument: Input to reshape is a tensor with 0 values, but the requested shape has 4
	 [[{{node tower0/encode_bbox_target/Reshape_2}}]]
0 successful operations.
0 derived errors ignored.

Please let me know the correct way to pass such images. Thanks in advance",exactly case want custom file returned error like root error found invalid argument input reshape tensor shape node invalid argument input reshape tensor shape node successful derived please let know correct way pas thanks advance,issue,negative,positive,positive,positive,positive,positive
834277937,Have found the version number of Tensorpack I used in pip. Thanks.,found version number used pip thanks,issue,negative,positive,positive,positive,positive,positive
834200831,this issue about cascade R-CNN should be fixed now by the above commit,issue cascade fixed commit,issue,negative,positive,neutral,neutral,positive,positive
834153480,"@ppwwyyxx Hi, After I changed a mechine, obove issue is disappered. But I meet a new error as follows:
```
Traceback (most recent call last):
  File ""train.py"", line 119, in <module>
    launch_train_with_config(traincfg, trainer)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorpack/train/interface.py"", line 89, in launch_train_with_config
    model.build_graph, model.get_optimizer)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorpack/utils/argtools.py"", line 168, in wrapper
    return func(*args, **kwargs)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorpack/train/tower.py"", line 219, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorpack/train/trainers.py"", line 211, in _setup_graph
    grad_list = self._builder.call_for_each_tower(tower_fn)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorpack/graph_builder/training.py"", line 228, in call_for_each_tower
    use_vs=[False] + [True] * (len(self.towers) - 1))
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorpack/graph_builder/training.py"", line 123, in build_on_towers
    return DataParallelBuilder.call_for_each_tower(*args, **kwargs)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorpack/graph_builder/training.py"", line 117, in call_for_each_tower
    ret.append(func())
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorpack/train/tower.py"", line 276, in get_grad_fn
    return compute_grad_from_inputs(*inputs)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorpack/train/tower.py"", line 250, in compute_grad_from_inputs
    cost = get_cost_fn(*inputs)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorpack/tfutils/tower.py"", line 294, in __call__
    output = self._tower_fn(*args)
  File ""/data/lc/tensorpack/examples/FasterRCNN/modeling/generalized_rcnn.py"", line 73, in build_graph
    head_losses = self.roi_heads(image, features, proposals, targets)
  File ""/data/lc/tensorpack/examples/FasterRCNN/modeling/generalized_rcnn.py"", line 287, in roi_heads
    (gt_boxes, gt_labels), image_shape2d, cfg.DATA.NUM_CATEGORY)
  File ""/data/lc/tensorpack/examples/FasterRCNN/modeling/model_cascade.py"", line 46, in __init__
    B1_proposal = self.match_box_with_gt(B1, ious[1])
  File ""/data/lc/tensorpack/examples/FasterRCNN/modeling/model_cascade.py"", line 92, in match_box_with_gt
    lambda: tf.zeros([tf.shape(iou)[0]], dtype=tf.int64))
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1235, in cond
    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1061, in BuildCondBranch
    original_result = fn()
  File ""/data/lc/tensorpack/examples/FasterRCNN/modeling/model_cascade.py"", line 92, in <lambda>
    lambda: tf.zeros([tf.shape(iou)[0]], dtype=tf.int64))
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py"", line 2338, in zeros
    output = _constant_if_small(zero, shape, dtype, name)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py"", line 2295, in _constant_if_small
    if np.prod(shape) < 1000:
  File ""<__array_function__ internals>"", line 6, in prod
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 3031, in prod
    keepdims=keepdims, initial=initial, where=where)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 87, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
  File ""/home/licheng/anaconda3/envs/tensorpack/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 736, in __array__
    "" array."".format(self.name))
NotImplementedError: Cannot convert a symbolic Tensor (tower0/cascade_rcnn_stage2/match_box_with_gt_0.6/cond/strided_slice:0) to a numpy array.
(tensorpack) licheng@amax:/data/lc/tensorpack/examples/FasterRCNN$ python train.py --config BACKBONE.WEIGHTS=/data/lc/tensorpack/checkpoint/ImageNet-R101-AlignPadding.npz DATA.BASEDIR=/data/lc/tensorpack/examples/FasterRCNN/dataset/coco-format MODE_MASK=False FPN.CASCADE=True BACKBONE.RESNET_NUM_BLOCKS=[3,4,23,3] TEST.RESULT_SCORE_THRESH=1e-4 PREPROC.TRAIN_SHORT_EDGE_SIZE=[640,800] TRAIN.LR_SCHEDULE=1x
```
It only happened if I add `FPN.CASCADE=True` option, means I successful train Faster RCNN, but Failed to train Cascade RCNN. How can I solve this error. Thanks for kindly reply.",hi issue meet new error recent call last file line module trainer file line file line wrapper return file line input file line file line false true file line return file line file line return file line cost file line output file line image file line file line file line lambda file line return file line cond file line file line lambda lambda file line output zero shape name file line shape file internals line prod file line prod file line return axis file line array convert symbolic tensor array python add option successful train faster train cascade solve error thanks kindly reply,issue,positive,positive,positive,positive,positive,positive
833953092,"The file can be loaded here: https://colab.research.google.com/drive/1GXPHFO7YvyPTtq4ppJmFcFznRD9Dsu65#scrollTo=_YAfAGqDFI3y

If you'd like someone to investigate it, we'll need proof that the issue exists in tensorpack or the weight file",file loaded like someone investigate need proof issue weight file,issue,negative,neutral,neutral,neutral,neutral,neutral
833594601,"@ppwwyyxx The weight file is correctly downloaded, sha245 sum is same. Error still exist as above. ",weight file correctly sha sum error still exist,issue,negative,neutral,neutral,neutral,neutral,neutral
833440154,You can check whether the weight file is correctly downloaded by the sha256 sum in the model zoo: http://models.tensorpack.com/,check whether weight file correctly sha sum model zoo,issue,negative,neutral,neutral,neutral,neutral,neutral
832424505,"I think I understand that those models you mentioned are for different goals(Object Detection/Instance Segmentation) rather than keypoints model that I want, thanks.",think understand different object segmentation rather model want thanks,issue,negative,positive,neutral,neutral,positive,positive
832415248,"Thank you for the quick reply. Actually I'm not familiar with the model architecture design. I only need to convert a keypoints detection model to tensorflow for my work. What alternative model is suggested? And what is the difference between Faster/Mask/Cascade R-CNNs? I think I only care about the model input/output format. And the reason why I choose COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml is just because it is the example model used in [Vidoepose3D](https://github.com/facebookresearch/VideoPose3D/blob/master/INFERENCE.md) (in the wild)
",thank quick reply actually familiar model architecture design need convert detection model work alternative model difference think care model format reason choose example model used wild,issue,positive,positive,positive,positive,positive,positive
828866066,"It will be similar to `MultiProcessMapData` but many details will be different. Basically the worker is responsible for iterating through the iterator and producing a stream of data and the main process receives whatever that comes up.

One big difference is that now we don't know beforehand how many datapoints the worker will send, so some mechanism to coordinate the workers will be needed.",similar many different basically worker responsible stream data main process whatever come one big difference know beforehand many worker send mechanism,issue,negative,positive,positive,positive,positive,positive
828572936,"@ppwwyyxx as a stopgap, any suggestions for how I might go about trying to implement this myself? Just make an analogous class to `_ParallelMapData` that interleaves instead of maps?",stopgap might go trying implement make analogous class instead,issue,negative,neutral,neutral,neutral,neutral,neutral
825030189,"Right, I think the utility here would be for very large files (shards of some large dataset), most likely hosted on some sort of cloud storage (S3 or GCS), such that reads should be streamed in parallel.",right think utility would large large likely sort cloud storage parallel,issue,negative,positive,positive,positive,positive,positive
825021377,"Thanks for the explanation! You're right and this is not implemented. It would be good to have it!

Unless the file iterator themselves are very large you can probably read and concat them first. ",thanks explanation right would good unless file large probably read first,issue,positive,positive,positive,positive,positive,positive
824412091,"Thanks for the prompt response, @ppwwyyxx. I think the key difference is that for `interleave` the input `map_func` is described as ""A function mapping a dataset *element* to a *dataset*"", whereas for `MapData` the `func` argument maps a datapoint to a datapoint.

So in the example I gave above you can think of the `map_func` for `interleave` as taking in a filename as input and returning an iterator as the output.",thanks prompt response think key difference interleave input function element whereas argument example gave think interleave taking input output,issue,negative,positive,neutral,neutral,positive,positive
824408115,From the documentation of interleave it sounds like it's achieved by `MapData`: https://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.MapData or the parallel versions of it: https://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.MultiThreadMapData and https://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.MultiProcessMapDataZMQ,documentation interleave like parallel,issue,negative,neutral,neutral,neutral,neutral,neutral
820236127,"> Your tensorflow is unable to access GPU.



> Closing due to no response.

why？ I also have the problem. I installed the tensorflow-gpu",unable access due response also problem,issue,negative,negative,negative,negative,negative,negative
812714544,@ppwwyyxx Ah sorry! I assumed from the commit messages that TF2 support has been added. Thanks for quick response.,ah sorry assumed commit support added thanks quick response,issue,positive,positive,neutral,neutral,positive,positive
804069722,"Hi Thanks for the time serving looks good. I was using wrong config that might cause the serving checkpoint to have less accuracy. 
Closing the issues.",hi thanks time serving good wrong might cause serving le accuracy,issue,negative,positive,positive,positive,positive,positive
804041598,I was expecting that model retain its accuracy after converting it but predictions was not accurate at all.,model retain accuracy converting accurate,issue,negative,positive,positive,positive,positive,positive
803994069,"The provided code looks reasonable.

The conversion looks successful as well according to the logs.

""What you expected"" in the issue template is not provided, so I'm not sure what is the issue being asked here. Note that if you expect a model to work better, that's not the library's issue.
",provided code reasonable conversion successful well according issue template provided sure issue note expect model work better library issue,issue,positive,positive,positive,positive,positive,positive
800536108,"For others to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected,issue,positive,positive,positive,positive,positive,positive
790659460,"Hey Admin,

I need to train for only two classes using faster rcnn, I checked the issues, but I did not find any useful info for my problem. 

Could you please help me with this! in config.py file where I need to make changes ?
Thanks in advance",hey need train two class faster checked find useful problem could please help file need make thanks advance,issue,positive,positive,positive,positive,positive,positive
789515833,"> TensorFlow does back propagation automatically.

Thanks for your reply, I will try it.",back propagation automatically thanks reply try,issue,negative,positive,neutral,neutral,positive,positive
789500862,"> 1. it's not needed and does not affect computation
> 2. I can't understand the question - `ret` is in the code and you can see what it is
> 3. I can't understand what is ""the back propagation problem"" - there is no problem as far as I can see.

I saw syncBN method in the link http://hangzhang.org/mxnet-zh-blog/syncbn, it said  in back propagation should  synchronizate the gradient of sum x and sum square x.

Thank your.
",affect computation ca understand question ret code see ca understand back propagation problem problem far see saw method link said back propagation gradient sum sum square thank,issue,negative,positive,neutral,neutral,positive,positive
789497128,"1. it's not needed and does not affect computation
2. I can't understand the question - `ret` is in the code and you can see what it is
3. I can't understand what is ""the back propagation problem"" - there is no problem as far as I can see.",affect computation ca understand question ret code see ca understand back propagation problem problem far see,issue,negative,positive,neutral,neutral,positive,positive
784850282,"> usually for each checkpoint we have a common prefix followed by .data, .index and .meta right?

No. It is expected that there is only one `.meta` file, and it does not share a common prefix with others.",usually common prefix right one file share common prefix,issue,negative,negative,negative,negative,negative,negative
784842839,"Not the graph one, usually for each checkpoint we have a common prefix followed by .data, .index and .meta right? Here only one meta is present. Is it because meta across different checkpoints will be the same?",graph one usually common prefix right one meta present meta across different,issue,negative,negative,neutral,neutral,negative,negative
784833947,Afer I run the command `./train.py --config DATA.BASEDIR=~/data/coco/` with no changes it does create a `graph-*.meta` file.,run command create file,issue,negative,neutral,neutral,neutral,neutral,neutral
784807112,"If you're asking about an unexpected problem which you do not know the root cause,
use this template. __PLEASE DO NOT DELETE THIS TEMPLATE, FILL IT__:

If you already know the root cause to your problem,
feel free to delete everything in this template.

### 1. What you did:
Train a FasterRCNN model
(1) **If you're using examples, what's the command you run:**
./train.py --config DATA.BASEDIR=~/data/augmentations/copy_paste/ FPN.CASCADE=True BACKBONE.RESNET_NUM_BLOCKS=[3,4,23,3] FPN.NORM=GN BACKBONE.NORM=GN FPN.FRCNN_HEAD_FUNC=fastrcnn_4conv1fc_gn_head FPN.MRCNN_HEAD_FUNC=maskrcnn_up4conv_gn_head PREPROC.TRAIN_SHORT_EDGE_SIZE=[640,800] TRAIN.LR_SCHEDULE=9x BACKBONE.FREEZE_AT=0 ""DATA.VAL=('satpole_val',)"" ""DATA.TRAIN=('satpole_train',)"" --logdir copy_paste_resnet101/
(2) **If you're using examples, have you made any changes to the examples? Paste `git status; git diff` here:**
No changes
(3) **If not using examples, help us reproduce your issue:**

  It's always better to copy-paste what you did than to describe them.

  Please try to provide enough information to let others __reproduce__ your issues.
  Without reproducing the issue, we may not be able to investigate it.

### 2. What you observed:

(1) **Include the ENTIRE logs here:**
```
I am not able to see the .meta file, I can see only .index and .data file being saved
```

It's always better to copy-paste what you observed instead of describing them.

It's always better to paste **as much as possible**, although sometimes a partial log is OK.

Tensorpack typically saves stdout to its training log.
If stderr is relevant, you can run a command with `my_command 2>&1 | tee logs.txt`
to save both stdout and stderr to one file.

(2) **Other observations, if any:**
For example, CPU/GPU utilization, output images, tensorboard curves, if relevant to your issue.

### 3. What you expected, if not obvious.

If you expect higher speed, please read
http://tensorpack.readthedocs.io/tutorial/performance-tuning.html
before posting.

If you expect the model to converge / work better, note that we do not help you on how to improve a model.
Only in one of the two conditions can we help with it:
(1) You're unable to reproduce the results documented in tensorpack examples.
(2) It indicates a tensorpack bug.

### 4. Your environment:

Paste the output of this command: `python -m tensorpack.tfutils.collect_env`
If this command failed, also tell us your version of Python/TF/tensorpack.

Note that:

  + You can install tensorpack master by `pip install -U git+https://github.com/tensorpack/tensorpack.git`
    and see if your issue is already solved.
  + If you're not using tensorpack under a normal command line shell (e.g.,
    using an IDE or jupyter notebook), please retry under a normal command line shell.

You may often want to provide extra information related to your issue, but
at the minimum please try to provide the above information __accurately__ to save effort in the investigation.
",unexpected problem know root cause use template delete template fill already know root cause problem feel free delete everything template train model command run made paste git status git help u reproduce issue always better describe please try provide enough information let without issue may able investigate include entire able see file see file saved always better instead always better paste much possible although sometimes partial log typically training log relevant run command tee save one file example utilization output relevant issue obvious expect higher speed please read posting expect model converge work better note help improve model one two help unable reproduce bug environment paste output command python command also tell u version note install master pip install see issue already normal command line shell ide notebook please retry normal command line shell may often want provide extra information related issue minimum please try provide information save effort investigation,issue,positive,positive,positive,positive,positive,positive
780931719,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
780930784,"The builtin trainers only work for single-dataloader single-cost optimization: https://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html#built-in-trainers 

Therefore to do this you need to either formulate your problem this way (e.g. by writing the loops in the model with `tf.while`) or implement a custom trainer (https://tensorpack.readthedocs.io/en/latest/tutorial/extend/trainer.html)",work optimization therefore need either formulate problem way writing model implement custom trainer,issue,negative,neutral,neutral,neutral,neutral,neutral
780925900,I can't understand the issue and it doesn't seem belong to any of the [issue categories](https://github.com/tensorpack/tensorpack/issues/new/choose).,ca understand issue seem belong issue,issue,negative,neutral,neutral,neutral,neutral,neutral
780797755,"Hey @ppwwyyxx, Where do implement the graph for batch inference, and is there any examples of a custom implementation of graph for other purposes",hey implement graph batch inference custom implementation graph,issue,negative,neutral,neutral,neutral,neutral,neutral
774646078,"Hi, what do you mean by  this?

> github master version of FasterRCNN example with an older version of tensorpack
Did you fork FasterRCNN from another repo? Could you please tell me which repo?",hi mean master version example older version fork another could please tell,issue,negative,negative,neutral,neutral,negative,negative
774645007,"I download this release, but still  

> cannot import name 'tfv1' 

![image](https://user-images.githubusercontent.com/53520949/107142925-091a4380-696d-11eb-9503-c7ed42f934cb.png)
",release still import name image,issue,negative,neutral,neutral,neutral,neutral,neutral
774407242,"There is no such plans.

Tensorpack is a training interface - implementing more complicated models is not aligned with this goal. And given that I don't think there is a reasonable way to achieve what you described within 1000 lines of code, it's probably not a PR that we'll accept.",training interface complicated goal given think reasonable way achieve within code probably accept,issue,negative,negative,negative,negative,negative,negative
770135138,"Hi, Where do we mention the input size for the model for custom datasets, like 256x256 or 512x512",hi mention input size model custom like,issue,negative,neutral,neutral,neutral,neutral,neutral
769529517,"Evaluation is not supported on this dataset because it's not in COCO format.
Please use `TRAIN.EVAL_PERIOD=0` as the example shows (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/BALLOON.md)",evaluation coco format please use example,issue,negative,neutral,neutral,neutral,neutral,neutral
765238317,"For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected,issue,positive,positive,positive,positive,positive,positive
764499145,"As the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md) suggests:

> If you expect the model to converge / work better, note that we do not help you on how to improve a model.
Only in one of the two conditions can we help with it:
(1) You're unable to reproduce the results documented in tensorpack examples.
(2) It indicates a tensorpack bug.",issue template expect model converge work better note help improve model one two help unable reproduce bug,issue,positive,neutral,neutral,neutral,neutral,neutral
747374040,"@Zraxix I am trying to understand model)frcnn.py. Here in the code, there is always an ""n"" and sometimes ""m"" in bounding boxes, labels shapes ( boxes: n#classx4 floatbox in float32,   gt_boxes: Mx4) (for example: https://github.com/tensorpack/tensorpack/blob/fb9c4fdf843890cbaf90c9ba50b3ccbb5144ccaf/examples/FasterRCNN/modeling/model_frcnn.py#L276, https://github.com/tensorpack/tensorpack/blob/fb9c4fdf843890cbaf90c9ba50b3ccbb5144ccaf/examples/FasterRCNN/modeling/model_frcnn.py#L310, https://github.com/tensorpack/tensorpack/blob/fb9c4fdf843890cbaf90c9ba50b3ccbb5144ccaf/examples/FasterRCNN/modeling/model_frcnn.py#L312).

Can you please tell me what do n and m represent?  Is it the number of classes/batch_size?",trying understand model code always sometimes bounding float example please tell represent number,issue,negative,neutral,neutral,neutral,neutral,neutral
745768317,"Thank you for the answer. Much appreciated. About the behaviour of `strict=True`, I ran the same code on a linux computer, it also hanged after a it produced a couple of numbers. Here is how the error looks like:

```
Python 3.8.5 (default, Sep  4 2020, 07:30:14)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import test6_simplified
[1216 16:09:01 @argtools.py:138] WRN Starting a process with 'fork' method is efficient but not safe and may cause deadlock or crash.Use 'forkserver' or 'spawn' method instead if you run into such issues.See https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods on how to set them.
[1216 16:09:01 @argtools.py:138] WRN ""import prctl"" failed! Install python-prctl so that processes can be cleaned with guarantee.
0
10
20
30
40
50
60
70
80
90
^CTraceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/hhyuaur/learn/tensorpack/test6_simplified.py"", line 22, in <module>
    for i in d1:
  File ""/home/hhyuaur/anaconda3/envs/nndev/lib/python3.8/site-packages/tensorpack/dataflow/parallel_map.py"", line 310, in __iter__
    yield from super(MultiProcessMapDataZMQ, self).__iter__()
  File ""/home/hhyuaur/anaconda3/envs/nndev/lib/python3.8/site-packages/tensorpack/dataflow/parallel_map.py"", line 87, in __iter__
    yield from self.get_data_strict()
  File ""/home/hhyuaur/anaconda3/envs/nndev/lib/python3.8/site-packages/tensorpack/dataflow/parallel_map.py"", line 79, in get_data_strict
    dp = self._recv_filter_none()
  File ""/home/hhyuaur/anaconda3/envs/nndev/lib/python3.8/site-packages/tensorpack/dataflow/parallel_map.py"", line 45, in _recv_filter_none
    ret = self._recv()
  File ""/home/hhyuaur/anaconda3/envs/nndev/lib/python3.8/site-packages/tensorpack/dataflow/parallel_map.py"", line 304, in _recv
    msg = self.socket.recv_multipart(copy=False)
  File ""/home/hhyuaur/anaconda3/envs/nndev/lib/python3.8/site-packages/zmq/sugar/socket.py"", line 566, in recv_multipart
    parts = [self.recv(flags, copy=copy, track=track)]
  File ""zmq/backend/cython/socket.pyx"", line 783, in zmq.backend.cython.socket.Socket.recv
  File ""zmq/backend/cython/socket.pyx"", line 821, in zmq.backend.cython.socket.Socket.recv
  File ""zmq/backend/cython/socket.pyx"", line 170, in zmq.backend.cython.socket._recv_frame
  File ""zmq/backend/cython/checkrc.pxd"", line 13, in zmq.backend.cython.checkrc._check_rc
KeyboardInterrupt
>>>
```
Looks like it's waiting for a process to return a value.

I discovered that if I changed `num_proc = 4` to `num_proc = 8`, the code worked. What do you think is causing this behaviour? Thank you.",thank answer much behaviour ran code computer also produced couple error like python default anaconda type help copyright license information import starting process method efficient safe may cause deadlock method instead run set import install guarantee recent call last file line module file line module file line yield super self file line yield file line file line ret file line file line file line file line file line file line like waiting process return value discovered code worked think causing behaviour thank,issue,positive,positive,positive,positive,positive,positive
745694309,"> In this example, the loop never stops. It just produces more and more numbers

This is expected and documented in https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.MultiProcessMapData: (because `RepeatedData(MapData(df, ...), -1)` means an infinite repeat).

> If I set strict to False, the code produces 5 numbers (0, 10, 20, 30, 40) and then freezes

This is not expected. The code works on Linux so this seems like a bug specific to macOS. I'll take a look some time.",example loop never infinite repeat set strict false code code work like bug specific take look time,issue,negative,negative,negative,negative,negative,negative
745207576,"I can train a model using some keras layers by mimic the example code, but the saved checkpoint is problematic.  
I defined a model like this:
```
# some layer definition
@memoized
def keras_rnn():
    with clear_tower0_name_scope():
        blstm = layers.Bidirectional(
            layers.CuDNNLSTM(HIDDEN, return_state=True, return_sequences=True))
    return blstm

@memoized
def keras_rnn2():
    with clear_tower0_name_scope():
        blstm = layers.Bidirectional(
            layers.CuDNNLSTM(HIDDEN, return_state=False, return_sequences=True))
    return blstm

feat = ...  # extract features using tf.nn standard layers
blstm1 = keras_rnn()
blstm2 = keras_rnn2()
outputs1, *states1 = blstm1(feat)#stacked lstm layer1
outputs2 = blstm2(outputs1, initial_state=states1) #stacked lstm layer2
if ctx.is_main_training_tower:
    for op in blstm1.updates:
        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, op)
    for op in blstm2.updates:
        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, op)

...

```
After training and export model using:
```
predcfg = PredictConfig(
            model = MyModel(),
            session_init=SmartInit('/train_log/crnn_stacked/model-10000'),
            input_names=['image'],
            output_names=['decoded_value', 'decoded_indice', 'decoded_shape'])
ModelExporter(predcfg).export_compact('./crnn_keras.pb', optimize=False)
```
which raise an error caused by mismatch of checkpoint and model:  
>[1215 10:22:52 @sessinit.py:87] WRN The following variables are in the graph, but not found in the checkpoint: bidirectional/forward_cu_dnnlstm/kernel, bidirectional/forward_cu_dnnlstm/recurrent_kernel, bidirectional/forward_cu_dnnlstm/bias, bidirectional/backward_cu_dnnlstm/kernel, bidirectional/backward_cu_dnnlstm/recurrent_kernel, bidirectional/backward_cu_dnnlstm/bias, bidirectional_1/forward_cu_dnnlstm_1/kernel, bidirectional_1/forward_cu_dnnlstm_1/recurrent_kernel, bidirectional_1/forward_cu_dnnlstm_1/bias, bidirectional_1/backward_cu_dnnlstm_1/kernel, bidirectional_1/backward_cu_dnnlstm_1/recurrent_kernel, bidirectional_1/backward_cu_dnnlstm_1/bias
...
>[1215 10:22:52 @sessinit.py:87] WRN The following variables are in the checkpoint, but not found in the graph: beta1_power, beta2_power, global_step, learning_rate, tower0/bidirectional/backward_cu_dnnlstm/bias, tower0/bidirectional/backward_cu_dnnlstm/kernel, tower0/bidirectional/backward_cu_dnnlstm/recurrent_kernel, tower0/bidirectional/forward_cu_dnnlstm/bias, tower0/bidirectional/forward_cu_dnnlstm/kernel, tower0/bidirectional/forward_cu_dnnlstm/recurrent_kernel, tower0/bidirectional_1/backward_cu_dnnlstm_1/bias, tower0/bidirectional_1/backward_cu_dnnlstm_1/kernel, tower0/bidirectional_1/backward_cu_dnnlstm_1/recurrent_kernel, tower0/bidirectional_1/forward_cu_dnnlstm_1/bias, tower0/bidirectional_1/forward_cu_dnnlstm_1/kernel, tower0/bidirectional_1/forward_cu_dnnlstm_1/recurrent_kernel
...
>Failed precondition: Error while reading resource variable bidirectional_1/backward_cu_dnnlstm_1/kernel from Container: localhost. This could mean that the variable was uninitialized

This is probably due to Keras name_scope issues and use `layers.Bidirectional(...,name=tower0/bidirectional)` explicitly works during export. But all weights except Keras layers are saved with `tower0` name scope, which seems weird.",train model mimic example code saved problematic defined model like layer definition hidden return hidden return feat extract standard feat layer layer training export model model raise error mismatch model following graph found following found graph precondition error reading resource variable container could mean variable probably due use explicitly work export except saved tower name scope weird,issue,negative,negative,negative,negative,negative,negative
737555126,"Thanks a lot, `DumpTensors` callback was exactly what I need to do this.",thanks lot exactly need,issue,negative,positive,positive,positive,positive,positive
735640870,"It seems you're using github master version of FasterRCNN example with an older version of tensorpack.

Please make sure tensorpack version is newer than (or equal to) the example.",master version example older version please make sure version equal example,issue,positive,positive,positive,positive,positive,positive
733953100,"As long as you can build a graph and return a loss in tensoflow it should be able to train in tensorpack. This appears to be only a tensorflow question, unless there is any specific question about tensorpack you'd like to ask.

To do something like this in tensorflow you can:
```python
pred_teacher = teacher_model(input)
pred_student = student_model(input)
return loss_func(pred_teacher, pred_student)
```",long build graph return loss able train question unless specific question like ask something like python input input return,issue,negative,positive,positive,positive,positive,positive
733952232,"> is it possible to make so that the tf.io.write_file function is run for each new image during training?

Yes. You need to create the operation and make it a dependency of the training op. Session shall never be created when building the graph. You seem to have a misunderstanding of the concept of graph/session and how tensorflow runs things.

> I would like to use it to store in a file the total cost associated to each image during training.

Using https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.DumpTensors or [write a callback](https://tensorpack.readthedocs.io/tutorial/extend/callback.html) following `DumpTensors` is a much easier way to do that",possible make function run new image training yes need create operation make dependency training session shall never building graph seem misunderstanding concept would like use store file total cost associated image training write following much easier way,issue,positive,positive,neutral,neutral,positive,positive
725045755,"You suggest that it ""fails when tf runs tests"". But tf is not running tests when it fails. 
Either way, it's unrelated to this project.",suggest running either way unrelated project,issue,negative,neutral,neutral,neutral,neutral,neutral
724609823,Could you please clarify what do you mean by saying it's not running tests?,could please clarify mean saying running,issue,negative,negative,negative,negative,negative,negative
724236238,"It's not running tests.

The issue happens in `import tensorflow as tf` and therefore is not related to this project.",running issue import therefore related project,issue,negative,neutral,neutral,neutral,neutral,neutral
718097158,"Settings are given in https://github.com/tensorpack/tensorpack/tree/master/examples/ImageNetModels#vgg16 already.
We don't plan to release the models.

> what does --batch parameter?


The above link also says: `see ./vgg16.py --help for usage.`

",given already plan release batch parameter link also see help,issue,negative,neutral,neutral,neutral,neutral,neutral
714274422,"Thank yoh very much,
My session is still running, will update when it's completed.

Best regards,
Shmulik

On Wed, Oct 21, 2020, 22:37 Yuxin Wu <notifications@github.com> wrote:

> I ran it again with -d 101 --mode resnext32x4d --weight-decay-norm and
> obtains 21.4%. This might better reflect its performance and the one in
> table might be a lucky run.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/1488#issuecomment-713828919>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ARJEKM6ZKUCWMMXHXDRJ4ZDSL42ALANCNFSM4SIH3U2Q>
> .
>
",thank much session still running update best wed wrote ran mode might better reflect performance one table might lucky run thread reply directly view,issue,positive,positive,positive,positive,positive,positive
713828919,I ran it again with `-d 101 --mode resnext32x4d --weight-decay-norm` and obtains 21.4%. This might better reflect its performance and the one in table might be a lucky run.,ran mode might better reflect performance one table might lucky run,issue,positive,positive,positive,positive,positive,positive
713725628,"It was tested. The comment also says ""a complicated model or a future version of Keras may break them.""",tested comment also complicated model future version may break,issue,negative,negative,negative,negative,negative,negative
713662572,"Thanks for the quick answer.
[this comment ](https://github.com/tensorpack/tensorpack/tree/master/examples/keras#imagenet-example) states that keras imagenet ""eproduce exactly the same"" as resnet example. Was this tested?

",thanks quick answer comment exactly example tested,issue,negative,positive,positive,positive,positive,positive
713653392,"As https://github.com/tensorpack/tensorpack/tree/master/examples/keras#note says the keras support is experimental and perhaps it break at some point. Perhaps we should just delete it

Using the code in https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet will get you the right accuracy.",support experimental perhaps break point perhaps delete code get right accuracy,issue,negative,positive,positive,positive,positive,positive
705436033,"Thanks Yuxin Wu,
I'll try it and update once training is completed.

Best regards,
Shmulik

On Thu, Oct 8, 2020 at 10:48 AM Yuxin Wu <notifications@github.com> wrote:

> I don't remember the exact setting used to train it, but I think
> --weight-decay-norm is needed for all resnet >50 layers.
> I probably used batch=256, though I don't think this matters.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/1488#issuecomment-705395578>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ARJEKMYJYCSTFZM62644V2TSJVVGJANCNFSM4SIH3U2Q>
> .
>
",thanks try update training best wrote remember exact setting used train think probably used though think thread reply directly view,issue,positive,positive,positive,positive,positive,positive
705395578,"I don't remember the exact setting used to train it, but I think `--weight-decay-norm` is needed for all resnet >50 layers.
I probably used batch=256, though I don't think this matters.",remember exact setting used train think probably used though think,issue,negative,positive,positive,positive,positive,positive
699064340,"Tensorpack's convolution is correct as long as TensorFlow's convolution is correct. Tensorpack does not implement convolution.

PyTorch and TensorFlow's definition of convolution padding is different. Getting different results out of the box is expected.

Closing as the issue is unrelated to tensorpack.

",convolution correct long convolution correct implement convolution definition convolution padding different getting different box issue unrelated,issue,negative,negative,neutral,neutral,negative,negative
698188107,"I also try to replace the MultiProcessRunnerZMQ to PrefetchDataZMQ. The same issue will encounter.  Is there any other requirements for using tensorpack, like tf, python version??",also try replace issue encounter like python version,issue,negative,neutral,neutral,neutral,neutral,neutral
698179869,"Please provide a minimal reproducible example that others can run. See more at https://stackoverflow.com/help/minimal-reproducible-example.

Maybe try removing `MultiProcessRunnerZMQ` in case the multiprocessing is causing any issues.",please provide minimal reproducible example run see maybe try removing case causing,issue,negative,negative,neutral,neutral,negative,negative
687347783,"cuDNNv8 deprecated the old algorithm selection APIs (https://github.com/tensorflow/tensorflow/commit/255f590ab64e637f49288883013d35efa0633b35#diff-3ddecd9a9809669183ca2750a865f73a) and the new API seems to have regression.
Reported upstream at https://forums.developer.nvidia.com/t/cudnn8-regression-in-algorithm-selection-heuristics/153667.",old algorithm selection new regression upstream,issue,negative,positive,positive,positive,positive,positive
686895129,"Thanks for the update! Confirmed that `TF_CUDNN_USE_AUTOTUNE=1` helped converge to the same performance level without regression. 

Re: TF 1.15 with cuda11, I used the `tf-latest-gpu-gvnic-debian-10` VM image in [GCP's Deep Learning VM](https://cloud.google.com/ai-platform/deep-learning-vm/docs/images) in which TF 1.15 has backported support for cu11. Unfortunately I'm not aware of other TF 1.15 distributions with cu11 support.",thanks update confirmed converge performance level without regression used image deep learning support unfortunately aware support,issue,positive,positive,positive,positive,positive,positive
686663569,"The pb model should accept inputs as defined in this line:https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/predict.py#L147

How to load and run a graph pb model is a tensorflow question and unrelated to tensorpack",model accept defined line load run graph model question unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
686412179,"Thanks for the quick reply and sorry for didn't mention clearly. The .pb file is actually getting from the `predict.py` in the FasterRCNN module (link: https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/predict.py) by setting the optional argument ""output-pb"" as `path/to/pb_model`. I've read the link that you mentioned but I'm not quite sure how to implement using that guideline as I'm not familiar with tensorflow's model (so far I've only been using keras), would appreciate alot if you can provide an example. Kindly let me know if you need further information.",thanks quick reply sorry mention clearly file actually getting module link setting optional argument read link quite sure implement guideline familiar model far would appreciate provide example kindly let know need information,issue,positive,positive,positive,positive,positive,positive
686379892,"I can reproduce the regression with TF2.3 on 1 GTX1080Ti.

The regression comes from cudnn8: cudnn8+cuda10.2 or cudnn8+cuda11 are equally slow, while cudnn7 + cuda10.2 is faster.

The regression only appears when cudnn autotune is disabled. If I apply the above patch, use `MODE_MASK=False` and `TF_CUDNN_USE_AUTOTUNE=1`, I found no regression.

So it seems cudnn8 change some algorithm selection heuristics that affects some convolution shapes used in this R-CNN.",reproduce regression regression come equally slow faster regression disabled apply patch use found regression change algorithm selection convolution used,issue,negative,negative,negative,negative,negative,negative
686351152,"I don't know how you get the pb file so I cannot answer. In general, please see https://tensorpack.readthedocs.io/tutorial/inference.html for any question about inference.",know get file answer general please see question inference,issue,negative,positive,neutral,neutral,positive,positive
685890003,"> I think this default behavior as in TF is what you would usually expect, right?

No. Ours is the normal standard behavior for 4D input. Note that tensorFlow also uses it as the example: https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/layers/normalization.py#L1022-L1027",think default behavior would usually expect right normal standard behavior input note also example,issue,negative,positive,neutral,neutral,positive,positive
685214086,"We cannot help with it unless we can reproduce it on linux.
Changes to data loading should have no effect on saving. The cause is something else.",help unless reproduce data loading effect saving cause something else,issue,negative,neutral,neutral,neutral,neutral,neutral
685213280,"This code is correct because the original author has no weight saved to 0 on Linux, the only difference between me and him is that I used Window, and I deleted #ds = PrefetchDataZMQ(DS, NR_procs), this line of code, because Window doesn't support,",code correct original author weight saved difference used window line code window support,issue,positive,positive,positive,positive,positive,positive
684176496,"> The solution is to write your function in a way that's pickleable (no closures, etc).
> If the unpickleable function is from tensorpack, please report it as an issue.
> 
> The original issue was fixed already because now the function `MapDataComponent.__init__.<locals>.f` was turned into a member function rather than a closure.

Hello, is the problem running on Windows now fixed in the first version?",solution write function way function please report issue original issue fixed already function turned member function rather closure hello problem running fixed first version,issue,negative,positive,positive,positive,positive,positive
683531488,"I got access to a machine with new enough nvidia driver for cuda 11, however, apparently TF 1.15 cannot be built with cuda 11 / cudnn 8: the support was added later at https://github.com/tensorflow/tensorflow/commit/28feb4df0d4ab386946bdee1a0e5c36cc58246cf , https://github.com/tensorflow/tensorflow/commit/255f590ab64e637f49288883013d35efa0633b35, etc.

How did you use TF 1.15 with cuda11/cudnn8? Is there a version maintained elsewhere?",got access machine new enough driver however apparently built support added later use version elsewhere,issue,negative,positive,neutral,neutral,positive,positive
682450173,"> bathsize=8, gpu_nums=4

I'm not sure what that means exactly. The way I understand it it's not even a supported configuration in tensorpack Faster R-CNN example.

There is no such thing that one framework ""should"" be faster than another in any models. It depends on implementation of that model. For example, being both in TensorFlow, our implementation is 4~5x faster than https://github.com/matterport/Mask_RCNN. Some potential ways to improve speed of the Faster R-CNN example are listed at https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md#efficiency. There are reports of 2x speed up (https://github.com/armandmcqueen/tensorpack-mask-rcnn) after doing some optimizations.",sure exactly way understand even configuration faster example thing one framework faster another implementation model example implementation faster potential way improve speed faster example listed speed,issue,positive,positive,positive,positive,positive,positive
678004004,"Gotcha. Thanks for the clarification. As always, I appreciate your diligent maintenance of this codebase and its users.",thanks clarification always appreciate diligent maintenance,issue,positive,positive,positive,positive,positive,positive
677973396,"Thanks for the heads up. Does that concern apply to the horovod trainer as well, or only for the `SyncMultiGPUTrainerReplicated(mode=""nccl"")` ?  ",thanks concern apply trainer well,issue,positive,positive,positive,positive,positive,positive
677920036,"Nice!
Btw, if you're using `mode=nccl`, please note that tensorflow's own NCCL has bugs since 1.15: https://github.com/tensorflow/tensorflow/issues/41539. Latest tensorpack will print a warning about it",nice please note since latest print warning,issue,negative,positive,positive,positive,positive,positive
677898075,"Turned out to be an issue with PCI Access Control Services... solved by following: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#pci-access-control-services-acs

Thanks.",turned issue access control following thanks,issue,negative,positive,neutral,neutral,positive,positive
676534449,"`--task eval` computes evaluation score on an environment and should be the only metric that's used for comparison with other implementation.

Scores of pre-trained models are given in readme.",task evaluation score environment metric used comparison implementation given,issue,negative,neutral,neutral,neutral,neutral,neutral
676324731,"@ppwwyyxx 
Hello,
Thanks for your reply, I'd like to save total reward for every episode, and 'step', or 'scores' so that I can measure it's performance. I have seen some algorithm has processed the reward. some don't. That made me puzzle, some of them use tensorboard, but it's graph looks strange. I don't know if the data are reliable. 
I don't know whether I have made my question clear. If you understand, I hope you can help me. I'll appreciate it~",hello thanks reply like save total reward every episode measure performance seen algorithm reward made puzzle use graph strange know data reliable know whether made question clear understand hope help appreciate,issue,positive,positive,neutral,neutral,positive,positive
675893428,"Also try `SyncMultiGPUTrainerReplicated(mode='gpu‘)` instead of `mode=nccl`, if that's what's been used.
You can also try a basic cifar10 multigpu training, if that doesn't even work, it's probably just a driver/hardware problem.",also try instead used also try basic training even work probably problem,issue,negative,neutral,neutral,neutral,neutral,neutral
675842728,"Correct. All runs use the same container.  And I forgot to include those experiments above, but 1 GPU training in a kubernetes pod works fine. The failed multi GPU runs have been stuck for a few hours.   I'll double check to see if I removed the timeout callback.

[Edit: timeout callback is still intact, but appears not to have triggered, given the pods are still running.]",correct use container forgot include training pod work fine stuck double check see removed edit still intact triggered given still running,issue,negative,positive,positive,positive,positive,positive
675836923,"The successful runs also use the same container right? So presumably the software environment should be the same.

Does 1-GPU training hang? I hope not ..

I would wait for quite a while (e.g. 10 minutes) since sometimes tensorflow is indeed doing some compiling work at the beginning of training.

The FasterRCNN code uses a timeout callback with 10min, so it is sometimes able to throw an error explaining what is hanging after 10 minutes. Maybe that provides some information as well.

",successful also use container right presumably environment training hope would wait quite since sometimes indeed work beginning training code min sometimes able throw error explaining hanging maybe information well,issue,positive,positive,positive,positive,positive,positive
674350628,Please describe what you'd like to do clearly.,please describe like clearly,issue,positive,positive,positive,positive,positive,positive
672107328,"As the [issue template](https://github.com/tensorpack/tensorpack/issues/new?assignees=&labels=&template=unexpected-problems---bugs.md&title=Please+read+%26+provide+the+following+information) mentions:

> If you expect the model to converge / work better, note that we do not give suggestions on how to train a new model. Only in one of the two conditions we will help with it: (1) You're unable to reproduce the results documented in tensorpack examples.
(2) It appears to be a tensorpack bug.
",issue template expect model converge work better note give train new model one two help unable reproduce bug,issue,positive,positive,neutral,neutral,positive,positive
671048913,"Thanks, it worked. cudnn problem. 
The installation is complete ‘source .bashrc’ .",thanks worked problem installation complete source,issue,negative,positive,positive,positive,positive,positive
670968163,"To future readers again: this is an environment problem, and other people's solution is unlikely to solve your problem. Please do not just blindly follow how others solve the issue without knowing the cause of the issue.",future environment problem people solution unlikely solve problem please blindly follow solve issue without knowing cause issue,issue,negative,negative,negative,negative,negative,negative
670936824,"@ppwwyyxx Can run tensorflow-gpu == 1.9 . but API need to be modified.
```
#twth = tf.math.log(wbhb / waha)  # may contain -inf for invalid boxes
  twth = tf.log(wbhb / waha)  # may contain -inf for invalid boxes

#bg_inds = tf.random.shuffle(bg_inds)[:num_bg]
  bg_inds = tf.random_shuffle(bg_inds)[:num_bg]

# fg_box_logits = tf.gather(fg_box_logits, fg_labels, batch_dims=1)
   fg_box_logits = tf.gather(fg_box_logits, fg_labels)
```
```
def gather(params, indices, validate_indices=None, name=None, axis=0)
```


InvalidArgumentError (see above for traceback): Incompatible shapes: [4,4] vs. [324,4]

How to modify？
",run need may contain invalid may contain invalid gather index see incompatible,issue,negative,neutral,neutral,neutral,neutral,neutral
670393077,"```python
        from tensorflow.python.client import device_lib
        local_device_protos = device_lib.list_local_devices()
        gpu_devices = [x.name for x in local_device_protos if x.device_type == 'GPU']
```
will show what devices tensorflow can see. I believe tensorflow is unable to see any GPU on your machine.",python import show see believe unable see machine,issue,negative,negative,negative,negative,negative,negative
670390781,"It would be more helpful if you provide the entire logs you saw on the terminal, not logs that are saved to the log file. The log file does not contain all information (such as the error itself) as you may have already found.

I believe it's an issue of your environment (duplicate of https://github.com/tensorpack/tensorpack/issues/986). Tensorflow would print such information to the terminal but they will not go to the log file.",would helpful provide entire saw terminal saved log file log file contain information error may already found believe issue environment duplicate would print information terminal go log file,issue,negative,neutral,neutral,neutral,neutral,neutral
670380623,"> 
> 
> The issue template says:
> 
> > Include the ENTIRE logs here:

finished editing",issue template include entire finished,issue,negative,neutral,neutral,neutral,neutral,neutral
670327549,"The issue template says:

>  Include the ENTIRE logs here:",issue template include entire,issue,negative,neutral,neutral,neutral,neutral,neutral
667882902,Ok.Thanks a lot. But when you will support more than one image per GPU in training?,lot support one image per training,issue,negative,neutral,neutral,neutral,neutral,neutral
667876276,We currently only support single image per GPU in this example?,currently support single image per example,issue,negative,negative,neutral,neutral,negative,negative
664696754,"The two models (plain ResNet and Mask R-CNN) are very different in the way they use cudnn. For Mask R-CNN we use `TF_CUDNN_USE_AUTOTUNE=0` to avoid the long tuning time, and with tuning disabled, it's very likely that different cudnn versions choose different algorithms (I've seen this in the past even with changes in minor cudnn version number).

The easiest way to verify this might be to enable tuning with `TF_CUDNN_USE_AUTOTUNE=1` instead of `0`. However, to avoid the long tuning time you'll need to resize images to the same resolution with this change:
```diff
diff --git i/examples/FasterRCNN/data.py w/examples/FasterRCNN/data.py
index 35d8bd4f..eefe193f 100644
--- i/examples/FasterRCNN/data.py
+++ w/examples/FasterRCNN/data.py
@@ -73,7 +73,8 @@ class TrainingDataPreprocessor:
     def __init__(self, cfg):
         self.cfg = cfg
         self.aug = imgaug.AugmentorList([
-            CustomResize(cfg.PREPROC.TRAIN_SHORT_EDGE_SIZE, cfg.PREPROC.MAX_SIZE),
+            #CustomResize(cfg.PREPROC.TRAIN_SHORT_EDGE_SIZE, cfg.PREPROC.MAX_SIZE),
+            imgaug.Resize((800, 800)),
             imgaug.Flip(horiz=True)
         ])

```
I often do this when need to benchmark the full power of GPUs. 

Then, benchmark the two environment, both with cudnn tuning enabled and see if they give similar speed. This should rule out differences in algorithm heuristics between cudnn versions, if any.",two plain mask different way use mask use avoid long tuning time tuning disabled likely different choose different seen past even minor version number easiest way verify might enable tuning instead however avoid long tuning time need resize resolution change git index class self often need full power two environment tuning see give similar speed rule algorithm,issue,negative,negative,neutral,neutral,negative,negative
664693802,"Yes, I also tested https://github.com/tensorpack/benchmarks/tree/master/ResNet-MultiGPU and CUDA 11 + CUDNN 8 shows slightly better performance than CUDA 10 + CUDNN 7. This might suggest that the issue is related to some ops in Mask R-CNN.",yes also tested slightly better performance might suggest issue related mask,issue,positive,positive,positive,positive,positive,positive
664678865,"Maybe I'll be able to test cuda 11 on a 1080 soon, but it will take a long time before I'm able to access a cuda11-capable V100 machine.

I guess you were using cuda 10 with cudnn7 but cuda 11 with cudnn8? It might help to try cudnn 8 with cuda 10 as well since nvidia has provided such combination. Presumably, cuda and cudnn are the only two variables between the two settings.",maybe able test soon take long time able access machine guess might help try well since provided combination presumably two two,issue,positive,positive,positive,positive,positive,positive
664672450,Quick update: I saw ~15% slowdown even in 1 GPU setting. Haven't had the chance to do any detailed profiling. Were you be able to reproduce the slowdown?,quick update saw slowdown even setting chance detailed able reproduce slowdown,issue,negative,positive,positive,positive,positive,positive
664177438,"@AziziShekoofeh you should add on an argument like "" --device='0' "" while using training command",add argument like training command,issue,negative,neutral,neutral,neutral,neutral,neutral
663147282,"There was never such a function in `GraphBuilder`. This method did exist in the class `ModelDesc` though, but it was a long time ago and it was replaced by `get_input_signature`. https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.ModelDescBase.get_input_signature",never function method exist class though long time ago,issue,negative,negative,neutral,neutral,negative,negative
662591421,"> AssertionError: [MultiThreadMapData] Map function cannot return None when strict mode is used.

As the error suggests, strict mode cannot be used if the function returns None. It was not well implemented",map function return none strict mode used error strict mode used function none well,issue,negative,neutral,neutral,neutral,neutral,neutral
662432245,"I've fixed an exception error in strict mode when the input was None: 
add  a judge after parallel_map.py, line 135 of OBJ. The code is as follows:

def run(self):
            try:
                while True:
                    dp = self.queue_get_stoppable(self.inq)
                    if self.stopped():
                        return
                    # cannot ignore None here. will lead to unsynced send/recv
                    obj = self.func(dp)
+-+-+-+-+-+-
                    **if obj is None:
                        continue**
+-+-+-+-+-+-
                    self.queue_put_stoppable(self.outq, obj)
            except Exception:
                if self.stopped():
                    pass        # skip duplicated error messages
                else:
                    raise
            finally:
                self.stop()",fixed exception error strict mode input none add judge line code run self try true return ignore none lead none continue except exception pas skip error else raise finally,issue,negative,positive,positive,positive,positive,positive
662352165,"Will strict mode affect the prediction, I'm using **NO** strict mode in the MultiThreadMapData function, and no errors have been reported yet.",strict mode affect prediction strict mode function yet,issue,negative,neutral,neutral,neutral,neutral,neutral
662344800,"  0%|          | 0/2721 [00:00<?, ?it/s]
[0722 09:13:18 @input_source.py:599] Pre-filling StagingArea ...
[0722 09:13:19 @input_source.py:603] 1 element was put into StagingArea on each tower.
  3%|▎         | 93/2721 [01:55<1:32:40,  2.12s/it]Process _Worker-1:
Traceback (most recent call last):
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel.py"", line 50, in _repeat_iter
    yield from get_itr()
  File ""/opt/app/conda/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel.py"", line 313, in run
    dp = next(itr)
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/common.py"", line 120, in __iter__
    for data in self.ds:
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel_map.py"", line 202, in __iter__
    yield from super(MultiThreadMapData, self).__iter__()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel_map.py"", line 87, in __iter__
    yield from self.get_data_strict()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel_map.py"", line 74, in get_data_strict
    yield self._recv_filter_none()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel_map.py"", line 47, in _recv_filter_none
    ""[{}] Map function cannot return None when strict mode is used."".format(type(self).__name__)
AssertionError: [MultiThreadMapData] Map function cannot return None when strict mode is used.
  4%|▍         | 117/2721 [02:54<1:54:50,  2.65s/it]
[0722 09:16:13 @parallel.py:351] ERR Exception '<class 'AssertionError'>' in worker:
[0722 09:16:13 @input_source.py:171] ERR [EnqueueThread] Exception in thread EnqueueThread: enqueue dataflow to TF queue ""QueueInput/input_queue"":
Traceback (most recent call last):
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/input_source/input_source.py"", line 159, in run
    dp = next(self._itr)
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/common.py"", line 389, in __iter__
    yield from self.ds
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel.py"", line 363, in __iter__
    yield self._recv()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel.py"", line 352, in _recv
    raise exc.exc_type(exc.exc_msg)
AssertionError: Traceback (most recent call last):
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel.py"", line 313, in run
    dp = next(itr)
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel.py"", line 50, in _repeat_iter
    yield from get_itr()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/common.py"", line 120, in __iter__
    for data in self.ds:
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel_map.py"", line 202, in __iter__
    yield from super(MultiThreadMapData, self).__iter__()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel_map.py"", line 87, in __iter__
    yield from self.get_data_strict()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel_map.py"", line 74, in get_data_strict
    yield self._recv_filter_none()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel_map.py"", line 47, in _recv_filter_none
    ""[{}] Map function cannot return None when strict mode is used."".format(type(self).__name__)
AssertionError: [MultiThreadMapData] Map function cannot return None when strict mode is used.

[0722 09:16:13 @input_source.py:177] [EnqueueThread] Thread EnqueueThread: enqueue dataflow to TF queue ""QueueInput/input_queue"" Exited.
  6%|▌         | 168/2721 [05:41<1:26:27,  2.03s/it]
---------------------------------------------------------------------------
OutOfRangeError                           Traceback (most recent call last)
/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
   1364     try:
-> 1365       return fn(*args)
   1366     except errors.OpError as e:

/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,
-> 1350                                       target_list, run_metadata)
   1351 

/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1442                                             fetch_list, target_list,
-> 1443                                             run_metadata)
   1444 

OutOfRangeError: FIFOQueue '_0_QueueInput/input_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[{{node QueueInput/input_deque}}]]

During handling of the above exception, another exception occurred:

OutOfRangeError                           Traceback (most recent call last)
<ipython-input-2-66c440eb6ad2> in <module>
     41         ds = get_imagenet_dataflow(args.data, 'val', batch)#,parallel=1
     42         Init=SmartInit(args.load)
---> 43         eval_classification(model,Init , ds)
     44     else:
     45         if args.fake:

~/work/label_classify/tensorpack/examples/ResNet/imagenet_utils.py in eval_classification(model, sessinit, dataflow)
    348     csv_write_path = all_path + csv_file_name_write
    349     for _ in tqdm.trange(dataflow.size()):
--> 350         top1, top5, prob,imgname,csvline=pred()
    351         #print(res.shape)
    352         #print([np.argmax(res[i,:]) for i in range(res.shape[0])])

~/.local/lib/python3.6/site-packages/tensorpack/predict/feedfree.py in __call__(self)
     66 
     67     def __call__(self):
---> 68         return self._hooked_sess.run(self._output_tensors)
     69 
     70     def _do_call(self):

/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
   1416         feed_dict=feed_dict,
   1417         options=options,
-> 1418         run_metadata=run_metadata)
   1419 
   1420     for hook in self._hooks:

/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py in run(self, *args, **kwargs)
   1174 
   1175   def run(self, *args, **kwargs):
-> 1176     return self._sess.run(*args, **kwargs)
   1177 
   1178   def run_step_fn(self, step_fn, raw_session, run_with_hooks):

/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    954     try:
    955       result = self._run(None, fetches, feed_dict, options_ptr,
--> 956                          run_metadata_ptr)
    957       if run_metadata:
    958         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1178     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1179       results = self._do_run(handle, final_targets, final_fetches,
-> 1180                              feed_dict_tensor, options, run_metadata)
   1181     else:
   1182       results = []

/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1357     if handle is None:
   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1359                            run_metadata)
   1360     else:
   1361       return self._do_call(_prun_fn, handle, feeds, fetches)

/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
   1382                     '\nsession_config.graph_options.rewrite_options.'
   1383                     'disable_meta_optimizer = True')
-> 1384       raise type(e)(node_def, op, message)
   1385 
   1386   def _extend_graph(self):

OutOfRangeError: FIFOQueue '_0_QueueInput/input_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[node QueueInput/input_deque (defined at /opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'QueueInput/input_deque':
  File ""/opt/app/conda/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/opt/app/conda/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/opt/app/conda/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/opt/app/conda/lib/python3.6/site-packages/traitlets/config/application.py"", line 664, in launch_instance
    app.start()
  File ""/opt/app/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 597, in start
    self.io_loop.start()
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 149, in start
    self.asyncio_loop.run_forever()
  File ""/opt/app/conda/lib/python3.6/asyncio/base_events.py"", line 422, in run_forever
    self._run_once()
  File ""/opt/app/conda/lib/python3.6/asyncio/base_events.py"", line 1434, in _run_once
    handle._run()
  File ""/opt/app/conda/lib/python3.6/asyncio/events.py"", line 145, in _run
    self._callback(*self._args)
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/ioloop.py"", line 690, in <lambda>
    lambda f: self._run_callback(functools.partial(callback, future))
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/ioloop.py"", line 743, in _run_callback
    ret = callback()
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/gen.py"", line 787, in inner
    self.run()
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/gen.py"", line 748, in run
    yielded = self.gen.send(value)
  File ""/opt/app/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 381, in dispatch_queue
    yield self.process_one()
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/gen.py"", line 225, in wrapper
    runner = Runner(result, future, yielded)
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/gen.py"", line 714, in __init__
    self.run()
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/gen.py"", line 748, in run
    yielded = self.gen.send(value)
  File ""/opt/app/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 365, in process_one
    yield gen.maybe_future(dispatch(*args))
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""/opt/app/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""/opt/app/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 545, in execute_request
    user_expressions, allow_stdin,
  File ""/opt/app/conda/lib/python3.6/site-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""/opt/app/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 300, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/opt/app/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 536, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/opt/app/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2858, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/opt/app/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2886, in _run_cell
    return runner(coro)
  File ""/opt/app/conda/lib/python3.6/site-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
    coro.send(None)
  File ""/opt/app/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3063, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/opt/app/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3254, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""/opt/app/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-66c440eb6ad2>"", line 43, in <module>
    eval_classification(model,Init , ds)
  File ""/home/powerop/work/label_classify/tensorpack/examples/ResNet/imagenet_utils.py"", line 344, in eval_classification
    pred = FeedfreePredictor(pred_config, StagingInput(QueueInput(dataflow), device='/gpu:0'))
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/predict/feedfree.py"", line 38, in __init__
    self._input_tensors = self._input_source.get_input_tensors()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/input_source/input_source_base.py"", line 114, in get_input_tensors
    return self._get_input_tensors()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/input_source/input_source.py"", line 665, in _get_input_tensors
    inputs = self._input.get_input_tensors()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/input_source/input_source_base.py"", line 114, in get_input_tensors
    return self._get_input_tensors()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/input_source/input_source.py"", line 270, in _get_input_tensors
    ret = self.queue.dequeue(name='input_deque')
  File ""/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/data_flow_ops.py"", line 446, in dequeue
    self._queue_ref, self._dtypes, name=name)
  File ""/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_data_flow_ops.py"", line 4140, in queue_dequeue_v2
    timeout_ms=timeout_ms, name=name)
  File ""/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/opt/app/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

**Another exception occur when return None**",element put tower process recent call last file line yield file line file line run next file line data file line yield super self file line yield file line yield file line map function return none strict mode used type self map function return none strict mode used err exception class worker err exception thread queue recent call last file line run next file line yield file line yield file line raise recent call last file line run next file line yield file line data file line yield super self file line yield file line yield file line map function return none strict mode used type self map function return none strict mode used thread queue recent call last self try return except return self closed insufficient current size node handling exception another exception recent call last module batch model else model top top prob print print range self self return self run self hook run self run self return self run self try result none self handle handle handle else self handle handle none return else return handle self true raise type message self closed insufficient current size node defined original stack trace file line file line code file line module file line file line start file line start file line file line file line file line lambda lambda future file line ret file line inner file line run value file line yield file line wrapper runner runner result future file line file line run value file line yield dispatch file line wrapper next result file line yield handler stream file line wrapper next result file line file line wrapper next result file line code file line return super self file line silent file line return runner file line none file line file line await code result file line file line module model file line file line file line return file line file line return file line ret file line file line file line file line return file line file line file line another exception occur return none,issue,positive,positive,neutral,neutral,positive,positive
662339621,"I just realize what stupid thing I have did，hhhhh~~

As you say, I didn't really return None, which is the format that returns [None, label]",realize stupid thing say really return none format none label,issue,negative,negative,negative,negative,negative,negative
662336849,"From the error, it looks like you did not return None, but threw an exception.

If you need help diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md)). Without knowing what you did we cannot help.",error like return none threw exception need help diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected without knowing help,issue,positive,positive,positive,positive,positive,positive
662330282,"  4%|▎         | 98/2721 [02:08<1:39:40,  2.28s/it]Exception in thread Thread-20:
Traceback (most recent call last):
  File ""/opt/app/conda/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/parallel_map.py"", line 135, in run
    obj = self.func(dp)
  File ""/home/powerop/work/label_classify/tensorpack/examples/ResNet/imagenet_utils.py"", line 123, in mapf
    im = aug.augment(im)
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/imgaug/base.py"", line 198, in augment
    check_dtype(img)
  File ""/home/powerop/.local/lib/python3.6/site-packages/tensorpack/dataflow/image.py"", line 18, in check_dtype
    assert isinstance(img, np.ndarray), ""[Augmentor] Needs an numpy array, but got a {}!"".format(type(img))
AssertionError: [Augmentor] Needs an numpy array, but got a <class 'NoneType'>!

  6%|▌         | 168/2721 [06:00<3:02:39,  4.29s/it]

It does not continue to work and I found the following comment in your code:
 **# cannot ignore None here. will lead to unsynced send/recv**
(in parallel_map.py ,line 134)

",exception thread recent call last file line file line run file line file line augment file line assert need array got type need array got class continue work found following comment code ignore none lead line,issue,negative,neutral,neutral,neutral,neutral,neutral
662325752,"> Does this sentence mean that if the image is not obtained successfully, I just need to return None, and it will automatically give up this image acquisition andproceed to the next image acquisition

yes",sentence mean image successfully need return none automatically give image acquisition next image acquisition yes,issue,positive,positive,positive,positive,positive,positive
662318154,"**map_func (callable): datapoint -> datapoint | None. Return None to
                discard/skip the datapoint.**
Hello, I have found the sentence you wrote in the comment. Does this sentence mean that if the image is not obtained successfully, I just need to return None, and it will  automatically  give up this image acquisition andproceed to the next image acquisition",callable none return none hello found sentence wrote comment sentence mean image successfully need return none automatically give image acquisition next image acquisition,issue,negative,positive,positive,positive,positive,positive
662310366,"Hello, I solved the problem by getting the classification probability of each image by obtaining the 'prob' in the resnet network.

But I run into another problem: Since I'm getting the image from the URL, And I can't guarantee that every image will be successful, I want to know in which function the ”mapf (dp)“ function in the ”get_imagenet_dataflow“ function is called so that I can do exception handling on that function.",hello problem getting classification probability image network run another problem since getting image ca guarantee every image successful want know function function function exception handling function,issue,negative,positive,positive,positive,positive,positive
660872411,"> see https://tensorpack.readthedocs.io/tutorial/inference.html


Are there any specific examples or previous answers, just a simple explanation is just too difficult for me to finish this work.",see specific previous simple explanation difficult finish work,issue,negative,negative,negative,negative,negative,negative
660412901,"As the error suggests, custom optimizer can be used as long as they subclass (and follow the interface of) `tf.train.Optimizer`.",error custom used long subclass follow interface,issue,negative,negative,neutral,neutral,negative,negative
659167687,"OK!

tensorpack is perfect!

I found the solution.

I had to understand get_variable, and variable_scope better.

The Tip is to understand the error of my code, I have to use jupyter notebook to analyze the py file.

Thank you~",perfect found solution understand better tip understand error code use notebook analyze file thank,issue,positive,positive,positive,positive,positive,positive
658709259,"I'm not sure what the question is about. If you have questions how to implement a tensorflow layer, it is unrelated to tensorpack and we cannot help with that. The tensorpack-specific parts like `convert_to_tflayer_args`, `layer_register`, `VariableHolder` can all be removed without affecting how it works (https://tensorpack.readthedocs.io/tutorial/symbolic.html)",sure question implement layer unrelated help like removed without affecting work,issue,positive,positive,positive,positive,positive,positive
658704737,"In tensorpack,
   conv2d.py file,
   ```
   with rename_get_variable({'kernel': 'W', 'bias': 'b'}):
            layer = tf.layers.Conv2D(
                filters,
                kernel_size,
                strides=strides,
                padding=padding,
                data_format=data_format,
                dilation_rate=dilation_rate,
                activation=activation,
                use_bias=use_bias,
                kernel_initializer=kernel_initializer,
                bias_initializer=bias_initializer,
                kernel_regularizer=kernel_regularizer,
                bias_regularizer=bias_regularizer,
                activity_regularizer=activity_regularizer,
                _reuse=tf.get_variable_scope().reuse)
            ret = layer.apply(inputs, scope=tf.get_variable_scope())
            ret = tf.identity(ret, name='output')

        ret.variables = VariableHolder(W=layer.kernel)
   ```
  There is no `W` weight kernel_shape. but Variable Holder keep W.
 
  in another part when split, and dilation_rate is over 1
  ```
        kernel_shape = shape2d(kernel_size)
        filter_shape = kernel_shape + [in_channel / split, out_channel]
        stride = shape4d(strides, data_format=data_format)
        W = tf.get_variable('W', filter_shape, dtype=inputs_dtype, initializer=kernel_initializer)
  ``` 
 The filter shape might include [kernel_size and in_channel and out_channel]

 I made similarly shift2d.py
 
```
from tensorflow_active_shift.python.ops import active_shift2d_ops
@layer_register(log_shape=True)
@convert_to_tflayer_args(
args_names=['filters','kernel_size'],
name_mapping={
'out_channel': 'filters',
'kernel_shape': 'kernel_size',
'strides': 'stride',
'paddings' : 'padding'
})

def Shift2D(
inputs,
filters, #n_shifts=2, #number of shifts position to compute
kernel_size, #shifts_size, #number of channels
stride=1,
padding='SAME', #'same',
data_format='NCHW',
normalize=True):
....

filter_shape = (filters, kernel_size)
input_dtype = inputs.dtype
W = tf.get_variable('W', filter_shape, dtype=input_dtype, initializer = tf.variance_scaling_initializer(scale=1.0 / 3, mode='fan_in', distribution='uniform'))
with tf.variable_scope(""shift""):
        layer = active_shift2d_ops.active_shift2d_op(
            inputs,
            shift=W,
            strides=strides,
            paddings=paddings,
            data_format=data_format) 
ret = tf.identity(layer, name='output')
ret.variables = VariableHolder(W=W)
return ret
```

I think it's the reason to show different accuracy...
How should I make the kernel shape?
Could you give me the guide to get correct accuracy?

Thank you in advance",file layer ret ret ret weight variable holder keep another part split shaped split stride shaped filter shape might include made similarly import number position compute number shift layer ret layer return ret think reason show different accuracy make kernel shape could give guide get correct accuracy thank advance,issue,negative,neutral,neutral,neutral,neutral,neutral
658667320,"As mentioned in https://github.com/tensorpack/tensorpack/issues/1465#issuecomment-658644938, evaluation results are supposed to show in console and tensorboard, whose log file location is defined in https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.TFEventWriter",evaluation supposed show console whose log file location defined,issue,negative,neutral,neutral,neutral,neutral,neutral
658654341,"Evaluation results are supposed to show, with original, unmodified code.",evaluation supposed show original unmodified code,issue,negative,positive,positive,positive,positive,positive
658649315,"@ppwwyyxx  I know how val dataset is named. But where do i change it in the code so it will show me the evaluation on test data ?
",know change code show evaluation test data,issue,negative,neutral,neutral,neutral,neutral,neutral
658644938,"> WHere exactly it will be written 

answered in https://tensorpack.readthedocs.io/tutorial/summary.html which points to https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.TFEventWriter for tensorboard, and a few other locations (such as console output)

> THere is entire stack of values:

This part of logs do not contain any evaluation on validation set. However I do not know what you did so I cannot comment on whether it's reasonable or not. If it's unexpected to you, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

These metrics are undocumented mainly because they are useful for experts only (who can already easily figure out what they mean).

The validation dataset name is defined by `DATA.VAL` which should be set by users",exactly written console output entire stack part contain evaluation validation set however know comment whether reasonable unexpected please post relevant following issue template click new issue unexpected visit link post unexpected metric undocumented mainly useful already easily figure mean validation name defined set,issue,positive,positive,positive,positive,positive,positive
658631068,"@ppwwyyxx  I cannot properly understand what you trying to say. 

""It will be written to tensorboard file after every 5 (or EVAL_PERIOD) epochs."" WHere exactly it will be written ? Which one ?
THere is entire stack of values:

```
[32m[0708 15:53:24 @monitor.py:476][0m fastrcnn_losses/box_loss: 0.14889
[32m[0708 15:53:24 @monitor.py:476][0m fastrcnn_losses/label_loss: 0.1246
[32m[0708 15:53:24 @monitor.py:476][0m fastrcnn_losses/label_metrics/accuracy: 0.95327
[32m[0708 15:53:24 @monitor.py:476][0m fastrcnn_losses/label_metrics/false_negative: 0.58954
[32m[0708 15:53:24 @monitor.py:476][0m fastrcnn_losses/label_metrics/fg_accuracy: 0.41046
[32m[0708 15:53:24 @monitor.py:476][0m fastrcnn_losses/num_fg_label: 31.466
[32m[0708 15:53:24 @monitor.py:476][0m learning_rate: 0.001
[32m[0708 15:53:24 @monitor.py:476][0m maskrcnn_loss/accuracy: 0.82446
[32m[0708 15:53:24 @monitor.py:476][0m maskrcnn_loss/fg_pixel_ratio: 0.43269
[32m[0708 15:53:24 @monitor.py:476][0m maskrcnn_loss/maskrcnn_loss: 0.37383
[32m[0708 15:53:24 @monitor.py:476][0m maskrcnn_loss/pos_accuracy: 0.34168
[32m[0708 15:53:24 @monitor.py:476][0m mean_gt_box_area: 1.4004e+05
[32m[0708 15:53:24 @monitor.py:476][0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level2: 262.26
[32m[0708 15:53:24 @monitor.py:476][0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level3: 126.41
[32m[0708 15:53:24 @monitor.py:476][0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level4: 99.47
[32m[0708 15:53:24 @monitor.py:476][0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level5: 23.859
[32m[0708 15:53:24 @monitor.py:476][0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level2: 6.2986
[32m[0708 15:53:24 @monitor.py:476][0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level3: 8.5419
[32m[0708 15:53:24 @monitor.py:476][0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level4: 10.951
[32m[0708 15:53:24 @monitor.py:476][0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level5: 5.6747
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/box_loss: 0.017103
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/label_loss: 0.025841
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level2/box_loss: 0.0011629
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level2/label_loss: 0.0044014
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level2/label_metrics/precision_th0.1: 0.34037
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level2/label_metrics/precision_th0.2: 0.43397
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level2/label_metrics/precision_th0.5: 0.52058
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level2/label_metrics/recall_th0.1: 0.56467
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level2/label_metrics/recall_th0.2: 0.55561
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level2/label_metrics/recall_th0.5: 0.53658
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level2/num_pos_anchor: 1.3542
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level2/num_valid_anchor: 188.18
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level3/box_loss: 0.0015958
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level3/label_loss: 0.0065112
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level3/label_metrics/precision_th0.1: 0.20162
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level3/label_metrics/precision_th0.2: 0.30449
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level3/label_metrics/precision_th0.5: 0.49202
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level3/label_metrics/recall_th0.1: 0.59069
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level3/label_metrics/recall_th0.2: 0.58801
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level3/label_metrics/recall_th0.5: 0.5625
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level3/num_pos_anchor: 1.6057
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level3/num_valid_anchor: 46.603
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level4/box_loss: 0.0034729
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level4/label_loss: 0.0079706
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level4/label_metrics/precision_th0.1: 0.3207
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level4/label_metrics/precision_th0.2: 0.37574
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level4/label_metrics/precision_th0.5: 0.49032
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level4/label_metrics/recall_th0.1: 0.63498
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level4/label_metrics/recall_th0.2: 0.62747
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level4/label_metrics/recall_th0.5: 0.60544
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level4/num_pos_anchor: 2.7482
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level4/num_valid_anchor: 11.931
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level5/box_loss: 0.0049636
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level5/label_loss: 0.0045337
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level5/label_metrics/precision_th0.1: 0.55933
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level5/label_metrics/precision_th0.2: 0.61226
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level5/label_metrics/precision_th0.5: 0.67149
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level5/label_metrics/recall_th0.1: 0.77204
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level5/label_metrics/recall_th0.2: 0.77204
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level5/label_metrics/recall_th0.5: 0.75253
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level5/num_pos_anchor: 3.9432
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level5/num_valid_anchor: 5.5695
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level6/box_loss: 0.0059082
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level6/label_loss: 0.0024245
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level6/label_metrics/precision_th0.1: 0.64074
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level6/label_metrics/precision_th0.2: 0.64075
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level6/label_metrics/precision_th0.5: 0.65951
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level6/label_metrics/recall_th0.1: 0.71601
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level6/label_metrics/recall_th0.2: 0.71598
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level6/label_metrics/recall_th0.5: 0.71147
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level6/num_pos_anchor: 3.5083
[32m[0708 15:53:24 @monitor.py:476][0m rpn_losses/level6/num_valid_anchor: 3.7146
[32m[0708 15:53:24 @monitor.py:476][0m sample_fast_rcnn_targets/num_bg: 480.53
[32m[0708 15:53:24 @monitor.py:476][0m sample_fast_rcnn_targets/num_fg: 31.466
[32m[0708 15:53:24 @monitor.py:476][0m sample_fast_rcnn_targets/proposal_metrics/best_iou_per_gt: 0.71313
[32m[0708 15:53:24 @monitor.py:476][0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.3: 0.93325
[32m[0708 15:53:24 @monitor.py:476][0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.5: 0.92957
[32m[0708 15:53:24 @monitor.py:476][0m total_cost: 0.92397
[32m[0708 15:53:24 @monitor.py:476][0m wd_cost: 0.23371
```
which as you mentioned in [here](https://github.com/tensorpack/tensorpack/issues/1440) ""There isn't a description of them now.""

You did fantastic work with the library, but I think you are missing some docs here. 

""The code does not compute any recall-related metrics on validation set write any recall-related metrics on validation set to tensorboard, but they will be computed and printed."", where they will be printed ?

And again, ""(the metrics) are computed on training data, unless the metric contains the test dataset name."" how and where to change that name ?
",properly understand trying say written file every exactly written one entire stack description fantastic work library think missing code compute metric validation set write metric validation set printed printed metric training data unless metric test name change name,issue,negative,positive,neutral,neutral,positive,positive
658627161,"It will be written to tensorboard file after every 5 (or EVAL_PERIOD) epochs.

The code does not <del>compute any recall-related metrics on validation set </del> write any recall-related metrics on validation set to tensorboard, but they will be computed and printed.

As I mentioned:

> (the metrics) are computed on training data, unless the metric contains the test dataset name.

",written file every code compute metric validation set write metric validation set printed metric training data unless metric test name,issue,negative,neutral,neutral,neutral,neutral,neutral
658616331,@ppwwyyxx  Then where will I see the AP value ? and how about recall ? if you mentioned earlier that tensorboard provides metrics for training data ?,see value recall metric training data,issue,negative,neutral,neutral,neutral,neutral,neutral
658615697,"If it is set to 5, it will run evaluation every 5 epochs, i.e. with a period of 5 epochs.",set run evaluation every period,issue,negative,neutral,neutral,neutral,neutral,neutral
658613084,"@ppwwyyxx  I know that this is in config, but I do not know how to configure it. Documentation on this is very poor. 
In config file EVAL_PERIOD defines ""# period (epochs) to run evaluation"", however there is no information of how to set it on test/val data.

How do i have to modify the code ? Could you give the example ?",know know configure documentation poor file period run evaluation however information set data modify code could give example,issue,negative,negative,negative,negative,negative,negative
658475358,"From the outputs of the commands, both tensorflow and `nvidia-smi` think that your machine only has two GPUs. However you're trying to use it with `--gpu 0,1,2,3`, which causes the problem.

As for why tensorflow or `nvidia-smi` can only find two GPUs, it's not a tensorpack issue.",think machine two however trying use problem find two issue,issue,negative,neutral,neutral,neutral,neutral,neutral
658464987,"(1) The output of 
`nvidia-smi -L`
GPU 0: Tesla V100-PCIE-32GB (UUID: GPU-a5786ce8-b0db-xxxxxxxx)
GPU 1: Tesla V100-PCIE-32GB (UUID: GPU-f4fddb82-ad6f-xxxxxxxxx)

(2) The output of 
`python -c 'import tensorflow as tf; print(tf.test.is_gpu_available())'`
2020-07-15 08:37:18.627108: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-07-15 08:37:18.883593: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-07-15 08:37:18.887883: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5571b84fcda0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-15 08:37:18.887928: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-15 08:37:18.896761: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-15 08:37:19.172793: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5571b84ff310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-15 08:37:19.172840: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-07-15 08:37:19.172851: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-07-15 08:37:19.176927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:b1:00.0
2020-07-15 08:37:19.178260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 1 with properties:
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:b2:00.0
2020-07-15 08:37:19.179906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-15 08:37:19.182603: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-15 08:37:19.185263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-07-15 08:37:19.186520: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-07-15 08:37:19.195217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-07-15 08:37:19.198328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-07-15 08:37:19.204918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-15 08:37:19.210144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0, 1
2020-07-15 08:37:19.210189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-15 08:37:19.213025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-15 08:37:19.213047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 1
2020-07-15 08:37:19.213059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N Y
2020-07-15 08:37:19.213066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 1:   Y N
2020-07-15 08:37:19.216875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/device:GPU:0 with 30583 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:b1:00.0, compute capability: 7.0)
2020-07-15 08:37:19.218647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/device:GPU:1 with 30583 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0)
True

(3) The output of 
`python -m tensorpack.utils.nvml`
2020-07-15 08:38:31.622943: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
/apps/applications/PYTHON/3.7/lib/python3.7/runpy.py:125: RuntimeWarning: 'tensorpack.utils.nvml' found in sys.modules after import of package 'tensorpack.utils', but prior to execution of 'tensorpack.utils.nvml'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
0 Tesla V100-PCIE-32GB
1 Tesla V100-PCIE-32GB
[<__main__.NvidiaDevice object at 0x2b93695e7210>, <__main__.NvidiaDevice object at 0x2b93695e7390>]
{'gpu': 0, 'memory': 0}
*** Received signal 11 ***
*** BEGIN MANGLED STACK TRACE ***
Alarm clock

(0) I leave the whole logs as follows

2020-07-14 16:16:48.016299: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
[0714 16:16:50 @logger.py:128] WRN Log directory train_log/imagenet-resnet-d50-batch256 exists! Use 'd' to delete it.
[0714 16:16:50 @logger.py:131] WRN If you're resuming from a previous run, you can choose to keep it.
Press any other key to exit.
Select Action: k (keep) / d (delete) / q (quit):k
[0714 16:17:04 @logger.py:85] Existing log file 'train_log/imagenet-resnet-d50-batch256/log.log' backuped to 'train_log/imagenet-resnet-d50-batch256/log.log.0714-161704'
[0714 16:17:05 @logger.py:92] Argv: imagenet-resnet.py --gpu 0,1,2,3 --data /scratch/hpc15a01/dataset/imagenet
[0714 16:17:06 @imagenet-resnet.py:48] Running on 4 towers. Batch size per tower: 64
[0714 16:17:06 @fs.py:101] WRN Env var $TENSORPACK_DATASET not set, using /home01/hpc15a01/tensorpack_data for datasets.
[0714 16:17:15 @parallel.py:339] [MultiProcessRunnerZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[0714 16:17:15 @ilsvrc.py:128] [ILSVRC12] Assuming directory /scratch/hpc15a01/dataset/imagenet/val has 'train' structure.
[0714 16:17:24 @training.py:48] [DataParallel] Training a model of 4 towers.
[0714 16:17:24 @interface.py:41] Automatically applying StagingInput on the DataFlow.
[0714 16:17:24 @input_source.py:221] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
WARNING:tensorflow:From /home01/hpc15a01/.local/lib/python3.7/site-packages/tensorpack/graph_builder/training.py:385: The name tf.train.replica_device_setter is deprecated. Please use tf.compat.v1.train.replica_device_setter instead.

[0714 16:17:24 @training.py:110] Building graph for training tower 0 ...
[0714 16:17:24 @registry.py:90] 'conv0': [?, 3, 224, 224] --> [?, 64, 112, 112]
[0714 16:17:24 @registry.py:90] 'pool0': [?, 64, 112, 112] --> [?, 64, 56, 56]
WARNING:tensorflow:From /scratch/hpc15a01/src/tensorpack/examples/ResNetSrc/resnet_model.py:117: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

[0714 16:17:24 @registry.py:90] 'group0/block0/conv1': [?, 64, 56, 56] --> [?, 64, 56, 56]
[0714 16:17:24 @registry.py:90] 'group0/block0/conv2': [?, 64, 56, 56] --> [?, 64, 56, 56]
[0714 16:17:24 @registry.py:90] 'group0/block0/conv3': [?, 64, 56, 56] --> [?, 256, 56, 56]
[0714 16:17:24 @registry.py:90] 'group0/block0/convshortcut': [?, 64, 56, 56] --> [?, 256, 56, 56]
[0714 16:17:24 @registry.py:90] 'group0/block1/conv1': [?, 256, 56, 56] --> [?, 64, 56, 56]
[0714 16:17:24 @registry.py:90] 'group0/block1/conv2': [?, 64, 56, 56] --> [?, 64, 56, 56]
[0714 16:17:24 @registry.py:90] 'group0/block1/conv3': [?, 64, 56, 56] --> [?, 256, 56, 56]
[0714 16:17:24 @registry.py:90] 'group0/block2/conv1': [?, 256, 56, 56] --> [?, 64, 56, 56]
[0714 16:17:24 @registry.py:90] 'group0/block2/conv2': [?, 64, 56, 56] --> [?, 64, 56, 56]
[0714 16:17:24 @registry.py:90] 'group0/block2/conv3': [?, 64, 56, 56] --> [?, 256, 56, 56]
[0714 16:17:24 @registry.py:90] 'group1/block0/conv1': [?, 256, 56, 56] --> [?, 128, 56, 56]
[0714 16:17:24 @registry.py:90] 'group1/block0/conv2': [?, 128, 56, 56] --> [?, 128, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block0/conv3': [?, 128, 28, 28] --> [?, 512, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block0/convshortcut': [?, 256, 56, 56] --> [?, 512, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block1/conv1': [?, 512, 28, 28] --> [?, 128, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block1/conv2': [?, 128, 28, 28] --> [?, 128, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block1/conv3': [?, 128, 28, 28] --> [?, 512, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block2/conv1': [?, 512, 28, 28] --> [?, 128, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block2/conv2': [?, 128, 28, 28] --> [?, 128, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block2/conv3': [?, 128, 28, 28] --> [?, 512, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block3/conv1': [?, 512, 28, 28] --> [?, 128, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block3/conv2': [?, 128, 28, 28] --> [?, 128, 28, 28]
[0714 16:17:24 @registry.py:90] 'group1/block3/conv3': [?, 128, 28, 28] --> [?, 512, 28, 28]
[0714 16:17:24 @registry.py:90] 'group2/block0/conv1': [?, 512, 28, 28] --> [?, 256, 28, 28]
[0714 16:17:24 @registry.py:90] 'group2/block0/conv2': [?, 256, 28, 28] --> [?, 256, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block0/conv3': [?, 256, 14, 14] --> [?, 1024, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block0/convshortcut': [?, 512, 28, 28] --> [?, 1024, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block1/conv1': [?, 1024, 14, 14] --> [?, 256, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block1/conv2': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block1/conv3': [?, 256, 14, 14] --> [?, 1024, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block2/conv1': [?, 1024, 14, 14] --> [?, 256, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block2/conv2': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block2/conv3': [?, 256, 14, 14] --> [?, 1024, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block3/conv1': [?, 1024, 14, 14] --> [?, 256, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block3/conv2': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block3/conv3': [?, 256, 14, 14] --> [?, 1024, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block4/conv1': [?, 1024, 14, 14] --> [?, 256, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block4/conv2': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block4/conv3': [?, 256, 14, 14] --> [?, 1024, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block5/conv1': [?, 1024, 14, 14] --> [?, 256, 14, 14]
[0714 16:17:24 @registry.py:90] 'group2/block5/conv2': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0714 16:17:25 @registry.py:90] 'group2/block5/conv3': [?, 256, 14, 14] --> [?, 1024, 14, 14]
[0714 16:17:25 @registry.py:90] 'group3/block0/conv1': [?, 1024, 14, 14] --> [?, 512, 14, 14]
[0714 16:17:25 @registry.py:90] 'group3/block0/conv2': [?, 512, 14, 14] --> [?, 512, 7, 7]
[0714 16:17:25 @registry.py:90] 'group3/block0/conv3': [?, 512, 7, 7] --> [?, 2048, 7, 7]
[0714 16:17:25 @registry.py:90] 'group3/block0/convshortcut': [?, 1024, 14, 14] --> [?, 2048, 7, 7]
[0714 16:17:25 @registry.py:90] 'group3/block1/conv1': [?, 2048, 7, 7] --> [?, 512, 7, 7]
[0714 16:17:25 @registry.py:90] 'group3/block1/conv2': [?, 512, 7, 7] --> [?, 512, 7, 7]
[0714 16:17:25 @registry.py:90] 'group3/block1/conv3': [?, 512, 7, 7] --> [?, 2048, 7, 7]
[0714 16:17:25 @registry.py:90] 'group3/block2/conv1': [?, 2048, 7, 7] --> [?, 512, 7, 7]
[0714 16:17:25 @registry.py:90] 'group3/block2/conv2': [?, 512, 7, 7] --> [?, 512, 7, 7]
[0714 16:17:25 @registry.py:90] 'group3/block2/conv3': [?, 512, 7, 7] --> [?, 2048, 7, 7]
[0714 16:17:25 @registry.py:90] 'gap': [?, 2048, 7, 7] --> [?, 2048]
[0714 16:17:25 @registry.py:90] 'linear': [?, 2048] --> [?, 1000]
[0714 16:17:25 @regularize.py:97] regularize_cost() found 54 variables to regularize.
[0714 16:17:25 @regularize.py:21] The following tensors will be regularized: conv0/W:0, group0/block0/conv1/W:0, group0/block0/conv2/W:0, group0/block0/conv3/W:0, group0/block0/convshortcut/W:0, group0/block1/conv1/W:0, group0/block1/conv2/W:0, group0/block1/conv3/W:0, group0/block2/conv1/W:0, group0/block2/conv2/W:0, group0/block2/conv3/W:0, group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/conv2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group2/block5/conv3/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, linear/W:0
WARNING:tensorflow:From /scratch/hpc15a01/src/tensorpack/examples/ResNetSrc/imagenet_utils.py:381: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /scratch/hpc15a01/src/tensorpack/examples/ResNetSrc/imagenet_utils.py:382: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /scratch/hpc15a01/src/tensorpack/examples/ResNetSrc/imagenet_utils.py:383: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

WARNING:tensorflow:From /home01/hpc15a01/.local/lib/python3.7/site-packages/tensorpack/graph_builder/training.py:35: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

[0714 16:17:26 @training.py:110] Building graph for training tower 1 ...
[0714 16:17:26 @regularize.py:97] regularize_cost() found 54 variables to regularize.
[0714 16:17:27 @training.py:110] Building graph for training tower 2 ...
[0714 16:17:28 @regularize.py:97] regularize_cost() found 54 variables to regularize.
[0714 16:17:28 @training.py:110] Building graph for training tower 3 ...
[0714 16:17:29 @regularize.py:97] regularize_cost() found 54 variables to regularize.
[0714 16:17:31 @model_utils.py:67] List of Trainable Variables:
name                                 shape                 #elements
-----------------------------------  ------------------  -----------
conv0/W                              [7, 7, 3, 64]              9408
conv0/bn/gamma                       [64]                         64
conv0/bn/beta                        [64]                         64
group0/block0/conv1/W                [1, 1, 64, 64]             4096
group0/block0/conv1/bn/gamma         [64]                         64
group0/block0/conv1/bn/beta          [64]                         64
group0/block0/conv2/W                [3, 3, 64, 64]            36864
group0/block0/conv2/bn/gamma         [64]                         64
group0/block0/conv2/bn/beta          [64]                         64
group0/block0/conv3/W                [1, 1, 64, 256]           16384
group0/block0/conv3/bn/gamma         [256]                       256
group0/block0/conv3/bn/beta          [256]                       256
group0/block0/convshortcut/W         [1, 1, 64, 256]           16384
group0/block0/convshortcut/bn/gamma  [256]                       256
group0/block0/convshortcut/bn/beta   [256]                       256
group0/block1/conv1/W                [1, 1, 256, 64]           16384
group0/block1/conv1/bn/gamma         [64]                         64
group0/block1/conv1/bn/beta          [64]                         64
group0/block1/conv2/W                [3, 3, 64, 64]            36864
group0/block1/conv2/bn/gamma         [64]                         64
group0/block1/conv2/bn/beta          [64]                         64
group0/block1/conv3/W                [1, 1, 64, 256]           16384
group0/block1/conv3/bn/gamma         [256]                       256
group0/block1/conv3/bn/beta          [256]                       256
group0/block2/conv1/W                [1, 1, 256, 64]           16384
group0/block2/conv1/bn/gamma         [64]                         64
group0/block2/conv1/bn/beta          [64]                         64
group0/block2/conv2/W                [3, 3, 64, 64]            36864
group0/block2/conv2/bn/gamma         [64]                         64
group0/block2/conv2/bn/beta          [64]                         64
group0/block2/conv3/W                [1, 1, 64, 256]           16384
group0/block2/conv3/bn/gamma         [256]                       256
group0/block2/conv3/bn/beta          [256]                       256
group1/block0/conv1/W                [1, 1, 256, 128]          32768
group1/block0/conv1/bn/gamma         [128]                       128
group1/block0/conv1/bn/beta          [128]                       128
group1/block0/conv2/W                [3, 3, 128, 128]         147456
group1/block0/conv2/bn/gamma         [128]                       128
group1/block0/conv2/bn/beta          [128]                       128
group1/block0/conv3/W                [1, 1, 128, 512]          65536
group1/block0/conv3/bn/gamma         [512]                       512
group1/block0/conv3/bn/beta          [512]                       512
group1/block0/convshortcut/W         [1, 1, 256, 512]         131072
group1/block0/convshortcut/bn/gamma  [512]                       512
group1/block0/convshortcut/bn/beta   [512]                       512
group1/block1/conv1/W                [1, 1, 512, 128]          65536
group1/block1/conv1/bn/gamma         [128]                       128
group1/block1/conv1/bn/beta          [128]                       128
group1/block1/conv2/W                [3, 3, 128, 128]         147456
group1/block1/conv2/bn/gamma         [128]                       128
group1/block1/conv2/bn/beta          [128]                       128
group1/block1/conv3/W                [1, 1, 128, 512]          65536
group1/block1/conv3/bn/gamma         [512]                       512
group1/block1/conv3/bn/beta          [512]                       512
group1/block2/conv1/W                [1, 1, 512, 128]          65536
group1/block2/conv1/bn/gamma         [128]                       128
group1/block2/conv1/bn/beta          [128]                       128
group1/block2/conv2/W                [3, 3, 128, 128]         147456
group1/block2/conv2/bn/gamma         [128]                       128
group1/block2/conv2/bn/beta          [128]                       128
group1/block2/conv3/W                [1, 1, 128, 512]          65536
group1/block2/conv3/bn/gamma         [512]                       512
group1/block2/conv3/bn/beta          [512]                       512
group1/block3/conv1/W                [1, 1, 512, 128]          65536
group1/block3/conv1/bn/gamma         [128]                       128
group1/block3/conv1/bn/beta          [128]                       128
group1/block3/conv2/W                [3, 3, 128, 128]         147456
group1/block3/conv2/bn/gamma         [128]                       128
group1/block3/conv2/bn/beta          [128]                       128
group1/block3/conv3/W                [1, 1, 128, 512]          65536
group1/block3/conv3/bn/gamma         [512]                       512
group1/block3/conv3/bn/beta          [512]                       512
group2/block0/conv1/W                [1, 1, 512, 256]         131072
group2/block0/conv1/bn/gamma         [256]                       256
group2/block0/conv1/bn/beta          [256]                       256
group2/block0/conv2/W                [3, 3, 256, 256]         589824
group2/block0/conv2/bn/gamma         [256]                       256
group2/block0/conv2/bn/beta          [256]                       256
group2/block0/conv3/W                [1, 1, 256, 1024]        262144
group2/block0/conv3/bn/gamma         [1024]                     1024
group2/block0/conv3/bn/beta          [1024]                     1024
group2/block0/convshortcut/W         [1, 1, 512, 1024]        524288
group2/block0/convshortcut/bn/gamma  [1024]                     1024
group2/block0/convshortcut/bn/beta   [1024]                     1024
group2/block1/conv1/W                [1, 1, 1024, 256]        262144
group2/block1/conv1/bn/gamma         [256]                       256
group2/block1/conv1/bn/beta          [256]                       256
group2/block1/conv2/W                [3, 3, 256, 256]         589824
group2/block1/conv2/bn/gamma         [256]                       256
group2/block1/conv2/bn/beta          [256]                       256
group2/block1/conv3/W                [1, 1, 256, 1024]        262144
group2/block1/conv3/bn/gamma         [1024]                     1024
group2/block1/conv3/bn/beta          [1024]                     1024
group2/block2/conv1/W                [1, 1, 1024, 256]        262144
group2/block2/conv1/bn/gamma         [256]                       256
group2/block2/conv1/bn/beta          [256]                       256
group2/block2/conv2/W                [3, 3, 256, 256]         589824
group2/block2/conv2/bn/gamma         [256]                       256
group2/block2/conv2/bn/beta          [256]                       256
group2/block2/conv3/W                [1, 1, 256, 1024]        262144
group2/block2/conv3/bn/gamma         [1024]                     1024
group2/block2/conv3/bn/beta          [1024]                     1024
group2/block3/conv1/W                [1, 1, 1024, 256]        262144
group2/block3/conv1/bn/gamma         [256]                       256
group2/block3/conv1/bn/beta          [256]                       256
group2/block3/conv2/W                [3, 3, 256, 256]         589824
group2/block3/conv2/bn/gamma         [256]                       256
group2/block3/conv2/bn/beta          [256]                       256
group2/block3/conv3/W                [1, 1, 256, 1024]        262144
group2/block3/conv3/bn/gamma         [1024]                     1024
group2/block3/conv3/bn/beta          [1024]                     1024
group2/block4/conv1/W                [1, 1, 1024, 256]        262144
group2/block4/conv1/bn/gamma         [256]                       256
group2/block4/conv1/bn/beta          [256]                       256
group2/block4/conv2/W                [3, 3, 256, 256]         589824
group2/block4/conv2/bn/gamma         [256]                       256
group2/block4/conv2/bn/beta          [256]                       256
group2/block4/conv3/W                [1, 1, 256, 1024]        262144
group2/block4/conv3/bn/gamma         [1024]                     1024
group2/block4/conv3/bn/beta          [1024]                     1024
group2/block5/conv1/W                [1, 1, 1024, 256]        262144
group2/block5/conv1/bn/gamma         [256]                       256
group2/block5/conv1/bn/beta          [256]                       256
group2/block5/conv2/W                [3, 3, 256, 256]         589824
group2/block5/conv2/bn/gamma         [256]                       256
group2/block5/conv2/bn/beta          [256]                       256
group2/block5/conv3/W                [1, 1, 256, 1024]        262144
group2/block5/conv3/bn/gamma         [1024]                     1024
group2/block5/conv3/bn/beta          [1024]                     1024
group3/block0/conv1/W                [1, 1, 1024, 512]        524288
group3/block0/conv1/bn/gamma         [512]                       512
group3/block0/conv1/bn/beta          [512]                       512
group3/block0/conv2/W                [3, 3, 512, 512]        2359296
group3/block0/conv2/bn/gamma         [512]                       512
group3/block0/conv2/bn/beta          [512]                       512
group3/block0/conv3/W                [1, 1, 512, 2048]       1048576
group3/block0/conv3/bn/gamma         [2048]                     2048
group3/block0/conv3/bn/beta          [2048]                     2048
group3/block0/convshortcut/W         [1, 1, 1024, 2048]      2097152
group3/block0/convshortcut/bn/gamma  [2048]                     2048
group3/block0/convshortcut/bn/beta   [2048]                     2048
group3/block1/conv1/W                [1, 1, 2048, 512]       1048576
group3/block1/conv1/bn/gamma         [512]                       512
group3/block1/conv1/bn/beta          [512]                       512
group3/block1/conv2/W                [3, 3, 512, 512]        2359296
group3/block1/conv2/bn/gamma         [512]                       512
group3/block1/conv2/bn/beta          [512]                       512
group3/block1/conv3/W                [1, 1, 512, 2048]       1048576
group3/block1/conv3/bn/gamma         [2048]                     2048
group3/block1/conv3/bn/beta          [2048]                     2048
group3/block2/conv1/W                [1, 1, 2048, 512]       1048576
group3/block2/conv1/bn/gamma         [512]                       512
group3/block2/conv1/bn/beta          [512]                       512
group3/block2/conv2/W                [3, 3, 512, 512]        2359296
group3/block2/conv2/bn/gamma         [512]                       512
group3/block2/conv2/bn/beta          [512]                       512
group3/block2/conv3/W                [1, 1, 512, 2048]       1048576
group3/block2/conv3/bn/gamma         [2048]                     2048
group3/block2/conv3/bn/beta          [2048]                     2048
linear/W                             [2048, 1000]            2048000
linear/b                             [1000]                     1000
Number of trainable variables: 161
Number of parameters (elements): 25557032
Storage space needed for all trainable variables: 97.49MB
[0714 16:17:31 @base.py:207] Setup callbacks graph ...
[0714 16:17:31 @argtools.py:138] WRN Starting a process with 'fork' method is not safe and may consume unnecessary extra CPU memory. Use 'forkserver' or 'spawn' method (available after Py3.4) instead if you run into any issues. See https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods on how to set them.
[0714 16:17:31 @argtools.py:138] WRN ""import prctl"" failed! Install python-prctl so that processes can be cleaned with guarantee.
[0714 16:17:32 @input_source.py:221] Setting up the queue 'DataParallelInferenceRunner/QueueInput/input_queue' for CPU prefetching ...
[0714 16:17:32 @inference_runner.py:234] [InferenceRunner] Building tower 'InferenceTower0' on device /gpu:0 ...
[0714 16:17:32 @inference_runner.py:234] [InferenceRunner] Building tower 'InferenceTower1' on device /gpu:1 ...
[0714 16:17:33 @inference_runner.py:234] [InferenceRunner] Building tower 'InferenceTower2' on device /gpu:2 ...
[0714 16:17:34 @inference_runner.py:234] [InferenceRunner] Building tower 'InferenceTower3' on device /gpu:3 ...
[0714 16:17:35 @argtools.py:138] WRN ""import prctl"" failed! Install python-prctl so that processes can be cleaned with guarantee.
[0714 16:17:35 @summary.py:47] [MovingAverageSummary] 4 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.
[0714 16:17:35 @summary.py:94] Summarizing collection 'summaries' of size 7.
[0714 16:17:35 @graph.py:99] Applying collection UPDATE_OPS of 106 ops.
[0714 16:17:35 @base.py:228] Creating the session ...
2020-07-14 16:17:35.783116: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
Process Process-22:
Traceback (most recent call last):
  File ""/apps/applications/PYTHON/3.7/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap
    self.run()
  File ""/apps/applications/PYTHON/3.7/lib/python3.7/multiprocessing/process.py"", line 99, in run
    self._target(*self._args, **self._kwargs)
  File ""/home01/hpc15a01/.local/lib/python3.7/site-packages/tensorpack/callbacks/prof.py"", line 128, in worker
    devices = [ctx.device(i) for i in devices]
  File ""/home01/hpc15a01/.local/lib/python3.7/site-packages/tensorpack/callbacks/prof.py"", line 128, in <listcomp>
    devices = [ctx.device(i) for i in devices]
  File ""/home01/hpc15a01/.local/lib/python3.7/site-packages/tensorpack/utils/nvml.py"", line 203, in device
    ""nvmlDeviceGetHandleByIndex_v2"")(c_index, byref(device)))
  File ""/home01/hpc15a01/.local/lib/python3.7/site-packages/tensorpack/utils/nvml.py"", line 48, in _check_return
    raise NvmlException(ret)
tensorpack.utils.nvml.NvmlException: NVML_ERROR_INVALID_ARGUMENT
2020-07-14 16:17:35.830109: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-07-14 16:17:35.830381: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5634815fae90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-14 16:17:35.830427: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-14 16:17:35.837946: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-14 16:17:36.672781: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5634815fcd30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-14 16:17:36.672830: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-07-14 16:17:36.672845: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-07-14 16:17:36.676733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:b1:00.0
2020-07-14 16:17:36.678481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 1 with properties:
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:b2:00.0
2020-07-14 16:17:36.678555: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-07-14 16:17:36.681767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-14 16:17:36.684656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-07-14 16:17:36.695008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-07-14 16:17:36.698601: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-07-14 16:17:36.701458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-07-14 16:17:36.707879: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-14 16:17:36.714676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0, 1
2020-07-14 16:17:36.743805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-14 16:17:36.743842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 1
2020-07-14 16:17:36.743864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N Y
2020-07-14 16:17:36.743884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 1:   Y N
2020-07-14 16:17:36.749138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 32185 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:b1:00.0, compute capability: 7.0)
2020-07-14 16:17:36.761724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 32185 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0)
[0714 16:18:06 @base.py:234] Initializing the session ...
[0714 16:18:06 @base.py:241] Graph Finalized.
[0714 16:18:07 @concurrency.py:37] Starting EnqueueThread: enqueue dataflow to TF queue ""QueueInput/input_queue"" ...
[0714 16:18:09 @param.py:162] [HyperParamSetter] At global_step=0, learning_rate is set to 0.100000
[0714 16:18:12 @concurrency.py:37] Starting EnqueueThread: enqueue dataflow to TF queue ""DataParallelInferenceRunner/QueueInput/input_queue"" ...
[0714 16:18:12 @inference_runner.py:95] [InferenceRunner] Will eval 782 iterations
[0714 16:18:15 @base.py:273] Start Epoch 1 ...
  0%|                                                                                                          |0/5004[00:00<?,?it/s][0714 16:18:15 @input_source.py:599] Pre-filling StagingArea ...
[0714 16:18:21 @input_source.py:603] 1 element was put into StagingArea on each tower.
2020-07-14 16:18:53.720139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-07-14 16:18:54.866311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-07-14 16:19:01.032700: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2020-07-14 16:19:04.779038: I tensorflow/stream_executor/cuda/cuda_driver.cc:831] failed to allocate 15.43G (16567914752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory",output output python print binary use frequency service platform host guarantee used device host default version successfully dynamic library service platform guarantee used device compute capability device compute capability found device name major minor found device name major minor successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible successfully dynamic library device interconnect strength edge matrix device memory physical device name bus id compute capability device memory physical device name bus id compute capability true output python successfully dynamic library found import package prior execution may result unpredictable behaviour warn object object received signal begin stack trace alarm clock leave whole successfully dynamic library log directory use delete previous run choose keep press key exit select action keep delete quit log file data running batch size per tower set fork one time assuming directory structure training model automatically setting queue warning name please use instead building graph training tower warning name please use instead found regularize following warning name please use instead warning name please use instead warning name please use instead warning name please use instead building graph training tower found regularize building graph training tower found regularize building graph training tower found regularize list trainable name shape number trainable number storage space trainable setup graph starting process method safe may consume unnecessary extra memory use method available instead run see set import install guarantee setting queue building tower device building tower device building tower device building tower device import install guarantee collection run session collection size collection session binary use process recent call last file line file line run file line worker file line file line device device file line raise ret frequency service platform host guarantee used device host default version successfully dynamic library service platform guarantee used device compute capability device compute capability found device name major minor found device name major minor successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible device interconnect strength edge matrix device memory physical device name bus id compute capability device memory physical device name bus id compute capability session graph starting queue set starting queue start epoch element put tower successfully dynamic library successfully dynamic library found found driver perform compilation message logged allocate device memory,issue,positive,positive,neutral,neutral,positive,positive
658380399,"After setting `TRAIN.EVAL_PERIOD`, evaluation AP on test/val data will show in tensorboard during training.",setting evaluation data show training,issue,negative,neutral,neutral,neutral,neutral,neutral
658332738,@ppwwyyxx  This is not the answer. I want to see the evaluation in tensor board during training using test/val data.,answer want see evaluation tensor board training data,issue,negative,neutral,neutral,neutral,neutral,neutral
658269969,@ppwwyyxx How can I set it so I Will be able to view evaluation on test data? ,set able view evaluation test data,issue,negative,positive,positive,positive,positive,positive
658234527,"They are computed on training data, unless the metric contains the test dataset name.",training data unless metric test name,issue,negative,neutral,neutral,neutral,neutral,neutral
658069427,"It would also be helpful to provide the __entire__ output of:
```
nvidia-smi -L
```
and 
```
python -c 'import tensorflow as tf; print(tf.test.is_gpu_available())'
```
and
```
python -m tensorpack.utils.nvml
```",would also helpful provide output python print python,issue,negative,neutral,neutral,neutral,neutral,neutral
658063493,Please include the ENTIRE logs as the issue template suggests.,please include entire issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
657902348,"OK, I'll seek another part. 
After I found solution, I'll close. 
Thank you for your clue~",seek another part found solution close thank,issue,positive,neutral,neutral,neutral,neutral,neutral
657662361,"Please check the unmodified example to see if there are any problems with training or test output. If so, please provide details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md)).
If the accuracy is low only when using your custom op, it's unlikely a tensorpack issue and not something we can help with.",please check unmodified example see training test output please provide following issue template click new issue unexpected visit link post unexpected accuracy low custom unlikely issue something help,issue,positive,negative,neutral,neutral,negative,negative
656156557,"@ppwwyyxx  Of course there is not. But you know, Lets say that someone want to control the number of epochs.... its a good idea ;)

config.py:
```
_C.TRAIN.epoch_number = 250
```

train.py:

```
    traincfg = TrainConfig(
        model=MODEL,
        data=QueueInput(train_dataflow),
        callbacks=callbacks,
        steps_per_epoch=stepnum,
        max_epoch=cfg.TRAIN.epoch_number,
        session_init=session_init,
        starting_epoch=cfg.TRAIN.STARTING_EPOCH
    )
```
",course know say someone want control number good idea,issue,positive,positive,positive,positive,positive,positive
655673400,"There is no such thing as ""epoch_number"" in tensorpack's Faster R-CNN. So I can not reason about the config file.",thing faster reason file,issue,negative,neutral,neutral,neutral,neutral,neutral
655405239,"@ppwwyyxx Yes, I just noticed that this message in not in the logs... however it is in the console.  
There is still one more question. I have set 250 epochs, but it finished after 20. Why ?",yes message however console still one question set finished,issue,negative,neutral,neutral,neutral,neutral,neutral
655400488,"I saw no errors in the logs provided, so I'm not sure what you expect from this issue.

If you expect to not see the log in the title, the log is a warning printed by tensorflow and is not something we can control.",saw provided sure expect issue expect see log title log warning printed something control,issue,negative,positive,positive,positive,positive,positive
650594532,Tensorpack is a training interface. It produces models in standard TensorFlow format and how to deploy them with tf-serving is not relevant to tensorpack.,training interface standard format deploy relevant,issue,negative,positive,positive,positive,positive,positive
650547174,"@ppwwyyxx  I think that I have posted my question in the right place.Yes, it is related to tensorpack since I am using model trained using tensorpack. 

Here is the metagraph I have:
```
MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['image:0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, -1, 3)
        name: image:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output/boxes:0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 4)
        name: output/boxes:0
    outputs['output/labels:0'] tensor_info:
        dtype: DT_INT64
        shape: (-1)
        name: output/labels:0
    outputs['output/masks:0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 28, 28)
        name: output/masks:0
    outputs['output/scores:0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1)
        name: output/scores:0
  Method name is: tensorflow/serving/predict
```

As you can see the input is:         dtype: DT_FLOAT
and here is the data I am sending to the model during inference:

```
np.array(Image.open(im_path)).tolist()
```

What I want, it to be able to send base64 encoded image ( base64.b64encode('image') )
And the question is, is it possible ? DO I have to change something in config ? or in the model architecture ? 

And by the way, sending the data in the form of list of floats to the model to do the inference its very inefficient .",think posted question right related since model trained following given following input shape name image given following output shape name shape name shape name shape name method name see input data sending model inference want able send base image question possible change something model architecture way sending data form list model inference inefficient,issue,negative,negative,neutral,neutral,negative,negative
650528020,The Mask R-CNN model implemented in tensorpack examples can accept any size that's not too small. What size is the best is a machine learning problem and not a valid tensorpack issue.,mask model accept size small size best machine learning problem valid issue,issue,positive,positive,positive,positive,positive,positive
650527898,"The question does not seem related to this project as ""tensorpack"" doesn't even appear in the question. 
You might want to ask in other communities such as tensorflow-serving.",question seem related project even appear question might want ask,issue,negative,neutral,neutral,neutral,neutral,neutral
650310102,"What model is ""the model""?

> are the images scaled down when the are pushed to the model ?

depends on how a model is used

A detection model can accept any size that's not too small. What size is the best is a machine learning problem and not a valid tensorpack issue.",model model scaled model model used detection model accept size small size best machine learning problem valid issue,issue,positive,positive,positive,positive,positive,positive
650068227,"Hi @silverlining21, I have a bug, I using tensorpack, my model stop training when finished first epoch, Don't continue epoch 2. Do you have idea for fix this bug ",hi bug model stop training finished first epoch continue epoch idea fix bug,issue,negative,positive,positive,positive,positive,positive
649075683,"Thanks for reporting. I have not had a chance to use cuda 11, but some things that can be used to root cause it:
1. compare the two settings using 1 machine and eventually 1GPU. This may tell whether this is a scaling issue with horovod.
2. If cuda 11 is slower even in 1 GPU setting, it's possible that there is a regression in some ops Mask R-CNN uses. What I would do is to bisect the model (e.g., cut the later half of the model and return a naive loss directly) to find ops that behave differently in the two settings. TensorFlow's profiler (https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.GraphProfiler) may help as well but I didn't have a good user experience with it in the past

maybe related: https://github.com/tensorpack/benchmarks/tree/master/ResNet-MultiGPU can be used to check whether there is a regression in a plain ResNet.",thanks chance use used root cause compare two machine eventually may tell whether scaling issue even setting possible regression mask would bisect model cut later half model return naive loss directly find behave differently two profiler may help well good user experience past maybe related used check whether regression plain,issue,positive,positive,neutral,neutral,positive,positive
647049937,"Hi @ppwwyyxx ,

I wanted to also share that I got the same error as above when I simply removed the `imgaug.Flip()` statement and used only the `CustomResize()` - this resulted in my network crashing randomly between 10-30 epochs with or without warning. I am also already using tensorflow==1.14 and have tried upgrading to 1.15 but that did not resolve the issue.

Do you have any intuition for why this might be happening and how I can fix this without setting the imgaug flip operation to an extremely low probability (as suggested by underchemist as a workaround)?

This error does not occur when the flip operation is still part of the `self.aug` statement.",hi also share got error simply removed statement used network randomly without warning also already tried resolve issue intuition might happening fix without setting flip operation extremely low probability error occur flip operation still part statement,issue,negative,negative,negative,negative,negative,negative
645375838,"> @vijaygill How does your metagraf def looks like ?

@Adblu Sorry I did not notice that you had different signature_def in your message, and noticed only when I pasted mine as below. It seems your code should work with the signature_name you have.

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['prediction_pipeline']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['image:0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, -1, 3)
        name: image:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output/boxes:0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 4)
        name: output/boxes:0
    outputs['output/labels:0'] tensor_info:
        dtype: DT_INT64
        shape: (-1)
        name: output/labels:0
    outputs['output/masks:0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 28, 28)
        name: output/masks:0
    outputs['output/scores:0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1)
        name: output/scores:0
  Method name is: tensorflow/serving/predict

",like sorry notice different message pasted mine code work following given following input shape name image given following output shape name shape name shape name shape name method name,issue,negative,negative,neutral,neutral,negative,negative
645331911,"@vijaygill Weird. I have following request object:

```
data = json.dumps({""signature_name"": ""serving_default"", ""instances"": x.tolist()})
headers = {""content-type"": ""application/json""}
``` 

and it works when I launch the server on ec2:

```
sudo docker run -p 8501:8501 --mount type=bind,source=/a/model,target=/models/a -e MODEL_NAME=a --gpus all -t tensorflow/serving:2.0.0-gpu --model_config_file=/home/ubuntu/car/serving/a/model/models.config/
```

But on sagemaker I still have the problem. ",weird following request object data work launch server docker run mount still problem,issue,negative,negative,negative,negative,negative,negative
645042520,"@Adblu In my following python code snippet, I use ""prediction_pipeline"" for signature_name and code works. 

```
    data = {
            ""signature_name"": ""prediction_pipeline"",
            ""inputs"": image
            }

    r = requests.post(url = ""http://192.168.0.94:8501/v1/models/gillsoft:predict"", json = data)

```",following python code snippet use code work data image data,issue,negative,neutral,neutral,neutral,neutral,neutral
644301121,"For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md)). Now I cannot see how exactly this question is related to this project.",anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected see exactly question related project,issue,positive,positive,positive,positive,positive,positive
641693251,"I found that the model I used was COCO-MaskRCNN-R101FPN9xGNCasAugScratch.npz, but the parameter setting _C.MODE_MASK = False. Is it because I used the wrong model? But I found that the COCO* models in http://models.tensorpack.com/ are all maskrcnn, so should I use the pre-trained model of ImageNet*?",found model used parameter setting false used wrong model found coco use model,issue,negative,negative,negative,negative,negative,negative
641692425,The dataset might be in a wrong format or require different parameters to train. This is not something we can help with. ,might wrong format require different train something help,issue,negative,negative,negative,negative,negative,negative
641130931,We do not help with parameter tuning for different tasks.,help parameter tuning different,issue,negative,neutral,neutral,neutral,neutral,neutral
639371812,"The command tried to load a ResNet101 dict to a ResNet 50 model, so some variables in the dict are not in the graph.
The command tried to load a imagenet-pretrained backbone model to a detection model, so some variables detection-specific variables are not in the dict.",command tried load model graph command tried load backbone model detection model,issue,negative,neutral,neutral,neutral,neutral,neutral
639253783,"btw , How set Epoch number? I check config.py and don't found。",set epoch number check,issue,negative,neutral,neutral,neutral,neutral,neutral
639252343,"Ok, I understand, train label is not coco format, its x1,y1,x2,y2 format. 
I change , it work, thank you",understand train label coco format format change work thank,issue,negative,neutral,neutral,neutral,neutral,neutral
639214985,"Since these are your modified version of code and I don't know how they are being used I can't comment much on whether they are correct or not.

Though at least I can see it seems your ""boxes2"" is not in the correct format described above.",since version code know used ca comment much whether correct though least see correct format,issue,negative,negative,neutral,neutral,negative,negative
639213164,"this code is  reference sample source and change. 

def get_boxes(file_name):
    label_full_name=os.path.join(""../train_data/labels_xml"",file_name+"".xml"")
    tree = ET.parse(label_full_name)
    all_boxes1=[]
    all_boxes2=[]
    for obj in tree.findall(""object""):
        bbox = obj.find(""bndbox"")
        xmin=float(bbox.find(""xmin"").text)
        ymin=float(bbox.find(""ymin"").text)
        xmax=float(bbox.find(""xmax"").text)
        ymax=float(bbox.find(""ymax"").text)
        xmin=xmin-1
        ymin=ymin-1
        o_width=abs(xmax-xmin)
        o_height=abs(ymax-ymin)
        all_boxes1.append([xmin,ymin,xmax,ymax])
        all_boxes2.append([xmin,ymin,o_width,o_height]) 
    boxes1=np.asarray(all_boxes1, dtype='float32')
    boxes2=np.asarray(all_boxes2, dtype='float32')
    return boxes1,boxes2

 aug = imgaug.AugmentorList([
            CustomResize([600,1200], 1333),
            imgaug.Flip(horiz=True,vert=False)
        ])
    file_name=""2c543fde74e811ea834cf834418b57e9""
    boxes1,boxes2=get_boxes(file_name)
    boxes1_area=area(boxes1) 
    print(boxes1_area)
    image_full_name=os.path.join(""../train_data/images"",file_name+"".jpg"")
    im = cv2.imread(image_full_name, cv2.IMREAD_COLOR)
    cv2.imwrite(""../test_data/2_1.jpg"",im)
    print(np.shape(im))
    tfms = aug.get_transform(im)
    im = tfms.apply_image(im)
    cv2.imwrite(""../test_data/2_2.jpg"",im)
    print(np.shape(im))
    points = box_to_point4(boxes2)
    points = tfms.apply_coords(points)
    boxes = point4_to_box(points)
    if len(boxes):
        boxes_area=area(boxes) 
        np_1=np.min(boxes_area)
        if np_1<=0:
            print(""is 0"")
            print(boxes_area)",code reference sample source change tree object return print print print print print,issue,negative,neutral,neutral,neutral,neutral,neutral
639205587,Can you explain what is zero and why do you think they are zero? Showing code is preferred over words.,explain zero think zero showing code preferred,issue,negative,neutral,neutral,neutral,neutral,neutral
639204754,"I check source and find 
self.aug = imgaug.AugmentorList([
            CustomResize(cfg.PREPROC.TRAIN_SHORT_EDGE_SIZE, cfg.PREPROC.MAX_SIZE),
            imgaug.Flip(horiz=True)
        ])
intend Flip source image and boxes, but it generate mistake boxes , it's area is 0.

[ 8722.98    8601.601   9726.057  10810.9375 12193.718  10905.922
 12472.299  11821.677  11750.395  17476.857  15899.3955 14148.084
 12815.163  13658.579   6031.53    5327.6895  9240.584   8773.35
 10922.114  13621.998  12751.847  15228.601  20511.127  22191.15
 31911.002  39885.31   51963.99   25655.7    50242.996 ]
(781, 938, 3)
(774, 930, 3)
is 0
[ 4014.742  18734.04   20794.148  25247.223  29158.096  39422.543
 45802.188  54128.867  60609.125  64086.535  77004.4    81250.375
 83103.64   82948.17   34606.242  36877.883  32779.734  36463.668
 39309.723  37651.934  31318.992  34056.21   21260.861  10262.306
  3247.0842 16384.432  36883.004      0.     19903.838 ]",check source find intend flip source image generate mistake area,issue,negative,neutral,neutral,neutral,neutral,neutral
639000986,"`TrainingDataPreprocessor` takes boxes from your original dataset directly. So you'll want to check whether your dataset follows the format https://github.com/tensorpack/tensorpack/blob/b43488746b5b794d36a17f63e6da322f289366bf/examples/FasterRCNN/dataset/dataset.py#L15-L41 correctly.
Note that coordinates by default are not in [0, 1].",original directly want check whether format correctly note default,issue,negative,positive,positive,positive,positive,positive
637935985,"@ppwwyyxx Thank you, I have found the answer in other issues. Set ignore_mismatch=True",thank found answer set,issue,negative,neutral,neutral,neutral,neutral,neutral
633364900,"Understood. Compiled codes can affect other packages. Thank you for your maintenance activity. I can't move to tf.data in tf 2.1, having a lot of unnecessary details.",understood affect thank maintenance activity ca move lot unnecessary,issue,negative,negative,negative,negative,negative,negative
633357255,"‘fork’ can cause potential problems when combined with anything that uses thread. `import sklearn` can affect the way numpy uses threads, depending on how sklearn/numpy are compiled.",fork cause potential combined anything thread import affect way depending,issue,negative,neutral,neutral,neutral,neutral,neutral
633353027,"I appreciate your reply. I can confirm that `multiprocessing.set_start_method('fork')` be the solution for this. (with slight changes of the code)
However, is there any reason related to sklearn , not to numpy?
I'm wandering why commenting out `import sklearn` makes all fine, even though multiprocessing.get_start_method() returns 'fork' in `tensorpack.utils.concurrency.py:start_proc_mask_signal()`. In my opinion, the problem would be from another point, not the combination of 'fork' and numpy.",appreciate reply confirm solution slight code however reason related wandering import fine even though opinion problem would another point combination,issue,negative,positive,neutral,neutral,positive,positive
632863573,"Closing as there is no repro.
As the issue already mentioned,  the use of `fork` could cause this and using ""spawn"" or ""forkserver"" https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods is more robust.

Related: #1447",issue already use fork could cause spawn robust related,issue,negative,neutral,neutral,neutral,neutral,neutral
632847569,"Well it can be caused by either the `r = np.random.normal(size=(100, 100))` line due to locks held by numpy https://github.com/numpy/numpy/issues/9248 or `rr = r @ r` due to locks held by MKL. Solution is the same, though.",well either line due due solution though,issue,positive,negative,negative,negative,negative,negative
632837539,Root cause is https://github.com/numpy/numpy/issues/9248. Easy workaround is to use `spawn` or `forkserver`. In fact tensorpack already prints a warning that asks for this.,root cause easy use spawn fact already warning,issue,negative,positive,positive,positive,positive,positive
632825248,This issue is caused by Keras's design difference with variable scope and cannot be fixed.,issue design difference variable scope fixed,issue,negative,positive,neutral,neutral,positive,positive
632722785,"Any update on this matter? The above seems to be true also for Tensorpack v0.9.9 with TensorFlow v1.14.
Sorry if this is not helpful.",update matter true also sorry helpful,issue,positive,negative,neutral,neutral,negative,negative
630977033,"> does tensorpack support Hard Example Mining ?

Tensorpack is a training interface and does not implement these training techniques. The detection example do not support hard example mining.

> what are other techniques to achieve lower value of false positives ?

We only discuss issues related to tensorpack at github issues.",support hard example mining training interface implement training detection example support hard example mining achieve lower value false discus related,issue,positive,negative,negative,negative,negative,negative
630650087,"I still do not know what you did.

If the `predict.py` you're using is the same as https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/predict.py, then it tells nothing about the speed of inference. To tell the speed you need to run inference in a loop. `predict.py --benchmark` does something closer to that.",still know nothing speed inference tell speed need run inference loop something closer,issue,negative,neutral,neutral,neutral,neutral,neutral
630644943,"Here is the all log information:

```
 python predict.py --predict /home/ubuntu/car2/test_images/c.jpg --load /home/ubuntu/car2/log_ex_3/model-12450.index --config DATA.BASEDIR=~/car2/balloon MODE_FPN=True ""DATA.VAL=('balloon_val',)""  ""DATA.TRAIN=('balloon_train',)"" TRAIN.BASE_LR=1e-3 TRAIN.EVAL_PERIOD=0 ""TRAIN.LR_SCHEDULE=[1000]"" ""PREPROC.TRAIN_SHORT_EDGE_SIZE=[600,1200]"" TRAIN.CHECKPOINT_PERIOD=1 DATA.NUM_WORKERS=1
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/ubuntu/anaconda3/envs/car2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-05-19 07:05:11.878565: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2020-05-19 07:05:32.707287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.708836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:17.0
2020-05-19 07:05:32.708924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.710398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:18.0
2020-05-19 07:05:32.710474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.711940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:19.0
2020-05-19 07:05:32.712014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.713498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1a.0
2020-05-19 07:05:32.713572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.715038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1b.0
2020-05-19 07:05:32.715110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.716575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1c.0
2020-05-19 07:05:32.716671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.718140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1d.0
2020-05-19 07:05:32.718212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.719676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2020-05-19 07:05:32.719894: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-05-19 07:05:32.721439: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-05-19 07:05:32.722639: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-05-19 07:05:32.722922: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-05-19 07:05:32.724554: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-05-19 07:05:32.725810: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-05-19 07:05:32.729637: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-05-19 07:05:32.729742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.731283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.732823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.734348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.735892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.737461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.739010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.740529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.742059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.743577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.745122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.746646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.748177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.749703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.751211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.752735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:32.754198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
[0519 07:05:32 @config.py:322] Config: ------------------------------------------
{'BACKBONE': {'FREEZE_AFFINE': False,
              'FREEZE_AT': 2,
              'NORM': 'FreezeBN',
              'RESNET_NUM_BLOCKS': [3, 4, 23, 3],
              'STRIDE_1X1': False,
              'TF_PAD_MODE': False,
              'WEIGHTS': ''},
 'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0],
                                  [30.0, 30.0, 15.0, 15.0]],
             'IOUS': [0.5, 0.6, 0.7]},
 'DATA': {'ABSOLUTE_COORD': True,
          'BASEDIR': '~/car2/balloon',
          'CLASS_NAMES': ['BG', 'damage'],
          'FILTER_EMPTY_ANNOTATIONS': True,
          'NUM_CATEGORY': 1,
          'NUM_WORKERS': 1,
          'TRAIN': ('balloon_train',),
          'VAL': ('balloon_val',)},
 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),
         'CASCADE': False,
         'FRCNN_CONV_HEAD_DIM': 256,
         'FRCNN_FC_HEAD_DIM': 1024,
         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',
         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',
         'NORM': 'None',
         'NUM_CHANNEL': 256,
         'PROPOSAL_MODE': 'Level',
         'RESOLUTION_REQUIREMENT': 32},
 'FRCNN': {'BATCH_PER_IM': 512,
           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],
           'FG_RATIO': 0.25,
           'FG_THRESH': 0.5},
 'MODE_FPN': True,
 'MODE_MASK': True,
 'MRCNN': {'ACCURATE_PASTE': True, 'HEAD_DIM': 256},
 'PREPROC': {'MAX_SIZE': 1344.0,
             'PIXEL_MEAN': [123.675, 116.28, 103.53],
             'PIXEL_STD': [58.395, 57.12, 57.375],
             'TEST_SHORT_EDGE_SIZE': 800,
             'TRAIN_SHORT_EDGE_SIZE': [600, 1200]},
 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),
         'ANCHOR_SIZES': (32, 64, 128, 256, 512),
         'ANCHOR_STRIDE': 16,
         'BATCH_PER_IM': 256,
         'CROWD_OVERLAP_THRESH': 9.99,
         'FG_RATIO': 0.5,
         'HEAD_DIM': 1024,
         'MIN_SIZE': 0,
         'NEGATIVE_ANCHOR_THRESH': 0.3,
         'NUM_ANCHOR': 15,
         'POSITIVE_ANCHOR_THRESH': 0.7,
         'PROPOSAL_NMS_THRESH': 0.7,
         'TEST_PER_LEVEL_NMS_TOPK': 1000,
         'TEST_POST_NMS_TOPK': 1000,
         'TEST_PRE_NMS_TOPK': 6000,
         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,
         'TRAIN_POST_NMS_TOPK': 2000,
         'TRAIN_PRE_NMS_TOPK': 12000},
 'TEST': {'FRCNN_NMS_THRESH': 0.5,
          'RESULTS_PER_IM': 100,
          'RESULT_SCORE_THRESH': 0.05,
          'RESULT_SCORE_THRESH_VIS': 0.5},
 'TRAIN': {'BASE_LR': 0.001,
           'CHECKPOINT_PERIOD': 1,
           'EVAL_PERIOD': 0,
           'LR_SCHEDULE': '[1000]',
           'NUM_GPUS': 8,
           'STARTING_EPOCH': 1,
           'STEPS_PER_EPOCH': 50,
           'WARMUP': 1000,
           'WARMUP_INIT_LR': 0.0,
           'WEIGHT_DECAY': 0.0001,
           'epoch_number': 250},
 'TRAINER': 'replicated'}
[0519 07:05:32 @varmanip.py:211] Checkpoint path /home/ubuntu/car2/log_ex_3/model-12450.index is auto-corrected to /home/ubuntu/car2/log_ex_3/model-12450.
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70198ff2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70198ff2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019822b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019822b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:33 @registry.py:90] 'conv0': [1, 3, ?, ?] --> [1, 64, ?, ?]
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f70198f2c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f70198f2c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:33 @registry.py:90] 'pool0': [1, 64, ?, ?] --> [1, 64, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70197a6a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70197a6a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019753c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019753c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:33 @registry.py:90] 'group0/block0/conv1': [1, 64, ?, ?] --> [1, 64, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70197a6668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70197a6668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70198644a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70198644a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:33 @registry.py:90] 'group0/block0/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019864390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019864390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019753320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019753320>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:33 @registry.py:90] 'group0/block0/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70197a60b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70197a60b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019894f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019894f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:33 @registry.py:90] 'group0/block0/convshortcut': [1, 64, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70196b32b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70196b32b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701967ba58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701967ba58>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:33 @registry.py:90] 'group0/block1/conv1': [1, 256, ?, ?] --> [1, 64, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70196c0908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70196c0908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701973e390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701973e390>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:33 @registry.py:90] 'group0/block1/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701973e828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701973e828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019567e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019567e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:33 @registry.py:90] 'group0/block1/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701956ca58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701956ca58>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701956cf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701956cf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:33 @registry.py:90] 'group0/block2/conv1': [1, 256, ?, ?] --> [1, 64, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701956c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701956c710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701973ed30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701973ed30>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:34 @registry.py:90] 'group0/block2/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70198a4a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70198a4a20>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701956cd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701956cd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:34 @registry.py:90] 'group0/block2/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019577e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019577e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701941e358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701941e358>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:34 @registry.py:90] 'group1/block0/conv1': [1, 256, ?, ?] --> [1, 128, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019482128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019482128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701953d518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701953d518>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:34 @registry.py:90] 'group1/block0/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019413e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019413e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70193672e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70193672e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:34 @registry.py:90] 'group1/block0/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019367048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019367048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70193cd518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70193cd518>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:34 @registry.py:90] 'group1/block0/convshortcut': [1, 256, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70193cd390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70193cd390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70192900b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70192900b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:34 @registry.py:90] 'group1/block1/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019837358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019837358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701956ce80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701956ce80>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:34 @registry.py:90] 'group1/block1/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019295438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019295438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701924e9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701924e9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:34 @registry.py:90] 'group1/block1/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70196a9978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70196a9978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70192c2860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70192c2860>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:35 @registry.py:90] 'group1/block2/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70196a9eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70196a9eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70191312e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70191312e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:35 @registry.py:90] 'group1/block2/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019131160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019131160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70192c2320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70192c2320>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:35 @registry.py:90] 'group1/block2/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70192c2b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70192c2b70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70190c1c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70190c1c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:35 @registry.py:90] 'group1/block3/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190cada0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190cada0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701914a0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701914a0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:35 @registry.py:90] 'group1/block3/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701920efd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701920efd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018ff2dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018ff2dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:35 @registry.py:90] 'group1/block3/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190d2f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190d2f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018f9af60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018f9af60>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:35 @registry.py:90] 'group2/block0/conv1': [1, 512, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018f9acc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018f9acc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018f4e278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018f4e278>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:35 @registry.py:90] 'group2/block0/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019000278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019000278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018ea0438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018ea0438>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:35 @registry.py:90] 'group2/block0/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018f236a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018f236a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e7e2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e7e2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:36 @registry.py:90] 'group2/block0/convshortcut': [1, 512, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018e24048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018e24048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019131b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019131b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:36 @registry.py:90] 'group2/block1/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018e28358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018e28358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018d66eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018d66eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:36 @registry.py:90] 'group2/block1/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190d2ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190d2ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70190c1cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70190c1cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:36 @registry.py:90] 'group2/block1/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701905a4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701905a4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e0b9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e0b9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:36 @registry.py:90] 'group2/block2/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019292358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019292358>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701935ccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701935ccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:36 @registry.py:90] 'group2/block2/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018ff2630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018ff2630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701936f828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701936f828>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:36 @registry.py:90] 'group2/block2/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701935c780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701935c780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70192b93c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70192b93c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:36 @registry.py:90] 'group2/block3/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190059b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190059b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019837d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019837d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:37 @registry.py:90] 'group2/block3/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019413400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7019413400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70193cdef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70193cdef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:37 @registry.py:90] 'group2/block3/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70193cdeb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70193cdeb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70193cd9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70193cd9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:37 @registry.py:90] 'group2/block4/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701973e748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701973e748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019691e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019691e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:37 @registry.py:90] 'group2/block4/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70196c0240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70196c0240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701914aef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701914aef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:37 @registry.py:90] 'group2/block4/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018f49400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018f49400>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e2d978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e2d978>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:37 @registry.py:90] 'group2/block5/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70193ec080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70193ec080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701935c710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701935c710>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:37 @registry.py:90] 'group2/block5/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190ca5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190ca5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018aa0400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018aa0400>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:37 @registry.py:90] 'group2/block5/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701914af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701914af28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701988c160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701988c160>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:37 @registry.py:90] 'group2/block6/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701914a048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701914a048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e28c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e28c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:38 @registry.py:90] 'group2/block6/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018feae48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018feae48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70189fa198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70189fa198>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:38 @registry.py:90] 'group2/block6/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018960780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018960780>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018aa01d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018aa01d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:38 @registry.py:90] 'group2/block7/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018e28630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018e28630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018aa0e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018aa0e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:38 @registry.py:90] 'group2/block7/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018ad54a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018ad54a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701895bc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701895bc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:38 @registry.py:90] 'group2/block7/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018a754a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018a754a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018a756a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018a756a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:38 @registry.py:90] 'group2/block8/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70188a9d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70188a9d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018aa0198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018aa0198>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:38 @registry.py:90] 'group2/block8/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018859ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018859ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019350e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7019350e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:38 @registry.py:90] 'group2/block8/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70187c52b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70187c52b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701895b780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701895b780>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:39 @registry.py:90] 'group2/block9/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190c9e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70190c9e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e242b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e242b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:39 @registry.py:90] 'group2/block9/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70187f8390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70187f8390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018727978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018727978>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:39 @registry.py:90] 'group2/block9/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018727748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018727748>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701885c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701885c390>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:39 @registry.py:90] 'group2/block10/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70187226a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70187226a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701868f9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f701868f9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:39 @registry.py:90] 'group2/block10/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018727518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018727518>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e0b0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018e0b0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:39 @registry.py:90] 'group2/block10/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018620208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018620208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018727978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018727978>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:39 @registry.py:90] 'group2/block11/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018671f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018671f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018525160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018525160>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:39 @registry.py:90] 'group2/block11/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70184e5f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70184e5f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018693400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018693400>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:39 @registry.py:90] 'group2/block11/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018466208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018466208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70184cb160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70184cb160>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:40 @registry.py:90] 'group2/block12/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018e28438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018e28438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70184c47f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70184c47f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:40 @registry.py:90] 'group2/block12/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018693128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018693128>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70184c4710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70184c4710>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:40 @registry.py:90] 'group2/block12/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018693198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018693198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018318780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018318780>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:40 @registry.py:90] 'group2/block13/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70183e94a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70183e94a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70185229b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70185229b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:40 @registry.py:90] 'group2/block13/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70188bed30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70188bed30>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70182fb5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70182fb5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:40 @registry.py:90] 'group2/block13/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70182fb630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70182fb630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70182b5ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70182b5ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:40 @registry.py:90] 'group2/block14/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70183a3b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70183a3b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70181deb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70181deb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:40 @registry.py:90] 'group2/block14/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70183a3b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70183a3b38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70181dedd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70181dedd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:40 @registry.py:90] 'group2/block14/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018310c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f7018310c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018184ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018184ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:41 @registry.py:90] 'group2/block15/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70181de320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70181de320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018318470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7018318470>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:41 @registry.py:90] 'group2/block15/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70182fb630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70182fb630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9aada20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9aada20>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:41 @registry.py:90] 'group2/block15/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701818b4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f701818b4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9aad7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9aad7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:41 @registry.py:90] 'group2/block16/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70182b2240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f70182b2240>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb99dae48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb99dae48>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:41 @registry.py:90] 'group2/block16/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9aad710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9aad710>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9ab50f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9ab50f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:41 @registry.py:90] 'group2/block16/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9aed6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9aed6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70181de5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f70181de5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:41 @registry.py:90] 'group2/block17/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9ab5f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9ab5f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9aed780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9aed780>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:41 @registry.py:90] 'group2/block17/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9aedd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9aedd68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb98a2780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb98a2780>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:42 @registry.py:90] 'group2/block17/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9ab5fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9ab5fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb98a2eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb98a2eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:42 @registry.py:90] 'group2/block18/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb98a2630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb98a2630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb98e6f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb98e6f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:42 @registry.py:90] 'group2/block18/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9aad438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9aad438>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9930828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9930828>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:42 @registry.py:90] 'group2/block18/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb99e3b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb99e3b00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9773ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9773ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:42 @registry.py:90] 'group2/block19/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9946c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9946c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb997f7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb997f7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:42 @registry.py:90] 'group2/block19/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9946278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9946278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb97a8550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb97a8550>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:42 @registry.py:90] 'group2/block19/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb97a8668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb97a8668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb96603c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb96603c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:42 @registry.py:90] 'group2/block20/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb97a8fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb97a8fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb997f2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb997f2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:42 @registry.py:90] 'group2/block20/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9729ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9729ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9a8d7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9a8d7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:43 @registry.py:90] 'group2/block20/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb96dbc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb96dbc88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb964c2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb964c2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:43 @registry.py:90] 'group2/block21/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb988a908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb988a908>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb964c470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb964c470>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:43 @registry.py:90] 'group2/block21/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb99db4e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb99db4e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9521cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9521cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:43 @registry.py:90] 'group2/block21/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9885278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9885278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb95329b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb95329b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:43 @registry.py:90] 'group2/block22/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9458fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9458fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb950ea58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb950ea58>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:43 @registry.py:90] 'group2/block22/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb964c2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb964c2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb94410b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb94410b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:43 @registry.py:90] 'group2/block22/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb950e2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb950e2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb935f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb935f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:43 @registry.py:90] 'group3/block0/conv1': [1, 1024, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb935fb00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb935fb00>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb93fc6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb93fc6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:43 @registry.py:90] 'group3/block0/conv2': [1, 512, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb988a278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb988a278>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9328e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb9328e48>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:44 @registry.py:90] 'group3/block0/conv3': [1, 512, ?, ?] --> [1, 2048, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9479208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb9479208>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb950ee48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb950ee48>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:44 @registry.py:90] 'group3/block0/convshortcut': [1, 1024, ?, ?] --> [1, 2048, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb93fc470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb93fc470>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb830ab70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb830ab70>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:44 @registry.py:90] 'group3/block1/conv1': [1, 2048, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb935f320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb935f320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb82367f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb82367f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:44 @registry.py:90] 'group3/block1/conv2': [1, 512, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb94791d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb94791d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb8152e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb8152e80>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:44 @registry.py:90] 'group3/block1/conv3': [1, 512, ?, ?] --> [1, 2048, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb8241ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb8241ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb815a4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb815a4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:44 @registry.py:90] 'group3/block2/conv1': [1, 2048, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb82415f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb82415f8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb80ff2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb80ff2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:44 @registry.py:90] 'group3/block2/conv2': [1, 512, ?, ?] --> [1, 512, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb8236da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb8236da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb8241668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f6fb8241668>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:44 @registry.py:90] 'group3/block2/conv3': [1, 512, ?, ?] --> [1, 2048, ?, ?]
[0519 07:05:44 @registry.py:80] 'fpn' input: [1, 256, ?, ?], [1, 512, ?, ?], [1, 1024, ?, ?], [1, 2048, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb80a6da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb80a6da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:44 @registry.py:90]   'fpn/lateral_1x1_c2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb82412e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb82412e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:45 @registry.py:90]   'fpn/lateral_1x1_c3': [1, 512, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb8241be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb8241be0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:45 @registry.py:90]   'fpn/lateral_1x1_c4': [1, 1024, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb8236908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb8236908>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:45 @registry.py:90]   'fpn/lateral_1x1_c5': [1, 2048, ?, ?] --> [1, 256, ?, ?]
[0519 07:05:45 @registry.py:90]   'fpn/upsample_lat5': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0519 07:05:45 @registry.py:90]   'fpn/upsample_lat4': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0519 07:05:45 @registry.py:90]   'fpn/upsample_lat3': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb0502d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb0502d30>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:45 @registry.py:90]   'fpn/posthoc_3x3_p2': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb05028d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb05028d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:45 @registry.py:90]   'fpn/posthoc_3x3_p3': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb049df28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb049df28>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:45 @registry.py:90]   'fpn/posthoc_3x3_p4': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb0502978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb0502978>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:45 @registry.py:90]   'fpn/posthoc_3x3_p5': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f6fb049ddd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f6fb049ddd8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:45 @registry.py:90]   'fpn/maxpool_p6': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0519 07:05:45 @registry.py:93] 'fpn' output: [1, 256, ?, ?], [1, 256, ?, ?], [1, 256, ?, ?], [1, 256, ?, ?], [1, 256, ?, ?]
[0519 07:05:45 @registry.py:80] 'rpn' input: [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03eb588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03eb588>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:45 @registry.py:90]   'rpn/conv0': [1, 256, ?, ?] --> [1, 256, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03eb6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03eb6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:45 @registry.py:90]   'rpn/class': [1, 256, ?, ?] --> [1, 3, ?, ?]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03ebd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03ebd30>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:46 @registry.py:90]   'rpn/box': [1, 256, ?, ?] --> [1, 12, ?, ?]
[0519 07:05:46 @registry.py:93] 'rpn' output: [?, ?, 3], [?, ?, 3, 4]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03cfe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03cfe80>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03cfef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03cfef0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03cf160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb03cf160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb037f390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb037f390>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb037f080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb037f080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb037f080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb037f080>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb0318898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb0318898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb0318048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb0318048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb0318898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb0318898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb031bb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb031bb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb031b6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb031b6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb031bb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6fb031bb70>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:47 @registry.py:80] 'fastrcnn' input: [?, 256, 7, 7]
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6fb0056c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6fb0056c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:47 @registry.py:90]   'fastrcnn/fc6': [?, 256, 7, 7] --> [?, 1024]
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6fb0056f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6fb0056f28>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:47 @registry.py:90]   'fastrcnn/fc7': [?, 1024] --> [?, 1024]
[0519 07:05:47 @registry.py:93] 'fastrcnn' output: [?, 1024]
[0519 07:05:47 @registry.py:80] 'fastrcnn/outputs' input: [?, 1024]
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6f7271e668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6f7271e668>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:47 @registry.py:90]   'fastrcnn/outputs/class': [?, 1024] --> [?, 2]
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6fb00569e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6fb00569e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:47 @registry.py:90]   'fastrcnn/outputs/box': [?, 1024] --> [?, 8]
[0519 07:05:47 @registry.py:93] 'fastrcnn/outputs' output: [?, 2], [?, 2, 4]
[0519 07:05:47 @registry.py:80] 'maskrcnn' input: [?, 256, 14, 14]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6f724ad6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6f724ad6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:48 @registry.py:90]   'maskrcnn/fcn0': [?, 256, 14, 14] --> [?, 256, 14, 14]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6f724ad710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6f724ad710>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:48 @registry.py:90]   'maskrcnn/fcn1': [?, 256, 14, 14] --> [?, 256, 14, 14]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6f724ad828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6f724ad828>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:48 @registry.py:90]   'maskrcnn/fcn2': [?, 256, 14, 14] --> [?, 256, 14, 14]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6f724ad7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6f724ad7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:48 @registry.py:90]   'maskrcnn/fcn3': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0519 07:05:48 @registry.py:90]   'maskrcnn/deconv': [?, 256, 14, 14] --> [?, 256, 28, 28]
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6f723d2e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6f723d2e10>>: AssertionError: Bad argument number for Name: 3, expecting 4
[0519 07:05:48 @registry.py:90]   'maskrcnn/conv': [?, 256, 28, 28] --> [?, 1, 28, 28]
[0519 07:05:48 @registry.py:93] 'maskrcnn' output: [?, 1, 28, 28]
[0519 07:05:48 @collection.py:146] New collections created in tower : tf.GraphKeys.MODEL_VARIABLES of size 55
[0519 07:05:48 @sessinit.py:87] WRN The following variables are in the checkpoint, but not found in the graph: global_step, learning_rate
2020-05-19 07:05:48.813385: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-19 07:05:49.736763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.768382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.801586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.821519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.826049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.837927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.852395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.867772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.869809: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fa2db289e0 executing computations on platform CUDA. Devices:
2020-05-19 07:05:49.869842: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2020-05-19 07:05:49.869854: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla K80, Compute Capability 3.7
2020-05-19 07:05:49.869862: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Tesla K80, Compute Capability 3.7
2020-05-19 07:05:49.869874: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Tesla K80, Compute Capability 3.7
2020-05-19 07:05:49.869881: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): Tesla K80, Compute Capability 3.7
2020-05-19 07:05:49.869892: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): Tesla K80, Compute Capability 3.7
2020-05-19 07:05:49.869899: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): Tesla K80, Compute Capability 3.7
2020-05-19 07:05:49.869907: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (7): Tesla K80, Compute Capability 3.7
2020-05-19 07:05:49.892790: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300070000 Hz
2020-05-19 07:05:49.893955: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fa2f665f30 executing computations on platform Host. Devices:
2020-05-19 07:05:49.893984: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-05-19 07:05:49.906310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.907806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:17.0
2020-05-19 07:05:49.907889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.909382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:18.0
2020-05-19 07:05:49.909460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.910916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:19.0
2020-05-19 07:05:49.910988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.912442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1a.0
2020-05-19 07:05:49.912514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.913986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1b.0
2020-05-19 07:05:49.914059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.915515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1c.0
2020-05-19 07:05:49.915588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.917064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1d.0
2020-05-19 07:05:49.917137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.918595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2020-05-19 07:05:49.918673: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-05-19 07:05:49.918707: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-05-19 07:05:49.918737: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-05-19 07:05:49.918768: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-05-19 07:05:49.918798: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-05-19 07:05:49.918828: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-05-19 07:05:49.918860: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-05-19 07:05:49.918932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.920427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.921963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.923489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.925027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.926540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.928043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.929563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.931066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.932567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.934083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.935583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.937101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.938600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.940099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.941616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.943073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2020-05-19 07:05:49.943130: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-05-19 07:05:49.958043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-19 07:05:49.958072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
2020-05-19 07:05:49.958091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y Y Y Y Y 
2020-05-19 07:05:49.958102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y Y Y Y Y 
2020-05-19 07:05:49.958110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y Y Y Y Y 
2020-05-19 07:05:49.958122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N Y Y Y Y 
2020-05-19 07:05:49.958130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   Y Y Y Y N Y Y Y 
2020-05-19 07:05:49.958143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   Y Y Y Y Y N Y Y 
2020-05-19 07:05:49.958154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   Y Y Y Y Y Y N Y 
2020-05-19 07:05:49.958167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   Y Y Y Y Y Y Y N 
2020-05-19 07:05:49.958481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.960013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.961564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.963094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.964665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.966199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.967734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.969277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.970811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.972317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11326 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:17.0, compute capability: 3.7)
2020-05-19 07:05:49.972729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.974230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11326 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:18.0, compute capability: 3.7)
2020-05-19 07:05:49.974615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.976101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11326 MB memory) -> physical GPU (device: 2, name: Tesla K80, pci bus id: 0000:00:19.0, compute capability: 3.7)
2020-05-19 07:05:49.976470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.977980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 11326 MB memory) -> physical GPU (device: 3, name: Tesla K80, pci bus id: 0000:00:1a.0, compute capability: 3.7)
2020-05-19 07:05:49.978377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.979881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 11326 MB memory) -> physical GPU (device: 4, name: Tesla K80, pci bus id: 0000:00:1b.0, compute capability: 3.7)
2020-05-19 07:05:49.980249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.981784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 11326 MB memory) -> physical GPU (device: 5, name: Tesla K80, pci bus id: 0000:00:1c.0, compute capability: 3.7)
2020-05-19 07:05:49.982158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.983661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 11326 MB memory) -> physical GPU (device: 6, name: Tesla K80, pci bus id: 0000:00:1d.0, compute capability: 3.7)
2020-05-19 07:05:49.984043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-19 07:05:49.985593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 11326 MB memory) -> physical GPU (device: 7, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
[0519 07:05:53 @sessinit.py:114] Restoring checkpoint from /home/ubuntu/car2/log_ex_3/model-12450 ...
2020-05-19 07:05:55.534149: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2020-05-19 07:05:55.631659: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-05-19 07:05:55.782505: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
[0519 07:05:56 @predict.py:110] Inference output for predicted_/home/ubuntu/car2/test_images/c.jpg written

```

Does this give you some more information ?",log information python predict load passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource successfully dynamic library successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible false false false true true false true true true path warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name input warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name output input warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name output warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name input warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name output input warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name output input warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name warning entity bound method object could executed please report team filing bug set verbosity export attach full output cause converting bound method object bad argument number name output new tower size following found graph binary use successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero service platform device compute capability device compute capability device compute capability device compute capability device compute capability device compute capability device compute capability device compute capability frequency service platform host device undefined undefined successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successful node read negative value must least one node node zero found device name major minor successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero visible successfully dynamic library device interconnect strength edge matrix successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero successful node read negative value must least one node node zero device memory physical device name bus id compute capability successful node read negative value must least one node node zero device memory physical device name bus id compute capability successful node read negative value must least one node node zero device memory physical device name bus id compute capability successful node read negative value must least one node node zero device memory physical device name bus id compute capability successful node read negative value must least one node node zero device memory physical device name bus id compute capability successful node read negative value must least one node node zero device memory physical device name bus id compute capability successful node read negative value must least one node node zero device memory physical device name bus id compute capability successful node read negative value must least one node node zero device memory physical device name bus id compute capability warning cluster set want either set use enable confirm active pas proper flag via set successfully dynamic library successfully dynamic library inference output written give information,issue,negative,negative,neutral,neutral,negative,negative
630637248,"You almost certainly did something wrong in the timing, but I don't know what you did so no suggestions can be given.
https://tensorpack.readthedocs.io/tutorial/inference.html has everything tensorpack does about inference.",almost certainly something wrong timing know given everything inference,issue,negative,negative,negative,negative,negative,negative
630329960,The tensor is missing because it is not defined. The tensors that should be used as inputs are defined by user when implementing the model.,tensor missing defined used defined user model,issue,negative,negative,negative,negative,negative,negative
628933490,"OK, got it. Thanks for confirming this, I will double check my own code then.",got thanks confirming double check code,issue,negative,positive,neutral,neutral,positive,positive
628926353,"The original `Image2Image.py` sets `g_loss` and will not have such error.
I'm not sure how I can help you since the issue is probably in your private code",original error sure help since issue probably private code,issue,positive,positive,positive,positive,positive,positive
628790345,"There isn't a description of them now.
These are details inside the model that are probably relevant only to those very familiar with every part of the model.",description inside model probably relevant familiar every part model,issue,negative,positive,positive,positive,positive,positive
628392355,It's up to you how many iterations you want to train. We do not answer questions that are not specific to tensorpack.,many want train answer specific,issue,negative,positive,positive,positive,positive,positive
627963229,@ppwwyyxx Could you point to those papers ? Cannot find it anywhere. I want to understand all those metrics. ,could point find anywhere want understand metric,issue,negative,neutral,neutral,neutral,neutral,neutral
626555672,"Thanks for sharing!
As the [issue template](https://github.com/tensorpack/tensorpack/issues/new?assignees=&labels=&template=feature-requests.md&title=) says, we consider feature requests for the library but not for its examples. So we won't add it to the example. ",thanks issue template consider feature library wo add example,issue,negative,positive,positive,positive,positive,positive
625715566,@ppwwyyxx I understand. Thanks for spending time on this though. I will close this issue.,understand thanks spending time though close issue,issue,negative,positive,positive,positive,positive,positive
625709895,"I can't see anything obviously wrong from the logs.
The code does scale well to more GPUs when running with the machine&dataset&configuration in the documentation. Since all of them are different in your training it's difficult to tell what's the reason.",ca see anything obviously wrong code scale well running machine configuration documentation since different training difficult tell reason,issue,negative,negative,negative,negative,negative,negative
625680314,@ppwwyyxx Thanks for keeping an eye on this. I have updated original post with more information..,thanks keeping eye original post information,issue,positive,positive,positive,positive,positive,positive
625666733,Including the details following the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/unexpected-problems---bugs.md) will give more information.,following issue template give information,issue,negative,neutral,neutral,neutral,neutral,neutral
625556208,"@ppwwyyxx Thanks again for such a quick reply!👍👍👍


I will try as suggested.",thanks quick reply try,issue,negative,positive,positive,positive,positive,positive
625551927,"As https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md#efficiency says, the speed for first 10k iterations is expected to be slow it does not get better with more GPUs.

Using `export TF_CUDNN_USE_AUTOTUNE=0` can skip this warmup period.",speed first slow get better export skip period,issue,negative,positive,positive,positive,positive,positive
621985662,"@ppwwyyxx Thanks!

Feeling stupid now for missing that bit of information.

I am closing this ticket now.
",thanks feeling stupid missing bit information ticket,issue,negative,negative,negative,negative,negative,negative
621983769,"That sounds right.
The format the model takes is decided by the data loader the model is trained with.
But `put_image` expects RGB format as https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.Monitors.put_image explains.",right format model decided data loader model trained format,issue,negative,positive,positive,positive,positive,positive
621977778,"@ppwwyyxx Just logged off from work and went through the code again and found the issue!

The step of converting BGR to RGB should not be done at the time of loading images in my code. It seems, tensorpack uses BGR format of images (the default way opencv loads an image) and predictions are to be done in the same channel order.

BUT the image needs to be converted into RGB format before ""self.trainer.monitors.put_image(name, img)"" else people look like characters from the movie Avatar.

I did that and got fantastic result!

If I am correct in my findings, please close the issue.

",logged work went code found issue step converting done time loading code format default way image done channel order image need converted format name else people look like movie got fantastic result correct please close issue,issue,positive,positive,positive,positive,positive,positive
620739858,"We use larger GPUs.
You can use smaller image resolution or change the model architecture

It's correct that batch size is 1",use use smaller image resolution change model architecture correct batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
620736963,"The code was not correct w.r.t. latest version of tensorpack: If TrainTowerContext is used for multi-gpu training, index of the GPU and total number of GPUs should be provided.",code correct latest version used training index total number provided,issue,negative,positive,positive,positive,positive,positive
620474303,"Oh, by the way, I want to change the batchsize in the train to reduce the use of  memory, but I find that the batchsize is 1, is that correct?",oh way want change train reduce use memory find correct,issue,negative,neutral,neutral,neutral,neutral,neutral
619278919,">  I removed LocallyShuffleData but the for loop still runs forever.

`MultiProcessMapDataZMQ(strict=False)` is also infinite by design.

> why doesn't it make sense to stop the iteration once len(loader) is reached?

explained in the docs and the above comment

> How can I simply create a for loop that will go over the dataset only one time to train the model for one epoch

Some of these dataflows do NOT give you exactly the same dataset as the docs explains. They also do not have the concept of ""beginning"" at all by definition of what they do. So don't use them if this is what you need. Most others dataflows including `MultiProcessMapDataZMQ(strict=True)` does terminate.",removed loop still forever also infinite design make sense stop iteration loader comment simply create loop go one time train model one epoch give exactly also concept beginning definition use need terminate,issue,negative,positive,positive,positive,positive,positive
619275428,"Thanks for the answer. I removed LocallyShuffleData but the for loop still runs forever.
May I ask, why doesn't it make sense to stop the iteration once len(loader) is reached?

And also, why does the iteration stop when I don't include BatchData?

How can I simply create a for loop that will go over the dataset only one time to train the model for one epoch? And how would I restart the loader to start from the beginning for the next epoch?",thanks answer removed loop still forever may ask make sense stop iteration loader also iteration stop include simply create loop go one time train model one epoch would restart loader start beginning next epoch,issue,negative,positive,neutral,neutral,positive,positive
619273426,"This is what the documentation says https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.LocallyShuffleData so it's working as expected.

> Datapoints from one pass of ds will get mixed with datapoints from a different pass. As a result, the iterator of this dataflow will run indefinitely because it does not make sense to stop the iteration anywhere.",documentation working one pas get mixed different pas result run indefinitely make sense stop iteration anywhere,issue,negative,neutral,neutral,neutral,neutral,neutral
619072722,"@ppwwyyxx Thank you for your reply. I have added `MODE_MASK=False` as suggested. One of the errors was that I had boxes with area of zero, however, I have fixed that issue and now it all seems to be working as it should. Thank you very much. 


",thank reply added one area zero however fixed issue working thank much,issue,positive,positive,positive,positive,positive,positive
618942050,"Hi, have you solved this problem? I met the same problem, could you please tell me the reason and how to solve it? Thank you very much!",hi problem met problem could please tell reason solve thank much,issue,negative,positive,positive,positive,positive,positive
618551644,"You probably have other errors before what you show.

You need `MODE_MASK=False` to not train on masks.",probably show need train,issue,negative,neutral,neutral,neutral,neutral,neutral
618545160,"1. correct
2.
> iscrowd=1 means that the segmentation is in RLE (run length encoding) format

Actually this is not true. It means something else. You can find something in coco's docs or papers but just setting it to False as the doc suggests is fine.

However, the data parsing code in `data.py` does not currently handle RLE format. It assumes polygons.

No other keys are needed in addition to these.",correct segmentation run length format actually true something else find something coco setting false doc fine however data code currently handle format addition,issue,positive,positive,neutral,neutral,positive,positive
618277654,There is no point using TFRecords. But you can because you can use anything that can be read from python (https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html).,point use anything read python,issue,negative,neutral,neutral,neutral,neutral,neutral
614185143,"The original code is meant to filter out negative samples on purpose.

To support the other behavior, one way is to add a config option `DATA.FILTER_EMPTY_ANNOTATIONS`.",original code meant filter negative purpose support behavior one way add option,issue,negative,positive,neutral,neutral,positive,positive
613640351,Sorry for the delayed response. Callback is working after your fix. Thanks!,sorry response working fix thanks,issue,negative,negative,negative,negative,negative,negative
612672979,"@ppwwyyxx that did not work either. Also even though the Open Images dataset has coordinates in 0.0..1.0 format, I convert them to absolute. Also I mentioned that the same dataset works with training  with single GPU.
Anyway, thanks for your reply.",work either also even though open format convert absolute also work training single anyway thanks reply,issue,negative,positive,neutral,neutral,positive,positive
612665814,"As the issue template says we're unable to help much if the model does not converge / work well on __your dataset__. This is not considered an issue of tensorpack.

My guess is you either need to double check that your data format is correct (e.g., open image may have box coordinates in [0, 1]) or you need lower learning rate (use smaller BASE_LR and set WARMUP_INIT_LR=0.0).",issue template unable help much model converge work well considered issue guess either need double check data format correct open image may box need lower learning rate use smaller set,issue,negative,negative,neutral,neutral,negative,negative
612625785,":( No luck. It started getting nan after about 10k steps.
Restarted with one GPU and its past 16k steps.",luck getting nan one past,issue,negative,negative,negative,negative,negative,negative
612614689,"I went though other issues related to mine once again and noticed someone had null polygons and I found that 13 instances of masks in my dataset also had null polygons. So I have made changes to my dataset class to ignore such instances.
I have started training again.
I will keep this issue open for some time and if everything goes well, I will close it.

Thanks",went though related mine someone null found also null made class ignore training keep issue open time everything go well close thanks,issue,negative,positive,neutral,neutral,positive,positive
610219669,I hope it can be done when I have more time. But the transition will be a pretty large effort so it probably not going to happen in the next 6 months.,hope done time transition pretty large effort probably going happen next,issue,positive,positive,positive,positive,positive,positive
609952779,"@ppwwyyxx Thanks for your response. Unfortunately it's very unlikely that we use both TF1 and TF2 in our projects, we will rather stick with TF2. I understand that it is far from simple to upgrade your code for TF2, but are there any plans at all? I assume eventually most people will have switched to TF2 - the same as with Python2 to Python3 ...",thanks response unfortunately unlikely use rather stick understand far simple upgrade code assume eventually people switched python python,issue,negative,negative,neutral,neutral,negative,negative
609951488,"Upgrade should be run just your own code. However, as mentioned above everything still runs in TF1 mode so there is no point using TF2 with tensorpack and it's not guaranteed to work (it's not regularly tested).

`pip install` works the same.",upgrade run code however everything still mode point work regularly tested pip install work,issue,negative,neutral,neutral,neutral,neutral,neutral
609775467,"Hey guys, what is the current status of this repo when using TF2? When I want to use the `tf_upgrade_v2` script, do I need to run it for the whole *tensorpack* repo, or just some parts of your code? Moreover, when I use TF2 I cannot use the PIP module installed via `pip install --upgrade git+https://github.com/tensorpack/tensorpack.git`, right?

Frankly it is a pitty that TF2 is not supported out of the box. This will make your awesome repo less and less usable in the future, as less and less people use TF1.x these days :(",hey current status want use script need run whole code moreover use use pip module via pip install upgrade right frankly box make awesome le le usable future le le people use day,issue,positive,positive,positive,positive,positive,positive
609369555,"SyncMultiGPUTrainerParameterServer: see docs: https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.SyncMultiGPUTrainerParameterServer
DataParallel is unrelated to this project so we do not answer questions about it.
",see unrelated project answer,issue,negative,neutral,neutral,neutral,neutral,neutral
609018438,"Just in case it helps anybody....
I was looking at items/sec in the progress bar during training when I thought the performance had not improved.

After a while when tensorboard was updated and looked at the throughput, I noticed significant difference from the single GPU training sessions I had before with same code and dataset.",case anybody looking progress bar training thought performance throughput significant difference single training session code,issue,positive,positive,positive,positive,positive,positive
608606015,"> FasterRCNN example will use all gpus it finds.

Thank you and closing the issue!",example use thank issue,issue,negative,neutral,neutral,neutral,neutral,neutral
608601593,"Sorry I did not want to create it as an issue as it was just a question about usage.

Also I should have mentioned that I am using it for training FasterRCNN using my custom dataset. If you still think this requires another ticket, I will create it.

And thanks for such a quick response!",sorry want create issue question usage also training custom still think another ticket create thanks quick response,issue,positive,positive,neutral,neutral,positive,positive
608600511,">  Is having multi-GPU training as simple as adding another GPU card?

Depends on what code you run. Yes for most example.

> I did not notice the desired items/sec going up as expected (from single GPU on same machine).

For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",training simple another card code run yes example notice desired going single machine anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected,issue,positive,positive,positive,positive,positive,positive
608173327,"You call `set_shape` immediately after a padding function that pads 5 pixels, so you should've set its shape to 1605 instead of 1600.

Again, I'm not supposed to debug it and I'm not going to.",call immediately padding function set shape instead supposed going,issue,negative,neutral,neutral,neutral,neutral,neutral
608169542,"@ppwwyyxx I have already checked the size of data by printing. It's (1600,1600,3).",already checked size data printing,issue,negative,neutral,neutral,neutral,neutral,neutral
608166676,You can check the size of your data by printing them. I'm not supposed to debug the code you write,check size data printing supposed code write,issue,negative,neutral,neutral,neutral,neutral,neutral
608165907,"@ppwwyyxx I have made changes in the code. Now i am using `l.set_shape([None,chan, 1600, 1600])`.
Also as you suggested, i have made changes in _examples/dataflow/imgaug/transform.py_ 
In line 118, i have modified the function as:
```
    def apply_image(self, img):
        assert img.shape[:2] == (self.h, self.w)
        ret = cv2.resize(
            img, (self.new_w, self.new_h),
            interpolation=self.interp)
        delta_w = 1600 - ret.shape[1]
        delta_h = 1600 - ret.shape[0]
        top, bottom = delta_h//2, delta_h-(delta_h//2)
        left, right = delta_w//2, delta_w-(delta_w//2)
        color = [0, 0, 0]
        ret2 = cv2.copyMakeBorder(ret, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)
        if img.ndim == 3 and ret2.ndim == 2:
            ret2 = ret2[:, :, np.newaxis]
        return ret2
```
After making this change, I am asuming that the model is taking an square image of 1600x1600.

But still i am getting the error. The output of log file is:

```
[0403 05:54:23 @logger.py:92] Argv: examples/FasterRCNN/train.py --config DATA.BASEDIR=/home/madhav3101/cascade/train_dataset/coco/ --load examples/FasterRCNN/def_model/COCO-MaskRCNN-R101FPN3xCasAug.npz --logdir /home/madhav3101/cascade/train_dataset/coco/logs/
[0403 05:54:23 @concurrency.py:266] WRN Command 'ldconfig -p' failed, return code=127
[0403 05:54:23 @concurrency.py:267] WRN /bin/sh: 1: ldconfig: not found

[0403 05:54:24 @train.py:56] Environment Information:
--------------------  -------------------------------------------------------------------
sys.platform          linux
Python                3.7.1 (default, Dec 14 2018, 19:28:38) [GCC 7.3.0]
Tensorpack            v0.9.8-62-gb28cfa8-dirty
Numpy                 1.17.4
TensorFlow            1.13.1/b'v1.13.0-rc2-5-g6612da8'
TF Compiler Version   5.4.0 20160609
TF CUDA support       True
TF MKL support        False
Nvidia Driver         libnvidia-ml.so.1
CUDA                  /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130
CUDNN                 /usr/local/apps/cuDNN/7.6-cuda10/lib64/libcudnn.so.7.6.2
NCCL                  /usr/lib/x86_64-linux-gnu/libnccl.so.2.5.6
CUDA_VISIBLE_DEVICES  0
GPU 0                 GeForce RTX 2080 Ti
Free RAM              123.94/125.78 GB
CPU Count             40
cv2                   3.4.4
msgpack               1.0.0
python-prctl          False
--------------------  -------------------------------------------------------------------
[0403 05:54:24 @config.py:322] Config: ------------------------------------------
{'BACKBONE': {'FREEZE_AFFINE': False,
              'FREEZE_AT': 2,
              'NORM': 'FreezeBN',
              'RESNET_NUM_BLOCKS': [3, 4, 23, 3],
              'STRIDE_1X1': False,
              'TF_PAD_MODE': False,
              'WEIGHTS': ''},
 'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [30.0, 30.0, 15.0, 15.0],
                                  [50.0, 50.0, 25.0, 25.0]],
             'IOUS': [0.5, 0.7, 0.9]},
 'DATA': {'ABSOLUTE_COORD': True,
          'BASEDIR': '/home/madhav3101/cascade/train_dataset/coco/',
          'CLASS_NAMES': ['BG', 'table'],
          'NUM_CATEGORY': 1,
          'NUM_WORKERS': 10,
          'TRAIN': ('coco_train2014',),
          'VAL': ('coco_val2014',)},
 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),
         'CASCADE': True,
         'FRCNN_CONV_HEAD_DIM': 256,
         'FRCNN_FC_HEAD_DIM': 1024,
         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',
         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',
         'NORM': 'None',
         'NUM_CHANNEL': 256,
         'PROPOSAL_MODE': 'Level',
         'RESOLUTION_REQUIREMENT': 32},
 'FRCNN': {'BATCH_PER_IM': 512,
           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],
           'FG_RATIO': 0.25,
           'FG_THRESH': 0.5},
 'MODE_FPN': True,
 'MODE_MASK': True,
 'MRCNN': {'ACCURATE_PASTE': True, 'HEAD_DIM': 256},
 'PREPROC': {'MAX_SIZE': 1344.0,
             'PIXEL_MEAN': [244.06, 244.12, 244.099],
             'PIXEL_STD': [39.927, 36.963, 36.936],
             'TEST_SHORT_EDGE_SIZE': 800,
             'TRAIN_SHORT_EDGE_SIZE': [1280, 1600]},
 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),
         'ANCHOR_SIZES': (32, 64, 128, 256, 512),
         'ANCHOR_STRIDE': 16,
         'BATCH_PER_IM': 256,
         'CROWD_OVERLAP_THRESH': 9.99,
         'FG_RATIO': 0.5,
         'HEAD_DIM': 1024,
         'MIN_SIZE': 0,
         'NEGATIVE_ANCHOR_THRESH': 0.3,
         'NUM_ANCHOR': 15,
         'POSITIVE_ANCHOR_THRESH': 0.7,
         'PROPOSAL_NMS_THRESH': 0.7,
         'TEST_PER_LEVEL_NMS_TOPK': 1000,
         'TEST_POST_NMS_TOPK': 1000,
         'TEST_PRE_NMS_TOPK': 6000,
         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,
         'TRAIN_POST_NMS_TOPK': 2000,
         'TRAIN_PRE_NMS_TOPK': 12000},
 'TEST': {'FRCNN_NMS_THRESH': 0.5,
          'RESULTS_PER_IM': 100,
          'RESULT_SCORE_THRESH': 0.0001,
          'RESULT_SCORE_THRESH_VIS': 0.5},
 'TRAIN': {'BASE_LR': 0.01,
           'CHECKPOINT_PERIOD': 10,
           'EVAL_PERIOD': 50,
           'LR_SCHEDULE': [420000, 500000, 540000],
           'NUM_GPUS': 1,
           'STARTING_EPOCH': 1,
           'STEPS_PER_EPOCH': 1000,
           'WARMUP': 100,
           'WARMUP_INIT_LR': 0.0033000000000000004,
           'WEIGHT_DECAY': 0.0001},
 'TRAINER': 'replicated'}
[0403 05:54:24 @train.py:78] Warm Up Schedule (steps, value): [(0, 0.0033000000000000004), (100, 0.01)]
[0403 05:54:24 @train.py:79] LR Schedule (epochs, value): [(0, 0.01), (150.0, 0.001), (200.0, 0.0001)]
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
[0403 05:54:24 @coco.py:63] Instances loaded from /home/madhav3101/cascade/train_dataset/coco/annotations/instances_train2014.json.
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 78/78 [00:00<00:00, 4256.35it/s]
[0403 05:54:24 @timer.py:45] Load annotations for instances_train2014.json finished, time:0.0203 sec.
[0403 05:54:24 @data.py:62] Ground-Truth category distribution:
|  class  | #box   |
|:-------:|:-------|
|  table  | 98     |
|  total  | 98     |
[0403 05:54:24 @data.py:351] Filtered 0 images which contain no non-crowd groudtruth boxes. Total #images for training: 78
[0403 05:54:24 @train.py:85] Total passes of the training set is: 250
[0403 05:54:24 @sessinit.py:294] Loading dictionary from examples/FasterRCNN/def_model/COCO-MaskRCNN-R101FPN3xCasAug.npz ...
[0403 05:54:27 @train.py:118] max_epoch:4320.0
[0403 05:54:27 @train.py:119] total_passes:250
[0403 05:54:27 @input_source.py:221] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0403 05:54:27 @training.py:108] Building graph for training tower 0 on device /gpu:0 ...
[0403 05:54:28 @argtools.py:138] WRN Some BatchNorm layer uses moving_mean/moving_variance in training.
[0403 05:54:28 @registry.py:90] 'conv0': [1, 3, 1600, 1600] --> [1, 64, 797, 797]
[0403 05:54:28 @registry.py:90] 'pool0': [1, 64, 798, 798] --> [1, 64, 398, 398]
[0403 05:54:28 @registry.py:90] 'group0/block0/conv1': [1, 64, 398, 398] --> [1, 64, 398, 398]
[0403 05:54:28 @registry.py:90] 'group0/block0/conv2': [1, 64, 398, 398] --> [1, 64, 398, 398]
[0403 05:54:28 @registry.py:90] 'group0/block0/conv3': [1, 64, 398, 398] --> [1, 256, 398, 398]
[0403 05:54:28 @registry.py:90] 'group0/block0/convshortcut': [1, 64, 398, 398] --> [1, 256, 398, 398]
[0403 05:54:28 @registry.py:90] 'group0/block1/conv1': [1, 256, 398, 398] --> [1, 64, 398, 398]
[0403 05:54:28 @registry.py:90] 'group0/block1/conv2': [1, 64, 398, 398] --> [1, 64, 398, 398]
[0403 05:54:28 @registry.py:90] 'group0/block1/conv3': [1, 64, 398, 398] --> [1, 256, 398, 398]
[0403 05:54:28 @registry.py:90] 'group0/block2/conv1': [1, 256, 398, 398] --> [1, 64, 398, 398]
[0403 05:54:28 @registry.py:90] 'group0/block2/conv2': [1, 64, 398, 398] --> [1, 64, 398, 398]
[0403 05:54:28 @registry.py:90] 'group0/block2/conv3': [1, 64, 398, 398] --> [1, 256, 398, 398]
[0403 05:54:28 @registry.py:90] 'group1/block0/conv1': [1, 256, 398, 398] --> [1, 128, 398, 398]
[0403 05:54:29 @registry.py:90] 'group1/block0/conv2': [1, 128, 399, 399] --> [1, 128, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block0/conv3': [1, 128, 199, 199] --> [1, 512, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block0/convshortcut': [1, 256, 398, 398] --> [1, 512, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block1/conv1': [1, 512, 199, 199] --> [1, 128, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block1/conv2': [1, 128, 199, 199] --> [1, 128, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block1/conv3': [1, 128, 199, 199] --> [1, 512, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block2/conv1': [1, 512, 199, 199] --> [1, 128, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block2/conv2': [1, 128, 199, 199] --> [1, 128, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block2/conv3': [1, 128, 199, 199] --> [1, 512, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block3/conv1': [1, 512, 199, 199] --> [1, 128, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block3/conv2': [1, 128, 199, 199] --> [1, 128, 199, 199]
[0403 05:54:29 @registry.py:90] 'group1/block3/conv3': [1, 128, 199, 199] --> [1, 512, 199, 199]
[0403 05:54:29 @registry.py:90] 'group2/block0/conv1': [1, 512, 199, 199] --> [1, 256, 199, 199]
[0403 05:54:29 @registry.py:90] 'group2/block0/conv2': [1, 256, 200, 200] --> [1, 256, 99, 99]
[0403 05:54:29 @registry.py:90] 'group2/block0/conv3': [1, 256, 99, 99] --> [1, 1024, 99, 99]
[0403 05:54:29 @registry.py:90] 'group2/block0/convshortcut': [1, 512, 199, 199] --> [1, 1024, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block1/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block1/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block1/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block2/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block2/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block2/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block3/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block3/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block3/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block4/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block4/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block4/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block5/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block5/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:29 @registry.py:90] 'group2/block5/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block6/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block6/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block6/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block7/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block7/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block7/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block8/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block8/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block8/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block9/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block9/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block9/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block10/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block10/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block10/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block11/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block11/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block11/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block12/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block12/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block12/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block13/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block13/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block13/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block14/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block14/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block14/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block15/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block15/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:30 @registry.py:90] 'group2/block15/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block16/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block16/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block16/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block17/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block17/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block17/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block18/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block18/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block18/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block19/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block19/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block19/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block20/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block20/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block20/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block21/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block21/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block21/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block22/conv1': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block22/conv2': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:31 @registry.py:90] 'group2/block22/conv3': [1, 256, 100, 100] --> [1, 1024, 100, 100]
[0403 05:54:31 @registry.py:90] 'group3/block0/conv1': [1, 1024, 100, 100] --> [1, 512, 100, 100]
[0403 05:54:31 @registry.py:90] 'group3/block0/conv2': [1, 512, 101, 101] --> [1, 512, 50, 50]
[0403 05:54:31 @registry.py:90] 'group3/block0/conv3': [1, 512, 50, 50] --> [1, 2048, 50, 50]
[0403 05:54:31 @registry.py:90] 'group3/block0/convshortcut': [1, 1024, 100, 100] --> [1, 2048, 50, 50]
[0403 05:54:31 @registry.py:90] 'group3/block1/conv1': [1, 2048, 50, 50] --> [1, 512, 50, 50]
[0403 05:54:31 @registry.py:90] 'group3/block1/conv2': [1, 512, 50, 50] --> [1, 512, 50, 50]
[0403 05:54:31 @registry.py:90] 'group3/block1/conv3': [1, 512, 50, 50] --> [1, 2048, 50, 50]
[0403 05:54:31 @registry.py:90] 'group3/block2/conv1': [1, 2048, 50, 50] --> [1, 512, 50, 50]
[0403 05:54:31 @registry.py:90] 'group3/block2/conv2': [1, 512, 50, 50] --> [1, 512, 50, 50]
[0403 05:54:32 @registry.py:90] 'group3/block2/conv3': [1, 512, 50, 50] --> [1, 2048, 50, 50]
[0403 05:54:32 @registry.py:80] 'fpn' input: [1, 256, 398, 398], [1, 512, 199, 199], [1, 1024, 100, 100], [1, 2048, 50, 50]
[0403 05:54:32 @registry.py:90]   'fpn/lateral_1x1_c2': [1, 256, 398, 398] --> [1, 256, 398, 398]
[0403 05:54:32 @registry.py:90]   'fpn/lateral_1x1_c3': [1, 512, 199, 199] --> [1, 256, 199, 199]
[0403 05:54:32 @registry.py:90]   'fpn/lateral_1x1_c4': [1, 1024, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:32 @registry.py:90]   'fpn/lateral_1x1_c5': [1, 2048, 50, 50] --> [1, 256, 50, 50]
[0403 05:54:32 @registry.py:90]   'fpn/upsample_lat5': [1, 256, 50, 50] --> [1, 256, 100, 100]
[0403 05:54:32 @registry.py:90]   'fpn/upsample_lat4': [1, 256, 100, 100] --> [1, 256, 200, 200]
[0403 05:54:32 @registry.py:90]   'fpn/upsample_lat3': [1, 256, 200, 200] --> [1, 256, 400, 400]
[0403 05:54:32 @registry.py:90]   'fpn/posthoc_3x3_p2': [1, 256, 400, 400] --> [1, 256, 400, 400]
[0403 05:54:32 @registry.py:90]   'fpn/posthoc_3x3_p3': [1, 256, 200, 200] --> [1, 256, 200, 200]
[0403 05:54:32 @registry.py:90]   'fpn/posthoc_3x3_p4': [1, 256, 100, 100] --> [1, 256, 100, 100]
[0403 05:54:32 @registry.py:90]   'fpn/posthoc_3x3_p5': [1, 256, 50, 50] --> [1, 256, 50, 50]
[0403 05:54:32 @registry.py:90]   'fpn/maxpool_p6': [1, 256, 50, 50] --> [1, 256, 25, 25]
[0403 05:54:32 @registry.py:93] 'fpn' output: [1, 256, 400, 400], [1, 256, 200, 200], [1, 256, 100, 100], [1, 256, 50, 50], [1, 256, 25, 25]
[0403 05:54:32 @registry.py:80] 'rpn' input: [1, 256, 400, 400]
[0403 05:54:32 @registry.py:90]   'rpn/conv0': [1, 256, 400, 400] --> [1, 256, 400, 400]
[0403 05:54:32 @registry.py:90]   'rpn/class': [1, 256, 400, 400] --> [1, 3, 400, 400]
[0403 05:54:32 @registry.py:90]   'rpn/box': [1, 256, 400, 400] --> [1, 12, 400, 400]
[0403 05:54:32 @registry.py:93] 'rpn' output: [400, 400, 3], [400, 400, 3, 4]
[0403 05:54:34 @registry.py:80] 'cascade_rcnn_stage1/head' input: [?, 256, 7, 7]
[0403 05:54:34 @registry.py:90]   'cascade_rcnn_stage1/head/fc6': [?, 256, 7, 7] --> [?, 1024]
[0403 05:54:34 @registry.py:90]   'cascade_rcnn_stage1/head/fc7': [?, 1024] --> [?, 1024]
[0403 05:54:34 @registry.py:93] 'cascade_rcnn_stage1/head' output: [?, 1024]
[0403 05:54:34 @registry.py:80] 'cascade_rcnn_stage1/outputs' input: [?, 1024]
[0403 05:54:34 @registry.py:90]   'cascade_rcnn_stage1/outputs/class': [?, 1024] --> [?, 2]
[0403 05:54:34 @registry.py:90]   'cascade_rcnn_stage1/outputs/box': [?, 1024] --> [?, 4]
[0403 05:54:34 @registry.py:93] 'cascade_rcnn_stage1/outputs' output: [?, 2], [?, 1, 4]
[0403 05:54:35 @registry.py:80] 'cascade_rcnn_stage2/head' input: [?, 256, 7, 7]
[0403 05:54:35 @registry.py:90]   'cascade_rcnn_stage2/head/fc6': [?, 256, 7, 7] --> [?, 1024]
[0403 05:54:35 @registry.py:90]   'cascade_rcnn_stage2/head/fc7': [?, 1024] --> [?, 1024]
[0403 05:54:35 @registry.py:93] 'cascade_rcnn_stage2/head' output: [?, 1024]
[0403 05:54:35 @registry.py:80] 'cascade_rcnn_stage2/outputs' input: [?, 1024]
[0403 05:54:35 @registry.py:90]   'cascade_rcnn_stage2/outputs/class': [?, 1024] --> [?, 2]
[0403 05:54:35 @registry.py:90]   'cascade_rcnn_stage2/outputs/box': [?, 1024] --> [?, 4]
[0403 05:54:35 @registry.py:93] 'cascade_rcnn_stage2/outputs' output: [?, 2], [?, 1, 4]
[0403 05:54:35 @registry.py:80] 'cascade_rcnn_stage3/head' input: [?, 256, 7, 7]
[0403 05:54:35 @registry.py:90]   'cascade_rcnn_stage3/head/fc6': [?, 256, 7, 7] --> [?, 1024]
[0403 05:54:35 @registry.py:90]   'cascade_rcnn_stage3/head/fc7': [?, 1024] --> [?, 1024]
[0403 05:54:35 @registry.py:93] 'cascade_rcnn_stage3/head' output: [?, 1024]
[0403 05:54:35 @registry.py:80] 'cascade_rcnn_stage3/outputs' input: [?, 1024]
[0403 05:54:35 @registry.py:90]   'cascade_rcnn_stage3/outputs/class': [?, 1024] --> [?, 2]
[0403 05:54:35 @registry.py:90]   'cascade_rcnn_stage3/outputs/box': [?, 1024] --> [?, 4]
[0403 05:54:35 @registry.py:93] 'cascade_rcnn_stage3/outputs' output: [?, 2], [?, 1, 4]
[0403 05:54:36 @registry.py:80] 'maskrcnn' input: [?, 256, 14, 14]
[0403 05:54:36 @registry.py:90]   'maskrcnn/fcn0': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0403 05:54:36 @registry.py:90]   'maskrcnn/fcn1': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0403 05:54:36 @registry.py:90]   'maskrcnn/fcn2': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0403 05:54:36 @registry.py:90]   'maskrcnn/fcn3': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0403 05:54:36 @registry.py:90]   'maskrcnn/deconv': [?, 256, 14, 14] --> [?, 256, 28, 28]
[0403 05:54:36 @registry.py:90]   'maskrcnn/conv': [?, 256, 28, 28] --> [?, 1, 28, 28]
[0403 05:54:36 @registry.py:93] 'maskrcnn' output: [?, 1, 28, 28]
[0403 05:54:37 @regularize.py:97] regularize_cost() found 122 variables to regularize.
[0403 05:54:37 @regularize.py:21] The following tensors will be regularized: group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/conv2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group2/block5/conv3/W:0, group2/block6/conv1/W:0, group2/block6/conv2/W:0, group2/block6/conv3/W:0, group2/block7/conv1/W:0, group2/block7/conv2/W:0, group2/block7/conv3/W:0, group2/block8/conv1/W:0, group2/block8/conv2/W:0, group2/block8/conv3/W:0, group2/block9/conv1/W:0, group2/block9/conv2/W:0, group2/block9/conv3/W:0, group2/block10/conv1/W:0, group2/block10/conv2/W:0, group2/block10/conv3/W:0, group2/block11/conv1/W:0, group2/block11/conv2/W:0, group2/block11/conv3/W:0, group2/block12/conv1/W:0, group2/block12/conv2/W:0, group2/block12/conv3/W:0, group2/block13/conv1/W:0, group2/block13/conv2/W:0, group2/block13/conv3/W:0, group2/block14/conv1/W:0, group2/block14/conv2/W:0, group2/block14/conv3/W:0, group2/block15/conv1/W:0, group2/block15/conv2/W:0, group2/block15/conv3/W:0, group2/block16/conv1/W:0, group2/block16/conv2/W:0, group2/block16/conv3/W:0, group2/block17/conv1/W:0, group2/block17/conv2/W:0, group2/block17/conv3/W:0, group2/block18/conv1/W:0, group2/block18/conv2/W:0, group2/block18/conv3/W:0, group2/block19/conv1/W:0, group2/block19/conv2/W:0, group2/block19/conv3/W:0, group2/block20/conv1/W:0, group2/block20/conv2/W:0, group2/block20/conv3/W:0, group2/block21/conv1/W:0, group2/block21/conv2/W:0, group2/block21/conv3/W:0, group2/block22/conv1/W:0, group2/block22/conv2/W:0, group2/block22/conv3/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, fpn/lateral_1x1_c2/W:0, fpn/lateral_1x1_c3/W:0, fpn/lateral_1x1_c4/W:0, fpn/lateral_1x1_c5/W:0, fpn/posthoc_3x3_p2/W:0, fpn/posthoc_3x3_p3/W:0, fpn/posthoc_3x3_p4/W:0, fpn/posthoc_3x3_p5/W:0, rpn/conv0/W:0, rpn/class/W:0, rpn/box/W:0, cascade_rcnn_stage1/head/fc6/W:0, cascade_rcnn_stage1/head/fc7/W:0, cascade_rcnn_stage1/outputs/class/W:0, cascade_rcnn_stage1/outputs/box/W:0, cascade_rcnn_stage2/head/fc6/W:0, cascade_rcnn_stage2/head/fc7/W:0, cascade_rcnn_stage2/outputs/class/W:0, cascade_rcnn_stage2/outputs/box/W:0, cascade_rcnn_stage3/head/fc6/W:0, cascade_rcnn_stage3/head/fc7/W:0, cascade_rcnn_stage3/outputs/class/W:0, cascade_rcnn_stage3/outputs/box/W:0, maskrcnn/fcn0/W:0, maskrcnn/fcn1/W:0, maskrcnn/fcn2/W:0, maskrcnn/fcn3/W:0, maskrcnn/deconv/W:0, maskrcnn/conv/W:0
/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0403 05:54:47 @model_utils.py:67] List of Trainable Variables: 
name                                 shape                 #elements
-----------------------------------  ------------------  -----------
group1/block0/conv1/W                [1, 1, 256, 128]          32768
group1/block0/conv1/bn/gamma         [128]                       128
group1/block0/conv1/bn/beta          [128]                       128
group1/block0/conv2/W                [3, 3, 128, 128]         147456
group1/block0/conv2/bn/gamma         [128]                       128
group1/block0/conv2/bn/beta          [128]                       128
group1/block0/conv3/W                [1, 1, 128, 512]          65536
group1/block0/conv3/bn/gamma         [512]                       512
group1/block0/conv3/bn/beta          [512]                       512
group1/block0/convshortcut/W         [1, 1, 256, 512]         131072
group1/block0/convshortcut/bn/gamma  [512]                       512
group1/block0/convshortcut/bn/beta   [512]                       512
group1/block1/conv1/W                [1, 1, 512, 128]          65536
group1/block1/conv1/bn/gamma         [128]                       128
group1/block1/conv1/bn/beta          [128]                       128
group1/block1/conv2/W                [3, 3, 128, 128]         147456
group1/block1/conv2/bn/gamma         [128]                       128
group1/block1/conv2/bn/beta          [128]                       128
group1/block1/conv3/W                [1, 1, 128, 512]          65536
group1/block1/conv3/bn/gamma         [512]                       512
group1/block1/conv3/bn/beta          [512]                       512
group1/block2/conv1/W                [1, 1, 512, 128]          65536
group1/block2/conv1/bn/gamma         [128]                       128
group1/block2/conv1/bn/beta          [128]                       128
group1/block2/conv2/W                [3, 3, 128, 128]         147456
group1/block2/conv2/bn/gamma         [128]                       128
group1/block2/conv2/bn/beta          [128]                       128
group1/block2/conv3/W                [1, 1, 128, 512]          65536
group1/block2/conv3/bn/gamma         [512]                       512
group1/block2/conv3/bn/beta          [512]                       512
group1/block3/conv1/W                [1, 1, 512, 128]          65536
group1/block3/conv1/bn/gamma         [128]                       128
group1/block3/conv1/bn/beta          [128]                       128
group1/block3/conv2/W                [3, 3, 128, 128]         147456
group1/block3/conv2/bn/gamma         [128]                       128
group1/block3/conv2/bn/beta          [128]                       128
group1/block3/conv3/W                [1, 1, 128, 512]          65536
group1/block3/conv3/bn/gamma         [512]                       512
group1/block3/conv3/bn/beta          [512]                       512
group2/block0/conv1/W                [1, 1, 512, 256]         131072
group2/block0/conv1/bn/gamma         [256]                       256
group2/block0/conv1/bn/beta          [256]                       256
group2/block0/conv2/W                [3, 3, 256, 256]         589824
group2/block0/conv2/bn/gamma         [256]                       256
group2/block0/conv2/bn/beta          [256]                       256
group2/block0/conv3/W                [1, 1, 256, 1024]        262144
group2/block0/conv3/bn/gamma         [1024]                     1024
group2/block0/conv3/bn/beta          [1024]                     1024
group2/block0/convshortcut/W         [1, 1, 512, 1024]        524288
group2/block0/convshortcut/bn/gamma  [1024]                     1024
group2/block0/convshortcut/bn/beta   [1024]                     1024
group2/block1/conv1/W                [1, 1, 1024, 256]        262144
group2/block1/conv1/bn/gamma         [256]                       256
group2/block1/conv1/bn/beta          [256]                       256
group2/block1/conv2/W                [3, 3, 256, 256]         589824
group2/block1/conv2/bn/gamma         [256]                       256
group2/block1/conv2/bn/beta          [256]                       256
group2/block1/conv3/W                [1, 1, 256, 1024]        262144
group2/block1/conv3/bn/gamma         [1024]                     1024
group2/block1/conv3/bn/beta          [1024]                     1024
group2/block2/conv1/W                [1, 1, 1024, 256]        262144
group2/block2/conv1/bn/gamma         [256]                       256
group2/block2/conv1/bn/beta          [256]                       256
group2/block2/conv2/W                [3, 3, 256, 256]         589824
group2/block2/conv2/bn/gamma         [256]                       256
group2/block2/conv2/bn/beta          [256]                       256
group2/block2/conv3/W                [1, 1, 256, 1024]        262144
group2/block2/conv3/bn/gamma         [1024]                     1024
group2/block2/conv3/bn/beta          [1024]                     1024
group2/block3/conv1/W                [1, 1, 1024, 256]        262144
group2/block3/conv1/bn/gamma         [256]                       256
group2/block3/conv1/bn/beta          [256]                       256
group2/block3/conv2/W                [3, 3, 256, 256]         589824
group2/block3/conv2/bn/gamma         [256]                       256
group2/block3/conv2/bn/beta          [256]                       256
group2/block3/conv3/W                [1, 1, 256, 1024]        262144
group2/block3/conv3/bn/gamma         [1024]                     1024
group2/block3/conv3/bn/beta          [1024]                     1024
group2/block4/conv1/W                [1, 1, 1024, 256]        262144
group2/block4/conv1/bn/gamma         [256]                       256
group2/block4/conv1/bn/beta          [256]                       256
group2/block4/conv2/W                [3, 3, 256, 256]         589824
group2/block4/conv2/bn/gamma         [256]                       256
group2/block4/conv2/bn/beta          [256]                       256
group2/block4/conv3/W                [1, 1, 256, 1024]        262144
group2/block4/conv3/bn/gamma         [1024]                     1024
group2/block4/conv3/bn/beta          [1024]                     1024
group2/block5/conv1/W                [1, 1, 1024, 256]        262144
group2/block5/conv1/bn/gamma         [256]                       256
group2/block5/conv1/bn/beta          [256]                       256
group2/block5/conv2/W                [3, 3, 256, 256]         589824
group2/block5/conv2/bn/gamma         [256]                       256
group2/block5/conv2/bn/beta          [256]                       256
group2/block5/conv3/W                [1, 1, 256, 1024]        262144
group2/block5/conv3/bn/gamma         [1024]                     1024
group2/block5/conv3/bn/beta          [1024]                     1024
group2/block6/conv1/W                [1, 1, 1024, 256]        262144
group2/block6/conv1/bn/gamma         [256]                       256
group2/block6/conv1/bn/beta          [256]                       256
group2/block6/conv2/W                [3, 3, 256, 256]         589824
group2/block6/conv2/bn/gamma         [256]                       256
group2/block6/conv2/bn/beta          [256]                       256
group2/block6/conv3/W                [1, 1, 256, 1024]        262144
group2/block6/conv3/bn/gamma         [1024]                     1024
group2/block6/conv3/bn/beta          [1024]                     1024
group2/block7/conv1/W                [1, 1, 1024, 256]        262144
group2/block7/conv1/bn/gamma         [256]                       256
group2/block7/conv1/bn/beta          [256]                       256
group2/block7/conv2/W                [3, 3, 256, 256]         589824
group2/block7/conv2/bn/gamma         [256]                       256
group2/block7/conv2/bn/beta          [256]                       256
group2/block7/conv3/W                [1, 1, 256, 1024]        262144
group2/block7/conv3/bn/gamma         [1024]                     1024
group2/block7/conv3/bn/beta          [1024]                     1024
group2/block8/conv1/W                [1, 1, 1024, 256]        262144
group2/block8/conv1/bn/gamma         [256]                       256
group2/block8/conv1/bn/beta          [256]                       256
group2/block8/conv2/W                [3, 3, 256, 256]         589824
group2/block8/conv2/bn/gamma         [256]                       256
group2/block8/conv2/bn/beta          [256]                       256
group2/block8/conv3/W                [1, 1, 256, 1024]        262144
group2/block8/conv3/bn/gamma         [1024]                     1024
group2/block8/conv3/bn/beta          [1024]                     1024
group2/block9/conv1/W                [1, 1, 1024, 256]        262144
group2/block9/conv1/bn/gamma         [256]                       256
group2/block9/conv1/bn/beta          [256]                       256
group2/block9/conv2/W                [3, 3, 256, 256]         589824
group2/block9/conv2/bn/gamma         [256]                       256
group2/block9/conv2/bn/beta          [256]                       256
group2/block9/conv3/W                [1, 1, 256, 1024]        262144
group2/block9/conv3/bn/gamma         [1024]                     1024
group2/block9/conv3/bn/beta          [1024]                     1024
group2/block10/conv1/W               [1, 1, 1024, 256]        262144
group2/block10/conv1/bn/gamma        [256]                       256
group2/block10/conv1/bn/beta         [256]                       256
group2/block10/conv2/W               [3, 3, 256, 256]         589824
group2/block10/conv2/bn/gamma        [256]                       256
group2/block10/conv2/bn/beta         [256]                       256
group2/block10/conv3/W               [1, 1, 256, 1024]        262144
group2/block10/conv3/bn/gamma        [1024]                     1024
group2/block10/conv3/bn/beta         [1024]                     1024
group2/block11/conv1/W               [1, 1, 1024, 256]        262144
group2/block11/conv1/bn/gamma        [256]                       256
group2/block11/conv1/bn/beta         [256]                       256
group2/block11/conv2/W               [3, 3, 256, 256]         589824
group2/block11/conv2/bn/gamma        [256]                       256
group2/block11/conv2/bn/beta         [256]                       256
group2/block11/conv3/W               [1, 1, 256, 1024]        262144
group2/block11/conv3/bn/gamma        [1024]                     1024
group2/block11/conv3/bn/beta         [1024]                     1024
group2/block12/conv1/W               [1, 1, 1024, 256]        262144
group2/block12/conv1/bn/gamma        [256]                       256
group2/block12/conv1/bn/beta         [256]                       256
group2/block12/conv2/W               [3, 3, 256, 256]         589824
group2/block12/conv2/bn/gamma        [256]                       256
group2/block12/conv2/bn/beta         [256]                       256
group2/block12/conv3/W               [1, 1, 256, 1024]        262144
group2/block12/conv3/bn/gamma        [1024]                     1024
group2/block12/conv3/bn/beta         [1024]                     1024
group2/block13/conv1/W               [1, 1, 1024, 256]        262144
group2/block13/conv1/bn/gamma        [256]                       256
group2/block13/conv1/bn/beta         [256]                       256
group2/block13/conv2/W               [3, 3, 256, 256]         589824
group2/block13/conv2/bn/gamma        [256]                       256
group2/block13/conv2/bn/beta         [256]                       256
group2/block13/conv3/W               [1, 1, 256, 1024]        262144
group2/block13/conv3/bn/gamma        [1024]                     1024
group2/block13/conv3/bn/beta         [1024]                     1024
group2/block14/conv1/W               [1, 1, 1024, 256]        262144
group2/block14/conv1/bn/gamma        [256]                       256
group2/block14/conv1/bn/beta         [256]                       256
group2/block14/conv2/W               [3, 3, 256, 256]         589824
group2/block14/conv2/bn/gamma        [256]                       256
group2/block14/conv2/bn/beta         [256]                       256
group2/block14/conv3/W               [1, 1, 256, 1024]        262144
group2/block14/conv3/bn/gamma        [1024]                     1024
group2/block14/conv3/bn/beta         [1024]                     1024
group2/block15/conv1/W               [1, 1, 1024, 256]        262144
group2/block15/conv1/bn/gamma        [256]                       256
group2/block15/conv1/bn/beta         [256]                       256
group2/block15/conv2/W               [3, 3, 256, 256]         589824
group2/block15/conv2/bn/gamma        [256]                       256
group2/block15/conv2/bn/beta         [256]                       256
group2/block15/conv3/W               [1, 1, 256, 1024]        262144
group2/block15/conv3/bn/gamma        [1024]                     1024
group2/block15/conv3/bn/beta         [1024]                     1024
group2/block16/conv1/W               [1, 1, 1024, 256]        262144
group2/block16/conv1/bn/gamma        [256]                       256
group2/block16/conv1/bn/beta         [256]                       256
group2/block16/conv2/W               [3, 3, 256, 256]         589824
group2/block16/conv2/bn/gamma        [256]                       256
group2/block16/conv2/bn/beta         [256]                       256
group2/block16/conv3/W               [1, 1, 256, 1024]        262144
group2/block16/conv3/bn/gamma        [1024]                     1024
group2/block16/conv3/bn/beta         [1024]                     1024
group2/block17/conv1/W               [1, 1, 1024, 256]        262144
group2/block17/conv1/bn/gamma        [256]                       256
group2/block17/conv1/bn/beta         [256]                       256
group2/block17/conv2/W               [3, 3, 256, 256]         589824
group2/block17/conv2/bn/gamma        [256]                       256
group2/block17/conv2/bn/beta         [256]                       256
group2/block17/conv3/W               [1, 1, 256, 1024]        262144
group2/block17/conv3/bn/gamma        [1024]                     1024
group2/block17/conv3/bn/beta         [1024]                     1024
group2/block18/conv1/W               [1, 1, 1024, 256]        262144
group2/block18/conv1/bn/gamma        [256]                       256
group2/block18/conv1/bn/beta         [256]                       256
group2/block18/conv2/W               [3, 3, 256, 256]         589824
group2/block18/conv2/bn/gamma        [256]                       256
group2/block18/conv2/bn/beta         [256]                       256
group2/block18/conv3/W               [1, 1, 256, 1024]        262144
group2/block18/conv3/bn/gamma        [1024]                     1024
group2/block18/conv3/bn/beta         [1024]                     1024
group2/block19/conv1/W               [1, 1, 1024, 256]        262144
group2/block19/conv1/bn/gamma        [256]                       256
group2/block19/conv1/bn/beta         [256]                       256
group2/block19/conv2/W               [3, 3, 256, 256]         589824
group2/block19/conv2/bn/gamma        [256]                       256
group2/block19/conv2/bn/beta         [256]                       256
group2/block19/conv3/W               [1, 1, 256, 1024]        262144
group2/block19/conv3/bn/gamma        [1024]                     1024
group2/block19/conv3/bn/beta         [1024]                     1024
group2/block20/conv1/W               [1, 1, 1024, 256]        262144
group2/block20/conv1/bn/gamma        [256]                       256
group2/block20/conv1/bn/beta         [256]                       256
group2/block20/conv2/W               [3, 3, 256, 256]         589824
group2/block20/conv2/bn/gamma        [256]                       256
group2/block20/conv2/bn/beta         [256]                       256
group2/block20/conv3/W               [1, 1, 256, 1024]        262144
group2/block20/conv3/bn/gamma        [1024]                     1024
group2/block20/conv3/bn/beta         [1024]                     1024
group2/block21/conv1/W               [1, 1, 1024, 256]        262144
group2/block21/conv1/bn/gamma        [256]                       256
group2/block21/conv1/bn/beta         [256]                       256
group2/block21/conv2/W               [3, 3, 256, 256]         589824
group2/block21/conv2/bn/gamma        [256]                       256
group2/block21/conv2/bn/beta         [256]                       256
group2/block21/conv3/W               [1, 1, 256, 1024]        262144
group2/block21/conv3/bn/gamma        [1024]                     1024
group2/block21/conv3/bn/beta         [1024]                     1024
group2/block22/conv1/W               [1, 1, 1024, 256]        262144
group2/block22/conv1/bn/gamma        [256]                       256
group2/block22/conv1/bn/beta         [256]                       256
group2/block22/conv2/W               [3, 3, 256, 256]         589824
group2/block22/conv2/bn/gamma        [256]                       256
group2/block22/conv2/bn/beta         [256]                       256
group2/block22/conv3/W               [1, 1, 256, 1024]        262144
group2/block22/conv3/bn/gamma        [1024]                     1024
group2/block22/conv3/bn/beta         [1024]                     1024
group3/block0/conv1/W                [1, 1, 1024, 512]        524288
group3/block0/conv1/bn/gamma         [512]                       512
group3/block0/conv1/bn/beta          [512]                       512
group3/block0/conv2/W                [3, 3, 512, 512]        2359296
group3/block0/conv2/bn/gamma         [512]                       512
group3/block0/conv2/bn/beta          [512]                       512
group3/block0/conv3/W                [1, 1, 512, 2048]       1048576
group3/block0/conv3/bn/gamma         [2048]                     2048
group3/block0/conv3/bn/beta          [2048]                     2048
group3/block0/convshortcut/W         [1, 1, 1024, 2048]      2097152
group3/block0/convshortcut/bn/gamma  [2048]                     2048
group3/block0/convshortcut/bn/beta   [2048]                     2048
group3/block1/conv1/W                [1, 1, 2048, 512]       1048576
group3/block1/conv1/bn/gamma         [512]                       512
group3/block1/conv1/bn/beta          [512]                       512
group3/block1/conv2/W                [3, 3, 512, 512]        2359296
group3/block1/conv2/bn/gamma         [512]                       512
group3/block1/conv2/bn/beta          [512]                       512
group3/block1/conv3/W                [1, 1, 512, 2048]       1048576
group3/block1/conv3/bn/gamma         [2048]                     2048
group3/block1/conv3/bn/beta          [2048]                     2048
group3/block2/conv1/W                [1, 1, 2048, 512]       1048576
group3/block2/conv1/bn/gamma         [512]                       512
group3/block2/conv1/bn/beta          [512]                       512
group3/block2/conv2/W                [3, 3, 512, 512]        2359296
group3/block2/conv2/bn/gamma         [512]                       512
group3/block2/conv2/bn/beta          [512]                       512
group3/block2/conv3/W                [1, 1, 512, 2048]       1048576
group3/block2/conv3/bn/gamma         [2048]                     2048
group3/block2/conv3/bn/beta          [2048]                     2048
fpn/lateral_1x1_c2/W                 [1, 1, 256, 256]          65536
fpn/lateral_1x1_c2/b                 [256]                       256
fpn/lateral_1x1_c3/W                 [1, 1, 512, 256]         131072
fpn/lateral_1x1_c3/b                 [256]                       256
fpn/lateral_1x1_c4/W                 [1, 1, 1024, 256]        262144
fpn/lateral_1x1_c4/b                 [256]                       256
fpn/lateral_1x1_c5/W                 [1, 1, 2048, 256]        524288
fpn/lateral_1x1_c5/b                 [256]                       256
fpn/posthoc_3x3_p2/W                 [3, 3, 256, 256]         589824
fpn/posthoc_3x3_p2/b                 [256]                       256
fpn/posthoc_3x3_p3/W                 [3, 3, 256, 256]         589824
fpn/posthoc_3x3_p3/b                 [256]                       256
fpn/posthoc_3x3_p4/W                 [3, 3, 256, 256]         589824
fpn/posthoc_3x3_p4/b                 [256]                       256
fpn/posthoc_3x3_p5/W                 [3, 3, 256, 256]         589824
fpn/posthoc_3x3_p5/b                 [256]                       256
rpn/conv0/W                          [3, 3, 256, 256]         589824
rpn/conv0/b                          [256]                       256
rpn/class/W                          [1, 1, 256, 3]              768
rpn/class/b                          [3]                           3
rpn/box/W                            [1, 1, 256, 12]            3072
rpn/box/b                            [12]                         12
cascade_rcnn_stage1/head/fc6/W       [12544, 1024]          12845056
cascade_rcnn_stage1/head/fc6/b       [1024]                     1024
cascade_rcnn_stage1/head/fc7/W       [1024, 1024]            1048576
cascade_rcnn_stage1/head/fc7/b       [1024]                     1024
cascade_rcnn_stage1/outputs/class/W  [1024, 2]                  2048
cascade_rcnn_stage1/outputs/class/b  [2]                           2
cascade_rcnn_stage1/outputs/box/W    [1024, 4]                  4096
cascade_rcnn_stage1/outputs/box/b    [4]                           4
cascade_rcnn_stage2/head/fc6/W       [12544, 1024]          12845056
cascade_rcnn_stage2/head/fc6/b       [1024]                     1024
cascade_rcnn_stage2/head/fc7/W       [1024, 1024]            1048576
cascade_rcnn_stage2/head/fc7/b       [1024]                     1024
cascade_rcnn_stage2/outputs/class/W  [1024, 2]                  2048
cascade_rcnn_stage2/outputs/class/b  [2]                           2
cascade_rcnn_stage2/outputs/box/W    [1024, 4]                  4096
cascade_rcnn_stage2/outputs/box/b    [4]                           4
cascade_rcnn_stage3/head/fc6/W       [12544, 1024]          12845056
cascade_rcnn_stage3/head/fc6/b       [1024]                     1024
cascade_rcnn_stage3/head/fc7/W       [1024, 1024]            1048576
cascade_rcnn_stage3/head/fc7/b       [1024]                     1024
cascade_rcnn_stage3/outputs/class/W  [1024, 2]                  2048
cascade_rcnn_stage3/outputs/class/b  [2]                           2
cascade_rcnn_stage3/outputs/box/W    [1024, 4]                  4096
cascade_rcnn_stage3/outputs/box/b    [4]                           4
maskrcnn/fcn0/W                      [3, 3, 256, 256]         589824
maskrcnn/fcn0/b                      [256]                       256
maskrcnn/fcn1/W                      [3, 3, 256, 256]         589824
maskrcnn/fcn1/b                      [256]                       256
maskrcnn/fcn2/W                      [3, 3, 256, 256]         589824
maskrcnn/fcn2/b                      [256]                       256
maskrcnn/fcn3/W                      [3, 3, 256, 256]         589824
maskrcnn/fcn3/b                      [256]                       256
maskrcnn/deconv/W                    [2, 2, 256, 256]         262144
maskrcnn/deconv/b                    [256]                       256
maskrcnn/conv/W                      [1, 1, 256, 1]              256
maskrcnn/conv/b                      [1]                           1
Number of trainable variables: 337
Number of parameters (elements): 90541602
Storage space needed for all trainable variables: 345.39MB
[0403 05:54:47 @base.py:207] Setup callbacks graph ...
[0403 05:54:47 @argtools.py:138] WRN Starting a process with 'fork' method is not safe and may consume unnecessary extra CPU memory. Use 'forkserver' or 'spawn' method (available after Py3.4) instead if you run into any issues. See https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods on how to set them.
[0403 05:54:47 @argtools.py:138] WRN ""import prctl"" failed! Install python-prctl so that processes can be cleaned with guarantee.
[0403 05:54:54 @prof.py:291] [HostMemoryTracker] Free RAM in setup_graph() is 118.21 GB.
[0403 05:54:54 @tower.py:140] Building graph for predict tower 'tower-pred-0' on device /gpu:0 ...
[0403 05:55:01 @collection.py:152] Size of these collections were changed in tower-pred-0: (tf.GraphKeys.MODEL_VARIABLES: 285->340)
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0403 05:55:01 @coco.py:63] Instances loaded from /home/madhav3101/cascade/train_dataset/coco/annotations/instances_val2014.json.
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:00<00:00, 341981.62it/s]
[0403 05:55:01 @timer.py:45] Load annotations for instances_val2014.json finished, time:0.0015 sec.
[0403 05:55:01 @data.py:379] Found 238 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0403 05:55:01 @coco.py:63] Instances loaded from /home/madhav3101/cascade/train_dataset/coco/annotations/instances_val2014.json.
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:00<00:00, 483879.96it/s]
[0403 05:55:01 @timer.py:45] Load annotations for instances_val2014.json finished, time:0.0009 sec.
[0403 05:55:01 @data.py:379] Found 238 images for inference.
[0403 05:55:01 @summary.py:47] [MovingAverageSummary] 98 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.
[0403 05:55:01 @summary.py:94] Summarizing collection 'summaries' of size 101.
[0403 05:55:01 @base.py:228] Creating the session ...
2020-04-03 05:55:01.667392: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-03 05:55:01.690823: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400020000 Hz
2020-04-03 05:55:01.691259: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c51ad7aa30 executing computations on platform Host. Devices:
2020-04-03 05:55:01.691279: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2020-04-03 05:55:01.801783: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c51ad9c0c0 executing computations on platform CUDA. Devices:
2020-04-03 05:55:01.801818: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-04-03 05:55:01.801951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:02:00.0
totalMemory: 10.76GiB freeMemory: 10.60GiB
2020-04-03 05:55:01.801971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2020-04-03 05:55:01.802857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-04-03 05:55:01.802872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2020-04-03 05:55:01.802879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2020-04-03 05:55:01.802941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10909 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5)
[0403 05:55:09 @base.py:234] Initializing the session ...
[0403 05:55:09 @sessinit.py:223] Variables to restore from dict: conv0/W, conv0/bn/gamma, conv0/bn/beta, conv0/bn/mean/EMA, conv0/bn/variance/EMA, group0/block0/conv1/W, group0/block0/conv1/bn/gamma, group0/block0/conv1/bn/beta, group0/block0/conv1/bn/mean/EMA, group0/block0/conv1/bn/variance/EMA, group0/block0/conv2/W, group0/block0/conv2/bn/gamma, group0/block0/conv2/bn/beta, group0/block0/conv2/bn/mean/EMA, group0/block0/conv2/bn/variance/EMA, group0/block0/conv3/W, group0/block0/conv3/bn/gamma, group0/block0/conv3/bn/beta, group0/block0/conv3/bn/mean/EMA, group0/block0/conv3/bn/variance/EMA, group0/block0/convshortcut/W, group0/block0/convshortcut/bn/gamma, group0/block0/convshortcut/bn/beta, group0/block0/convshortcut/bn/mean/EMA, group0/block0/convshortcut/bn/variance/EMA, group0/block1/conv1/W, group0/block1/conv1/bn/gamma, group0/block1/conv1/bn/beta, group0/block1/conv1/bn/mean/EMA, group0/block1/conv1/bn/variance/EMA, group0/block1/conv2/W, group0/block1/conv2/bn/gamma, group0/block1/conv2/bn/beta, group0/block1/conv2/bn/mean/EMA, group0/block1/conv2/bn/variance/EMA, group0/block1/conv3/W, group0/block1/conv3/bn/gamma, group0/block1/conv3/bn/beta, group0/block1/conv3/bn/mean/EMA, group0/block1/conv3/bn/variance/EMA, group0/block2/conv1/W, group0/block2/conv1/bn/gamma, group0/block2/conv1/bn/beta, group0/block2/conv1/bn/mean/EMA, group0/block2/conv1/bn/variance/EMA, group0/block2/conv2/W, group0/block2/conv2/bn/gamma, group0/block2/conv2/bn/beta, group0/block2/conv2/bn/mean/EMA, group0/block2/conv2/bn/variance/EMA, group0/block2/conv3/W, group0/block2/conv3/bn/gamma, group0/block2/conv3/bn/beta, group0/block2/conv3/bn/mean/EMA, group0/block2/conv3/bn/variance/EMA, group1/block0/conv1/W, group1/block0/conv1/bn/gamma, group1/block0/conv1/bn/beta, group1/block0/conv1/bn/mean/EMA, group1/block0/conv1/bn/variance/EMA, group1/block0/conv2/W, group1/block0/conv2/bn/gamma, group1/block0/conv2/bn/beta, group1/block0/conv2/bn/mean/EMA, group1/block0/conv2/bn/variance/EMA, group1/block0/conv3/W, group1/block0/conv3/bn/gamma, group1/block0/conv3/bn/beta, group1/block0/conv3/bn/mean/EMA, group1/block0/conv3/bn/variance/EMA, group1/block0/convshortcut/W, group1/block0/convshortcut/bn/gamma, group1/block0/convshortcut/bn/beta, group1/block0/convshortcut/bn/mean/EMA, group1/block0/convshortcut/bn/variance/EMA, group1/block1/conv1/W, group1/block1/conv1/bn/gamma, group1/block1/conv1/bn/beta, group1/block1/conv1/bn/mean/EMA, group1/block1/conv1/bn/variance/EMA, group1/block1/conv2/W, group1/block1/conv2/bn/gamma, group1/block1/conv2/bn/beta, group1/block1/conv2/bn/mean/EMA, group1/block1/conv2/bn/variance/EMA, group1/block1/conv3/W, group1/block1/conv3/bn/gamma, group1/block1/conv3/bn/beta, group1/block1/conv3/bn/mean/EMA, group1/block1/conv3/bn/variance/EMA, group1/block2/conv1/W, group1/block2/conv1/bn/gamma, group1/block2/conv1/bn/beta, group1/block2/conv1/bn/mean/EMA, group1/block2/conv1/bn/variance/EMA, group1/block2/conv2/W, group1/block2/conv2/bn/gamma, group1/block2/conv2/bn/beta, group1/block2/conv2/bn/mean/EMA, group1/block2/conv2/bn/variance/EMA, group1/block2/conv3/W, group1/block2/conv3/bn/gamma, group1/block2/conv3/bn/beta, group1/block2/conv3/bn/mean/EMA, group1/block2/conv3/bn/variance/EMA, group1/block3/conv1/W, group1/block3/conv1/bn/gamma, group1/block3/conv1/bn/beta, group1/block3/conv1/bn/mean/EMA, group1/block3/conv1/bn/variance/EMA, group1/block3/conv2/W, group1/block3/conv2/bn/gamma, group1/block3/conv2/bn/beta, group1/block3/conv2/bn/mean/EMA, group1/block3/conv2/bn/variance/EMA, group1/block3/conv3/W, group1/block3/conv3/bn/gamma, group1/block3/conv3/bn/beta, group1/block3/conv3/bn/mean/EMA, group1/block3/conv3/bn/variance/EMA, group2/block0/conv1/W, group2/block0/conv1/bn/gamma, group2/block0/conv1/bn/beta, group2/block0/conv1/bn/mean/EMA, group2/block0/conv1/bn/variance/EMA, group2/block0/conv2/W, group2/block0/conv2/bn/gamma, group2/block0/conv2/bn/beta, group2/block0/conv2/bn/mean/EMA, group2/block0/conv2/bn/variance/EMA, group2/block0/conv3/W, group2/block0/conv3/bn/gamma, group2/block0/conv3/bn/beta, group2/block0/conv3/bn/mean/EMA, group2/block0/conv3/bn/variance/EMA, group2/block0/convshortcut/W, group2/block0/convshortcut/bn/gamma, group2/block0/convshortcut/bn/beta, group2/block0/convshortcut/bn/mean/EMA, group2/block0/convshortcut/bn/variance/EMA, group2/block1/conv1/W, group2/block1/conv1/bn/gamma, group2/block1/conv1/bn/beta, group2/block1/conv1/bn/mean/EMA, group2/block1/conv1/bn/variance/EMA, group2/block1/conv2/W, group2/block1/conv2/bn/gamma, group2/block1/conv2/bn/beta, group2/block1/conv2/bn/mean/EMA, group2/block1/conv2/bn/variance/EMA, group2/block1/conv3/W, group2/block1/conv3/bn/gamma, group2/block1/conv3/bn/beta, group2/block1/conv3/bn/mean/EMA, group2/block1/conv3/bn/variance/EMA, group2/block2/conv1/W, group2/block2/conv1/bn/gamma, group2/block2/conv1/bn/beta, group2/block2/conv1/bn/mean/EMA, group2/block2/conv1/bn/variance/EMA, group2/block2/conv2/W, group2/block2/conv2/bn/gamma, group2/block2/conv2/bn/beta, group2/block2/conv2/bn/mean/EMA, group2/block2/conv2/bn/variance/EMA, group2/block2/conv3/W, group2/block2/conv3/bn/gamma, group2/block2/conv3/bn/beta, group2/block2/conv3/bn/mean/EMA, group2/block2/conv3/bn/variance/EMA, group2/block3/conv1/W, group2/block3/conv1/bn/gamma, group2/block3/conv1/bn/beta, group2/block3/conv1/bn/mean/EMA, group2/block3/conv1/bn/variance/EMA, group2/block3/conv2/W, group2/block3/conv2/bn/gamma, group2/block3/conv2/bn/beta, group2/block3/conv2/bn/mean/EMA, group2/block3/conv2/bn/variance/EMA, group2/block3/conv3/W, group2/block3/conv3/bn/gamma, group2/block3/conv3/bn/beta, group2/block3/conv3/bn/mean/EMA, group2/block3/conv3/bn/variance/EMA, group2/block4/conv1/W, group2/block4/conv1/bn/gamma, group2/block4/conv1/bn/beta, group2/block4/conv1/bn/mean/EMA, group2/block4/conv1/bn/variance/EMA, group2/block4/conv2/W, group2/block4/conv2/bn/gamma, group2/block4/conv2/bn/beta, group2/block4/conv2/bn/mean/EMA, group2/block4/conv2/bn/variance/EMA, group2/block4/conv3/W, group2/block4/conv3/bn/gamma, group2/block4/conv3/bn/beta, group2/block4/conv3/bn/mean/EMA, group2/block4/conv3/bn/variance/EMA, group2/block5/conv1/W, group2/block5/conv1/bn/gamma, group2/block5/conv1/bn/beta, group2/block5/conv1/bn/mean/EMA, group2/block5/conv1/bn/variance/EMA, group2/block5/conv2/W, group2/block5/conv2/bn/gamma, group2/block5/conv2/bn/beta, group2/block5/conv2/bn/mean/EMA, group2/block5/conv2/bn/variance/EMA, group2/block5/conv3/W, group2/block5/conv3/bn/gamma, group2/block5/conv3/bn/beta, group2/block5/conv3/bn/mean/EMA, group2/block5/conv3/bn/variance/EMA, group2/block6/conv1/W, group2/block6/conv1/bn/gamma, group2/block6/conv1/bn/beta, group2/block6/conv1/bn/mean/EMA, group2/block6/conv1/bn/variance/EMA, group2/block6/conv2/W, group2/block6/conv2/bn/gamma, group2/block6/conv2/bn/beta, group2/block6/conv2/bn/mean/EMA, group2/block6/conv2/bn/variance/EMA, group2/block6/conv3/W, group2/block6/conv3/bn/gamma, group2/block6/conv3/bn/beta, group2/block6/conv3/bn/mean/EMA, group2/block6/conv3/bn/variance/EMA, group2/block7/conv1/W, group2/block7/conv1/bn/gamma, group2/block7/conv1/bn/beta, group2/block7/conv1/bn/mean/EMA, group2/block7/conv1/bn/variance/EMA, group2/block7/conv2/W, group2/block7/conv2/bn/gamma, group2/block7/conv2/bn/beta, group2/block7/conv2/bn/mean/EMA, group2/block7/conv2/bn/variance/EMA, group2/block7/conv3/W, group2/block7/conv3/bn/gamma, group2/block7/conv3/bn/beta, group2/block7/conv3/bn/mean/EMA, group2/block7/conv3/bn/variance/EMA, group2/block8/conv1/W, group2/block8/conv1/bn/gamma, group2/block8/conv1/bn/beta, group2/block8/conv1/bn/mean/EMA, group2/block8/conv1/bn/variance/EMA, group2/block8/conv2/W, group2/block8/conv2/bn/gamma, group2/block8/conv2/bn/beta, group2/block8/conv2/bn/mean/EMA, group2/block8/conv2/bn/variance/EMA, group2/block8/conv3/W, group2/block8/conv3/bn/gamma, group2/block8/conv3/bn/beta, group2/block8/conv3/bn/mean/EMA, group2/block8/conv3/bn/variance/EMA, group2/block9/conv1/W, group2/block9/conv1/bn/gamma, group2/block9/conv1/bn/beta, group2/block9/conv1/bn/mean/EMA, group2/block9/conv1/bn/variance/EMA, group2/block9/conv2/W, group2/block9/conv2/bn/gamma, group2/block9/conv2/bn/beta, group2/block9/conv2/bn/mean/EMA, group2/block9/conv2/bn/variance/EMA, group2/block9/conv3/W, group2/block9/conv3/bn/gamma, group2/block9/conv3/bn/beta, group2/block9/conv3/bn/mean/EMA, group2/block9/conv3/bn/variance/EMA, group2/block10/conv1/W, group2/block10/conv1/bn/gamma, group2/block10/conv1/bn/beta, group2/block10/conv1/bn/mean/EMA, group2/block10/conv1/bn/variance/EMA, group2/block10/conv2/W, group2/block10/conv2/bn/gamma, group2/block10/conv2/bn/beta, group2/block10/conv2/bn/mean/EMA, group2/block10/conv2/bn/variance/EMA, group2/block10/conv3/W, group2/block10/conv3/bn/gamma, group2/block10/conv3/bn/beta, group2/block10/conv3/bn/mean/EMA, group2/block10/conv3/bn/variance/EMA, group2/block11/conv1/W, group2/block11/conv1/bn/gamma, group2/block11/conv1/bn/beta, group2/block11/conv1/bn/mean/EMA, group2/block11/conv1/bn/variance/EMA, group2/block11/conv2/W, group2/block11/conv2/bn/gamma, group2/block11/conv2/bn/beta, group2/block11/conv2/bn/mean/EMA, group2/block11/conv2/bn/variance/EMA, group2/block11/conv3/W, group2/block11/conv3/bn/gamma, group2/block11/conv3/bn/beta, group2/block11/conv3/bn/mean/EMA, group2/block11/conv3/bn/variance/EMA, group2/block12/conv1/W, group2/block12/conv1/bn/gamma, group2/block12/conv1/bn/beta, group2/block12/conv1/bn/mean/EMA, group2/block12/conv1/bn/variance/EMA, group2/block12/conv2/W, group2/block12/conv2/bn/gamma, group2/block12/conv2/bn/beta, group2/block12/conv2/bn/mean/EMA, group2/block12/conv2/bn/variance/EMA, group2/block12/conv3/W, group2/block12/conv3/bn/gamma, group2/block12/conv3/bn/beta, group2/block12/conv3/bn/mean/EMA, group2/block12/conv3/bn/variance/EMA, group2/block13/conv1/W, group2/block13/conv1/bn/gamma, group2/block13/conv1/bn/beta, group2/block13/conv1/bn/mean/EMA, group2/block13/conv1/bn/variance/EMA, group2/block13/conv2/W, group2/block13/conv2/bn/gamma, group2/block13/conv2/bn/beta, group2/block13/conv2/bn/mean/EMA, group2/block13/conv2/bn/variance/EMA, group2/block13/conv3/W, group2/block13/conv3/bn/gamma, group2/block13/conv3/bn/beta, group2/block13/conv3/bn/mean/EMA, group2/block13/conv3/bn/variance/EMA, group2/block14/conv1/W, group2/block14/conv1/bn/gamma, group2/block14/conv1/bn/beta, group2/block14/conv1/bn/mean/EMA, group2/block14/conv1/bn/variance/EMA, group2/block14/conv2/W, group2/block14/conv2/bn/gamma, group2/block14/conv2/bn/beta, group2/block14/conv2/bn/mean/EMA, group2/block14/conv2/bn/variance/EMA, group2/block14/conv3/W, group2/block14/conv3/bn/gamma, group2/block14/conv3/bn/beta, group2/block14/conv3/bn/mean/EMA, group2/block14/conv3/bn/variance/EMA, group2/block15/conv1/W, group2/block15/conv1/bn/gamma, group2/block15/conv1/bn/beta, group2/block15/conv1/bn/mean/EMA, group2/block15/conv1/bn/variance/EMA, group2/block15/conv2/W, group2/block15/conv2/bn/gamma, group2/block15/conv2/bn/beta, group2/block15/conv2/bn/mean/EMA, group2/block15/conv2/bn/variance/EMA, group2/block15/conv3/W, group2/block15/conv3/bn/gamma, group2/block15/conv3/bn/beta, group2/block15/conv3/bn/mean/EMA, group2/block15/conv3/bn/variance/EMA, group2/block16/conv1/W, group2/block16/conv1/bn/gamma, group2/block16/conv1/bn/beta, group2/block16/conv1/bn/mean/EMA, group2/block16/conv1/bn/variance/EMA, group2/block16/conv2/W, group2/block16/conv2/bn/gamma, group2/block16/conv2/bn/beta, group2/block16/conv2/bn/mean/EMA, group2/block16/conv2/bn/variance/EMA, group2/block16/conv3/W, group2/block16/conv3/bn/gamma, group2/block16/conv3/bn/beta, group2/block16/conv3/bn/mean/EMA, group2/block16/conv3/bn/variance/EMA, group2/block17/conv1/W, group2/block17/conv1/bn/gamma, group2/block17/conv1/bn/beta, group2/block17/conv1/bn/mean/EMA, group2/block17/conv1/bn/variance/EMA, group2/block17/conv2/W, group2/block17/conv2/bn/gamma, group2/block17/conv2/bn/beta, group2/block17/conv2/bn/mean/EMA, group2/block17/conv2/bn/variance/EMA, group2/block17/conv3/W, group2/block17/conv3/bn/gamma, group2/block17/conv3/bn/beta, group2/block17/conv3/bn/mean/EMA, group2/block17/conv3/bn/variance/EMA, group2/block18/conv1/W, group2/block18/conv1/bn/gamma, group2/block18/conv1/bn/beta, group2/block18/conv1/bn/mean/EMA, group2/block18/conv1/bn/variance/EMA, group2/block18/conv2/W, group2/block18/conv2/bn/gamma, group2/block18/conv2/bn/beta, group2/block18/conv2/bn/mean/EMA, group2/block18/conv2/bn/variance/EMA, group2/block18/conv3/W, group2/block18/conv3/bn/gamma, group2/block18/conv3/bn/beta, group2/block18/conv3/bn/mean/EMA, group2/block18/conv3/bn/variance/EMA, group2/block19/conv1/W, group2/block19/conv1/bn/gamma, group2/block19/conv1/bn/beta, group2/block19/conv1/bn/mean/EMA, group2/block19/conv1/bn/variance/EMA, group2/block19/conv2/W, group2/block19/conv2/bn/gamma, group2/block19/conv2/bn/beta, group2/block19/conv2/bn/mean/EMA, group2/block19/conv2/bn/variance/EMA, group2/block19/conv3/W, group2/block19/conv3/bn/gamma, group2/block19/conv3/bn/beta, group2/block19/conv3/bn/mean/EMA, group2/block19/conv3/bn/variance/EMA, group2/block20/conv1/W, group2/block20/conv1/bn/gamma, group2/block20/conv1/bn/beta, group2/block20/conv1/bn/mean/EMA, group2/block20/conv1/bn/variance/EMA, group2/block20/conv2/W, group2/block20/conv2/bn/gamma, group2/block20/conv2/bn/beta, group2/block20/conv2/bn/mean/EMA, group2/block20/conv2/bn/variance/EMA, group2/block20/conv3/W, group2/block20/conv3/bn/gamma, group2/block20/conv3/bn/beta, group2/block20/conv3/bn/mean/EMA, group2/block20/conv3/bn/variance/EMA, group2/block21/conv1/W, group2/block21/conv1/bn/gamma, group2/block21/conv1/bn/beta, group2/block21/conv1/bn/mean/EMA, group2/block21/conv1/bn/variance/EMA, group2/block21/conv2/W, group2/block21/conv2/bn/gamma, group2/block21/conv2/bn/beta, group2/block21/conv2/bn/mean/EMA, group2/block21/conv2/bn/variance/EMA, group2/block21/conv3/W, group2/block21/conv3/bn/gamma, group2/block21/conv3/bn/beta, group2/block21/conv3/bn/mean/EMA, group2/block21/conv3/bn/variance/EMA, group2/block22/conv1/W, group2/block22/conv1/bn/gamma, group2/block22/conv1/bn/beta, group2/block22/conv1/bn/mean/EMA, group2/block22/conv1/bn/variance/EMA, group2/block22/conv2/W, group2/block22/conv2/bn/gamma, group2/block22/conv2/bn/beta, group2/block22/conv2/bn/mean/EMA, group2/block22/conv2/bn/variance/EMA, group2/block22/conv3/W, group2/block22/conv3/bn/gamma, group2/block22/conv3/bn/beta, group2/block22/conv3/bn/mean/EMA, group2/block22/conv3/bn/variance/EMA, group3/block0/conv1/W, group3/block0/conv1/bn/gamma, group3/block0/conv1/bn/beta, group3/block0/conv1/bn/mean/EMA, group3/block0/conv1/bn/variance/EMA, group3/block0/conv2/W, group3/block0/conv2/bn/gamma, group3/block0/conv2/bn/beta, group3/block0/conv2/bn/mean/EMA, group3/block0/conv2/bn/variance/EMA, group3/block0/conv3/W, group3/block0/conv3/bn/gamma, group3/block0/conv3/bn/beta, group3/block0/conv3/bn/mean/EMA, group3/block0/conv3/bn/variance/EMA, group3/block0/convshortcut/W, group3/block0/convshortcut/bn/gamma, group3/block0/convshortcut/bn/beta, group3/block0/convshortcut/bn/mean/EMA, group3/block0/convshortcut/bn/variance/EMA, group3/block1/conv1/W, group3/block1/conv1/bn/gamma, group3/block1/conv1/bn/beta, group3/block1/conv1/bn/mean/EMA, group3/block1/conv1/bn/variance/EMA, group3/block1/conv2/W, group3/block1/conv2/bn/gamma, group3/block1/conv2/bn/beta, group3/block1/conv2/bn/mean/EMA, group3/block1/conv2/bn/variance/EMA, group3/block1/conv3/W, group3/block1/conv3/bn/gamma, group3/block1/conv3/bn/beta, group3/block1/conv3/bn/mean/EMA, group3/block1/conv3/bn/variance/EMA, group3/block2/conv1/W, group3/block2/conv1/bn/gamma, group3/block2/conv1/bn/beta, group3/block2/conv1/bn/mean/EMA, group3/block2/conv1/bn/variance/EMA, group3/block2/conv2/W, group3/block2/conv2/bn/gamma, group3/block2/conv2/bn/beta, group3/block2/conv2/bn/mean/EMA, group3/block2/conv2/bn/variance/EMA, group3/block2/conv3/W, group3/block2/conv3/bn/gamma, group3/block2/conv3/bn/beta, group3/block2/conv3/bn/mean/EMA, group3/block2/conv3/bn/variance/EMA, fpn/lateral_1x1_c2/W, fpn/lateral_1x1_c2/b, fpn/lateral_1x1_c3/W, fpn/lateral_1x1_c3/b, fpn/lateral_1x1_c4/W, fpn/lateral_1x1_c4/b, fpn/lateral_1x1_c5/W, fpn/lateral_1x1_c5/b, fpn/posthoc_3x3_p2/W, fpn/posthoc_3x3_p2/b, fpn/posthoc_3x3_p3/W, fpn/posthoc_3x3_p3/b, fpn/posthoc_3x3_p4/W, fpn/posthoc_3x3_p4/b, fpn/posthoc_3x3_p5/W, fpn/posthoc_3x3_p5/b, rpn/conv0/W, rpn/conv0/b, rpn/class/W, rpn/class/b, rpn/box/W, rpn/box/b, cascade_rcnn_stage1/head/fc6/W, cascade_rcnn_stage1/head/fc6/b, cascade_rcnn_stage1/head/fc7/W, cascade_rcnn_stage1/head/fc7/b, cascade_rcnn_stage1/outputs/class/W, cascade_rcnn_stage1/outputs/class/b, cascade_rcnn_stage1/outputs/box/W, cascade_rcnn_stage1/outputs/box/b, cascade_rcnn_stage2/head/fc6/W, cascade_rcnn_stage2/head/fc6/b, cascade_rcnn_stage2/head/fc7/W, cascade_rcnn_stage2/head/fc7/b, cascade_rcnn_stage2/outputs/class/W, cascade_rcnn_stage2/outputs/class/b, cascade_rcnn_stage2/outputs/box/W, cascade_rcnn_stage2/outputs/box/b, cascade_rcnn_stage3/head/fc6/W, cascade_rcnn_stage3/head/fc6/b, cascade_rcnn_stage3/head/fc7/W, cascade_rcnn_stage3/head/fc7/b, cascade_rcnn_stage3/outputs/class/W, cascade_rcnn_stage3/outputs/class/b, cascade_rcnn_stage3/outputs/box/W, cascade_rcnn_stage3/outputs/box/b, maskrcnn/fcn0/W, maskrcnn/fcn0/b, maskrcnn/fcn1/W, maskrcnn/fcn1/b, maskrcnn/fcn2/W, maskrcnn/fcn2/b, maskrcnn/fcn3/W, maskrcnn/fcn3/b, maskrcnn/deconv/W, maskrcnn/deconv/b, maskrcnn/conv/W, maskrcnn/conv/b
[0403 05:55:09 @sessinit.py:87] WRN The following variables are in the graph, but not found in the dict: global_step, learning_rate
[0403 05:55:09 @sessinit.py:236] Restoring 578 variables from dict ...
[0403 05:55:09 @varmanip.py:80] WRN Cannot load an array of shape (1, 1, 256, 80) into variable 'maskrcnn/conv/W' whose shape is (1, 1, 256, 1).
[0403 05:55:09 @varmanip.py:80] WRN Cannot load an array of shape (81,) into variable 'cascade_rcnn_stage2/outputs/class/b' whose shape is (2,).
[0403 05:55:09 @varmanip.py:80] WRN Cannot load an array of shape (81,) into variable 'cascade_rcnn_stage1/outputs/class/b' whose shape is (2,).
[0403 05:55:09 @varmanip.py:80] WRN Cannot load an array of shape (1024, 81) into variable 'cascade_rcnn_stage2/outputs/class/W' whose shape is (1024, 2).
[0403 05:55:09 @varmanip.py:80] WRN Cannot load an array of shape (1024, 81) into variable 'cascade_rcnn_stage1/outputs/class/W' whose shape is (1024, 2).
[0403 05:55:09 @varmanip.py:80] WRN Cannot load an array of shape (1024, 81) into variable 'cascade_rcnn_stage3/outputs/class/W' whose shape is (1024, 2).
[0403 05:55:09 @varmanip.py:80] WRN Cannot load an array of shape (81,) into variable 'cascade_rcnn_stage3/outputs/class/b' whose shape is (2,).
[0403 05:55:09 @varmanip.py:80] WRN Cannot load an array of shape (80,) into variable 'maskrcnn/conv/b' whose shape is (1,).
[0403 05:55:10 @base.py:241] Graph Finalized.
[0403 05:55:10 @concurrency.py:37] Starting EnqueueThread QueueInput/input_queue ...
[0403 05:55:16 @param.py:158] [HyperParamSetter] At global_step=0, learning_rate is set to 0.003300
[0403 05:55:17 @param.py:158] [HyperParamSetter] At global_step=0, learning_rate is set to 0.010000
[0403 05:55:17 @prof.py:294] [HostMemoryTracker] Free RAM in before_train() is 110.16 GB.
[0403 05:55:17 @eval.py:259] [EvalCallback] Will evaluate every 50 epochs
[0403 05:55:18 @base.py:273] Start Epoch 1 ...
  0%|                                                                                                                   |0/1000[00:00<?,?it/s]2020-04-03 05:55:35.986967: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
  0%|                                                                                                                   |0/1000[00:18<?,?it/s]
2020-04-03 05:55:36.864532: W tensorflow/core/kernels/queue_base.cc:277] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed
Traceback (most recent call last):
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 336], but got 400
	 [[{{node tower0/FPN_slice_lvl0/narrow_to/Slice_1}}]]
	 [[{{node tower0/sample_fast_rcnn_targets/proposal_metrics/Max}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""examples/FasterRCNN/train.py"", line 135, in <module>
    launch_train_with_config(traincfg, trainer)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/interface.py"", line 101, in launch_train_with_config
    extra_callbacks=config.extra_callbacks)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/base.py"", line 342, in train_with_defaults
    steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/base.py"", line 314, in train
    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/utils/argtools.py"", line 168, in wrapper
    return func(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/base.py"", line 279, in main_loop
    self.run_step()  # implemented by subclass
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/base.py"", line 179, in run_step
    self.hooked_sess.run(self.train_op)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 676, in run
    run_metadata=run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1171, in run
    run_metadata=run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1270, in run
    raise six.reraise(*original_exc_info)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1327, in run
    run_metadata=run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1091, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 336], but got 400
	 [[node tower0/FPN_slice_lvl0/narrow_to/Slice_1 (defined at /home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_box.py:193) ]]
	 [[node tower0/sample_fast_rcnn_targets/proposal_metrics/Max (defined at /home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_frcnn.py:29) ]]

Caused by op 'tower0/FPN_slice_lvl0/narrow_to/Slice_1', defined at:
  File ""examples/FasterRCNN/train.py"", line 135, in <module>
    launch_train_with_config(traincfg, trainer)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/interface.py"", line 91, in launch_train_with_config
    model.build_graph, model.get_optimizer)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/utils/argtools.py"", line 168, in wrapper
    return func(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/tower.py"", line 224, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/trainers.py"", line 190, in _setup_graph
    grad_list = self._builder.call_for_each_tower(tower_fn)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/graph_builder/training.py"", line 225, in call_for_each_tower
    use_vs=[False] + [True] * (len(self.towers) - 1))
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/graph_builder/training.py"", line 121, in build_on_towers
    return DataParallelBuilder.call_for_each_tower(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/graph_builder/training.py"", line 115, in call_for_each_tower
    ret.append(func())
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/tower.py"", line 281, in get_grad_fn
    return compute_grad_from_inputs(*inputs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/tower.py"", line 255, in compute_grad_from_inputs
    cost = get_cost_fn(*inputs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/tfutils/tower.py"", line 291, in __call__
    output = self._tower_fn(*args)
  File ""/home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/generalized_rcnn.py"", line 69, in build_graph
    proposals, rpn_losses = self.rpn(image, features, anchor_inputs)  # inputs?
  File ""/home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/generalized_rcnn.py"", line 244, in rpn
    self.slice_feature_and_anchors(features, multilevel_anchors)
  File ""/home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/generalized_rcnn.py"", line 224, in slice_feature_and_anchors
    anchors[i] = anchors[i].narrow_to(p23456[i])
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/tfutils/scope_utils.py"", line 110, in wrapper
    return func(*args, **kwargs)
  File ""/home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_box.py"", line 193, in narrow_to
    gt_labels = tf.slice(self.gt_labels, [0, 0, 0], slice3d)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 707, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 8236, in _slice
    ""Slice"", input=input, begin=begin, size=size, name=name)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Expected size[0] in [0, 336], but got 400
	 [[node tower0/FPN_slice_lvl0/narrow_to/Slice_1 (defined at /home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_box.py:193) ]]
	 [[node tower0/sample_fast_rcnn_targets/proposal_metrics/Max (defined at /home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_frcnn.py:29) ]]

MultiProcessMapDataZMQ successfully cleaned-up.

```",made code none also made line function self assert ret top bottom left right color ret ret top bottom left right ret ret return ret making change model taking square image still getting error output log file load command return found environment information python default compiler version support true support false driver ti free ram count false false false false true true true true true warm schedule value schedule value loading memory done index index loaded load finished time sec category distribution class box table total contain total training total training set loading dictionary setting queue building graph training tower device layer training input output input output input output input output input output input output input output input output input output found regularize following converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown list trainable name shape number trainable number storage space trainable setup graph starting process method safe may consume unnecessary extra memory use method available instead run see set import install guarantee free ram building graph predict tower device size loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference collection run session collection size session binary use frequency service platform host device undefined undefined service platform device ti compute capability found device name ti major minor visible device interconnect strength edge matrix device memory physical device name ti bus id compute capability session restore following graph found load array shape variable whose shape load array shape variable whose shape load array shape variable whose shape load array shape variable whose shape load array shape variable whose shape load array shape variable whose shape load array shape variable whose shape load array shape variable whose shape graph starting set set free ram evaluate every start epoch successfully library locally skipping attempt queue closed recent call last file line return file line file line size got node node handling exception another exception recent call last file line module trainer file line file line file line train file line wrapper return file line subclass file line file line run file line run file line run raise file line reraise raise value file line run return file line run file line run return file line run file line file line file line raise type message size got node defined node defined defined file line module trainer file line file line wrapper return file line input file line file line false true file line return file line file line return file line cost file line output file line image file line file line file line wrapper return file line sliced file line slice return begin size file line slice file line file line return file line file line see size got node defined node defined successfully,issue,positive,positive,neutral,neutral,positive,positive
608135063,"You can implement your own rsize logic at
https://github.com/tensorpack/tensorpack/blob/031e698d3fa0c9afc60d981c3dbd0b1502e92e0c/examples/FasterRCNN/data.py#L76

On Thu, Apr 2, 2020 at 3:52 PM Madhav Agarwal <notifications@github.com>
wrote:

> @ppwwyyxx <https://github.com/ppwwyyxx> Is there any other way by which
> we can fix the input shape by getting the height and width of the model? Or
> the height and width is always dynamic?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/1417#issuecomment-608132455>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAKRHNJA6TERG6JFOFUO4CTRKUJKPANCNFSM4L26IFGQ>
> .
>
",implement logic agarwal wrote way fix input shape getting height width model height width always dynamic reply directly view,issue,negative,positive,neutral,neutral,positive,positive
608132455,@ppwwyyxx Is there any other way by which we can fix the input shape by getting the height and width of the model? Or the height and width is always dynamic?,way fix input shape getting height width model height width always dynamic,issue,negative,neutral,neutral,neutral,neutral,neutral
608122898,"@ppwwyyxx whenever i am  using a symbolic tensor i.e. `l.set_shape([None, chan, None, None])`, the code is working fine. Also the size which i am using i.e. _1280, 1600_ in  `l.set_shape([None,chan, 1280, 1600])` is taken from config.py file `_C.PREPROC.TRAIN_SHORT_EDGE_SIZE=[1280,1600] `. 
I don't understand what is going wrong. ",whenever symbolic tensor none none none code working fine also size none taken file understand going wrong,issue,negative,negative,neutral,neutral,negative,negative
608118814,The error is saying that you set a size that is wrong and does not agree with the inputs,error saying set size wrong agree,issue,negative,negative,negative,negative,negative,negative
608118300,"@ppwwyyxx Sorry, my mistake. I have put the log file of a different issue. (Which i am currently working on). Can you please go through the correct log output. I have made the changes.

",sorry mistake put log file different issue currently working please go correct log output made,issue,negative,negative,negative,negative,negative,negative
608112783,"Your deformable conv layer is not correctly implemented and you didn't mention the use of it in the ""what you did"" section. A layer should use `tf.get_variable` to create variables along with some other rules in https://tensorpack.readthedocs.io/tutorial/extend/trainer.html#rules-of-tower-function.",deformable layer correctly mention use section layer use create along,issue,negative,neutral,neutral,neutral,neutral,neutral
607775474,"Hi @jwook1004, several sources of GPU-related non-determinism have now been addressed in TensorFlow. You may find that by setting the environment variable `TF_DETERMINISTIC_OPS` to ""true"" or ""1"" resolves the issues you were seeing.

Work is ongoing, but for more information, please see https://github.com/NVIDIA/tensorflow-determinism",hi several may find setting environment variable true seeing work ongoing information please see,issue,positive,positive,positive,positive,positive,positive
606996861,and a issue/update were @ppwwyyxx kindly gave great advice: https://github.com/AlexEMG/DeepLabCut/issues/429,kindly gave great advice,issue,positive,positive,positive,positive,positive,positive
606996484,"@tbenst here is the original PR that introduced tensorpack, if helpful to look at the minimal code: https://github.com/AlexEMG/DeepLabCut/pull/409 ",original helpful look minimal code,issue,positive,positive,positive,positive,positive,positive
606986294,@AlexEMG @MMathisLab let us know if you're able to provide any more specificity on the hanging issue? Many thanks!,let u know able provide specificity hanging issue many thanks,issue,negative,positive,positive,positive,positive,positive
606977632,"Most part of your code is unrelated to tensorpack and have a lot of dependencies. So I can't easily run it and even if I can it's not clear that the issue is in tensorpack, rather than some other code or code that uses tensorpack incorrectly.

It would make more sense to create a [MVCE](https://stackoverflow.com/help/minimal-reproducible-example) for anyone to be able to investigate an issue.",part code unrelated lot ca easily run even clear issue rather code code incorrectly would make sense create anyone able investigate issue,issue,positive,positive,positive,positive,positive,positive
606334711,"See https://tensorpack.readthedocs.io/tutorial/symbolic.html#symbolic-layers:
> Tensorpack contains a small collection of common model primitives, such as conv/deconv, fc, bn, pooling layers. However, tensorpack is model-agnostic, which means you do not need to use tensorpack’s symbolic layers and can skip this tutorial.
These layers were written only because there were no alternatives when tensorpack was first developed. Nowadays, many of these implementation actually call tf.layers directly. Tensorpack will not add any more layers into its core library because this is not the focus of tensorpack, and there are many other alternative symbolic libraries today.
Today, you can just use tf.layers or any other symbolic libraries inside tensorpack.",see small collection common model however need use symbolic skip tutorial written first nowadays many implementation actually call directly add core library focus many alternative symbolic today today use symbolic inside,issue,negative,positive,positive,positive,positive,positive
606329172,"@ppwwyyxx I made changes, as suggested, in Cascade RCNN. It works fine for me.",made cascade work fine,issue,negative,positive,positive,positive,positive,positive
606312077,"That's good to hear. Could you explain ""what"" worked for you? Regular R-CNN, or the code change above?",good hear could explain worked regular code change,issue,negative,positive,positive,positive,positive,positive
606277831,@ppwwyyxx thanks for the support. It worked for me. Closing this issue.,thanks support worked issue,issue,positive,positive,positive,positive,positive,positive
606173588,"The results are contradicting your own findings in #1254. So I don't think I can give any help on that.

As always, an issue cannot be investigated unless instructions are provided to reproduce it. For this specific case it might be related to your environment so it might not be reproducible even with the instructions. But if you want to provide them, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",think give help always issue unless provided reproduce specific case might related environment might reproducible even want provide please post relevant following issue template click new issue unexpected visit link post unexpected,issue,positive,positive,positive,positive,positive,positive
605953844,"I test the throughput on single node with 1, 4, and 8 GPUs respectively. The results are as follows. 
![Picture1](https://user-images.githubusercontent.com/22028950/77909387-d7057300-72bf-11ea-9ac2-29a9e6a85e3a.png)
",test throughput single node respectively picture,issue,negative,negative,neutral,neutral,negative,negative
605917432,"I have resized all the images to the same size and I also measured the flops per image. The results is 
the amount of calculation is very close. I doubt the measure of throughput since the GPU utilization of two-node training  is as the same as the single node.  And if I use Hierarchical All-reduce for communication, the GPU Utilization reduces by half (about 35%) but the throughput is also 40 imges/s, which is the same as when the GPU utilization is 70%. So different GPU Utilizations have the same throughput. This is my confusion. ",size also measured per image amount calculation close doubt measure throughput since utilization training single node use hierarchical communication utilization half throughput also utilization different throughput confusion,issue,negative,negative,neutral,neutral,negative,negative
605870196,The throughput is accurate and uses all workers. Otherwise you wouldn't have measured a speedup in #1254 ,throughput accurate otherwise would measured,issue,negative,positive,positive,positive,positive,positive
605330027,"Really sorry to take up your time. I absolutely know nothing about linux, otherwise I would try installing and working with it. Thank you again for helping.",really sorry take time absolutely know nothing otherwise would try working thank helping,issue,negative,negative,negative,negative,negative,negative
605157531,"Alright..
Most of the code is not tested on windows so things like this could happen. I'll fix it shortly.",alright code tested like could happen fix shortly,issue,positive,neutral,neutral,neutral,neutral,neutral
604888422,"Wow, that was fast. Thank you very much.",wow fast thank much,issue,positive,positive,positive,positive,positive,positive
604732249,fixed now. To use other image sizes you'll need to modify the model architecture,fixed use image size need modify model architecture,issue,negative,positive,neutral,neutral,positive,positive
599394653,"Cascade R-CNN can probably support it after using 
https://github.com/tensorpack/tensorpack/blob/d89b6f074d3a37a88aa35df49da58f934bd694f8/examples/FasterRCNN/modeling/model_frcnn.py#L91-L93 
to replace https://github.com/tensorpack/tensorpack/blob/d89b6f074d3a37a88aa35df49da58f934bd694f8/examples/FasterRCNN/modeling/model_cascade.py#L90",cascade probably support replace,issue,negative,neutral,neutral,neutral,neutral,neutral
599391405,Cascade R-CNN may not support such training - could you try a regular R-CNN?,cascade may support training could try regular,issue,negative,neutral,neutral,neutral,neutral,neutral
599390282,"As you suggested, I have modified roidbs to include all the images. But now i am getting the following error.

  0%|          |0/1000[00:00<?,?it/s]2020-03-16 13:10:52.352836: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
  0%|          |1/1000[00:20<5:45:08, 0.05it/s]2020-03-16 13:10:55.243286: W tensorflow/core/kernels/queue_base.cc:277] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed

MultiProcessMapDataZMQ successfully cleaned-up.
Traceback (most recent call last):
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Reduction axis 1 is empty in shape [512,0]
	 [[{{node tower0/cascade_rcnn_stage2/match_box_with_gt_0.6/ArgMax}}]]
	 [[{{node tower0/cascade_rcnn_stage2/match_box_with_gt_0.6/mul}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 132, in <module>
    launch_train_with_config(traincfg, trainer)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/interface.py"", line 101, in launch_train_with_config
    extra_callbacks=config.extra_callbacks)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/base.py"", line 342, in train_with_defaults
    steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/base.py"", line 314, in train
    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/utils/argtools.py"", line 168, in wrapper
    return func(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/base.py"", line 279, in main_loop
    self.run_step()  # implemented by subclass
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/base.py"", line 179, in run_step
    self.hooked_sess.run(self.train_op)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 676, in run
    run_metadata=run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1171, in run
    run_metadata=run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1270, in run
    raise six.reraise(*original_exc_info)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1327, in run
    run_metadata=run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1091, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Reduction axis 1 is empty in shape [512,0]
	 [[node tower0/cascade_rcnn_stage2/match_box_with_gt_0.6/ArgMax (defined at /home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_cascade.py:90) ]]
	 [[node tower0/cascade_rcnn_stage2/match_box_with_gt_0.6/mul (defined at /home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_cascade.py:94) ]]

Caused by op 'tower0/cascade_rcnn_stage2/match_box_with_gt_0.6/ArgMax', defined at:
  File ""train.py"", line 132, in <module>
    launch_train_with_config(traincfg, trainer)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/interface.py"", line 91, in launch_train_with_config
    model.build_graph, model.get_optimizer)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/utils/argtools.py"", line 168, in wrapper
    return func(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/tower.py"", line 224, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/trainers.py"", line 190, in _setup_graph
    grad_list = self._builder.call_for_each_tower(tower_fn)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/graph_builder/training.py"", line 225, in call_for_each_tower
    use_vs=[False] + [True] * (len(self.towers) - 1))
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/graph_builder/training.py"", line 121, in build_on_towers
    return DataParallelBuilder.call_for_each_tower(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/graph_builder/training.py"", line 115, in call_for_each_tower
    ret.append(func())
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/tower.py"", line 281, in get_grad_fn
    return compute_grad_from_inputs(*inputs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/train/tower.py"", line 255, in compute_grad_from_inputs
    cost = get_cost_fn(*inputs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorpack-0.9.8-py3.7.egg/tensorpack/tfutils/tower.py"", line 291, in __call__
    output = self._tower_fn(*args)
  File ""/home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/generalized_rcnn.py"", line 74, in build_graph
    head_losses = self.roi_heads(image, features, proposals, targets)
  File ""/home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/generalized_rcnn.py"", line 288, in roi_heads
    (gt_boxes, gt_labels), image_shape2d, cfg.DATA.NUM_CATEGORY)
  File ""/home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_cascade.py"", line 46, in __init__
    B1_proposal = self.match_box_with_gt(B1, ious[1])
  File ""/home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_cascade.py"", line 90, in match_box_with_gt
    best_iou_ind = tf.argmax(iou, axis=1)  # N
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py"", line 86, in argmax
    return argmax_v2(input, axis, output_type, name)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py"", line 115, in argmax_v2
    return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 879, in arg_max
    name=name)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/home/madhav3101/env1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Reduction axis 1 is empty in shape [512,0]
	 [[node tower0/cascade_rcnn_stage2/match_box_with_gt_0.6/ArgMax (defined at /home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_cascade.py:90) ]]
	 [[node tower0/cascade_rcnn_stage2/match_box_with_gt_0.6/mul (defined at /home/madhav3101/cascade/tensorpack/examples/FasterRCNN/modeling/model_cascade.py:94) ]]",include getting following error successfully library locally skipping attempt queue closed successfully recent call last file line return file line file line reduction axis empty shape node node handling exception another exception recent call last file line module trainer file line file line file line train file line wrapper return file line subclass file line file line run file line run file line run raise file line reraise raise value file line run return file line run file line run return file line run file line file line file line raise type message reduction axis empty shape node defined node defined defined file line module trainer file line file line wrapper return file line input file line file line false true file line return file line file line return file line cost file line output file line image file line file line file line file line return file line return input axis name file line return input axis file line file line file line return file line file line see reduction axis empty shape node defined node defined,issue,positive,positive,neutral,neutral,positive,positive
599383933,The steps are correct and you'll also need to remove https://github.com/tensorpack/tensorpack/blob/d89b6f074d3a37a88aa35df49da58f934bd694f8/examples/FasterRCNN/data.py#L346-L348 otherwise those images will be filtered,correct also need remove otherwise,issue,negative,neutral,neutral,neutral,neutral,neutral
599380132,"If i am not getting wrong, For the coco format all i have to do is:
* In the 'images' of json, create an entry for every image in dataset (both positive and negative images). 
* In the 'annoatations' of json, create entry for every positive image (i.e. image which contains the bounding box)

If i follow these steps, will it going to include negative images (i.e. images which doesn't contain the object, only background) during the training. Or is it just going to ignore them and keeps on training using the positive images only? ",getting wrong coco format create entry every image positive negative create entry every positive image image bounding box follow going include negative contain object background training going ignore training positive,issue,negative,negative,neutral,neutral,negative,negative
599275875,"The above comment is for custom format.
If you're using COCO format, what you do is to simply not create an annotation for the image in the json file.",comment custom format coco format simply create annotation image file,issue,negative,neutral,neutral,neutral,neutral,neutral
599273366,"Setting ""boxes"" to empty generates the following error (during loading of annotations)
`File ""/home/madhav3101/cascade/tensorpack/examples/FasterRCNN/dataset/coco.py"", line 149, in _add_detection_gt`
 `x1, y1, w, h = list(map(float, obj['bbox']))`
`ValueError: not enough values to unpack (expected 4, got 0)`",setting empty following error loading file line list map float enough unpack got,issue,negative,negative,neutral,neutral,negative,negative
599270718,"If there are no objects you should leave ""boxes/labels"" empty rather than an array full of zeros.",leave empty rather array full,issue,negative,positive,positive,positive,positive,positive
598864820,"Accessing `linear/W` should be allowed. I'll look at addressing this.

what do you mean by
> and using linear/output (self.trainer.get_predictor(self._in_names, [""predictions"", ""group3/block2/conv3/output"", ""linear/output""]) yields the wrong tensor.

? are you saying that it can run correctly, but it is not what you want?",look mean wrong tensor saying run correctly want,issue,negative,negative,negative,negative,negative,negative
598433766,"@ppwwyyxx Thanks for the quick reply (once again)!

I will try implementing something this weekend.",thanks quick reply try something weekend,issue,negative,positive,positive,positive,positive,positive
597924712,"You're right about the approach.
https://github.com/tensorpack/tensorpack/blob/07e464d81392c4e918e8c0f394793449866b0166/examples/GAN/CycleGAN.py#L184-L200
does something similar despite that you'll need some extra code to draw boxes",right approach something similar despite need extra code draw,issue,negative,positive,neutral,neutral,positive,positive
597502177,tflite does not support arbitrary tensorflow model. How to convert a model to tflite is not a feature of tensorpack and therefore we do not provide support for it.,support arbitrary model convert model feature therefore provide support,issue,positive,negative,neutral,neutral,negative,negative
596912677,"I think you can already use `tf.contrib.opt.NadamOptimizer` because it is a subclass of `tf.train.Optimizer`.

We don't support keras optimizers.",think already use subclass support,issue,negative,neutral,neutral,neutral,neutral,neutral
596379167,I do not know. I would attribute this to bad historical implementations.,know would attribute bad historical,issue,negative,negative,negative,negative,negative,negative
595890589,"Seems like it's solved. Feel free to reopen if it's not the case.
You can learn about the issue in https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.MapData",like feel free reopen case learn issue,issue,positive,positive,positive,positive,positive,positive
595888367,"@ppwwyyxx Wondering if `transformer/transformer-XL architecture` based models could be implemented / trained on `tensorpack`.

Kindly advise.

Would be a very nice use case to have an example: all such frameworks re-implement `training loop` related abstractions. Would be great to compare in terms of distributed training.

The `data flow` in these models is quite complex so it would be test for `DataFlow` framework.

",wondering architecture based could trained kindly advise would nice use case example training loop related would great compare distributed training data flow quite complex would test framework,issue,positive,positive,positive,positive,positive,positive
595767839,"It makes a difference after I revise the code :      
        ds = LMDBSerializer.load('./ILSVRC-train.lmdb', shuffle=False)
        ds = LocallyShuffleData(ds, 50000)
        augmentor = imgaug.AugmentorList(augmentors)
        def f(x):
            return augmentor.augment(cv2.imdecode(x[0], cv2.IMREAD_COLOR)), x[1]
        ds = MultiProcessMapDataZMQ(ds, num_proc=25, map_func=f)
        ds = BatchData(ds, batch_size, remainder=False, use_list=False)",difference revise code return,issue,negative,neutral,neutral,neutral,neutral,neutral
595758478,"The code for generating ILSVRC-train.lmdb is as follows:

import numpy as np
from tensorpack.dataflow import *

class BinaryILSVRC12(dataset.ILSVRC12Files):
   def __iter__(self):
        for fname, label in super(BinaryILSVRC12, self).__iter__():
            with open(fname, 'rb') as f:
                jpeg = f.read()
            jpeg = np.asarray(bytearray(jpeg), dtype='uint8')
            yield [jpeg, label]

ds0 = BinaryILSVRC12('/path/to/imagenet/', 'train', meta_dir=""/path/to/imagenet/meta/"") 
ds1 = MultiProcessRunnerZMQ(ds0, num_proc=1)
LMDBSerializer.save(ds1, './ILSVRC-train.lmdb')",code generating import import class self label super self open yield label,issue,positive,positive,positive,positive,positive,positive
593546820,"You can implement your customized version of the layer and use it.
See https://tensorpack.readthedocs.io/tutorial/extend/model.html",implement version layer use see,issue,negative,neutral,neutral,neutral,neutral,neutral
590013832,"We do not provide support for ancient versions of the software.
There is a working TTQ implementation in https://github.com/tensorpack/tensorpack/tree/master/examples/DoReFa-Net with similar results using latest software.",provide support ancient working implementation similar latest,issue,negative,positive,positive,positive,positive,positive
588788574,"Thanks for your instant reply. I got it. But I print those variables names and indeed lacks slots variables.
```
reader = NewCheckpointReader(path)
var_names = reader.get_variable_to_shape_map().keys()
```
",thanks instant reply got print indeed reader path,issue,negative,positive,neutral,neutral,positive,positive
588779688,"`load_chkpt_vars` returns everything in the checkpoint. So those variables you're interested in do not exist in the checkpoint.

It's ok (and also normal) to fine tune without momentum.",everything interested exist also normal fine tune without momentum,issue,positive,positive,positive,positive,positive,positive
588370420,"Duplicate of https://github.com/tensorpack/tensorpack/issues/817#issuecomment-481762893

It will then not be able to produce correct output using the pre-trained model.",duplicate able produce correct output model,issue,negative,positive,positive,positive,positive,positive
585182582,"one more help , i need your help for this. When i am kept the training of Pytorch faster rcnn model then my system was restarted.please help.

Thanking you in advance.",one help need help kept training faster model system help advance,issue,positive,neutral,neutral,neutral,neutral,neutral
584993296,"Thank you so much for your response.

On Wed 12 Feb, 2020, 00:51 Yuxin Wu, <notifications@github.com> wrote:

> It is.
> Set MDOE_MASK=False.
> It's strongly recommended to read papers mentioned in the README to
> understand what some of the terminologies mean.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/1396?email_source=notifications&email_token=ALLD6LVXUI4DFX3RHFJXP4TRCL3D3A5CNFSM4KS2GUZKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELNW6GY#issuecomment-584806171>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ALLD6LVE5YELOUBAXMMEMD3RCL3D3ANCNFSM4KS2GUZA>
> .
>
",thank much response wed wrote set strongly read understand mean thread reply directly view,issue,positive,positive,positive,positive,positive,positive
584806171,"It is.
Set `MDOE_MASK=False`.
It's strongly recommended to read papers mentioned in the README to understand what some of the terminologies mean.",set strongly read understand mean,issue,negative,positive,neutral,neutral,positive,positive
584561688,"I have a doubt , I want to train the model for giving only bounding boxes on the image (without masking on the class ). Is it possible to train the model without masking and how ?

kindly help  me !

Thanking you !",doubt want train model giving bounding image without class possible train model without kindly help,issue,negative,positive,positive,positive,positive,positive
584493766,"Thank you so much for your reply , but that is trained on coco dataset only , could you tell where i need to give my custom dataset and where i need to change the code ??

kindly help me .

once again thank you!",thank much reply trained coco could tell need give custom need change code kindly help thank,issue,positive,positive,positive,positive,positive,positive
584475577,"How can i train the faster rcnn model with my own custom data , please help to fingout the solution.
",train faster model custom data please help solution,issue,positive,neutral,neutral,neutral,neutral,neutral
584415588,"This seems to be fixed by latest TF nightly and should be available on 2.2. It requires fp16, NCHW, stride==1, and cuDNN version >= 7.6.3 though.

See https://github.com/tensorflow/tensorflow/pull/33836 for details.",fixed latest nightly available version though see,issue,negative,positive,positive,positive,positive,positive
582503835,"As the error says, TensorFlow's MaxPooling only supports NHWC option on CPU.
You need to use either GPU or NHWC.",error option need use either,issue,negative,neutral,neutral,neutral,neutral,neutral
582270220,"I have got similar error, you can see it below:
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'MutableDenseHashTableV2' used by node MutableDenseHashTable (defined at /Desktop/temp2/model/model.py:166) with these attrs: [value_dtype=DT_STRING, max_load_factor=0.8, initial_num_buckets=131072, shared_name="""", use_node_name_sharing=true, key_dtype=DT_INT64, container="""", value_shape=[]]
Registered devices: [CPU, XLA_CPU, XLA_GPU]
Registered kernels:
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT64]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_DOUBLE]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_BOOL]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_VARIANT]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT64]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_DOUBLE]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_BOOL]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_DOUBLE]

	 [[MutableDenseHashTable]]
Note: The error line is where i used deleted key = -1 indside this function tf.lookup.experimental.DenseHashTable

",got similar error see registered support used node defined registered registered note error line used key function,issue,negative,neutral,neutral,neutral,neutral,neutral
581481935,"Tensorpack is a training framework and does not provide such features. You can try to do them yourself.
See https://tensorpack.readthedocs.io/tutorial/inference.html#",training framework provide try see,issue,negative,neutral,neutral,neutral,neutral,neutral
580669144,"That was my understanding, actually. There must be a problem somewhere else. The fact that it hangs make me think of a race condition. Could it be the dataset? I will double check then",understanding actually must problem somewhere else fact make think race condition could double check,issue,negative,neutral,neutral,neutral,neutral,neutral
580381645,`build_graph` is called with `self.training==False` during inference. You can use `if self.training` to build a different graph for inference.,inference use build different graph inference,issue,negative,neutral,neutral,neutral,neutral,neutral
578570027,"2020-01-26 18:04:02.457549: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-01-26 18:04:07.434753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll

...
l GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:0a:00.0, compute capability: 6.1)
2020-01-26 18:04:10.446438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:1 with 6382 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:0b:00.0, compute capability: 6.1)
2020-01-26 18:04:10.453885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:2 with 6382 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1070, pci bus id: 0000:43:00.0, compute capability: 6.1)
2020-01-26 18:04:10.460182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:3 with 6382 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1070, pci bus id: 0000:44:00.0, compute capability: 6.1)
2020-01-26 18:04:21.053155: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.
2020-01-26 18:04:21.109841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cupti64_100.dll
2020-01-26 18:04:21.392770: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 0 kernel records, 1 memcpy records.

InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node Adam/NcclAllReduce}}with these attrs: [reduction=""sum"", shared_name=""c1"", T=DT_FLOAT, num_devices=4]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>

	 [[Adam/NcclAllReduce]] [Op:__inference_distributed_function_3448]",successfully dynamic library successfully dynamic library device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability profiler session successfully dynamic library kernel registered support used node sum registered registered registered,issue,positive,neutral,neutral,neutral,neutral,neutral
577011455,I fixed the typo and code format so that all checks are passed. My code can use the fp16 right now.,fixed typo code format code use right,issue,negative,positive,positive,positive,positive,positive
576517969,"> As the error says, the image `E:/projects/下载程序/tensorpack/examples/GAN/mytry1/img_align_celeba\060315.jpg` cannot be read. Please make sure it's a valid image.

The image is exist in this filepath. I tried run the code three times, each time showing a different image. These image are exist in this filepath. 
`AssertionError: E:/projects/下载程序/tensorpack/examples/GAN/mytry1/img_align_celeba\034658.jpg`",error image read please make sure valid image image exist tried run code three time time showing different image image exist,issue,negative,positive,positive,positive,positive,positive
576505599,"As the error says, the image `E:/projects/下载程序/tensorpack/examples/GAN/mytry1/img_align_celeba\060315.jpg` cannot be read. Please make sure it's a valid image.",error image read please make sure valid image,issue,negative,positive,positive,positive,positive,positive
576464947,"Thanks! We've only tested regular ResNet and it looks like the group conv implementation & convtranspose does not work for fp16.

However your PR is not working yet: https://github.com/tensorpack/tensorpack/pull/1386/checks?check_run_id=399832218
Please make sure it passes flake8, and run the command in your issue again. In the log you're supposed to see https://github.com/facebookresearch/ImageNet-Adversarial-Training/blob/1ad68f08b8533083b0b8823ac3fd85cede191646/adv_model.py#L96 printed.",thanks tested regular like group implementation work however working yet please make sure flake run command issue log supposed see printed,issue,positive,positive,positive,positive,positive,positive
576112858,"https://github.com/tensorpack/tensorpack/tree/master/examples/GAN says:
> Please see the docstring in each script for detailed usage and pretrained models. MultiGPU training is supported.

",please see script detailed usage training,issue,negative,positive,positive,positive,positive,positive
576052260,@ppwwyyxx Just reporting that now my training has gone beyond the number of epochs where it used to stop. Thanks again for your quick response!,training gone beyond number used stop thanks quick response,issue,negative,positive,positive,positive,positive,positive
576019070,"> You specify how long it trains with the `LR_SCHEDULE` option.

@ppwwyyxx Thank you once again! I just copied the command-line from balloon example and did not attempt to read comments in config.py (a bit of hurry and excitement to get things going).

I will give it a go and report here.",specify long option thank copied balloon example attempt read bit hurry excitement get going give go report,issue,positive,negative,neutral,neutral,negative,negative
576006055,"@ppwwyyxx You beat me to it! :) I was just about to post about my mistake!

I fixed it and yes indeed, it was just as you pointed out.

But thanks nevertheless for so prompt reply.

Keep up the good work!",beat post mistake fixed yes indeed pointed thanks nevertheless prompt reply keep good work,issue,positive,positive,positive,positive,positive,positive
576005425,"It says the two vector ""class"" and ""is_crowd"" in your dataset does not have the same length, which is illegal.",two vector class length illegal,issue,negative,negative,negative,negative,negative,negative
575830195,@ppwwyyxx Finally I got the chance to setup training today and within an hour I had training going. Thanks for your help!,finally got chance setup training today within hour training going thanks help,issue,positive,positive,neutral,neutral,positive,positive
573572713,"Thank you, I will follow your last answer. Thanks a lot.",thank follow last answer thanks lot,issue,positive,positive,neutral,neutral,positive,positive
573567606,"My comment above works for the case. To be more concrete, see this diff:
```
diff --git i/examples/basics/mnist-convnet.py w/examples/basics/mnist-convnet.py
index 4e887e62..aa0afa6e 100755
--- i/examples/basics/mnist-convnet.py
+++ w/examples/basics/mnist-convnet.py
@@ -23,15 +23,20 @@ class Model(ModelDesc):
         Define all the inputs (with type, shape, name) that the graph will need.
         """"""
         return [tf.TensorSpec((None, IMAGE_SIZE, IMAGE_SIZE), tf.float32, 'input'),
-                tf.TensorSpec((None,), tf.int32, 'label')]
+                tf.TensorSpec((None,), tf.int32, 'label'),
+                tf.TensorSpec((1,), tf.float32, 'extra')
+                ]
 
-    def build_graph(self, image, label):
+    def build_graph(self, image, label, extra):
         """"""This function should build the model which takes the input variables (defined above)
         and return cost at the end.""""""
 
+        if self.training:
+            extra = 0.5
+
         # In tensorflow, inputs to convolution function are assumed to be
         # NHWC. Add a single channel here.
-        image = tf.expand_dims(image, 3)
+        image = tf.expand_dims(image, 3) + extra
 
         image = image * 2 - 1   # center the pixels values at zero
         # The context manager `argscope` sets the default option for all the layers under
@@ -120,7 +125,7 @@ if __name__ == '__main__':
         # The input source for training. FeedInput is slow, this is just for demo purpose.
         # In practice it's best to use QueueInput or others.
         # See tutorial at https://tensorpack.readthedocs.io/tutorial/extend/input-source.html
-        data=FeedInput(dataset_train),
+        data=remap_input_source(FeedInput(dataset_train), [""input"", ""label""]),
         # We use a few simple callbacks in this demo.
         # See tutorial at https://tensorpack.readthedocs.io/tutorial/callback.html
         callbacks=[
```

It will then use a constant 0.5 during training and require you to feed the tensor during inference.",comment work case concrete see git index class model define type shape name graph return none none none self image label self image label extra function build model input defined return cost end extra convolution function assumed add single channel image image image image extra image image center zero context manager default option input source training slow purpose practice best use see tutorial input label use simple see tutorial use constant training require feed tensor inference,issue,positive,positive,neutral,neutral,positive,positive
573556151,"Thank you for your quick response. I'm really happy and feel sorry that I didn't make it clear. I want to feed value using feed_dict since it's the only way to set the value after I create the model pb file. 

I want to feed different values to the placeholder (the pb file) and see how the result will be different. 

I guess I can follow your suggestion. 
https://github.com/tensorpack/tensorpack/issues/921#issuecomment-573538073

But for me in terms of complexity, it seems too much since the value for extra placeholder will not be changed during training or testing. Would you leave a comment on this situation? 
Thank you",thank quick response really happy feel sorry make clear want feed value since way set value create model file want feed different file see result different guess follow suggestion complexity much since value extra training testing would leave comment situation thank,issue,positive,positive,positive,positive,positive,positive
573547935,"You can do
```
if self.training:
   value = 0.1
else:
   value = inputs[-1]

loss = f(inputs[:-1], value)
```
so you don't need to add the extra input as part of your input data during training.
EDIT: a catch is that you'll need https://tensorpack.readthedocs.io/modules/input_source.html#tensorpack.input_source.remap_input_source for training to skip the extra unused input. Its documentation explains the exact scenario ",value else value loss value need add extra input part input data training edit catch need training skip extra unused input documentation exact scenario,issue,positive,positive,neutral,neutral,positive,positive
573547499,"@ppwwyyxx Thank you for your quick response. 

I have one more question. I want to use fixed value (The value will not be changed during training) and then feed using various values in inference time. So, I think the Callback implementation is right for my situation. To make a simple arch for training code, it seems quite heavy for me if I create extra input to feed fixed value. Do you have any recommendations in this situation?

By the way, I will follow your suggestion. Again, thank you for your lightning response. ",thank quick response one question want use fixed value value training feed various inference time think implementation right situation make simple arch training code quite heavy create extra input feed fixed value situation way follow suggestion thank lightning response,issue,positive,positive,neutral,neutral,positive,positive
573538073,"@junhongGenesislab  the method applies only when you're not using queue or multiGPU. Otherwise data loading and model are disentangled for efficiency.

If the extra input is something you'll need and change for each iteration, you should simply make it part of your dataset, because it literally is part of your input training data.",method queue otherwise data loading model efficiency extra input something need change iteration simply make part literally part input training data,issue,negative,neutral,neutral,neutral,neutral,neutral
573536941,"I followed the first way to feed extra input from this comment. 
https://github.com/tensorpack/tensorpack/issues/921#issuecomment-427264569
And I got an error message. 

```
100%|##########|1/1[00:02<00:00, 0.49it/s]
[0113 16:12:44 @base.py:285] Epoch 1 (global_step 1) finished, time:2.02 seconds.
[0113 16:12:44 @saver.py:82] Model saved to somewhere.
  0%|          |0/1[00:00<?,?it/s]
[0113 16:12:45 @input_source.py:179] [EnqueueThread] Thread EnqueueThread QueueInput/input_queue Exited.

Traceback (most recent call last):

tensorflow.python.framework.errors_impl.InvalidArgumentError: 
You must feed a value for placeholder tensor 'tensor_name' with dtype float
```


```Python
class DropoutKeepProbFeeder(Callback):
    def __init__(self):
        self._tensor_name = 0.3

    def _before_run(self, _):
        return tf.train.SessionRunArgs(fetches=[], feed_dict={'tensor_name:0': self._tensor_name})
```

Add this to the TrainConfig as a callback, the full code is not available sorry. 
I want to know if there is any workaround. 

Thank you,
Junhong",first way feed extra input comment got error message epoch finished time model saved somewhere thread recent call last must feed value tensor float python class self self return add full code available sorry want know thank,issue,positive,positive,neutral,neutral,positive,positive
573370748,"> Nevertheless, you can convert masks to polygons using `cv2.findContours`.

Thank you for the tip! I will try it soon.

I have one small dataset but another one is an adapted version of Google Open Images V5 dataset. So I was thinking if something could be done using less CPU. But that's not a show-stopper.
",nevertheless convert thank tip try soon one small another one version open thinking something could done le,issue,negative,negative,negative,negative,negative,negative
573351827,"The polygons are converted to masks eventually (after augmentation) at https://github.com/tensorpack/tensorpack/blob/ffd9eaeb68fff978af23156236df661981017941/examples/FasterRCNN/data.py#L131-L141

so you can add code that handles masks directly.

You may not be able to easily use it for evaluation, because the COCO api may not support masks very well.

Nevertheless, you can convert masks to polygons using `cv2.findContours`.",converted eventually augmentation add code directly may able easily use evaluation coco may support well nevertheless convert,issue,positive,positive,positive,positive,positive,positive
572341084,"Thanks a lot. My problem's solved. d2f9564's worked
",thanks lot problem worked,issue,negative,positive,positive,positive,positive,positive
572339348,"In fact, the saliency code will need the update I made two days ago in d2f956450da9075912d78c8108900b9284d19712 to be able to run with TF 1.8.
If you have not updated, then you're not using TF 1.8",fact code need update made two day ago able run,issue,negative,positive,positive,positive,positive,positive
572338412,"Using tensorflow 1.8, I obtained expected output when running the code on this cat image from https://christopher5106.github.io/img/cat.jpg:
![image](https://christopher5106.github.io/img/cat.jpg)",output running code cat image image,issue,negative,neutral,neutral,neutral,neutral,neutral
572328085,"1. What you did
(1) **If you're using examples, what's the command you run:**
I ran ./saliency-maps.py

(2) **If you're using examples, have you made any changes to the examples? Paste `git status; git diff` here:**
And I changed the code to put picture.
 run(""/home/di/resnet_v1_50.ckpt"", ""/home/di/01.jpg"")

### 2. What you observed:

(1) **Include the ENTIRE logs here:**
I've got this logs.

keep_dims is deprecated, use keepdims instead
[0109 09:40:57 @collection.py:146] New collections created in tower : tf.GraphKeys.MODEL_VARIABLES of size 267, tf.GraphKeys.REGULARIZATION_LOSSES of size 54, resnet_v1_50/_end_points of size 74
[0109 09:40:57 @collection.py:165] These collections were modified but restored in : (tf.GraphKeys.UPDATE_OPS: 0->106)
[0109 09:40:57 @sessinit.py:87] WRN The following variables are in the checkpoint, but not found in the graph: global_step
2020-01-09 09:40:57.373495: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-09 09:40:57.437488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-09 09:40:57.437810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 9.89GiB
2020-01-09 09:40:57.437839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2020-01-09 09:40:57.689203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-09 09:40:57.689247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2020-01-09 09:40:57.689253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2020-01-09 09:40:57.689340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11066 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
[0109 09:40:58 @sessinit.py:114] Restoring checkpoint from /home/di/resnet_v1_50.ckpt ...

",command run ran made paste git status git code put picture run include entire got use instead new tower size size size following found graph binary use successful node read negative value must least one node node zero found device name ti major minor visible device interconnect strength edge matrix device memory physical device name ti bus id compute capability,issue,positive,positive,neutral,neutral,positive,positive
572323078,"please read the template and post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",please read template post relevant following issue template click new issue unexpected visit link post unexpected,issue,negative,positive,positive,positive,positive,positive
572322697,"I'm using anaconda 4.6.11
python 3.6.7
tensorpack 0.9.8
tensorflow 1.8.0
opencv 3.2.0
numpy 1.17.3
I tried lots of other pic but had a same result.
What is the problem?",anaconda python tried lot pic result problem,issue,negative,neutral,neutral,neutral,neutral,neutral
567854495,"```diff
diff --git a/examples/FasterRCNN/data.py b/examples/FasterRCNN/data.py
index ea23e0f..f0eba55 100644
--- a/examples/FasterRCNN/data.py
+++ b/examples/FasterRCNN/data.py
@@ -48,8 +48,6 @@ def print_class_histogram(roidbs):
         # filter crowd?
         gt_inds = np.where((entry[""class""] > 0) & (entry[""is_crowd""] == 0))[0]
         gt_classes = entry[""class""][gt_inds]
-        if len(gt_classes):
-            assert gt_classes.max() <= len(class_names) - 1
         gt_hist += np.histogram(gt_classes, bins=hist_bins)[0]
     data = list(itertools.chain(*[[class_names[i + 1], v] for i, v in enumerate(gt_hist[1:])]))
     COL = min(6, len(data))
@@ -99,8 +97,6 @@ class TrainingDataPreprocessor:
         points = tfms.apply_coords(points)
         boxes = point8_to_box(points)
         if len(boxes):
-            assert klass.max() <= self.cfg.DATA.NUM_CATEGORY, \
-                ""Invalid category {}!"".format(klass.max())
             assert np.min(np_area(boxes)) > 0, ""Some boxes have zero area!""
 
         ret = {""image"": im}
```
I'm not sure why they would be removed. Invalid classes will likely lead to NaNs in training.
Other part of the diff looks reasonable. The diff is however not based on tensorpack's commits, but on some of your private commits. So I can only assume your previous commits are also reasonable.

Following the diff I run a 4-GPU training with zero changes to tensorpack's current master, with `--config TRAINER=horovod DATA.BASEDIR=~/data/coco MODE_MASK=False`. It finished 94 epochs without any errors or NaNs. My environment is
```
sys.platform          linux
Python                3.7.3 (default, Mar 27 2019, 22:11:17) [GCC 7.3.0]
Tensorpack            0.9.8
Numpy                 1.16.4
TensorFlow            1.15.0-rc2/v1.15.0-rc2-0-g5adb433d78
TF Compiler Version   7.4.0
TF CUDA support       True
TF MKL support        True
TF XLA support        False
Nvidia Driver         /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.410.79
CUDA                  /cuda/10.0/lib64/libcudart.so.10.0.130
CUDNN                 /cudnn-7.6.4-cuda10/lib64/libcudnn.so.7.6.4
NCCL                  /nccl_2.4.7-1_cuda10.0/lib/libnccl.so.2.4.7
CUDA_VISIBLE_DEVICES  0,1,2,3
GPU 0,1,2,3           Tesla V100-SXM2-16GB
Free RAM              464.05/503.79 GB
CPU Count             80
Horovod               0.18.2
cv2                   3.4.1
msgpack               0.6.2
python-prctl          False
```

Since I cannot reproduce the issue with the given instructions I'll close the issue.",git index filter crowd entry class entry entry class assert data list enumerate col min data class assert invalid category assert zero area ret image sure would removed invalid class likely lead training part reasonable however based private assume previous also reasonable following run training zero current master finished without environment python default mar compiler version support true support true support false driver free ram count false since reproduce issue given close issue,issue,positive,positive,neutral,neutral,positive,positive
567822521,"Since the crash is likely to be a tensorflow bug, I would upgrade tensorflow first.

From the second log you provided, the model diverges (has nan loss) in the end. Such behavior does sometimes trigger a tensorflow crash (e.g. a bug fixed in https://github.com/tensorflow/tensorflow/commit/1c94a5bed5402c4f764ec9b6b25d00af529c541b). 
However I don't expect that a normal training on COCO will diverge. 

I can try to reproduce but I'll need to know what changes you've made as requested in the issue template. For example, you've changed `MODE_MASK` to False according to the log but you have not mentioned it so far.",since crash likely bug would upgrade first second log provided model nan loss end behavior sometimes trigger crash bug fixed however expect normal training coco diverge try reproduce need know made issue template example false according log far,issue,negative,positive,neutral,neutral,positive,positive
567804431,@ppwwyyxx  Hi~ Could you give me some advice about this error?  or How should I debug the program?,could give advice error program,issue,negative,neutral,neutral,neutral,neutral,neutral
567746208,"@ppwwyyxx  I am not change the model code  and  the complete log file is 
[xuan.log](https://github.com/tensorpack/tensorpack/files/3986243/xuan.log)
",change model code complete log file,issue,negative,positive,neutral,neutral,positive,positive
567599012,"This is a python bug that's fixed in later versions.
At least it works for me on Python 3.8.",python bug fixed later least work python,issue,negative,negative,neutral,neutral,negative,negative
567368159,"Please also answer: 

> (2) If you're using examples, have you made any changes to the examples?

If you have not changed the model code, then this reads like a tensorflow bug to me. However I cannot proceed with debugging unless there are instructions for me to reproduce it on a common dataset.",please also answer made model code like bug however proceed unless reproduce common,issue,positive,negative,negative,negative,negative,negative
567360321,Please answer the questions in the issue template and post full logs.,please answer issue template post full,issue,negative,positive,positive,positive,positive,positive
567331280,"It converts a bounding box to its 4 vertex points (8 numbers) and vice versa.
It should be 4 instead of 8 to reduce confusion.",bounding box vertex vice instead reduce confusion,issue,negative,neutral,neutral,neutral,neutral,neutral
567322535,"@ppwwyyxx  hi,I also meet the similiar error,could you give me some advice?
`W tensorflow/core/kernels/queue_base.cc:277] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed
Traceback (most recent call last):
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Input to reshape is a tensor with 600 values, but the requested shape has 120000
         [[{{node gradients/rpn_1/Reshape_grad/Reshape}}]]
  (1) Invalid argument: Input to reshape is a tensor with 600 values, but the requested shape has 120000
         [[{{node gradients/rpn_1/Reshape_grad/Reshape}}]]
         [[gradients/AddN_154/_4309]]
0 successful operations.
0 derived errors ignored. `

`During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""train.py"", line 130, in <module>
    launch_train_with_config(traincfg, trainer)
  File ""../../tensorpack/train/interface.py"", line 101, in launch_train_with_config
    extra_callbacks=config.extra_callbacks)
  File ""../../tensorpack/train/base.py"", line 342, in train_with_defaults
    steps_per_epoch, starting_epoch, max_epoch)
  File ""../../tensorpack/train/base.py"", line 314, in train
    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
  File ""../../tensorpack/utils/argtools.py"", line 168, in wrapper
    return func(*args, **kwargs)
  File ""../../tensorpack/train/base.py"", line 279, in main_loop
    self.run_step()  # implemented by subclass
  File ""../../tensorpack/train/base.py"", line 179, in run_step
    self.hooked_sess.run(self.train_op)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 754, in run
    run_metadata=run_metadata)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1252, in run
    run_metadata=run_metadata)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1353, in run
    raise six.reraise(*original_exc_info)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1338, in run
    return self._sess.run(*args, **kwargs)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1411, in run
    run_metadata=run_metadata)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1169, in run
    return self._sess.run(*args, **kwargs)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.`

`Original stack trace for 'gradients/rpn_1/Reshape_grad/Reshape':
  File ""train.py"", line 130, in <module>
    launch_train_with_config(traincfg, trainer)
  File ""../../tensorpack/train/interface.py"", line 91, in launch_train_with_config
    model.build_graph, model.get_optimizer)
  File ""../../tensorpack/utils/argtools.py"", line 168, in wrapper
    return func(*args, **kwargs)
  File ""../../tensorpack/train/tower.py"", line 224, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""../../tensorpack/train/trainers.py"", line 447, in _setup_graph
    grads = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)()
  File ""../../tensorpack/train/tower.py"", line 281, in get_grad_fn
    return compute_grad_from_inputs(*inputs)
  File ""../../tensorpack/train/tower.py"", line 276, in compute_grad_from_inputs
    aggregation_method=self.AGGREGATION_METHOD)
  File ""../../tensorpack/tfutils/optimizer.py"", line 29, in compute_gradients
    return self._opt.compute_gradients(*args, **kwargs)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 512, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 158, in gradients
    unconnected_gradients)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py"", line 731, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py"", line 403, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py"", line 731, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py"", line 540, in _ReshapeGrad
    return [array_ops.reshape(grad, array_ops.shape(op.inputs[0])), None]
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 7715, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/public/home/yangxuan1/Build/python-3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()`",hi also meet error could give advice skipping attempt queue closed recent call last file line return file line file line root error found invalid argument input reshape tensor shape node invalid argument input reshape tensor shape node successful derived handling exception another exception recent call last file line module trainer file line file line file line train file line wrapper return file line subclass file line file line run file line run file line run raise file line reraise raise value file line run return file line run file line run return file line run file line file line file line raise type message root error original stack trace file line module trainer file line file line wrapper return file line input file line input file line return file line file line return file line file line file line lambda file line return exit early file line lambda lambda file line return grad none file line reshape reshape file line file line return file line file line,issue,negative,positive,positive,positive,positive,positive
566837656,GIL affects anything that is python code. So its effect depends on how much of `np.random.randint` and `np.unique` is implemented in python.,anything python code effect much python,issue,negative,positive,positive,positive,positive,positive
566825915,"I see. After I skimmed what GIL is, what I am curious now is: In this example case, the `np.unique` input and output are defined ONLY IN a function (so when assuming a function runs in a thread, I feel like no python object is shared by different thread). Still, the GIL affects this case?

(Sorry, a detailed answer about GIL would be long, but can I have some brief idea?)
",see skimmed curious example case input output defined function assuming function thread feel like python object different thread still case sorry detailed answer would long brief idea,issue,positive,negative,neutral,neutral,negative,negative
566487017,"Let me ask another question. This time I made a simple example.

```
class XData(RNGDataFlow):
    def __init__(self):
        super(XData, self).__init__()
        self.dataset = np.arange(5000)
        
    def __len__(self):
        return len(self.dataset)
    
    def __iter__(self):
        idxs = np.arange(len(self.dataset))
        self.rng.shuffle(idxs)
        for idx in idxs:
            yield self.dataset[idx]

xx = XData()
print(len(xx))     # 5000
```
As you see, this pretty much a dummy data class, and I added a map function.
```
import time
def map_for_train_fn(dp):
    sleep(1)
    return dp
yy1 = MapData(xx, map_for_train_fn)
yy2 = MultiThreadMapData(xx, 10, map_for_train_fn)
````
`TestDataSpeed(yy1).start()` gets 1 it/s.
`TestDataSpeed(yy2).start()` gets 10 it/s.
So, this works beautifully.

However, when I tested a different map function:
```
def map_for_train_fn(dp):
    t = np.random.randint(100, size=(230000, 2))
    a, b, c = np.unique(t, axis=0, return_counts=True, return_index=True)
    return dp
yy1 = MapData(xx, map_for_train_fn)
yy2 = MultiThreadMapData(xx, 10, map_for_train_fn)
```
Both `yy1` and `yy2` shows 6.02 it/s and 6.26 it/s. (No advantage by parallelism. The np.unique takes ~164 ms, so ~6 it/s makes sense when without parallelism.) How I can understand this? 

",let ask another question time made simple example class self super self self return self yield print see pretty much dummy data class added map function import time sleep return work beautifully however tested different map function return advantage parallelism sense without parallelism understand,issue,positive,positive,positive,positive,positive,positive
566256789,This is not a security thread because the file is only used to build the docs.,security thread file used build,issue,negative,neutral,neutral,neutral,neutral,neutral
564363862,"I think DepthConv may be the truely problem. issue from tensorflow https://github.com/tensorflow/tensorflow/issues/12940
https://github.com/tensorflow/tensorflow/issues/12132",think may problem issue,issue,negative,neutral,neutral,neutral,neutral,neutral
563494265,"> So, if I ds = MapThreadData(data, 8, fn_C), then will fn_A, fn_B have benefits of parallelism?

No. You can let `f = fn_C(fn_B(fn_A()))` and parallelize `f`. 

> Will it be better just having light Data Class (maybe only pointing file paths), and keeping IO functions or transforming functions in Map Functions even when those functions are common repetitive (like class number remapping)?

To save memory this may make sense.",data parallelism let parallelize better light data class maybe pointing file keeping io transforming map even common repetitive like class number save memory may make sense,issue,positive,positive,neutral,neutral,positive,positive
563401729,"Let me ask a few follow-up questions:

Q1.
> You can just apply it in the end and then it will parallelize all functions

So, if I `ds = MapThreadData(data, 8, fn_C)`, then will `fn_A`, `fn_B` have benefits of parallelism? 

Q2.

In my case, to reduce the burden of map functions (`fn_A`, `fn_B`, `fn_C`, ...), I tried to prepare dataset as much as in `SomeData` class. 
- For example, all-image loading is impossible, but at least I can fully load all labels in advance, and do all GT class remapping in advance in `SomeData`. I did this to save label file loading IO on the fly, at least.
- `self.dataset` maybe look like `[{'img_path': ..., 'gt_classes': [0, 3, 1], 'gt_boxes': ..., 'some_meta': ..., ...}, ...]`
- This turns out, `SomeData` class requires quite a large memory with big data (than just having `gt_path`).

Question is:
- When I tried to use ZMQ, it seemed `SomeData` was too large to be cloned.
- I'm not sure how `MapThreadData` will be effective. (I will keep experimenting)
- Will it be better just having light Data Class (maybe only pointing file paths), and keeping IO functions or transforming functions in Map Functions even when those functions are common repetitive (like class number remapping)?

Sorry, since this must be not a specific question and example. I was wondering if I could get some general ideas.

",let ask apply end parallelize data parallelism case reduce burden map tried prepare much class example loading impossible least fully load advance class advance save label file loading io fly least maybe look like turn class quite large memory big data question tried use large sure effective keep better light data class maybe pointing file keeping io transforming map even common repetitive like class number sorry since must specific question example wondering could get general,issue,positive,positive,neutral,neutral,positive,positive
563368493,"Then there might be a bug in either the cudnn or the tensorflow you're using.
You can try to upgrade them or try on a different machine.
Closing as I don't think it's related to tensorpack.",might bug either try upgrade try different machine think related,issue,negative,neutral,neutral,neutral,neutral,neutral
563187034,"> Most likely you're using a wrong build of cudnn

Thanks for your reply. But if I ran train.py successfully, is there still the problem with cudnn?",likely wrong build thanks reply ran successfully still problem,issue,negative,positive,positive,positive,positive,positive
562965187,"> I'm guessing here that I can have only one Multi{Thread,Process}MapData, is it right?

I have not verified that it works but I think you can have a `MultiProcessMapData(ds)` where `ds` comes from a `MultiThreadMapData`. This is not a common pattern though.

> If true and if I give enough parallelism to fn_B, then the data pipeline can't be faster than 50ms per data (next bottleneck fn_C)?

yes

> Or, can I apply Multi{Thread,Process}MapData to all three functions?

You can just apply it in the end and then it will parallelize all functions

>  Let's say I'm using the ""ZMQ"" thing with num_proc=10. Let's also say the class SomeData requires 2GB memory (e.g. self.dataset). Since ZMQ will have multiple clones of the entire class, it will require ~20GB, right?

Yes.

> In contrast, Multi{Thread,Process}MapData with num_proc=10 applies only a mapping function, so memory usage is still ~2GB?

Yes. For `MultiProcessMapData` you also need to make sure you don't make reference to data such as `self.dataset` in the mapping function.

> If I'm using ""ZMQ"", should I put it before BatchData or after? Does it matter?

It can parallelize the batching, but whether it helps depends. You could try both.

> In parallel examples, I didn't see any usage of 'Prefetch' functions (like PrefetchData, MultiProcessPrefetchData). What are these? In what cases are these useful?

They are renamed to `MultiProcessRunner{ZMQ}`. https://tensorpack.readthedocs.io/tutorial/parallel-dataflow.html#run-multiple-identical-dataflows",guessing one thread process right work think come common pattern though true give enough parallelism data pipeline ca faster per data next bottleneck yes apply thread process three apply end parallelize let say thing let also say class memory since multiple entire class require right yes contrast thread process function memory usage still yes also need make sure make reference data function put matter parallelize whether could try parallel see usage like useful,issue,positive,positive,positive,positive,positive,positive
561338165,"> `export-model.py` is unrelated to Faster R-CNN. It's an example to teach users how to export models to serving.
> 
> Faster R-CNN's `./predict.py` can export a Faster R-CNN model to serving.

Thank you, now I understand. 
I completely missed the **--output-pb** and **--output-serving** arguments for the predict.py Script. 
**I'm sorry**. Thank your for your help!

",unrelated faster example teach export serving faster export faster model serving thank understand completely script sorry thank help,issue,positive,negative,negative,negative,negative,negative
561279919,"`export-model.py` is unrelated to Faster R-CNN. It's an example to teach users how to export models to serving.

Faster R-CNN's `./predict.py` can export a Faster R-CNN model to serving.",unrelated faster example teach export serving faster export faster model serving,issue,negative,neutral,neutral,neutral,neutral,neutral
561244291,"If you're asking about an unexpected problem which you do not know the root cause,
use this template. __PLEASE DO NOT DELETE THIS TEMPLATE, FILL IT__:

If you already know the root cause to your problem,
feel free to delete everything in this template.

### 1. What you did:

(1) **If you're using examples, what's the command you run:**

`python export-model.py --export serving --load /home/kingdav33/Schreibtisch/RUNS/672x672R3/checkpoint`

(2) **If you're using examples, have you made any changes to the examples? Paste `git status; git diff` here:**

**I've only changed the train.py, coco.py and config.py and can provide the code differences if needed.** **Currently no changes on the export-model.py script, but this is the area that most likely causes the issue due to tensor_name mismatch with my network:**

```
def export_serving(model_path):
    """"""Export trained model to use it in TensorFlow Serving or cloudML. """"""
    pred_config = PredictConfig(
        session_init=get_model_loader(model_path),
        model=InferenceOnlyModel(),
        input_names=['image', 'gt_boxes', 'gt_labels'],
        output_names=['output/boxes', 'output/scores', 'output/labels'])
    ModelExporter(pred_config).export_serving('/tmp/exported')
```

(3) **If not using examples, help us reproduce your issue:**

  It's always better to copy-paste what you did than to describe them.

  Please try to provide enough information to let others __reproduce__ your issues.
  Without reproducing the issue, we may not be able to investigate it.

### 2. What you observed:

(1) **Include the ENTIRE logs here:**

```
(test) kingdav33@X470:~/Schreibtisch/Utility$ python export-model.py --export serving --load /home/kingdav33/Schreibtisch/RUNS/672x672R3/checkpoint

/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
[1203 17:07:33 @sesscreate.py:38] WRN User-provided custom session config may not work due to TF bugs. See https://github.com/tensorpack/tensorpack/issues/497 for workarounds.
Traceback (most recent call last):
  File ""export-model.py"", line 191, in <module>
    export_serving(args.load)
  File ""export-model.py"", line 113, in export_serving
    ModelExporter(pred_config).export_serving('/tmp/exported')
  File ""/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorpack/tfutils/export.py"", line 124, in export_serving
    input_tensors = get_tensors_by_names(self.config.input_names)
  File ""/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorpack/tfutils/common.py"", line 124, in get_tensors_by_names
    ret.append(G.get_tensor_by_name(varn))
  File ""/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3972, in get_tensor_by_name
    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)
  File ""/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3796, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3838, in _as_graph_element_locked
    ""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'image:0' refers to a Tensor which does not exist. The operation, 'image', does not exist in the graph.""
```

It's always better to copy-paste what you observed instead of describing them.

It's always better to paste **as much as possible**, although sometimes a partial log is OK.

Tensorpack typically saves stdout to its training log.
If stderr is relevant, you can run a command with `my_command 2>&1 | tee logs.txt`
to save both stdout and stderr to one file.

(2) **Other observations, if any:**
For example, CPU/GPU utilization, output images, tensorboard curves, if relevant to your issue.

### 3. What you expected, if not obvious.

**I expected that export-model.py would generate the layout explained in the documentation, which contains a .pb file for further deployment.**

If you expect higher speed, please read
http://tensorpack.readthedocs.io/tutorial/performance-tuning.html
before posting.

If you expect the model to work better, only in one of the two conditions can we help with it:
(1) You're unable to reproduce the results documented in tensorpack examples.
(2) It appears to be a tensorpack bug.

Otherwise, how to train a good model on your task or your
modifications is a machine learning question.
We do not answer machine learning questions and it is your responsibility to
figure out how to make your models more accurate.

### 4. Your environment:

Paste the output of this command: `python -c 'import tensorpack.tfutils as u; print(u.collect_env_info())'`
If this command failed, tell us your version of Python/TF/tensorpack.

```
(test) kingdav33@X470:~/Schreibtisch/Utility$ python -c 'import tensorpack.tfutils as u; print(u.collect_env_info())'
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
--------------------  ------------------------------------------------------------------------------
sys.platform          linux
Python                3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) [GCC 7.3.0]
Tensorpack            v0.9.8-0-gb6318616
Numpy                 1.17.4
TensorFlow            1.14.0/v1.14.0-rc1-22-gaf24dc9
TF Compiler Version   5.4.0 20160609
TF CUDA support       True
TF MKL support        False
TF XLA support        False
Nvidia Driver         /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.435.21
CUDA                  /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130
CUDNN                 /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7
NCCL
CUDA_VISIBLE_DEVICES  None
GPU 0                 GeForce RTX 2060 SUPER
Free RAM              13.25/15.67 GB
CPU Count             12
cv2                   4.1.2
msgpack               0.6.2
python-prctl          False

```

Note that:

  + You can install Tensorpack master by `pip install -U git+https://github.com/tensorpack/tensorpack.git`
    and see if your issue is already solved.
  + If you're not using tensorpack under a normal command line shell (e.g.,
    using an IDE or jupyter notebook), please retry under a normal command line shell.

You may often want to provide extra information related to your issue, but
at the minimum please try to provide the above information __accurately__ to save effort in the investigation.",unexpected problem know root cause use template delete template fill already know root cause problem feel free delete everything template command run python export serving load made paste git status git provide code currently script area likely issue due mismatch network export trained model use serving help u reproduce issue always better describe please try provide enough information let without issue may able investigate include entire test python export serving load passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource custom session may work due see recent call last file line module file line file line file line file line return name file line return file line graph name name tensor exist operation exist graph always better instead always better paste much possible although sometimes partial log typically training log relevant run command tee save one file example utilization output relevant issue obvious would generate layout documentation file deployment expect higher speed please read posting expect model work better one two help unable reproduce bug otherwise train good model task machine learning question answer machine learning responsibility figure make accurate environment paste output command python print command tell u version test python print passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource python default compiler version support true support false support false driver none super free ram count false note install master pip install see issue already normal command line shell ide notebook please retry normal command line shell may often want provide extra information related issue minimum please try provide information save effort investigation,issue,positive,positive,neutral,neutral,positive,positive
561239628,"Thank you for the response.
I've now used the input_names ['image', 'gt_boxes', 'gt_labels'] and output_names ['output/boxes', 'output/scores', 'output/labels'], which are in the predict.py.

**However, the issue still persists:**

```
Traceback (most recent call last):
  File ""export-model.py"", line 191, in <module>
    export_serving(args.load)
  File ""export-model.py"", line 113, in export_serving
    ModelExporter(pred_config).export_serving('/tmp/exported')
  File ""/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorpack/tfutils/export.py"", line 124, in export_serving
    input_tensors = get_tensors_by_names(self.config.input_names)
  File ""/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorpack/tfutils/common.py"", line 124, in get_tensors_by_names
    ret.append(G.get_tensor_by_name(varn))
  File ""/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3972, in get_tensor_by_name
    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)
  File ""/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3796, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/kingdav33/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3838, in _as_graph_element_locked
    ""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'image:0' refers to a Tensor which does not exist. The operation, 'image', does not exist in the graph.""
```

________________________________________________

My assumption was, that I can get the input and output tensors of my network by reading the npz dict. But it seems that the tensors are not in that dict? Maybe you can help me to understand how I can get the tensor names of my graph? I would be really glad.

**Best Regards, David.**",thank response used however issue still recent call last file line module file line file line file line file line return name file line return file line graph name name tensor exist operation exist graph assumption get input output network reading maybe help understand get tensor graph would really glad best,issue,positive,positive,positive,positive,positive,positive
560507017,The Faster RCNN example already contains code to export to serving in `predict.py`. You can see how it is done.,faster example already code export serving see done,issue,negative,neutral,neutral,neutral,neutral,neutral
559929580,"I set the mode of ""cpu""..not works,error is still ""1 element was put into StagingArea on each tower."",
Could u please help me, save children?",set mode work error still element put tower could please help save,issue,positive,neutral,neutral,neutral,neutral,neutral
559849101,"Please read the definition of `c` in Sec 2.1:
`x = c_0(x) 2^0  + c_1(x) 2^1 = c_0(x) + 2 * c_1(x)`
when `x = [3, 1]`. We have `[3, 1] = [1, 1] + 2 * [1, 0]`
So it's correct.",please read definition sec correct,issue,negative,negative,neutral,neutral,negative,negative
559846250,"I am reading your paper with interest, but the equation (3) is really not understandable.
You write:

> if say M=K=2, p=q=2. x = [3, 1] = [11, 01] in base 2, , y = [2, 0] = [10, 00].

ok

> Then c_0(x) = [1,1]

ok

> c_1(x) = [1, 0]

What ? should be [0,1] following your logic

> c_0(y) = [0,0]

What ? should be [1,0]

> c_1(y) = [1,0]

again should b [0,0]

There is something I do not understand 


",reading paper interest equation really understandable write say base following logic something understand,issue,negative,negative,negative,negative,negative,negative
557974051,"Good idea! I'll try that. In fact, I've tested the dataflow in small dataset `roidbs = roidbs[:5000]`, seems OK. I'm not sure if I understood the dataflow scheme correctly. Thank you Yuxin! @ppwwyyxx",good idea try fact tested small sure understood scheme correctly thank,issue,positive,positive,positive,positive,positive,positive
557970771,"If you think dataflow is the cause, you can just run it alone without training to see whether it will get stuck, and start from there.
One possible reason why such dataflow could get stuck is that it takes too much memory and some dataflow workers are killed by the operating system. ",think cause run alone without training see whether get stuck start one possible reason could get stuck much memory operating system,issue,negative,positive,neutral,neutral,positive,positive
556036542,">  I was wondering which metric is monitored during the training/validation for saving the best model?

no metric is monitored to save the best model

By writing a callback, you can access history monitor data and stop training when you need to. See https://tensorpack.readthedocs.io/tutorial/extend/callback.html#what-you-can-do-in-the-callback",wondering metric saving best model metric save best model writing access history monitor data stop training need see,issue,positive,positive,positive,positive,positive,positive
555608136,"@Seraphli @DeeperDeeper I'm running into the similar problem with the Warning and training is stopped midway. Upon inspecting, it was because of the dataloader memory consumption (100% of 120G) and thus got killed by slurm. Any idea how it was solved?",running similar problem warning training stopped midway upon memory consumption thus got idea,issue,negative,neutral,neutral,neutral,neutral,neutral
555384684,"The script needs to download a file but it cannot do so:
> ERR Failed to download http://dl.caffe.berkeleyvision.org/caffe_ilsvrc12.tar.gz

This is a network issue.",script need file err network issue,issue,negative,neutral,neutral,neutral,neutral,neutral
554625864,"@ppwwyyxx Thank you Yuxin! But in the batch with size of 256, the foreground anchors seems much less than the number of background anchors. ",thank batch size foreground much le number background,issue,negative,positive,positive,positive,positive,positive
553958228,"In general we cannot help users find parameters for their models, so closing.

The anchor settings do not look reasonable. I suggest you not changing them.",general help find anchor look reasonable suggest,issue,negative,positive,positive,positive,positive,positive
553177943,"Hi Yuxin, thanks a lot for comments! Sorry for my misunderstanding. `Tensorpack` has automatically adopted the `StagingInput` in `FasterRCNN` with multi-gpu settings. I'll learn that.

Yes, I'm trying to use the `StagingArea` in an independent network. My understanding is that the data prefetching queue will directly cache the data in the GPU memory (i.e., `StagingArea`), but when I set the `capacity=1` or `capacity=64`, it seems no obvious increase in terms of GPU memory, and the GPU utilization sometimes are 60%, sometimes 99%. ",hi thanks lot sorry misunderstanding automatically adopted learn yes trying use independent network understanding data queue directly cache data memory set obvious increase memory utilization sometimes sometimes,issue,positive,negative,neutral,neutral,negative,negative
552960969,"I don't get what you're trying to do. Are you attempting to use `StagingArea` without tensorpack trainer? In that case it's not a tensorpack question.

The `MultiProcessMapData` dataflow produces anything that the `map_func` (which you provide to it) returns.",get trying use without trainer case question anything provide,issue,negative,neutral,neutral,neutral,neutral,neutral
551169496,How to use dataflow as a pure python generator is documented at https://tensorpack.readthedocs.io/tutorial/dataflow.html#run-the-dataflow. If anything else is unclear it should be Keras.,use pure python generator anything else unclear,issue,negative,positive,positive,positive,positive,positive
551154323,"Unfortunally  https://tensorpack.readthedocs.io/tutorial/dataflow.html this won't work as a data generator for keras. It yields shape is not defined.
AttributeError: 'BatchData' object has no attribute 'shape'",wo work data generator shape defined object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
551114072,I see. Are there plans for tensorflow 2.0.? Is it possible to create a data generator using dataflow so keras can use it for training? Any light very welcome.  ,see possible create data generator use training light welcome,issue,positive,positive,positive,positive,positive,positive
551062267,"Any iterable can be a dataflow so in the most general case you can always access `trainer.epoch_num` in your dataflow. However that means some of the builtin tools about multiprocessing will need more effort to work since the processes don't have access to the information available in trainer.


An alternative is just to pass some epoch length to your dataflow and let it do this independently.",iterable general case always access however need effort work since access information available trainer alternative pas epoch length let independently,issue,negative,positive,positive,positive,positive,positive
550095990,"The builtin example https://github.com/tensorpack/tensorpack/blob/master/examples/keras/mnist-keras.py runs well in my environment of TF 1.13.
If you've made changes, please undo them. If the issue persists, please include environment information as requested in the issue template.",example well environment made please undo issue please include environment information issue template,issue,positive,neutral,neutral,neutral,neutral,neutral
549458217,"It doesn't seem reasonable to add more Keras support, so closing.",seem reasonable add support,issue,negative,positive,positive,positive,positive,positive
548280791,The [PTB example](https://github.com/tensorpack/tensorpack/tree/master/examples/PennTreebank) does this by saving the states into an variable. A StagingArea (i.e. a queue on GPU) can also be used for this. ,example saving variable queue also used,issue,negative,neutral,neutral,neutral,neutral,neutral
547932660,"As said in the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md):

> If you expect the model to work better, only in one of the two conditions can we help with it:
(1) You're unable to reproduce the results documented in tensorpack examples.
(2) It appears to be a tensorpack bug.",said issue template expect model work better one two help unable reproduce bug,issue,positive,neutral,neutral,neutral,neutral,neutral
547029297,The example command has a typo. Will fix it sooon,example command typo fix,issue,negative,neutral,neutral,neutral,neutral,neutral
546615202,"Hi Yuxin, thanks a lot for your kind comments! 
In regard to the gradients of `nccl` ops, I followed your great suggestions noted in the `Performance tuning` section of the documentation of `Tensorpack`. I found those  gradients are not expected in the above case. I'm not sure if those branches would run in the training procedure. If they are executed but they have no contribution to the parameter update, then they can be cut or removed in the graph, and then speed up the training.",hi thanks lot kind regard great noted performance tuning section documentation found case sure would run training procedure executed contribution parameter update cut removed graph speed training,issue,positive,positive,positive,positive,positive,positive
546610833,"> Is my understanding correct?

yes

> But in other backbone for ResNet50, there are some biass:

It's a bug in those models and they are not used.

> there are some configurations on the learning_rate for the bias

they do not matter.",understanding correct yes backbone bug used bias matter,issue,negative,neutral,neutral,neutral,neutral,neutral
546605654,"Yes, from the last line:

    final_loss = tf.reduce_sum(loss) / one_scalar_sum_avg

It is expected that the error from the `final_loss` would just back to the `loss` branch, but not back to the `one_scalar_sum_avg` branch. So I stop the gradient flow on the `one_scalar_sum_avg`. But if the error flow is stopped in `loss`, then the parameters will not be updated by the gradient descent. Is my understanding correct?

In addition, I have another question on the `backbone` in `FasterRCNN` module. In the 

https://github.com/tensorpack/tensorpack/blob/7e923bfe7fdb97f5e5f9c9ef0df9967658fa5b5f/examples/FasterRCNN/modeling/backbone.py#L66

![CA3E977F-9CE8-4dc3-93E5-2FD0BFEAF863](https://user-images.githubusercontent.com/43327429/67620622-7cad4580-f83b-11e9-98d7-2d7a6d1fc307.png)

But in other backbone for `ResNet50`,  there are some `bias`s:

![0E71DA90-52B5-457e-BB53-7B1350897BDE](https://user-images.githubusercontent.com/43327429/67620650-bda55a00-f83b-11e9-844a-9d9a338115c8.png)

And from the code:

https://github.com/tianzhi0549/FCOS/blob/b516eb54dc069d9c0fadb0d51eeead7dd1bb4910/fcos_core/solver/build.py#L15

there are some configurations on the `learning_rate` for the `bias`. But these configurations are removed from the `FasterRCNN` module. I must miss something, could you please give some advice? Thanks for your time.",yes last line loss error would back loss branch back branch stop gradient flow error flow stopped loss gradient descent understanding correct addition another question backbone module backbone bias code bias removed module must miss something could please give advice thanks time,issue,negative,positive,neutral,neutral,positive,positive
546603613,gradient will not propagate from `one_scalar_sum_avg` but will still propagate from `loss` because you did not stop gradient on `loss`.,gradient propagate still propagate loss stop gradient loss,issue,negative,neutral,neutral,neutral,neutral,neutral
546431048,"> I use tensorflow 1.14 from pip but I don't think tensorflow version matters here.

@ppwwyyxx: I remove anaconda3 (conda) and use system ""pip3"" to directly install tensorflow-1.15. The training performance is tremendously improved. With 8xGPU V100 and 40 cores CPU, training one epoch took 14 minutes which is pretty good. Thank for your help.",use pip think version remove anaconda use system pip directly install training performance tremendously training one epoch took pretty good thank help,issue,positive,positive,positive,positive,positive,positive
546179464,"@ppwwyyxx Thanks a lot, Yuxin. Sorry for my misunderstanding for `GroupNorm` and `tf.nn.batch_normalization`. I have read your paper on the `GroupNorm`. `GroupNorm` is indenpendent of the batch size. So no synchronization is needed. ",thanks lot sorry misunderstanding read paper batch size synchronization,issue,negative,negative,negative,negative,negative,negative
545813162,@roshan-gopalakrishnan  I could not understand what you're asking and it also seems unrelated to this issue.,could understand also unrelated issue,issue,negative,neutral,neutral,neutral,neutral,neutral
545810250,"If I use the 1 bit activation for my last layer, I see there is an error in the accuracy calculation. Is that right? How is the calculation becoming wrong?",use bit activation last layer see error accuracy calculation right calculation becoming wrong,issue,negative,positive,neutral,neutral,positive,positive
545636816,I use tensorflow 1.14 from pip but I don't think tensorflow version matters here.,use pip think version,issue,negative,neutral,neutral,neutral,neutral,neutral
545636160,"@ppwwyyxx: btw, I installed tensorflow-1.14 and tensorpack-0.9.8 via ""conda"" environment. Is it same to the one that you used to runDoReFa alexnet or others (virtualenv, install from source, docker etc.)",via environment one used install source docker,issue,negative,neutral,neutral,neutral,neutral,neutral
545582479,"> The speed is not slow when I run it. You can follow https://tensorpack.readthedocs.io/tutorial/performance-tuning.html#figure-out-the-bottleneck to see whether your training is bottlenecked by data loading.

@ppwwyyxx: Thanks for tips. I will deep dive into the link. I also thought that is due to data loading from CPU to GPU. I have upto 40 cores so the data loading should be fast. I will get back to you later when I have more information.",speed slow run follow see whether training data loading thanks deep dive link also thought due data loading data loading fast get back later information,issue,negative,negative,neutral,neutral,negative,negative
545501589,"I mean speed because the original post is about speed.

If you believe there is an issue, please point out where. Otherwise I don't know what to check.",mean speed original post speed believe issue please point otherwise know check,issue,positive,positive,neutral,neutral,positive,positive
545496940,"I din't mean speed but the top 1 accuracy calculation whether it is correct in the tensorpack package?
I see some issue with the implementation of top 1 accuracy and top 5 accuracy. Can you check if the implementation is correct or not ?",di mean speed top accuracy calculation whether correct package see issue implementation top accuracy top accuracy check implementation correct,issue,positive,positive,positive,positive,positive,positive
545479944,The speed is not slow when I run it. You can follow https://tensorpack.readthedocs.io/tutorial/performance-tuning.html#figure-out-the-bottleneck to see whether your training is bottlenecked by data loading.,speed slow run follow see whether training data loading,issue,negative,negative,negative,negative,negative,negative
545311223,"Hi, Is the top 1 accuracy calculation used in the alexnet-dorefa.py correct? I am using a different architecture like MobileNet and 1,1,32 dorefa structure to train and the top 1 accuracy seems quite good.    ",hi top accuracy calculation used correct different architecture like structure train top accuracy quite good,issue,positive,positive,positive,positive,positive,positive
545191923,"@ppwwyyxx Yeah, got it. Thanks a lot for your kind comment!",yeah got thanks lot kind comment,issue,positive,positive,positive,positive,positive,positive
545191335,"@ppwwyyxx Hi Yuxin, got it. Thanks a lot for your kindly comment!",hi got thanks lot kindly comment,issue,positive,positive,positive,positive,positive,positive
545051798,"That's usually a safe message from tensorflow and no need to worry. I do not know how to trace it and it's a tensorflow message, so you can ask in tensorflow issues for help.",usually safe message need worry know trace message ask help,issue,negative,positive,positive,positive,positive,positive
545051101,"`build_graph` returns a scalar loss, so how to obtain it is up to you.
The loss from each `build_graph` is either averaged or added based on the `average` option in https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.SyncMultiGPUTrainerReplicated or other trainers.",scalar loss obtain loss either added based average option,issue,negative,negative,negative,negative,negative,negative
544845350,"Take the alexnet for example. 
logits = FullyConnected('fc8', l, 1000, kernel_initializer=gauss_init)
return logits

After Fc, it directly returns the logits. Thanks for your help",take example return directly thanks help,issue,positive,positive,positive,positive,positive,positive
544834140,"I am training the Resnet34 model as explained in the tensorpack githbub repository. I am initialising some of the weight with some weights using Dict. However, I will like to freeze the weight after transferring them and I dont know how to do it?
Please can you give me some ideas?
Thank you.
",training model repository weight however like freeze weight transferring dont know please give thank,issue,positive,neutral,neutral,neutral,neutral,neutral
544730474,"Thanks for your response. Does this ""tfutils.collection.freeze_collection(keys)"" achieve the same purpose?",thanks response achieve purpose,issue,negative,positive,positive,positive,positive,positive
544714785,"thanks for your help.
I want to freeze some weight in the model during training.
Please can you give me an idea on how to do it?",thanks help want freeze weight model training please give idea,issue,positive,positive,positive,positive,positive,positive
544291359,"The FasterRCNN dataflow already has enough randomness from random shuffle, so there is not much need to set the augmentation seed. Similar for the imagenet one.

In the imagenet example, you're using threads for each worker, instead of processes. And there is no need to reset RNG for threads.",already enough randomness random shuffle much need set augmentation seed similar one example worker instead need reset,issue,negative,negative,neutral,neutral,negative,negative
544251464,"In [FasterRCNN dataflow](https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/data.py#L96), augmentation is done without builtins like `AugmentImageComponent()` who are responsible for  calling `self.augs.reset_state()`  in dataflow's `reset_state()` method, so dose this dataflow assumes python >=3.7 ? Shall we call `self.aug.reset_state()`  before `tfms = self.aug.get_transform(im) `if using python < 3.7?  

And which one of the below is ""correct"" if I want a train dataflow processed in this way(modified from [imagenet_utils.py](https://github.com/tensorpack/tensorpack/blob/master/examples/ImageNetModels/imagenet_utils.py#L96)):
```
ds = dataset.ILSVRC12Files(datadir, name, shuffle=True)
aug = imgaug.AugmentorList(augmentors)
# call aug.reset_state() here? if not, where and when?
def mapf(dp):
        fname, cls = dp
        im = cv2.imread(fname, cv2.IMREAD_COLOR)
        im = aug.augment(im)
        return im, cls
ds = MultiThreadMapData(ds, parallel, mapf, buffer_size=2000)
ds = BatchData(ds, batch_size, remainder=True)
ds = MultiProcessRunnerZMQ(ds, 1)
```
vs
```
ds = dataset.ILSVRC12Files(datadir, name, shuffle=True)
aug = imgaug.AugmentorList(augmentors)
def mapf(dp):
        fname, cls = dp
        im = cv2.imread(fname, cv2.IMREAD_COLOR)
        #call aug.reset_state() here? if not, where and when?
        im = aug.augment(im)
        return im, cls
ds = MultiThreadMapData(ds, parallel, mapf, buffer_size=2000)
ds = BatchData(ds, batch_size, remainder=True)
ds = MultiProcessRunnerZMQ(ds, 1)
```

",augmentation done without like responsible calling method dose python shall call python one correct want train way name call return parallel name call return parallel,issue,negative,positive,neutral,neutral,positive,positive
544111957,"@ppwwyyxx Yuxin, thanks a lot for your kind reply!",thanks lot kind reply,issue,positive,positive,positive,positive,positive,positive
544069453,"That's usually because some libraries you imported have mistakenly set start method at import time.
I know that old versions of tqdm do this, but maybe there are certain versions of other libraries making similar mistakes that I'm not aware of.

Setting `force=True` should be fine.

> 'get_batch_train_dataflow.<locals>.preprocess'

Our code no longer has this function because lambda is not supported by ""spawn""",usually mistakenly set start method import time know old maybe certain making similar aware setting fine code longer function lambda spawn,issue,negative,positive,positive,positive,positive,positive
544069172,"But there is another problem:

    AttributeError: Can't pickle local object 'get_batch_train_dataflow.<locals>.preprocess'",another problem ca pickle local object,issue,negative,neutral,neutral,neutral,neutral,neutral
544068633,"@ppwwyyxx  I found add `force=True` will solve the above problem.

    mp.set_start_method('spawn', force=True)",found add solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
544067446,"@ppwwyyxx Hi Yuxin, thank you for your kind advice. 

In fact, I have commented the two lines as there is an error like this:

    Traceback (most recent call last):
      File ""train.py"", line 32, in <module>
         mp.set_start_method('spawn')
      File ""/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/context.py"", line 242, in set_start_method
         raise RuntimeError('context has already been set')
     RuntimeError: context has already been set",hi thank kind advice fact two error like recent call last file line module file line raise already set context already set,issue,positive,positive,positive,positive,positive,positive
544036703,"Run the dataflow alone first to see whether it has memory leak: https://tensorpack.readthedocs.io/tutorial/dataflow.html#run-the-dataflow.
Then, simplify the dataflow gradually.

You may want to try `set_start_method` (exists in our `FasterRCNN/train.py`), which should avoid an issue with forking that looks like memory leak.",run alone first see whether memory leak simplify gradually may want try avoid issue like memory leak,issue,negative,positive,positive,positive,positive,positive
543030936,"You can use the returned image in `sample()` without having to modify any tensorpack code.

> It is taking very long to generate. Is there any reason for that?

maybe that's just how fast it is. maybe your hardware is slow. maybe you install some softwares incorrectly. maybe it spends time on something else other than generating",use returned image sample without modify code taking long generate reason maybe fast maybe hardware slow maybe install incorrectly maybe time something else generating,issue,negative,negative,neutral,neutral,negative,negative
543029847,"I passed **viz=False** in the argument of sample() function. In the **tensorpack/utils/viz.py** I added an extra line as **imsave(canvas.canvas,""out.jpg"")** in stack_patches() fn before return call.

It is taking very long to generate. Is there any reason for that? ",argument sample function added extra line return call taking long generate reason,issue,negative,negative,neutral,neutral,negative,negative
542553040,"> class ImageList(object):

It takes much more code than this class to handle batches in pytorch as well.

We do not handle batches in this implementation. There is a fork of this implementation that handles batches, implemented for a subset of models: https://github.com/aws-samples/mask-rcnn-tensorflow",class object much code class handle well handle implementation fork implementation subset,issue,negative,positive,positive,positive,positive,positive
542551127,"> As it said, we do not support batch size > 1.

我目前的输入图片尺寸 是固定的 ，都是 600*800；要支持多batchsize训练，直接稍微改一下一应就可以了吧；
pytorch 如此处理：

class ImageList(object):
    """"""
    Structure that holds a list of images (of possibly
    varying sizes) as a single tensor.
    This works by padding the images to the same size,
    and storing in a field the original sizes of each image
    """"""

    def __init__(self, tensors, image_sizes):
        """"""
        Arguments:
            tensors (tensor)
            image_sizes (list[tuple[int, int]])
        """"""
        self.tensors = tensors
        self.image_sizes = image_sizes

    def to(self, *args, **kwargs):
        cast_tensor = self.tensors.to(*args, **kwargs)
        return ImageList(cast_tensor, self.image_sizes)



那么对于 tf 给点建议",said support batch size class object structure list possibly size single tensor work padding size field original size image self tensor list self return,issue,positive,positive,positive,positive,positive,positive
542492195,"As it said, we do not support batch size > 1.",said support batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
542487844,"> answered in https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md

Possible Future Enhancements:

Support batch>1 per GPU. Batching with inconsistent shapes is non-trivial to implement in TensorFlow.

Use dedicated ops to improve speed. (e.g. a TF implementation of ROIAlign op can be found in light-head RCNN)


给个建议 怎么修改 batchsize 》 1

",possible future support batch per inconsistent implement use improve speed implementation found,issue,positive,neutral,neutral,neutral,neutral,neutral
542463916,"> I understood that for each image we have a list of numpy arrays, where each numpy array corresponds to an instance and is composed of (x,y) coordinates.

This is not true. As the document says:

> segmentation: k lists of numpy arrays (one for each instance).
Each list of numpy arrays corresponds to the mask for one instance.
Each numpy array in the list is a polygon of shape Nx2,
because one mask can be represented by N polygons.

It's k lists of numpy arrays. Not a list of numpy arrays.",understood image list array instance composed true document segmentation one instance list mask one instance array list polygon shape one mask list,issue,negative,positive,positive,positive,positive,positive
542126626,"    def preprocess(self, image):
        image = tf.expand_dims(image, 0)
        image = image_preprocess(image, bgr=True)
        return tf.transpose(image, [0, 3, 1, 2])",self image image image image image return image,issue,negative,neutral,neutral,neutral,neutral,neutral
541924773,"> can I expect correct BatchNorm behavior with multiple GPU?

Depends on what behavior you want exactly..

The way you use `tf.slim` will work the same as using tensorpack's `BatchNorm`.",expect correct behavior multiple behavior want exactly way use work,issue,negative,positive,positive,positive,positive,positive
540912691,"This is about the usage of the COCO evaluation API and is not related to tensorpack.
The evaluation API is able to provide per-category results. Some example usage of the API can be found at https://github.com/facebookresearch/detectron2/blob/b84b7beec8666209140a235e3c7ba3428270eb0d/detectron2/evaluation/coco_evaluation.py#L257-L270",usage coco evaluation related evaluation able provide example usage found,issue,negative,positive,positive,positive,positive,positive
540912038,"My model has only 2 classes. BG and Person. So when I run through evaluation it will give me only persons in the output. While comparing with the ground truth, I need to make sure I calculate the numbers with only a person's ground truth and not anything else. That's why I need to pass the GT boxes of the person when we read the data.",model class person run evaluation give output ground truth need make sure calculate person ground truth anything else need pas person read data,issue,positive,positive,positive,positive,positive,positive
540904268,You can filter out all the outputs whose predicted label is not person.,filter whose label person,issue,negative,neutral,neutral,neutral,neutral,neutral
540725905,"If they will be trained without loading from dict, they will be trained after loaded from dict.",trained without loading trained loaded,issue,negative,neutral,neutral,neutral,neutral,neutral
540717614,Whether a parameter will be trained is only determined by the model itself.,whether parameter trained determined model,issue,negative,neutral,neutral,neutral,neutral,neutral
540715672,"> http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html has everything about this topic.
> Three ways you can do this:
> 
> 1. rename variables in your graph
> 2. SaverRestore supports the argument to ignore variables
> 3. convert the model to a dict and remove some variables from it.

I just attempted to do this. I will like to know if the parameters being initialised from the dict will be trained. thanks",everything topic three way rename graph argument ignore convert model remove like know trained thanks,issue,negative,positive,positive,positive,positive,positive
540307723,The ZMQ protocol we use does not support windows. Please replace the ZMQ dataflow in your code by the non-zmq version.,protocol use support please replace code version,issue,positive,neutral,neutral,neutral,neutral,neutral
540110218,"The log is saying that ""./train_log/maskrcnn/model-1170000.data-00000-of-00001"" you provided in the command line does not exist.

If this is not known to you: you need to let your new job use a new directory, otherwise the job will overwrite the old directory (train_log/maskrcnn).",log saying provided command line exist known need let new job use new directory otherwise job overwrite old directory,issue,negative,positive,positive,positive,positive,positive
539941276,Didn't pass the index file in the argument while running predict py file.,pas index file argument running predict file,issue,negative,neutral,neutral,neutral,neutral,neutral
539922981,Ok. Understood. Thanks for the time and explanation.,understood thanks time explanation,issue,negative,positive,positive,positive,positive,positive
539069178,"Because a checkpoint contains optimizer state, typically doubles its size.
They also have different format, which may cause difference in size.",state typically size also different format may cause difference size,issue,negative,negative,neutral,neutral,negative,negative
538905431,I am using 'COCO-MaskRCNN-R50FPN2x.npz' as the starting point for my training which is 158 MB in size. So how can there be so much relative difference in size?,starting point training size much relative difference size,issue,negative,neutral,neutral,neutral,neutral,neutral
538902096,I think that's just how large the checkpoint is supposed to be. I don't see anything unexpected here so I'm not sure what to answer.,think large supposed see anything unexpected sure answer,issue,negative,positive,positive,positive,positive,positive
538900656,"![model_ckpt](https://user-images.githubusercontent.com/16095226/66297045-7e56ae00-e90c-11e9-97c4-8f35dc6f944b.png)

The trained models generated while training faster rcnn are 472MB which is quite high. Can you explain the reason why the models are heavy in size? ",trained training faster quite high explain reason heavy size,issue,negative,negative,neutral,neutral,negative,negative
538835188,"That's interesting. It seems like the behavior of out-of-bound access may depend on the type of GPU. An assertion was added in bbf29a18e6df2a84878663a343f751a05fa103f1

After every epoch, tensorboard event files are saved in the output directory.",interesting like behavior access may depend type assertion added every epoch event saved output directory,issue,positive,positive,positive,positive,positive,positive
538833609,Is there any way through which I can visualize the training losses and accuracy through tensorboard?,way visualize training accuracy,issue,negative,neutral,neutral,neutral,neutral,neutral
538833390,"By adding the line you mentioned above, the training started properly. Thanks for the prompt and accurate response. 

Although, I am a bit confused as even though it went out of bound access for GPU while loading data annotations it didn't crash or showed anything in the log. I think an assert for checking the upper bound while loading data would be helpful.

Thanks.
Deval Shah",line training properly thanks prompt accurate response although bit confused even though went bound access loading data crash anything log think assert upper bound loading data would helpful thanks shah,issue,positive,positive,neutral,neutral,positive,positive
538777098,"First I assume you also changed `config.py` and modify `CLASS_NAMES` to `[""BG"", ""person""]`.
After applying your changes and running the command I cannot reproduce the stuck. 
One suspicious issue is that you have not modified the data loader to let it only return annotations of ""person"" class. You need this as well:
```diff
diff --git i/examples/FasterRCNN/dataset/coco.py w/examples/FasterRCNN/dataset/coco.py
index 78b9d086..bb89586d 100644
--- i/examples/FasterRCNN/dataset/coco.py
+++ w/examples/FasterRCNN/dataset/coco.py
@@ -142,6 +142,8 @@ class COCODetection(DatasetSplit):
         all_cls = []
         all_iscrowd = []
         for objid, obj in enumerate(objs):
+            if obj[""category_id""] != 1:
+                continue
             if obj.get('ignore', 0) == 1:
                 continue
             x1, y1, w, h = list(map(float, obj['bbox']))
```
Otherwise, it will send all annotations for training, and probably will cause out-of-bound memory access. Despite it is still runnable in my environment, maybe this out-of-bound access can cause your GPU to stuck. 

Also, if you have not, wait for >5 minutes for the first iteration.
If there is still no findings, see if using one GPU and `DATA.NUM_WORKERS=0` can provide more observations.",first assume also modify person running command reproduce stuck one suspicious issue data loader let return person class need well git index class enumerate continue continue list map float otherwise send training probably cause memory access despite still runnable environment maybe access cause stuck also wait first iteration still see one provide,issue,negative,positive,positive,positive,positive,positive
538746206,Shifted the issue under the given template. Please check.,issue given template please check,issue,negative,neutral,neutral,neutral,neutral,neutral
538724025,"I cannot tell what modifications you did. It would be more useful if you could describe what you did following the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md). More specifically:

(1) **If you're using examples, what's the command you run:**

(2) **If you're using examples, have you made any changes to the examples? Paste `git status; git diff` here:**

To train on only one category you'll also need to skip objects of other categories in `coco.py` when loading the annotations.",tell would useful could describe following issue template specifically command run made paste git status git train one category also need skip loading,issue,negative,positive,positive,positive,positive,positive
538723167,"I ran the code without my modifications. Its running fine. 

I want to train faster rcnn for only two classes [""BG"",""Person""] from coco dataset, so I did modifications in register coco for setting class names as the above instead of taking all 80 classes.

The other modification I did was while loading the Backbone.Weights, to avoid the last layer mismatch I added ""ignore_mismatch =True"" in the SmartInit call to avoid the consideration of last layer while loading pretrained weights

The second screenshot attached above shows the last screen I saw. Is there any other modification I need to do in order to meet above specifications.",ran code without running fine want train faster two class person coco register coco setting class instead taking class modification loading avoid last layer mismatch added call avoid consideration last layer loading second attached last screen saw modification need order meet,issue,negative,positive,neutral,neutral,positive,positive
538675297,"Could you first try the command in the example and code without modifications, to rule out environment issues?
If it is not an environment issue, could you post following the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md) that you deleted?

More specifically:
* By ""train person class only"", what exactly did you do?
* By ""stuck"", what exactly did you observe: What are the logs, What is the last screen you saw, and how long have you waited for it.",could first try command example code without rule environment environment issue could post following issue template specifically train person class exactly stuck exactly observe last screen saw long,issue,negative,positive,positive,positive,positive,positive
537232490,"This almost surely have nothing to do with augmentation. In fact, the root cause of any cuda error should not be in tensorpack. I think your best chance to address such error is to upgrade nvidia driver or cuda or tensorflow. ",almost surely nothing augmentation fact root cause error think best chance address error upgrade driver,issue,positive,positive,positive,positive,positive,positive
537225648,"Reopening because I get some kind of crash if I pass `imgaug.AugmentorList` an empty list or a list containing only a `CustomResize` object. 

The reason I would like to disable this is my data is based on synthetic-aperture radar satellite images and flipping is not a physical transformation in this context. 

```
2019-09-25 16:00:26.655938: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-09-25 16:00:26.655999: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event sus: 1
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
/home/ya006948/venvs/sbda/lib64/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x7bb76b)[0x7fe7abcab76b]
/usr/lib64/libpthread.so.0(+0xf5d0)[0x7fe84653e5d0]
/usr/lib64/libc.so.6(gsignal+0x37)[0x7fe845a8f2c7]
/usr/lib64/libc.so.6(abort+0x148)[0x7fe845a909b8]
/home/ya006948/venvs/sbda/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x753eb24)[0x7fe7b3b4fb24]
/home/ya006948/venvs/sbda/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow8EventMgr10PollEventsEbPN4absl13InlinedVectorINS0_5InUseELm4ESaIS3_EEE+0x105)[0x7fe7b3dfe515]
/home/ya006948/venvs/sbda/lib64/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow8EventMgr8PollLoopEv+0xce)[0x7fe7b3dfe9fe]
/home/ya006948/venvs/sbda/lib64/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN5Eigen15ThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x306)[0x7fe7abc84dc6]
/home/ya006948/venvs/sbda/lib64/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x44)[0x7fe7abc83c84]
/usr/lib64/libstdc++.so.6(+0xb5070)[0x7fe8398e6070]
/usr/lib64/libpthread.so.0(+0x7dd5)[0x7fe846536dd5]
/usr/lib64/libc.so.6(clone+0x6d)[0x7fe845b5702d]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
        tensorflow::CurrentStackTrace()


        gsignal
        abort

        tensorflow::EventMgr::PollEvents(bool, absl::InlinedVector<tensorflow::EventMgr::InUse, 4ul, std::allocator<tensorflow::EventMgr::InUse> >*)
        tensorflow::EventMgr::PollLoop()
        Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)
        std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)


        clone
*** End stack trace ***
```

This crash does not happen consistently at the same epoch (sometimes early, sometimes after 10 - 15 epochs, sometimes after 30+ epochs).  By comparison, I left a training run training fro 166+ epochs before stopping training and there was no similar crash.

Thanks for any insight

EDIT: My current workaround is to use a low probability of flipping i.e. `prob=1e-9`",get kind crash pas empty list list object reason would like disable data based radar satellite physical transformation context error polling event status query event unspecified launch failure unexpected event received signal begin stack trace end stack trace begin stack trace abort bool void void lambda clone end stack trace crash happen consistently epoch sometimes early sometimes sometimes comparison left training run training fro stopping training similar crash thanks insight edit current use low probability,issue,negative,positive,neutral,neutral,positive,positive
536274926,"hello, i have a related question
i want to get all the prediction, but i want to run prediction on all the images in a given directory not a single image.Is there an already implemented way or i have to modify on the code ?
",hello related question want get prediction want run prediction given directory single already way modify code,issue,negative,negative,neutral,neutral,negative,negative
535819927,"Using different tower_names is fine, I just missed this part of the docs. Thank you!",different fine part thank,issue,positive,positive,positive,positive,positive,positive
535599868,"Thank you, I will close this thread as the issue now seems unrelated to tensorpack",thank close thread issue unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
535598733,">  After a day of training wouldn't the network at least come to the conclusion that it should propose something

I would not say so very definitely.

You can load a provided model and test it on an image of common objects (people, cars, etc) to see whether your usage is correct. Otherwise it sounds like your model does not produce useful results, which is more of a machine learning question than a tensorpack issue..",day training would network least come conclusion propose something would say definitely load provided model test image common people see whether usage correct otherwise like model produce useful machine learning question issue,issue,positive,negative,neutral,neutral,negative,negative
535594580,"I considered this possibility, but every image contains a bounding box. After a day of training wouldn't the network at least come to the conclusion that it should propose something? Also when training the network I can clearly see that it loaded in the data set properly and identified the 8000 or so bounding boxes in my data set. ",considered possibility every image bounding box day training would network least come conclusion propose something also training network clearly see loaded data set properly bounding data set,issue,negative,negative,neutral,neutral,negative,negative
534908214,"Hi @monjurulkarim , 

Yes, it takes time. As long as the script is working you should be happy. 

It took me 7-8 days to split the instances images into small elements and 4-5 days to create the json files. I guess it also depend on the machine you are using, mine has a CORE i7 7th Gen.  ",hi yes time long script working happy took day split small day create guess also depend machine mine core th gen,issue,positive,positive,positive,positive,positive,positive
534661780,"```python
from tensorpack.models.registry import disable_layer_logging
disable_layer_logging()
```

this is not yet a public API. but it seems reasonable to make it one",python import yet public reasonable make one,issue,negative,positive,neutral,neutral,positive,positive
534625976,"@AlbertoMCS Hi, how long did it take for you to completely convert the dataset? It is taking too long for me. I started 8 days ago, but still it's working.",hi long take completely convert taking long day ago still working,issue,negative,neutral,neutral,neutral,neutral,neutral
534366579,"Could you install `python-prctl` and retry? https://pythonhosted.org/python-prctl/

Tensorpack logs probably have told you that some processes will be left if it is not installed. ",could install retry probably told left,issue,negative,neutral,neutral,neutral,neutral,neutral
534366158,"thx for your reply. 
```
sys.platform          linux
Python                3.6.0 (default, Dec 24 2018, 17:02:27) [GCC 5.4.0 20160609]
Tensorpack            v0.9.8-0-gb6318616
Numpy                 1.15.4
TensorFlow            1.12.0/v1.12.0-0-ga6d8ffae09
TF Compiler Version   4.8.5
TF CUDA support       True
TF MKL support        False
Nvidia Driver         /usr/lib/nvidia-384/libnvidia-ml.so.384.130
CUDA                  /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
CUDNN                 /usr/lib/libcudnn.so.7.1.4
NCCL                  /usr/lib/x86_64-linux-gnu/libnccl.so.2.4.8
CUDA_VISIBLE_DEVICES  None
GPU 0,1,2             GeForce GTX 1080 Ti
Free RAM              61.99/62.83 GB
CPU Count             12
cv2                   4.1.0
msgpack               0.6.1
python-prctl          False
```",reply python default compiler version support true support false driver none ti free ram count false,issue,positive,negative,neutral,neutral,negative,negative
534365720,"Could you include your environment information following the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/unexpected-problems---bugs.md)?

> 4. Your environment:
Paste the output of this command: python -c 'import tensorpack.tfutils as u; print(u.collect_env_info())' If this command failed, tell us your version of Python/TF/tensorpack.
Note that:
You can install Tensorpack master by pip install -U git+https://github.com/ppwwyyxx/tensorpack.git and see if your issue is already solved.
If you're not using tensorpack under a normal command line shell (e.g., using an IDE or jupyter notebook), please retry under a normal command line shell.
You may often want to provide extra information related to your issue, but at the minimum please try to provide the above information accurately to save effort in the investigation.",could include environment information following issue template environment paste output command python print command tell u version note install master pip install see issue already normal command line shell ide notebook please retry normal command line shell may often want provide extra information related issue minimum please try provide information accurately save effort investigation,issue,positive,positive,positive,positive,positive,positive
533779615,Feel free to reopen if this does not solve your issue.,feel free reopen solve issue,issue,positive,positive,positive,positive,positive,positive
533678720,"This specific issue can be fixed in the above commit, but in general it's still best to use different names because other types of name conflicts may also exist.",specific issue fixed commit general still best use different name may also exist,issue,positive,positive,positive,positive,positive,positive
533672207,"Maybe this can be made automatic, though, i.e., detect that the name scope has been used and create the graph in a new name scope. But for now you need to specify the name for new `InferenceRunner`.",maybe made automatic though detect name scope used create graph new name scope need specify name new,issue,negative,positive,positive,positive,positive,positive
533671786,"Please check https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.InferenceRunner:

> tower_name (str) – the name scope of the tower to build. Need to set a different one if multiple InferenceRunner are used.",please check name scope tower build need set different one multiple used,issue,negative,neutral,neutral,neutral,neutral,neutral
533616378,You are right! It was my mistake. I had an extra '/' in my path. It worked in other part of the code and cli but this check failed. Sorry for the confusion.,right mistake extra path worked part code check sorry confusion,issue,negative,negative,neutral,neutral,negative,negative
533566267,"Hi, I am facing the same problem(Trying to load a tensor of shape (80,) into the variable 'maskrcnn/conv/b' whose shape is (1,).), but I am still confused about what to do next with np.load and np.savez.",hi facing problem trying load tensor shape variable whose shape still confused next,issue,negative,negative,negative,negative,negative,negative
533436363,"(1) because x and y are coordinates defined in https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md#model in range [0, W] or [0, H].
(2) that's true, although it is very rare to have a box that's off by so much.",defined range true although rare box much,issue,negative,positive,positive,positive,positive,positive
533434095,"hi, I have some questions with clip_boxes function in common.py and model_box.py.
1. when clip boxes, upper bound is w and h, why not w-1 and h-1?
2. boxes[:, [0, 1]] also need to minimum with w and h in clip_boxes function in common.py 

def clip_boxes(boxes, shape):
    """"""
    Args:
        boxes: (...)x4, float
        shape: h, w
    """"""
    orig_shape = boxes.shape
    boxes = boxes.reshape([-1, 4])
    h, w = shape
    boxes[:, [0, 1]] = np.maximum(boxes[:, [0, 1]], 0)
    boxes[:, 2] = np.minimum(boxes[:, 2], w)
    boxes[:, 3] = np.minimum(boxes[:, 3], h)
    return boxes.reshape(orig_shape)


@under_name_scope()
def clip_boxes(boxes, window, name=None):
    """"""
    Args:
        boxes: nx4, xyxy
        window: [h, w]
    """"""
    boxes = tf.maximum(boxes, 0.0)
    m = tf.tile(tf.reverse(window, [0]), [2])    # (4,)
    boxes = tf.minimum(boxes, tf.cast(m, tf.float32), name=name)
    return boxes",hi function clip upper bound also need minimum function shape float shape shape return window window window return,issue,negative,neutral,neutral,neutral,neutral,neutral
532764177,"Could you use `lambda x: PReLU('prelu', x)` instead? It should work the same so I think it's not necessary to support a hidden API for something that can be done in a more straightforward way. Sorry if that breaks your code.",could use lambda instead work think necessary support hidden something done straightforward way sorry code,issue,negative,negative,neutral,neutral,negative,negative
532552446,"Solved.
Unlike the official model, the preprocessing of image scaling are not packaged into the model in tensorpack. When using the Pb files, it is necessary to add preprocessing (resize the image) and post-processing (boxes in the results need to be resized according to the resize ratio of preprocessing).",unlike official model image scaling model necessary add resize image need according resize ratio,issue,negative,neutral,neutral,neutral,neutral,neutral
532509679,"You can use `if-else` before calling `decode`. The error is from your code (`cocoeval.py`), and doesn't seem related to tensorpack.",use calling decode error code seem related,issue,negative,neutral,neutral,neutral,neutral,neutral
532377750,"It looks like the commit ""Removed NCHW test from TestConv2DTranspose"" actually also removes the tests for valid padding.",like commit removed test actually also valid padding,issue,positive,neutral,neutral,neutral,neutral,neutral
532367647,"Done.
Also, I decided to remove support for 'full' padding since neither tf.layers.Conv2DTranspose nor tf.nn.conv2d_transpose seem to support that.",done also decided remove support padding since neither seem support,issue,positive,neutral,neutral,neutral,neutral,neutral
532315237,"Thanks for catching this! I just added a unittest in f7a79d48cb00e9bda8d5f6928348d0a90788edc2 for such errors. Could you add ""valid/full"" to the test as well and make sure it passes? 

You can run it with `python3 -m tensorpack.models.models_test   `",thanks catching added could add test well make sure run python,issue,positive,positive,positive,positive,positive,positive
532301804,"Your problem is unrelated to this issue. If that does not solve your problem, please either comment at #1217 or open an issue following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit this link to post issues about unexpected problems).
The comment at https://github.com/tensorpack/tensorpack/issues/1217#issuecomment-496349051 is most relevant to your issue.",problem unrelated issue solve problem please either comment open issue following issue template click new issue unexpected visit link post unexpected comment relevant issue,issue,negative,positive,positive,positive,positive,positive
532127600,"well,thanks.
But i didnt solve it that way, i just set the parameter force=True in mp.set_start_method('spawn')
is it wrong to do that? 
",well thanks didnt solve way set parameter wrong,issue,negative,negative,negative,negative,negative,negative
532116136,"See https://github.com/tensorpack/tensorpack/issues/1217. If that does not solve your problem, 
please open an issue following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",see solve problem please open issue following issue template click new issue unexpected visit link post unexpected,issue,negative,positive,neutral,neutral,positive,positive
532115198,"Hi, when i run the train file i get this error:
    runfile('/home/hasan/Desktop/fstr-rcnn-resnext/train.py', wdir='/home/hasan/Desktop/fstr-rcnn-resnext')

  File ""/home/hasan/.local/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""/home/hasan/.local/lib/python3.6/site-packages/spyder_kernels/customize/spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/home/hasan/Desktop/fstr-rcnn-resnext/train.py"", line 45, in <module>
    mp.set_start_method('spawn')

  File ""/usr/lib/python3.6/multiprocessing/context.py"", line 242, in set_start_method
    raise RuntimeError('context has already been set')

RuntimeError: context has already been set
what could be the problem?",hi run train file get error file line file line compile file line module file line raise already set context already set could problem,issue,negative,neutral,neutral,neutral,neutral,neutral
531861923,"There might be a bug in either a library or your code, but one cannot tell much with limited information you provide.
If you would like others to look into your issue, please provide enough information to let others reproduce your issue.",might bug either library code one tell much limited information provide would like look issue please provide enough information let reproduce issue,issue,negative,negative,neutral,neutral,negative,negative
531630304,"If you understand the shape of ""scores"", then Tensorflow documentation can tell you the shape of ""filtered_ids"".
https://www.tensorflow.org/api_docs/python/tf/where",understand shape documentation tell shape,issue,negative,neutral,neutral,neutral,neutral,neutral
531629914,"Thanks for the reply, it does help me a lot, but I have bit confused about ""filtered_ids"" in examples/FasterRCNN/modeling/model_frcnn.py
https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_frcnn.py#L196
It' said that it'll return the value with shape Fx2, So, what is the value of F , and what the value of 2 columns represent for ?",thanks reply help lot bit confused said return value shape value value represent,issue,positive,negative,neutral,neutral,negative,negative
531482794,"```diff
diff --git i/examples/FasterRCNN/data.py w/examples/FasterRCNN/data.py
index f0eba551..b8e81836 100644
--- i/examples/FasterRCNN/data.py
+++ w/examples/FasterRCNN/data.py
@@ -384,7 +384,7 @@ def get_eval_dataflow(name, shard=0, num_shards=1):
     def f(fname):
         im = cv2.imread(fname, cv2.IMREAD_COLOR)
         assert im is not None, fname
-        return im
+        return fname, im
 
     ds = MapDataComponent(ds, f, 0)
     # Evaluation itself may be multi-threaded, therefore don't add prefetch here.
diff --git i/examples/FasterRCNN/eval.py w/examples/FasterRCNN/eval.py
index faf81b8c..5651516d 100644
--- i/examples/FasterRCNN/eval.py
+++ w/examples/FasterRCNN/eval.py
@@ -160,11 +160,12 @@ def predict_dataflow(df, model_func, tqdm_bar=None):
         # tqdm is not quite thread-safe: https://github.com/tqdm/tqdm/issues/323
         if tqdm_bar is None:
             tqdm_bar = stack.enter_context(get_tqdm(total=df.size()))
-        for img, img_id in df:
+        for (fname, img), img_id in df:
             results = predict_image(img, model_func)
             for r in results:
                 # int()/float() to make it json-serializable
                 res = {
+                    ""file_name"": fname,
                     'image_id': img_id,
                     'category_id': int(r.class_id),
                     'bbox': [round(float(x), 4) for x in r.box],
```",git index name assert none return return evaluation may therefore add git index quite none make round float,issue,negative,negative,negative,negative,negative,negative
531114958,"My guess is that it's most likely related to 
```
2019-09-13 13:22:42.780580: I tensorflow/stream_executor/platform/default/dso_lo
ader.cc:53] Could not dlopen library 'libcudnn.so.7'; dlerror: libcudnn.so.7: ca
nnot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/l
ocal/nvidia/lib64:/data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cuda/lib6
4:/data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cud
a-10.0/cuda/lib64:/data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cuda/lib6
4:/data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cuda/lib64
```
printed in your log. Installing cudnn might fix it.

Although I'm not sure why tensorpack did not find GPUs either. The way tensorpack looks for GPU does not rely on cudnn.",guess likely related could library ca open object file file directory printed log might fix although sure find either way rely,issue,negative,positive,positive,positive,positive,positive
531114438,"Tensorflow has printed this warning:
```
 W tensorflow/core/common_runtime/gpu/gpu_device.cc:1
663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
```
Tensorpack also printed this:
```
GPU                   Not found with NVML
```

Your GPUs are not configured properly. You may try: reboot, reinstall nvidia driver, contact your system administrator, etc.",printed warning skipping also printed found properly may try reinstall driver contact system administrator,issue,negative,neutral,neutral,neutral,neutral,neutral
531100136,"Or, if you don't want to modify the model, include `""fastrcnn_all_scores""` in the `output_names` at here (or the `output_names` of other predictors)
https://github.com/tensorpack/tensorpack/blob/e8e8b01484c56f4d9ba952e95b997215023f9a71/examples/FasterRCNN/predict.py#L151",want modify model include,issue,negative,neutral,neutral,neutral,neutral,neutral
531081337,"Thanks for the reply but I think you get me wrong.

Your code is already have a built-in operation name: ""fastrcnn_all_scores"" that return all found bounding boxes with score/probability with each classes ( from what I'm aware )
https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/predict.py#L46

I want ""final_scores"" to return all classes score instead of just one score",thanks reply think get wrong code already operation name return found bounding class aware want return class score instead one score,issue,negative,negative,neutral,neutral,negative,negative
530895953,You can just use the input of the function https://github.com/tensorpack/tensorpack/blob/0ffd53379beb6ff7cb798479b33621f2e60c2fb8/examples/FasterRCNN/modeling/model_frcnn.py#L178 instead.,use input function instead,issue,negative,neutral,neutral,neutral,neutral,neutral
528971703,"`normpath` is needed on windows.

Perhaps a solution is to add a custom `normpath` in `utils/fs.py`, which skips schema based paths (anything that contains `://`).",perhaps solution add custom schema based anything,issue,negative,neutral,neutral,neutral,neutral,neutral
528750067,"hmm, I made a stupid mistake that I forgot to freeze config...",made stupid mistake forgot freeze,issue,negative,negative,negative,negative,negative,negative
528452526,"To run on CPU, you need to implement the model in a way that supports CPU. i.e., use NHWC format, or use TensorFlow with MKL enabled.",run need implement model way use format use,issue,negative,neutral,neutral,neutral,neutral,neutral
528104962,"Yes, you were completely right. Commenting out  `tf.test.is_gpu_available` meant that the above code worked. 

Thanks for this and thanks even more for TensorPack!",yes completely right meant code worked thanks thanks even,issue,positive,positive,positive,positive,positive,positive
528097801,"Did you or your code initialize the GPU before creating the predict config? Functions like `tf.test.is_gpu_available` or `get_num_gpu` may also initialize the GPU. TensorFlow cannot accept options after the GPU has been initialized. Maybe that's something to look at.

One rule of thumb is to make sure that
```
tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices:
```
is not printed before creating the predict config.",code initialize predict like may also initialize accept maybe something look one rule thumb make sure visible printed predict,issue,positive,positive,positive,positive,positive,positive
528095203,"Thanks for the speedy reply!

In light of your response, I have modified the following code:

```
 gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.33)
        pred_config = PredictConfig(
            model        = model_constructor(),
            session_creator = NewSessionCreator(config=tf.ConfigProto(gpu_options=gpu_options)),
            session_init = get_model_loader(model_path),
            input_names  = self.eval_inf_input_tensor_names,
            output_names = self.eval_inf_output_tensor_names)
        predictor = OfflinePredictor(pred_config)
```

However, despite it running it doesn't seem to actually put a cap on the memory usage of the GPU.",thanks speedy reply light response following code model predictor however despite running seem actually put cap memory usage,issue,negative,positive,positive,positive,positive,positive
526787382,"The model is implemented by the user, and the user pass the model to tensorpack. So how to pass parameters to model depend on how the user implement the model.",model user user pas model pas model depend user implement model,issue,negative,neutral,neutral,neutral,neutral,neutral
524752423,"> As said in the [documentation](https://tensorpack.readthedocs.io/tutorial/inference.html#step-1-build-the-model-graph), you should not attempt to use metagraph for inference. There is no guarantee on what names appear in the metagraph.
> 
> If you have to, you can inspect the pb file to see what tensors are inside. It could be something like `tower-pred-0/image`.

ok, i got it, thank you!",said documentation attempt use inference guarantee appear inspect file see inside could something like got thank,issue,positive,neutral,neutral,neutral,neutral,neutral
524749017,"As said in the [documentation](https://tensorpack.readthedocs.io/tutorial/inference.html#step-1-build-the-model-graph), you should not attempt to use metagraph for inference. There is no guarantee on what names appear in the metagraph.

If you have to, you can inspect the pb file to see what tensors are inside. It could be something like `tower-pred-0/image`.",said documentation attempt use inference guarantee appear inspect file see inside could something like,issue,positive,neutral,neutral,neutral,neutral,neutral
524738857,"I cannot understand how is your issue related to tensorpack. For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",understand issue related anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected,issue,positive,positive,positive,positive,positive,positive
524000462,"Thanks,

Indeed, that is training accuracy (it's printed with ""val_"" which makes me confuse). Perhaps, I should move to ImageNet",thanks indeed training accuracy printed confuse perhaps move,issue,negative,positive,positive,positive,positive,positive
523998705,"There seems to be no signal indicating a tensorpack issue, so closing. Feel free to reopen if you can isolate and identify an issue of tensorpack.",signal issue feel free reopen isolate identify issue,issue,negative,positive,positive,positive,positive,positive
523997948,"That may be the right thing to do. I just updated my TP version, and it
seems this is something that changed between versions, so my code still has
the AutoResumeTrainConfig usage.
I'll try your suggestion.
Thanks!

בתאריך יום ה׳, 22 באוג׳ 2019, 20:17, מאת Yuxin Wu ‏<notifications@github.com
>:

> It is working as expected since it says that it will look for checkpoint
> in logger.get_logger_dir, and it says when always_resume is True, resume
> takes priority over provided arguments.
>
> My suggestion is to not use AutoResumeTrainConfig if you need to manually
> provide a session_init. In that case why not just pass your session_init
> to TrainConfig?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/1304?email_source=notifications&email_token=AIQTY5WROWT5T7IQIWPBL2DQF3C2XA5CNFSM4IOTWUJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD45Y22A#issuecomment-523996520>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AIQTY5T5HISN2CFQ54EEZPTQF3C2XANCNFSM4IOTWUJA>
> .
>
",may right thing version something code still usage try suggestion thanks working since look true resume priority provided suggestion use need manually provide case pas thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
523997028,"That's why Cifar10 should not be used to test model compression algorithm..

Also, you should expect training accuracy to be higher for 8,8,32, not validation accuracy.",used test model compression algorithm also expect training accuracy higher validation accuracy,issue,negative,positive,positive,positive,positive,positive
523996520,"It is working as expected since it says that it will look for checkpoint in `logger.get_logger_dir`, and it says when `always_resume` is True, resume takes priority over provided arguments.

My suggestion is to not use `AutoResumeTrainConfig` if you need to manually provide a session_init. In that case why not just pass your `session_init` to `TrainConfig`?",working since look true resume priority provided suggestion use need manually provide case pas,issue,negative,positive,positive,positive,positive,positive
523994512,"You don't have to do anything if it can train.

If you want to reduce memory usage, you can use smaller backbone, smaller `FRCNN.BATCH_PER_IM`, smaller image size, etc.",anything train want reduce memory usage use smaller backbone smaller smaller image size,issue,negative,neutral,neutral,neutral,neutral,neutral
523699735,@AlbertoMCS Many thanks! I really appreciate it.,many thanks really appreciate,issue,positive,positive,positive,positive,positive,positive
523472769,"thanks for the friendly example.
Is the repo also suppport the training of Cascade R-CNN models only for object detection?
I mean ignore the segmentation.
I am trying to follow the Balloon example to run my custom training, in the preparation I found the annotation include segmentation mask.",thanks friendly example also training cascade object detection mean ignore segmentation trying follow balloon example run custom training preparation found annotation include segmentation mask,issue,positive,positive,neutral,neutral,positive,positive
523404537,"Hi @monjurulkarim you have to request the data from Mapillary Dataset website: 

https://www.mapillary.com/dataset/vistas?pKey=cc5dEAyQECBFF9MN3MbdZA&lat=-1.696362232706349&lng=-35.07353824409381&z=2.59180623161258

After the request is sent they will get back to you by email providing a link to the data. ",hi request data request sent get back providing link data,issue,negative,neutral,neutral,neutral,neutral,neutral
523276924,"This isn't related to tensorpack, so closing. I've run the object detection example with SyncBN and TF 1.13 without any issues.",related run object detection example without,issue,negative,neutral,neutral,neutral,neutral,neutral
523211792,"@AlbertoMCS Many thanks! One more question, how did you get Mapillary Vistas dataset? Can you share a downloadable link to me? ",many thanks one question get share link,issue,positive,positive,positive,positive,positive,positive
523158988,"Hi @monjurulkarim,  I used this repo to convert Mapillary dataset into COCO format: 

https://github.com/Luodian/Mapillary2COCO
",hi used convert coco format,issue,negative,neutral,neutral,neutral,neutral,neutral
523100550,"@AlbertoMCS Hi, how did you transform Mapillary Vistas dataset into COCO format? I am also trying to train the Mask RCNN inplementation by matterport  with Mapillary Vistas dataset. 
If you could share how you did transform the data would be very helpful for me.
Thanks.",hi transform coco format also trying train mask could share transform data would helpful thanks,issue,positive,positive,positive,positive,positive,positive
522887832,"https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md:

> Inference is unoptimized. Tensorpack is a training interface: it produces the trained weights in standard format but it does not help you on optimized inference. In fact, the current implementation uses some slow numpy operations in inference (in eval.py:_paste_mask).",inference training interface trained standard format help inference fact current implementation slow inference,issue,negative,negative,negative,negative,negative,negative
522309075,"Please open an issue with details.

The tfdbg hook does work with a simple multi-gpu example, but there may be other edge cases.",please open issue hook work simple example may edge,issue,negative,neutral,neutral,neutral,neutral,neutral
522307654,"That was extremely fast. Thanks!
On a different topic, (and let me know if this should be opened via a new issue) - It seems there's some incompatibility between TP multi-gpu training and tfdbg. I don't know exactly what the issue is, but it's probably something to do with the tower-context name-scopes, because the tfdbg process fails with an error regarding multiple with the same name...(specifically the multiplicity of the repeated node is the same number of gpus in usage).
This is important because when debugging the rise of NaNs, reducing the number of gpus effectively reduces the batch size which in turn can cause numerical instability NaNs which have nothing to do with the original problem being investigated.
Hope that's a clear enough description of the problem, let me know if more details are needed or if this should be raised in a different format or context.",extremely fast thanks different topic let know via new issue incompatibility training know exactly issue probably something process error regarding multiple name specifically multiplicity repeated node number usage important rise reducing number effectively batch size turn cause numerical instability nothing original problem hope clear enough description problem let know raised different format context,issue,positive,positive,positive,positive,positive,positive
522304501,"I might be wrong, but I think there might be a problem (pseudo-bug?) with the option of passing `LocalCLIDebugHook` optional arguments. 
This is probably easily fixed by passing the `**kwargs` passed into `TFLocalCLIDebugHook` into the super-initializer, i.e: `super(TFLocalCLIDebugHook, self).__init__(tfdbg.LocalCLIDebugHook(**kwargs))`.
Example use-case: changing the `dump_root`.
I might be missing some complications and intricate nuances, but for now I've added the change above in my code and it seems to be working fine.

Great work BTW!
",might wrong think might problem option passing optional probably easily fixed passing super self example might missing intricate added change code working fine great work,issue,positive,positive,positive,positive,positive,positive
522220212,"The output from a dataflow is, by definition, either a list or a dict: https://tensorpack.readthedocs.io/tutorial/dataflow.html#what-is-dataflow.

Tuple is not supported, although it can work in certain dataflow (e.g., `MapData`).",output definition either list although work certain,issue,negative,positive,positive,positive,positive,positive
522217852,"You are right! And I change my code.

But a new error occurs, here is main information:

```
2019-08-17 16:23:19.056975: W tensorflow/core/framework/op_kernel.cc:1389] Invalid argument: TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float32, tf.int64, tf.int64, tf.float32, tf.float32, tf.string, tf.string), but the yielded element was [array([[[130.,  50., 223.],
        [ 70.,  38., 111.],
        [177., 178., 170.],
        ...,
        [ 64.,  51., 156.],
        [ 42., 108.,  81.],
        [ 89., 225.,  89.]],

       [[145.,   0.,  97.],
        [237., 189., 239.],
        [ 48.,  73.,  80.],
        ...,
        [107., 150., 169.],
        [125., 182., 150.],
        [170., 215., 134.]],

       [[243.,  65.,  91.],
        [117.,  63.,  95.],
        [143., 187., 216.],
        ...,
        [ 92., 188., 125.],
        [ 70., 112.,  88.],
        [105.,  52.,  60.]],

       ...,

       [[116., 137.,  60.],
        [208., 203., 137.],
        [179., 160., 118.],
        ...,
        [ 80.,  51.,  73.],
        [137., 134., 117.],
        [200., 237., 196.]],

       [[103., 109., 109.],
        [115.,  97.,  85.],
        [140., 127.,  85.],
        ...,
        [ 70.,  76., 110.],
        [ 66.,  58.,  97.],
        [187., 158., 206.]],

       [[112.,  56., 167.],
        [172., 126., 173.],
        [220., 216., 155.],
        ...,
        [ 47., 110., 145.],
        [178., 183., 255.],
        [ 87.,   0., 126.]]], dtype=float32), array([[[188.,  55., 175., ...,  25.,  71.,  99.],
        [ 66., 241.,   4., ..., 164.,  39., 246.],
        [241., 117., 132., ...,  27.,   8., 115.],
        ...,
        [ 53., 215., 197., ...,  60.,  61., 204.],
        [ 58., 243.,  99., ..., 224., 166., 250.],
        [194.,  62., 238., ...,  58., 173.,  21.]],

       [[188.,  55., 175., ...,  25.,  71.,  99.],
        [ 66., 241.,   4., ..., 164.,  39., 246.],
        [241., 117., 132., ...,  27.,   8., 115.],
        ...,
        [ 53., 215., 197., ...,  60.,  61., 204.],
        [ 58., 243.,  99., ..., 224., 166., 250.],
        [194.,  62., 238., ...,  58., 173.,  21.]]], dtype=float32), 2, array([0, 0]), 0.0, array([0., 0.]), '0.0', ['0.0', '0.0']].
Traceback (most recent call last):

  File ""/media/envs/anaconda3/envs/lk-tf36/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 455, in generator_py_func
    flattened_values = nest.flatten_up_to(output_types, values)

  File ""/media/envs/anaconda3/envs/lk-tf36/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py"", line 399, in flatten_up_to
    assert_shallow_structure(shallow_tree, input_tree)

  File ""/media/envs/anaconda3/envs/lk-tf36/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py"", line 301, in assert_shallow_structure
    ""Input has type: %s."" % type(input_tree))

TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <class 'list'>.


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/media/envs/anaconda3/envs/lk-tf36/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 207, in __call__
    ret = func(*args)

  File ""/media/envs/anaconda3/envs/lk-tf36/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 460, in generator_py_func
    ""element was %s."" % (output_types, values))
```

I add a [`MapData` operation](https://github.com/grandesty-ml/dataflow4estimator/blob/8e1ba0846942ef73dd23d142181bb58a3873fc3e/gen_input.py#L41):

```python
lmdb_dataset = tp.MapData(lmdb_dataset, tuple)
```

before running `reset_state`, it works.

The returned element is tuple in `parse_fn`, and in `from_generator`, it is list. Maybe the type of element has been changed by some `DataFlow` class.",right change code new error main information invalid argument generator element match structure structure element array array array array recent call last file line file line file line input type type shallow structure sequence input must also sequence input type class handling exception another exception recent call last file line ret file line element add operation python running work returned element list maybe type element class,issue,negative,positive,neutral,neutral,positive,positive
522211559,"Btw, thanks for the clear issue description: makes it very easy to investigate.",thanks clear issue description easy investigate,issue,positive,positive,positive,positive,positive,positive
522211494,"https://github.com/grandesty-ml/dataflow4estimator/blob/master/gen_input.py#L43

 You cannot call `tuple()` on an iterator.

```
def f():
    while True:
        yield 1

x = tuple(f())
```

This code will hang and take all your memory as well.",call true yield code take memory well,issue,positive,positive,positive,positive,positive,positive
521300632,"It's not due to tf 2.0
Please open an issue following the issue template if you still have questions.",due please open issue following issue template still,issue,negative,negative,neutral,neutral,negative,negative
521297753,"Hi, I am dealing with the problem that ""no module called tensorpack"" in colab, but I really pip install it so I think there may be dut to tensorflow 2.0 ? Is there any solutions?",hi dealing problem module really pip install think may,issue,negative,positive,positive,positive,positive,positive
520141179,"I'll double check the dataset and thanks Yuyin again for the insights provided, due to the data privacy I close this issue.",double check thanks provided due data privacy close issue,issue,negative,positive,neutral,neutral,positive,positive
520130111,"If you really made your settings the same as the balloon example, then literally it means that the only difference is the dataset, and the issue is unlikely to be something I can help you with since it's your own dataset. For example, another user mentioned there are wrong annotations, which is apparently something I cannot foresee.

If you can prepare something in a clean shape that one can easily reproduce the issue (with diff, data, command, etc) then I can probably tell you what's going wrong. Or at least provide the dataset since you're indicating that the dataset is the only difference that causes the issue. 
Without more information I feel this discussion will lead to nowhere.",really made balloon example literally difference issue unlikely something help since example another user wrong apparently something foresee prepare something clean shape one easily reproduce issue data command probably tell going wrong least provide since difference issue without information feel discussion lead nowhere,issue,negative,negative,neutral,neutral,negative,negative
520129369,"The following is the logs for using default config:
(tensorflow) [bigdata@bigdata-cv-2019-2 FasterRCNN]$ ./train.sh 
[0810 16:00:48 @logger.py:90] Argv: ./train.py --config DATA.BASEDIR=/home/bigdata/data MODE_MASK=True MODE_FPN=False --load /home/bigdata/coco/COCO-MaskRCNN-R101C41x.npz
[0810 16:00:48 @train.py:56] Environment Information:
--------------------  -------------------------------------------------------------------
sys.platform          linux
Python                3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0]
Tensorpack            0.9.7.1-1-g13ee370-dirty
Numpy                 1.16.2
TensorFlow            1.14.0/v1.14.0-rc1-22-gaf24dc91b5
TF Compiler Version   4.8.5
TF CUDA support       True
TF MKL support        False
TF XLA support        False
Nvidia Driver         /usr/lib64/libnvidia-ml.so.430.40
CUDA                  /usr/local/cuda-10.1/lib64/libcudart.so.10.0.130
CUDNN                 /usr/local/cuda-10.1/lib64/libcudnn.so
NCCL                  /usr/lib64/libnccl.so.2.4.8
CUDA_VISIBLE_DEVICES  None
GPU 0,1,2,3           TITAN RTX
Free RAM              211.22/220.12 GB
CPU Count             72
cv2                   4.1.0
msgpack               0.6.1
python-prctl          True
--------------------  -------------------------------------------------------------------
[0810 16:00:48 @config.py:305] Config: ------------------------------------------
{'BACKBONE': {'FREEZE_AFFINE': False,
              'FREEZE_AT': 2,
              'NORM': 'FreezeBN',
              'RESNET_NUM_BLOCKS': [3, 4, 23, 3],
              'STRIDE_1X1': False,
              'TF_PAD_MODE': False,
              'WEIGHTS': ''},
 'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0],
                                  [30.0, 30.0, 15.0, 15.0]],
             'IOUS': [0.5, 0.6, 0.7]},
 'DATA': {'ABSOLUTE_COORD': True,
          'BASEDIR': '/home/bigdata/data',
          'CLASS_NAMES': ['BG', 'connector'],
          'NUM_CATEGORY': 1,
          'NUM_WORKERS': 10,
          'TRAIN': ('coco_train',),
          'VAL': ('coco_val',)},
 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),
         'CASCADE': False,
         'FRCNN_CONV_HEAD_DIM': 256,
         'FRCNN_FC_HEAD_DIM': 1024,
         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',
         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',
         'NORM': 'None',
         'NUM_CHANNEL': 256,
         'PROPOSAL_MODE': 'Level',
         'RESOLUTION_REQUIREMENT': 32},
 'FRCNN': {'BATCH_PER_IM': 512,
           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],
           'FG_RATIO': 0.25,
           'FG_THRESH': 0.5},
 'MODE_FPN': False,
 'MODE_MASK': True,
 'MRCNN': {'HEAD_DIM': 256},
 'PREPROC': {'MAX_SIZE': 1333,
             'PIXEL_MEAN': [123.675, 116.28, 103.53],
             'PIXEL_STD': [58.395, 57.12, 57.375],
             'TEST_SHORT_EDGE_SIZE': 800,
             'TRAIN_SHORT_EDGE_SIZE': [800, 800]},
 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),
         'ANCHOR_SIZES': (32, 64, 128, 256, 512),
         'ANCHOR_STRIDE': 16,
         'BATCH_PER_IM': 256,
         'CROWD_OVERLAP_THRESH': 9.99,
         'FG_RATIO': 0.5,
         'HEAD_DIM': 1024,
         'MIN_SIZE': 0,
         'NEGATIVE_ANCHOR_THRESH': 0.3,
         'NUM_ANCHOR': 15,
         'POSITIVE_ANCHOR_THRESH': 0.7,
         'PROPOSAL_NMS_THRESH': 0.7,
         'TEST_PER_LEVEL_NMS_TOPK': 1000,
         'TEST_POST_NMS_TOPK': 1000,
         'TEST_PRE_NMS_TOPK': 6000,
         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,
         'TRAIN_POST_NMS_TOPK': 2000,
         'TRAIN_PRE_NMS_TOPK': 12000},
 'TEST': {'FRCNN_NMS_THRESH': 0.5,
          'RESULTS_PER_IM': 100,
          'RESULT_SCORE_THRESH': 0.05,
          'RESULT_SCORE_THRESH_VIS': 0.5},
 'TRAIN': {'BASE_LR': 0.01,
           'CHECKPOINT_PERIOD': 20,
           'EVAL_PERIOD': 25,
           'LR_SCHEDULE': [120000, 160000, 180000],
           'NUM_GPUS': 4,
           'STARTING_EPOCH': 1,
           'STEPS_PER_EPOCH': 50,
           'WARMUP': 1000,
           'WARMUP_INIT_LR': 0.0033000000000000004,
           'WEIGHT_DECAY': 0.0001},
 'TRAINER': 'replicated'}
[0810 16:00:48 @train.py:73] Warm Up Schedule (steps, value): [(0, 0.0033000000000000004), (1000, 0.01)]
[0810 16:00:48 @train.py:74] LR Schedule (epochs, value): [(20, 0.01), (4800.0, 0.001), (6400.0, 0.00010000000000000002)]
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
[0810 16:00:49 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_train.json.
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 247/247 [00:00<00:00, 14660.00it/s]
[0810 16:00:49 @timer.py:50] Load annotations for instances_train.json finished, time:0.0259 sec.
[0810 16:00:49 @data.py:60] Ground-Truth category distribution:
|   class   | #box   |  class  |  #box  |  class  |  #box  |
|:---------:|:-------|:-------:|:------:|:-------:|:------:|
| connector | 445    |         |        |         |        |
|   total   | 445    |         |        |         |        |
[0810 16:00:49 @data.py:347] Filtered 0 images which contain no non-crowd groudtruth boxes. Total #images for training: 247
[0810 16:00:49 @train.py:78] Total passes of the training set is: 5830
[0810 16:00:49 @prof.py:50] WRN [GPUUtilizationTracker] Both devices and CUDA_VISIBLE_DEVICES are None! Will monitor all 4 visible GPUs!
[0810 16:00:51 @training.py:50] [DataParallel] Training a model of 4 towers.
[0810 16:00:51 @interface.py:43] Automatically applying StagingInput on the DataFlow.
[0810 16:00:51 @input_source.py:223] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0810 16:00:51 @training.py:110] Building graph for training tower 0 on device /gpu:0 ...
[0810 16:00:51 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:51 @registry.py:90] 'conv0': [1, 3, ?, ?] --> [1, 64, ?, ?]
[0810 16:00:51 @registry.py:90] 'pool0': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0810 16:00:51 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:51 @registry.py:90] 'group0/block0/conv1': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0810 16:00:51 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:51 @registry.py:90] 'group0/block0/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0810 16:00:51 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:51 @registry.py:90] 'group0/block0/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:51 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:51 @registry.py:90] 'group0/block0/convshortcut': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:51 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:51 @registry.py:90] 'group0/block1/conv1': [1, 256, ?, ?] --> [1, 64, ?, ?]
[0810 16:00:51 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:51 @registry.py:90] 'group0/block1/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0810 16:00:51 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:51 @registry.py:90] 'group0/block1/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:51 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:51 @registry.py:90] 'group0/block2/conv1': [1, 256, ?, ?] --> [1, 64, ?, ?]
[0810 16:00:51 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group0/block2/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group0/block2/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block0/conv1': [1, 256, ?, ?] --> [1, 128, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block0/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block0/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block0/convshortcut': [1, 256, ?, ?] --> [1, 512, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block1/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block1/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block1/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block2/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block2/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block2/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block3/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block3/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group1/block3/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group2/block0/conv1': [1, 512, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group2/block0/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group2/block0/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group2/block0/convshortcut': [1, 512, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group2/block1/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:52 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:52 @registry.py:90] 'group2/block1/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block1/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block2/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block2/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block2/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block3/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block3/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block3/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block4/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block4/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block4/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block5/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block5/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block5/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block6/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block6/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block6/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:53 @registry.py:90] 'group2/block7/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:53 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block7/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block7/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block8/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block8/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block8/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block9/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block9/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block9/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block10/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block10/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block10/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block11/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block11/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block11/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block12/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block12/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block12/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block13/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block13/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block13/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block14/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:54 @registry.py:90] 'group2/block14/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:54 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block14/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block15/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block15/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block15/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block16/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block16/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block16/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block17/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block17/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block17/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block18/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block18/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block18/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block19/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block19/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block19/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block20/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block20/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block20/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block21/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block21/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block21/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:55 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:55 @registry.py:90] 'group2/block22/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:56 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:56 @registry.py:90] 'group2/block22/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0810 16:00:56 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:56 @registry.py:90] 'group2/block22/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:56 @registry.py:80] 'rpn' input: [1, 1024, ?, ?]
[0810 16:00:56 @registry.py:90]   'rpn/conv0': [1, 1024, ?, ?] --> [1, 1024, ?, ?]
[0810 16:00:56 @registry.py:90]   'rpn/class': [1, 1024, ?, ?] --> [1, 15, ?, ?]
[0810 16:00:56 @registry.py:90]   'rpn/box': [1, 1024, ?, ?] --> [1, 60, ?, ?]
[0810 16:00:56 @registry.py:93] 'rpn' output: [?, ?, 15], [?, ?, 15, 4]
[0810 16:00:56 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:56 @registry.py:90] 'group3/block0/conv1': [?, 1024, 14, 14] --> [?, 512, 14, 14]
[0810 16:00:56 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:56 @registry.py:90] 'group3/block0/conv2': [?, 512, 15, 15] --> [?, 512, 7, 7]
[0810 16:00:56 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:57 @registry.py:90] 'group3/block0/conv3': [?, 512, 7, 7] --> [?, 2048, 7, 7]
[0810 16:00:57 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:57 @registry.py:90] 'group3/block0/convshortcut': [?, 1024, 13, 13] --> [?, 2048, 7, 7]
[0810 16:00:57 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:57 @registry.py:90] 'group3/block1/conv1': [?, 2048, 7, 7] --> [?, 512, 7, 7]
[0810 16:00:57 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:57 @registry.py:90] 'group3/block1/conv2': [?, 512, 7, 7] --> [?, 512, 7, 7]
[0810 16:00:57 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:57 @registry.py:90] 'group3/block1/conv3': [?, 512, 7, 7] --> [?, 2048, 7, 7]
[0810 16:00:57 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:57 @registry.py:90] 'group3/block2/conv1': [?, 2048, 7, 7] --> [?, 512, 7, 7]
[0810 16:00:57 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:57 @registry.py:90] 'group3/block2/conv2': [?, 512, 7, 7] --> [?, 512, 7, 7]
[0810 16:00:57 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0810 16:00:57 @registry.py:90] 'group3/block2/conv3': [?, 512, 7, 7] --> [?, 2048, 7, 7]
[0810 16:00:57 @registry.py:90] 'gap': [?, 2048, 7, 7] --> [?, 2048]
[0810 16:00:57 @registry.py:80] 'fastrcnn' input: [?, 2048]
[0810 16:00:57 @registry.py:90]   'fastrcnn/class': [?, 2048] --> [?, 2]
[0810 16:00:57 @registry.py:90]   'fastrcnn/box': [?, 2048] --> [?, 8]
[0810 16:00:57 @registry.py:93] 'fastrcnn' output: [?, 2], [?, 2, 4]
[0810 16:00:57 @registry.py:80] 'maskrcnn' input: [?, 2048, 7, 7]
[0810 16:00:57 @registry.py:90]   'maskrcnn/deconv': [?, 2048, 7, 7] --> [?, 256, 14, 14]
[0810 16:00:57 @registry.py:90]   'maskrcnn/conv': [?, 256, 14, 14] --> [?, 1, 14, 14]
[0810 16:00:57 @registry.py:93] 'maskrcnn' output: [?, 1, 14, 14]
[0810 16:00:58 @regularize.py:97] regularize_cost() found 100 variables to regularize.
[0810 16:00:58 @regularize.py:21] The following tensors will be regularized: group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/conv2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group2/block5/conv3/W:0, group2/block6/conv1/W:0, group2/block6/conv2/W:0, group2/block6/conv3/W:0, group2/block7/conv1/W:0, group2/block7/conv2/W:0, group2/block7/conv3/W:0, group2/block8/conv1/W:0, group2/block8/conv2/W:0, group2/block8/conv3/W:0, group2/block9/conv1/W:0, group2/block9/conv2/W:0, group2/block9/conv3/W:0, group2/block10/conv1/W:0, group2/block10/conv2/W:0, group2/block10/conv3/W:0, group2/block11/conv1/W:0, group2/block11/conv2/W:0, group2/block11/conv3/W:0, group2/block12/conv1/W:0, group2/block12/conv2/W:0, group2/block12/conv3/W:0, group2/block13/conv1/W:0, group2/block13/conv2/W:0, group2/block13/conv3/W:0, group2/block14/conv1/W:0, group2/block14/conv2/W:0, group2/block14/conv3/W:0, group2/block15/conv1/W:0, group2/block15/conv2/W:0, group2/block15/conv3/W:0, group2/block16/conv1/W:0, group2/block16/conv2/W:0, group2/block16/conv3/W:0, group2/block17/conv1/W:0, group2/block17/conv2/W:0, group2/block17/conv3/W:0, group2/block18/conv1/W:0, group2/block18/conv2/W:0, group2/block18/conv3/W:0, group2/block19/conv1/W:0, group2/block19/conv2/W:0, group2/block19/conv3/W:0, group2/block20/conv1/W:0, group2/block20/conv2/W:0, group2/block20/conv3/W:0, group2/block21/conv1/W:0, group2/block21/conv2/W:0, group2/block21/conv3/W:0, group2/block22/conv1/W:0, group2/block22/conv2/W:0, group2/block22/conv3/W:0, rpn/conv0/W:0, rpn/class/W:0, rpn/box/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, fastrcnn/class/W:0, fastrcnn/box/W:0, maskrcnn/deconv/W:0, maskrcnn/conv/W:0
/home/bigdata/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0810 16:01:01 @training.py:110] Building graph for training tower 1 on device /gpu:1 ...
[0810 16:01:07 @regularize.py:97] regularize_cost() found 100 variables to regularize.
/home/bigdata/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0810 16:01:11 @collection.py:152] Size of these collections were changed in tower1: (tf.GraphKeys.MODEL_VARIABLES: 285->340)
[0810 16:01:11 @collection.py:165] These collections were modified but restored in tower1: (tf.GraphKeys.SUMMARIES: 31->32)
[0810 16:01:11 @training.py:110] Building graph for training tower 2 on device /gpu:2 ...
[0810 16:01:16 @regularize.py:97] regularize_cost() found 100 variables to regularize.
/home/bigdata/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0810 16:01:20 @collection.py:152] Size of these collections were changed in tower2: (tf.GraphKeys.MODEL_VARIABLES: 340->395)
[0810 16:01:20 @collection.py:165] These collections were modified but restored in tower2: (tf.GraphKeys.SUMMARIES: 31->32)
[0810 16:01:20 @training.py:110] Building graph for training tower 3 on device /gpu:3 ...
[0810 16:01:25 @regularize.py:97] regularize_cost() found 100 variables to regularize.
/home/bigdata/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0810 16:01:29 @collection.py:152] Size of these collections were changed in tower3: (tf.GraphKeys.MODEL_VARIABLES: 395->450)
[0810 16:01:29 @collection.py:165] These collections were modified but restored in tower3: (tf.GraphKeys.SUMMARIES: 31->32)
[0810 16:01:51 @training.py:351] 'sync_variables_from_main_tower' includes 3360 operations.
[0810 16:01:51 @model_utils.py:67] List of Trainable Variables: 
name                                 shape                 #elements
-----------------------------------  ------------------  -----------
group1/block0/conv1/W                [1, 1, 256, 128]          32768
group1/block0/conv1/bn/gamma         [128]                       128
group1/block0/conv1/bn/beta          [128]                       128
group1/block0/conv2/W                [3, 3, 128, 128]         147456
group1/block0/conv2/bn/gamma         [128]                       128
group1/block0/conv2/bn/beta          [128]                       128
group1/block0/conv3/W                [1, 1, 128, 512]          65536
group1/block0/conv3/bn/gamma         [512]                       512
group1/block0/conv3/bn/beta          [512]                       512
group1/block0/convshortcut/W         [1, 1, 256, 512]         131072
group1/block0/convshortcut/bn/gamma  [512]                       512
group1/block0/convshortcut/bn/beta   [512]                       512
group1/block1/conv1/W                [1, 1, 512, 128]          65536
group1/block1/conv1/bn/gamma         [128]                       128
group1/block1/conv1/bn/beta          [128]                       128
group1/block1/conv2/W                [3, 3, 128, 128]         147456
group1/block1/conv2/bn/gamma         [128]                       128
group1/block1/conv2/bn/beta          [128]                       128
group1/block1/conv3/W                [1, 1, 128, 512]          65536
group1/block1/conv3/bn/gamma         [512]                       512
group1/block1/conv3/bn/beta          [512]                       512
group1/block2/conv1/W                [1, 1, 512, 128]          65536
group1/block2/conv1/bn/gamma         [128]                       128
group1/block2/conv1/bn/beta          [128]                       128
group1/block2/conv2/W                [3, 3, 128, 128]         147456
group1/block2/conv2/bn/gamma         [128]                       128
group1/block2/conv2/bn/beta          [128]                       128
group1/block2/conv3/W                [1, 1, 128, 512]          65536
group1/block2/conv3/bn/gamma         [512]                       512
group1/block2/conv3/bn/beta          [512]                       512
group1/block3/conv1/W                [1, 1, 512, 128]          65536
group1/block3/conv1/bn/gamma         [128]                       128
group1/block3/conv1/bn/beta          [128]                       128
group1/block3/conv2/W                [3, 3, 128, 128]         147456
group1/block3/conv2/bn/gamma         [128]                       128
group1/block3/conv2/bn/beta          [128]                       128
group1/block3/conv3/W                [1, 1, 128, 512]          65536
group1/block3/conv3/bn/gamma         [512]                       512
group1/block3/conv3/bn/beta          [512]                       512
group2/block0/conv1/W                [1, 1, 512, 256]         131072
group2/block0/conv1/bn/gamma         [256]                       256
group2/block0/conv1/bn/beta          [256]                       256
group2/block0/conv2/W                [3, 3, 256, 256]         589824
group2/block0/conv2/bn/gamma         [256]                       256
group2/block0/conv2/bn/beta          [256]                       256
group2/block0/conv3/W                [1, 1, 256, 1024]        262144
group2/block0/conv3/bn/gamma         [1024]                     1024
group2/block0/conv3/bn/beta          [1024]                     1024
group2/block0/convshortcut/W         [1, 1, 512, 1024]        524288
group2/block0/convshortcut/bn/gamma  [1024]                     1024
group2/block0/convshortcut/bn/beta   [1024]                     1024
group2/block1/conv1/W                [1, 1, 1024, 256]        262144
group2/block1/conv1/bn/gamma         [256]                       256
group2/block1/conv1/bn/beta          [256]                       256
group2/block1/conv2/W                [3, 3, 256, 256]         589824
group2/block1/conv2/bn/gamma         [256]                       256
group2/block1/conv2/bn/beta          [256]                       256
group2/block1/conv3/W                [1, 1, 256, 1024]        262144
group2/block1/conv3/bn/gamma         [1024]                     1024
group2/block1/conv3/bn/beta          [1024]                     1024
group2/block2/conv1/W                [1, 1, 1024, 256]        262144
group2/block2/conv1/bn/gamma         [256]                       256
group2/block2/conv1/bn/beta          [256]                       256
group2/block2/conv2/W                [3, 3, 256, 256]         589824
group2/block2/conv2/bn/gamma         [256]                       256
group2/block2/conv2/bn/beta          [256]                       256
group2/block2/conv3/W                [1, 1, 256, 1024]        262144
group2/block2/conv3/bn/gamma         [1024]                     1024
group2/block2/conv3/bn/beta          [1024]                     1024
group2/block3/conv1/W                [1, 1, 1024, 256]        262144
group2/block3/conv1/bn/gamma         [256]                       256
group2/block3/conv1/bn/beta          [256]                       256
group2/block3/conv2/W                [3, 3, 256, 256]         589824
group2/block3/conv2/bn/gamma         [256]                       256
group2/block3/conv2/bn/beta          [256]                       256
group2/block3/conv3/W                [1, 1, 256, 1024]        262144
group2/block3/conv3/bn/gamma         [1024]                     1024
group2/block3/conv3/bn/beta          [1024]                     1024
group2/block4/conv1/W                [1, 1, 1024, 256]        262144
group2/block4/conv1/bn/gamma         [256]                       256
group2/block4/conv1/bn/beta          [256]                       256
group2/block4/conv2/W                [3, 3, 256, 256]         589824
group2/block4/conv2/bn/gamma         [256]                       256
group2/block4/conv2/bn/beta          [256]                       256
group2/block4/conv3/W                [1, 1, 256, 1024]        262144
group2/block4/conv3/bn/gamma         [1024]                     1024
group2/block4/conv3/bn/beta          [1024]                     1024
group2/block5/conv1/W                [1, 1, 1024, 256]        262144
group2/block5/conv1/bn/gamma         [256]                       256
group2/block5/conv1/bn/beta          [256]                       256
group2/block5/conv2/W                [3, 3, 256, 256]         589824
group2/block5/conv2/bn/gamma         [256]                       256
group2/block5/conv2/bn/beta          [256]                       256
group2/block5/conv3/W                [1, 1, 256, 1024]        262144
group2/block5/conv3/bn/gamma         [1024]                     1024
group2/block5/conv3/bn/beta          [1024]                     1024
group2/block6/conv1/W                [1, 1, 1024, 256]        262144
group2/block6/conv1/bn/gamma         [256]                       256
group2/block6/conv1/bn/beta          [256]                       256
group2/block6/conv2/W                [3, 3, 256, 256]         589824
group2/block6/conv2/bn/gamma         [256]                       256
group2/block6/conv2/bn/beta          [256]                       256
group2/block6/conv3/W                [1, 1, 256, 1024]        262144
group2/block6/conv3/bn/gamma         [1024]                     1024
group2/block6/conv3/bn/beta          [1024]                     1024
group2/block7/conv1/W                [1, 1, 1024, 256]        262144
group2/block7/conv1/bn/gamma         [256]                       256
group2/block7/conv1/bn/beta          [256]                       256
group2/block7/conv2/W                [3, 3, 256, 256]         589824
group2/block7/conv2/bn/gamma         [256]                       256
group2/block7/conv2/bn/beta          [256]                       256
group2/block7/conv3/W                [1, 1, 256, 1024]        262144
group2/block7/conv3/bn/gamma         [1024]                     1024
group2/block7/conv3/bn/beta          [1024]                     1024
group2/block8/conv1/W                [1, 1, 1024, 256]        262144
group2/block8/conv1/bn/gamma         [256]                       256
group2/block8/conv1/bn/beta          [256]                       256
group2/block8/conv2/W                [3, 3, 256, 256]         589824
group2/block8/conv2/bn/gamma         [256]                       256
group2/block8/conv2/bn/beta          [256]                       256
group2/block8/conv3/W                [1, 1, 256, 1024]        262144
group2/block8/conv3/bn/gamma         [1024]                     1024
group2/block8/conv3/bn/beta          [1024]                     1024
group2/block9/conv1/W                [1, 1, 1024, 256]        262144
group2/block9/conv1/bn/gamma         [256]                       256
group2/block9/conv1/bn/beta          [256]                       256
group2/block9/conv2/W                [3, 3, 256, 256]         589824
group2/block9/conv2/bn/gamma         [256]                       256
group2/block9/conv2/bn/beta          [256]                       256
group2/block9/conv3/W                [1, 1, 256, 1024]        262144
group2/block9/conv3/bn/gamma         [1024]                     1024
group2/block9/conv3/bn/beta          [1024]                     1024
group2/block10/conv1/W               [1, 1, 1024, 256]        262144
group2/block10/conv1/bn/gamma        [256]                       256
group2/block10/conv1/bn/beta         [256]                       256
group2/block10/conv2/W               [3, 3, 256, 256]         589824
group2/block10/conv2/bn/gamma        [256]                       256
group2/block10/conv2/bn/beta         [256]                       256
group2/block10/conv3/W               [1, 1, 256, 1024]        262144
group2/block10/conv3/bn/gamma        [1024]                     1024
group2/block10/conv3/bn/beta         [1024]                     1024
group2/block11/conv1/W               [1, 1, 1024, 256]        262144
group2/block11/conv1/bn/gamma        [256]                       256
group2/block11/conv1/bn/beta         [256]                       256
group2/block11/conv2/W               [3, 3, 256, 256]         589824
group2/block11/conv2/bn/gamma        [256]                       256
group2/block11/conv2/bn/beta         [256]                       256
group2/block11/conv3/W               [1, 1, 256, 1024]        262144
group2/block11/conv3/bn/gamma        [1024]                     1024
group2/block11/conv3/bn/beta         [1024]                     1024
group2/block12/conv1/W               [1, 1, 1024, 256]        262144
group2/block12/conv1/bn/gamma        [256]                       256
group2/block12/conv1/bn/beta         [256]                       256
group2/block12/conv2/W               [3, 3, 256, 256]         589824
group2/block12/conv2/bn/gamma        [256]                       256
group2/block12/conv2/bn/beta         [256]                       256
group2/block12/conv3/W               [1, 1, 256, 1024]        262144
group2/block12/conv3/bn/gamma        [1024]                     1024
group2/block12/conv3/bn/beta         [1024]                     1024
group2/block13/conv1/W               [1, 1, 1024, 256]        262144
group2/block13/conv1/bn/gamma        [256]                       256
group2/block13/conv1/bn/beta         [256]                       256
group2/block13/conv2/W               [3, 3, 256, 256]         589824
group2/block13/conv2/bn/gamma        [256]                       256
group2/block13/conv2/bn/beta         [256]                       256
group2/block13/conv3/W               [1, 1, 256, 1024]        262144
group2/block13/conv3/bn/gamma        [1024]                     1024
group2/block13/conv3/bn/beta         [1024]                     1024
group2/block14/conv1/W               [1, 1, 1024, 256]        262144
group2/block14/conv1/bn/gamma        [256]                       256
group2/block14/conv1/bn/beta         [256]                       256
group2/block14/conv2/W               [3, 3, 256, 256]         589824
group2/block14/conv2/bn/gamma        [256]                       256
group2/block14/conv2/bn/beta         [256]                       256
group2/block14/conv3/W               [1, 1, 256, 1024]        262144
group2/block14/conv3/bn/gamma        [1024]                     1024
group2/block14/conv3/bn/beta         [1024]                     1024
group2/block15/conv1/W               [1, 1, 1024, 256]        262144
group2/block15/conv1/bn/gamma        [256]                       256
group2/block15/conv1/bn/beta         [256]                       256
group2/block15/conv2/W               [3, 3, 256, 256]         589824
group2/block15/conv2/bn/gamma        [256]                       256
group2/block15/conv2/bn/beta         [256]                       256
group2/block15/conv3/W               [1, 1, 256, 1024]        262144
group2/block15/conv3/bn/gamma        [1024]                     1024
group2/block15/conv3/bn/beta         [1024]                     1024
group2/block16/conv1/W               [1, 1, 1024, 256]        262144
group2/block16/conv1/bn/gamma        [256]                       256
group2/block16/conv1/bn/beta         [256]                       256
group2/block16/conv2/W               [3, 3, 256, 256]         589824
group2/block16/conv2/bn/gamma        [256]                       256
group2/block16/conv2/bn/beta         [256]                       256
group2/block16/conv3/W               [1, 1, 256, 1024]        262144
group2/block16/conv3/bn/gamma        [1024]                     1024
group2/block16/conv3/bn/beta         [1024]                     1024
group2/block17/conv1/W               [1, 1, 1024, 256]        262144
group2/block17/conv1/bn/gamma        [256]                       256
group2/block17/conv1/bn/beta         [256]                       256
group2/block17/conv2/W               [3, 3, 256, 256]         589824
group2/block17/conv2/bn/gamma        [256]                       256
group2/block17/conv2/bn/beta         [256]                       256
group2/block17/conv3/W               [1, 1, 256, 1024]        262144
group2/block17/conv3/bn/gamma        [1024]                     1024
group2/block17/conv3/bn/beta         [1024]                     1024
group2/block18/conv1/W               [1, 1, 1024, 256]        262144
group2/block18/conv1/bn/gamma        [256]                       256
group2/block18/conv1/bn/beta         [256]                       256
group2/block18/conv2/W               [3, 3, 256, 256]         589824
group2/block18/conv2/bn/gamma        [256]                       256
group2/block18/conv2/bn/beta         [256]                       256
group2/block18/conv3/W               [1, 1, 256, 1024]        262144
group2/block18/conv3/bn/gamma        [1024]                     1024
group2/block18/conv3/bn/beta         [1024]                     1024
group2/block19/conv1/W               [1, 1, 1024, 256]        262144
group2/block19/conv1/bn/gamma        [256]                       256
group2/block19/conv1/bn/beta         [256]                       256
group2/block19/conv2/W               [3, 3, 256, 256]         589824
group2/block19/conv2/bn/gamma        [256]                       256
group2/block19/conv2/bn/beta         [256]                       256
group2/block19/conv3/W               [1, 1, 256, 1024]        262144
group2/block19/conv3/bn/gamma        [1024]                     1024
group2/block19/conv3/bn/beta         [1024]                     1024
group2/block20/conv1/W               [1, 1, 1024, 256]        262144
group2/block20/conv1/bn/gamma        [256]                       256
group2/block20/conv1/bn/beta         [256]                       256
group2/block20/conv2/W               [3, 3, 256, 256]         589824
group2/block20/conv2/bn/gamma        [256]                       256
group2/block20/conv2/bn/beta         [256]                       256
group2/block20/conv3/W               [1, 1, 256, 1024]        262144
group2/block20/conv3/bn/gamma        [1024]                     1024
group2/block20/conv3/bn/beta         [1024]                     1024
group2/block21/conv1/W               [1, 1, 1024, 256]        262144
group2/block21/conv1/bn/gamma        [256]                       256
group2/block21/conv1/bn/beta         [256]                       256
group2/block21/conv2/W               [3, 3, 256, 256]         589824
group2/block21/conv2/bn/gamma        [256]                       256
group2/block21/conv2/bn/beta         [256]                       256
group2/block21/conv3/W               [1, 1, 256, 1024]        262144
group2/block21/conv3/bn/gamma        [1024]                     1024
group2/block21/conv3/bn/beta         [1024]                     1024
group2/block22/conv1/W               [1, 1, 1024, 256]        262144
group2/block22/conv1/bn/gamma        [256]                       256
group2/block22/conv1/bn/beta         [256]                       256
group2/block22/conv2/W               [3, 3, 256, 256]         589824
group2/block22/conv2/bn/gamma        [256]                       256
group2/block22/conv2/bn/beta         [256]                       256
group2/block22/conv3/W               [1, 1, 256, 1024]        262144
group2/block22/conv3/bn/gamma        [1024]                     1024
group2/block22/conv3/bn/beta         [1024]                     1024
rpn/conv0/W                          [3, 3, 1024, 1024]      9437184
rpn/conv0/b                          [1024]                     1024
rpn/class/W                          [1, 1, 1024, 15]          15360
rpn/class/b                          [15]                         15
rpn/box/W                            [1, 1, 1024, 60]          61440
rpn/box/b                            [60]                         60
group3/block0/conv1/W                [1, 1, 1024, 512]        524288
group3/block0/conv1/bn/gamma         [512]                       512
group3/block0/conv1/bn/beta          [512]                       512
group3/block0/conv2/W                [3, 3, 512, 512]        2359296
group3/block0/conv2/bn/gamma         [512]                       512
group3/block0/conv2/bn/beta          [512]                       512
group3/block0/conv3/W                [1, 1, 512, 2048]       1048576
group3/block0/conv3/bn/gamma         [2048]                     2048
group3/block0/conv3/bn/beta          [2048]                     2048
group3/block0/convshortcut/W         [1, 1, 1024, 2048]      2097152
group3/block0/convshortcut/bn/gamma  [2048]                     2048
group3/block0/convshortcut/bn/beta   [2048]                     2048
group3/block1/conv1/W                [1, 1, 2048, 512]       1048576
group3/block1/conv1/bn/gamma         [512]                       512
group3/block1/conv1/bn/beta          [512]                       512
group3/block1/conv2/W                [3, 3, 512, 512]        2359296
group3/block1/conv2/bn/gamma         [512]                       512
group3/block1/conv2/bn/beta          [512]                       512
group3/block1/conv3/W                [1, 1, 512, 2048]       1048576
group3/block1/conv3/bn/gamma         [2048]                     2048
group3/block1/conv3/bn/beta          [2048]                     2048
group3/block2/conv1/W                [1, 1, 2048, 512]       1048576
group3/block2/conv1/bn/gamma         [512]                       512
group3/block2/conv1/bn/beta          [512]                       512
group3/block2/conv2/W                [3, 3, 512, 512]        2359296
group3/block2/conv2/bn/gamma         [512]                       512
group3/block2/conv2/bn/beta          [512]                       512
group3/block2/conv3/W                [1, 1, 512, 2048]       1048576
group3/block2/conv3/bn/gamma         [2048]                     2048
group3/block2/conv3/bn/beta          [2048]                     2048
fastrcnn/class/W                     [2048, 2]                  4096
fastrcnn/class/b                     [2]                           2
fastrcnn/box/W                       [2048, 8]                 16384
fastrcnn/box/b                       [8]                           8
maskrcnn/deconv/W                    [2, 2, 256, 2048]       2097152
maskrcnn/deconv/b                    [256]                       256
maskrcnn/conv/W                      [1, 1, 256, 1]              256
maskrcnn/conv/b                      [1]                           1
Number of trainable variables: 293
Number of parameters (elements): 53908054
Storage space needed for all trainable variables: 205.64MB
[0810 16:01:51 @base.py:209] Setup callbacks graph ...
[0810 16:01:54 @prof.py:271] [HostMemoryTracker] Free RAM in setup_graph() is 208.84 GB.
[0810 16:01:54 @tower.py:140] Building graph for predict tower 'tower-pred-0' on device /gpu:0 ...
[0810 16:01:56 @collection.py:152] Size of these collections were changed in tower-pred-0: (tf.GraphKeys.MODEL_VARIABLES: 450->505)
[0810 16:01:56 @tower.py:140] Building graph for predict tower 'tower-pred-1' on device /gpu:1 with variable scope 'tower1'...
[0810 16:01:58 @collection.py:152] Size of these collections were changed in tower-pred-1: (tf.GraphKeys.MODEL_VARIABLES: 505->560)
[0810 16:01:58 @tower.py:140] Building graph for predict tower 'tower-pred-2' on device /gpu:2 with variable scope 'tower2'...
[0810 16:02:00 @collection.py:152] Size of these collections were changed in tower-pred-2: (tf.GraphKeys.MODEL_VARIABLES: 560->615)
[0810 16:02:00 @tower.py:140] Building graph for predict tower 'tower-pred-3' on device /gpu:3 with variable scope 'tower3'...
[0810 16:02:01 @collection.py:152] Size of these collections were changed in tower-pred-3: (tf.GraphKeys.MODEL_VARIABLES: 615->670)
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0810 16:02:01 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 115363.96it/s]
[0810 16:02:01 @timer.py:50] Load annotations for instances_val.json finished, time:0.0010 sec.
[0810 16:02:01 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0810 16:02:01 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 116856.23it/s]
[0810 16:02:01 @timer.py:50] Load annotations for instances_val.json finished, time:0.0007 sec.
[0810 16:02:01 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0810 16:02:01 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 127514.13it/s]
[0810 16:02:01 @timer.py:50] Load annotations for instances_val.json finished, time:0.0006 sec.
[0810 16:02:01 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0810 16:02:01 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 160437.86it/s]
[0810 16:02:01 @timer.py:50] Load annotations for instances_val.json finished, time:0.0005 sec.
[0810 16:02:01 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0810 16:02:01 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 129197.48it/s]
[0810 16:02:01 @timer.py:50] Load annotations for instances_val.json finished, time:0.0006 sec.
[0810 16:02:01 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0810 16:02:01 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 112923.57it/s]
[0810 16:02:01 @timer.py:50] Load annotations for instances_val.json finished, time:0.0007 sec.
[0810 16:02:01 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0810 16:02:01 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 137679.38it/s]
[0810 16:02:01 @timer.py:50] Load annotations for instances_val.json finished, time:0.0006 sec.
[0810 16:02:01 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0810 16:02:01 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 155139.38it/s]
[0810 16:02:01 @timer.py:50] Load annotations for instances_val.json finished, time:0.0006 sec.
[0810 16:02:01 @data.py:375] Found 28 images for inference.
[0810 16:02:01 @summary.py:47] [MovingAverageSummary] 28 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.
[0810 16:02:01 @summary.py:94] Summarizing collection 'summaries' of size 31.
[0810 16:02:01 @base.py:230] Creating the session ...
2019-08-10 16:02:01.911023: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-10 16:02:01.919862: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-08-10 16:02:03.118613: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5583f0b17fa0 executing computations on platform CUDA. Devices:
2019-08-10 16:02:03.118694: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5
2019-08-10 16:02:03.118708: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): TITAN RTX, Compute Capability 7.5
2019-08-10 16:02:03.118717: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): TITAN RTX, Compute Capability 7.5
2019-08-10 16:02:03.118725: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): TITAN RTX, Compute Capability 7.5
2019-08-10 16:02:03.123605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300010000 Hz
2019-08-10 16:02:03.133951: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5583f0cdeba0 executing computations on platform Host. Devices:
2019-08-10 16:02:03.133999: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-10 16:02:03.137014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:02:00.0
2019-08-10 16:02:03.139550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:03:00.0
2019-08-10 16:02:03.142103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:82:00.0
2019-08-10 16:02:03.144504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:83:00.0
2019-08-10 16:02:03.144858: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-10 16:02:03.147223: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-10 16:02:03.149418: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-10 16:02:03.149884: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-10 16:02:03.152825: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-10 16:02:03.154944: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-10 16:02:03.161360: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-10 16:02:03.180823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3
2019-08-10 16:02:03.180896: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-10 16:02:03.199424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-10 16:02:03.199488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 
2019-08-10 16:02:03.199531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N N N N 
2019-08-10 16:02:03.199551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N N N N 
2019-08-10 16:02:03.199579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   N N N N 
2019-08-10 16:02:03.199612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   N N N N 
2019-08-10 16:02:03.225211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 23978 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:02:00.0, compute capability: 7.5)
2019-08-10 16:02:03.231325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 23978 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:03:00.0, compute capability: 7.5)
2019-08-10 16:02:03.236913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 23978 MB memory) -> physical GPU (device: 2, name: TITAN RTX, pci bus id: 0000:82:00.0, compute capability: 7.5)
2019-08-10 16:02:03.240844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 23978 MB memory) -> physical GPU (device: 3, name: TITAN RTX, pci bus id: 0000:83:00.0, compute capability: 7.5)
2019-08-10 16:02:10.504896: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[0810 16:02:32 @base.py:236] Initializing the session ...
[0810 16:02:32 @sessinit.py:223] Variables to restore from dict: conv0/W, conv0/bn/gamma, conv0/bn/beta, conv0/bn/mean/EMA, conv0/bn/variance/EMA, group0/block0/conv1/W, group0/block0/conv1/bn/gamma, group0/block0/conv1/bn/beta, group0/block0/conv1/bn/mean/EMA, group0/block0/conv1/bn/variance/EMA, group0/block0/conv2/W, group0/block0/conv2/bn/gamma, group0/block0/conv2/bn/beta, group0/block0/conv2/bn/mean/EMA, group0/block0/conv2/bn/variance/EMA, group0/block0/conv3/W, group0/block0/conv3/bn/gamma, group0/block0/conv3/bn/beta, group0/block0/conv3/bn/mean/EMA, group0/block0/conv3/bn/variance/EMA, group0/block0/convshortcut/W, group0/block0/convshortcut/bn/gamma, group0/block0/convshortcut/bn/beta, group0/block0/convshortcut/bn/mean/EMA, group0/block0/convshortcut/bn/variance/EMA, group0/block1/conv1/W, group0/block1/conv1/bn/gamma, group0/block1/conv1/bn/beta, group0/block1/conv1/bn/mean/EMA, group0/block1/conv1/bn/variance/EMA, group0/block1/conv2/W, group0/block1/conv2/bn/gamma, group0/block1/conv2/bn/beta, group0/block1/conv2/bn/mean/EMA, group0/block1/conv2/bn/variance/EMA, group0/block1/conv3/W, group0/block1/conv3/bn/gamma, group0/block1/conv3/bn/beta, group0/block1/conv3/bn/mean/EMA, group0/block1/conv3/bn/variance/EMA, group0/block2/conv1/W, group0/block2/conv1/bn/gamma, group0/block2/conv1/bn/beta, group0/block2/conv1/bn/mean/EMA, group0/block2/conv1/bn/variance/EMA, group0/block2/conv2/W, group0/block2/conv2/bn/gamma, group0/block2/conv2/bn/beta, group0/block2/conv2/bn/mean/EMA, group0/block2/conv2/bn/variance/EMA, group0/block2/conv3/W, group0/block2/conv3/bn/gamma, group0/block2/conv3/bn/beta, group0/block2/conv3/bn/mean/EMA, group0/block2/conv3/bn/variance/EMA, group1/block0/conv1/W, group1/block0/conv1/bn/gamma, group1/block0/conv1/bn/beta, group1/block0/conv1/bn/mean/EMA, group1/block0/conv1/bn/variance/EMA, group1/block0/conv2/W, group1/block0/conv2/bn/gamma, group1/block0/conv2/bn/beta, group1/block0/conv2/bn/mean/EMA, group1/block0/conv2/bn/variance/EMA, group1/block0/conv3/W, group1/block0/conv3/bn/gamma, group1/block0/conv3/bn/beta, group1/block0/conv3/bn/mean/EMA, group1/block0/conv3/bn/variance/EMA, group1/block0/convshortcut/W, group1/block0/convshortcut/bn/gamma, group1/block0/convshortcut/bn/beta, group1/block0/convshortcut/bn/mean/EMA, group1/block0/convshortcut/bn/variance/EMA, group1/block1/conv1/W, group1/block1/conv1/bn/gamma, group1/block1/conv1/bn/beta, group1/block1/conv1/bn/mean/EMA, group1/block1/conv1/bn/variance/EMA, group1/block1/conv2/W, group1/block1/conv2/bn/gamma, group1/block1/conv2/bn/beta, group1/block1/conv2/bn/mean/EMA, group1/block1/conv2/bn/variance/EMA, group1/block1/conv3/W, group1/block1/conv3/bn/gamma, group1/block1/conv3/bn/beta, group1/block1/conv3/bn/mean/EMA, group1/block1/conv3/bn/variance/EMA, group1/block2/conv1/W, group1/block2/conv1/bn/gamma, group1/block2/conv1/bn/beta, group1/block2/conv1/bn/mean/EMA, group1/block2/conv1/bn/variance/EMA, group1/block2/conv2/W, group1/block2/conv2/bn/gamma, group1/block2/conv2/bn/beta, group1/block2/conv2/bn/mean/EMA, group1/block2/conv2/bn/variance/EMA, group1/block2/conv3/W, group1/block2/conv3/bn/gamma, group1/block2/conv3/bn/beta, group1/block2/conv3/bn/mean/EMA, group1/block2/conv3/bn/variance/EMA, group1/block3/conv1/W, group1/block3/conv1/bn/gamma, group1/block3/conv1/bn/beta, group1/block3/conv1/bn/mean/EMA, group1/block3/conv1/bn/variance/EMA, group1/block3/conv2/W, group1/block3/conv2/bn/gamma, group1/block3/conv2/bn/beta, group1/block3/conv2/bn/mean/EMA, group1/block3/conv2/bn/variance/EMA, group1/block3/conv3/W, group1/block3/conv3/bn/gamma, group1/block3/conv3/bn/beta, group1/block3/conv3/bn/mean/EMA, group1/block3/conv3/bn/variance/EMA, group2/block0/conv1/W, group2/block0/conv1/bn/gamma, group2/block0/conv1/bn/beta, group2/block0/conv1/bn/mean/EMA, group2/block0/conv1/bn/variance/EMA, group2/block0/conv2/W, group2/block0/conv2/bn/gamma, group2/block0/conv2/bn/beta, group2/block0/conv2/bn/mean/EMA, group2/block0/conv2/bn/variance/EMA, group2/block0/conv3/W, group2/block0/conv3/bn/gamma, group2/block0/conv3/bn/beta, group2/block0/conv3/bn/mean/EMA, group2/block0/conv3/bn/variance/EMA, group2/block0/convshortcut/W, group2/block0/convshortcut/bn/gamma, group2/block0/convshortcut/bn/beta, group2/block0/convshortcut/bn/mean/EMA, group2/block0/convshortcut/bn/variance/EMA, group2/block1/conv1/W, group2/block1/conv1/bn/gamma, group2/block1/conv1/bn/beta, group2/block1/conv1/bn/mean/EMA, group2/block1/conv1/bn/variance/EMA, group2/block1/conv2/W, group2/block1/conv2/bn/gamma, group2/block1/conv2/bn/beta, group2/block1/conv2/bn/mean/EMA, group2/block1/conv2/bn/variance/EMA, group2/block1/conv3/W, group2/block1/conv3/bn/gamma, group2/block1/conv3/bn/beta, group2/block1/conv3/bn/mean/EMA, group2/block1/conv3/bn/variance/EMA, group2/block2/conv1/W, group2/block2/conv1/bn/gamma, group2/block2/conv1/bn/beta, group2/block2/conv1/bn/mean/EMA, group2/block2/conv1/bn/variance/EMA, group2/block2/conv2/W, group2/block2/conv2/bn/gamma, group2/block2/conv2/bn/beta, group2/block2/conv2/bn/mean/EMA, group2/block2/conv2/bn/variance/EMA, group2/block2/conv3/W, group2/block2/conv3/bn/gamma, group2/block2/conv3/bn/beta, group2/block2/conv3/bn/mean/EMA, group2/block2/conv3/bn/variance/EMA, group2/block3/conv1/W, group2/block3/conv1/bn/gamma, group2/block3/conv1/bn/beta, group2/block3/conv1/bn/mean/EMA, group2/block3/conv1/bn/variance/EMA, group2/block3/conv2/W, group2/block3/conv2/bn/gamma, group2/block3/conv2/bn/beta, group2/block3/conv2/bn/mean/EMA, group2/block3/conv2/bn/variance/EMA, group2/block3/conv3/W, group2/block3/conv3/bn/gamma, group2/block3/conv3/bn/beta, group2/block3/conv3/bn/mean/EMA, group2/block3/conv3/bn/variance/EMA, group2/block4/conv1/W, group2/block4/conv1/bn/gamma, group2/block4/conv1/bn/beta, group2/block4/conv1/bn/mean/EMA, group2/block4/conv1/bn/variance/EMA, group2/block4/conv2/W, group2/block4/conv2/bn/gamma, group2/block4/conv2/bn/beta, group2/block4/conv2/bn/mean/EMA, group2/block4/conv2/bn/variance/EMA, group2/block4/conv3/W, group2/block4/conv3/bn/gamma, group2/block4/conv3/bn/beta, group2/block4/conv3/bn/mean/EMA, group2/block4/conv3/bn/variance/EMA, group2/block5/conv1/W, group2/block5/conv1/bn/gamma, group2/block5/conv1/bn/beta, group2/block5/conv1/bn/mean/EMA, group2/block5/conv1/bn/variance/EMA, group2/block5/conv2/W, group2/block5/conv2/bn/gamma, group2/block5/conv2/bn/beta, group2/block5/conv2/bn/mean/EMA, group2/block5/conv2/bn/variance/EMA, group2/block5/conv3/W, group2/block5/conv3/bn/gamma, group2/block5/conv3/bn/beta, group2/block5/conv3/bn/mean/EMA, group2/block5/conv3/bn/variance/EMA, group2/block6/conv1/W, group2/block6/conv1/bn/gamma, group2/block6/conv1/bn/beta, group2/block6/conv1/bn/mean/EMA, group2/block6/conv1/bn/variance/EMA, group2/block6/conv2/W, group2/block6/conv2/bn/gamma, group2/block6/conv2/bn/beta, group2/block6/conv2/bn/mean/EMA, group2/block6/conv2/bn/variance/EMA, group2/block6/conv3/W, group2/block6/conv3/bn/gamma, group2/block6/conv3/bn/beta, group2/block6/conv3/bn/mean/EMA, group2/block6/conv3/bn/variance/EMA, group2/block7/conv1/W, group2/block7/conv1/bn/gamma, group2/block7/conv1/bn/beta, group2/block7/conv1/bn/mean/EMA, group2/block7/conv1/bn/variance/EMA, group2/block7/conv2/W, group2/block7/conv2/bn/gamma, group2/block7/conv2/bn/beta, group2/block7/conv2/bn/mean/EMA, group2/block7/conv2/bn/variance/EMA, group2/block7/conv3/W, group2/block7/conv3/bn/gamma, group2/block7/conv3/bn/beta, group2/block7/conv3/bn/mean/EMA, group2/block7/conv3/bn/variance/EMA, group2/block8/conv1/W, group2/block8/conv1/bn/gamma, group2/block8/conv1/bn/beta, group2/block8/conv1/bn/mean/EMA, group2/block8/conv1/bn/variance/EMA, group2/block8/conv2/W, group2/block8/conv2/bn/gamma, group2/block8/conv2/bn/beta, group2/block8/conv2/bn/mean/EMA, group2/block8/conv2/bn/variance/EMA, group2/block8/conv3/W, group2/block8/conv3/bn/gamma, group2/block8/conv3/bn/beta, group2/block8/conv3/bn/mean/EMA, group2/block8/conv3/bn/variance/EMA, group2/block9/conv1/W, group2/block9/conv1/bn/gamma, group2/block9/conv1/bn/beta, group2/block9/conv1/bn/mean/EMA, group2/block9/conv1/bn/variance/EMA, group2/block9/conv2/W, group2/block9/conv2/bn/gamma, group2/block9/conv2/bn/beta, group2/block9/conv2/bn/mean/EMA, group2/block9/conv2/bn/variance/EMA, group2/block9/conv3/W, group2/block9/conv3/bn/gamma, group2/block9/conv3/bn/beta, group2/block9/conv3/bn/mean/EMA, group2/block9/conv3/bn/variance/EMA, group2/block10/conv1/W, group2/block10/conv1/bn/gamma, group2/block10/conv1/bn/beta, group2/block10/conv1/bn/mean/EMA, group2/block10/conv1/bn/variance/EMA, group2/block10/conv2/W, group2/block10/conv2/bn/gamma, group2/block10/conv2/bn/beta, group2/block10/conv2/bn/mean/EMA, group2/block10/conv2/bn/variance/EMA, group2/block10/conv3/W, group2/block10/conv3/bn/gamma, group2/block10/conv3/bn/beta, group2/block10/conv3/bn/mean/EMA, group2/block10/conv3/bn/variance/EMA, group2/block11/conv1/W, group2/block11/conv1/bn/gamma, group2/block11/conv1/bn/beta, group2/block11/conv1/bn/mean/EMA, group2/block11/conv1/bn/variance/EMA, group2/block11/conv2/W, group2/block11/conv2/bn/gamma, group2/block11/conv2/bn/beta, group2/block11/conv2/bn/mean/EMA, group2/block11/conv2/bn/variance/EMA, group2/block11/conv3/W, group2/block11/conv3/bn/gamma, group2/block11/conv3/bn/beta, group2/block11/conv3/bn/mean/EMA, group2/block11/conv3/bn/variance/EMA, group2/block12/conv1/W, group2/block12/conv1/bn/gamma, group2/block12/conv1/bn/beta, group2/block12/conv1/bn/mean/EMA, group2/block12/conv1/bn/variance/EMA, group2/block12/conv2/W, group2/block12/conv2/bn/gamma, group2/block12/conv2/bn/beta, group2/block12/conv2/bn/mean/EMA, group2/block12/conv2/bn/variance/EMA, group2/block12/conv3/W, group2/block12/conv3/bn/gamma, group2/block12/conv3/bn/beta, group2/block12/conv3/bn/mean/EMA, group2/block12/conv3/bn/variance/EMA, group2/block13/conv1/W, group2/block13/conv1/bn/gamma, group2/block13/conv1/bn/beta, group2/block13/conv1/bn/mean/EMA, group2/block13/conv1/bn/variance/EMA, group2/block13/conv2/W, group2/block13/conv2/bn/gamma, group2/block13/conv2/bn/beta, group2/block13/conv2/bn/mean/EMA, group2/block13/conv2/bn/variance/EMA, group2/block13/conv3/W, group2/block13/conv3/bn/gamma, group2/block13/conv3/bn/beta, group2/block13/conv3/bn/mean/EMA, group2/block13/conv3/bn/variance/EMA, group2/block14/conv1/W, group2/block14/conv1/bn/gamma, group2/block14/conv1/bn/beta, group2/block14/conv1/bn/mean/EMA, group2/block14/conv1/bn/variance/EMA, group2/block14/conv2/W, group2/block14/conv2/bn/gamma, group2/block14/conv2/bn/beta, group2/block14/conv2/bn/mean/EMA, group2/block14/conv2/bn/variance/EMA, group2/block14/conv3/W, group2/block14/conv3/bn/gamma, group2/block14/conv3/bn/beta, group2/block14/conv3/bn/mean/EMA, group2/block14/conv3/bn/variance/EMA, group2/block15/conv1/W, group2/block15/conv1/bn/gamma, group2/block15/conv1/bn/beta, group2/block15/conv1/bn/mean/EMA, group2/block15/conv1/bn/variance/EMA, group2/block15/conv2/W, group2/block15/conv2/bn/gamma, group2/block15/conv2/bn/beta, group2/block15/conv2/bn/mean/EMA, group2/block15/conv2/bn/variance/EMA, group2/block15/conv3/W, group2/block15/conv3/bn/gamma, group2/block15/conv3/bn/beta, group2/block15/conv3/bn/mean/EMA, group2/block15/conv3/bn/variance/EMA, group2/block16/conv1/W, group2/block16/conv1/bn/gamma, group2/block16/conv1/bn/beta, group2/block16/conv1/bn/mean/EMA, group2/block16/conv1/bn/variance/EMA, group2/block16/conv2/W, group2/block16/conv2/bn/gamma, group2/block16/conv2/bn/beta, group2/block16/conv2/bn/mean/EMA, group2/block16/conv2/bn/variance/EMA, group2/block16/conv3/W, group2/block16/conv3/bn/gamma, group2/block16/conv3/bn/beta, group2/block16/conv3/bn/mean/EMA, group2/block16/conv3/bn/variance/EMA, group2/block17/conv1/W, group2/block17/conv1/bn/gamma, group2/block17/conv1/bn/beta, group2/block17/conv1/bn/mean/EMA, group2/block17/conv1/bn/variance/EMA, group2/block17/conv2/W, group2/block17/conv2/bn/gamma, group2/block17/conv2/bn/beta, group2/block17/conv2/bn/mean/EMA, group2/block17/conv2/bn/variance/EMA, group2/block17/conv3/W, group2/block17/conv3/bn/gamma, group2/block17/conv3/bn/beta, group2/block17/conv3/bn/mean/EMA, group2/block17/conv3/bn/variance/EMA, group2/block18/conv1/W, group2/block18/conv1/bn/gamma, group2/block18/conv1/bn/beta, group2/block18/conv1/bn/mean/EMA, group2/block18/conv1/bn/variance/EMA, group2/block18/conv2/W, group2/block18/conv2/bn/gamma, group2/block18/conv2/bn/beta, group2/block18/conv2/bn/mean/EMA, group2/block18/conv2/bn/variance/EMA, group2/block18/conv3/W, group2/block18/conv3/bn/gamma, group2/block18/conv3/bn/beta, group2/block18/conv3/bn/mean/EMA, group2/block18/conv3/bn/variance/EMA, group2/block19/conv1/W, group2/block19/conv1/bn/gamma, group2/block19/conv1/bn/beta, group2/block19/conv1/bn/mean/EMA, group2/block19/conv1/bn/variance/EMA, group2/block19/conv2/W, group2/block19/conv2/bn/gamma, group2/block19/conv2/bn/beta, group2/block19/conv2/bn/mean/EMA, group2/block19/conv2/bn/variance/EMA, group2/block19/conv3/W, group2/block19/conv3/bn/gamma, group2/block19/conv3/bn/beta, group2/block19/conv3/bn/mean/EMA, group2/block19/conv3/bn/variance/EMA, group2/block20/conv1/W, group2/block20/conv1/bn/gamma, group2/block20/conv1/bn/beta, group2/block20/conv1/bn/mean/EMA, group2/block20/conv1/bn/variance/EMA, group2/block20/conv2/W, group2/block20/conv2/bn/gamma, group2/block20/conv2/bn/beta, group2/block20/conv2/bn/mean/EMA, group2/block20/conv2/bn/variance/EMA, group2/block20/conv3/W, group2/block20/conv3/bn/gamma, group2/block20/conv3/bn/beta, group2/block20/conv3/bn/mean/EMA, group2/block20/conv3/bn/variance/EMA, group2/block21/conv1/W, group2/block21/conv1/bn/gamma, group2/block21/conv1/bn/beta, group2/block21/conv1/bn/mean/EMA, group2/block21/conv1/bn/variance/EMA, group2/block21/conv2/W, group2/block21/conv2/bn/gamma, group2/block21/conv2/bn/beta, group2/block21/conv2/bn/mean/EMA, group2/block21/conv2/bn/variance/EMA, group2/block21/conv3/W, group2/block21/conv3/bn/gamma, group2/block21/conv3/bn/beta, group2/block21/conv3/bn/mean/EMA, group2/block21/conv3/bn/variance/EMA, group2/block22/conv1/W, group2/block22/conv1/bn/gamma, group2/block22/conv1/bn/beta, group2/block22/conv1/bn/mean/EMA, group2/block22/conv1/bn/variance/EMA, group2/block22/conv2/W, group2/block22/conv2/bn/gamma, group2/block22/conv2/bn/beta, group2/block22/conv2/bn/mean/EMA, group2/block22/conv2/bn/variance/EMA, group2/block22/conv3/W, group2/block22/conv3/bn/gamma, group2/block22/conv3/bn/beta, group2/block22/conv3/bn/mean/EMA, group2/block22/conv3/bn/variance/EMA, rpn/conv0/W, rpn/conv0/b, rpn/class/W, rpn/class/b, rpn/box/W, rpn/box/b, group3/block0/conv1/W, group3/block0/conv1/bn/gamma, group3/block0/conv1/bn/beta, group3/block0/conv1/bn/mean/EMA, group3/block0/conv1/bn/variance/EMA, group3/block0/conv2/W, group3/block0/conv2/bn/gamma, group3/block0/conv2/bn/beta, group3/block0/conv2/bn/mean/EMA, group3/block0/conv2/bn/variance/EMA, group3/block0/conv3/W, group3/block0/conv3/bn/gamma, group3/block0/conv3/bn/beta, group3/block0/conv3/bn/mean/EMA, group3/block0/conv3/bn/variance/EMA, group3/block0/convshortcut/W, group3/block0/convshortcut/bn/gamma, group3/block0/convshortcut/bn/beta, group3/block0/convshortcut/bn/mean/EMA, group3/block0/convshortcut/bn/variance/EMA, group3/block1/conv1/W, group3/block1/conv1/bn/gamma, group3/block1/conv1/bn/beta, group3/block1/conv1/bn/mean/EMA, group3/block1/conv1/bn/variance/EMA, group3/block1/conv2/W, group3/block1/conv2/bn/gamma, group3/block1/conv2/bn/beta, group3/block1/conv2/bn/mean/EMA, group3/block1/conv2/bn/variance/EMA, group3/block1/conv3/W, group3/block1/conv3/bn/gamma, group3/block1/conv3/bn/beta, group3/block1/conv3/bn/mean/EMA, group3/block1/conv3/bn/variance/EMA, group3/block2/conv1/W, group3/block2/conv1/bn/gamma, group3/block2/conv1/bn/beta, group3/block2/conv1/bn/mean/EMA, group3/block2/conv1/bn/variance/EMA, group3/block2/conv2/W, group3/block2/conv2/bn/gamma, group3/block2/conv2/bn/beta, group3/block2/conv2/bn/mean/EMA, group3/block2/conv2/bn/variance/EMA, group3/block2/conv3/W, group3/block2/conv3/bn/gamma, group3/block2/conv3/bn/beta, group3/block2/conv3/bn/mean/EMA, group3/block2/conv3/bn/variance/EMA, fastrcnn/class/W, fastrcnn/class/b, fastrcnn/box/W, fastrcnn/box/b, maskrcnn/deconv/W, maskrcnn/deconv/b, maskrcnn/conv/W, maskrcnn/conv/b
[0810 16:02:32 @sessinit.py:87] WRN The following variables are in the graph, but not found in the dict: global_step, learning_rate
[0810 16:02:32 @sessinit.py:236] Restoring 534 variables from dict ...
[0810 16:02:32 @varmanip.py:80] WRN Cannot load a tensor of shape (1, 1, 256, 80) into the variable 'maskrcnn/conv/W' whose shape is (1, 1, 256, 1).
[0810 16:02:32 @varmanip.py:80] WRN Cannot load a tensor of shape (2048, 81) into the variable 'fastrcnn/class/W' whose shape is (2048, 2).
[0810 16:02:32 @varmanip.py:80] WRN Cannot load a tensor of shape (81,) into the variable 'fastrcnn/class/b' whose shape is (2,).
[0810 16:02:32 @varmanip.py:80] WRN Cannot load a tensor of shape (2048, 324) into the variable 'fastrcnn/box/W' whose shape is (2048, 8).
[0810 16:02:32 @varmanip.py:80] WRN Cannot load a tensor of shape (324,) into the variable 'fastrcnn/box/b' whose shape is (8,).
[0810 16:02:32 @varmanip.py:80] WRN Cannot load a tensor of shape (80,) into the variable 'maskrcnn/conv/b' whose shape is (1,).
[0810 16:02:33 @base.py:243] Graph Finalized.
[0810 16:02:34 @concurrency.py:37] Starting EnqueueThread QueueInput/input_queue ...
[0810 16:02:34 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
Corrupt JPEG data: premature end of data segment
[0810 16:02:53 @param.py:158] [HyperParamSetter] At global_step=0, learning_rate is set to 0.003300
[0810 16:02:56 @prof.py:274] [HostMemoryTracker] Free RAM in before_train() is 201.36 GB.
[0810 16:02:56 @eval.py:259] [EvalCallback] Will evaluate every 25 epochs
[0810 16:02:57 @base.py:275] Start Epoch 1 ...
  0%|                                                                                                                                                                |0/50[00:00<?,?it/s][0810 16:02:57 @input_source.py:556] Pre-filling StagingArea ...
[0810 16:02:59 @input_source.py:560] 1 element was put into StagingArea on each tower.
2019-08-10 16:04:19.202542: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-10 16:04:22.562062: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
[0810 16:04:33 @param.py:161] [HyperParamSetter] At global_step=1, learning_rate changes from 0.003300 to 0.003307
 36%|######################################################3                                                                                                |18/50[02:02<00:38, 0.83it/s]Corrupt JPEG data: premature end of data segment
100%|#######################################################################################################################################################|50/50[03:26<00:00, 0.03it/s]
[0810 16:06:23 @base.py:285] Epoch 1 (global_step 50) finished, time:3 minutes 26 seconds.
[0810 16:06:23 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
[0810 16:06:24 @misc.py:109] Estimated Time Left: 17 days 7 hours 37 minutes 33 seconds
[0810 16:06:24 @monitor.py:474] GPUUtil/0: 15.927
[0810 16:06:24 @monitor.py:474] GPUUtil/1: 17.005
[0810 16:06:24 @monitor.py:474] GPUUtil/2: 17.463
[0810 16:06:24 @monitor.py:474] GPUUtil/3: 17.556
[0810 16:06:24 @monitor.py:474] HostFreeMemory (GB): 193.59
[0810 16:06:24 @monitor.py:474] PeakMemory(MB)/gpu:0: 10020
[0810 16:06:24 @monitor.py:474] QueueInput/queue_size: 49.464
[0810 16:06:24 @monitor.py:474] Throughput (samples/sec): 0.96746
[0810 16:06:24 @monitor.py:474] fastrcnn_losses/box_loss: 0.02821
[0810 16:06:24 @monitor.py:474] fastrcnn_losses/label_loss: nan
[0810 16:06:24 @monitor.py:474] fastrcnn_losses/label_metrics/accuracy: 0.98203
[0810 16:06:24 @monitor.py:474] fastrcnn_losses/label_metrics/false_negative: 0.99561
[0810 16:06:24 @monitor.py:474] fastrcnn_losses/label_metrics/fg_accuracy: 0
[0810 16:06:24 @monitor.py:474] fastrcnn_losses/num_fg_label: 2.8304
[0810 16:06:24 @monitor.py:474] learning_rate: 0.0036283
[0810 16:06:24 @monitor.py:474] maskrcnn_loss/accuracy: 0.62613
[0810 16:06:24 @monitor.py:474] maskrcnn_loss/fg_pixel_ratio: 0.37387
[0810 16:06:24 @monitor.py:474] maskrcnn_loss/maskrcnn_loss: 0.69315
[0810 16:06:24 @monitor.py:474] maskrcnn_loss/pos_accuracy: 0
[0810 16:06:24 @monitor.py:474] mean_gt_box_area: 63784
[0810 16:06:24 @monitor.py:474] rpn_losses/box_loss: 0.031278
[0810 16:06:24 @monitor.py:474] rpn_losses/label_loss: 0.3122
[0810 16:06:24 @monitor.py:474] rpn_losses/label_metrics/precision_th0.1: 0.065993
[0810 16:06:24 @monitor.py:474] rpn_losses/label_metrics/precision_th0.2: 0.1322
[0810 16:06:24 @monitor.py:474] rpn_losses/label_metrics/precision_th0.5: 0.15644
[0810 16:06:24 @monitor.py:474] rpn_losses/label_metrics/recall_th0.1: 0.99927
[0810 16:06:24 @monitor.py:474] rpn_losses/label_metrics/recall_th0.2: 0.58366
[0810 16:06:24 @monitor.py:474] rpn_losses/label_metrics/recall_th0.5: 0.17936
[0810 16:06:24 @monitor.py:474] rpn_losses/num_pos_anchor: 16.552
[0810 16:06:24 @monitor.py:474] rpn_losses/num_valid_anchor: 255.77
[0810 16:06:24 @monitor.py:474] sample_fast_rcnn_targets/num_bg: 187.04
[0810 16:06:24 @monitor.py:474] sample_fast_rcnn_targets/num_fg: 2.8304
[0810 16:06:24 @monitor.py:474] sample_fast_rcnn_targets/proposal_metrics/best_iou_per_gt: 0.2385
[0810 16:06:24 @monitor.py:474] sample_fast_rcnn_targets/proposal_metrics/recall_iou0.3: 0.32906
[0810 16:06:24 @monitor.py:474] sample_fast_rcnn_targets/proposal_metrics/recall_iou0.5: 0.18301
[0810 16:06:24 @monitor.py:474] total_cost: nan
[0810 16:06:24 @monitor.py:474] wd_cost: nan
",following default load environment information python default compiler version support true support false support false driver none free ram count true false false false true false false true warm schedule value schedule value loading memory done index index loaded load finished time sec category distribution class box class box class box connector total contain total training total training set none monitor visible training model automatically setting queue building graph training tower device training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training input output training training training training training training training training training training input output input output found regularize following converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown building graph training tower device found regularize converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown size tower tower building graph training tower device found regularize converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown size tower tower building graph training tower device found regularize converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown size tower tower list trainable name shape number trainable number storage space trainable setup graph free ram building graph predict tower device size building graph predict tower device variable scope size building graph predict tower device variable scope size building graph predict tower device variable scope size loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference collection run session collection size session binary use successfully dynamic library service platform device compute capability device compute capability device compute capability device compute capability frequency service platform host device undefined undefined found device name major minor found device name major minor found device name major minor found device name major minor successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible successfully dynamic library device interconnect strength edge matrix device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability warning cluster set want either set use enable confirm active pas proper flag via set session restore following graph found load tensor shape variable whose shape load tensor shape variable whose shape load tensor shape variable whose shape load tensor shape variable whose shape load tensor shape variable whose shape load tensor shape variable whose shape graph starting running corrupt data premature end data segment set free ram evaluate every start epoch element put tower successfully dynamic library successfully dynamic library corrupt data premature end data segment epoch finished time running time left day throughput nan nan nan,issue,positive,negative,neutral,neutral,negative,negative
520128425,"> If you mean it fails on your own data, again there isn't much I can do since it is your own data. It does work with COCO, and it does work with the balloon dataset - I think they are enough resources for one to learn how to use the code, but of course it does not mean you don't have to tune for a new dataset.
> 
> What I can see from the log, is that it's not reasonable to use SyncBN in your setup. If you don't have a lot of experience it's better to start with the exact config of the balloon example.

Hi Yuxin, I used FreezeBN at the first and it gives the loss Nan, so I decided to try difference settings. What I paste here is the last experiment I've done",mean data much since data work coco work balloon think enough one learn use code course mean tune new see log reasonable use setup lot experience better start exact balloon example hi used first loss nan decided try difference paste last experiment done,issue,negative,positive,neutral,neutral,positive,positive
520128113,"If you mean it fails on your own data, again there isn't much I can do since it is your own data. It does work with COCO, and it does work with the balloon dataset - I think they are enough resources for one to learn how to use the code, but of course it does not mean you don't have to tune for a new dataset.

What I can see from the log, is that it's not reasonable to use SyncBN in your setup. If you don't have a lot of experience it's better to start with the exact config of the balloon example.",mean data much since data work coco work balloon think enough one learn use code course mean tune new see log reasonable use setup lot experience better start exact balloon example,issue,negative,positive,neutral,neutral,positive,positive
520126793,"> Did you make any modifications? If not, could you post full logs and the environment following the issue template?

1. What I've done:
the full logs as follow: 
```
(tensorflow) [bigdata@bigdata-cv-2019-2 FasterRCNN]$ ./train.sh 
[0808 20:57:11 @logger.py:90] Argv: ./train.py --config DATA.BASEDIR=/home/bigdata/data MODE_MASK=True MODE_FPN=False --load /home/bigdata/coco/COCO-MaskRCNN-R101C41x.npz
[0808 20:57:11 @train.py:56] Environment Information:
--------------------  -------------------------------------------------------------------
sys.platform          linux
Python                3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0]
Tensorpack            0.9.7.1-1-g13ee370-dirty
Numpy                 1.16.2
TensorFlow            1.14.0/v1.14.0-rc1-22-gaf24dc91b5
TF Compiler Version   4.8.5
TF CUDA support       True
TF MKL support        False
TF XLA support        False
Nvidia Driver         /usr/lib64/libnvidia-ml.so.430.40
CUDA                  /usr/local/cuda-10.1/lib64/libcudart.so.10.0.130
CUDNN                 /usr/local/cuda-10.1/lib64/libcudnn.so
NCCL                  /usr/lib64/libnccl.so.2.4.8
CUDA_VISIBLE_DEVICES  None
GPU 0,1,2,3           TITAN RTX
Free RAM              211.48/220.12 GB
CPU Count             72
cv2                   4.1.0
msgpack               0.6.1
python-prctl          True
--------------------  -------------------------------------------------------------------
[0808 20:57:11 @config.py:305] Config: ------------------------------------------
{'BACKBONE': {'FREEZE_AFFINE': False,
              'FREEZE_AT': 0,
              'NORM': 'SyncBN',
              'RESNET_NUM_BLOCKS': [3, 4, 23, 3],
              'STRIDE_1X1': False,
              'TF_PAD_MODE': False,
              'WEIGHTS': ''},
 'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0],
                                  [30.0, 30.0, 15.0, 15.0]],
             'IOUS': [0.5, 0.6, 0.7]},
 'DATA': {'ABSOLUTE_COORD': True,
          'BASEDIR': '/home/bigdata/data',
          'CLASS_NAMES': ['BG', 'connector'],
          'NUM_CATEGORY': 1,
          'NUM_WORKERS': 10,
          'TRAIN': ('coco_train',),
          'VAL': ('coco_val',)},
 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),
         'CASCADE': False,
         'FRCNN_CONV_HEAD_DIM': 256,
         'FRCNN_FC_HEAD_DIM': 1024,
         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',
         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',
         'NORM': 'None',
         'NUM_CHANNEL': 256,
         'PROPOSAL_MODE': 'Level',
         'RESOLUTION_REQUIREMENT': 32},
 'FRCNN': {'BATCH_PER_IM': 512,
           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],
           'FG_RATIO': 0.25,
           'FG_THRESH': 0.5},
 'MODE_FPN': False,
 'MODE_MASK': True,
 'MRCNN': {'HEAD_DIM': 256},
 'PREPROC': {'MAX_SIZE': 1333,
             'PIXEL_MEAN': [123.675, 116.28, 103.53],
             'PIXEL_STD': [58.395, 57.12, 57.375],
             'TEST_SHORT_EDGE_SIZE': 800,
             'TRAIN_SHORT_EDGE_SIZE': [800, 800]},
 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),
         'ANCHOR_SIZES': (32, 64, 128, 256, 512),
         'ANCHOR_STRIDE': 16,
         'BATCH_PER_IM': 256,
         'CROWD_OVERLAP_THRESH': 9.99,
         'FG_RATIO': 0.5,
         'HEAD_DIM': 1024,
         'MIN_SIZE': 0,
         'NEGATIVE_ANCHOR_THRESH': 0.3,
         'NUM_ANCHOR': 15,
         'POSITIVE_ANCHOR_THRESH': 0.7,
         'PROPOSAL_NMS_THRESH': 0.7,
         'TEST_PER_LEVEL_NMS_TOPK': 1000,
         'TEST_POST_NMS_TOPK': 1000,
         'TEST_PRE_NMS_TOPK': 6000,
         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,
         'TRAIN_POST_NMS_TOPK': 2000,
         'TRAIN_PRE_NMS_TOPK': 12000},
 'TEST': {'FRCNN_NMS_THRESH': 0.5,
          'RESULTS_PER_IM': 100,
          'RESULT_SCORE_THRESH': 0.05,
          'RESULT_SCORE_THRESH_VIS': 0.5},
 'TRAIN': {'BASE_LR': 0.01,
           'CHECKPOINT_PERIOD': 20,
           'EVAL_PERIOD': 25,
           'LR_SCHEDULE': [120000, 160000, 180000],
           'NUM_GPUS': 4,
           'STARTING_EPOCH': 1,
           'STEPS_PER_EPOCH': 50,
           'WARMUP': 1000,
           'WARMUP_INIT_LR': 0.0033000000000000004,
           'WEIGHT_DECAY': 0.0001},
 'TRAINER': 'replicated'}
[0808 20:57:11 @train.py:73] Warm Up Schedule (steps, value): [(0, 0.0033000000000000004), (1000, 0.01)]
[0808 20:57:11 @train.py:74] LR Schedule (epochs, value): [(20, 0.01), (4800.0, 0.001), (6400.0, 0.00010000000000000002)]
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
[0808 20:57:11 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_train.json.
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 247/247 [00:00<00:00, 14843.37it/s]
[0808 20:57:11 @timer.py:50] Load annotations for instances_train.json finished, time:0.0256 sec.
[0808 20:57:11 @data.py:60] Ground-Truth category distribution:
|   class   | #box   |  class  |  #box  |  class  |  #box  |
|:---------:|:-------|:-------:|:------:|:-------:|:------:|
| connector | 445    |         |        |         |        |
|   total   | 445    |         |        |         |        |
[0808 20:57:11 @data.py:347] Filtered 0 images which contain no non-crowd groudtruth boxes. Total #images for training: 247
[0808 20:57:11 @train.py:78] Total passes of the training set is: 5830
[0808 20:57:11 @prof.py:50] WRN [GPUUtilizationTracker] Both devices and CUDA_VISIBLE_DEVICES are None! Will monitor all 4 visible GPUs!
[0808 20:57:13 @training.py:50] [DataParallel] Training a model of 4 towers.
[0808 20:57:13 @interface.py:43] Automatically applying StagingInput on the DataFlow.
[0808 20:57:13 @input_source.py:223] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0808 20:57:13 @training.py:110] Building graph for training tower 0 on device /gpu:0 ...
[0808 20:57:14 @registry.py:90] 'conv0': [1, 3, ?, ?] --> [1, 64, ?, ?]
[0808 20:57:14 @registry.py:90] 'pool0': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0808 20:57:14 @registry.py:90] 'group0/block0/conv1': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0808 20:57:14 @registry.py:90] 'group0/block0/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0808 20:57:14 @registry.py:90] 'group0/block0/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:14 @registry.py:90] 'group0/block0/convshortcut': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:14 @registry.py:90] 'group0/block1/conv1': [1, 256, ?, ?] --> [1, 64, ?, ?]
[0808 20:57:14 @registry.py:90] 'group0/block1/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0808 20:57:14 @registry.py:90] 'group0/block1/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:14 @registry.py:90] 'group0/block2/conv1': [1, 256, ?, ?] --> [1, 64, ?, ?]
[0808 20:57:14 @registry.py:90] 'group0/block2/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0808 20:57:14 @registry.py:90] 'group0/block2/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:14 @registry.py:90] 'group1/block0/conv1': [1, 256, ?, ?] --> [1, 128, ?, ?]
[0808 20:57:14 @registry.py:90] 'group1/block0/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0808 20:57:14 @registry.py:90] 'group1/block0/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0808 20:57:15 @registry.py:90] 'group1/block0/convshortcut': [1, 256, ?, ?] --> [1, 512, ?, ?]
[0808 20:57:15 @registry.py:90] 'group1/block1/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
[0808 20:57:15 @registry.py:90] 'group1/block1/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0808 20:57:15 @registry.py:90] 'group1/block1/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0808 20:57:15 @registry.py:90] 'group1/block2/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
[0808 20:57:15 @registry.py:90] 'group1/block2/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0808 20:57:15 @registry.py:90] 'group1/block2/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0808 20:57:15 @registry.py:90] 'group1/block3/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
[0808 20:57:15 @registry.py:90] 'group1/block3/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0808 20:57:15 @registry.py:90] 'group1/block3/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0808 20:57:15 @registry.py:90] 'group2/block0/conv1': [1, 512, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:15 @registry.py:90] 'group2/block0/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:15 @registry.py:90] 'group2/block0/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:15 @registry.py:90] 'group2/block0/convshortcut': [1, 512, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:15 @registry.py:90] 'group2/block1/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:15 @registry.py:90] 'group2/block1/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:15 @registry.py:90] 'group2/block1/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:15 @registry.py:90] 'group2/block2/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:15 @registry.py:90] 'group2/block2/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block2/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block3/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block3/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block3/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block4/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block4/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block4/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block5/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block5/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block5/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block6/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block6/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block6/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block7/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block7/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block7/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block8/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block8/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:16 @registry.py:90] 'group2/block8/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block9/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block9/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block9/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block10/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block10/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block10/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block11/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block11/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block11/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block12/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block12/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block12/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block13/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block13/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block13/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block14/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block14/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block14/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block15/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:17 @registry.py:90] 'group2/block15/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block15/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block16/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block16/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block16/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block17/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block17/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block17/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block18/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block18/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block18/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block19/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block19/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block19/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block20/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block20/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block20/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block21/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block21/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block21/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block22/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:18 @registry.py:90] 'group2/block22/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0808 20:57:19 @registry.py:90] 'group2/block22/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:19 @registry.py:80] 'rpn' input: [1, 1024, ?, ?]
[0808 20:57:19 @registry.py:90]   'rpn/conv0': [1, 1024, ?, ?] --> [1, 1024, ?, ?]
[0808 20:57:19 @registry.py:90]   'rpn/class': [1, 1024, ?, ?] --> [1, 15, ?, ?]
[0808 20:57:19 @registry.py:90]   'rpn/box': [1, 1024, ?, ?] --> [1, 60, ?, ?]
[0808 20:57:19 @registry.py:93] 'rpn' output: [?, ?, 15], [?, ?, 15, 4]
[0808 20:57:19 @registry.py:90] 'group3/block0/conv1': [?, 1024, 14, 14] --> [?, 512, 14, 14]
[0808 20:57:19 @registry.py:90] 'group3/block0/conv2': [?, 512, 15, 15] --> [?, 512, 7, 7]
[0808 20:57:20 @registry.py:90] 'group3/block0/conv3': [?, 512, 7, 7] --> [?, 2048, 7, 7]
[0808 20:57:20 @registry.py:90] 'group3/block0/convshortcut': [?, 1024, 13, 13] --> [?, 2048, 7, 7]
[0808 20:57:20 @registry.py:90] 'group3/block1/conv1': [?, 2048, 7, 7] --> [?, 512, 7, 7]
[0808 20:57:20 @registry.py:90] 'group3/block1/conv2': [?, 512, 7, 7] --> [?, 512, 7, 7]
[0808 20:57:20 @registry.py:90] 'group3/block1/conv3': [?, 512, 7, 7] --> [?, 2048, 7, 7]
[0808 20:57:20 @registry.py:90] 'group3/block2/conv1': [?, 2048, 7, 7] --> [?, 512, 7, 7]
[0808 20:57:20 @registry.py:90] 'group3/block2/conv2': [?, 512, 7, 7] --> [?, 512, 7, 7]
[0808 20:57:20 @registry.py:90] 'group3/block2/conv3': [?, 512, 7, 7] --> [?, 2048, 7, 7]
[0808 20:57:20 @registry.py:90] 'gap': [?, 2048, 7, 7] --> [?, 2048]
[0808 20:57:20 @registry.py:80] 'fastrcnn' input: [?, 2048]
[0808 20:57:20 @registry.py:90]   'fastrcnn/class': [?, 2048] --> [?, 2]
[0808 20:57:20 @registry.py:90]   'fastrcnn/box': [?, 2048] --> [?, 8]
[0808 20:57:20 @registry.py:93] 'fastrcnn' output: [?, 2], [?, 2, 4]
[0808 20:57:20 @registry.py:80] 'maskrcnn' input: [?, 2048, 7, 7]
[0808 20:57:20 @registry.py:90]   'maskrcnn/deconv': [?, 2048, 7, 7] --> [?, 256, 14, 14]
[0808 20:57:20 @registry.py:90]   'maskrcnn/conv': [?, 256, 14, 14] --> [?, 1, 14, 14]
[0808 20:57:20 @registry.py:93] 'maskrcnn' output: [?, 1, 14, 14]
[0808 20:57:21 @regularize.py:97] regularize_cost() found 111 variables to regularize.
[0808 20:57:21 @regularize.py:21] The following tensors will be regularized: conv0/W:0, group0/block0/conv1/W:0, group0/block0/conv2/W:0, group0/block0/conv3/W:0, group0/block0/convshortcut/W:0, group0/block1/conv1/W:0, group0/block1/conv2/W:0, group0/block1/conv3/W:0, group0/block2/conv1/W:0, group0/block2/conv2/W:0, group0/block2/conv3/W:0, group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/conv2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group2/block5/conv3/W:0, group2/block6/conv1/W:0, group2/block6/conv2/W:0, group2/block6/conv3/W:0, group2/block7/conv1/W:0, group2/block7/conv2/W:0, group2/block7/conv3/W:0, group2/block8/conv1/W:0, group2/block8/conv2/W:0, group2/block8/conv3/W:0, group2/block9/conv1/W:0, group2/block9/conv2/W:0, group2/block9/conv3/W:0, group2/block10/conv1/W:0, group2/block10/conv2/W:0, group2/block10/conv3/W:0, group2/block11/conv1/W:0, group2/block11/conv2/W:0, group2/block11/conv3/W:0, group2/block12/conv1/W:0, group2/block12/conv2/W:0, group2/block12/conv3/W:0, group2/block13/conv1/W:0, group2/block13/conv2/W:0, group2/block13/conv3/W:0, group2/block14/conv1/W:0, group2/block14/conv2/W:0, group2/block14/conv3/W:0, group2/block15/conv1/W:0, group2/block15/conv2/W:0, group2/block15/conv3/W:0, group2/block16/conv1/W:0, group2/block16/conv2/W:0, group2/block16/conv3/W:0, group2/block17/conv1/W:0, group2/block17/conv2/W:0, group2/block17/conv3/W:0, group2/block18/conv1/W:0, group2/block18/conv2/W:0, group2/block18/conv3/W:0, group2/block19/conv1/W:0, group2/block19/conv2/W:0, group2/block19/conv3/W:0, group2/block20/conv1/W:0, group2/block20/conv2/W:0, group2/block20/conv3/W:0, group2/block21/conv1/W:0, group2/block21/conv2/W:0, group2/block21/conv3/W:0, group2/block22/conv1/W:0, group2/block22/conv2/W:0, group2/block22/conv3/W:0, rpn/conv0/W:0, rpn/class/W:0, rpn/box/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, fastrcnn/class/W:0, fastrcnn/box/W:0, maskrcnn/deconv/W:0, maskrcnn/conv/W:0
/home/bigdata/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0808 20:57:32 @training.py:110] Building graph for training tower 1 on device /gpu:1 ...
[0808 20:57:38 @regularize.py:97] regularize_cost() found 111 variables to regularize.
/home/bigdata/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0808 20:57:48 @collection.py:165] These collections were modified but restored in tower1: (tf.GraphKeys.SUMMARIES: 31->32)
[0808 20:57:48 @training.py:110] Building graph for training tower 2 on device /gpu:2 ...
[0808 20:57:54 @regularize.py:97] regularize_cost() found 111 variables to regularize.
/home/bigdata/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0808 20:58:05 @collection.py:165] These collections were modified but restored in tower2: (tf.GraphKeys.SUMMARIES: 31->32)
[0808 20:58:05 @training.py:110] Building graph for training tower 3 on device /gpu:3 ...
[0808 20:58:10 @regularize.py:97] regularize_cost() found 111 variables to regularize.
/home/bigdata/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0808 20:58:21 @collection.py:165] These collections were modified but restored in tower3: (tf.GraphKeys.SUMMARIES: 31->32)
[0808 20:58:46 @training.py:351] 'sync_variables_from_main_tower' includes 3558 operations.
[0808 20:58:46 @model_utils.py:67] List of Trainable Variables: 
name                                 shape                 #elements
-----------------------------------  ------------------  -----------
conv0/W                              [7, 7, 3, 64]              9408
conv0/bn/beta                        [64]                         64
conv0/bn/gamma                       [64]                         64
group0/block0/conv1/W                [1, 1, 64, 64]             4096
group0/block0/conv1/bn/beta          [64]                         64
group0/block0/conv1/bn/gamma         [64]                         64
group0/block0/conv2/W                [3, 3, 64, 64]            36864
group0/block0/conv2/bn/beta          [64]                         64
group0/block0/conv2/bn/gamma         [64]                         64
group0/block0/conv3/W                [1, 1, 64, 256]           16384
group0/block0/conv3/bn/beta          [256]                       256
group0/block0/conv3/bn/gamma         [256]                       256
group0/block0/convshortcut/W         [1, 1, 64, 256]           16384
group0/block0/convshortcut/bn/beta   [256]                       256
group0/block0/convshortcut/bn/gamma  [256]                       256
group0/block1/conv1/W                [1, 1, 256, 64]           16384
group0/block1/conv1/bn/beta          [64]                         64
group0/block1/conv1/bn/gamma         [64]                         64
group0/block1/conv2/W                [3, 3, 64, 64]            36864
group0/block1/conv2/bn/beta          [64]                         64
group0/block1/conv2/bn/gamma         [64]                         64
group0/block1/conv3/W                [1, 1, 64, 256]           16384
group0/block1/conv3/bn/beta          [256]                       256
group0/block1/conv3/bn/gamma         [256]                       256
group0/block2/conv1/W                [1, 1, 256, 64]           16384
group0/block2/conv1/bn/beta          [64]                         64
group0/block2/conv1/bn/gamma         [64]                         64
group0/block2/conv2/W                [3, 3, 64, 64]            36864
group0/block2/conv2/bn/beta          [64]                         64
group0/block2/conv2/bn/gamma         [64]                         64
group0/block2/conv3/W                [1, 1, 64, 256]           16384
group0/block2/conv3/bn/beta          [256]                       256
group0/block2/conv3/bn/gamma         [256]                       256
group1/block0/conv1/W                [1, 1, 256, 128]          32768
group1/block0/conv1/bn/beta          [128]                       128
group1/block0/conv1/bn/gamma         [128]                       128
group1/block0/conv2/W                [3, 3, 128, 128]         147456
group1/block0/conv2/bn/beta          [128]                       128
group1/block0/conv2/bn/gamma         [128]                       128
group1/block0/conv3/W                [1, 1, 128, 512]          65536
group1/block0/conv3/bn/beta          [512]                       512
group1/block0/conv3/bn/gamma         [512]                       512
group1/block0/convshortcut/W         [1, 1, 256, 512]         131072
group1/block0/convshortcut/bn/beta   [512]                       512
group1/block0/convshortcut/bn/gamma  [512]                       512
group1/block1/conv1/W                [1, 1, 512, 128]          65536
group1/block1/conv1/bn/beta          [128]                       128
group1/block1/conv1/bn/gamma         [128]                       128
group1/block1/conv2/W                [3, 3, 128, 128]         147456
group1/block1/conv2/bn/beta          [128]                       128
group1/block1/conv2/bn/gamma         [128]                       128
group1/block1/conv3/W                [1, 1, 128, 512]          65536
group1/block1/conv3/bn/beta          [512]                       512
group1/block1/conv3/bn/gamma         [512]                       512
group1/block2/conv1/W                [1, 1, 512, 128]          65536
group1/block2/conv1/bn/beta          [128]                       128
group1/block2/conv1/bn/gamma         [128]                       128
group1/block2/conv2/W                [3, 3, 128, 128]         147456
group1/block2/conv2/bn/beta          [128]                       128
group1/block2/conv2/bn/gamma         [128]                       128
group1/block2/conv3/W                [1, 1, 128, 512]          65536
group1/block2/conv3/bn/beta          [512]                       512
group1/block2/conv3/bn/gamma         [512]                       512
group1/block3/conv1/W                [1, 1, 512, 128]          65536
group1/block3/conv1/bn/beta          [128]                       128
group1/block3/conv1/bn/gamma         [128]                       128
group1/block3/conv2/W                [3, 3, 128, 128]         147456
group1/block3/conv2/bn/beta          [128]                       128
group1/block3/conv2/bn/gamma         [128]                       128
group1/block3/conv3/W                [1, 1, 128, 512]          65536
group1/block3/conv3/bn/beta          [512]                       512
group1/block3/conv3/bn/gamma         [512]                       512
group2/block0/conv1/W                [1, 1, 512, 256]         131072
group2/block0/conv1/bn/beta          [256]                       256
group2/block0/conv1/bn/gamma         [256]                       256
group2/block0/conv2/W                [3, 3, 256, 256]         589824
group2/block0/conv2/bn/beta          [256]                       256
group2/block0/conv2/bn/gamma         [256]                       256
group2/block0/conv3/W                [1, 1, 256, 1024]        262144
group2/block0/conv3/bn/beta          [1024]                     1024
group2/block0/conv3/bn/gamma         [1024]                     1024
group2/block0/convshortcut/W         [1, 1, 512, 1024]        524288
group2/block0/convshortcut/bn/beta   [1024]                     1024
group2/block0/convshortcut/bn/gamma  [1024]                     1024
group2/block1/conv1/W                [1, 1, 1024, 256]        262144
group2/block1/conv1/bn/beta          [256]                       256
group2/block1/conv1/bn/gamma         [256]                       256
group2/block1/conv2/W                [3, 3, 256, 256]         589824
group2/block1/conv2/bn/beta          [256]                       256
group2/block1/conv2/bn/gamma         [256]                       256
group2/block1/conv3/W                [1, 1, 256, 1024]        262144
group2/block1/conv3/bn/beta          [1024]                     1024
group2/block1/conv3/bn/gamma         [1024]                     1024
group2/block2/conv1/W                [1, 1, 1024, 256]        262144
group2/block2/conv1/bn/beta          [256]                       256
group2/block2/conv1/bn/gamma         [256]                       256
group2/block2/conv2/W                [3, 3, 256, 256]         589824
group2/block2/conv2/bn/beta          [256]                       256
group2/block2/conv2/bn/gamma         [256]                       256
group2/block2/conv3/W                [1, 1, 256, 1024]        262144
group2/block2/conv3/bn/beta          [1024]                     1024
group2/block2/conv3/bn/gamma         [1024]                     1024
group2/block3/conv1/W                [1, 1, 1024, 256]        262144
group2/block3/conv1/bn/beta          [256]                       256
group2/block3/conv1/bn/gamma         [256]                       256
group2/block3/conv2/W                [3, 3, 256, 256]         589824
group2/block3/conv2/bn/beta          [256]                       256
group2/block3/conv2/bn/gamma         [256]                       256
group2/block3/conv3/W                [1, 1, 256, 1024]        262144
group2/block3/conv3/bn/beta          [1024]                     1024
group2/block3/conv3/bn/gamma         [1024]                     1024
group2/block4/conv1/W                [1, 1, 1024, 256]        262144
group2/block4/conv1/bn/beta          [256]                       256
group2/block4/conv1/bn/gamma         [256]                       256
group2/block4/conv2/W                [3, 3, 256, 256]         589824
group2/block4/conv2/bn/beta          [256]                       256
group2/block4/conv2/bn/gamma         [256]                       256
group2/block4/conv3/W                [1, 1, 256, 1024]        262144
group2/block4/conv3/bn/beta          [1024]                     1024
group2/block4/conv3/bn/gamma         [1024]                     1024
group2/block5/conv1/W                [1, 1, 1024, 256]        262144
group2/block5/conv1/bn/beta          [256]                       256
group2/block5/conv1/bn/gamma         [256]                       256
group2/block5/conv2/W                [3, 3, 256, 256]         589824
group2/block5/conv2/bn/beta          [256]                       256
group2/block5/conv2/bn/gamma         [256]                       256
group2/block5/conv3/W                [1, 1, 256, 1024]        262144
group2/block5/conv3/bn/beta          [1024]                     1024
group2/block5/conv3/bn/gamma         [1024]                     1024
group2/block6/conv1/W                [1, 1, 1024, 256]        262144
group2/block6/conv1/bn/beta          [256]                       256
group2/block6/conv1/bn/gamma         [256]                       256
group2/block6/conv2/W                [3, 3, 256, 256]         589824
group2/block6/conv2/bn/beta          [256]                       256
group2/block6/conv2/bn/gamma         [256]                       256
group2/block6/conv3/W                [1, 1, 256, 1024]        262144
group2/block6/conv3/bn/beta          [1024]                     1024
group2/block6/conv3/bn/gamma         [1024]                     1024
group2/block7/conv1/W                [1, 1, 1024, 256]        262144
group2/block7/conv1/bn/beta          [256]                       256
group2/block7/conv1/bn/gamma         [256]                       256
group2/block7/conv2/W                [3, 3, 256, 256]         589824
group2/block7/conv2/bn/beta          [256]                       256
group2/block7/conv2/bn/gamma         [256]                       256
group2/block7/conv3/W                [1, 1, 256, 1024]        262144
group2/block7/conv3/bn/beta          [1024]                     1024
group2/block7/conv3/bn/gamma         [1024]                     1024
group2/block8/conv1/W                [1, 1, 1024, 256]        262144
group2/block8/conv1/bn/beta          [256]                       256
group2/block8/conv1/bn/gamma         [256]                       256
group2/block8/conv2/W                [3, 3, 256, 256]         589824
group2/block8/conv2/bn/beta          [256]                       256
group2/block8/conv2/bn/gamma         [256]                       256
group2/block8/conv3/W                [1, 1, 256, 1024]        262144
group2/block8/conv3/bn/beta          [1024]                     1024
group2/block8/conv3/bn/gamma         [1024]                     1024
group2/block9/conv1/W                [1, 1, 1024, 256]        262144
group2/block9/conv1/bn/beta          [256]                       256
group2/block9/conv1/bn/gamma         [256]                       256
group2/block9/conv2/W                [3, 3, 256, 256]         589824
group2/block9/conv2/bn/beta          [256]                       256
group2/block9/conv2/bn/gamma         [256]                       256
group2/block9/conv3/W                [1, 1, 256, 1024]        262144
group2/block9/conv3/bn/beta          [1024]                     1024
group2/block9/conv3/bn/gamma         [1024]                     1024
group2/block10/conv1/W               [1, 1, 1024, 256]        262144
group2/block10/conv1/bn/beta         [256]                       256
group2/block10/conv1/bn/gamma        [256]                       256
group2/block10/conv2/W               [3, 3, 256, 256]         589824
group2/block10/conv2/bn/beta         [256]                       256
group2/block10/conv2/bn/gamma        [256]                       256
group2/block10/conv3/W               [1, 1, 256, 1024]        262144
group2/block10/conv3/bn/beta         [1024]                     1024
group2/block10/conv3/bn/gamma        [1024]                     1024
group2/block11/conv1/W               [1, 1, 1024, 256]        262144
group2/block11/conv1/bn/beta         [256]                       256
group2/block11/conv1/bn/gamma        [256]                       256
group2/block11/conv2/W               [3, 3, 256, 256]         589824
group2/block11/conv2/bn/beta         [256]                       256
group2/block11/conv2/bn/gamma        [256]                       256
group2/block11/conv3/W               [1, 1, 256, 1024]        262144
group2/block11/conv3/bn/beta         [1024]                     1024
group2/block11/conv3/bn/gamma        [1024]                     1024
group2/block12/conv1/W               [1, 1, 1024, 256]        262144
group2/block12/conv1/bn/beta         [256]                       256
group2/block12/conv1/bn/gamma        [256]                       256
group2/block12/conv2/W               [3, 3, 256, 256]         589824
group2/block12/conv2/bn/beta         [256]                       256
group2/block12/conv2/bn/gamma        [256]                       256
group2/block12/conv3/W               [1, 1, 256, 1024]        262144
group2/block12/conv3/bn/beta         [1024]                     1024
group2/block12/conv3/bn/gamma        [1024]                     1024
group2/block13/conv1/W               [1, 1, 1024, 256]        262144
group2/block13/conv1/bn/beta         [256]                       256
group2/block13/conv1/bn/gamma        [256]                       256
group2/block13/conv2/W               [3, 3, 256, 256]         589824
group2/block13/conv2/bn/beta         [256]                       256
group2/block13/conv2/bn/gamma        [256]                       256
group2/block13/conv3/W               [1, 1, 256, 1024]        262144
group2/block13/conv3/bn/beta         [1024]                     1024
group2/block13/conv3/bn/gamma        [1024]                     1024
group2/block14/conv1/W               [1, 1, 1024, 256]        262144
group2/block14/conv1/bn/beta         [256]                       256
group2/block14/conv1/bn/gamma        [256]                       256
group2/block14/conv2/W               [3, 3, 256, 256]         589824
group2/block14/conv2/bn/beta         [256]                       256
group2/block14/conv2/bn/gamma        [256]                       256
group2/block14/conv3/W               [1, 1, 256, 1024]        262144
group2/block14/conv3/bn/beta         [1024]                     1024
group2/block14/conv3/bn/gamma        [1024]                     1024
group2/block15/conv1/W               [1, 1, 1024, 256]        262144
group2/block15/conv1/bn/beta         [256]                       256
group2/block15/conv1/bn/gamma        [256]                       256
group2/block15/conv2/W               [3, 3, 256, 256]         589824
group2/block15/conv2/bn/beta         [256]                       256
group2/block15/conv2/bn/gamma        [256]                       256
group2/block15/conv3/W               [1, 1, 256, 1024]        262144
group2/block15/conv3/bn/beta         [1024]                     1024
group2/block15/conv3/bn/gamma        [1024]                     1024
group2/block16/conv1/W               [1, 1, 1024, 256]        262144
group2/block16/conv1/bn/beta         [256]                       256
group2/block16/conv1/bn/gamma        [256]                       256
group2/block16/conv2/W               [3, 3, 256, 256]         589824
group2/block16/conv2/bn/beta         [256]                       256
group2/block16/conv2/bn/gamma        [256]                       256
group2/block16/conv3/W               [1, 1, 256, 1024]        262144
group2/block16/conv3/bn/beta         [1024]                     1024
group2/block16/conv3/bn/gamma        [1024]                     1024
group2/block17/conv1/W               [1, 1, 1024, 256]        262144
group2/block17/conv1/bn/beta         [256]                       256
group2/block17/conv1/bn/gamma        [256]                       256
group2/block17/conv2/W               [3, 3, 256, 256]         589824
group2/block17/conv2/bn/beta         [256]                       256
group2/block17/conv2/bn/gamma        [256]                       256
group2/block17/conv3/W               [1, 1, 256, 1024]        262144
group2/block17/conv3/bn/beta         [1024]                     1024
group2/block17/conv3/bn/gamma        [1024]                     1024
group2/block18/conv1/W               [1, 1, 1024, 256]        262144
group2/block18/conv1/bn/beta         [256]                       256
group2/block18/conv1/bn/gamma        [256]                       256
group2/block18/conv2/W               [3, 3, 256, 256]         589824
group2/block18/conv2/bn/beta         [256]                       256
group2/block18/conv2/bn/gamma        [256]                       256
group2/block18/conv3/W               [1, 1, 256, 1024]        262144
group2/block18/conv3/bn/beta         [1024]                     1024
group2/block18/conv3/bn/gamma        [1024]                     1024
group2/block19/conv1/W               [1, 1, 1024, 256]        262144
group2/block19/conv1/bn/beta         [256]                       256
group2/block19/conv1/bn/gamma        [256]                       256
group2/block19/conv2/W               [3, 3, 256, 256]         589824
group2/block19/conv2/bn/beta         [256]                       256
group2/block19/conv2/bn/gamma        [256]                       256
group2/block19/conv3/W               [1, 1, 256, 1024]        262144
group2/block19/conv3/bn/beta         [1024]                     1024
group2/block19/conv3/bn/gamma        [1024]                     1024
group2/block20/conv1/W               [1, 1, 1024, 256]        262144
group2/block20/conv1/bn/beta         [256]                       256
group2/block20/conv1/bn/gamma        [256]                       256
group2/block20/conv2/W               [3, 3, 256, 256]         589824
group2/block20/conv2/bn/beta         [256]                       256
group2/block20/conv2/bn/gamma        [256]                       256
group2/block20/conv3/W               [1, 1, 256, 1024]        262144
group2/block20/conv3/bn/beta         [1024]                     1024
group2/block20/conv3/bn/gamma        [1024]                     1024
group2/block21/conv1/W               [1, 1, 1024, 256]        262144
group2/block21/conv1/bn/beta         [256]                       256
group2/block21/conv1/bn/gamma        [256]                       256
group2/block21/conv2/W               [3, 3, 256, 256]         589824
group2/block21/conv2/bn/beta         [256]                       256
group2/block21/conv2/bn/gamma        [256]                       256
group2/block21/conv3/W               [1, 1, 256, 1024]        262144
group2/block21/conv3/bn/beta         [1024]                     1024
group2/block21/conv3/bn/gamma        [1024]                     1024
group2/block22/conv1/W               [1, 1, 1024, 256]        262144
group2/block22/conv1/bn/beta         [256]                       256
group2/block22/conv1/bn/gamma        [256]                       256
group2/block22/conv2/W               [3, 3, 256, 256]         589824
group2/block22/conv2/bn/beta         [256]                       256
group2/block22/conv2/bn/gamma        [256]                       256
group2/block22/conv3/W               [1, 1, 256, 1024]        262144
group2/block22/conv3/bn/beta         [1024]                     1024
group2/block22/conv3/bn/gamma        [1024]                     1024
rpn/conv0/W                          [3, 3, 1024, 1024]      9437184
rpn/conv0/b                          [1024]                     1024
rpn/class/W                          [1, 1, 1024, 15]          15360
rpn/class/b                          [15]                         15
rpn/box/W                            [1, 1, 1024, 60]          61440
rpn/box/b                            [60]                         60
group3/block0/conv1/W                [1, 1, 1024, 512]        524288
group3/block0/conv1/bn/beta          [512]                       512
group3/block0/conv1/bn/gamma         [512]                       512
group3/block0/conv2/W                [3, 3, 512, 512]        2359296
group3/block0/conv2/bn/beta          [512]                       512
group3/block0/conv2/bn/gamma         [512]                       512
group3/block0/conv3/W                [1, 1, 512, 2048]       1048576
group3/block0/conv3/bn/beta          [2048]                     2048
group3/block0/conv3/bn/gamma         [2048]                     2048
group3/block0/convshortcut/W         [1, 1, 1024, 2048]      2097152
group3/block0/convshortcut/bn/beta   [2048]                     2048
group3/block0/convshortcut/bn/gamma  [2048]                     2048
group3/block1/conv1/W                [1, 1, 2048, 512]       1048576
group3/block1/conv1/bn/beta          [512]                       512
group3/block1/conv1/bn/gamma         [512]                       512
group3/block1/conv2/W                [3, 3, 512, 512]        2359296
group3/block1/conv2/bn/beta          [512]                       512
group3/block1/conv2/bn/gamma         [512]                       512
group3/block1/conv3/W                [1, 1, 512, 2048]       1048576
group3/block1/conv3/bn/beta          [2048]                     2048
group3/block1/conv3/bn/gamma         [2048]                     2048
group3/block2/conv1/W                [1, 1, 2048, 512]       1048576
group3/block2/conv1/bn/beta          [512]                       512
group3/block2/conv1/bn/gamma         [512]                       512
group3/block2/conv2/W                [3, 3, 512, 512]        2359296
group3/block2/conv2/bn/beta          [512]                       512
group3/block2/conv2/bn/gamma         [512]                       512
group3/block2/conv3/W                [1, 1, 512, 2048]       1048576
group3/block2/conv3/bn/beta          [2048]                     2048
group3/block2/conv3/bn/gamma         [2048]                     2048
fastrcnn/class/W                     [2048, 2]                  4096
fastrcnn/class/b                     [2]                           2
fastrcnn/box/W                       [2048, 8]                 16384
fastrcnn/box/b                       [8]                           8
maskrcnn/deconv/W                    [2, 2, 256, 2048]       2097152
maskrcnn/deconv/b                    [256]                       256
maskrcnn/conv/W                      [1, 1, 256, 1]              256
maskrcnn/conv/b                      [1]                           1
Number of trainable variables: 326
Number of parameters (elements): 54133398
Storage space needed for all trainable variables: 206.50MB
[0808 20:58:46 @base.py:209] Setup callbacks graph ...
[0808 20:58:50 @prof.py:271] [HostMemoryTracker] Free RAM in setup_graph() is 208.71 GB.
[0808 20:58:50 @tower.py:140] Building graph for predict tower 'tower-pred-0' on device /gpu:0 ...
[0808 20:58:52 @tower.py:140] Building graph for predict tower 'tower-pred-1' on device /gpu:1 with variable scope 'tower1'...
[0808 20:58:54 @tower.py:140] Building graph for predict tower 'tower-pred-2' on device /gpu:2 with variable scope 'tower2'...
[0808 20:58:56 @tower.py:140] Building graph for predict tower 'tower-pred-3' on device /gpu:3 with variable scope 'tower3'...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0808 20:58:57 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 166346.33it/s]
[0808 20:58:57 @timer.py:50] Load annotations for instances_val.json finished, time:0.0008 sec.
[0808 20:58:57 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0808 20:58:57 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 168252.88it/s]
[0808 20:58:57 @timer.py:50] Load annotations for instances_val.json finished, time:0.0006 sec.
[0808 20:58:57 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0808 20:58:57 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 179025.17it/s]
[0808 20:58:57 @timer.py:50] Load annotations for instances_val.json finished, time:0.0005 sec.
[0808 20:58:57 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0808 20:58:57 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 173216.09it/s]
[0808 20:58:57 @timer.py:50] Load annotations for instances_val.json finished, time:0.0005 sec.
[0808 20:58:57 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0808 20:58:57 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 186709.88it/s]
[0808 20:58:57 @timer.py:50] Load annotations for instances_val.json finished, time:0.0005 sec.
[0808 20:58:57 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0808 20:58:57 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 164252.46it/s]
[0808 20:58:57 @timer.py:50] Load annotations for instances_val.json finished, time:0.0005 sec.
[0808 20:58:57 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0808 20:58:57 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 196717.78it/s]
[0808 20:58:57 @timer.py:50] Load annotations for instances_val.json finished, time:0.0004 sec.
[0808 20:58:57 @data.py:375] Found 28 images for inference.
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
[0808 20:58:57 @coco.py:61] Instances loaded from /home/bigdata/data/annotations/instances_val.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:00<00:00, 187007.18it/s]
[0808 20:58:57 @timer.py:50] Load annotations for instances_val.json finished, time:0.0004 sec.
[0808 20:58:57 @data.py:375] Found 28 images for inference.
[0808 20:58:57 @summary.py:47] [MovingAverageSummary] 28 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.
[0808 20:58:57 @summary.py:94] Summarizing collection 'summaries' of size 31.
[0808 20:58:57 @base.py:230] Creating the session ...
2019-08-08 20:58:57.827215: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-08 20:58:57.836899: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-08-08 20:58:59.035853: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5613edc4c170 executing computations on platform CUDA. Devices:
2019-08-08 20:58:59.035912: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5
2019-08-08 20:58:59.035927: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): TITAN RTX, Compute Capability 7.5
2019-08-08 20:58:59.035940: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): TITAN RTX, Compute Capability 7.5
2019-08-08 20:58:59.035953: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): TITAN RTX, Compute Capability 7.5
2019-08-08 20:58:59.040846: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300010000 Hz
2019-08-08 20:58:59.050286: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5613ede12fe0 executing computations on platform Host. Devices:
2019-08-08 20:58:59.050349: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-08 20:58:59.052859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:02:00.0
2019-08-08 20:58:59.054809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:03:00.0
2019-08-08 20:58:59.056709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:82:00.0
2019-08-08 20:58:59.058583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77
pciBusID: 0000:83:00.0
2019-08-08 20:58:59.058906: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-08 20:58:59.060816: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-08 20:58:59.062593: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-08 20:58:59.062928: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-08 20:58:59.065187: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-08 20:58:59.066896: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-08 20:58:59.072073: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-08 20:58:59.086819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3
2019-08-08 20:58:59.086886: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-08 20:58:59.094946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-08 20:58:59.094977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 
2019-08-08 20:58:59.095009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N N N N 
2019-08-08 20:58:59.095022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N N N N 
2019-08-08 20:58:59.095034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   N N N N 
2019-08-08 20:58:59.095045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   N N N N 
2019-08-08 20:58:59.104470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 23978 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:02:00.0, compute capability: 7.5)
2019-08-08 20:58:59.107136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 23978 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:03:00.0, compute capability: 7.5)
2019-08-08 20:58:59.109762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 23978 MB memory) -> physical GPU (device: 2, name: TITAN RTX, pci bus id: 0000:82:00.0, compute capability: 7.5)
2019-08-08 20:58:59.112757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 23978 MB memory) -> physical GPU (device: 3, name: TITAN RTX, pci bus id: 0000:83:00.0, compute capability: 7.5)
2019-08-08 20:59:07.720312: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
Corrupt JPEG data: premature end of data segment
[0808 20:59:32 @base.py:236] Initializing the session ...
[0808 20:59:32 @sessinit.py:223] Variables to restore from dict: conv0/W, conv0/bn/beta, conv0/bn/gamma, conv0/bn/mean/EMA, conv0/bn/variance/EMA, group0/block0/conv1/W, group0/block0/conv1/bn/beta, group0/block0/conv1/bn/gamma, group0/block0/conv1/bn/mean/EMA, group0/block0/conv1/bn/variance/EMA, group0/block0/conv2/W, group0/block0/conv2/bn/beta, group0/block0/conv2/bn/gamma, group0/block0/conv2/bn/mean/EMA, group0/block0/conv2/bn/variance/EMA, group0/block0/conv3/W, group0/block0/conv3/bn/beta, group0/block0/conv3/bn/gamma, group0/block0/conv3/bn/mean/EMA, group0/block0/conv3/bn/variance/EMA, group0/block0/convshortcut/W, group0/block0/convshortcut/bn/beta, group0/block0/convshortcut/bn/gamma, group0/block0/convshortcut/bn/mean/EMA, group0/block0/convshortcut/bn/variance/EMA, group0/block1/conv1/W, group0/block1/conv1/bn/beta, group0/block1/conv1/bn/gamma, group0/block1/conv1/bn/mean/EMA, group0/block1/conv1/bn/variance/EMA, group0/block1/conv2/W, group0/block1/conv2/bn/beta, group0/block1/conv2/bn/gamma, group0/block1/conv2/bn/mean/EMA, group0/block1/conv2/bn/variance/EMA, group0/block1/conv3/W, group0/block1/conv3/bn/beta, group0/block1/conv3/bn/gamma, group0/block1/conv3/bn/mean/EMA, group0/block1/conv3/bn/variance/EMA, group0/block2/conv1/W, group0/block2/conv1/bn/beta, group0/block2/conv1/bn/gamma, group0/block2/conv1/bn/mean/EMA, group0/block2/conv1/bn/variance/EMA, group0/block2/conv2/W, group0/block2/conv2/bn/beta, group0/block2/conv2/bn/gamma, group0/block2/conv2/bn/mean/EMA, group0/block2/conv2/bn/variance/EMA, group0/block2/conv3/W, group0/block2/conv3/bn/beta, group0/block2/conv3/bn/gamma, group0/block2/conv3/bn/mean/EMA, group0/block2/conv3/bn/variance/EMA, group1/block0/conv1/W, group1/block0/conv1/bn/beta, group1/block0/conv1/bn/gamma, group1/block0/conv1/bn/mean/EMA, group1/block0/conv1/bn/variance/EMA, group1/block0/conv2/W, group1/block0/conv2/bn/beta, group1/block0/conv2/bn/gamma, group1/block0/conv2/bn/mean/EMA, group1/block0/conv2/bn/variance/EMA, group1/block0/conv3/W, group1/block0/conv3/bn/beta, group1/block0/conv3/bn/gamma, group1/block0/conv3/bn/mean/EMA, group1/block0/conv3/bn/variance/EMA, group1/block0/convshortcut/W, group1/block0/convshortcut/bn/beta, group1/block0/convshortcut/bn/gamma, group1/block0/convshortcut/bn/mean/EMA, group1/block0/convshortcut/bn/variance/EMA, group1/block1/conv1/W, group1/block1/conv1/bn/beta, group1/block1/conv1/bn/gamma, group1/block1/conv1/bn/mean/EMA, group1/block1/conv1/bn/variance/EMA, group1/block1/conv2/W, group1/block1/conv2/bn/beta, group1/block1/conv2/bn/gamma, group1/block1/conv2/bn/mean/EMA, group1/block1/conv2/bn/variance/EMA, group1/block1/conv3/W, group1/block1/conv3/bn/beta, group1/block1/conv3/bn/gamma, group1/block1/conv3/bn/mean/EMA, group1/block1/conv3/bn/variance/EMA, group1/block2/conv1/W, group1/block2/conv1/bn/beta, group1/block2/conv1/bn/gamma, group1/block2/conv1/bn/mean/EMA, group1/block2/conv1/bn/variance/EMA, group1/block2/conv2/W, group1/block2/conv2/bn/beta, group1/block2/conv2/bn/gamma, group1/block2/conv2/bn/mean/EMA, group1/block2/conv2/bn/variance/EMA, group1/block2/conv3/W, group1/block2/conv3/bn/beta, group1/block2/conv3/bn/gamma, group1/block2/conv3/bn/mean/EMA, group1/block2/conv3/bn/variance/EMA, group1/block3/conv1/W, group1/block3/conv1/bn/beta, group1/block3/conv1/bn/gamma, group1/block3/conv1/bn/mean/EMA, group1/block3/conv1/bn/variance/EMA, group1/block3/conv2/W, group1/block3/conv2/bn/beta, group1/block3/conv2/bn/gamma, group1/block3/conv2/bn/mean/EMA, group1/block3/conv2/bn/variance/EMA, group1/block3/conv3/W, group1/block3/conv3/bn/beta, group1/block3/conv3/bn/gamma, group1/block3/conv3/bn/mean/EMA, group1/block3/conv3/bn/variance/EMA, group2/block0/conv1/W, group2/block0/conv1/bn/beta, group2/block0/conv1/bn/gamma, group2/block0/conv1/bn/mean/EMA, group2/block0/conv1/bn/variance/EMA, group2/block0/conv2/W, group2/block0/conv2/bn/beta, group2/block0/conv2/bn/gamma, group2/block0/conv2/bn/mean/EMA, group2/block0/conv2/bn/variance/EMA, group2/block0/conv3/W, group2/block0/conv3/bn/beta, group2/block0/conv3/bn/gamma, group2/block0/conv3/bn/mean/EMA, group2/block0/conv3/bn/variance/EMA, group2/block0/convshortcut/W, group2/block0/convshortcut/bn/beta, group2/block0/convshortcut/bn/gamma, group2/block0/convshortcut/bn/mean/EMA, group2/block0/convshortcut/bn/variance/EMA, group2/block1/conv1/W, group2/block1/conv1/bn/beta, group2/block1/conv1/bn/gamma, group2/block1/conv1/bn/mean/EMA, group2/block1/conv1/bn/variance/EMA, group2/block1/conv2/W, group2/block1/conv2/bn/beta, group2/block1/conv2/bn/gamma, group2/block1/conv2/bn/mean/EMA, group2/block1/conv2/bn/variance/EMA, group2/block1/conv3/W, group2/block1/conv3/bn/beta, group2/block1/conv3/bn/gamma, group2/block1/conv3/bn/mean/EMA, group2/block1/conv3/bn/variance/EMA, group2/block2/conv1/W, group2/block2/conv1/bn/beta, group2/block2/conv1/bn/gamma, group2/block2/conv1/bn/mean/EMA, group2/block2/conv1/bn/variance/EMA, group2/block2/conv2/W, group2/block2/conv2/bn/beta, group2/block2/conv2/bn/gamma, group2/block2/conv2/bn/mean/EMA, group2/block2/conv2/bn/variance/EMA, group2/block2/conv3/W, group2/block2/conv3/bn/beta, group2/block2/conv3/bn/gamma, group2/block2/conv3/bn/mean/EMA, group2/block2/conv3/bn/variance/EMA, group2/block3/conv1/W, group2/block3/conv1/bn/beta, group2/block3/conv1/bn/gamma, group2/block3/conv1/bn/mean/EMA, group2/block3/conv1/bn/variance/EMA, group2/block3/conv2/W, group2/block3/conv2/bn/beta, group2/block3/conv2/bn/gamma, group2/block3/conv2/bn/mean/EMA, group2/block3/conv2/bn/variance/EMA, group2/block3/conv3/W, group2/block3/conv3/bn/beta, group2/block3/conv3/bn/gamma, group2/block3/conv3/bn/mean/EMA, group2/block3/conv3/bn/variance/EMA, group2/block4/conv1/W, group2/block4/conv1/bn/beta, group2/block4/conv1/bn/gamma, group2/block4/conv1/bn/mean/EMA, group2/block4/conv1/bn/variance/EMA, group2/block4/conv2/W, group2/block4/conv2/bn/beta, group2/block4/conv2/bn/gamma, group2/block4/conv2/bn/mean/EMA, group2/block4/conv2/bn/variance/EMA, group2/block4/conv3/W, group2/block4/conv3/bn/beta, group2/block4/conv3/bn/gamma, group2/block4/conv3/bn/mean/EMA, group2/block4/conv3/bn/variance/EMA, group2/block5/conv1/W, group2/block5/conv1/bn/beta, group2/block5/conv1/bn/gamma, group2/block5/conv1/bn/mean/EMA, group2/block5/conv1/bn/variance/EMA, group2/block5/conv2/W, group2/block5/conv2/bn/beta, group2/block5/conv2/bn/gamma, group2/block5/conv2/bn/mean/EMA, group2/block5/conv2/bn/variance/EMA, group2/block5/conv3/W, group2/block5/conv3/bn/beta, group2/block5/conv3/bn/gamma, group2/block5/conv3/bn/mean/EMA, group2/block5/conv3/bn/variance/EMA, group2/block6/conv1/W, group2/block6/conv1/bn/beta, group2/block6/conv1/bn/gamma, group2/block6/conv1/bn/mean/EMA, group2/block6/conv1/bn/variance/EMA, group2/block6/conv2/W, group2/block6/conv2/bn/beta, group2/block6/conv2/bn/gamma, group2/block6/conv2/bn/mean/EMA, group2/block6/conv2/bn/variance/EMA, group2/block6/conv3/W, group2/block6/conv3/bn/beta, group2/block6/conv3/bn/gamma, group2/block6/conv3/bn/mean/EMA, group2/block6/conv3/bn/variance/EMA, group2/block7/conv1/W, group2/block7/conv1/bn/beta, group2/block7/conv1/bn/gamma, group2/block7/conv1/bn/mean/EMA, group2/block7/conv1/bn/variance/EMA, group2/block7/conv2/W, group2/block7/conv2/bn/beta, group2/block7/conv2/bn/gamma, group2/block7/conv2/bn/mean/EMA, group2/block7/conv2/bn/variance/EMA, group2/block7/conv3/W, group2/block7/conv3/bn/beta, group2/block7/conv3/bn/gamma, group2/block7/conv3/bn/mean/EMA, group2/block7/conv3/bn/variance/EMA, group2/block8/conv1/W, group2/block8/conv1/bn/beta, group2/block8/conv1/bn/gamma, group2/block8/conv1/bn/mean/EMA, group2/block8/conv1/bn/variance/EMA, group2/block8/conv2/W, group2/block8/conv2/bn/beta, group2/block8/conv2/bn/gamma, group2/block8/conv2/bn/mean/EMA, group2/block8/conv2/bn/variance/EMA, group2/block8/conv3/W, group2/block8/conv3/bn/beta, group2/block8/conv3/bn/gamma, group2/block8/conv3/bn/mean/EMA, group2/block8/conv3/bn/variance/EMA, group2/block9/conv1/W, group2/block9/conv1/bn/beta, group2/block9/conv1/bn/gamma, group2/block9/conv1/bn/mean/EMA, group2/block9/conv1/bn/variance/EMA, group2/block9/conv2/W, group2/block9/conv2/bn/beta, group2/block9/conv2/bn/gamma, group2/block9/conv2/bn/mean/EMA, group2/block9/conv2/bn/variance/EMA, group2/block9/conv3/W, group2/block9/conv3/bn/beta, group2/block9/conv3/bn/gamma, group2/block9/conv3/bn/mean/EMA, group2/block9/conv3/bn/variance/EMA, group2/block10/conv1/W, group2/block10/conv1/bn/beta, group2/block10/conv1/bn/gamma, group2/block10/conv1/bn/mean/EMA, group2/block10/conv1/bn/variance/EMA, group2/block10/conv2/W, group2/block10/conv2/bn/beta, group2/block10/conv2/bn/gamma, group2/block10/conv2/bn/mean/EMA, group2/block10/conv2/bn/variance/EMA, group2/block10/conv3/W, group2/block10/conv3/bn/beta, group2/block10/conv3/bn/gamma, group2/block10/conv3/bn/mean/EMA, group2/block10/conv3/bn/variance/EMA, group2/block11/conv1/W, group2/block11/conv1/bn/beta, group2/block11/conv1/bn/gamma, group2/block11/conv1/bn/mean/EMA, group2/block11/conv1/bn/variance/EMA, group2/block11/conv2/W, group2/block11/conv2/bn/beta, group2/block11/conv2/bn/gamma, group2/block11/conv2/bn/mean/EMA, group2/block11/conv2/bn/variance/EMA, group2/block11/conv3/W, group2/block11/conv3/bn/beta, group2/block11/conv3/bn/gamma, group2/block11/conv3/bn/mean/EMA, group2/block11/conv3/bn/variance/EMA, group2/block12/conv1/W, group2/block12/conv1/bn/beta, group2/block12/conv1/bn/gamma, group2/block12/conv1/bn/mean/EMA, group2/block12/conv1/bn/variance/EMA, group2/block12/conv2/W, group2/block12/conv2/bn/beta, group2/block12/conv2/bn/gamma, group2/block12/conv2/bn/mean/EMA, group2/block12/conv2/bn/variance/EMA, group2/block12/conv3/W, group2/block12/conv3/bn/beta, group2/block12/conv3/bn/gamma, group2/block12/conv3/bn/mean/EMA, group2/block12/conv3/bn/variance/EMA, group2/block13/conv1/W, group2/block13/conv1/bn/beta, group2/block13/conv1/bn/gamma, group2/block13/conv1/bn/mean/EMA, group2/block13/conv1/bn/variance/EMA, group2/block13/conv2/W, group2/block13/conv2/bn/beta, group2/block13/conv2/bn/gamma, group2/block13/conv2/bn/mean/EMA, group2/block13/conv2/bn/variance/EMA, group2/block13/conv3/W, group2/block13/conv3/bn/beta, group2/block13/conv3/bn/gamma, group2/block13/conv3/bn/mean/EMA, group2/block13/conv3/bn/variance/EMA, group2/block14/conv1/W, group2/block14/conv1/bn/beta, group2/block14/conv1/bn/gamma, group2/block14/conv1/bn/mean/EMA, group2/block14/conv1/bn/variance/EMA, group2/block14/conv2/W, group2/block14/conv2/bn/beta, group2/block14/conv2/bn/gamma, group2/block14/conv2/bn/mean/EMA, group2/block14/conv2/bn/variance/EMA, group2/block14/conv3/W, group2/block14/conv3/bn/beta, group2/block14/conv3/bn/gamma, group2/block14/conv3/bn/mean/EMA, group2/block14/conv3/bn/variance/EMA, group2/block15/conv1/W, group2/block15/conv1/bn/beta, group2/block15/conv1/bn/gamma, group2/block15/conv1/bn/mean/EMA, group2/block15/conv1/bn/variance/EMA, group2/block15/conv2/W, group2/block15/conv2/bn/beta, group2/block15/conv2/bn/gamma, group2/block15/conv2/bn/mean/EMA, group2/block15/conv2/bn/variance/EMA, group2/block15/conv3/W, group2/block15/conv3/bn/beta, group2/block15/conv3/bn/gamma, group2/block15/conv3/bn/mean/EMA, group2/block15/conv3/bn/variance/EMA, group2/block16/conv1/W, group2/block16/conv1/bn/beta, group2/block16/conv1/bn/gamma, group2/block16/conv1/bn/mean/EMA, group2/block16/conv1/bn/variance/EMA, group2/block16/conv2/W, group2/block16/conv2/bn/beta, group2/block16/conv2/bn/gamma, group2/block16/conv2/bn/mean/EMA, group2/block16/conv2/bn/variance/EMA, group2/block16/conv3/W, group2/block16/conv3/bn/beta, group2/block16/conv3/bn/gamma, group2/block16/conv3/bn/mean/EMA, group2/block16/conv3/bn/variance/EMA, group2/block17/conv1/W, group2/block17/conv1/bn/beta, group2/block17/conv1/bn/gamma, group2/block17/conv1/bn/mean/EMA, group2/block17/conv1/bn/variance/EMA, group2/block17/conv2/W, group2/block17/conv2/bn/beta, group2/block17/conv2/bn/gamma, group2/block17/conv2/bn/mean/EMA, group2/block17/conv2/bn/variance/EMA, group2/block17/conv3/W, group2/block17/conv3/bn/beta, group2/block17/conv3/bn/gamma, group2/block17/conv3/bn/mean/EMA, group2/block17/conv3/bn/variance/EMA, group2/block18/conv1/W, group2/block18/conv1/bn/beta, group2/block18/conv1/bn/gamma, group2/block18/conv1/bn/mean/EMA, group2/block18/conv1/bn/variance/EMA, group2/block18/conv2/W, group2/block18/conv2/bn/beta, group2/block18/conv2/bn/gamma, group2/block18/conv2/bn/mean/EMA, group2/block18/conv2/bn/variance/EMA, group2/block18/conv3/W, group2/block18/conv3/bn/beta, group2/block18/conv3/bn/gamma, group2/block18/conv3/bn/mean/EMA, group2/block18/conv3/bn/variance/EMA, group2/block19/conv1/W, group2/block19/conv1/bn/beta, group2/block19/conv1/bn/gamma, group2/block19/conv1/bn/mean/EMA, group2/block19/conv1/bn/variance/EMA, group2/block19/conv2/W, group2/block19/conv2/bn/beta, group2/block19/conv2/bn/gamma, group2/block19/conv2/bn/mean/EMA, group2/block19/conv2/bn/variance/EMA, group2/block19/conv3/W, group2/block19/conv3/bn/beta, group2/block19/conv3/bn/gamma, group2/block19/conv3/bn/mean/EMA, group2/block19/conv3/bn/variance/EMA, group2/block20/conv1/W, group2/block20/conv1/bn/beta, group2/block20/conv1/bn/gamma, group2/block20/conv1/bn/mean/EMA, group2/block20/conv1/bn/variance/EMA, group2/block20/conv2/W, group2/block20/conv2/bn/beta, group2/block20/conv2/bn/gamma, group2/block20/conv2/bn/mean/EMA, group2/block20/conv2/bn/variance/EMA, group2/block20/conv3/W, group2/block20/conv3/bn/beta, group2/block20/conv3/bn/gamma, group2/block20/conv3/bn/mean/EMA, group2/block20/conv3/bn/variance/EMA, group2/block21/conv1/W, group2/block21/conv1/bn/beta, group2/block21/conv1/bn/gamma, group2/block21/conv1/bn/mean/EMA, group2/block21/conv1/bn/variance/EMA, group2/block21/conv2/W, group2/block21/conv2/bn/beta, group2/block21/conv2/bn/gamma, group2/block21/conv2/bn/mean/EMA, group2/block21/conv2/bn/variance/EMA, group2/block21/conv3/W, group2/block21/conv3/bn/beta, group2/block21/conv3/bn/gamma, group2/block21/conv3/bn/mean/EMA, group2/block21/conv3/bn/variance/EMA, group2/block22/conv1/W, group2/block22/conv1/bn/beta, group2/block22/conv1/bn/gamma, group2/block22/conv1/bn/mean/EMA, group2/block22/conv1/bn/variance/EMA, group2/block22/conv2/W, group2/block22/conv2/bn/beta, group2/block22/conv2/bn/gamma, group2/block22/conv2/bn/mean/EMA, group2/block22/conv2/bn/variance/EMA, group2/block22/conv3/W, group2/block22/conv3/bn/beta, group2/block22/conv3/bn/gamma, group2/block22/conv3/bn/mean/EMA, group2/block22/conv3/bn/variance/EMA, rpn/conv0/W, rpn/conv0/b, rpn/class/W, rpn/class/b, rpn/box/W, rpn/box/b, group3/block0/conv1/W, group3/block0/conv1/bn/beta, group3/block0/conv1/bn/gamma, group3/block0/conv1/bn/mean/EMA, group3/block0/conv1/bn/variance/EMA, group3/block0/conv2/W, group3/block0/conv2/bn/beta, group3/block0/conv2/bn/gamma, group3/block0/conv2/bn/mean/EMA, group3/block0/conv2/bn/variance/EMA, group3/block0/conv3/W, group3/block0/conv3/bn/beta, group3/block0/conv3/bn/gamma, group3/block0/conv3/bn/mean/EMA, group3/block0/conv3/bn/variance/EMA, group3/block0/convshortcut/W, group3/block0/convshortcut/bn/beta, group3/block0/convshortcut/bn/gamma, group3/block0/convshortcut/bn/mean/EMA, group3/block0/convshortcut/bn/variance/EMA, group3/block1/conv1/W, group3/block1/conv1/bn/beta, group3/block1/conv1/bn/gamma, group3/block1/conv1/bn/mean/EMA, group3/block1/conv1/bn/variance/EMA, group3/block1/conv2/W, group3/block1/conv2/bn/beta, group3/block1/conv2/bn/gamma, group3/block1/conv2/bn/mean/EMA, group3/block1/conv2/bn/variance/EMA, group3/block1/conv3/W, group3/block1/conv3/bn/beta, group3/block1/conv3/bn/gamma, group3/block1/conv3/bn/mean/EMA, group3/block1/conv3/bn/variance/EMA, group3/block2/conv1/W, group3/block2/conv1/bn/beta, group3/block2/conv1/bn/gamma, group3/block2/conv1/bn/mean/EMA, group3/block2/conv1/bn/variance/EMA, group3/block2/conv2/W, group3/block2/conv2/bn/beta, group3/block2/conv2/bn/gamma, group3/block2/conv2/bn/mean/EMA, group3/block2/conv2/bn/variance/EMA, group3/block2/conv3/W, group3/block2/conv3/bn/beta, group3/block2/conv3/bn/gamma, group3/block2/conv3/bn/mean/EMA, group3/block2/conv3/bn/variance/EMA, fastrcnn/class/W, fastrcnn/class/b, fastrcnn/box/W, fastrcnn/box/b, maskrcnn/deconv/W, maskrcnn/deconv/b, maskrcnn/conv/W, maskrcnn/conv/b
[0808 20:59:32 @sessinit.py:87] WRN The following variables are in the graph, but not found in the dict: global_step, learning_rate
[0808 20:59:32 @sessinit.py:236] Restoring 534 variables from dict ...
[0808 20:59:32 @varmanip.py:80] WRN Cannot load a tensor of shape (1, 1, 256, 80) into the variable 'maskrcnn/conv/W' whose shape is (1, 1, 256, 1).
[0808 20:59:32 @varmanip.py:80] WRN Cannot load a tensor of shape (2048, 81) into the variable 'fastrcnn/class/W' whose shape is (2048, 2).
[0808 20:59:32 @varmanip.py:80] WRN Cannot load a tensor of shape (81,) into the variable 'fastrcnn/class/b' whose shape is (2,).
[0808 20:59:32 @varmanip.py:80] WRN Cannot load a tensor of shape (2048, 324) into the variable 'fastrcnn/box/W' whose shape is (2048, 8).
[0808 20:59:32 @varmanip.py:80] WRN Cannot load a tensor of shape (324,) into the variable 'fastrcnn/box/b' whose shape is (8,).
[0808 20:59:32 @varmanip.py:80] WRN Cannot load a tensor of shape (80,) into the variable 'maskrcnn/conv/b' whose shape is (1,).
[0808 20:59:34 @base.py:243] Graph Finalized.
[0808 20:59:36 @concurrency.py:37] Starting EnqueueThread QueueInput/input_queue ...
[0808 20:59:36 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
[0808 20:59:55 @param.py:158] [HyperParamSetter] At global_step=0, learning_rate is set to 0.003300
[0808 20:59:58 @prof.py:274] [HostMemoryTracker] Free RAM in before_train() is 200.80 GB.
[0808 20:59:58 @eval.py:259] [EvalCallback] Will evaluate every 25 epochs
[0808 20:59:59 @base.py:275] Start Epoch 1 ...
  0%|                                                                                                                                                                |0/50[00:00<?,?it/s][0808 20:59:59 @input_source.py:556] Pre-filling StagingArea ...
[0808 21:00:01 @input_source.py:560] 1 element was put into StagingArea on each tower.
2019-08-08 21:03:14.394048: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-08 21:03:18.676962: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
[0808 21:03:29 @param.py:161] [HyperParamSetter] At global_step=1, learning_rate changes from 0.003300 to 0.003307
 40%|############################################################4                                                                                          |20/50[04:01<00:39, 0.76it/s]Corrupt JPEG data: premature end of data segment
100%|#######################################################################################################################################################|50/50[05:56<00:00, 0.03it/s]
[0808 21:05:56 @base.py:285] Epoch 1 (global_step 50) finished, time:5 minutes 56 seconds.
[0808 21:05:56 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
[0808 21:05:56 @misc.py:109] Estimated Time Left: 29 days 19 hours 35 minutes 11 seconds
[0808 21:05:56 @monitor.py:474] GPUUtil/0: 10.655
[0808 21:05:56 @monitor.py:474] GPUUtil/1: 13.257
[0808 21:05:56 @monitor.py:474] GPUUtil/2: 12.082
[0808 21:05:56 @monitor.py:474] GPUUtil/3: 11.828
[0808 21:05:56 @monitor.py:474] HostFreeMemory (GB): 192.6
[0808 21:05:56 @monitor.py:474] PeakMemory(MB)/gpu:0: 10616
[0808 21:05:56 @monitor.py:474] QueueInput/queue_size: 48.624
[0808 21:05:56 @monitor.py:474] Throughput (samples/sec): 0.5607
[0808 21:05:56 @monitor.py:474] fastrcnn_losses/box_loss: 0.0082934
[0808 21:05:56 @monitor.py:474] fastrcnn_losses/label_loss: nan
[0808 21:05:56 @monitor.py:474] fastrcnn_losses/label_metrics/accuracy: 0.98364
[0808 21:05:56 @monitor.py:474] fastrcnn_losses/label_metrics/false_negative: 0.99737
[0808 21:05:56 @monitor.py:474] fastrcnn_losses/label_metrics/fg_accuracy: 0
[0808 21:05:56 @monitor.py:474] fastrcnn_losses/num_fg_label: 2.4354
[0808 21:05:56 @monitor.py:474] learning_rate: 0.0036283
[0808 21:05:56 @monitor.py:474] maskrcnn_loss/accuracy: 0.58662
[0808 21:05:56 @monitor.py:474] maskrcnn_loss/fg_pixel_ratio: 0.41338
[0808 21:05:56 @monitor.py:474] maskrcnn_loss/maskrcnn_loss: 0.69315
[0808 21:05:56 @monitor.py:474] maskrcnn_loss/pos_accuracy: 0
[0808 21:05:56 @monitor.py:474] mean_gt_box_area: 49981
[0808 21:05:56 @monitor.py:474] rpn_losses/box_loss: 0.02925
[0808 21:05:56 @monitor.py:474] rpn_losses/label_loss: 0.31184
[0808 21:05:56 @monitor.py:474] rpn_losses/label_metrics/precision_th0.1: 0.048655
[0808 21:05:56 @monitor.py:474] rpn_losses/label_metrics/precision_th0.2: 0.091988
[0808 21:05:56 @monitor.py:474] rpn_losses/label_metrics/precision_th0.5: 0.12551
[0808 21:05:56 @monitor.py:474] rpn_losses/label_metrics/recall_th0.1: 0.99624
[0808 21:05:56 @monitor.py:474] rpn_losses/label_metrics/recall_th0.2: 0.60615
[0808 21:05:56 @monitor.py:474] rpn_losses/label_metrics/recall_th0.5: 0.14237
[0808 21:05:56 @monitor.py:474] rpn_losses/num_pos_anchor: 12.427
[0808 21:05:56 @monitor.py:474] rpn_losses/num_valid_anchor: 255.5
[0808 21:05:56 @monitor.py:474] sample_fast_rcnn_targets/num_bg: 184.64
[0808 21:05:56 @monitor.py:474] sample_fast_rcnn_targets/num_fg: 2.4354
[0808 21:05:56 @monitor.py:474] sample_fast_rcnn_targets/proposal_metrics/best_iou_per_gt: 0.22597
[0808 21:05:56 @monitor.py:474] sample_fast_rcnn_targets/proposal_metrics/recall_iou0.3: 0.33836
[0808 21:05:56 @monitor.py:474] sample_fast_rcnn_targets/proposal_metrics/recall_iou0.5: 0.11704
[0808 21:05:56 @monitor.py:474] total_cost: nan
[0808 21:05:56 @monitor.py:474] wd_cost: nan
```
In train.py:
I modify the following line by adding ignore_mismatch=True to directly load the pretrained model:
```
        if args.load:
            # ignore mismatched values, so you can `--load` a model for fine-tuning
            session_init = get_model_loader(args.load, ignore_mismatch=True)
        else:
            session_init = get_model_loader(cfg.BACKBONE.WEIGHTS, ignore_mismatch=True) if cfg.BACKBONE.WEIGHTS else None
```
In coco.py:
I modify the following line in register_coco function for changing the class name to match my dataset:
```
def register_coco(basedir):
    """"""
    Add COCO datasets like ""coco_train201x"" to the registry,
    so you can refer to them with names in `cfg.DATA.TRAIN/VAL`.

    Note that train2017==trainval35k==train2014+val2014-minival2014, and val2017==minival2014.
    """"""

    # 80 names for COCO
    # For your own coco-format dataset, change this.
    class_names = [""connector""]  # noqa
    class_names = [""BG""] + class_names

    for split in [""train"", ""val""]:
        name = ""coco_"" + split
        DatasetRegistry.register(name, lambda x=split: COCODetection(basedir, x))
        DatasetRegistry.register_metadata(name, 'class_names', class_names)
```
(2) If you're using examples, have you made any changes to the examples? Paste git status; git diff here:
```
(base) [bigdata@bigdata-cv-2019-2 FasterRCNN]$ git diff
diff --git a/examples/FasterRCNN/config.py b/examples/FasterRCNN/config.py
index f4bea0d..d71643d 100644
--- a/examples/FasterRCNN/config.py
+++ b/examples/FasterRCNN/config.py
@@ -86,9 +86,9 @@ _C.MODE_FPN = False
 _C.DATA.BASEDIR = '/path/to/your/DATA/DIR'
 # All available dataset names are defined in `dataset/coco.py:register_coco`.
 # All TRAIN dataset will be concatenated for training.
-_C.DATA.TRAIN = ('coco_train2017',)   # i.e. trainval35k
+_C.DATA.TRAIN = ('coco_train',)   # i.e. trainval35k
 # Each VAL dataset will be evaluated separately (instead of concatenated)
-_C.DATA.VAL = ('coco_val2017',)  # AKA minival2014
+_C.DATA.VAL = ('coco_val',)  # AKA minival2014
 
 # These two configs will be populated later inside `finalize_configs`.
 _C.DATA.NUM_CATEGORY = -1  # without the background class (e.g., 80 for COCO)
@@ -109,11 +109,11 @@ _C.BACKBONE.WEIGHTS = ''
 # To train from an existing COCO model, use the path to that file, and change
 #   the other configurations according to that model.
 
-_C.BACKBONE.RESNET_NUM_BLOCKS = [3, 4, 6, 3]     # for resnet50
+_C.BACKBONE.RESNET_NUM_BLOCKS = [3, 4, 23, 3]     # for resnet50
 # RESNET_NUM_BLOCKS = [3, 4, 23, 3]    # for resnet101
 _C.BACKBONE.FREEZE_AFFINE = False   # do not train affine parameters inside norm layers
-_C.BACKBONE.NORM = 'FreezeBN'  # options: FreezeBN, SyncBN, GN, None
-_C.BACKBONE.FREEZE_AT = 2  # options: 0, 1, 2. How many stages in backbone to freeze (not training)
+_C.BACKBONE.NORM = 'SyncBN'  # options: FreezeBN, SyncBN, GN, None
+_C.BACKBONE.FREEZE_AT = 0  # options: 0, 1, 2. How many stages in backbone to freeze (not training)
 
 # Use a base model with TF-preferred padding mode,
 # which may pad more pixels on right/bottom than top/left.
@@ -131,7 +131,7 @@ _C.TRAIN.WEIGHT_DECAY = 1e-4
 _C.TRAIN.BASE_LR = 1e-2  # defined for total batch size=8. Otherwise it will be adjusted automatically
 _C.TRAIN.WARMUP = 1000   # in terms of iterations. This is not affected by #GPUs
 _C.TRAIN.WARMUP_INIT_LR = 1e-2 * 0.33  # defined for total batch size=8. Otherwise it will be adjusted automatically
-_C.TRAIN.STEPS_PER_EPOCH = 500
+_C.TRAIN.STEPS_PER_EPOCH = 50
 _C.TRAIN.STARTING_EPOCH = 1  # the first epoch to start with, useful to continue a training
 
 # LR_SCHEDULE means equivalent steps when the total batch size is 8.
diff --git a/examples/FasterRCNN/dataset/coco.py b/examples/FasterRCNN/dataset/coco.py
index 3abd6fc..09c2105 100644
--- a/examples/FasterRCNN/dataset/coco.py
+++ b/examples/FasterRCNN/dataset/coco.py
@@ -25,7 +25,8 @@ class COCODetection(DatasetSplit):
     Mapping from the incontinuous COCO category id to an id in [1, #category]
     For your own coco-format, dataset, change this to an **empty dict**.
     """"""
-    COCO_id_to_category_id = {13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 32
+    #COCO_id_to_category_id = {13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 3
+    COCO_id_to_category_id = {}
 
     def __init__(self, basedir, split):
         """"""
@@ -114,6 +115,7 @@ class COCODetection(DatasetSplit):
                     assert os.path.isfile(img[""file_name""]), img[""file_name""]
                 if add_gt:
                     self._add_detection_gt(img, add_mask)
+                    # print(img)
             return imgs
 
     def _add_detection_gt(self, img, add_mask):
@@ -225,12 +227,10 @@ def register_coco(basedir):
 
     # 80 names for COCO
     # For your own coco-format dataset, change this.
-    class_names = [
-        ""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"", ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""b
+    class_names = [""connector""]  # noqa
     class_names = [""BG""] + class_names
 
-    for split in [""train2017"", ""val2017"", ""train2014"", ""val2014"",
-                  ""valminusminival2014"", ""minival2014"", ""trainsingle""]:
+    for split in [""train"", ""val""]:
         name = ""coco_"" + split
         DatasetRegistry.register(name, lambda x=split: COCODetection(basedir, x))
         DatasetRegistry.register_metadata(name, 'class_names', class_names)
diff --git a/examples/FasterRCNN/train.py b/examples/FasterRCNN/train.py
index 68c014e..47dc615 100755
--- a/examples/FasterRCNN/train.py
+++ b/examples/FasterRCNN/train.py
@@ -109,7 +109,7 @@ if __name__ == '__main__':
             # ignore mismatched values, so you can `--load` a model for fine-tuning
             session_init = get_model_loader(args.load, ignore_mismatch=True)
         else:
-            session_init = get_model_loader(cfg.BACKBONE.WEIGHTS) if cfg.BACKBONE.WEIGHTS else None
+            session_init = get_model_loader(cfg.BACKBONE.WEIGHTS, ignore_mismatch=True) if cfg.BACKBONE.WEIGHTS else None
 
     traincfg = TrainConfig(
         model=MODEL,
```",make could post full environment following issue template done full follow load environment information python default compiler version support true support false support false driver none free ram count true false false false true false false true warm schedule value schedule value loading memory done index index loaded load finished time sec category distribution class box class box class box connector total contain total training total training set none monitor visible training model automatically setting queue building graph training tower device input output input output input output found regularize following converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown building graph training tower device found regularize converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown tower building graph training tower device found regularize converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown tower building graph training tower device found regularize converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown tower list trainable name shape number trainable number storage space trainable setup graph free ram building graph predict tower device building graph predict tower device variable scope building graph predict tower device variable scope building graph predict tower device variable scope loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference collection run session collection size session binary use successfully dynamic library service platform device compute capability device compute capability device compute capability device compute capability frequency service platform host device undefined undefined found device name major minor found device name major minor found device name major minor found device name major minor successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library successfully dynamic library visible successfully dynamic library device interconnect strength edge matrix device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability warning cluster set want either set use enable confirm active pas proper flag via set corrupt data premature end data segment session restore following graph found load tensor shape variable whose shape load tensor shape variable whose shape load tensor shape variable whose shape load tensor shape variable whose shape load tensor shape variable whose shape load tensor shape variable whose shape graph starting running set free ram evaluate every start epoch element put tower successfully dynamic library successfully dynamic library corrupt data premature end data segment epoch finished time running time left day throughput nan nan nan modify following line directly load model ignore load model else else none modify following line function class name match add coco like registry refer note coco change connector split train name split name lambda name made paste git status git base git git index false available defined train training separately instead aka aka two later inside without background class coco train coco model use path file change according model false train affine inside norm none many backbone freeze training none many backbone freeze training use base model padding mode may pad defined total batch otherwise automatically affected defined total batch otherwise automatically first epoch start useful continue training equivalent total batch size git index class incontinuous coco category id id category change empty self split class assert print return self coco change person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter connector split train train split train name split name lambda name git index ce ignore load model else else none else none,issue,positive,negative,neutral,neutral,negative,negative
519590441,"Did you make any modifications? If not, could you post full logs and the environment following the issue template?",make could post full environment following issue template,issue,negative,positive,positive,positive,positive,positive
519584868,"> Just added an example of fine-tuning on toy dataset: https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/BALLOON.md.

Install the latest version of tensorpack and clone the latest code from github, then refer this toy example, still be Nan.",added example toy install latest version clone latest code refer toy example still nan,issue,negative,positive,positive,positive,positive,positive
519349482,"> Even with a dataset of random images, with one random box per image of a single class, I'm still able to train it normally (with the commit [8932306](https://github.com/tensorpack/tensorpack/commit/8932306ff2967023133b1dafd3508c8468c6b5ce)).
> If you cannot, I would assume that's either because of a wrong annotation format or some uncommon anomalies in the dataset.

@ppwwyyxx thanks for your reply. i had check my datasets by drawing all images and annos, i do not found any erros. When i continue to train it, box_loss and total_cost is nan always. Is it normal?",even random one random box per image single class still able train normally commit would assume either wrong annotation format uncommon thanks reply check drawing found continue train nan always normal,issue,negative,positive,neutral,neutral,positive,positive
519339084,"> @roger1993 hello, i met same problem with you, my dataset nearly has 1 object per image, but box_loss=nan when i continue training. Do you had solved this problem?

Not yet, I am still working on it, have you tried any methods?",roger hello met problem nearly object per image continue training problem yet still working tried,issue,negative,positive,neutral,neutral,positive,positive
519338915,"> Just added an example of fine-tuning on toy dataset: https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/BALLOON.md.

Thanks Yuxin, I am still working on it, I'll report here if I've made any progresses",added example toy thanks still working report made,issue,negative,positive,positive,positive,positive,positive
519309667,"All done. Thanks for your suggestions!

For users' own dataset, just added an example of fine-tuning on toy dataset: https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/BALLOON.md",done thanks added example toy,issue,negative,positive,positive,positive,positive,positive
519293118,"> session_init=SaverRestore(model-500.data-00000-of-00001, prefix=None, ignore=[])),

You may need a correct path, e.g., ""train_log/xx/model-500.data-00000-of-00001"". But if it's in the current directory then it is correct.",may need correct path current directory correct,issue,negative,neutral,neutral,neutral,neutral,neutral
519287283,"That makes sense...

In order to resume the model from the checkpoint I've got (model-500.data-00000-of-00001), 

would it be this code correct (train.py)? 

traincfg = TrainConfig(
        model=MODEL,
        data=QueueInput(train_dataflow),
        callbacks=callbacks,
        steps_per_epoch=stepnum,
        max_epoch=cfg.TRAIN.LR_SCHEDULE[-1] * factor // stepnum,
        session_init=**SaverRestore(model-500.data-00000-of-00001, prefix=None, ignore=[])),**
        starting_epoch=cfg.TRAIN.STARTING_EPOCH

Thank you very much. ",sense order resume model got would code correct factor thank much,issue,negative,positive,positive,positive,positive,positive
519272989,"Even with a dataset of random images, with one random box per image of a single class, I'm still able to train it normally (with the commit 8932306). 
If you cannot, I would assume that's either because of a wrong annotation format or some uncommon anomalies in the dataset.",even random one random box per image single class still able train normally commit would assume either wrong annotation format uncommon,issue,negative,negative,neutral,neutral,negative,negative
519181992,"Please install tensorpack following the readme.

If that fails, please post following the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md)",please install following please post following issue template,issue,positive,neutral,neutral,neutral,neutral,neutral
519013153,"@roger1993 hello, i met same problem with you, my dataset nearly has 1 object per image, but box_loss=nan when i continue training. Do you had solved this problem?",roger hello met problem nearly object per image continue training problem,issue,negative,positive,neutral,neutral,positive,positive
518928551,"@LandyGuo  hello, you had solved this problem, i met too. If you, Could you tell me?",hello problem met could tell,issue,negative,neutral,neutral,neutral,neutral,neutral
518880647,The change of initialization in https://github.com/tensorpack/tensorpack/commit/8932306ff2967023133b1dafd3508c8468c6b5ce may help with your issue.,change may help issue,issue,negative,neutral,neutral,neutral,neutral,neutral
518563834,"The format may not be the same as what's used in object detection API, e.g., the coordinates.",format may used object detection,issue,negative,neutral,neutral,neutral,neutral,neutral
518559328,"> If dataset is the only thing you changed, perhaps first check the correctness of its annotations and formats.
> Your dataset seems to only have very few (usually 1?) objects per image. It may require some other tuning to balance the loss otherwise the model is overwhelmed with negative samples.

Thanks Yuxin, Yes, the dataset nearly has 1 object per image with correct annotation and coco format since I drew the annotation on the image and I used the same dataset to get succeed by using tensorflow object detection api, I will try to balance the loss and if I succeed I'll report here",thing perhaps first check correctness usually per image may require tuning balance loss otherwise model negative thanks yes nearly object per image correct annotation coco format since drew annotation image used get succeed object detection try balance loss succeed report,issue,positive,positive,neutral,neutral,positive,positive
518557144,"If dataset is the only thing you changed, perhaps first check the correctness of its annotations and formats.
Your dataset seems to only have very few (usually 1?) objects per image. It may require some other tuning to balance the loss otherwise the model is overwhelmed with negative samples.",thing perhaps first check correctness usually per image may require tuning balance loss otherwise model negative,issue,negative,negative,neutral,neutral,negative,negative
518547252,"> > ./train.py --config
> > DATA.BASEDIR=/home/bigdata/data
> > MODE_MASK=True MODE_FPN=False
> > BACKBONE.WEIGHTS=/home/bigdata/COCO-MaskRCNN-R101C41x.npz
> 
> No guarantee what should make it work, since it is your own dataset. But using R101 would not make sense if your config says ResNet 50.

Could you kindly provide some insights for this problem, I have struggled for it near a week, What I've tried is: 1. train from scratch by using restnet101 as backbone, using SynBN or GN and meanwhile set BACKBONE.FREEZE_AT=0 2.train from pretrained model by using resnet101 as backbone, using freezeBN and meanwhile set BACKBONE.FREEZE_AT=2, both way give total loss = Nan. 3. I was trying the same architecture and default config to train coco2017, the total loss is not Nan and seems reasonable. 4 I was using the same dataset and Tensorflow Object detection API, the loss is not be Nan and get reasonable result",guarantee make work since would make sense could kindly provide problem near week tried train scratch backbone meanwhile set model backbone meanwhile set way give total loss nan trying architecture default train coco total loss nan reasonable object detection loss nan get reasonable result,issue,negative,positive,positive,positive,positive,positive
518523855,"> ./train.py --config
DATA.BASEDIR=/home/bigdata/data
MODE_MASK=True MODE_FPN=False
BACKBONE.WEIGHTS=/home/bigdata/COCO-MaskRCNN-R101C41x.npz

No guarantee what should make it work, since it is your own dataset. But using R101 would not make sense if your config says ResNet 50.",guarantee make work since would make sense,issue,negative,neutral,neutral,neutral,neutral,neutral
518519307,"> If you expect certain training results (e.g., accuracy), only in one of the two conditions can we help with it: (1) You're unable to reproduce the results documented in tensorpack examples. (2) It appears to be a tensorpack bug.
Otherwise, how to train a model is a machine learning question. We do not answer machine learning questions and it is your responsibility to figure out how to make your models more accurate.

Also see https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md:

> ""The examples do not perform as expected after I change the models/dataset/parameters/etc.""
Tensorpack maintainers make sure the examples perform well without modifications. But it's your job to pick the model and parameters that are suitable for your own situation. We cannot help with such questions unless they appear to be a bug in tensorpack.",expect certain training accuracy one two help unable reproduce bug otherwise train model machine learning question answer machine learning responsibility figure make accurate also see perform change make sure perform well without job pick model suitable situation help unless appear bug,issue,positive,positive,positive,positive,positive,positive
518517718,"> Please use `BACKBONE.WEIGHTS` as said in the README. Otherwise getting NaN is very expected.
> 
> Please also note this in the issue template:
> 
> > If you expect certain training results (e.g., accuracy), only in one of the two conditions can we help with it: (1) You're unable to reproduce the results documented in tensorpack examples. (2) It appears to be a tensorpack bug.
> > Otherwise, how to train a model is a machine learning question. We do not answer machine learning questions and it is your responsibility to figure out how to make your models more accurate.

I was trying to download the pretrained model from model zoo and use it as the backbone.weights, but the total loss still be NaN, below is the command I was using to start training:
./train.py --config \
    DATA.BASEDIR=/home/bigdata/data \
    MODE_MASK=True MODE_FPN=False \
    BACKBONE.WEIGHTS=/home/bigdata/COCO-MaskRCNN-R101C41x.npz
since I was training the single-class detector so I modify the .npz file by removing the maskrcnn/conv/w:0 and maskrcnn/conv/b:0 in the original npz file and generate a new .npz file ",please use said otherwise getting nan please also note issue template expect certain training accuracy one two help unable reproduce bug otherwise train model machine learning question answer machine learning responsibility figure make accurate trying model model zoo use total loss still nan command start training since training detector modify file removing original file generate new file,issue,positive,positive,positive,positive,positive,positive
518287068,"Please use `BACKBONE.WEIGHTS` as said in the README. Otherwise getting NaN is very expected.

Please also note this in the issue template:

> If you expect certain training results (e.g., accuracy), only in one of the two conditions can we help with it: (1) You're unable to reproduce the results documented in tensorpack examples. (2) It appears to be a tensorpack bug.
Otherwise, how to train a model is a machine learning question. We do not answer machine learning questions and it is your responsibility to figure out how to make your models more accurate.",please use said otherwise getting nan please also note issue template expect certain training accuracy one two help unable reproduce bug otherwise train model machine learning question answer machine learning responsibility figure make accurate,issue,positive,positive,neutral,neutral,positive,positive
518038417,"No. It does not provide such feature, and since it is the user who provides optimizer to trainer, such feature doesn't seem necessary.",provide feature since user trainer feature seem necessary,issue,negative,neutral,neutral,neutral,neutral,neutral
518030478,Thanks for the quick reply! I'd rather read it (and more configuration data) directly from the config or trainer objects. Is it possible?,thanks quick reply rather read configuration data directly trainer possible,issue,negative,positive,positive,positive,positive,positive
517978332,"Thanks for your reply. I prefer to use paths with only slashes since it works on both Linux and Windows and paths like `E:/train_log` or `./train_log` are actually recognized in Windows Python. Before I solved this issue by removing slashes, it confuses me a little bit that why ModelSaver works but MinSaver fails. So maybe there should be some warnings or a standardization as you proposed. ",thanks reply prefer use since work like actually python issue removing little bit work maybe standardization,issue,positive,positive,neutral,neutral,positive,positive
517959830,"This message supposed to only appear when you kill your dataflow or there are bugs in your dataflow.
Again, if you think it's a tensorpack issue, please show runnable code that demonstrates such issues.",message supposed appear kill think issue please show runnable code,issue,negative,neutral,neutral,neutral,neutral,neutral
517952885,"I am still running my code and still having problem with CPU memory accumulation but not that much. Today, I get  this exception
`tensorpack.dataflow.base.DataFlow Terminated`.
` [PrefetchDataZMQ] Context terminated`
 I am using 2000 samples, 200 epochs and this exception is raised at 142 epochs.",still running code still problem memory accumulation much today get exception context exception raised,issue,negative,positive,positive,positive,positive,positive
517774791,"I'm not sure whether you're assuming the issue is still related to tensorpack. If you think so, it's better to show runnable code that demonstrates such issues.",sure whether assuming issue still related think better show runnable code,issue,positive,positive,positive,positive,positive,positive
517693215,"Thanks. I release some references and now it is much better. However, it is still increasing while it is training! It is a generator, so, normally the memory use should be ~ constant!",thanks release much better however still increasing training generator normally memory use constant,issue,positive,positive,positive,positive,positive,positive
517531166,"`E:/train_log` doesn't seem like a legal windows path to me. 

Perhaps this can be automatically fixed by standardizing the user input, e.g. adding a line `self.checkpoint_dir = os.path.abspath(self.checkpoint_dir)` in `__init__`.",seem like legal path perhaps automatically fixed user input line,issue,negative,positive,positive,positive,positive,positive
517435344,Could you disable PrefetchDataZMQ to see whether the memory comes from your own dataflow?,could disable see whether memory come,issue,negative,neutral,neutral,neutral,neutral,neutral
517434134,"I can confirm that the problem is coming from TensorPack. I run:
   ```
    df = dataset.get_dataflow(True)
    df = PrefetchDataZMQ(df, nr_proc=1)
    df = BatchData(df, batch_size, use_list=False)
    TestDataSpeed(df, size=2000).start()
```
The memory start from ~=3G and start accumulated till 5G after 22% !!! I thin the problem is coming from this warning 
 **WRN Starting a process with 'fork' method is not safe and may consume unnecessary extra memory. Use 'forkserver' method (available after Py3.4) instead if you run into any issues**",confirm problem coming run true memory start start till thin problem coming warning starting process method safe may consume unnecessary extra memory use method available instead run,issue,negative,positive,neutral,neutral,positive,positive
517425900,The best way to disentangle dataflow from other libraries is to use [TestDataSpeed](https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.TestDataSpeed) to run the dataflow alone for a while.,best way disentangle use run alone,issue,negative,positive,positive,positive,positive,positive
517424476,Thanks. I am reading about and I think the problem is coming from keras fit_generator which accumulate memory!!!??,thanks reading think problem coming accumulate memory,issue,negative,positive,positive,positive,positive,positive
517406427,Then may be it's not related to tensorpack at all. Without more information I cannot tell why would you use 7G of memory.,may related without information tell would use memory,issue,negative,neutral,neutral,neutral,neutral,neutral
517405626,"By disabling `PrefetchDataZMQ` I got 7 GB!! so this is not the problem
I don't read anything on the memory. I read the content of the image in the `get_dataflow()`  function, in this instruction ` df = MapData(df, self.read_img)`",got problem read anything memory read content image function instruction,issue,negative,neutral,neutral,neutral,neutral,neutral
517397928,"This depends on how you implement your dataflow. If you read everything to memory then it's certainly going to be high.
You can see what the memory use is by just disabling `PrefetchDataZMQ`.",implement read everything memory certainly going high see memory use,issue,negative,positive,positive,positive,positive,positive
517395713,I change `nr_proc=1` and get `8 G` of consumption but I think still too high!!,change get consumption think still high,issue,negative,positive,positive,positive,positive,positive
517107325,">  it's different from tensorpack official model --‘http://models.tensorpack.com/ResNet/ImageNet-ResNet50-SE.npz’

I don't know what do you mean by ""different"". It would be more clear if you could post follow the issue template.

If you mean the values are different: this is expected because the models in tensorpack is not converted from caffe. It is a model trained in tensorpack.

Also, there is no guarantee that the model architecture is identical between the caffe model and the SE implementation in tensorpack. Different framework has small implementation differences which one needs to manually verify.",different official model know mean different would clear could post follow issue template mean different converted model trained also guarantee model architecture identical model se implementation different framework small implementation one need manually verify,issue,positive,negative,neutral,neutral,negative,negative
517010937,"1. The warning itself already has information on using a different method (see python documentation): https://github.com/tensorpack/tensorpack/blob/c9fde6302a11a06acf53a378adf18a2a579276fe/tensorpack/utils/concurrency.py#L240-L242

2. This warning affects CPU memory. It does not affect GPU memory and unrelated to your GPU model.

Your problem is unrelated to this issue. If you still have unexpected issues, please open a new issue following the issue template.",warning already information different method see python documentation warning memory affect memory unrelated model problem unrelated issue still unexpected please open new issue following issue template,issue,negative,positive,neutral,neutral,positive,positive
517008939,"@Seraphli   I got this warning alos when I use `prefetchdatazmq`

**WRN Starting a process with 'fork' method is not safe and may consume unnecessary extra memory. Use 'forkserver' method (available after Py3.4) instead if you run into any issues.** 
I think it causes really a problem of memory because on GTX 1080, I could not use more than batch_size=2 (image_size 512*512*3). How to remove this warning?",got warning use starting process method safe may consume unnecessary extra memory use method available instead run think really problem memory could use remove warning,issue,negative,positive,positive,positive,positive,positive
516624944,"The branch `fpn_cpu_inference` should be abandoned and was just made for a quick demo.

If you do want to contribute, it would make sense to:
1. only contribute to the master branch
2. only contribute things that are related to tensorpack as a training interface.

Regarding (2), it seems like a large portion of your commits are modification of tensorflow's quantization code. It would not fit as part of the tensorpack FasterRCNN example, since it is an example made to demonstrate how to train a FasterRCNN in tensorpack. 
Such changes would make more sense to stay in a separate project, if a small amount of changes (that do not overwhelm the goal of being a training example) can be made in this tensorpack example to support them. If not, you can also start your own repo with the FasterRCNN example. ",branch abandoned made quick want contribute would make sense contribute master branch contribute related training interface regarding like large portion modification quantization code would fit part example since example made demonstrate train would make sense stay separate project small amount overwhelm goal training example made example support also start example,issue,positive,positive,positive,positive,positive,positive
516622295,"Hi. I am trying to make Fasterrcnn FPN run on cpu with NHWC format so that I can perform quantization and improve performance. Related to this, I have made some changes with a mix of both the master branch and cpu branch. I have submitted the code here with all the additional tool.

Please let me know if the CI failure is related(as it complains about numpy library absence)

I am awaiting this to be merged for integrating it to our Intel Model Zoo. Some expedited help on what you need from me to accept these changes would be highly appreciated.",hi trying make run format perform quantization improve performance related made mix master branch branch code additional tool please let know failure related library absence model zoo expedited help need accept would highly,issue,positive,negative,neutral,neutral,negative,negative
515750276,Feel free to post a new issue with details if you still have questions to ask.,feel free post new issue still ask,issue,positive,positive,positive,positive,positive,positive
515723702,"When you pointed out the weights I was incorrectly using, I suddenly realized what ""GN"" meant, and the table also became very clear to me.  Not sure if necessary for most, but it would be nice for newbies like me if that was mentioned in the README.",pointed incorrectly suddenly meant table also clear sure necessary would nice like,issue,positive,positive,positive,positive,positive,positive
515723414,"> Whether you want to change other configs is up to you. 

Despite of this, if you're not very familiar with the models, it would be better to use one of the reasonable configs in the table instead of making up a new one.",whether want change despite familiar would better use one reasonable table instead making new one,issue,negative,positive,positive,positive,positive,positive
515723231,"Since you load a GroupNorm backbone, at least you have to set `BACKBONE.NORM=GN`. Loading weights from one model to a different model will usually produce garbage outputs. 

Whether you want to change other configs is up to you. But at least this will give you a valid training setting.

You can also start with other backbones in the model zoo that does not use GroupNorm.",since load backbone least set loading one model different model usually produce garbage whether want change least give valid training setting also start model zoo use,issue,negative,negative,negative,negative,negative,negative
515722990,"Sorry for not elaborating on what my configuration is, I think it's best to just paste anything I changed here:

```python
_C.MODE_MASK = False  # FasterRCNN or MaskRCNN

_C.DATA.BASEDIR = "".../data/training_data/COCO""
_C.BACKBONE.WEIGHTS = "".../data/weights/ImageNet-R50-GroupNorm32-AlignPadding.npz""
```
Btw I used absolute paths but shortened them above.

So I'm pretty sure my config was not changed between training and prediction.

But I see what you are saying, is this (from the README):
```
MODE_FPN=True
FPN.NORM=GN
BACKBONE.NORM=GN
FPN.FRCNN_HEAD_FUNC=fastrcnn_4conv1fc_gn_head
FPN.MRCNN_HEAD_FUNC=maskrcnn_up4conv_gn_head
TRAIN.LR_SCHEDULE=[240000,320000,360000]
```
what you mean by needing a different set of configs? Minus the FPN stuff?
",sorry configuration think best paste anything python false used absolute pretty sure training prediction see saying mean needing different set minus stuff,issue,positive,positive,neutral,neutral,positive,positive
515720123,"The [README](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN#inference) clearly says that you need to pass in the correct config items that are used during training, which you seem to miss.
If you did not change any config in training, you should not load the model `ImageNet-R50-GroupNorm32-AlignPadding.npz` at all because it needs a different set of configs.",clearly need pas correct used training seem miss change training load model need different set,issue,negative,positive,neutral,neutral,positive,positive
515306386,"well, Is this problem different OS?
when i processed tensorpack, i use ubuntu. but, when i support pull request, i use windows.",well problem different o use support pull request use,issue,negative,neutral,neutral,neutral,neutral,neutral
515279066,"You are right.
I recreate pull request!",right recreate pull request,issue,negative,positive,positive,positive,positive,positive
515277445,"Hi!

Thanks for your contribution! Since most the code are identical from `predict.py`, could you add a new flag in `predict.py` to simplify the implementation?
A flag like `./predict.py --export compact or --export serving` would be nice.",hi thanks contribution since code identical could add new flag simplify implementation flag like export compact export serving would nice,issue,positive,positive,positive,positive,positive,positive
515050209,The server needs to reboot because of security updates (yet another CPU related security issue fix). I will restart the web server when at home today.,server need security yet another related security issue fix restart web server home today,issue,positive,neutral,neutral,neutral,neutral,neutral
514920971,"Oh, Thank you for the quick reply!
Then I'll be waiting. Thanks!",oh thank quick reply waiting thanks,issue,positive,positive,positive,positive,positive,positive
514849455,@ppwwyyxx We upgraded our machines on GCP (from 32 CPU -> 96) and haven't been able to re-produce the error either since. Will re-open if we encounter it again. ,able error either since encounter,issue,negative,positive,positive,positive,positive,positive
514717860,"The distributed training seems to be running fine on my side. Could you provide more logs, hopefully from each machine?

The partial error you provided does look like a side effect of a different error.",distributed training running fine side could provide hopefully machine partial error provided look like side effect different error,issue,negative,positive,positive,positive,positive,positive
514695997,@ppwwyyxx I updated the original comment with the required information. Really appreciate your help debugging this issue. ,original comment information really appreciate help issue,issue,positive,positive,positive,positive,positive,positive
514685624,"Then the most suitable combination is (32, 32, 32). 
The slightly less suitable combination is (8, 32, 32).",suitable combination slightly le suitable combination,issue,negative,positive,positive,positive,positive,positive
514684885,"Alright. I can take a look at it if you can provide details about the problem. See https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/unexpected-problems---bugs.md.

Also there is a [BytePSTrainer](https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.BytePSTrainer) -- does not require MPI and works similar to horovod, but the setup is a bit complicated.",alright take look provide problem see also require work similar setup bit complicated,issue,negative,negative,negative,negative,negative,negative
514633028,@ppwwyyxx In our cluster configuration we are unable to use MPI which is why we are using `DistributedTrainerReplicated`. We have found that it works effectively outside of this error. Do you have any suggestions on what could be causing it?,cluster configuration unable use found work effectively outside error could causing,issue,negative,negative,negative,negative,negative,negative
514565574,I figured it out. Use tf.shape() can fix this problem,figured use fix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
514524040,"Hello.
In my view, the most suitable combination of (W,A,G) is  to get closer to unquantified network accuracy.
So, from your experience, what is the most suitable combination of (W,A,G)?",hello view suitable combination get closer unquantified network accuracy experience suitable combination,issue,negative,positive,positive,positive,positive,positive
514502621,"The readme of Mask R-CNN says you need to use HorovodTrainer.

`DistributedTrainerReplicated` does not exist in tensorpack documentation at all (https://tensorpack.readthedocs.io/modules/train.html). 
It exists in the codebase but that doesn't mean you can use it.
They don't work mainly because tensorflow has basically abandoned them due to the poor performance.

See also https://tensorpack.readthedocs.io/tutorial/trainer.html#distributed-trainers",mask need use exist documentation mean use work mainly basically abandoned due poor performance see also,issue,negative,negative,negative,negative,negative,negative
514266997,"There is no most suitable combination if there is no definition of ""suitable"". You need to pick your own parameters for your situation.",suitable combination definition suitable need pick situation,issue,negative,positive,positive,positive,positive,positive
513912728,"That's good. It still says `v0.9.4` because somehow the `v0.9.5` tag is missing in the repo. `v0.9.4-127-g2069cfdc-dirty` is correct because 2069cfdc is the commit hash and it's one of the latest.
Just pushed a new 0.9.6 tag and it should correct the version printed in the log.",good still somehow tag missing correct commit hash one latest new tag correct version printed log,issue,negative,positive,positive,positive,positive,positive
513694616,"Hi, 

I found the error, it was a dependencies issue. When updating dependencies I typed:

pip install --upgrade git+https://github.com/tensorpack/tensorpack.git

Instead of 

**pip3** install --upgrade git+https://github.com/tensorpack/tensorpack.git

Now is trainning but stil appear version Tensorpack            v0.9.4-37-g59829770-dirty in the log (not sure why this appear as I clone the repository as it is now). 

Now is working so very happy with this.

Thank you very much, below the log just in case if it'd be useful for anyone;

```
federicolondon2019@instance-1:~/tensorpack/examples/FasterRCNN$ python3 train.py --config     MODE_MASK=True MODE_FPN=True  
   DATA.BASEDIR=/home/federicolondon2019/tensorpack/COCO/DIR     BACKBONE.WEIGHTS=/home/federicolondon2019/tensorpack/models
/ImageNet-R50-AlignPadding.npz
[0722 08:14:55 @logger.py:90] Argv: train.py --config MODE_MASK=True MODE_FPN=True DATA.BASEDIR=/home/federicolondon2019/tensorpack/COCO/DIR BACKBONE.WEIGHTS=/home/federicolondon2019/tensorpack/models/ImageNet-R
50-AlignPadding.npz
[0722 08:14:57 @train.py:55] Environment Information:
--------------------  -----------------------------------------------------------
sys.platform          linux
Python                3.5.3 (default, Sep 27 2018, 17:25:39) [GCC 6.3.0 20170516]
Tensorpack            v0.9.4-127-g2069cfdc-dirty
Numpy                 1.16.4
TensorFlow            1.13.1/b'v1.13.1-0-g6612da8951'
TF Compiler Version   4.8.5
TF CUDA support       True
TF MKL support        False
Nvidia Driver         /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.410.72
CUDA                  /usr/local/cuda-10.0/lib64/libcudart.so.10.0.130
CUDNN                 /usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1
NCCL                  /usr/local/nccl2/lib/libnccl.so.2.3.4
CUDA_VISIBLE_DEVICES  None
GPU 0,1               Tesla T4
Free RAM              57.88/58.99 GB
CPU Count             16
horovod               0.16.0
cv2                   4.1.0
msgpack               0.6.1
python-prctl          True
--------------------  -----------------------------------------------------------
[0722 08:14:59 @config.py:281] Config: ------------------------------------------
{'BACKBONE': {'FREEZE_AFFINE': False,
              'FREEZE_AT': 2,
              'NORM': 'FreezeBN',
              'RESNET_NUM_BLOCKS': [3, 4, 6, 3],
              'STRIDE_1X1': False,
              'TF_PAD_MODE': False,
              'WEIGHTS': '/home/federicolondon2019/tensorpack/models/ImageNet-R50-AlignPadding.npz'},
 'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0],
                                  [30.0, 30.0, 15.0, 15.0]],
             'IOUS': [0.5, 0.6, 0.7]},
 'DATA': {'ABSOLUTE_COORD': True,
          'BASEDIR': '/home/federicolondon2019/tensorpack/COCO/DIR',
          'CLASS_NAMES': ['BG', 'Bird', 'Ground_Animal', 'Crosswalk_Plain', 'Person', 'Bicyclist',
                          'Motorcyclist', 'Other_Rider', 'Lane_Marking_-_Crosswalk', 'Banner',
                          'Bench', 'Bike_Rack', 'Billboard', 'Catch_Basin', 'CCTV_Camera',
                          'Fire_Hydrant', 'Junction_Box', 'Mailbox', 'Manhole', 'Phone_Booth',
                          'Street_Light', 'Pole', 'Traffic_Sign_Frame', 'Utility_Pole',
                          'Traffic_Light', 'Traffic_Sign_(Back)', 'Traffic_Sign_(Front)',
                          'Trash_Can', 'Bicycle', 'Boat', 'Bus', 'Car', 'Caravan', 'Motorcycle',
                          'Other_Vehicle', 'Trailer', 'Truck', 'Wheeled_Slow'],
          'NUM_CATEGORY': 37,
          'NUM_WORKERS': 10,
          'TRAIN': ('train2017',),
          'VAL': ('val2017',)},
 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),
         'CASCADE': False,
         'FRCNN_CONV_HEAD_DIM': 256,
         'FRCNN_FC_HEAD_DIM': 1024,
         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',
         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',
         'NORM': 'None',
         'NUM_CHANNEL': 256,
         'PROPOSAL_MODE': 'Level',
         'RESOLUTION_REQUIREMENT': 32},
 'FRCNN': {'BATCH_PER_IM': 512,
           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],
           'FG_RATIO': 0.25,
           'FG_THRESH': 0.5},
 'MODE_FPN': True,
 'MODE_MASK': True,
 'MRCNN': {'HEAD_DIM': 256},
 'PREPROC': {'MAX_SIZE': 1344.0,
             'PIXEL_MEAN': [123.675, 116.28, 103.53],
             'PIXEL_STD': [58.395, 57.12, 57.375],
             'TEST_SHORT_EDGE_SIZE': 800,
             'TRAIN_SHORT_EDGE_SIZE': [800, 800]},
 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),
         'ANCHOR_SIZES': (32, 64, 128, 256, 512),
         'ANCHOR_STRIDE': 16,
         'BATCH_PER_IM': 256,
         'CROWD_OVERLAP_THRESH': 9.99,
         'FG_RATIO': 0.5,
         'HEAD_DIM': 1024,
         'MIN_SIZE': 0,
         'NEGATIVE_ANCHOR_THRESH': 0.3,
         'NUM_ANCHOR': 15,
'POSITIVE_ANCHOR_THRESH': 0.7,
         'PROPOSAL_NMS_THRESH': 0.7,
         'TEST_PER_LEVEL_NMS_TOPK': 1000,
         'TEST_POST_NMS_TOPK': 1000,
         'TEST_PRE_NMS_TOPK': 6000,
         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,
         'TRAIN_POST_NMS_TOPK': 2000,
         'TRAIN_PRE_NMS_TOPK': 12000},
 'TEST': {'FRCNN_NMS_THRESH': 0.5,
          'RESULTS_PER_IM': 100,
          'RESULT_SCORE_THRESH': 0.05,
          'RESULT_SCORE_THRESH_VIS': 0.5},
 'TRAIN': {'BASE_LR': 0.01,
           'EVAL_PERIOD': 25,
           'LR_SCHEDULE': [120000, 160000, 180000],
           'NUM_GPUS': 2,
           'STARTING_EPOCH': 1,
           'STEPS_PER_EPOCH': 500,
           'WARMUP': 1000,
           'WARMUP_INIT_LR': 0.0033000000000000004,
           'WEIGHT_DECAY': 0.0001},
 'TRAINER': 'replicated'}
[0722 08:14:59 @train.py:72] Warm Up Schedule (steps, value): [(0, 0.0033000000000000004), (1000, 0.01)]
[0722 08:14:59 @train.py:73] LR Schedule (epochs, value): [(2, 0.01), (960.0, 0.001), (1280.0, 0.00010000000000000002)]
loading annotations into memory...
Done (t=14.37s)
creating index...
index created!
[0722 08:15:15 @coco.py:68] Instances loaded from /home/federicolondon2019/tensorpack/COCO/DIR/annotations/instances_train2017.json.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18000/18000 [00:12<00:00, 1462.26it/s]
[0722 08:15:28 @timer.py:50] Load annotations for instances_train2017.json finished, time:12.3794 sec.
[0722 08:15:29 @data.py:59] Ground-Truth category distribution:
|      class      | #box   |        class        | #box   |          class           | #box   |
|:---------------:|:-------|:-------------------:|:-------|:------------------------:|:-------|
|       BG        | 1146   |        Bird         | 312    |      Ground_Animal       | 0      |
| Crosswalk_Plain | 44340  |       Person        | 2902   |        Bicyclist         | 3167   |
|  Motorcyclist   | 229    |     Other_Rider     | 9047   | Lane_Marking_-_Crosswalk | 7808   |
|     Banner      | 1539   |        Bench        | 1335   |        Bike_Rack         | 64995  |
|    Billboard    | 5964   |     Catch_Basin     | 5258   |       CCTV_Camera        | 1722   |
|  Fire_Hydrant   | 4412   |    Junction_Box     | 688    |         Mailbox          | 7997   |
|     Manhole     | 263    |     Phone_Booth     | 106093 |       Street_Light       | 291445 |
|      Pole       | 2858   | Traffic_Sign_Frame  | 0      |       Utility_Pole       | 59384  |
|  Traffic_Light  | 31815  | Traffic_Sign_(Back) | 96168  |   Traffic_Sign_(Front)   | 5726   |
|    Trash_Can    | 5447   |       Bicycle       | 442    |           Boat           | 3787   |
|       Bus       | 115615 |         Car         | 0      |         Caravan          | 4765   |
|   Motorcycle    | 1157   |    Other_Vehicle    | 187    |         Trailer          | 5851   |
|      Truck      | 705    |                     |        |                          |        |
|      total      | 894569 |                     |        |                          |        |
[0722 08:15:29 @data.py:344] Filtered 2789 images which contain no non-crowd groudtruth boxes. Total #images for training: 15211
[0722 08:15:29 @train.py:77] Total passes of the training set is: 94.668
[0722 08:15:31 @prof.py:50] WRN [GPUUtilizationTracker] Both devices and CUDA_VISIBLE_DEVICES are None! Will monitor all 2 visible GPUs!
[0722 08:15:33 @training.py:50] [DataParallel] Training a model of 2 towers.
[0722 08:15:33 @interface.py:43] Automatically applying StagingInput on the DataFlow.
[0722 08:15:33 @input_source.py:223] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0722 08:15:33 @training.py:110] Building graph for training tower 0 on device /gpu:0 ...
[0722 08:15:33 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:33 @registry.py:90] 'conv0': [1, 3, ?, ?] --> [1, 64, ?, ?]
[0722 08:15:33 @registry.py:90] 'pool0': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0722 08:15:33 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:33 @registry.py:90] 'group0/block0/conv1': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0722 08:15:33 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:33 @registry.py:90] 'group0/block0/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0722 08:15:33 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:33 @registry.py:90] 'group0/block0/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:33 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:33 @registry.py:90] 'group0/block0/convshortcut': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:33 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:33 @registry.py:90] 'group0/block1/conv1': [1, 256, ?, ?] --> [1, 64, ?, ?]
[0722 08:15:33 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:33 @registry.py:90] 'group0/block1/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0722 08:15:33 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:33 @registry.py:90] 'group0/block1/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:33 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:33 @registry.py:90] 'group0/block2/conv1': [1, 256, ?, ?] --> [1, 64, ?, ?]
[0722 08:15:33 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group0/block2/conv2': [1, 64, ?, ?] --> [1, 64, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group0/block2/conv3': [1, 64, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block0/conv1': [1, 256, ?, ?] --> [1, 128, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block0/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block0/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block0/convshortcut': [1, 256, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block1/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block1/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block1/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block2/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block2/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block2/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block3/conv1': [1, 512, ?, ?] --> [1, 128, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block3/conv2': [1, 128, ?, ?] --> [1, 128, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group1/block3/conv3': [1, 128, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block0/conv1': [1, 512, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block0/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block0/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block0/convshortcut': [1, 512, ?, ?] --> [1, 1024, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block1/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block1/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block1/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block2/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block2/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block2/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:34 @registry.py:90] 'group2/block3/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:34 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group2/block3/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group2/block3/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group2/block4/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group2/block4/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group2/block4/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group2/block5/conv1': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group2/block5/conv2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group2/block5/conv3': [1, 256, ?, ?] --> [1, 1024, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group3/block0/conv1': [1, 1024, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group3/block0/conv2': [1, 512, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group3/block0/conv3': [1, 512, ?, ?] --> [1, 2048, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group3/block0/convshortcut': [1, 1024, ?, ?] --> [1, 2048, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group3/block1/conv1': [1, 2048, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group3/block1/conv2': [1, 512, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group3/block1/conv3': [1, 512, ?, ?] --> [1, 2048, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group3/block2/conv1': [1, 2048, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group3/block2/conv2': [1, 512, ?, ?] --> [1, 512, ?, ?]
[0722 08:15:35 @batch_norm.py:227] WRN [BatchNorm] Using moving_mean/moving_variance in training.
[0722 08:15:35 @registry.py:90] 'group3/block2/conv3': [1, 512, ?, ?] --> [1, 2048, ?, ?]
[0722 08:15:35 @registry.py:80] 'fpn' input: [1, 256, ?, ?], [1, 512, ?, ?], [1, 1024, ?, ?], [1, 2048, ?, ?]
[0722 08:15:35 @registry.py:90]   'fpn/lateral_1x1_c2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:35 @registry.py:90]   'fpn/lateral_1x1_c3': [1, 512, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:35 @registry.py:90]   'fpn/lateral_1x1_c4': [1, 1024, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:35 @registry.py:90]   'fpn/lateral_1x1_c5': [1, 2048, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:36 @registry.py:90]   'fpn/upsample_lat5': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:36 @registry.py:90]   'fpn/upsample_lat4': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:36 @registry.py:90]   'fpn/upsample_lat3': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:36 @registry.py:90]   'fpn/posthoc_3x3_p2': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:36 @registry.py:90]   'fpn/posthoc_3x3_p3': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:36 @registry.py:90]   'fpn/posthoc_3x3_p4': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:36 @registry.py:90]   'fpn/posthoc_3x3_p5': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:36 @registry.py:90]   'fpn/maxpool_p6': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:36 @registry.py:93] 'fpn' output: [1, 256, ?, ?], [1, 256, ?, ?], [1, 256, ?, ?], [1, 256, ?, ?], [1, 256, ?, ?]
[0722 08:15:36 @registry.py:80] 'rpn' input: [1, 256, ?, ?]
[0722 08:15:36 @registry.py:90]   'rpn/conv0': [1, 256, ?, ?] --> [1, 256, ?, ?]
[0722 08:15:36 @registry.py:90]   'rpn/class': [1, 256, ?, ?] --> [1, 3, ?, ?]
[0722 08:15:36 @registry.py:90]   'rpn/box': [1, 256, ?, ?] --> [1, 12, ?, ?]
[0722 08:15:36 @registry.py:93] 'rpn' output: [?, ?, 3], [?, ?, 3, 4]
[0722 08:15:39 @registry.py:80] 'fastrcnn' input: [?, 256, 7, 7]
[0722 08:15:39 @registry.py:90]   'fastrcnn/fc6': [?, 256, 7, 7] --> [?, 1024]
[0722 08:15:39 @registry.py:90]   'fastrcnn/fc7': [?, 1024] --> [?, 1024]
[0722 08:15:39 @registry.py:93] 'fastrcnn' output: [?, 1024]
[0722 08:15:39 @registry.py:80] 'fastrcnn/outputs' input: [?, 1024]
[0722 08:15:39 @registry.py:90]   'fastrcnn/outputs/class': [?, 1024] --> [?, 38]
[0722 08:15:39 @registry.py:90]   'fastrcnn/outputs/box': [?, 1024] --> [?, 152]
[0722 08:15:39 @registry.py:93] 'fastrcnn/outputs' output: [?, 38], [?, 38, 4]
[0722 08:15:40 @registry.py:80] 'maskrcnn' input: [?, 256, 14, 14]
[0722 08:15:40 @registry.py:90]   'maskrcnn/fcn0': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0722 08:15:40 @registry.py:90]   'maskrcnn/fcn1': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0722 08:15:40 @registry.py:90]   'maskrcnn/fcn2': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0722 08:15:40 @registry.py:90]   'maskrcnn/fcn3': [?, 256, 14, 14] --> [?, 256, 14, 14]
[0722 08:15:40 @registry.py:90]   'maskrcnn/deconv': [?, 256, 14, 14] --> [?, 256, 28, 28]
[0722 08:15:40 @registry.py:90]   'maskrcnn/conv': [?, 256, 28, 28] --> [?, 37, 28, 28]
[0722 08:15:40 @registry.py:93] 'maskrcnn' output: [?, 37, 28, 28]
[0722 08:15:40 @regularize.py:97] regularize_cost() found 63 variables to regularize.
[0722 08:15:40 @regularize.py:21] The following tensors will be regularized: group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, gr
oup1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0
/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/con
v2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, gro
up2/block5/conv2/W:0, group2/block5/conv3/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/
block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, fpn/lateral_1x1_c2/W:0, fpn/lateral_1x1_c3/W:0, fpn/lateral_1x1_c4/W:0, fpn/lateral_1x1_c5/W:0, fpn/posthoc_3x3_p2/W:0
, fpn/posthoc_3x3_p3/W:0, fpn/posthoc_3x3_p4/W:0, fpn/posthoc_3x3_p5/W:0, rpn/conv0/W:0, rpn/class/W:0, rpn/box/W:0, fastrcnn/fc6/W:0, fastrcnn/fc7/W:0, fastrcnn/outputs/class/W:0, fastrcnn/outputs/box/W:0, mask
rcnn/fcn0/W:0, maskrcnn/fcn1/W:0, maskrcnn/fcn2/W:0, maskrcnn/fcn3/W:0, maskrcnn/deconv/W:0, maskrcnn/conv/W:0
/home/federicolondon2019/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large 
amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0722 08:15:44 @training.py:110] Building graph for training tower 1 on device /gpu:1 ...
[0722 08:15:49 @regularize.py:97] regularize_cost() found 63 variables to regularize.
[0722 08:15:53 @collection.py:152] Size of these collections were changed in tower1: (tf.GraphKeys.MODEL_VARIABLES: 183->238)
[0722 08:15:53 @collection.py:165] These collections were modified but restored in tower1: (tf.GraphKeys.SUMMARIES: 80->81)
[0722 08:15:59 @training.py:351] 'sync_variables_from_main_tower' includes 643 operations.
[0722 08:15:59 @model_utils.py:67] List of Trainable Variables: 
name                                 shape                 #elements
-----------------------------------  ------------------  -----------
group1/block0/conv1/W                [1, 1, 256, 128]          32768
group1/block0/conv1/bn/gamma         [128]                       128
group1/block0/conv1/bn/beta          [128]                       128
group1/block0/conv2/W                [3, 3, 128, 128]         147456
group1/block0/conv2/bn/gamma         [128]                       128
group1/block0/conv2/bn/beta          [128]                       128
group1/block0/conv3/W                [1, 1, 128, 512]          65536
group1/block0/conv3/bn/gamma         [512]                       512
group1/block0/conv3/bn/beta          [512]                       512
group1/block0/convshortcut/W         [1, 1, 256, 512]         131072
group1/block0/convshortcut/bn/gamma  [512]                       512
group1/block0/convshortcut/bn/beta   [512]                       512
group1/block1/conv1/W                [1, 1, 512, 128]          65536
group1/block1/conv1/bn/gamma         [128]                       128
group1/block1/conv1/bn/beta          [128]                       128
group1/block1/conv2/W                [3, 3, 128, 128]         147456
group1/block1/conv2/bn/gamma         [128]                       128
group1/block1/conv2/bn/beta          [128]                       128
group1/block1/conv3/W                [1, 1, 128, 512]          65536
group1/block1/conv3/bn/gamma         [512]                       512
group1/block1/conv3/bn/beta          [512]                       512
group1/block2/conv1/W                [1, 1, 512, 128]          65536
group1/block2/conv1/bn/gamma         [128]                       128
group1/block2/conv1/bn/beta          [128]                       128
group1/block2/conv2/W                [3, 3, 128, 128]         147456
group1/block2/conv2/bn/gamma         [128]                       128
group1/block2/conv2/bn/beta          [128]                       128
group1/block2/conv3/W                [1, 1, 128, 512]          65536
group1/block2/conv3/bn/gamma         [512]                       512
group1/block2/conv3/bn/beta          [512]                       512
group1/block3/conv1/W                [1, 1, 512, 128]          65536
group1/block3/conv1/bn/gamma         [128]                       128
group1/block3/conv1/bn/beta          [128]                       128
group1/block3/conv2/W                [3, 3, 128, 128]         147456
group1/block3/conv2/bn/gamma         [128]                       128
group1/block3/conv2/bn/beta          [128]                       128
group1/block3/conv3/W                [1, 1, 128, 512]          65536
group1/block3/conv3/bn/gamma         [512]                       512
group1/block3/conv3/bn/beta          [512]                       512
group2/block0/conv1/W                [1, 1, 512, 256]         131072
group2/block0/conv1/bn/gamma         [256]                       256
group2/block0/conv1/bn/beta          [256]                       256
group2/block0/conv2/W                [3, 3, 256, 256]         589824
group2/block0/conv2/bn/gamma         [256]                       256
group2/block0/conv2/bn/beta          [256]                       256
group2/block0/conv3/W                [1, 1, 256, 1024]        262144
group2/block0/conv3/bn/gamma         [1024]                     1024
group2/block0/conv3/bn/beta          [1024]                     1024
group2/block0/convshortcut/W         [1, 1, 512, 1024]        524288
group2/block0/convshortcut/bn/gamma  [1024]                     1024
group2/block0/convshortcut/bn/beta   [1024]                     1024
group2/block1/conv1/W                [1, 1, 1024, 256]        262144
group2/block1/conv1/bn/gamma         [256]                       256
group2/block1/conv1/bn/beta          [256]                       256
group2/block1/conv2/W                [3, 3, 256, 256]         589824
group2/block1/conv2/bn/gamma         [256]                       256
group2/block1/conv2/bn/beta          [256]                       256
group2/block1/conv3/W                [1, 1, 256, 1024]        262144
group2/block1/conv3/bn/gamma         [1024]                     1024
group2/block1/conv3/bn/beta          [1024]                     1024
group2/block2/conv1/W                [1, 1, 1024, 256]        262144
group2/block2/conv1/bn/gamma         [256]                       256
group2/block2/conv1/bn/beta          [256]                       256
group2/block2/conv2/W                [3, 3, 256, 256]         589824
group2/block2/conv2/bn/gamma         [256]                       256
group2/block2/conv2/bn/beta          [256]                       256
group2/block2/conv3/W                [1, 1, 256, 1024]        262144
group2/block2/conv3/bn/gamma         [1024]                     1024
group2/block2/conv3/bn/beta          [1024]                     1024
group2/block3/conv1/W                [1, 1, 1024, 256]        262144
group2/block3/conv1/bn/gamma         [256]                       256
group2/block3/conv1/bn/beta          [256]                       256
group2/block3/conv2/W                [3, 3, 256, 256]         589824
group2/block3/conv2/bn/gamma         [256]                       256
group2/block3/conv2/bn/beta          [256]                       256
group2/block3/conv3/W                [1, 1, 256, 1024]        262144
group2/block3/conv3/bn/gamma         [1024]                     1024
group2/block3/conv3/bn/beta          [1024]                     1024
group2/block4/conv1/W                [1, 1, 1024, 256]        262144
group2/block4/conv1/bn/gamma         [256]                       256
group2/block4/conv1/bn/beta          [256]                       256
group2/block4/conv2/W                [3, 3, 256, 256]         589824
group2/block4/conv2/bn/gamma         [256]                       256
group2/block4/conv2/bn/beta          [256]                       256
group2/block4/conv3/W                [1, 1, 256, 1024]        262144
group2/block4/conv3/bn/gamma         [1024]                     1024
group2/block4/conv3/bn/beta          [1024]                     1024
group2/block5/conv1/W                [1, 1, 1024, 256]        262144
group2/block5/conv1/bn/gamma         [256]                       256
group2/block5/conv1/bn/beta          [256]                       256
group2/block5/conv2/W                [3, 3, 256, 256]         589824
group2/block5/conv2/bn/gamma         [256]                       256
group2/block5/conv2/bn/beta          [256]                       256
group2/block5/conv3/W                [1, 1, 256, 1024]        262144
group2/block5/conv3/bn/gamma         [1024]                     1024
group2/block5/conv3/bn/beta          [1024]                     1024
group3/block0/conv1/W                [1, 1, 1024, 512]        524288
group3/block0/conv1/bn/gamma         [512]                       512
group3/block0/conv1/bn/beta          [512]                       512
group3/block0/conv2/W                [3, 3, 512, 512]        2359296
group3/block0/conv2/bn/gamma         [512]                       512
group3/block0/conv2/bn/beta          [512]                       512
group3/block0/conv3/W                [1, 1, 512, 2048]       1048576
group3/block0/conv3/bn/gamma         [2048]                     2048
group3/block0/conv3/bn/beta          [2048]                     2048
group3/block0/convshortcut/W         [1, 1, 1024, 2048]      2097152
group3/block0/convshortcut/bn/gamma  [2048]                     2048
group3/block0/convshortcut/bn/beta   [2048]                     2048
group3/block1/conv1/W                [1, 1, 2048, 512]       1048576
group3/block1/conv1/bn/gamma         [512]                       512
group3/block1/conv1/bn/beta          [512]                       512
group3/block1/conv2/W                [3, 3, 512, 512]        2359296
group3/block1/conv2/bn/gamma         [512]                       512
group3/block1/conv2/bn/beta          [512]                       512
group3/block1/conv3/W                [1, 1, 512, 2048]       1048576
group3/block1/conv3/bn/gamma         [2048]                     2048
group3/block1/conv3/bn/beta          [2048]                     2048
group3/block2/conv1/W                [1, 1, 2048, 512]       1048576
group3/block2/conv1/bn/gamma         [512]                       512
group3/block2/conv1/bn/beta          [512]                       512
group3/block2/conv2/W                [3, 3, 512, 512]        2359296
group3/block2/conv2/bn/gamma         [512]                       512
group3/block2/conv2/bn/beta          [512]                       512
group3/block2/conv3/W                [1, 1, 512, 2048]       1048576
group3/block2/conv3/bn/gamma         [2048]                     2048
group3/block2/conv3/bn/beta          [2048]                     2048
fpn/lateral_1x1_c2/W                 [1, 1, 256, 256]          65536
fpn/lateral_1x1_c2/b                 [256]                       256
fpn/lateral_1x1_c3/W                 [1, 1, 512, 256]         131072
fpn/lateral_1x1_c3/b                 [256]                       256
fpn/lateral_1x1_c4/W                 [1, 1, 1024, 256]        262144
fpn/lateral_1x1_c4/b                 [256]                       256
fpn/lateral_1x1_c5/W                 [1, 1, 2048, 256]        524288
fpn/lateral_1x1_c5/b                 [256]                       256
fpn/posthoc_3x3_p2/W                 [3, 3, 256, 256]         589824
fpn/posthoc_3x3_p2/b                 [256]                       256
fpn/posthoc_3x3_p3/W                 [3, 3, 256, 256]         589824
fpn/posthoc_3x3_p3/b                 [256]                       256
fpn/posthoc_3x3_p4/W                 [3, 3, 256, 256]         589824
fpn/posthoc_3x3_p4/b                 [256]                       256
fpn/posthoc_3x3_p5/W                 [3, 3, 256, 256]         589824
fpn/posthoc_3x3_p5/b                 [256]                       256
rpn/conv0/W                          [3, 3, 256, 256]         589824
rpn/conv0/b                          [256]                       256
rpn/class/W                          [1, 1, 256, 3]              768
rpn/class/b                          [3]                           3
rpn/box/W                            [1, 1, 256, 12]            3072
rpn/box/b                            [12]                         12
fastrcnn/fc6/W                       [12544, 1024]          12845056
fastrcnn/fc6/b                       [1024]                     1024
fastrcnn/fc7/W                       [1024, 1024]            1048576
fastrcnn/fc7/b                       [1024]                     1024
fastrcnn/outputs/class/W             [1024, 38]                38912
fastrcnn/outputs/class/b             [38]                         38
fastrcnn/outputs/box/W               [1024, 152]              155648
fastrcnn/outputs/box/b               [152]                       152
maskrcnn/fcn0/W                      [3, 3, 256, 256]         589824
maskrcnn/fcn0/b                      [256]                       256
maskrcnn/fcn1/W                      [3, 3, 256, 256]         589824
maskrcnn/fcn1/b                      [256]                       256
maskrcnn/fcn2/W                      [3, 3, 256, 256]         589824
maskrcnn/fcn2/b                      [256]                       256
maskrcnn/fcn3/W                      [3, 3, 256, 256]         589824
maskrcnn/fcn3/b                      [256]                       256
maskrcnn/deconv/W                    [2, 2, 256, 256]         262144
maskrcnn/deconv/b                    [256]                       256
maskrcnn/conv/W                      [1, 1, 256, 37]            9472
maskrcnn/conv/b                      [37]                         37
Number of trainable variables: 168
Number of parameters (elements): 43943666
Storage space needed for all trainable variables: 167.63MB
[0722 08:15:59 @base.py:209] Setup callbacks graph ...
[0722 08:16:12 @prof.py:271] [HostMemoryTracker] Free RAM in setup_graph() is 54.69 GB.
[0722 08:16:12 @tower.py:140] Building graph for predict tower 'tower-pred-0' on device /gpu:0 ...
[0722 08:16:13 @collection.py:152] Size of these collections were changed in tower-pred-0: (tf.GraphKeys.MODEL_VARIABLES: 238->293)
[0722 08:16:13 @tower.py:140] Building graph for predict tower 'tower-pred-1' on device /gpu:1 with variable scope 'tower1'...
[0722 08:16:15 @collection.py:152] Size of these collections were changed in tower-pred-1: (tf.GraphKeys.MODEL_VARIABLES: 293->348)
loading annotations into memory...
Done (t=0.69s)
creating index...
index created!
[0722 08:16:16 @coco.py:68] Instances loaded from /home/federicolondon2019/tensorpack/COCO/DIR/annotations/instances_val2017.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 703/703 [00:00<00:00, 194665.33it/s]
[0722 08:16:16 @timer.py:50] Load annotations for instances_val2017.json finished, time:0.0045 sec.
[0722 08:16:16 @data.py:372] Found 703 images for inference.
loading annotations into memory...
Done (t=0.31s)
creating index...
index created!
[0722 08:16:16 @coco.py:68] Instances loaded from /home/federicolondon2019/tensorpack/COCO/DIR/annotations/instances_val2017.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 703/703 [00:00<00:00, 550111.14it/s]
[0722 08:16:16 @timer.py:50] Load annotations for instances_val2017.json finished, time:0.0020 sec.
[0722 08:16:16 @data.py:372] Found 703 images for inference.
loading annotations into memory...
Done (t=1.11s)
creating index...
index created!
[0722 08:16:18 @coco.py:68] Instances loaded from /home/federicolondon2019/tensorpack/COCO/DIR/annotations/instances_val2017.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 703/703 [00:00<00:00, 519393.29it/s]
[0722 08:16:18 @timer.py:50] Load annotations for instances_val2017.json finished, time:0.0021 sec.
[0722 08:16:18 @data.py:372] Found 703 images for inference.
loading annotations into memory...
Done (t=0.32s)
creating index...
index created!
[0722 08:16:18 @coco.py:68] Instances loaded from /home/federicolondon2019/tensorpack/COCO/DIR/annotations/instances_val2017.json.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 703/703 [00:00<00:00, 526253.03it/s]
[0722 08:16:18 @timer.py:50] Load annotations for instances_val2017.json finished, time:0.0021 sec.
[0722 08:16:18 @data.py:372] Found 703 images for inference.
[0722 08:16:18 @summary.py:47] [MovingAverageSummary] 77 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.
[0722 08:16:18 @summary.py:94] Summarizing collection 'summaries' of size 80.
[0722 08:16:18 @base.py:230] Creating the session ...
2019-07-22 08:16:18.499851: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-22 08:16:21.504443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA n
ode zero
2019-07-22 08:16:21.506710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA n
ode zero
2019-07-22 08:16:21.542882: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55a96a0d21f0 executing computations on platform CUDA. Devices:
2019-07-22 08:16:21.542936: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5
2019-07-22 08:16:21.542944: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5
2019-07-22 08:16:22.034470: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-07-22 08:16:22.036143: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55a96729f130 executing computations on platform Host. Devices:
2019-07-22 08:16:22.036203: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-22 08:16:22.036519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59
pciBusID: 0000:00:04.0
totalMemory: 14.73GiB freeMemory: 14.60GiB
2019-07-22 08:16:22.036717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties: 
name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59
pciBusID: 0000:00:05.0
totalMemory: 14.73GiB freeMemory: 14.60GiB
2019-07-22 08:16:22.040814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1
2019-07-22 08:16:22.199640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-22 08:16:22.199710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 
2019-07-22 08:16:22.199719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N Y 
2019-07-22 08:16:22.199723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   Y N 
2019-07-22 08:16:22.200156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14928 MB memory) -> physical GPU (device: 0, name
: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
2019-07-22 08:16:22.200911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14928 MB memory) -> physical GPU (device: 1, name
: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5)
[0722 08:16:53 @base.py:236] Initializing the session ...
[0722 08:16:53 @sessinit.py:209] Variables to restore from dict: conv0/W, conv0/bn/gamma, conv0/bn/beta, conv0/bn/mean/EMA, conv0/bn/variance/EMA, group0/block0/conv1/W, group0/block0/conv1/bn/gamma, group0/bloc
k0/conv1/bn/beta, group0/block0/conv1/bn/mean/EMA, group0/block0/conv1/bn/variance/EMA, group0/block0/conv2/W, group0/block0/conv2/bn/gamma, group0/block0/conv2/bn/beta, group0/block0/conv2/bn/mean/EMA, group0/b
lock0/conv2/bn/variance/EMA, group0/block0/conv3/W, group0/block0/conv3/bn/gamma, group0/block0/conv3/bn/beta, group0/block0/conv3/bn/mean/EMA, group0/block0/conv3/bn/variance/EMA, group0/block0/convshortcut/W, 
group0/block0/convshortcut/bn/gamma, group0/block0/convshortcut/bn/beta, group0/block0/convshortcut/bn/mean/EMA, group0/block0/convshortcut/bn/variance/EMA, group0/block1/conv1/W, group0/block1/conv1/bn/gamma, g
roup0/block1/conv1/bn/beta, group0/block1/conv1/bn/mean/EMA, group0/block1/conv1/bn/variance/EMA, group0/block1/conv2/W, group0/block1/conv2/bn/gamma, group0/block1/conv2/bn/beta, group0/block1/conv2/bn/mean/EMA
, group0/block1/conv2/bn/variance/EMA, group0/block1/conv3/W, group0/block1/conv3/bn/gamma, group0/block1/conv3/bn/beta, group0/block1/conv3/bn/mean/EMA, group0/block1/conv3/bn/variance/EMA, group0/block2/conv1/
W, group0/block2/conv1/bn/gamma, group0/block2/conv1/bn/beta, group0/block2/conv1/bn/mean/EMA, group0/block2/conv1/bn/variance/EMA, group0/block2/conv2/W, group0/block2/conv2/bn/gamma, group0/block2/conv2/bn/bet
a, group0/block2/conv2/bn/mean/EMA, group0/block2/conv2/bn/variance/EMA, group0/block2/conv3/W, group0/block2/conv3/bn/gamma, group0/block2/conv3/bn/beta, group0/block2/conv3/bn/mean/EMA, group0/block2/conv3/bn/
variance/EMA, group1/block0/conv1/W, group1/block0/conv1/bn/gamma, group1/block0/conv1/bn/beta, group1/block0/conv1/bn/mean/EMA, group1/block0/conv1/bn/variance/EMA, group1/block0/conv2/W, group1/block0/conv2/bn
/gamma, group1/block0/conv2/bn/beta, group1/block0/conv2/bn/mean/EMA, group1/block0/conv2/bn/variance/EMA, group1/block0/conv3/W, group1/block0/conv3/bn/gamma, group1/block0/conv3/bn/beta, group1/block0/conv3/bn
/mean/EMA, group1/block0/conv3/bn/variance/EMA, group1/block0/convshortcut/W, group1/block0/convshortcut/bn/gamma, group1/block0/convshortcut/bn/beta, group1/block0/convshortcut/bn/mean/EMA, group1/block0/convsh
ortcut/bn/variance/EMA, group1/block1/conv1/W, group1/block1/conv1/bn/gamma, group1/block1/conv1/bn/beta, group1/block1/conv1/bn/mean/EMA, group1/block1/conv1/bn/variance/EMA, group1/block1/conv2/W, group1/block
1/conv2/bn/gamma, group1/block1/conv2/bn/beta, group1/block1/conv2/bn/mean/EMA, group1/block1/conv2/bn/variance/EMA, group1/block1/conv3/W, group1/block1/conv3/bn/gamma, group1/block1/conv3/bn/beta, group1/block
1/conv3/bn/mean/EMA, group1/block1/conv3/bn/variance/EMA, group1/block2/conv1/W, group1/block2/conv1/bn/gamma, group1/block2/conv1/bn/beta, group1/block2/conv1/bn/mean/EMA, group1/block2/conv1/bn/variance/EMA, g
roup1/block2/conv2/W, group1/block2/conv2/bn/gamma, group1/block2/conv2/bn/beta, group1/block2/conv2/bn/mean/EMA, group1/block2/conv2/bn/variance/EMA, group1/block2/conv3/W, group1/block2/conv3/bn/gamma, group1/
block2/conv3/bn/beta, group1/block2/conv3/bn/mean/EMA, group1/block2/conv3/bn/variance/EMA, group1/block3/conv1/W, group1/block3/conv1/bn/gamma, group1/block3/conv1/bn/beta, group1/block3/conv1/bn/mean/EMA, grou
p1/block3/conv1/bn/variance/EMA, group1/block3/conv2/W, group1/block3/conv2/bn/gamma, group1/block3/conv2/bn/beta, group1/block3/conv2/bn/mean/EMA, group1/block3/conv2/bn/variance/EMA, group1/block3/conv3/W, gro
up1/block3/conv3/bn/gamma, group1/block3/conv3/bn/beta, group1/block3/conv3/bn/mean/EMA, group1/block3/conv3/bn/variance/EMA, group2/block0/conv1/W, group2/block0/conv1/bn/gamma, group2/block0/conv1/bn/beta, gro
up2/block0/conv1/bn/mean/EMA, group2/block0/conv1/bn/variance/EMA, group2/block0/conv2/W, group2/block0/conv2/bn/gamma, group2/block0/conv2/bn/beta, group2/block0/conv2/bn/mean/EMA, group2/block0/conv2/bn/varian
ce/EMA, group2/block0/conv3/W, group2/block0/conv3/bn/gamma, group2/block0/conv3/bn/beta, group2/block0/conv3/bn/mean/EMA, group2/block0/conv3/bn/variance/EMA, group2/block0/convshortcut/W, group2/block0/convsho
rtcut/bn/gamma, group2/block0/convshortcut/bn/beta, group2/block0/convshortcut/bn/mean/EMA, group2/block0/convshortcut/bn/variance/EMA, group2/block1/conv1/W, group2/block1/conv1/bn/gamma, group2/block1/conv1/bn
/beta, group2/block1/conv1/bn/mean/EMA, group2/block1/conv1/bn/variance/EMA, group2/block1/conv2/W, group2/block1/conv2/bn/gamma, group2/block1/conv2/bn/beta, group2/block1/conv2/bn/mean/EMA, group2/block1/conv2
/bn/variance/EMA, group2/block1/conv3/W, group2/block1/conv3/bn/gamma, group2/block1/conv3/bn/beta, group2/block1/conv3/bn/mean/EMA, group2/block1/conv3/bn/variance/EMA, group2/block2/conv1/W, group2/block2/conv
1/bn/gamma, group2/block2/conv1/bn/beta, group2/block2/conv1/bn/mean/EMA, group2/block2/conv1/bn/variance/EMA, group2/block2/conv2/W, group2/block2/conv2/bn/gamma, group2/block2/conv2/bn/beta, group2/block2/conv
2/bn/mean/EMA, group2/block2/conv2/bn/variance/EMA, group2/block2/conv3/W, group2/block2/conv3/bn/gamma, group2/block2/conv3/bn/beta, group2/block2/conv3/bn/mean/EMA, group2/block2/conv3/bn/variance/EMA, group2/
block3/conv1/W, group2/block3/conv1/bn/gamma, group2/block3/conv1/bn/beta, group2/block3/conv1/bn/mean/EMA, group2/block3/conv1/bn/variance/EMA, group2/block3/conv2/W, group2/block3/conv2/bn/gamma, group2/block3
/conv2/bn/beta, group2/block3/conv2/bn/mean/EMA, group2/block3/conv2/bn/variance/EMA, group2/block3/conv3/W, group2/block3/conv3/bn/gamma, group2/block3/conv3/bn/beta, group2/block3/conv3/bn/mean/EMA, group2/blo
ck3/conv3/bn/variance/EMA, group2/block4/conv1/W, group2/block4/conv1/bn/gamma, group2/block4/conv1/bn/beta, group2/block4/conv1/bn/mean/EMA, group2/block4/conv1/bn/variance/EMA, group2/block4/conv2/W, group2/bl
ock4/conv2/bn/gamma, group2/block4/conv2/bn/beta, group2/block4/conv2/bn/mean/EMA, group2/block4/conv2/bn/variance/EMA, group2/block4/conv3/W, group2/block4/conv3/bn/gamma, group2/block4/conv3/bn/beta, group2/bl
ock4/conv3/bn/mean/EMA, group2/block4/conv3/bn/variance/EMA, group2/block5/conv1/W, group2/block5/conv1/bn/gamma, group2/block5/conv1/bn/beta, group2/block5/conv1/bn/mean/EMA, group2/block5/conv1/bn/variance/EMA
, group2/block5/conv2/W, group2/block5/conv2/bn/gamma, group2/block5/conv2/bn/beta, group2/block5/conv2/bn/mean/EMA, group2/block5/conv2/bn/variance/EMA, group2/block5/conv3/W, group2/block5/conv3/bn/gamma, grou
p2/block5/conv3/bn/beta, group2/block5/conv3/bn/mean/EMA, group2/block5/conv3/bn/variance/EMA, group3/block0/conv1/W, group3/block0/conv1/bn/gamma, group3/block0/conv1/bn/beta, group3/block0/conv1/bn/mean/EMA, g
roup3/block0/conv1/bn/variance/EMA, group3/block0/conv2/W, group3/block0/conv2/bn/gamma, group3/block0/conv2/bn/beta, group3/block0/conv2/bn/mean/EMA, group3/block0/conv2/bn/variance/EMA, group3/block0/conv3/W, 
group3/block0/conv3/bn/gamma, group3/block0/conv3/bn/beta, group3/block0/conv3/bn/mean/EMA, group3/block0/conv3/bn/variance/EMA, group3/block0/convshortcut/W, group3/block0/convshortcut/bn/gamma, group3/block0/c
onvshortcut/bn/beta, group3/block0/convshortcut/bn/mean/EMA, group3/block0/convshortcut/bn/variance/EMA, group3/block1/conv1/W, group3/block1/conv1/bn/gamma, group3/block1/conv1/bn/beta, group3/block1/conv1/bn/m
ean/EMA, group3/block1/conv1/bn/variance/EMA, group3/block1/conv2/W, group3/block1/conv2/bn/gamma, group3/block1/conv2/bn/beta, group3/block1/conv2/bn/mean/EMA, group3/block1/conv2/bn/variance/EMA, group3/block1
/conv3/W, group3/block1/conv3/bn/gamma, group3/block1/conv3/bn/beta, group3/block1/conv3/bn/mean/EMA, group3/block1/conv3/bn/variance/EMA, group3/block2/conv1/W, group3/block2/conv1/bn/gamma, group3/block2/conv1
/bn/beta, group3/block2/conv1/bn/mean/EMA, group3/block2/conv1/bn/variance/EMA, group3/block2/conv2/W, group3/block2/conv2/bn/gamma, group3/block2/conv2/bn/beta, group3/block2/conv2/bn/mean/EMA, group3/block2/co
nv2/bn/variance/EMA, group3/block2/conv3/W, group3/block2/conv3/bn/gamma, group3/block2/conv3/bn/beta, group3/block2/conv3/bn/mean/EMA, group3/block2/conv3/bn/variance/EMA
[0722 08:16:53 @sessinit.py:87] WRN The following variables are in the graph, but not found in the dict: fastrcnn/fc6/W, fastrcnn/fc6/b, fastrcnn/fc7/W, fastrcnn/fc7/b, fastrcnn/outputs/box/W, fastrcnn/outputs/b
ox/b, fastrcnn/outputs/class/W, fastrcnn/outputs/class/b, fpn/lateral_1x1_c2/W, fpn/lateral_1x1_c2/b, fpn/lateral_1x1_c3/W, fpn/lateral_1x1_c3/b, fpn/lateral_1x1_c4/W, fpn/lateral_1x1_c4/b, fpn/lateral_1x1_c5/W,
 fpn/lateral_1x1_c5/b, fpn/posthoc_3x3_p2/W, fpn/posthoc_3x3_p2/b, fpn/posthoc_3x3_p3/W, fpn/posthoc_3x3_p3/b, fpn/posthoc_3x3_p4/W, fpn/posthoc_3x3_p4/b, fpn/posthoc_3x3_p5/W, fpn/posthoc_3x3_p5/b, global_step,
 learning_rate, maskrcnn/conv/W, maskrcnn/conv/b, maskrcnn/deconv/W, maskrcnn/deconv/b, maskrcnn/fcn0/W, maskrcnn/fcn0/b, maskrcnn/fcn1/W, maskrcnn/fcn1/b, maskrcnn/fcn2/W, maskrcnn/fcn2/b, maskrcnn/fcn3/W, mask
rcnn/fcn3/b, rpn/box/W, rpn/box/b, rpn/class/W, rpn/class/b, rpn/conv0/W, rpn/conv0/b
[0722 08:16:53 @sessinit.py:87] WRN The following variables are in the dict, but not found in the graph: linear/W, linear/b
[0722 08:16:53 @sessinit.py:222] Restoring 265 variables from dict ...
[0722 08:16:53 @base.py:243] Graph Finalized.
[0722 08:16:54 @concurrency.py:37] Starting EnqueueThread QueueInput/input_queue ...
[0722 08:16:54 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
[0722 08:17:02 @param.py:158] [HyperParamSetter] At global_step=0, learning_rate is set to 0.003300
[0722 08:17:04 @prof.py:274] [HostMemoryTracker] Free RAM in before_train() is 49.09 GB.
[0722 08:17:04 @eval.py:259] [EvalCallback] Will evaluate every 25 epochs
[0722 08:17:05 @base.py:275] Start Epoch 1 ...
  0%|                                                                                                                                                                                         |0/500[00:00<?,?it/s]
[0722 08:17:05 @input_source.py:556] Pre-filling StagingArea ...
[0722 08:17:06 @input_source.py:560] 1 element was put into StagingArea on each tower.
2019-07-22 08:17:40.296580: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
[0722 08:17:53 @param.py:161] [HyperParamSetter] At global_step=1, learning_rate changes from 0.003300 to 0.003307
 55%|################################################################################################2      
```
",hi found error issue pip install upgrade instead pip install upgrade appear version log sure appear clone repository working happy thank much log case useful anyone python environment information python default compiler version support true support false driver none free ram count true false false false true back front false true true warm schedule value schedule value loading memory done index index loaded load finished time sec category distribution class box class box class box bird person bicyclist motorcyclist banner bench billboard mailbox manhole pole back front bicycle boat bus car caravan motorcycle trailer truck total contain total training total training set none monitor visible training model automatically setting queue building graph training tower device training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training training input output input output input output input output input output found regularize following mask converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown building graph training tower device found regularize size tower tower list trainable name shape number trainable number storage space trainable setup graph free ram building graph predict tower device size building graph predict tower device variable scope size loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference collection run session collection size session binary use successful node read negative value must least one node ode zero successful node read negative value must least one node ode zero service platform device compute capability device compute capability frequency service platform host device undefined undefined found device name major minor found device name major minor visible device interconnect strength edge matrix device memory physical device name bus id compute capability device memory physical device name bus id compute capability session restore following graph found mask following found graph graph starting running set free ram evaluate every start epoch element put tower successfully library locally,issue,positive,positive,neutral,neutral,positive,positive
513548257,"Hi, 

I've been trying different ways but still got the same issue:
```
ThroughputTracker(samples_per_step=cfg.TRAIN.NUM_GPUS),
NameError: name 'ThroughputTracker' is not defined
```
Firstly I tried updating tensorpack (I am working on Google cloud):

```
federicolondon2019@instance-1:~/tensorpack/examples/FasterRCNN$ pip install --upgrade git+https://github.com/tensorpack/tensorpack.git
Collecting git+https://github.com/tensorpack/tensorpack.git
  Cloning https://github.com/tensorpack/tensorpack.git to /tmp/pip-P9VYhg-build
Collecting numpy>=1.14 (from tensorpack==0.9.5)
  Using cached https://files.pythonhosted.org/packages/1f/c7/198496417c9c2f6226616cff7dedf2115a4f4d0276613bab842ec8ac1e23/numpy-1.16.4-cp27-cp27mu-manylinux1_x86_64.whl
Collecting six (from tensorpack==0.9.5)
  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl
Collecting termcolor>=1.1 (from tensorpack==0.9.5)
Collecting tabulate>=0.7.7 (from tensorpack==0.9.5)
Collecting tqdm>4.29.0 (from tensorpack==0.9.5)
  Using cached https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl
Collecting msgpack>=0.5.2 (from tensorpack==0.9.5)
  Using cached https://files.pythonhosted.org/packages/6f/74/06f950e5a4c1fb94b676640b8fea839b80bffdc247144bd054dd5eed250e/msgpack-0.6.1-cp27-cp27mu-manylinux1_x86_64.whl
Collecting msgpack-numpy>=0.4.4.2 (from tensorpack==0.9.5)
  Using cached https://files.pythonhosted.org/packages/c8/ab/09904a909bccc471f219fb8f5d0838cbcb10cc26089a2b29e84c893e216e/msgpack_numpy-0.4.4.3-py2.py3-none-any.whl
Collecting pyzmq>=16 (from tensorpack==0.9.5)
  Using cached https://files.pythonhosted.org/packages/8e/92/e054852c15f9571909d313541e8e2f102d0cd5fff3a29e4ed7a1bb4f847b/pyzmq-18.0.2-cp27-cp27mu-manylinux1_x86_64.whl
Collecting psutil>=5 (from tensorpack==0.9.5)
Collecting subprocess32 (from tensorpack==0.9.5)
Collecting functools32 (from tensorpack==0.9.5)
Installing collected packages: numpy, six, termcolor, tabulate, tqdm, msgpack, msgpack-numpy, pyzmq, psutil, subprocess32, functools32, tensorpack
  Running setup.py install for tensorpack ... done
Successfully installed functools32-3.2.3.post2 msgpack-0.6.1 msgpack-numpy-0.4.4.3 numpy-1.16.4 psutil-5.6.3 pyzmq-18.0.2 six-1.12.0 subprocess32-3.5.4 tabulate-0.8.3 tensorpac
k-0.9.5 termcolor-1.1.0 tqdm-4.32.2
```
I got the same error when running trainning so I cloned again the latest version of tensorpack into my environment: 

```
federicolondon2019@instance-1:~$ git clone https://github.com/tensorpack/tensorpack.git
Cloning into 'tensorpack'...
remote: Enumerating objects: 92, done.
# -*- coding: utf-8 -*-
remote: Counting objects: 100% (92/92), done.
# -*- coding: utf-8 -*-
remote: Compressing objects: 100% (66/66), done.
remote: Total 27076 (delta 40), reused 45 (delta 26), pack-reused 26984
Receiving objects: 100% (27076/27076), 8.38 MiB | 0 bytes/s, done.
Resolving deltas: 100% (21062/21062), done.
```
I changed coco.py and config files to match my won dataset and I run trainning but I got the same error, what am I doing wrong? 

Many thanks. 

```
federicolondon2019@instance-1:~/tensorpack/examples/FasterRCNN$ python3 train.py --config     MODE_MASK=True MODE_FPN=True     DATA.BASEDIR=/home/federicolondon2019/tensorpack/
COCO/DIR     BACKBONE.WEIGHTS=/home/federicolondon2019/tensorpack/models/ImageNet-R50-AlignPadding.npz
[0721 12:07:21 @logger.py:90] Argv: train.py --config MODE_MASK=True MODE_FPN=True DATA.BASEDIR=/home/federicolondon2019/tensorpack/COCO/DIR BACKBONE.WEIGHTS=/home/federicolond
on2019/tensorpack/models/ImageNet-R50-AlignPadding.npz
[0721 12:07:23 @train.py:55] Environment Information:
--------------------  -----------------------------------------------------------
sys.platform          linux
Python                3.5.3 (default, Sep 27 2018, 17:25:39) [GCC 6.3.0 20170516]
Tensorpack            v0.9.4-37-g59829770-dirty
Numpy                 1.16.3
TensorFlow            1.13.1/b'v1.13.1-0-g6612da8951'
TF Compiler Version   4.8.5
TF CUDA support       True
TF MKL support        False
Nvidia Driver         /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.410.72
CUDA                  /usr/local/cuda-10.0/lib64/libcudart.so.10.0.130
CUDNN                 /usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1
NCCL                  /usr/local/nccl2/lib/libnccl.so.2.3.4
CUDA_VISIBLE_DEVICES  None
GPU 0,1               Tesla T4
Free RAM              57.87/58.99 GB
CPU Count             16
horovod               0.16.0
cv2                   4.1.0
msgpack               0.6.1
python-prctl          True
--------------------  -----------------------------------------------------------
[0721 12:07:24 @config.py:281] Config: ------------------------------------------
{'BACKBONE': {'FREEZE_AFFINE': False,
              'FREEZE_AT': 2,
              'NORM': 'FreezeBN',
              'RESNET_NUM_BLOCKS': [3, 4, 6, 3],
              'STRIDE_1X1': False,
              'TF_PAD_MODE': False,
              'WEIGHTS': '/home/federicolondon2019/tensorpack/models/ImageNet-R50-AlignPadding.npz'},
 'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0],
                                  [30.0, 30.0, 15.0, 15.0]],
             'IOUS': [0.5, 0.6, 0.7]},
 'DATA': {'ABSOLUTE_COORD': True,
          'BASEDIR': '/home/federicolondon2019/tensorpack/COCO/DIR',
          'CLASS_NAMES': ['BG', 'Bird', 'Ground_Animal', 'Crosswalk_Plain', 'Person', 'Bicyclist',
                          'Motorcyclist', 'Other_Rider', 'Lane_Marking_-_Crosswalk', 'Banner',
                          'Bench', 'Bike_Rack', 'Billboard', 'Catch_Basin', 'CCTV_Camera',
                          'Fire_Hydrant', 'Junction_Box', 'Mailbox', 'Manhole', 'Phone_Booth',
                          'Street_Light', 'Pole', 'Traffic_Sign_Frame', 'Utility_Pole',
                          'Traffic_Light', 'Traffic_Sign_(Back)', 'Traffic_Sign_(Front)',
                          'Trash_Can', 'Bicycle', 'Boat', 'Bus', 'Car', 'Caravan', 'Motorcycle',
                          'Other_Vehicle', 'Trailer', 'Truck', 'Wheeled_Slow'],
          'NUM_CATEGORY': 37,
          'NUM_WORKERS': 10,
          'TRAIN': ('train2017',),
          'VAL': ('val2017',)},
 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),
         'CASCADE': False,
         'FRCNN_CONV_HEAD_DIM': 256,
         'FRCNN_FC_HEAD_DIM': 1024,
         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',
         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',
         'NORM': 'None',
         'NUM_CHANNEL': 256,
         'PROPOSAL_MODE': 'Level',
         'RESOLUTION_REQUIREMENT': 32},
 'FRCNN': {'BATCH_PER_IM': 512,
           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],
           'FG_RATIO': 0.25,
           'FG_THRESH': 0.5},
 'MODE_FPN': True,
 'MODE_MASK': True,
 'MRCNN': {'HEAD_DIM': 256},
 'PREPROC': {'MAX_SIZE': 1344.0,
             'PIXEL_MEAN': [123.675, 116.28, 103.53],
             'PIXEL_STD': [58.395, 57.12, 57.375],
             'TEST_SHORT_EDGE_SIZE': 800,
             'TRAIN_SHORT_EDGE_SIZE': [800, 800]},
 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),
         'ANCHOR_SIZES': (32, 64, 128, 256, 512),
         'ANCHOR_STRIDE': 16,
         'BATCH_PER_IM': 256,
         'CROWD_OVERLAP_THRESH': 9.99,
         'FG_RATIO': 0.5,
         'HEAD_DIM': 1024,
         'MIN_SIZE': 0,
         'NEGATIVE_ANCHOR_THRESH': 0.3,
         'NUM_ANCHOR': 15,
         'POSITIVE_ANCHOR_THRESH': 0.7,
         'PROPOSAL_NMS_THRESH': 0.7,
         'TEST_PER_LEVEL_NMS_TOPK': 1000,
         'TEST_POST_NMS_TOPK': 1000,
         'TEST_PRE_NMS_TOPK': 6000,
         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,
         'TRAIN_POST_NMS_TOPK': 2000,
         'TRAIN_PRE_NMS_TOPK': 12000},
 'TEST': {'FRCNN_NMS_THRESH': 0.5,
          'RESULTS_PER_IM': 100,
          'RESULT_SCORE_THRESH': 0.05,
          'RESULT_SCORE_THRESH_VIS': 0.5},
 'TRAIN': {'BASE_LR': 0.01,
           'EVAL_PERIOD': 25,
           'LR_SCHEDULE': [120000, 160000, 180000],
           'NUM_GPUS': 2,
           'STARTING_EPOCH': 1,
           'STEPS_PER_EPOCH': 500,
           'WARMUP': 1000,
           'WARMUP_INIT_LR': 0.0033000000000000004,
           'WEIGHT_DECAY': 0.0001},
 'TRAINER': 'replicated'}
[0721 12:07:24 @train.py:72] Warm Up Schedule (steps, value): [(0, 0.0033000000000000004), (1000, 0.01)]
[0721 12:07:24 @train.py:73] LR Schedule (epochs, value): [(2, 0.01), (960.0, 0.001), (1280.0, 0.00010000000000000002)]
loading annotations into memory...
Done (t=11.71s)
creating index...
index created!
[0721 12:07:37 @coco.py:68] Instances loaded from /home/federicolondon2019/tensorpack/COCO/DIR/annotations/instances_train2017.json.
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18000/18000 [00:12<00:00, 1406.50it/s]
[0721 12:07:50 @timer.py:50] Load annotations for instances_train2017.json finished, time:12.8611 sec.
[0721 12:07:51 @data.py:59] Ground-Truth category distribution:
|      class      | #box   |        class        | #box   |          class           | #box   |
|:---------------:|:-------|:-------------------:|:-------|:------------------------:|:-------|
|       BG        | 1146   |        Bird         | 312    |      Ground_Animal       | 0      |
| Crosswalk_Plain | 44340  |       Person        | 2902   |        Bicyclist         | 3167   |
|  Motorcyclist   | 229    |     Other_Rider     | 9047   | Lane_Marking_-_Crosswalk | 7808   |
|     Banner      | 1539   |        Bench        | 1335   |        Bike_Rack         | 64995  |
|    Billboard    | 5964   |     Catch_Basin     | 5258   |       CCTV_Camera        | 1722   |
|  Fire_Hydrant   | 4412   |    Junction_Box     | 688    |         Mailbox          | 7997   |
|     Manhole     | 263    |     Phone_Booth     | 106093 |       Street_Light       | 291445 |
|      Pole       | 2858   | Traffic_Sign_Frame  | 0      |       Utility_Pole       | 59384  |
|  Traffic_Light  | 31815  | Traffic_Sign_(Back) | 96168  |   Traffic_Sign_(Front)   | 5726   |
|    Trash_Can    | 5447   |       Bicycle       | 442    |           Boat           | 3787   |
|       Bus       | 115615 |         Car         | 0      |         Caravan          | 4765   |
|   Motorcycle    | 1157   |    Other_Vehicle    | 187    |         Trailer          | 5851   |
|      Truck      | 705    |                     |        |                          |        |
|      total      | 894569 |                     |        |                          |        |
[0721 12:07:52 @data.py:344] Filtered 2789 images which contain no non-crowd groudtruth boxes. Total #images for training: 15211
[0721 12:07:52 @train.py:77] Total passes of the training set is: 94.668
Traceback (most recent call last):
  File ""train.py"", line 92, in <module>
    ThroughputTracker(samples_per_step=cfg.TRAIN.NUM_GPUS),
NameError: name 'ThroughputTracker' is not defined
```",hi trying different way still got issue name defined firstly tried working cloud pip install upgrade six tabulate collected six tabulate running install done successfully post got error running latest version environment git clone remote done remote counting done remote done remote total delta delta mib done done match run got error wrong many thanks python environment information python default compiler version support true support false driver none free ram count true false false false true back front false true true warm schedule value schedule value loading memory done index index loaded load finished time sec category distribution class box class box class box bird person bicyclist motorcyclist banner bench billboard mailbox manhole pole back front bicycle boat bus car caravan motorcycle trailer truck total contain total training total training set recent call last file line module name defined,issue,positive,positive,neutral,neutral,positive,positive
513504694,"Hi, 

Thanks, it seems now is loading the annotation but I am getting this error: 

```
[0720 22:30:07 @timer.py:50] Load annotations for instances_train2017.json finished, time:12.5990 sec.
[0720 22:30:08 @data.py:57] Ground-Truth category distribution:
|        class        | #box   |          class           | #box   |      class      | #box   |
|:-------------------:|:-------|:------------------------:|:-------|:---------------:|:-------|
|        Bird         | 1146   |      Ground_Animal       | 312    | Crosswalk_Plain | 0      |
|       Person        | 44340  |        Bicyclist         | 2902   |  Motorcyclist   | 3167   |
|     Other_Rider     | 229    | Lane_Marking_-_Crosswalk | 9047   |     Banner      | 7808   |
|        Bench        | 1539   |        Bike_Rack         | 1335   |    Billboard    | 64995  |
|     Catch_Basin     | 5964   |       CCTV_Camera        | 5258   |  Fire_Hydrant   | 1722   |
|    Junction_Box     | 4412   |         Mailbox          | 688    |     Manhole     | 7997   |
|     Phone_Booth     | 263    |       Street_Light       | 106093 |      Pole       | 291445 |
| Traffic_Sign_Frame  | 2858   |       Utility_Pole       | 0      |  Traffic_Light  | 59384  |
| Traffic_Sign_(Back) | 31815  |   Traffic_Sign_(Front)   | 96168  |    Trash_Can    | 5726   |
|       Bicycle       | 5447   |           Boat           | 442    |       Bus       | 3787   |
|         Car         | 115615 |         Caravan          | 0      |   Motorcycle    | 4765   |
|    Other_Vehicle    | 1157   |         Trailer          | 187    |      Truck      | 5851   |
|    Wheeled_Slow     | 705    |                          |        |                 |        |
|        total        | 894569 |                          |        |                 |        |
[0720 22:30:08 @data.py:400] Filtered 2789 images which contain no non-crowd groudtruth boxes. Total #images for training: 15211
[0720 22:30:08 @train.py:77] Total passes of the training set is: 94.668
Traceback (most recent call last):
  File ""train.py"", line 92, in <module>
    ThroughputTracker(samples_per_step=cfg.TRAIN.NUM_GPUS),
NameError: name 'ThroughputTracker' is not defined
```
I tried to set cfg.TRAIN.NUM_GPUS=2 in config.py but it doesn't work

Let me know if you need more information so I can fill the template and open a new issue.

Many thanks.
",hi thanks loading annotation getting error load finished time sec category distribution class box class box class box bird person bicyclist motorcyclist banner bench billboard mailbox manhole pole back front bicycle boat bus car caravan motorcycle trailer truck total contain total training total training set recent call last file line module name defined tried set work let know need information fill template open new issue many thanks,issue,negative,positive,neutral,neutral,positive,positive
513502426,"You mean I made the mistake in config.py:
_C.DATA.TRAIN should be (""train2017"", ) instead of (""train2017"")?

Thanks",mean made mistake train instead train thanks,issue,negative,negative,neutral,neutral,negative,negative
513074381,"Add the tensor you want to evaluate to:
`output_names=['wrong-top1']`.
Then `pred()` will return it.",add tensor want evaluate return,issue,negative,neutral,neutral,neutral,neutral,neutral
513065501,"I am sorry but I still don't get it. I implement the ""OfflinePredictor"" like this:

def eval_classification(model, sessinit, dataflow):
    pred_config = PredictConfig(
        model=model,
        session_init=sessinit,
        input_names=['input', 'label'],
        output_names=['wrong-top1']
    )
    acc1 = RatioCounter()
    pred = FeedfreePredictor(pred_config, StagingInput(QueueInput(dataflow), device='/gpu:0'))
    for _ in tqdm.trange(dataflow.size()):
        top1 = pred()[0]
        batch_size = top1.shape[0]
        acc1.feed(top1.sum(), batch_size)

Then, I call the model using this:
`
python ./tensorpack/examples/DoReFa-Net-eidt/svhn-digit-dorefa.py --dorefa 1,2,32 --load ./dorefa_log/svhn-dorefa-1,2,32/model-4721 --eval
`

But the 'DumpTensor' callback doesn't work. It seems that 'PredictConfig' doesn't have 'Callbacks' parameter, so how can I get the tensor value without ""DumpTensor"".
Thansk a lot   
",sorry still get implement like model top call model python load work parameter get tensor value without lot,issue,positive,neutral,neutral,neutral,neutral,neutral
512924063,"See
https://tensorpack.readthedocs.io/tutorial/inference.html#inference-after-training-what-tensorpack-does

On Thu, Jul 18, 2019 at 10:59 AM tianzhenyu <notifications@github.com>
wrote:

> I am using ""DumpTensors"" to retrieve tensor values duing Training. How can
> I get the same tensor's value during inference? As far as I know, this
> callbacks only works during training.
> Thanks a lot.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/1274?email_source=notifications&email_token=AAKRHNOGZZYV4RE7WXVA63LQACVPNA5CNFSM4IE6HBS2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4HACI5BQ>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAKRHNMBCXEDAAPLKT3JOV3QACVPNANCNFSM4IE6HBSQ>
> .
>
",see wrote retrieve tensor training get tensor value inference far know work training thanks lot thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
512903422,"> It is just that without MultiProcessRunner the whole data flow got forked by many subsequent MultiProcessRunnerZMQ's such that each of them has their own LocallyShuffleData

I did not know you're using them together so I was not able to comment anything on that. 

For the same reason, please ask questions with details: please ask questions with what you did and what you observed. Please include what you did and what you observed by pasting code & logs/screenshots, not by words. The [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/unexpected-problems---bugs.md) will help you include necessary information.
If you have trouble improving the performance, please include details following the [performance guide](https://tensorpack.readthedocs.io/tutorial/performance-tuning.html)
If you want to ask a usage question, please also describe clearly what you want to achieve.

The discussion that is going on in this thread seems heavily tied to the specific dataflow pipeline you're using. Without details I cannot comment anything on it.",without whole data flow got forked many subsequent know together able comment anything reason please ask please ask please include pasting code issue template help include necessary information trouble improving performance please include following performance guide want ask usage question please also describe clearly want achieve discussion going thread heavily tied specific pipeline without comment anything,issue,positive,positive,neutral,neutral,positive,positive
512816384,"By the way removing `MultiProcessRunner` seems unreasonable as suggested in 😢: https://tensorpack.readthedocs.io/tutorial/efficient-dataflow.html

> Since we are reading the database sequentially, having multiple forked instances of the
base LMDB reader will result in biased data distribution. Therefore we use MultiProcessRunner to
launch the base DataFlow in only one process, ...

",way removing unreasonable since reading sequentially multiple forked base reader result data distribution therefore use launch base one process,issue,negative,negative,negative,negative,negative,negative
512813311,"@Rov67777 It seems reasonable to think so, but it doesn't work I just tested. I think there needs to be a different kind of flow that some how ""joins"".",reasonable think work tested think need different kind flow,issue,positive,positive,positive,positive,positive,positive
512806295,My understanding is that if you apply the **LocallyShuffleData** after the **MultiProcessRunnerZMQ** with num_proc=K it should only has one **LocallyShuffleData** that gets feed by K forked process of **MultiProcessRunnerZMQ**?,understanding apply one feed forked process,issue,negative,neutral,neutral,neutral,neutral,neutral
512801913,"@ppwwyyxx Okay, it is not a memory leak. It is just that without `MultiProcessRunner` the whole data flow got forked by many subsequent `MultiProcessRunnerZMQ`'s such that each of them has their own `LocallyShuffleData`. This proves to be too large for my RAM effectively I saw a spike in the memory usage.

This begs a question isn't there a way to prevent forking the flow without using `MultiProcessRunner`?",memory leak without whole data flow got forked many subsequent large ram effectively saw spike memory usage question way prevent flow without,issue,negative,positive,positive,positive,positive,positive
512111313,"> To scale gradients instead of loss, you can use:
> https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.gradproc.ScaleGradient
> and for example,
> 
> https://github.com/tensorpack/tensorpack/blob/c9e03b7356ae1c613707e626452b3e5a0392c86a/examples/SpatialTransformer/mnist-addition.py#L174-L180

Thanks!",scale instead loss use example thanks,issue,negative,positive,positive,positive,positive,positive
512091049,"It's now supported in the above commit. There aren't many changes needed to handle empty inputs.

",commit many handle empty,issue,negative,positive,positive,positive,positive,positive
512072067,"To scale gradients instead of loss, you can use:
https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.gradproc.ScaleGradient
and for example, https://github.com/tensorpack/tensorpack/blob/c9e03b7356ae1c613707e626452b3e5a0392c86a/examples/SpatialTransformer/mnist-addition.py#L174-L180",scale instead loss use example,issue,negative,neutral,neutral,neutral,neutral,neutral
512068585,"I also want to downscale gradients after loss upscaling, as in https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html

In optimizers like SGD, I can just downscale my learning rate correspond to reciprocal of loss scale. But some optimizers, say Momentum Optimizer,  combined with some parameter schedule like [fit_one_cycle](https://docs.fast.ai/callbacks.one_cycle.html#What-is-1cycle?), things become subtle how to change parameters from a fp32 precision training schedule.  In theory, I can just transfer this upscale effect to the scale-parameters-of-gradient in back propagation, but it's kind of involved for some optimizers.",also want loss like learning rate correspond reciprocal loss scale say momentum combined parameter schedule like become subtle change precision training schedule theory transfer upscale effect back propagation kind involved,issue,positive,positive,neutral,neutral,positive,positive
511871198,"> Is it possible that dataflow.MultiProcessRunner with num_proc=1 be a bottleneck itself? By removing it, I could reach better results.

It is possible. Because it needs to send data between the processes it could be bottleneck if the communication alone is more expensive than getting the data.

> there comes a problem that LocallyShuffleData seems to memory leak.

Could you reproduce the issue with simpler code and post it?",possible bottleneck removing could reach better possible need send data could bottleneck communication alone expensive getting data come problem memory leak could reproduce issue simpler code post,issue,negative,neutral,neutral,neutral,neutral,neutral
511870198,"These are coco metrics and are unrelated to tensorpack.
See http://cocodataset.org/#detection-eval
You may find more details about them in earlier papers introducing the PASCAL VOC dataset, or some online blog posts.",coco metric unrelated see may find,issue,negative,neutral,neutral,neutral,neutral,neutral
511868988,"You define the loss in your model, so you can scale it however you want. e.g.: https://github.com/tensorpack/tensorpack/blob/610a8b9baed5ff8f9e8624538141b414f8578d4a/examples/ImageNetModels/imagenet_utils.py#L362-L365",define loss model scale however want,issue,negative,neutral,neutral,neutral,neutral,neutral
511747137,"@ppwwyyxx 
I'd like to use loss scaling to prevent underflow together with `Trainer()` api, so I need to implement `optimizer()` method which  can not control how gradients are computed. How can I implement it without modifying the code at `SingleCostTrainer._make_get_grad_fn()`.",like use loss scaling prevent underflow together trainer need implement method control implement without code,issue,negative,neutral,neutral,neutral,neutral,neutral
511744009,"Is it possible that `dataflow.MultiProcessRunner` with `num_proc=1` be a bottleneck itself? By removing it, I could reach better results. 
However, there comes a problem that `LocallyShuffleData` seems to memory leak.",possible bottleneck removing could reach better however come problem memory leak,issue,negative,positive,positive,positive,positive,positive
511635222,"Aha, I see. 
Thank you for you comment.",aha see thank comment,issue,negative,neutral,neutral,neutral,neutral,neutral
511634056,"The code already set them to use channels_first, at https://github.com/tensorpack/tensorpack/blob/610a8b9baed5ff8f9e8624538141b414f8578d4a/examples/FasterRCNN/modeling/backbone.py#L75",code already set use,issue,negative,neutral,neutral,neutral,neutral,neutral
511285470,"If you're looking for conversion, it is unrelated to what tensorpack is for (https://tensorpack.readthedocs.io/tutorial/inference.html). 
Tensorpack provides a basic version of such conversion as explained in https://tensorpack.readthedocs.io/tutorial/inference.html#exporter.
If you need anything more complicated, you need to learn tensorflow, learn the meaning of pb file, and do it yourself since it is unrelated to what tensorpack does.",looking conversion unrelated basic version conversion need anything complicated need learn learn meaning file since unrelated,issue,negative,negative,negative,negative,negative,negative
511270735,"> I tried to train and predict an official model.

This is not an official model because it seems obvious that you modified the examples and trained your own models.

> , i think below content is the problem.

I don't know what is the word ""fault"". The original code is not supposed to print it. The other two warnings are not a problem.

For any unexpected problems, please read the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",tried train predict official model official model obvious trained think content problem know word fault original code supposed print two problem unexpected please read issue template click new issue unexpected visit link,issue,negative,positive,positive,positive,positive,positive
511269191,"Thank you for your comment.
I tried to train and predict an official model.

In other words, i think below content is the problem.

> (tensorpack) yoohj@8gpu-ib:~/FasterRCNN$ ./predict.py --evaluate train_log/maskrcnn/coco_minival2  014-outputs250000.json --load train_log/maskrcnn/model-250000.data-00000-of-00001 --config DATA.B  ASEDIR=COCO/DIR TRAINER=horovod
2019-07-15 12:17:11.962262: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU support  s instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-15 12:17:12.491240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   0 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:04:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:13.217303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   1 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:05:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:13.646987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   2 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:08:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:14.330565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   3 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:09:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:14.935276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   4 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:83:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:15.465313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   5 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:84:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:16.211808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   6 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:87:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:16.899989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   7 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:88:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:16.937499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visib  le gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-07-15 12:17:21.092605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interc  onnect StreamExecutor with strength 1 edge matrix:
2019-07-15 12:17:21.092676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3   4 5 6 7
2019-07-15 12:17:21.092687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y   N N N N
2019-07-15 12:17:21.092694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y   N N N N
2019-07-15 12:17:21.092700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y   N N N N
2019-07-15 12:17:21.092709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N   N N N N
2019-07-15 12:17:21.092716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 4:   N N N N   N Y Y Y
2019-07-15 12:17:21.092725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 5:   N N N N   Y N Y Y
2019-07-15 12:17:21.092732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 6:   N N N N   Y Y N Y
2019-07-15 12:17:21.092740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 7:   N N N N   Y Y Y N
2019-07-15 12:17:21.095430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:0 with 7128 MB memory) -> physical GPU (device: 0, name: TITAN X (Pasc  al), pci bus id: 0000:04:00.0, compute capability: 6.1)
2019-07-15 12:17:21.190078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:1 with 7126 MB memory) -> physical GPU (device: 1, name: TITAN X (Pasc  al), pci bus id: 0000:05:00.0, compute capability: 6.1)
2019-07-15 12:17:21.271067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:2 with 7126 MB memory) -> physical GPU (device: 2, name: TITAN X (Pasc  al), pci bus id: 0000:08:00.0, compute capability: 6.1)
2019-07-15 12:17:21.353350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:3 with 7126 MB memory) -> physical GPU (device: 3, name: TITAN X (Pasc  al), pci bus id: 0000:09:00.0, compute capability: 6.1)
2019-07-15 12:17:21.471389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:4 with 7126 MB memory) -> physical GPU (device: 4, name: TITAN X (Pasc  al), pci bus id: 0000:83:00.0, compute capability: 6.1)
2019-07-15 12:17:21.608467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:5 with 7126 MB memory) -> physical GPU (device: 5, name: TITAN X (Pasc  al), pci bus id: 0000:84:00.0, compute capability: 6.1)
2019-07-15 12:17:21.720262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:6 with 7126 MB memory) -> physical GPU (device: 6, name: TITAN X (Pasc  al), pci bus id: 0000:87:00.0, compute capability: 6.1)
2019-07-15 12:17:21.817799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:7 with 7126 MB memory) -> physical GPU (device: 7, name: TITAN X (Pasc  al), pci bus id: 0000:88:00.0, compute capability: 6.1)
fault

this last line ""fault"". but, continue to run.

and, 
> [0715 12:17:22 @sesscreate.py:38] WRN User-provided custom session config may not work due to TF   bugs. See https://github.com/tensorpack/tensorpack/issues/497 for workarounds.

and, finally 
>  [0715 12:18:03 @sessinit.py:87] WRN The following variables are in the checkpoint, but not found   in the graph: global_step, learning_rate


Below is the full code
> (tensorpack) yoohj@8gpu-ib:~/FasterRCNN$ ./predict.py --evaluate train_log/maskrcnn/coco_minival2  014-outputs250000.json --load train_log/maskrcnn/model-250000.data-00000-of-00001 --config DATA.B  ASEDIR=COCO/DIR TRAINER=horovod
2019-07-15 12:17:11.962262: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU support  s instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-15 12:17:12.491240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   0 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:04:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:13.217303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   1 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:05:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:13.646987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   2 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:08:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:14.330565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   3 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:09:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:14.935276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   4 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:83:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:15.465313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   5 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:84:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:16.211808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   6 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:87:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:16.899989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device   7 with properties:
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:88:00.0
totalMemory: 11.90GiB freeMemory: 7.38GiB
2019-07-15 12:17:16.937499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visib  le gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-07-15 12:17:21.092605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interc  onnect StreamExecutor with strength 1 edge matrix:
2019-07-15 12:17:21.092676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3   4 5 6 7
2019-07-15 12:17:21.092687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y   N N N N
2019-07-15 12:17:21.092694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y   N N N N
2019-07-15 12:17:21.092700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y   N N N N
2019-07-15 12:17:21.092709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N   N N N N
2019-07-15 12:17:21.092716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 4:   N N N N   N Y Y Y
2019-07-15 12:17:21.092725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 5:   N N N N   Y N Y Y
2019-07-15 12:17:21.092732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 6:   N N N N   Y Y N Y
2019-07-15 12:17:21.092740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 7:   N N N N   Y Y Y N
2019-07-15 12:17:21.095430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:0 with 7128 MB memory) -> physical GPU (device: 0, name: TITAN X (Pasc  al), pci bus id: 0000:04:00.0, compute capability: 6.1)
2019-07-15 12:17:21.190078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:1 with 7126 MB memory) -> physical GPU (device: 1, name: TITAN X (Pasc  al), pci bus id: 0000:05:00.0, compute capability: 6.1)
2019-07-15 12:17:21.271067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:2 with 7126 MB memory) -> physical GPU (device: 2, name: TITAN X (Pasc  al), pci bus id: 0000:08:00.0, compute capability: 6.1)
2019-07-15 12:17:21.353350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:3 with 7126 MB memory) -> physical GPU (device: 3, name: TITAN X (Pasc  al), pci bus id: 0000:09:00.0, compute capability: 6.1)
2019-07-15 12:17:21.471389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:4 with 7126 MB memory) -> physical GPU (device: 4, name: TITAN X (Pasc  al), pci bus id: 0000:83:00.0, compute capability: 6.1)
2019-07-15 12:17:21.608467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:5 with 7126 MB memory) -> physical GPU (device: 5, name: TITAN X (Pasc  al), pci bus id: 0000:84:00.0, compute capability: 6.1)
2019-07-15 12:17:21.720262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:6 with 7126 MB memory) -> physical GPU (device: 6, name: TITAN X (Pasc  al), pci bus id: 0000:87:00.0, compute capability: 6.1)
2019-07-15 12:17:21.817799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/device:GPU:7 with 7126 MB memory) -> physical GPU (device: 7, name: TITAN X (Pasc  al), pci bus id: 0000:88:00.0, compute capability: 6.1)
fault
[0715 12:17:22 @config.py:287] Config: ------------------------------------------
{'BACKBONE': {'FREEZE_AFFINE': False,
              'FREEZE_AT': 0,
              'NORM': 'GN',
              'RESNET_NUM_BLOCKS': [3, 4, 23, 3],
              'STRIDE_1X1': False,
              'TF_PAD_MODE': False,
              'WEIGHTS': ''},
 'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0],
                                  [30.0, 30.0, 15.0, 15.0]],
             'IOUS': [0.5, 0.6, 0.7]},
 'DATA': {'ABSOLUTE_COORD': True,
          'BASEDIR': 'COCO/DIR',
          'CLASS_NAMES': ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
                          'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',
                          'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow'  ,
                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handba  g',
                          'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite  ',
                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon  ',
                          'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',
                          'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant',
                          'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',
                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
                          'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',
                          'hair drier', 'toothbrush'],
          'NUM_CATEGORY': 80,
          'NUM_WORKERS': 10,
          'TRAIN': ('coco_train2014', 'coco_valminusminival2014'),
          'VAL': ('coco_minival2014',)},
 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),
         'CASCADE': True,
         'FRCNN_CONV_HEAD_DIM': 256,
         'FRCNN_FC_HEAD_DIM': 1024,
         'FRCNN_HEAD_FUNC': 'fastrcnn_4conv1fc_gn_head',
         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_gn_head',
         'NORM': 'GN',
         'NUM_CHANNEL': 256,
         'PROPOSAL_MODE': 'Level',
         'RESOLUTION_REQUIREMENT': 32},
 'FRCNN': {'BATCH_PER_IM': 512,
           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],
           'FG_RATIO': 0.25,
           'FG_THRESH': 0.5},
 'MODE_FPN': True,
 'MODE_MASK': True,
 'MRCNN': {'HEAD_DIM': 256},
 'PREPROC': {'MAX_SIZE': 1344.0,
             'PIXEL_MEAN': [123.675, 116.28, 103.53],
             'PIXEL_STD': [58.395, 57.12, 57.375],
             'TEST_SHORT_EDGE_SIZE': 800,
             'TRAIN_SHORT_EDGE_SIZE': [640, 800]},
 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),
         'ANCHOR_SIZES': (32, 64, 128, 256, 512),
         'ANCHOR_STRIDE': 16,
         'BATCH_PER_IM': 256,
         'CROWD_OVERLAP_THRESH': 9.99,
         'FG_RATIO': 0.5,
         'HEAD_DIM': 1024,
         'MIN_SIZE': 0,
         'NEGATIVE_ANCHOR_THRESH': 0.3,
         'NUM_ANCHOR': 15,
         'POSITIVE_ANCHOR_THRESH': 0.7,
         'PROPOSAL_NMS_THRESH': 0.7,
         'TEST_PER_LEVEL_NMS_TOPK': 1000,
         'TEST_POST_NMS_TOPK': 1000,
         'TEST_PRE_NMS_TOPK': 6000,
         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,
         'TRAIN_POST_NMS_TOPK': 2000,
         'TRAIN_PRE_NMS_TOPK': 12000},
 'TEST': {'FRCNN_NMS_THRESH': 0.5,
          'RESULTS_PER_IM': 100,
          'RESULT_SCORE_THRESH': 0.05,
          'RESULT_SCORE_THRESH_VIS': 0.5},
 'TRAIN': {'BASE_LR': 0.01,
           'EVAL_PERIOD': 25,
           'LR_SCHEDULE': [420000, 500000, 540000],
           'NUM_GPUS': 8,
           'STARTING_EPOCH': 1,
           'STEPS_PER_EPOCH': 500,
           'WARMUP': 1000,
           'WARMUP_INIT_LR': 0.0033000000000000004,
           'WEIGHT_DECAY': 0.0001},
 'TRAINER': 'horovod'}
[0715 12:17:22 @varmanip.py:195] Checkpoint path train_log/maskrcnn/model-250000.data-00000-of-00  001 is auto-corrected to train_log/maskrcnn/model-250000.
[0715 12:17:22 @sesscreate.py:38] WRN User-provided custom session config may not work due to TF   bugs. See https://github.com/tensorpack/tensorpack/issues/497 for workarounds.
[0715 12:17:22 @multigpu.py:44] Building graph for predict tower 'tower0' on device /gpu:0 ...
[0715 12:17:22 @registry.py:135] stem1 input: [None, 3, None, None]
[0715 12:17:22 @registry.py:135] stem1/gn input: [None, 64, None, None]
[0715 12:17:22 @registry.py:143] stem1/gn output: [None, 64, None, None]
[0715 12:17:22 @registry.py:143] stem1 output: [None, 64, None, None]
[0715 12:17:22 @registry.py:135] stem2 input: [None, 64, None, None]
[0715 12:17:22 @registry.py:135] stem2/gn input: [None, 64, None, None]
[0715 12:17:22 @registry.py:143] stem2/gn output: [None, 64, None, None]
[0715 12:17:22 @registry.py:143] stem2 output: [None, 64, None, None]
[0715 12:17:22 @registry.py:135] stem3 input: [None, 64, None, None]
[0715 12:17:22 @registry.py:135] stem3/gn input: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] stem3/gn output: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] stem3 output: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/OSA2_1_conv1 input: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/OSA2_1_conv1/gn input: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] OSA2/OSA2_1_conv1/gn output: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] OSA2/OSA2_1_conv1 output: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/OSA2_1_conv2 input: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/OSA2_1_conv2/gn input: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] OSA2/OSA2_1_conv2/gn output: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] OSA2/OSA2_1_conv2 output: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/OSA2_1_conv3 input: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/OSA2_1_conv3/gn input: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] OSA2/OSA2_1_conv3/gn output: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] OSA2/OSA2_1_conv3 output: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/OSA2_1_conv4 input: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/OSA2_1_conv4/gn input: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] OSA2/OSA2_1_conv4/gn output: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] OSA2/OSA2_1_conv4 output: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/OSA2_1_conv5 input: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/OSA2_1_conv5/gn input: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] OSA2/OSA2_1_conv5/gn output: [None, 128, None, None]
[0715 12:17:22 @registry.py:143] OSA2/OSA2_1_conv5 output: [None, 128, None, None]
[0715 12:17:22 @registry.py:135] OSA2/last input: [None, 768, None, None]
[0715 12:17:22 @registry.py:135] OSA2/last/gn input: [None, 256, None, None]
[0715 12:17:22 @registry.py:143] OSA2/last/gn output: [None, 256, None, None]
[0715 12:17:22 @registry.py:143] OSA2/last output: [None, 256, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_Pooling input: [None, 256, None, None]
[0715 12:17:22 @registry.py:143] OSA3/OSA3_Pooling output: [None, 256, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_1_conv1 input: [None, 256, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_1_conv1/gn input: [None, 160, None, None]
[0715 12:17:22 @registry.py:143] OSA3/OSA3_1_conv1/gn output: [None, 160, None, None]
[0715 12:17:22 @registry.py:143] OSA3/OSA3_1_conv1 output: [None, 160, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_1_conv2 input: [None, 160, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_1_conv2/gn input: [None, 160, None, None]
[0715 12:17:22 @registry.py:143] OSA3/OSA3_1_conv2/gn output: [None, 160, None, None]
[0715 12:17:22 @registry.py:143] OSA3/OSA3_1_conv2 output: [None, 160, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_1_conv3 input: [None, 160, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_1_conv3/gn input: [None, 160, None, None]
[0715 12:17:22 @registry.py:143] OSA3/OSA3_1_conv3/gn output: [None, 160, None, None]
[0715 12:17:22 @registry.py:143] OSA3/OSA3_1_conv3 output: [None, 160, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_1_conv4 input: [None, 160, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_1_conv4/gn input: [None, 160, None, None]
[0715 12:17:22 @registry.py:143] OSA3/OSA3_1_conv4/gn output: [None, 160, None, None]
[0715 12:17:22 @registry.py:143] OSA3/OSA3_1_conv4 output: [None, 160, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_1_conv5 input: [None, 160, None, None]
[0715 12:17:22 @registry.py:135] OSA3/OSA3_1_conv5/gn input: [None, 160, None, None]
[0715 12:17:23 @registry.py:143] OSA3/OSA3_1_conv5/gn output: [None, 160, None, None]
[0715 12:17:23 @registry.py:143] OSA3/OSA3_1_conv5 output: [None, 160, None, None]
[0715 12:17:23 @registry.py:135] OSA3/last input: [None, 1056, None, None]
[0715 12:17:23 @registry.py:135] OSA3/last/gn input: [None, 512, None, None]
[0715 12:17:23 @registry.py:143] OSA3/last/gn output: [None, 512, None, None]
[0715 12:17:23 @registry.py:143] OSA3/last output: [None, 512, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_Pooling input: [None, 512, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_Pooling output: [None, 512, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_1_conv1 input: [None, 512, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_1_conv1/gn input: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_1_conv1/gn output: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_1_conv1 output: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_1_conv2 input: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_1_conv2/gn input: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_1_conv2/gn output: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_1_conv2 output: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_1_conv3 input: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_1_conv3/gn input: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_1_conv3/gn output: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_1_conv3 output: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_1_conv4 input: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_1_conv4/gn input: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_1_conv4/gn output: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_1_conv4 output: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_1_conv5 input: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_1_conv5/gn input: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_1_conv5/gn output: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_1_conv5 output: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/last input: [None, 1472, None, None]
[0715 12:17:23 @registry.py:135] OSA4/last/gn input: [None, 768, None, None]
[0715 12:17:23 @registry.py:143] OSA4/last/gn output: [None, 768, None, None]
[0715 12:17:23 @registry.py:143] OSA4/last output: [None, 768, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_2_conv1 input: [None, 768, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_2_conv1 output: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_2_conv2 input: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_2_conv2 output: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_2_conv3 input: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_2_conv3 output: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_2_conv4 input: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_2_conv4 output: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/OSA4_2_conv5 input: [None, 192, None, None]
[0715 12:17:23 @registry.py:143] OSA4/OSA4_2_conv5 output: [None, 192, None, None]
[0715 12:17:23 @registry.py:135] OSA4/last2 input: [None, 1728, None, None]
[0715 12:17:23 @registry.py:135] OSA4/last2/gn input: [None, 768, None, None]
[0715 12:17:23 @registry.py:143] OSA4/last2/gn output: [None, 768, None, None]
[0715 12:17:23 @registry.py:143] OSA4/last2 output: [None, 768, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_Pooling input: [None, 768, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_Pooling output: [None, 768, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_1_conv1 input: [None, 768, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_1_conv1/gn input: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_1_conv1/gn output: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_1_conv1 output: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_1_conv2 input: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_1_conv2/gn input: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_1_conv2/gn output: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_1_conv2 output: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_1_conv3 input: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_1_conv3/gn input: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_1_conv3/gn output: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_1_conv3 output: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_1_conv4 input: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_1_conv4/gn input: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_1_conv4/gn output: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_1_conv4 output: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_1_conv5 input: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_1_conv5/gn input: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_1_conv5/gn output: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_1_conv5 output: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/last input: [None, 1888, None, None]
[0715 12:17:23 @registry.py:135] OSA5/last/gn input: [None, 1024, None, None]
[0715 12:17:23 @registry.py:143] OSA5/last/gn output: [None, 1024, None, None]
[0715 12:17:23 @registry.py:143] OSA5/last output: [None, 1024, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_2_conv1 input: [None, 1024, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_2_conv1 output: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_2_conv2 input: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_2_conv2 output: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_2_conv3 input: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_2_conv3 output: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_2_conv4 input: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_2_conv4 output: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/OSA5_2_conv5 input: [None, 224, None, None]
[0715 12:17:23 @registry.py:143] OSA5/OSA5_2_conv5 output: [None, 224, None, None]
[0715 12:17:23 @registry.py:135] OSA5/last2 input: [None, 2144, None, None]
[0715 12:17:23 @registry.py:135] OSA5/last2/gn input: [None, 1024, None, None]
[0715 12:17:23 @registry.py:143] OSA5/last2/gn output: [None, 1024, None, None]
[0715 12:17:23 @registry.py:143] OSA5/last2 output: [None, 1024, None, None]
[0715 12:17:23 @registry.py:135] fpn input: [None, 256, None, None],[None, 512, None, None],[None  , 768, None, None],[None, 1024, None, None]
[0715 12:17:23 @registry.py:135] fpn/lateral_1x1_c2 input: [None, 256, None, None]
[0715 12:17:23 @registry.py:143] fpn/lateral_1x1_c2 output: [None, 256, None, None]
[0715 12:17:23 @registry.py:135] fpn/lateral_1x1_c3 input: [None, 512, None, None]
[0715 12:17:24 @registry.py:143] fpn/lateral_1x1_c3 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/lateral_1x1_c4 input: [None, 768, None, None]
[0715 12:17:24 @registry.py:143] fpn/lateral_1x1_c4 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/lateral_1x1_c5 input: [None, 1024, None, None]
[0715 12:17:24 @registry.py:143] fpn/lateral_1x1_c5 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/gn_c2 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/gn_c2 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/gn_c3 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/gn_c3 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/gn_c4 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/gn_c4 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/gn_c5 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/gn_c5 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/upsample_lat5 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/upsample_lat5 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/upsample_lat4 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/upsample_lat4 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/upsample_lat3 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/upsample_lat3 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/posthoc_3x3_p2 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/posthoc_3x3_p2 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/posthoc_3x3_p3 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/posthoc_3x3_p3 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/posthoc_3x3_p4 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/posthoc_3x3_p4 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/posthoc_3x3_p5 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/posthoc_3x3_p5 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/gn_p2 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/gn_p2 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/gn_p3 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/gn_p3 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/gn_p4 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/gn_p4 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/gn_p5 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/gn_p5 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] fpn/maxpool_p6 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn/maxpool_p6 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] fpn output: [None, 256, None, None],[None, 256, None, None],[Non  e, 256, None, None],[None, 256, None, None],[None, 256, None, None]
[0715 12:17:24 @registry.py:135] rpn input: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] rpn/conv0 input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] rpn/conv0 output: [None, 256, None, None]
[0715 12:17:24 @registry.py:135] rpn/class input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] rpn/class output: [None, 3, None, None]
[0715 12:17:24 @registry.py:135] rpn/box input: [None, 256, None, None]
[0715 12:17:24 @registry.py:143] rpn/box output: [None, 12, None, None]
[0715 12:17:24 @registry.py:143] rpn output: [None, None, 3],[None, None, 3, 4]
[0715 12:17:25 @registry.py:135] cascade_rcnn_stage1/head input: [None, 256, 7, 7]
[0715 12:17:25 @registry.py:135] cascade_rcnn_stage1/head/conv0 input: [None, 256, 7, 7]
[0715 12:17:25 @registry.py:143] cascade_rcnn_stage1/head/conv0 output: [None, 256, 7, 7]
[0715 12:17:25 @registry.py:135] cascade_rcnn_stage1/head/gn0 input: [None, 256, 7, 7]
[0715 12:17:25 @registry.py:143] cascade_rcnn_stage1/head/gn0 output: [None, 256, 7, 7]
[0715 12:17:25 @registry.py:135] cascade_rcnn_stage1/head/conv1 input: [None, 256, 7, 7]
[0715 12:17:25 @registry.py:143] cascade_rcnn_stage1/head/conv1 output: [None, 256, 7, 7]
[0715 12:17:25 @registry.py:135] cascade_rcnn_stage1/head/gn1 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage1/head/gn1 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage1/head/conv2 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage1/head/conv2 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage1/head/gn2 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage1/head/gn2 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage1/head/conv3 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage1/head/conv3 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage1/head/gn3 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage1/head/gn3 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage1/head/fc input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage1/head/fc output: [None, 1024]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage1/head output: [None, 1024]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage1/outputs input: [None, 1024]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage1/outputs/class input: [None, 1024]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage1/outputs/class output: [None, 81]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage1/outputs/box input: [None, 1024]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage1/outputs/box output: [None, 4]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage1/outputs output: [None, 81],[None, 1, 4]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/head input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/head/conv0 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/head/conv0 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/head/gn0 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/head/gn0 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/head/conv1 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/head/conv1 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/head/gn1 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/head/gn1 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/head/conv2 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/head/conv2 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/head/gn2 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/head/gn2 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/head/conv3 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/head/conv3 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/head/gn3 input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/head/gn3 output: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/head/fc input: [None, 256, 7, 7]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/head/fc output: [None, 1024]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/head output: [None, 1024]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/outputs input: [None, 1024]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/outputs/class input: [None, 1024]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/outputs/class output: [None, 81]
[0715 12:17:26 @registry.py:135] cascade_rcnn_stage2/outputs/box input: [None, 1024]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/outputs/box output: [None, 4]
[0715 12:17:26 @registry.py:143] cascade_rcnn_stage2/outputs output: [None, 81],[None, 1, 4]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/head input: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/head/conv0 input: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/head/conv0 output: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/head/gn0 input: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/head/gn0 output: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/head/conv1 input: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/head/conv1 output: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/head/gn1 input: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/head/gn1 output: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/head/conv2 input: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/head/conv2 output: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/head/gn2 input: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/head/gn2 output: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/head/conv3 input: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/head/conv3 output: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/head/gn3 input: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/head/gn3 output: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/head/fc input: [None, 256, 7, 7]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/head/fc output: [None, 1024]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/head output: [None, 1024]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/outputs input: [None, 1024]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/outputs/class input: [None, 1024]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/outputs/class output: [None, 81]
[0715 12:17:27 @registry.py:135] cascade_rcnn_stage3/outputs/box input: [None, 1024]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/outputs/box output: [None, 4]
[0715 12:17:27 @registry.py:143] cascade_rcnn_stage3/outputs output: [None, 81],[None, 1, 4]
[0715 12:17:28 @registry.py:135] maskrcnn input: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:135] maskrcnn/fcn0 input: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:143] maskrcnn/fcn0 output: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:135] maskrcnn/gn0 input: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:143] maskrcnn/gn0 output: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:135] maskrcnn/fcn1 input: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:143] maskrcnn/fcn1 output: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:135] maskrcnn/gn1 input: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:143] maskrcnn/gn1 output: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:135] maskrcnn/fcn2 input: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:143] maskrcnn/fcn2 output: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:135] maskrcnn/gn2 input: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:143] maskrcnn/gn2 output: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:135] maskrcnn/fcn3 input: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:143] maskrcnn/fcn3 output: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:135] maskrcnn/gn3 input: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:143] maskrcnn/gn3 output: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:135] maskrcnn/deconv input: [None, 256, 14, 14]
[0715 12:17:28 @registry.py:143] maskrcnn/deconv output: [None, 256, 28, 28]
[0715 12:17:28 @registry.py:135] maskrcnn/conv input: [None, 256, 28, 28]
[0715 12:17:28 @registry.py:143] maskrcnn/conv output: [None, 80, 28, 28]
[0715 12:17:28 @registry.py:143] maskrcnn output: [None, 80, 28, 28]
[0715 12:17:28 @multigpu.py:44] Building graph for predict tower 'tower1' on device /gpu:1 ...
[0715 12:17:33 @multigpu.py:44] Building graph for predict tower 'tower2' on device /gpu:2 ...
[0715 12:17:38 @multigpu.py:44] Building graph for predict tower 'tower3' on device /gpu:3 ...
[0715 12:17:43 @multigpu.py:44] Building graph for predict tower 'tower4' on device /gpu:4 ...
[0715 12:17:48 @multigpu.py:44] Building graph for predict tower 'tower5' on device /gpu:5 ...
[0715 12:17:53 @multigpu.py:44] Building graph for predict tower 'tower6' on device /gpu:6 ...
[0715 12:17:58 @multigpu.py:44] Building graph for predict tower 'tower7' on device /gpu:7 ...
[0715 12:18:03 @sessinit.py:87] WRN The following variables are in the checkpoint, but not found   in the graph: global_step, learning_rate
2019-07-15 12:18:03.765130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visib  le gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-07-15 12:18:03.765780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interc  onnect StreamExecutor with strength 1 edge matrix:
2019-07-15 12:18:03.765796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3   4 5 6 7
2019-07-15 12:18:03.765807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y   N N N N
2019-07-15 12:18:03.765814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y   N N N N
2019-07-15 12:18:03.765821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y   N N N N
2019-07-15 12:18:03.765828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N   N N N N
2019-07-15 12:18:03.765835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 4:   N N N N   N Y Y Y
2019-07-15 12:18:03.765841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 5:   N N N N   Y N Y Y
2019-07-15 12:18:03.765848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 6:   N N N N   Y Y N Y
2019-07-15 12:18:03.765855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 7:   N N N N   Y Y Y N
2019-07-15 12:18:03.769425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7128 MB memory) -> physical GPU   (device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1)
2019-07-15 12:18:03.770513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7126 MB memory) -> physical GPU   (device: 1, name: TITAN X (Pascal), pci bus id: 0000:05:00.0, compute capability: 6.1)
2019-07-15 12:18:03.771434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 7126 MB memory) -> physical GPU   (device: 2, name: TITAN X (Pascal), pci bus id: 0000:08:00.0, compute capability: 6.1)
2019-07-15 12:18:03.771765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 7126 MB memory) -> physical GPU   (device: 3, name: TITAN X (Pascal), pci bus id: 0000:09:00.0, compute capability: 6.1)
2019-07-15 12:18:03.772998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 7126 MB memory) -> physical GPU   (device: 4, name: TITAN X (Pascal), pci bus id: 0000:83:00.0, compute capability: 6.1)
2019-07-15 12:18:03.773397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 7126 MB memory) -> physical GPU   (device: 5, name: TITAN X (Pascal), pci bus id: 0000:84:00.0, compute capability: 6.1)
2019-07-15 12:18:03.773628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 7126 MB memory) -> physical GPU   (device: 6, name: TITAN X (Pascal), pci bus id: 0000:87:00.0, compute capability: 6.1)
2019-07-15 12:18:03.774036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created Tens  orFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 7126 MB memory) -> physical GPU   (device: 7, name: TITAN X (Pascal), pci bus id: 0000:88:00.0, compute capability: 6.1)
[0715 12:18:08 @sessinit.py:114] Restoring checkpoint from train_log/maskrcnn/model-250000 ...
[0715 12:18:09 @predict.py:87] Evaluating coco_minival2014 ...
loading annotations into memory...
Done (t=0.51s)
creating index...
index created!
[0715 12:18:10 @coco.py:68] Instances loaded from COCO/DIR/annotations/instances_minival2014.json  .
100%|████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 141571.28it/s]
[0715 12:18:10 @timer.py:50] Load Groundtruth Boxes for instances_minival2014.json finished, time  :0.0417 sec.
[0715 12:18:10 @data.py:412] Found 5000 images for inference.
loading annotations into memory...
Done (t=1.18s)
creating index...
index created!
[0715 12:18:11 @coco.py:68] Instances loaded from COCO/DIR/annotations/instances_minival2014.json  .
100%|████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 177079.46it/s]
[0715 12:18:11 @timer.py:50] Load Groundtruth Boxes for instances_minival2014.json finished, time  :0.0315 sec.
[0715 12:18:11 @data.py:412] Found 5000 images for inference.
loading annotations into memory...
Done (t=0.49s)
creating index...
index created!
[0715 12:18:12 @coco.py:68] Instances loaded from COCO/DIR/annotations/instances_minival2014.json  .
100%|████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 145065.37it/s]
[0715 12:18:12 @timer.py:50] Load Groundtruth Boxes for instances_minival2014.json finished, time  :0.0380 sec.
[0715 12:18:12 @data.py:412] Found 5000 images for inference.
loading annotations into memory...
Done (t=0.48s)
creating index...
index created!
[0715 12:18:12 @coco.py:68] Instances loaded from COCO/DIR/annotations/instances_minival2014.json  .
100%|████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 146833.68it/s]
[0715 12:18:13 @timer.py:50] Load Groundtruth Boxes for instances_minival2014.json finished, time  :0.0376 sec.
[0715 12:18:13 @data.py:412] Found 5000 images for inference.
loading annotations into memory...
Done (t=1.26s)
creating index...
index created!
[0715 12:18:14 @coco.py:68] Instances loaded from COCO/DIR/annotations/instances_minival2014.json  .
100%|████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 172926.76it/s]
[0715 12:18:14 @timer.py:50] Load Groundtruth Boxes for instances_minival2014.json finished, time  :0.0324 sec.
[0715 12:18:14 @data.py:412] Found 5000 images for inference.
loading annotations into memory...
Done (t=0.50s)
creating index...
index created!
[0715 12:18:15 @coco.py:68] Instances loaded from COCO/DIR/annotations/instances_minival2014.json  .
100%|████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 141440.47it/s]
[0715 12:18:15 @timer.py:50] Load Groundtruth Boxes for instances_minival2014.json finished, time  :0.0390 sec.
[0715 12:18:15 @data.py:412] Found 5000 images for inference.
loading annotations into memory...
Done (t=0.48s)
creating index...
index created!
[0715 12:18:15 @coco.py:68] Instances loaded from COCO/DIR/annotations/instances_minival2014.json  .
100%|████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 154850.22it/s]
[0715 12:18:15 @timer.py:50] Load Groundtruth Boxes for instances_minival2014.json finished, time  :0.0374 sec.
[0715 12:18:15 @data.py:412] Found 5000 images for inference.
loading annotations into memory...
Done (t=1.32s)
creating index...
index created!
[0715 12:18:17 @coco.py:68] Instances loaded from COCO/DIR/annotations/instances_minival2014.json  .
100%|████████████████████████████████████████████████████| 5000/5000 [00:00<00:00, 168513.62it/s]
[0715 12:18:17 @timer.py:50] Load Groundtruth Boxes for instances_minival2014.json finished, time  :0.0324 sec.
[0715 12:18:17 @data.py:412] Found 5000 images for inference.
  2%|█▏                                                       | 105/5000 [00:39<05:58, 13.67it/s]  100%|████████████████████████████████████████████████████████| 5000/5000 [08:21<00:00,  1.73it/s]
loading annotations into memory...
Done (t=0.56s)
creating index...
index created!
[0715 12:26:39 @coco.py:68] Instances loaded from COCO/DIR/annotations/instances_minival2014.json.
",thank comment tried train predict official model think content problem evaluate load support binary use found device name major minor found device name major minor found device name major minor found device name major minor found device name major minor found device name major minor found device name major minor found device name major minor device strength edge matrix device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability fault last line fault continue run custom session may work due see finally following found graph full code evaluate load support binary use found device name major minor found device name major minor found device name major minor found device name major minor found device name major minor found device name major minor found device name major minor found device name major minor device strength edge matrix device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability device memory physical device name al bus id compute capability fault false false false true light hydrant sign meter ball bat glove racket glass dog plant table phone bear drier true true true path custom session may work due see building graph predict tower device stem input none none none input none none none output none none none stem output none none none stem input none none none input none none none output none none none stem output none none none stem input none none none input none none none output none none none stem output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none input none none none output none none none output none none none input none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none input none none none output none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none input none none none output none none none output none none none input none none none none none none none none none none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none input none none none output none none none output none none none none none none non none none none none none none none none input none none none input none none none output none none none input none none none output none none none input none none none output none none none output none none none none input none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none output none input none input none output none input none output none output none none input none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none output none input none input none output none input none output none output none none input none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none output none input none input none output none input none output none output none none input none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none output none building graph predict tower device building graph predict tower device building graph predict tower device building graph predict tower device building graph predict tower device building graph predict tower device building graph predict tower device following found graph device strength edge matrix device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability device memory physical device name bus id compute capability loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded load finished time sec found inference loading memory done index index loaded,issue,positive,positive,neutral,neutral,positive,positive
511253505,There is a command in the readme where you can predict and get results using an official model.,command predict get official model,issue,negative,neutral,neutral,neutral,neutral,neutral
511252172,"Thank you for your comment.
then, How do I get results when I make a  predict with cocomedel?
I am curious about the contents of the file.
Thank you",thank comment get make predict curious content file thank,issue,positive,negative,neutral,neutral,negative,negative
511093316,This is how the model works. The model by design does not avoid making different predictions in nearby bounding boxes.,model work model design avoid making different nearby bounding,issue,negative,neutral,neutral,neutral,neutral,neutral
510833106,"I run this example under TF1.13.1-gpu and tensorpack master(0.9.5), not show any slowdown.
@ppwwyyxx Thanks a lot!",run example master show slowdown thanks lot,issue,negative,positive,positive,positive,positive,positive
510713177,"> I want to train or optimize only certain layers(fct layer) of DoRefo-net

https://tensorpack.readthedocs.io/tutorial/faq.html#how-to-freeze-some-variables-in-training",want train optimize certain layer,issue,positive,positive,positive,positive,positive,positive
510552488,"The model is written by users and it's up to users whether to / 255.0.
In inference the model needs to do the same operation as training.",model written whether inference model need operation training,issue,negative,neutral,neutral,neutral,neutral,neutral
510519833,"My mistake... I was too quick asking... the learning rate when down in the epoch 1251. 
For some reason I though it will go down as soon as it announced the change... :-)

Thanks for a great work!",mistake quick learning rate epoch reason though go soon change thanks great work,issue,positive,positive,positive,positive,positive,positive
510394427,"when i use the Weights and  Bias for inference,  i need make image/255.0 first?",use bias inference need make first,issue,negative,positive,positive,positive,positive,positive
510385703,"@ppwwyyxx thank you very much firstly.
i think tf maybe is the problem, i try to run again under TF1.13 and tensorpack master.",thank much firstly think maybe problem try run master,issue,negative,positive,positive,positive,positive,positive
510385619,"Ah ha!! 
weight will be radomly initialized!! 
Thank you. i understand.
What I wanted to tell you is that i need the ckpt i need to predict.

This problem seems to be resolved.",ah ha weight thank understand tell need need predict problem resolved,issue,negative,neutral,neutral,neutral,neutral,neutral
510365944,"> if there is no backbone weight here, where is the learned weight left?

Since you don't have backbone weight, what do you mean by ""the learned weight""?

If you don't specify the config, the weight will be randomly initialized.",backbone weight learned weight left since backbone weight mean learned weight specify weight randomly,issue,negative,negative,negative,negative,negative,negative
510355461,"Thank you for your comment.
hmm... This is another question. i want to use vovnet(this is what i made) instead of resnet.

so i modify backbone.py and i try to train this command 
`python -m train.py --config \ DATA.BASEDIR=COCO/DIR  `

but, the readme is written this command.
`./train.py --config \
    MODE_MASK=True MODE_FPN=True \
    DATA.BASEDIR=/path/to/COCO/DIR \
    BACKBONE.WEIGHTS=/path/to/ImageNet-R50-AlignPadding.npz`


so, i don't use 'BACKBONE.WEIGHTS' parameter because i don't have backbone weight of vovnet.
if there is no backbone weight here, where is the learned weight left? 

thank you for reading!! ",thank comment another question want use made instead modify try train command python written command use parameter backbone weight backbone weight learned weight left thank reading,issue,positive,neutral,neutral,neutral,neutral,neutral
510326237,"I cannot understand your question. I do not know what is ""monitor.py"" and how is it related to tensorpack. I cannot understand what you observed and what you expected to see.

The [tensorpack issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/unexpected-problems---bugs.md) can help you explain more details.",understand question know related understand see issue template help explain,issue,negative,neutral,neutral,neutral,neutral,neutral
510325605,"python program error message. i find it !!!!!
I look at the mylog. and i find it problem. 
this is my mistake.... 
`CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 NCCL_DEBUG=INFO mpirun -np 8 --output-filename mylog python -m train.py --config \ DATA.BASEDIR=COCO/DIR \ TRAINER=horovod `
it don't correct command!!
`python -m train.py` -> `python train.py` 
my mistake i'm so sorry.

and.. i try to train my backbone.py 
but, the monitor.py contents are no longer visible.
and  i check the contents of monitor.py in 'mylog.1.0', which is the result of mpirun.....
Is this what it is?
Thank you!!",python program error message find look find problem mistake python correct command python python mistake sorry try train content longer visible check content result thank,issue,negative,negative,negative,negative,negative,negative
510323233,"Thanks, I was asking about the error message of the python program, i.e. the assertion error that you reported.",thanks error message python program assertion error,issue,negative,positive,positive,positive,positive,positive
510322619,"here, error message 
```
(tensorpack) yoohj@8gpu-ib:~/FasterRCNN$ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 NCCL_DEBUG=INFO mpirun -np 8 --output-filename mylog python train.py --config     MODE_FPN=True FPN.CASCADE=True     DATA.BADIR=COCO/DIR     TRAINER=horovod
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
""fork()"" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11256,1],3] (PID 3346)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
[8gpu-ib:03337] 7 more processes have sent help message help-opal-runtime.txt / opal_init:warn-fork
[8gpu-ib:03337] Set MCA parameter ""orte_base_help_aggregate"" to 0 to see all help / error messages
```
Thank you
",error message python process executed operation call fork system call create child process open currently operating condition could result memory corruption system job may crash produce silent data corruption use fork system create child strongly process fork local host absolutely sure application successfully correctly survive call fork may disable warning setting parameter sent help message set parameter see help error thank,issue,positive,positive,positive,positive,positive,positive
510315678,Please include __full error message__ as well,please include error well,issue,negative,neutral,neutral,neutral,neutral,neutral
510315552,"i process mpirun --version; mpirun -np 1 env | grep MPI
and the result is blow.

`(tensorpack) yoohj@8gpu-ib:~/FasterRCNN$ mpirun --version; mpirun -np 1 env | grep MPI

mpirun (Open MPI) 2.0.1

Report bugs to http://www.open-mpi.org/community/help/
OMPI_MCA_pmix=^s1,s2,cray
OMPI_COMMAND=env
OMPI_MCA_orte_precondition_transports=a5527937ebbaef35-c57ecb5b53a2ec85
OMPI_MCA_orte_local_daemon_uri=315686912.0;usock;tcp://xxx.xxx.xxx.xxx,200.200.200.100,100.100.100 .15,172.17.0.1:39517;ud://1108.25.1
OMPI_MCA_orte_hnp_uri=315686912.0;usock;tcp://xxx.xxx.xxx.xxx,200.200.200.100,100.100.100.15,172.1 7.0.1:39517;ud://1108.25.1
OMPI_MCA_mpi_yield_when_idle=0
OMPI_MCA_orte_app_num=0
OMPI_UNIVERSE_SIZE=1
OMPI_MCA_orte_num_nodes=1
OMPI_MCA_shmem_RUNTIME_QUERY_hint=mmap
OMPI_MCA_orte_bound_at_launch=1
OMPI_MCA_ess=pmi
OMPI_MCA_orte_ess_num_procs=1
OMPI_COMM_WORLD_SIZE=1
OMPI_COMM_WORLD_LOCAL_SIZE=1
OMPI_MCA_orte_tmpdir_base=/tmp
OMPI_NUM_APP_CTX=1
OMPI_FIRST_RANKS=0
OMPI_APP_CTX_NUM_PROCS=1
OMPI_MCA_initial_wdir=/home/yoohj/FasterRCNN
OMPI_MCA_orte_launch=1
OMPI_MCA_ess_base_jobid=315686913
OMPI_MCA_ess_base_vpid=0
OMPI_COMM_WORLD_RANK=0
OMPI_COMM_WORLD_LOCAL_RANK=0
OMPI_COMM_WORLD_NODE_RANK=0
OMPI_MCA_orte_ess_node_rank=0
OMPI_FILE_LOCATION=/tmp/openmpi-sessions-1007@8gpu-ib_0/4817/1/0
`

And,
Can i remove the assertion line? so, Does not it happend critical problem?
thank you.",process version result blow version open report remove assertion line critical problem thank,issue,negative,neutral,neutral,neutral,neutral,neutral
510310600,"If you don't know what to include to let others reproduce your issue, [tensorpack issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/unexpected-problems---bugs.md) tells you.

Running 
```
python DQN.py --task train --algo DQN --gpu 0 --files './data/filenames/image_files.txt' './data/filenames/landmark_files.txt'
``` 
with the only changes from https://github.com/amiralansary/tensorpack-medical/issues/9#issuecomment-510309070 on tensorpack master and TF 1.13 does not show any slowdown.
",know include let reproduce issue issue template running python task train master show slowdown,issue,negative,neutral,neutral,neutral,neutral,neutral
510306478,"Please include full error message and the output of `mpirun --version; mpirun -np 1 env | grep MPI`.
You can just remove the assertion line and I think it should work well then.",please include full error message output version remove assertion line think work well,issue,negative,positive,positive,positive,positive,positive
510299042,"Ah, Thank you. i tried on
but, I encountered another errors... I'm sorry to keep asking.
i use to trainer is horovod!. and i have eight gpu. so, I entered the command.
here...
`CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 NCCL_DEBUG=INFO mpirun -np 8 --output-filename mylog python -m train.py --config \
    DATA.BASEDIR=COCO/DIR \
    TRAINER=horovod
`
i meet error message.

> 'MPI_COMM_WORLD SIZE' not in os.environ

so.... Is this error an 'mpirun'?",ah thank tried another sorry keep use trainer eight command python meet error message size error,issue,negative,negative,negative,negative,negative,negative
510295861,"If you think code is the problem, please provide something that others can easily run to reproduce the issue.

If you think windows system is the problem, then it's unlikely I can help with that.",think code problem please provide something easily run reproduce issue think system problem unlikely help,issue,negative,negative,neutral,neutral,negative,negative
510291131,"i just run https://github.com/amiralansary/tensorpack-medical/tree/master/examples/LandmarkDetection/DQN examples using https://github.com/amiralansary/tensorpack-medical/tree/master/examples/LandmarkDetection/DQN/data under my env.
the main difference is my windows system.
some ops be placed at CPU when using gradproc.GlobalNormClip()
",run main difference system,issue,negative,positive,positive,positive,positive,positive
510286192,"> i don't have backbone weight..... How should i do?

You should train your backbone first. The existing backbones are trained with tensorpack on ImageNet.

> and i want to scratch training. so, i make a config.py

It's very expensive, and you need to implement your backbones following the rules from the paper of training from scratch (https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN#faster-r-cnn--mask-r-cnn-on-coco). In brief, the rules in the paper are:
1. use GN or SyncBN
2. train longer",backbone weight train backbone first trained want scratch training make expensive need implement following paper training scratch brief paper use train longer,issue,negative,negative,neutral,neutral,negative,negative
510171717,"> Case 01
lr = tf.get_variable('learning_rate', initializer=1e-3, trainable=False)
opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)
return optimizer.apply_grad_processors(opt, [gradproc.GlobalNormClip(10)])
get tf timeline result and too slow training speed

I add this optimizer to an official tensorpack example (imagenet-resnet.py) and tested it on TF 1.6. No significant slow down is observed.

Please try to provide enough details to reproduce the issue if you can.",case opt return opt get result slow training speed add official example tested significant slow please try provide enough reproduce issue,issue,negative,negative,neutral,neutral,negative,negative
509920713,"This is a pretty tricky issue. Thanks for finding out the reason that a `relu` call will affect the name of the tensors defined later due to name conflict.
I've updated the code in the above commit so that now the code will:
1. In tensorpack, print a warning when it expects to enter the name scope but fails due to name conflict
2. In the Faster R-CNN example, crash earlier with better messages when it does not find the tensor of the expected name",pretty tricky issue thanks finding reason call affect name defined later due name conflict code commit code print warning enter name scope due name conflict faster example crash better find tensor name,issue,negative,positive,positive,positive,positive,positive
509910119,"I find it!!
i use many 'tf.nn.relu(layer, name=""output"")'. so, i meet the errors. i solve this problem.
Thank you!!",find use many layer output meet solve problem thank,issue,negative,positive,positive,positive,positive,positive
509894114,"> i modify module name(""group0"") to ""OSA2"".

If this is the only change you made, I don't think you'll meet the errors you show. Your errors are likely to come from other changes you made that causes ""output/boxes"" to be undefined.",modify module name group change made think meet show likely come made undefined,issue,negative,neutral,neutral,neutral,neutral,neutral
509888146,"oh, Thank you. but, i don't understand your advice. 
well, i use to example(FasterRCNN). Here, i modify module name(""group0"") to ""OSA2"". 
then, this is problom??
Thank you!!",oh thank understand advice well use example modify module name group thank,issue,positive,neutral,neutral,neutral,neutral,neutral
509724508,"> Is there any use to call dataflow.MultiProcessRunner with num_proc=1 ?

Yes. It serves as prefetching & latency hiding. The dataflow runs in a separate process in parallel.

> will dataflow.MultiProcessRunner with num_proc >2 actually have any impact on the efficiency if the bottleneck is disk read performance ? 

It will not.

> what is ZeroMQ for communication used in dataflow.MultiProcessRunnerZMQ and the difference with the standard dataflow.MultiProcessRunner ?

http://zeromq.org/  It's faster than MultiProcessRunner in terms of the communication overhead.

You are recommended to benchmark first to quantify where is the bottleneck. ",use call yes latency separate process parallel actually impact efficiency bottleneck disk read performance communication used difference standard faster communication overhead first quantify bottleneck,issue,positive,positive,neutral,neutral,positive,positive
509689351,"> KeyError: ""The name 'tower-pred-0/output/boxes:0' refers to a Tensor which does not exist. The operation, 'tower-pred-0/output/boxes', does not exist in the graph.""

The original code has ""output/boxes"" but yours doesn't.",name tensor exist operation exist graph original code,issue,negative,positive,positive,positive,positive,positive
509405636,"""modifying the models at where it breaks"" is what I have been doing. Was wondering if anyone has managed to achieve this yet. If we work together, we all succeed!",wondering anyone achieve yet work together succeed,issue,negative,neutral,neutral,neutral,neutral,neutral
509404860,">  after playing around with the scripts for a while it seems like it'll take more than removing the filter in data.py to fix it.

You are right. See also #1230 ",around like take removing filter fix right see also,issue,negative,positive,positive,positive,positive,positive
508655850,"The warning is from tensorflow's autograph since TF1.14, and has zero effect on our models.",warning autograph since zero effect,issue,negative,neutral,neutral,neutral,neutral,neutral
508547586,"IIRC I resize all images to the same shape. In this case the amount of computation will still be different across GPUs, but closer. I will mention this in NOTES.md.

An opportunity for speed up might be to produce images of similar shapes together instead of in arbitrary order.",resize shape case amount computation still different across closer mention opportunity speed might produce similar together instead arbitrary order,issue,negative,negative,neutral,neutral,negative,negative
508427630,Got it. I noticed the scaling efficiency of FasterRcnn in [NOTES.md](https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md) is up to 90%.  How do you measure it? Does every rank have to process exactly the same data?,got scaling efficiency measure every rank process exactly data,issue,negative,negative,negative,negative,negative,negative
508413159,But in any case there isn't much point measuring scalability of heterogeneous workload. Using a homogeneous model like https://github.com/tensorpack/benchmarks/tree/master/ResNet-MultiGPU to measure scalability makes more sense.,case much point measuring heterogeneous homogeneous model like measure sense,issue,negative,positive,positive,positive,positive,positive
508410831,"In R-CNN each sample goes through different amount of computation, so it is unreasonable to compare speedup with traditional models.
In R-CNN each sample has different sizes and require longer cudnn warmup time, so how you measure the speedup matters a lot. It will be better if you show raw logs rather than your interpretation of them.

",sample go different amount computation unreasonable compare traditional sample different size require longer time measure lot better show raw rather interpretation,issue,negative,positive,neutral,neutral,positive,positive
508344056,"> This should be impossible, and it might be due to an incorrect way of measuring speed (e.g. If the speed has high variance.

Yes, you are right.  The reason that it's slow in the beginning is that opening too many threads on my computer costs a lot of time, but it gradually speeds up. I should've run a thorough test before asking.  Thanks a lot! ",impossible might due incorrect way measuring speed speed high variance yes right reason slow beginning opening many computer lot time gradually run thorough test thanks lot,issue,positive,positive,neutral,neutral,positive,positive
508331387,"This should be impossible, and it might be due to an incorrect way of measuring speed (e.g. If the speed has high variance.",impossible might due incorrect way measuring speed speed high variance,issue,negative,negative,negative,negative,negative,negative
507953147,"@ppwwyyxx The maximum displacement for the correlation layer is fixed
https://github.com/tensorpack/tensorpack/blob/7e7e577fe301302632d0232967c39f26d2908849/examples/OpticalFlow/flownet_models.py#L401
As there are several transposed convolutions afterwards, the `max_rad` might become much larger than 40 in some corner cases. Your current fix seems to be better.
",maximum displacement correlation layer fixed several afterwards might become much corner current fix better,issue,negative,positive,positive,positive,positive,positive
507869493,"Thanks! It looks like `max_rad = max(max_rad, 1.0)` is good enough in filtering out the noises. This is implemented in the above commit. Now the visualization for identical frames should be empty.
A typical `max_rad` is 10~40 in several videos I tested. Setting it to a constant 40 also seems to work well.",thanks like good enough filtering commit visualization identical empty typical several tested setting constant also work well,issue,positive,positive,neutral,neutral,positive,positive
507866702,"By all means I am not an expert in optical flow either.

But the visualization seems really just showing what is not there. As a quick test you might want to just print the values to the terminal. Just to confirm they are indeed in the same range as in FlowNet2-C. It would be interesting to see what's the mean, max and min of the optical flow values.

Looking at different implementations of flow visualizations most keep max_rad as a user specified parameter btw.

Could you attach the individual input image here (drag and drop). Otherwise would be difficult to reproduce.",expert optical flow either visualization really showing quick test might want print terminal confirm indeed range would interesting see mean min optical flow looking different flow keep user parameter could attach individual input image drag drop otherwise would difficult reproduce,issue,negative,positive,neutral,neutral,positive,positive
507865129,"@PatWie and @ppwwyyxx Thanks for the clarification and the prompt response. The above results are from the original code and the models that you provided inside /examples/OpticalFlow/. I am going to try this corner cases with the other re-implementation (PyTorch, and so on) to see how they perform, if I get any clue I will let you know. ",thanks clarification prompt response original code provided inside going try corner see perform get clue let know,issue,positive,positive,positive,positive,positive,positive
507860167,"@PatWie thanks for the confirmation. Asking because I'm not familiar with the physical meaning of this normalizer -- is it OK to use a constant normalizer? This may also make visualizations more consistent between a sequence of (>2) frames. Right now when I tried on >2 frames, the outputs of each pair have dramatically different background colors.",thanks confirmation familiar physical meaning normalizer use constant normalizer may also make consistent sequence right tried pair dramatically different background color,issue,negative,positive,positive,positive,positive,positive
507858150,"@ppwwyyxx is correct. You cannot expect these optical flow fields to be blank with the current implementation. Any small change (noise) will be amplified in
https://github.com/tensorpack/tensorpack/blob/ca0969089847c37a893a8e99317214c5899278db/examples/OpticalFlow/helper.py#L86

And there are some numerical differences between the original Caffe model and the re-implementation (within differences within 1e-7). Which I *believe* is not the reason.

Have you tried the reference implementation? I haven't tested the corner cases there. The reference model might also produce these artifacts since you are testing corner cases.

The pattern in your post looks clearly like noise in the first optical flow estimate which is then simply upscaled in the refinement steps. ",correct expect optical flow blank current implementation small change noise numerical original model within within believe reason tried reference implementation tested corner reference model might also produce since testing corner pattern post clearly like noise first optical flow estimate simply refinement,issue,positive,positive,neutral,neutral,positive,positive
507847799,"Thank you so much for your prompt response. 

(1) I thought maybe for any reproduction matter I should provide my current version of the software. My tensorpack version is ""0.9.5"", my Python version is ""Python 3.5.2 "", and also my Tensorflow version is ""1.11.0""

(2) This is what I did to minimally change the flownet2.py code for evaluation purpose:
- Test 1: inside apply() function, I added to following line after imread(): 
```
    left = np.zeros_like(left)
    right = np.zeros_like(right)
``` 
![flow output_screenshot_02 07 2019 - flownet2-c](https://user-images.githubusercontent.com/8812771/60546942-520b1000-9cd3-11e9-8c28-48169db111e5.png)
![flow output_screenshot_02 07 2019-flownet2](https://user-images.githubusercontent.com/8812771/60546944-52a3a680-9cd3-11e9-95f0-d053fa91ab35.png)

- Test 2: load the same image as the left and right image. I used DAVIS/Bear/00000.jpg

![flow output_screenshot_02 07 2019-flownet2-bear](https://user-images.githubusercontent.com/8812771/60546955-58998780-9cd3-11e9-9db8-e358d529e6b0.png)
![flow output_screenshot_02 07 2019-flownet2-c-bear](https://user-images.githubusercontent.com/8812771/60546957-59321e00-9cd3-11e9-9ff1-5276313d3c3a.png)

The above results are for FlowNet2 and FlowNet2-C networks, respectively. Interestingly, in the FlowNet2 cases the edges of the bear body and background scene are distinguishable.  However, the results seems reasonable when you actually use two distinguishable and non-identical images. ",thank much prompt response thought maybe reproduction matter provide current version version python version python also version minimally change code evaluation purpose test inside apply function added following line left left right right flow flow test load image left right image used flow flow respectively interestingly bear body background scene distinguishable however reasonable actually use two distinguishable,issue,positive,positive,positive,positive,positive,positive
507835658,"> (2) And also, do you have a more straight forward suggestion, either for variable key changes in the restored dictionary or eliminating the argscope names to make sure the reproducing of the results in the restored model?

`np.load` will give you the dict and you can do whatever renaming on that dict, and set them to the variables in your graph. This is basically what you did in Method 2, and on a high level the strategy should definitely work. ",also straight forward suggestion either variable key dictionary make sure model give whatever set graph basically method high level strategy definitely work,issue,positive,positive,positive,positive,positive,positive
507833650,"> (1) Interestingly, using the pre-trained models and your flownet2.py code, the optical flow of two still frames (two identical image, or two totally black or white images) are not near zero and they generate some patterns with different and random color values. My question is that if this much of errors is normal?

I can verify this issue, but running on two consecutive frames works well.
This looks more like an issue with the visualization logic. For example, this line normalizes the color with a data-dependent normalizer which looks suspicious: https://github.com/tensorpack/tensorpack/blob/ca0969089847c37a893a8e99317214c5899278db/examples/OpticalFlow/helper.py#L100
@PatWie do you have cycles to take a look at why this happens? I'm not very familiar with the flow code.",interestingly code optical flow two still two identical image two totally black white near zero generate different random color question much normal verify issue running two consecutive work well like issue visualization logic example line color normalizer suspicious take look familiar flow code,issue,positive,positive,neutral,neutral,positive,positive
506956622,"For reading better, I paste the picture of code:
![image](https://user-images.githubusercontent.com/33385095/60384734-265e0f00-9ab4-11e9-88b3-13151b9805d4.png)


Doesn't the code implement the following function copied from _DoreFa-Net_ paper ?. In this paper, the function is for quantizing weights? Is there anything I misunderstand ?
![image](https://user-images.githubusercontent.com/33385095/60384709-aafc5d80-9ab3-11e9-8a04-03bc250a0b23.png)

![image](https://user-images.githubusercontent.com/33385095/60384617-a7b4a200-9ab2-11e9-8a25-4544f91f1c1e.png)
",reading better paste picture code image code implement following function copied paper paper function anything misunderstand image image,issue,negative,positive,positive,positive,positive,positive
506761985,"@Remember2018 
Can you please make a summary of the steps required to train and run inference on the CPU without MKL support? It would be of great help. I am trying to export a model trained using this library to ONNX.",remember please make summary train run inference without support would great help trying export model trained library,issue,negative,positive,positive,positive,positive,positive
506454273,"thx a lot, I use the model in the table and get the right output. ",lot use model table get right output,issue,negative,positive,positive,positive,positive,positive
505945440,There is not enough code to reproduce your issue. But it seems like you just need `config_proto.allow_soft_placement=True`.,enough code reproduce issue like need,issue,negative,neutral,neutral,neutral,neutral,neutral
505826593,"im using keras ResNet50 in a tensorflow training loop as:
```
self.sess = tf.Session(config=config_proto)
tf.keras.backend.set_session(self.sess)
self.img = tf.placeholder(tf.float32, [None, 256, 256, 3])
self.model = tf.keras.applications.ResNet50(input_tensor=self.img,
                                                include_top=False,
                                                weights=None,
                                                pooling='avg')
dense = tf.keras.layers.Dense(64)
out = dense(init_model.layers[-1].output)
.
.
.
self.sess.run(tf.global_variables_initializer())
self.model.load_weights(self.model_weights)
opt = tf.train.MomentumOptimizer(learning_rate=self.lr, momentum=0.9)
opt = AccumGradOptimizer(opt,self.iter_size)
.
.
.

```

and I got this error: 

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource conv1/kernel located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0
	 [[Node: AccumGradOptimizer/cond/Momentum/update_conv1/kernel/ResourceApplyMomentum = ResourceApplyMomentum[T=DT_FLOAT, use_locking=false, use_nesterov=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](AccumGradOptimizer/cond/Momentum/update_conv1/kernel/Read/ReadVariableOp/Switch/_4783, AccumGradOptimizer/cond/Momentum/update_conv1/kernel/ResourceApplyMomentum/Switch:1, AccumGradOptimizer/cond/Momentum/update_conv1/kernel/ResourceApplyMomentum/Switch_1/_4785, AccumGradOptimizer/cond/ReadVariableOp, AccumGradOptimizer/cond/Momentum/momentum/_4787)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_val_script.py"", line 75, in <module>
    model_weights = model.train(train_img, database_img, query_img, args)
  File ""/DeepHash/DeepHash/model/dtq/__init__.py"", line 10, in train
    model.train_cq(img_train, img_query, img_database, config.R)
  File ""/DeepHash/DeepHash/model/dtq/dtq.py"", line 382, in train_cq
    self.b_img: codes})
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource conv1/kernel located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0
	 [[Node: AccumGradOptimizer/cond/Momentum/update_conv1/kernel/ResourceApplyMomentum = ResourceApplyMomentum[T=DT_FLOAT, use_locking=false, use_nesterov=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](AccumGradOptimizer/cond/Momentum/update_conv1/kernel/Read/ReadVariableOp/Switch/_4783, AccumGradOptimizer/cond/Momentum/update_conv1/kernel/ResourceApplyMomentum/Switch:1, AccumGradOptimizer/cond/Momentum/update_conv1/kernel/ResourceApplyMomentum/Switch_1/_4785, AccumGradOptimizer/cond/ReadVariableOp, AccumGradOptimizer/cond/Momentum/momentum/_4787)]]

Caused by op 'AccumGradOptimizer/cond/Momentum/update_conv1/kernel/ResourceApplyMomentum', defined at:
  File ""train_val_script.py"", line 75, in <module>
    model_weights = model.train(train_img, database_img, query_img, args)
  File ""/DeepHash/DeepHash/model/dtq/__init__.py"", line 6, in train
    model = DTQ(config)
  File ""/DeepHash/DeepHash/model/dtq/dtq.py"", line 112, in __init__
    self.train_op = self.apply_loss_function(self.global_step)
  File ""/DeepHash/DeepHash/model/dtq/dtq.py"", line 244, in apply_loss_function
    return opt.apply_gradients(grads_and_vars, global_step=global_step)
  File ""/usr/local/lib/python3.5/dist-packages/tensorpack/tfutils/optimizer.py"", line 219, in apply_gradients
    op = tf.cond(pred, update_grad, tf.no_op)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2048, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1895, in BuildCondBranch
    original_result = fn()
  File ""/usr/local/lib/python3.5/dist-packages/tensorpack/tfutils/optimizer.py"", line 210, in update_grad
    update_op = self._opt.apply_gradients(slots_and_vars)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 605, in apply_gradients
    update_ops.append(processor.update_op(self, grad))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 166, in update_op
    update_op = optimizer._resource_apply_dense(g, self._v)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/momentum.py"", line 108, in _resource_apply_dense
    use_nesterov=self._use_nesterov)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/gen_training_ops.py"", line 1750, in resource_apply_momentum
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Trying to access resource conv1/kernel located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0
	 [[Node: AccumGradOptimizer/cond/Momentum/update_conv1/kernel/ResourceApplyMomentum = ResourceApplyMomentum[T=DT_FLOAT, use_locking=false, use_nesterov=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](AccumGradOptimizer/cond/Momentum/update_conv1/kernel/Read/ReadVariableOp/Switch/_4783, AccumGradOptimizer/cond/Momentum/update_conv1/kernel/ResourceApplyMomentum/Switch:1, AccumGradOptimizer/cond/Momentum/update_conv1/kernel/ResourceApplyMomentum/Switch_1/_4785, AccumGradOptimizer/cond/ReadVariableOp, AccumGradOptimizer/cond/Momentum/momentum/_4787)]]

Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7efd3b8c9da0>>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 686, in __del__
TypeError: 'NoneType' object is not callable
```

tensorflow: 1.10.0
tensorpack: 0.9.5
",training loop none dense dense opt opt opt got error recent call last file line return file line file line trying access resource device device node handling exception another exception recent call last file line module file line train file line file line run file line file line file line raise type message trying access resource device device node defined file line module file line train model file line file line return file line file line return file line cond file line file line file line self grad file line file line file line file line file line return file line file line see trying access resource device device node exception bound method object recent call last file line object callable,issue,negative,neutral,neutral,neutral,neutral,neutral
505665972,Closing since it seems there isn't a elegant way to make examples importable.,since elegant way make importable,issue,negative,positive,positive,positive,positive,positive
505665659,"Closing since there is no activity.
If you want others to diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))
",since activity want diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected,issue,negative,positive,positive,positive,positive,positive
505665309,"> I think this is pretty much the same Keras problem as this:
tensorflow/tensorflow#29371
But I think it hasn't been fixed yet. I guess it's more of a Keras problem for now..

It does look like this is just Keras limitation that makes it hard to use it with other libraries.",think pretty much problem think fixed yet guess problem look like limitation hard use,issue,negative,positive,neutral,neutral,positive,positive
505665033,"> Somehow, python imagenet-resnet-keras.py --fake doesn't seem to work on my machine..

You did not clone the code correctly. The code contains a soft link (imagenet_utils.py) to a file in a different directory. It seems that softlink is somehow broken when you clone the code.",somehow python fake seem work machine clone code correctly code soft link file different directory somehow broken clone code,issue,negative,negative,negative,negative,negative,negative
505523287,"I hava a model that uses slicing operations, did you fix the problem ? ",model slicing fix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
505086080,"It stops iteration because the mapping function you provided returns `None`.
As the [documentation](https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.MapData) said if your mapping function returns None the data is discarded/skipped. Therefore all your data are skipped and the dataflow stops.",iteration function provided none documentation said function none data therefore data,issue,negative,neutral,neutral,neutral,neutral,neutral
505082503,"As said in nvidia documentation: https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceEnumvs.html

> NVML_ERROR_LIB_RM_VERSION_MISMATCH = 18
 RM detects a driver/library version mismatch.

It's likely that you did not reboot the machine since last time the driver is updated.
",said documentation version mismatch likely machine since last time driver,issue,negative,neutral,neutral,neutral,neutral,neutral
504603231,remap_variables does not affect the mechanism with which the gradients are being computed: Each op in the graph defines how its gradient is computed using its input and output,affect mechanism graph gradient input output,issue,negative,neutral,neutral,neutral,neutral,neutral
504591335,"Thank you for your response. I think I am confused about the working of ""with remap_variables(binarize_weight)"" and its use with the graph. 

I understand The ""remap_variables"" function gets executed, which fetches all the trainable variables and quantizes them and returns a tensor containing quantized values. This tensor is in turn used by the operations in the graph to produce outputs. 
But then during the backward pass, will the compute_gradients use the quantized tensor or the floating values of the variable for computing gradients?
Can you please explain that how does this ""remap_variables"" functionality interact with the graph, especially when the gradients are being computed. 
Apologies if I am not framing my question correctly. I am not extremely familiar with complete flow of tensorflow. Thanks again for your help. ",thank response think confused working use graph understand function executed trainable tensor tensor turn used graph produce backward pas use tensor floating variable please explain functionality interact graph especially framing question correctly extremely familiar complete flow thanks help,issue,positive,positive,neutral,neutral,positive,positive
504211317,"Here is the loss function. 
![image](https://user-images.githubusercontent.com/40010065/59867433-08b9d880-9343-11e9-9db9-968581c4750a.png)
In the loss function, Nreg is not exactly the same as Ncls, where Ncls = 256 and λ = 10, Nreg ∼ 2400. Nreg/λ is approximately close to Ncls. Your code should be right. Thank you for your replay. ",loss function image loss function exactly approximately close code right thank replay,issue,negative,negative,neutral,neutral,negative,negative
504119299,"If you only set the graph-level seed, the op-level seed will be the same as long as the graph stays the same. If you change the graph, the op seed will change (we're not 100% sure how much the graph needs to change before causing a new op seed). 

We rely on the op-level seed to ensure consistency during development, particularly when we are optimizing or rewriting a certain part of the model and we want to rule out the possibility of a changed seed in a different part of the model impacting our accuracy. 

It might not be necessary if we had a better understanding of what graph changes lead to new op seeds and which don't.",set seed seed long graph stay change graph seed change sure much graph need change causing new seed rely seed ensure consistency development particularly certain part model want rule possibility seed different part model accuracy might necessary better understanding graph lead new,issue,positive,positive,positive,positive,positive,positive
504068395,"The line was explained as ""a bug"", so there isn't much more to say. In a normal network you apparently do not need to have this line.

See also https://github.com/tensorpack/tensorpack/issues/40#issuecomment-271124312",line bug much say normal network apparently need line see also,issue,negative,positive,positive,positive,positive,positive
503879660,"@ppwwyyxx , Thanks all the same. But, I would suggest that it is better to provide more details on some weird codes to help us figure out the problems. such as:
https://github.com/tensorpack/tensorpack/blob/22f410e244a51fc4106396963b42a801fac23d0e/examples/DoReFa-Net/resnet-dorefa.py#L107
",thanks would suggest better provide weird help u figure,issue,positive,positive,neutral,neutral,positive,positive
503821040,"> But from the paper, it said it should be divided by the number of positive anchors

Where in the paper?",paper said divided number positive paper,issue,positive,positive,positive,positive,positive,positive
503670160,"Somehow, python imagenet-resnet-keras.py --fake doesn't seem to work on my machine..

```
(dps42_dev) C:\Users\dps42\Desktop\dps42\tensorpack\examples\keras>python imagenet-resnet-keras.py --fake
Traceback (most recent call last):
  File ""imagenet-resnet-keras.py"", line 20, in <module>
    from imagenet_utils import fbresnet_augmentor, get_imagenet_dataflow
  File ""C:\Users\dps42\Desktop\dps42\tensorpack\examples\keras\imagenet_utils.py"", line 1
    ../ResNet/imagenet_utils.py
    ^
SyntaxError: invalid syntax
```

Anyway, I tried to see if I could convert the simple MNIST Keras sequential model to a functional one. 
Here's the code: 
[keras_functional.zip](https://github.com/tensorpack/tensorpack/files/3307518/keras_functional.zip)



But I ran into a problem.

```
(dps42_dev) C:\Users\dps42\Desktop\dps42\tensorpack\examples\keras>python mnist-keras-experiment-functional2.py
Using TensorFlow backend.
[0619 14:05:51 @logger.py:125] WRN Log directory train_log\mnist-keras-experiment-functional2 exists! Use 'd' to delete it.
[0619 14:05:51 @logger.py:128] WRN If you're resuming from a previous run, you can choose to keep it.
Press any other key to exit.
Select Action: k (keep) / d (delete) / q (quit):d
[0619 14:06:01 @logger.py:90] Argv: mnist-keras-experiment-functional2.py
[0619 14:06:01 @fs.py:101] WRN Env var $TENSORPACK_DATASET not set, using C:\Users\dps42\tensorpack_data for datasets.
[0619 14:06:01 @concurrency.py:268] WRN Command 'nvidia-smi -L' failed, return code=1
[0619 14:06:01 @concurrency.py:269] WRN 'nvidia-smi' is not recognized as an internal or external command,
operable program or batch file.

[0619 14:06:01 @gpu.py:59] Loading local devices by TensorFlow ...
2019-06-19 14:06:01.474642: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-06-19 14:06:01.743960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:19:00.0
totalMemory: 11.00GiB freeMemory: 8.99GiB
2019-06-19 14:06:01.864999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:1a:00.0
totalMemory: 11.00GiB freeMemory: 8.99GiB
2019-06-19 14:06:01.997348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 2 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:67:00.0
totalMemory: 11.00GiB freeMemory: 8.99GiB
2019-06-19 14:06:02.231076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 3 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:68:00.0
totalMemory: 11.00GiB freeMemory: 8.99GiB
2019-06-19 14:06:02.238300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1, 2, 3
2019-06-19 14:06:03.843315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-19 14:06:03.847226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1 2 3
2019-06-19 14:06:03.849686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N N N N
2019-06-19 14:06:03.852069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   N N N N
2019-06-19 14:06:03.855023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 2:   N N N N
2019-06-19 14:06:03.857381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 3:   N N N N
2019-06-19 14:06:03.860107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:0 with 8668 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:19:00.0, compute capability: 7.5)
2019-06-19 14:06:04.979376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:1 with 8668 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5)
2019-06-19 14:06:06.093458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:2 with 8668 MB memory) -> physical GPU (device: 2, name: GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5)
2019-06-19 14:06:07.446622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/device:GPU:3 with 8667 MB memory) -> physical GPU (device: 3, name: GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5)
[0619 14:06:10 @training.py:50] [DataParallel] Training a model of 4 towers.
[0619 14:06:10 @interface.py:31] Automatically applying QueueInput on the DataFlow.
[0619 14:06:10 @interface.py:43] Automatically applying StagingInput on the DataFlow.
[0619 14:06:10 @input_source.py:223] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0619 14:06:10 @training.py:110] Building graph for training tower 0 on device /gpu:0 ...
[0619 14:06:10 @registry.py:135] conv0 input: [None, 28, 28, 1]
[0619 14:06:10 @registry.py:143] conv0 output: [None, 28, 28, 32]
[0619 14:06:10 @registry.py:135] conv1_1 input: [None, 28, 14, 16]
[0619 14:06:10 @registry.py:143] conv1_1 output: [None, 28, 14, 32]
[0619 14:06:10 @registry.py:135] conv1_2 input: [None, 28, 14, 32]
[0619 14:06:10 @registry.py:143] conv1_2 output: [None, 28, 14, 32]
[0619 14:06:10 @registry.py:135] conv2_1 input: [None, 28, 7, 16]
[0619 14:06:10 @registry.py:143] conv2_1 output: [None, 28, 7, 32]
Traceback (most recent call last):
  File ""mnist-keras-experiment-functional2.py"", line 151, in <module>
    metrics='categorical_accuracy'
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\contrib\keras.py"", line 282, in compile
    metrics=metrics)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\contrib\keras.py"", line 219, in setup_keras_trainer
    lambda: optimizer)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\utils\argtools.py"", line 176, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\train\tower.py"", line 224, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\train\trainers.py"", line 189, in _setup_graph
    grad_list = self._builder.call_for_each_tower(tower_fn)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\graph_builder\training.py"", line 226, in call_for_each_tower
    use_vs=[False] + [True] * (len(self.towers) - 1))
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\graph_builder\training.py"", line 122, in build_on_towers
    return DataParallelBuilder.call_for_each_tower(*args, **kwargs)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\graph_builder\training.py"", line 117, in call_for_each_tower
    ret.append(func())
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\train\tower.py"", line 280, in get_grad_fn
    return compute_grad_from_inputs(*inputs)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\train\tower.py"", line 255, in compute_grad_from_inputs
    cost = get_cost_fn(*inputs)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\tfutils\tower.py"", line 290, in __call__
    output = self._tower_fn(*args)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\contrib\keras.py"", line 172, in get_cost
    outputs = model_caller(*input_tensors)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\contrib\keras.py"", line 92, in __call__
    model = self.cached_model = self.get_model(*input_tensors)
  File ""mnist-keras-experiment-functional2.py"", line 105, in get_keras_model
    M  = tf.keras.models.Model(visible, x, name='my_model')
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 113, in __init__
    super(Model, self).__init__(*args, **kwargs)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 79, in __init__
    self._init_graph_network(*args, **kwargs)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorflow\python\training\checkpointable\base.py"", line 364, in _method_wrapper
    method(self, *args, **kwargs)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 217, in _init_graph_network
    '(thus holding past layer metadata). Found: ' + str(x))
ValueError: Output tensors to a Model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: Tensor(""fc10/Softmax:0"", shape=(?, 10), dtype=float32, device=/device:GPU:0)
```

I think this is pretty much the same Keras problem as this:
https://github.com/tensorflow/tensorflow/issues/29371

But I think it hasn't been fixed yet. I guess it's more of a Keras problem for now..









",somehow python fake seem work machine python fake recent call last file line module import file line invalid syntax anyway tried see could convert simple sequential model functional one code ran problem python log directory use delete previous run choose keep press key exit select action keep delete quit set command return internal external command operable program batch file loading local binary use found device name ti major minor found device name ti major minor found device name ti major minor found device name ti major minor visible device interconnect strength edge matrix device memory physical device name ti bus id compute capability device memory physical device name ti bus id compute capability device memory physical device name ti bus id compute capability device memory physical device name ti bus id compute capability training model automatically automatically setting queue building graph training tower device input none output none input none output none input none output none input none output none recent call last file line module file line compile file line lambda file line wrapper return file line input file line file line false true file line return file line file line return file line cost file line output file line file line model file line visible file line super model self file line file line method self file line thus holding past layer found output model must output layer thus holding past layer found tensor think pretty much problem think fixed yet guess problem,issue,negative,negative,neutral,neutral,negative,negative
503579117,"See the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md):

## Some typical questions that we DO NOT answer:

""The examples do not perform as expected after I change the models/dataset/parameters/etc.""

Tensorpack maintainers make sure the examples perform well without modifications. But it's your job to pick the model and parameters that are suitable for your own situation. We cannot help with such questions unless they appear to be a bug in tensorpack.",see issue template typical answer perform change make sure perform well without job pick model suitable situation help unless appear bug,issue,positive,positive,positive,positive,positive,positive
503578352,I cannot diagnose anything from this limited information. My best guess is that your validation set is not large enough to cause OOM.,diagnose anything limited information best guess validation set large enough cause,issue,positive,positive,positive,positive,positive,positive
503447540,"We encounter OOM error during training. Is the validation set too big? Our validation set is an (1460,1024,768,16) array. Our training image is (1024,768,16), batch_size = 1 and GPU memory is 12GB. ",encounter error training validation set big validation set array training image memory,issue,negative,neutral,neutral,neutral,neutral,neutral
503408020,"1. There is no concept of ""copy"" in tensorflow so I'm not sure what you're trying to ask. The variables stay unquantized, if that's what you want to know.
2. The floating point variables are updated.
3. Because in tensorflow the optimizer optimizes trainable variables (in this case the unquantized floating point variables). ",concept copy sure trying ask stay want know floating point trainable case floating point,issue,negative,positive,positive,positive,positive,positive
503319391,"Please post issues following the issue template:
(1) If you're using examples, what's the command you run:
(2) If you're using examples, have you made any changes to the examples? Paste `git status; git diff` here:

",please post following issue template command run made paste git status git,issue,negative,neutral,neutral,neutral,neutral,neutral
503177396,thank you. I'll take a look. Sorry that I missed it.,thank take look sorry,issue,negative,negative,negative,negative,negative,negative
503170997,Closing due to lack of activity. And the issue is very likely caused by a third-party library.,due lack activity issue likely library,issue,negative,negative,neutral,neutral,negative,negative
503170109,"You can know your rank by `hvd.rank()`. It's up to you to decide whether you want to divide the dataset according to rank.
I cannot answer why your code stucks without any details, unless you can fill out the issue template.",know rank decide whether want divide according rank answer code without unless fill issue template,issue,negative,negative,negative,negative,negative,negative
502458692,"See the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md):


+ ""The examples do not perform as expected after I change the models/dataset/parameters/etc."":

  Tensorpack maintainers make sure the examples perform well without modifications.
  But it's your job to pick the model and parameters that are suitable for your own situation.
  We cannot help with such questions unless they appear to be a bug in tensorpack.

+ ""Why my own model doesn't perform well?"", ""I don't understand this paper you implement."",
  ""How should I change the parameters for my own dataset?"":

  We do not answer machine learning questions.",see issue template perform change make sure perform well without job pick model suitable situation help unless appear bug model perform well understand paper implement change answer machine learning,issue,positive,positive,positive,positive,positive,positive
502023122,Keras's `model.summary` prints shape of outputs and parameters. Both are already printed in tensorpack before training.,shape already printed training,issue,negative,neutral,neutral,neutral,neutral,neutral
502012388,"Hi @ppwwyyxx, Is there a model.summary() (just like Keras) method for printing model architecture in TensorPack?",hi like method printing model architecture,issue,negative,neutral,neutral,neutral,neutral,neutral
501902138,"Hi Yuxin, @ppwwyyxx Thank you very much for your kind comment!",hi thank much kind comment,issue,positive,positive,positive,positive,positive,positive
501869016,"Loss scaling should be doable directly on the graph/optimizer level without touching tensorpack.

For seeds, my impression is that `tf.random.set_random_seed` can achieve the same effect of determinism. Why is per-op seed needed?
",loss scaling doable directly level without touching impression achieve effect determinism seed,issue,negative,positive,positive,positive,positive,positive
501861853,"I definitely agree - we really want to get away from having changes inside of the tensorpack lib. 

Our thinking on mixed precision is that once 1.14 is fully out, NVIDIA's auto mixed precision is the easiest path. We also added code to downcast variables, but it looks like Tensorpack now supports that directly.

There are a couple changes I need to follow up on (I'm not 100% sure why we did [this](https://github.com/armandmcqueen/tensorpack-mask-rcnn/blame/master/tensorpack/models/pool.py#L130)  or what the performance impact was for example).

One thing that might be useful to upstream is the ability to set op-level seeds, like we do [here](https://github.com/armandmcqueen/tensorpack-mask-rcnn/blame/master/tensorpack/models/conv2d.py#L38).

",definitely agree really want get away inside thinking mixed precision fully auto mixed precision easiest path also added code downcast like directly couple need follow sure performance impact example one thing might useful upstream ability set like,issue,positive,positive,positive,positive,positive,positive
501843744,"Thanks for the great work!
Merging the Mask R-CNN changes back seems to be pretty hard indeed. However it does seem that your changes on the tensorpack library (e.g., about fp16, loss scaling etc) can be implemented without modifying the library, or merged to the library. If that's successful it will also make your repo much simpler. If you think that's worth doing, we can start discussions about the individual changes.",thanks great work mask back pretty hard indeed however seem library loss scaling without library library successful also make much simpler think worth start individual,issue,positive,positive,positive,positive,positive,positive
501782997,"Tensorpack removes summaries not from the first tower since this is the expected behavior most of the time.
To add custom summaries, use a different collection by `tf.summary.image(collections=[""test""])`.
And summarize this collection by the callback `MergeAllSummaries(key=""test"")`.
See https://tensorpack.readthedocs.io/tutorial/summary.html",first tower since behavior time add custom use different collection test summarize collection test see,issue,negative,positive,positive,positive,positive,positive
501561432,"This is one of the things I tried but converting a TOCO model (or even a
non-optimized model that's not TOCO compatible) didn't work.

Thanks

On Wed, Jun 12, 2019 at 9:38 PM Yuxin Wu <notifications@github.com> wrote:

> The output is defined by the output_names option in PredictConfig. So you
> can let it only output boxes.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/1233?email_source=notifications&email_token=AGAQQDR6LFNSZOFYMIGZFTLP2HFLHA5CNFSM4HXQN6DKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXSPTTI#issuecomment-501545421>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AGAQQDS7ZMOID56R5TTR3NDP2HFLHANCNFSM4HXQN6DA>
> .
>


-- 
Best,
ilian H. Herzi
",one tried converting toco model even model toco compatible work thanks wed wrote output defined option let output thread reply directly view mute thread best,issue,positive,positive,positive,positive,positive,positive
501545421,The output is defined by the `output_names` option in `PredictConfig`. So you can let it only output boxes.,output defined option let output,issue,negative,neutral,neutral,neutral,neutral,neutral
501545231,I have no knowledge about these functions and in general don't trust functions I don't understand.,knowledge general trust understand,issue,positive,positive,neutral,neutral,positive,positive
501544948,"The document is correct (since it is built from master), and the pip package is lagging behind (it is built from last release tag).",document correct since built master pip package lagging behind built last release tag,issue,negative,negative,negative,negative,negative,negative
501534430,"@ppwwyyxx It seems the document is still using MultiProcessRunner, but the API in the pip package has been updated to MultiProcessPrefetchData. So do you plan to update the document recently?",document still pip package plan update document recently,issue,negative,neutral,neutral,neutral,neutral,neutral
501520580,The only difference is I added **tf.contrib.quantize.create_training_graph** and **tf.contrib.quantize.create_eval_graph** in the tensorpack's code. It seems I used them in wrong way. Do you have any idea about these two functions? ,difference added code used wrong way idea two,issue,negative,negative,negative,negative,negative,negative
501512199,"Ok thanks.

One more question then, is there a way to explicitly remove the masking operation from the pretrianed graph during conversion? 
Meaning somehow load COCO-MaskRCNN-R101FPN9xGNCasAugScratch.npz and modify it to only produce bounding boxes? 

Because I've been trying to determine which operations are not supported but I'm having trouble pinpointing those exact operators.

Thanks!",thanks one question way explicitly remove operation graph conversion meaning somehow load modify produce bounding trying determine trouble exact thanks,issue,negative,positive,positive,positive,positive,positive
501453539,"I do not remember what is not supported. 
To run the model with TOCO you need to either add the missing operators to TOCO, or implement Mask R-CNN with supported operators. Both are unrelated to tensorpack so we cannot help with it more.",remember run model toco need either add missing toco implement mask unrelated help,issue,negative,negative,negative,negative,negative,negative
501434266,"Hi ppwwyyxx, 

Thanks for the quick response!
And ok I will try that. 

If that's the case then, is there a way to create a TOCO operator that can support the Mask R-CNN implementation? In particular what's not supported?

",hi thanks quick response try case way create toco operator support mask implementation particular,issue,positive,positive,positive,positive,positive,positive
501432089,"The issue you run into is because you didn't call `finalize_configs`. See `predict.py` for where it is called.

However, last time I checked TOCO does not fully support the operators we use in this Mask R-CNN implementation, so there is probably no way to convert the graph to TOCO.",issue run call see however last time checked toco fully support use mask implementation probably way convert graph toco,issue,negative,neutral,neutral,neutral,neutral,neutral
501361327,"I don't think I understand your question, since a large part of it seems to be related to your own code.
But in general, there is no reason for a training graph to have the same node/tensor names as the evaluation graph. A model may very often perform different operations in training vs evaluation. 
The only things from training that need to be used in evaluation are the variables and they are saved in the checkpoint without any prefix.",think understand question since large part related code general reason training graph evaluation graph model may often perform different training evaluation training need used evaluation saved without prefix,issue,negative,positive,neutral,neutral,positive,positive
501182415,"It stops at 3125 because it is the size of your dataflow (1e5 // 32).

It does not stop at 3125 for the multi-threaded one because the multi-threaded one does not properly handle the size for now.

Note that as mentioned in https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.DataFlow.__len__, the size is only a rough guidance and is not always well-defined or correct. 
Although in this particular case it would be better to make the multi-thread and multi-process dataflow consistent with each other. Therefore marked as a feature request.",size stop one one properly handle size note size rough guidance always correct although particular case would better make consistent therefore marked feature request,issue,negative,positive,positive,positive,positive,positive
501106268,"You can find the answers in Tensorflow documentation: https://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth

To use it with tensorpack, there is a `session_config` argument in `TrainConfig`.",find documentation use argument,issue,negative,neutral,neutral,neutral,neutral,neutral
501101693,"The model is not as big as 16 G, so what is the memory used for? If I switch to a GPU with a smaller memory (12G), it takes up 8G and hints that I can gain performance by expanding the memory. ",model big memory used switch smaller memory gain performance expanding memory,issue,positive,neutral,neutral,neutral,neutral,neutral
501092406,"Hello, @ppwwyyxx . When I was training(batch_size=1), I found that FasterRCNN (ResNet-C4) takes up 16GB of GPU memory.  Is this normal? ",hello training found memory normal,issue,negative,positive,positive,positive,positive,positive
500647264,Training using images without groundtruth is not supported. You can implement the relevant changes by modifying the models at where it breaks.,training without implement relevant,issue,negative,positive,positive,positive,positive,positive
499985450,Batch size is equal to the number of GPUs used. But you can still tune it equivalently by using AccumGradOptimizer.,batch size equal number used still tune equivalently,issue,negative,neutral,neutral,neutral,neutral,neutral
499982016,"Thanks for your tips, It works on the weather data, and I'm curious where batchsize can be tuned, it seems to be fixed in the fasterrcnn code.      ",thanks work weather data curious tuned fixed code,issue,positive,positive,neutral,neutral,positive,positive
499267456,We have an implementation based on Tensorpack's that supports larger batch sizes [here](https://github.com/armandmcqueen/tensorpack-mask-rcnn). ,implementation based batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
499266101,"This is not a feature that someone could add easily. 
And as mentioned in https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md this is a limitation of tensorflow (or a limitation of tensor-based static graph computation framework, in general).",feature someone could add easily limitation limitation static graph computation framework general,issue,negative,positive,positive,positive,positive,positive
499264684,"This is a feature that someone could add easily? I am surprised that I couldn't find some hard-coded variable to change the batch size. Or, is this a limitation of tensorpack or the implementation of Faster R-CNN.",feature someone could add easily could find variable change batch size limitation implementation faster,issue,negative,positive,positive,positive,positive,positive
499253205,"You're right.

As said in the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=feature-requests.md):

> + ""Could you improve/implement an example/paper ?""
  -- The answer is: we have no plans to do so. We don't consider feature
  requests for examples or implement a paper for you, unless it demonstrates 
  some Tensorpack features not yet demonstrated in the existing examples.
  If you don't know how to do something yourself, you may ask a usage question.
",right said issue template could answer consider feature implement paper unless yet know something may ask usage question,issue,negative,positive,positive,positive,positive,positive
499162724,"1. As said in the README, we support training from scratch. But you can also see from the README and the relevant paper that it is not easy.
2. You need to make sure your images can be correctly read (by `TrainingDataPreprocessor`).
And you need to modify the input channels from 3 to 16 (in `model.inputs()`).
And you need to read https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md",said support training scratch also see relevant paper easy need make sure correctly read need modify input need read,issue,positive,positive,positive,positive,positive,positive
498967281,Thanks for finding this issue. It should be fixed now in the above commit.,thanks finding issue fixed commit,issue,positive,positive,positive,positive,positive,positive
498953761,"I find a tricky way to do this. Add init after https://github.com/tensorpack/tensorpack/blob/7c8cbba082a577c7eb48efd1925b59a38d41682f/tensorpack/tfutils/export.py#L64
with the two lines
```python
init=tf.global_variables_initializer()
sess.run(init)
``` ",find tricky way add two python,issue,negative,neutral,neutral,neutral,neutral,neutral
498562444,@ppwwyyxx Thank you so much. I'll close this issue.,thank much close issue,issue,negative,positive,positive,positive,positive,positive
498132952,The code you're running probably uses an ancient version of tensorpack. It is expecting methods that have been deprecated in tensorpack for >1 years. You can try tensorpack 0.9 or older versions which keeps the compatibility.,code running probably ancient version try older compatibility,issue,negative,positive,positive,positive,positive,positive
498128678,"> The TensorPack bases on TensorFlow, So should it support any back end that TF supports?

Yes. 

But note that TF does not support all the operations on all the backend.",base support back end yes note support,issue,positive,negative,negative,negative,negative,negative
498124724,"@ppwwyyxx Thanks for your quick reply. The TensorPack bases on TensorFlow, So should it support any back end that TF supports? ",thanks quick reply base support back end,issue,positive,negative,neutral,neutral,negative,negative
498107352,"The FasterRCNN code has made no assumptions on the type of GPU it runs on.
So it should be able to run on AMD GPUs as long as the GPUs support the relevant ops (NCHW format convolution/pooling, etc).
But I have not used an AMD GPU so I don't know in general whether there are other issues on the tensorflow side.",code made type able run long support relevant format used know general whether side,issue,negative,positive,positive,positive,positive,positive
497988890,Thanks! A suggestion - it might be helpful to include it in the table ,thanks suggestion might helpful include table,issue,positive,positive,positive,positive,positive,positive
496730201,"Hi @ppwwyyxx , thanks a lot for your comment! The `CONFIG_STR` is the parameters of `--config` when using `python train.py`.",hi thanks lot comment python,issue,negative,positive,positive,positive,positive,positive
496674137,Great. I've added these common issues in the efficient dataflow tutorial.,great added common efficient tutorial,issue,positive,positive,positive,positive,positive,positive
496651962,"Hi. The only lambda I'm using is one line from the example code I copied from ""Efficient Dataflow"" tutorial:
``` 
ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
```
The following code worked, without a lambda function as you suggested.

```
def decode_function(x):
    return cv2.imdecode(x, cv2.IMREAD_COLOR)

def load_lmdb3():
    ds = LMDBSerializer.load( LMDB_PATH, shuffle=False)
    
    num_images = len(ds)
    print( num_images )
    
    ds = LocallyShuffleData(ds, 50000)
    ds = MultiProcessRunner(ds, 5000, 2)       # NOTE: PrefetchData() deprecated in May 2019
    ds = MapDataComponent(ds, decode_function, 0)
    # ds = AugmentImageComponent( ds, lots_of_augmentors)
    ds = AugmentImageComponent( ds, [imgaug.Resize(224)] )  # temporary, will replace
    ds = MultiProcessRunner(ds, 25,2) 
    ds = BatchData(ds, 256, use_list=True)
    
    TestDataSpeed(ds).start()

# load LMDB
if __name__ == '__main__':
    load_lmdb3()    
```
Thank you so much for the patience.



",hi lambda one line example code copied efficient tutorial lambda following code worked without lambda function return print note may temporary replace load thank much patience,issue,positive,positive,neutral,neutral,positive,positive
496647344,This is a different problem. That means you should not use a lambda function in `load_lmdb3` (because it's not pickleable and it's running on windows). You should write a new function instead of the lambda function.,different problem use lambda function running write new function instead lambda function,issue,negative,positive,neutral,neutral,positive,positive
496646867,"Sorry. I copied it from the other terminal window. I apologize. 

I updated Tensorpack and tried the new code with the additional argument, but it seems that the previous problem about pickle still exists. 
```
[0528 15:00:32 @parallel.py:195] WRN MultiProcessRunner does support Windows. However, Windows requires more strict picklability on processes, which may lead of failure on some of the code.
Traceback (most recent call last):
  File ""load_lmdb.py"", line 80, in <module>
    load_lmdb3()
  File ""load_lmdb.py"", line 73, in load_lmdb3
    ds = MultiProcessRunner(ds, 25,1)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\dataflow\parallel.py"", line 214, in __init__
    start_proc_mask_signal(self.procs)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\utils\concurrency.py"", line 244, in start_proc_mask_signal
    p.start()
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\context.py"", line 212, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\context.py"", line 313, in _Popen
    return Popen(process_obj)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\popen_spawn_win32.py"", line 66, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\reduction.py"", line 59, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'load_lmdb3.<locals>.<lambda>'

(dps42_dev) C:\AI_Workspace\z_debug>Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\spawn.py"", line 106, in spawn_main
    exitcode = _main(fd)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\spawn.py"", line 116, in _main
    self = pickle.load(from_parent)
EOFError: Ran out of input
```",sorry copied terminal window apologize tried new code additional argument previous problem pickle still support however strict may lead failure code recent call last file line module file line file line file line file line start self file line return file line return file line file line dump file protocol ca pickle local object lambda recent call last file string line module file line file line self ran input,issue,negative,negative,neutral,neutral,negative,negative
496645005,"The log clearly says that 
> File ""load_lmdb.py"", line 73, in load_lmdb3
ds = MultiProcessRunner(ds, 25)

which means at line 73 in `load_lmdb.py`, you have `ds = MultiProcessRunner(ds, 25)` which is wrong and  should be changed. If you do change it, you will not see the same log.",log clearly file line line wrong change see log,issue,negative,negative,negative,negative,negative,negative
496644228,"From the previous conversation, I reported that I'd tried each of the three combinations instead of ds = MultiProcessRunner(ds, 25) below:
```
ds = MultiProcessRunner(ds, 25,1)
ds = MultiProcessRunner(ds, 25,2)
ds = MultiProcessRunner(ds, 5000,1)
```
But I had the same error log. Is there any missing argument?",previous conversation tried three instead error log missing argument,issue,negative,negative,negative,negative,negative,negative
496643126,"> File ""load_lmdb.py"", line 73, in load_lmdb3
ds = MultiProcessRunner(ds, 25)

As shown in the error log, you missed an argument here.",file line shown error log argument,issue,negative,neutral,neutral,neutral,neutral,neutral
496642227,"As posted above, my code was 
```
def load_lmdb3():
ds = LMDBSerializer.load( LMDB_PATH, shuffle=False)

num_images = len(ds)
print( num_images )

ds = LocallyShuffleData(ds, 50000)
ds = MultiProcessRunner(ds, 5000, 1)       # NOTE: PrefetchData() deprecated in May 2019
ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
# ds = AugmentImageComponent( ds, lots_of_augmentors)
ds = AugmentImageComponent( ds, [imaug.Resize(224)] )
ds = MultiProcessRunner(ds, 25) 
ds = BatchData(ds, 256, use_list=True)

TestDataSpeed(ds).start()
```
I replaced MultiProcessRunnerZMQ() with MultiProcessRunner, because ZMQ is not available on Windows.  But as mentioned above, I faced another issue:
```
[0528 14:04:13 @parallel.py:195] WRN MultiProcessRunner does support Windows. However, Windows requires more strict picklability on processes, which may lead of failure on some of the code.
Traceback (most recent call last):
File ""load_lmdb.py"", line 80, in 
load_lmdb3()
File ""load_lmdb.py"", line 73, in load_lmdb3
ds = MultiProcessRunner(ds, 25)
File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\dataflow\parallel.py"", line 201, in init
assert num_proc > 0, num_proc
TypeError: unorderable types: NoneType() > int()
```
Thank you for the patience and support.
",posted code print note may lambda available faced another issue support however strict may lead failure code recent call last file line file line file line assert unorderable thank patience support,issue,positive,positive,neutral,neutral,positive,positive
496640973,"It is very hard to understand the conversation. If this is your code:
> ds = LMDBSerializer.load(db, shuffle=False)
ds = LocallyShuffleData(ds, 50000)
ds = MultiProcessRunner(ds, 5000, 1)
ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
ds = AugmentImageComponent(ds, lots_of_augmentors)
ds = MultiProcessRunnerZMQ(ds, 25)
ds = BatchData(ds, 256)



Could you post again what is the error?",hard understand conversation code lambda could post error,issue,negative,negative,negative,negative,negative,negative
496639761,"I directly copied the example code from ""Efficient Dataflow"" tutorial.

ds = LMDBSerializer.load(db, shuffle=False)
ds = LocallyShuffleData(ds, 50000)
ds = MultiProcessRunner(ds, 5000, 1)
ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
ds = AugmentImageComponent(ds, lots_of_augmentors)
ds = MultiProcessRunnerZMQ(ds, 25)
ds = BatchData(ds, 256)

And I replaced MultiProcessRunnerZMQ with MultiProcessRunner. 
Did you mean I need only one MultiProcessRunner()?


",directly copied example code efficient tutorial lambda mean need one,issue,negative,negative,negative,negative,negative,negative
496634241,"yes. I tried each of the combinations below instead of ""ds = MultiProcessRunner(ds, 25) ""
ds = MultiProcessRunner(ds, 25,1)
ds = MultiProcessRunner(ds, 25,2)
ds = MultiProcessRunner(ds, 5000,1)   

But I seem to have the same issue...",yes tried instead seem issue,issue,negative,neutral,neutral,neutral,neutral,neutral
496632299,wait. sorry. I'll try to insert another argument to MultiProcessRunner().,wait sorry try insert another argument,issue,negative,negative,negative,negative,negative,negative
496630925,"If I don't use the code snippet above, I have another issue: 
[0528 14:04:13 @parallel.py:195] WRN MultiProcessRunner does support Windows. However, Windows requires more strict picklability on processes, which may lead of failure on some of the code.
Traceback (most recent call last):
  File ""load_lmdb.py"", line 80, in <module>
    load_lmdb3()
  File ""load_lmdb.py"", line 73, in load_lmdb3
    ds = MultiProcessRunner(ds, 25)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\dataflow\parallel.py"", line 201, in __init__
    assert num_proc > 0, num_proc
TypeError: unorderable types: NoneType() > int()

Before trying that above,  I had a few typos. Sorry. fixed below. 
def load_lmdb3():
    ds = LMDBSerializer.load( LMDB_PATH, shuffle=False)
    
    num_images = len(ds)
    print( num_images )
    
    ds = LocallyShuffleData(ds, 50000)
    ds = MultiProcessRunner(ds, 5000, 1)       # NOTE: PrefetchData() deprecated in May 2019
    ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
    # ds = AugmentImageComponent( ds, lots_of_augmentors)
    ds = AugmentImageComponent( ds, [imaug.Resize(224)] )
    ds = MultiProcessRunner(ds, 25) 
    ds = BatchData(ds, 256, use_list=True)
    
    TestDataSpeed(ds).start()",use code snippet another issue support however strict may lead failure code recent call last file line module file line file line assert unorderable trying sorry fixed print note may lambda,issue,negative,negative,negative,negative,negative,negative
496628902,did you mean it should work without the code snippet you suggested above?,mean work without code snippet,issue,negative,negative,negative,negative,negative,negative
496617596,"Sorry the correct code should be:
```python
class MyOwnLMDBData(DataFlow):
    def __init__(self, path):
        self.path = path
    def __iter__(self):
        d = LMDBSerializer.load(self.path)
        d.reset_state()
        yield from d
```",sorry correct code python class self path path self yield,issue,negative,negative,negative,negative,negative,negative
496614839,"It does not seem like a tensorpack bug to me. And your gray curve is not stable either.
I recommend you to use a smaller learning rate instead.",seem like bug gray curve stable either recommend use smaller learning rate instead,issue,positive,neutral,neutral,neutral,neutral,neutral
496614824,thanks for the suggestion. I'll try it later and see if it works.,thanks suggestion try later see work,issue,negative,positive,neutral,neutral,positive,positive
496613059,"```python
class MyOwnLMDBData(DataFlow):
    def __init__(self, path):
        self.path = path
    def __iter__(self):
        d = LMDBSerializer.load(self.path)
        yield from d
```
Using this instead of `LDMBSerializer.load` might makes it possible to run under multiprocessing, but perhaps less efficient than it should be.",python class self path path self yield instead might possible run perhaps le efficient,issue,negative,neutral,neutral,neutral,neutral,neutral
496612927,"Hi,
Like I said I have an implementation of my model in multi gpu with towers paradigm which converges, it seems to me like a tensor pack bug, synchronization or something else.. Do you have any idea or tool to debug? ",hi like said implementation model paradigm like tensor pack bug synchronization something else idea tool,issue,positive,neutral,neutral,neutral,neutral,neutral
496611925,"Looks like the ""Environment"" class is part of the lmdb database. So I guess you cannot use multiprocessing with LMDBSerializer.load on windows. ",like environment class part guess use,issue,negative,neutral,neutral,neutral,neutral,neutral
496607622,"That's interesting. Perhaps the dataflow you use refers to some other objects or functions in third-party libraries which involve a class named ""Environment"".",interesting perhaps use involve class environment,issue,negative,positive,positive,positive,positive,positive
496605680,"So do you have a suggestion for how to resolve the issue then? I don't have a class named Environment, and I'm not sure what's causing the issue. ",suggestion resolve issue class environment sure causing issue,issue,positive,positive,positive,positive,positive,positive
496594217,"As said in the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/unexpected-problems---bugs.md):

> If you expect certain accuracy, only in one of the two conditions can we help with it: (1) You're unable to reproduce the accuracy documented in tensorpack examples. (2) It appears to be a tensorpack bug.
Otherwise, how to train a model to certain accuracy is a machine learning question. We do not answer machine learning questions and it is your responsibility to figure out how to make your models more accurate.",said issue template expect certain accuracy one two help unable reproduce accuracy bug otherwise train model certain accuracy machine learning question answer machine learning responsibility figure make accurate,issue,positive,positive,neutral,neutral,positive,positive
496583171,"It's required by python's multiprocessing on windows, that your class needs to be pickled to be run in parallel.",python class need run parallel,issue,negative,neutral,neutral,neutral,neutral,neutral
496575763,"I don't understand what you mean, can you provide some more details?? And why is pickle even relevant here?? I'm just trying to load from an LMDB file. load_lmdb2() with BatchData() just worked fine. I start to have an issue when MultiProcessRunner() comes in. ",understand mean provide pickle even relevant trying load file worked fine start issue come,issue,negative,positive,positive,positive,positive,positive
496574688,"That's a different issue.

> _pickle.PicklingError: Can't pickle <class 'Environment'>: attribute lookup Environment on builtins failed

Your class cannot be serialized by Python. You need to make it serializable by Python's pickle.",different issue ca pickle class attribute environment class python need make python pickle,issue,negative,neutral,neutral,neutral,neutral,neutral
496573970,"Hi. I changed the code to run the function in if __name__ == '__main__': but the issue still exists. Here's the error message:

[0528 11:44:49 @parallel.py:195] WRN MultiProcessRunner does support Windows. However, Windows requires more strict picklability on processes, which may lead of failure on some of the code.
Traceback (most recent call last):
  File ""load_lmdb.py"", line 80, in <module>
    load_lmdb3()
  File ""load_lmdb.py"", line 69, in load_lmdb3
    ds = MultiProcessRunner(ds, 5000, 1)       # NOTE: PrefetchData() deprecated in May 2019
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\dataflow\parallel.py"", line 214, in __init__
    start_proc_mask_signal(self.procs)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\site-packages\tensorpack\utils\concurrency.py"", line 244, in start_proc_mask_signal
    p.start()
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\context.py"", line 212, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\context.py"", line 313, in _Popen
    return Popen(process_obj)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\popen_spawn_win32.py"", line 66, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\reduction.py"", line 59, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class 'Environment'>: attribute lookup Environment on builtins failed

(dps42_dev) C:\AI_Workspace\z_debug>Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\spawn.py"", line 106, in spawn_main
    exitcode = _main(fd)
  File ""C:\Users\dps42\AppData\Local\Continuum\miniconda3\envs\dps42_dev\lib\multiprocessing\spawn.py"", line 116, in _main
    self = pickle.load(from_parent)
EOFError: Ran out of input",hi code run function issue still error message support however strict may lead failure code recent call last file line module file line note may file line file line file line start self file line return file line return file line file line dump file protocol ca pickle class attribute environment recent call last file string line module file line file line self ran input,issue,negative,negative,neutral,neutral,negative,negative
496569533,"For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))
In particular, I don't know what is ""CONFIG_STR"" how is it related to tensorpack.",anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected particular know related,issue,positive,positive,positive,positive,positive,positive
496569190,"Your log has already told you the reason:
```
This probably means that you are not using fork to start your
    child processes and you have forgotten to use the proper idiom
    in the main module:

        if __name__ == '__main__':
            freeze_support()
            ...

    The ""freeze_support()"" line can be omitted if the program
    is not going to be frozen to produce an executable.
```

basically, a python main module needs to be in the __main__ block.",log already told reason probably fork start child forgotten use proper idiom main module line program going frozen produce executable basically python main module need block,issue,negative,positive,positive,positive,positive,positive
496554319,Hi. I will post a separate issue.,hi post separate issue,issue,negative,neutral,neutral,neutral,neutral,neutral
496460991,"I have fixed it. the problem was with the config file.
The name of `cfg.DATA.TRAIN` in config.py file was coco_train and my data name was train 
Now when i try to train it with the following parameters i get an OOM problem:
1. NVIDIA Quadro p2000.
2. Cuda 10.1
3. ImageNet-ResNet101.

i think i need more GPU.
Thanks for the help. ",fixed problem file name file data name train try train following get problem think need thanks help,issue,negative,positive,positive,positive,positive,positive
496387541,i set num_workers=5 and the memory used is stable (about 70G for 17 epoch right now),set memory used stable epoch right,issue,negative,positive,positive,positive,positive,positive
496369348,"I have total 250G for train, and 58G is free after about 25 epoch",total train free epoch,issue,positive,positive,positive,positive,positive,positive
496360124,"> Free RAM 58.28/250.86 GB

Despite the wrong start method which uses more memory, if you only have 58G free memory to run the training, you may need to reduce the buffer size or the number of workers at https://github.com/tensorpack/tensorpack/blob/4df295ca185afd58b787ed204e0199858ef889ec/examples/FasterRCNN/data.py#L388-L394",free ram despite wrong start method memory free memory run training may need reduce buffer size number,issue,positive,positive,positive,positive,positive,positive
496355832,"tensorpack does not set the start method, therefore it could be a dependency of tensorpack that sets the start method. You can try importing the dependencies of tensorpack to see which one triggers the error.",set start method therefore could dependency start method try see one error,issue,negative,neutral,neutral,neutral,neutral,neutral
496351701,"thanks, what i mean is I use the last week tensorpack version is ok, but the tommorrow version have the ""RuntimeError: context has already been set"", and can you give me some suggestion which software is new for the lastest version.",thanks mean use last week version version context already set give suggestion new version,issue,negative,positive,neutral,neutral,positive,positive
496351105,"I do not know what are you expecting by posting the list of packages. There is no way I could tell which package set the start method, and the packages pip shows is not necessarily the package you'll import anyway.",know posting list way could tell package set start method pip necessarily package import anyway,issue,negative,neutral,neutral,neutral,neutral,neutral
496349879,"absl-py             0.5.0   0.7.1    sdist
astor               0.7.1   0.8.0    wheel
bleach              2.1.4   3.1.0    wheel
decorator           4.3.0   4.4.0    wheel
defusedxml          0.5.0   0.6.0    wheel
entrypoints         0.2.3   0.3      wheel
gast                0.2.0   0.2.2    sdist
grpcio              1.15.0  1.21.1   wheel
h5py                2.8.0   2.9.0    wheel
ipykernel           4.9.0   5.1.1    wheel
ipython             6.5.0   7.5.0    wheel
jedi                0.12.1  0.13.3   wheel
Jinja2              2.10    2.10.1   wheel
jsonschema          2.6.0   3.0.1    wheel
jupyter-client      5.2.3   5.2.4    wheel
jupyter-console     5.2.0   6.0.0    wheel
Keras               2.2.2   2.2.4    wheel
Keras-Applications  1.0.6   1.0.7    wheel
Keras-Preprocessing 1.0.5   1.0.9    wheel
kiwisolver          1.0.1   1.1.0    wheel
Markdown            3.0     3.1.1    wheel
MarkupSafe          1.0     1.1.1    wheel
mistune             0.8.3   0.8.4    wheel
mock                2.0.0   3.0.5    wheel
msgpack-numpy       0.4.4.2 0.4.4.3  wheel
nbconvert           5.4.0   5.5.0    wheel
notebook            5.7.0   5.7.8    wheel
numpy               1.15.2  1.16.3   wheel
pandas              0.23.4  0.24.2   wheel
parso               0.3.1   0.4.0    wheel
pbr                 4.2.0   5.2.0    wheel
pexpect             4.6.0   4.7.0    wheel
Pillow              5.2.0   6.0.0    wheel
pip                 18.0    19.1.1   wheel
prometheus-client   0.3.1   0.6.0    sdist
prompt-toolkit      1.0.15  2.0.9    wheel
protobuf            3.6.1   3.7.1    wheel
pycurl              7.43.0  7.43.0.2 sdist
Pygments            2.2.0   2.4.1    wheel
pygobject           3.20.0  3.32.1   sdist
pyparsing           2.2.1   2.4.0    wheel
python-dateutil     2.7.3   2.8.0    wheel
pytz                2018.5  2019.1   wheel
PyYAML              3.13    5.1      sdist
pyzmq               17.1.2  18.0.1   wheel
qtconsole           4.4.1   4.5.1    wheel
scikit-learn        0.19.2  0.21.2   wheel
scipy               1.1.0   1.3.0    wheel
setuptools          39.1.0  41.0.1   wheel
six                 1.11.0  1.12.0   wheel
style               1.1.0   1.1.6    wheel
tensorboard         1.12.0  1.13.1   wheel
tensorflow          1.11.0  1.13.1   wheel
tensorflow-gpu      1.12.0  1.13.1   wheel
terminado           0.8.1   0.8.2    wheel
testpath            0.3.1   0.4.2    wheel
tornado             5.1.1   6.0.2    sdist
Werkzeug            0.14.1  0.15.4   wheel
wheel               0.31.1  0.33.4   wheel",astor wheel bleach wheel decorator wheel wheel wheel gast wheel wheel wheel wheel wheel jinja wheel wheel wheel wheel wheel wheel wheel wheel markdown wheel wheel wheel mock wheel wheel wheel notebook wheel wheel wheel wheel wheel wheel pillow wheel pip wheel wheel wheel wheel wheel wheel wheel wheel wheel wheel wheel wheel six wheel style wheel wheel wheel wheel wheel wheel tornado wheel wheel wheel,issue,negative,neutral,neutral,neutral,neutral,neutral
496349566,"thanks, I use the version last week is ok",thanks use version last week,issue,negative,positive,neutral,neutral,positive,positive
496349051,Then you should check which `import` makes `set_start_method` fail to figure out which library is doing bad things.,check import fail figure library bad,issue,negative,negative,negative,negative,negative,negative
496347845,"> I use the train.py with my own data. and comment the mp.set_start_method('spawn'), because with it, runtimeerror context has already been set.

You should not comment it out. The code itself says that it's for saving memory.
You should figure out why the context has already been set. One possibility is that you're using an old version of tqdm that doesn't meet tensorpack's requirement: https://github.com/tensorpack/tensorpack/blob/4df295ca185afd58b787ed204e0199858ef889ec/setup.py#L59

The issue template says:

> ### 2. What you observed:
> (1) **Include the ENTIRE logs here:**
It's always better to copy-paste what you observed instead of describing them.

Please copy-paste what you observed (logs or screenshots) instead of describing them.",use data comment context already set comment code saving memory figure context already set one possibility old version meet requirement issue template include entire always better instead please instead,issue,positive,positive,positive,positive,positive,positive
496347007,"ok, I submit new issue. thanks!",submit new issue thanks,issue,negative,positive,positive,positive,positive,positive
496336972,"```python
def register_coco(basedir):
    DatasetRegistry.register(""train"", lambda: COCODetection(basedir, ""train""))
    DatasetRegistry.register(""validation"", lambda: COCODetection(basedir, ""validation""))
```
write this in coco.py and use ""train"" and ""validation"" in `cfg.DATA.TRAIN/VAL`. 

If you met any issues, please post the issue following the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md)",python train lambda train validation lambda validation write use train validation met please post issue following issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
496318827,"And your code will register a dataset named ""train2019train2019"" because you add two strings.",code register add two,issue,negative,neutral,neutral,neutral,neutral,neutral
496316227,"Thanks again

`def register_coco(basedir):
    for split in [""train2019"", ""val2019""]:
        DatasetRegistry.register(""train2019"" + split, lambda x=split: COCODetection(basedir, x))`

this is what did i edit and i am still struggling even when i tested with small coco 2014 Dataset it did not work
",thanks split train train split lambda edit still struggling even tested small coco work,issue,negative,negative,neutral,neutral,negative,negative
496315727,"You should register it at the beginning of your code, or see datasets/coco.py",register beginning code see,issue,negative,neutral,neutral,neutral,neutral,neutral
496287032,"Thank you for your quick response
my train 2019 is ""dir"" so to make it clear:
1. /home/detection/train2019/:
      * annotations/:
              * instances_train.json
              * instaces_validation.json
      * /train/xxx.jpg
      * /validation/xxx.jpg

so what i did not understand is where do i need to call
`DatasetRegistry.register(name,lambda: YourDatasetSplit())`
in train.py?",thank quick response train make clear understand need call name lambda,issue,positive,positive,positive,positive,positive,positive
496280353,"As https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md#implementation-notes suggested, you need to call 
```
DatasetRegistry.register(name, lambda: YourDatasetSplit())
```
where name is ""train2019"" and YourDatasetSplit is `COCODetection(.....)`",need call name lambda name train,issue,negative,neutral,neutral,neutral,neutral,neutral
496111468,"I see, thanks for the quick reply! ",see thanks quick reply,issue,negative,positive,positive,positive,positive,positive
496068540,"Because you're using tensorflow, what you print is a symbolic tensor. You may learn how to evaluate a tensor at tensorflow's documentation: https://www.tensorflow.org/guide/low_level_intro. You can create a `tf.Session` and then use `session.run`.

It is recommended to learn about basic usage of tensorflow before using tensorpack. ",print symbolic tensor may learn evaluate tensor documentation create use learn basic usage,issue,negative,neutral,neutral,neutral,neutral,neutral
496017956,"You can just use `tf.image.summary`.

The alexnet example uses it:  https://github.com/tensorpack/tensorpack/blob/master/examples/ImageNetModels/alexnet.py

If you want to save tensor to files, you can use a callback, such as https://github.com/tensorpack/tensorpack/blob/eccba14e4461dc1d174de9db97896c8adf346780/tensorpack/callbacks/steps.py#L19-L43",use example want save tensor use,issue,positive,neutral,neutral,neutral,neutral,neutral
495921460,"Hi thank you for the support.

I was following the ""Efficient Dataflow"" tutorial on Friday morning, but didn't have an issue with LMDBSerializer.load(), LocallyShuffleData(), and BatchData() back then even on a large lmdb file with several Gigabytes. But I had an error with PrefetchData(), but I don't remember the details. 

Unfortunately, I don't have the access right now. Monday will be Memorial day holiday, so I'll go check it again on Tuesday. I noticed that there are many limitations for using Windows like being unable to use ZMQ etc. BTW I don't want to use Windows as a development environment too, but I'm forced to use  Windows machines and those are the only ones I'm given. 

Again, thanks for the great support.  ",hi thank support following efficient tutorial morning issue back even large file several error remember unfortunately access right memorial day holiday go check many like unable use want use development environment forced use given thanks great support,issue,positive,positive,neutral,neutral,positive,positive
495885689,"I don't have a windows to test, but could you check whether similar issue would arise when you use `LMDBSerialize.load`? There are some similar logic there which might also cause this issue.",test could check whether similar issue would arise use similar logic might also cause issue,issue,negative,neutral,neutral,neutral,neutral,neutral
495528465,"Hi @ppwwyyxx , thanks a lot for your kind comment! I'll check them. In fact I'm not sure how to load the shared data for training in Tensorpack.",hi thanks lot kind comment check fact sure load data training,issue,positive,positive,positive,positive,positive,positive
495515250,"> Cache a number of those tiny images in the disk with numpy format and then load these files one by one into memory

This is a reasonable approach but I don't see how it is related to tensorpack. 

Creating such files on disk is a pure numpy task.
Loading such files is a pure numpy function, which you can then connect to tensorpack dataflow (e.g., with `DataFromGenerator`.",cache number tiny disk format load one one memory reasonable approach see related disk pure task loading pure function connect,issue,negative,positive,positive,positive,positive,positive
495461854,I will try. Thank you for your patient and professional reply. @ppwwyyxx ,try thank patient professional reply,issue,negative,positive,neutral,neutral,positive,positive
495461543,"You can try reboot.
Also, despite that you said you did not install cudnn, tensorflow finds cudnn at `/usr/lib64/libcudnn.so.7.5.1`.

You also need to make sure this cudnn is compatible with cuda 10.",try also despite said install also need make sure compatible,issue,negative,positive,positive,positive,positive,positive
495461076,"@ppwwyyxx 

> This is an issue in your GPU environment. You can find more information online, e.g., https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html

`CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE = 804
This error indicates that the system was upgraded to run with forward compatibility but the visible hardware detected by CUDA does not support this configuration. Refer to the compatibility documentation for the supported hardware matrix or ensure that only supported hardware is visible during initialization via the CUDA_VISIBLE_DEVICES environment variable.`

My GPU is 1080ti and Cuda 10.0.
I can't find any solution to solve this problem.
Do you have any advice?",issue environment find information error system run forward compatibility visible hardware support configuration refer compatibility documentation hardware matrix ensure hardware visible via environment ti ca find solution solve problem advice,issue,negative,neutral,neutral,neutral,neutral,neutral
495456671,">  E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW

This is an issue in your GPU environment. You can find more information online, e.g., https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html

Closing as it's unrelated to tensorpack.",call forward compatibility non issue environment find information unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
495448569,"For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))
Include full logs and environment info.",anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected include full environment,issue,positive,positive,positive,positive,positive,positive
495447544,"@ppwwyyxx 
I try to train use `./alexnet-dorefa.py --dorefa 1,2,6 --data PATH --gpu 0,1
`
I get this error:
![image](https://user-images.githubusercontent.com/33385095/58297905-1778a280-7e0c-11e9-8d85-56cf28665a84.png)

I have installed tensorflow-gpu and Cuda-10.0 but **not** installed CuDNN. If I install CuDNN, this problem will be solved?
",try train use data path get error image install problem,issue,negative,neutral,neutral,neutral,neutral,neutral
495433451,"> Hi,
> 
> I was wondering if there's a way to use different weights for the forward and backward pass.
> 
> This question is originated from the DoReFa-Net implementation in Tensorpack, where ""remap_variables (func)"" is used to quantize weights of the operations (e.g., conv2D, fullyconnected) under the scope using the predefined quantization function, func (as shown in [this](https://github.com/tensorpack/tensorpack/blob/89ce90466bb4186ff8e90cec43b470336a205c26/examples/DoReFa-Net/resnet-dorefa.py#L90)). I checked via [Tensorflow debugger](https://github.com/tensorpack/tensorpack/issues/191) that in the forward and backward convolutions (i.e, Conv2D and Conv2DBackpropInput) the quantized weights are used as the weight for both forward and backward computation. Note that the original (i.e., non-quantized) weight can still be updated with the gradients via [apply_gradients](https://github.com/tensorpack/tensorpack/blob/e943200b1d86cc9254d05118872b40b20c685825/tensorpack/graph_builder/training.py#L174).
> 
> An interesting test-case is to use the quantized weight in the forward pass but use the full-precision weight in the backward pass as motivated by [this paper](https://arxiv.org/abs/1805.11233). This case would be particularly interesting when we try to observe an impact of quantization on the forward and backward passes separately.
> 
> As long as I understood, however, quant-weight-forward/fullprec-weight-backward cannot be implemented via ""remap_variables ()"", since it replaces the weights with the quantized weights for both forward and backward convolutions.
> 
> Is there any way in Tensorpack to assign different weights for the forward and backward passes of training? Any insights would be very helpful!
> 
> Best regards,
> Jungwook

Hi, Jungwook. I think this page could help you solve this problem. https://github.com/1202kbs/Understanding-NN/blob/master/models/models_2_4.py",hi wondering way use different forward backward pas question implementation used quantize scope quantization function shown checked via forward backward used weight forward backward computation note original weight still via interesting use weight forward pas use weight backward pas paper case would particularly interesting try observe impact quantization forward backward separately long understood however via since forward backward way assign different forward backward training would helpful best hi think page could help solve problem,issue,positive,positive,positive,positive,positive,positive
495418293,The above commit implements the logic in https://github.com/NVIDIA/DIGITS/pull/209. Let me know if the issue still exists!,commit logic let know issue still,issue,negative,neutral,neutral,neutral,neutral,neutral
495415455,"This is a windows-specific issue, and explained in https://github.com/NVIDIA/DIGITS/issues/206 as well, with a workaround in https://github.com/NVIDIA/DIGITS/pull/209. The solution there is to start with a small `map_size` on windows, but increase gradually when the dataset is full.",issue well solution start small increase gradually full,issue,positive,positive,neutral,neutral,positive,positive
495408113,"The keras models are here: https://github.com/see--/keras-centernet. I now want to reproduce some training results. I thought about using 100% tensorpack, but I already defined the models. It would be a lot duplicate code. I can't use `tf.data` neither as it doesn't have all the `cv2` ops that are required.

> My guess is that the mnist-keras-v2 style may be less prone to such error.

I just started using tensorpack. It is easier to just follow the examples, and they mostly use the config + trainer style. This style is also better tested & documented. I actually thought that these were most of the rough edges :) Please let me know if you are aware of more.",want reproduce training thought already defined would lot duplicate code ca use neither guess style may le prone error easier follow mostly use trainer style style also better tested actually thought rough please let know aware,issue,positive,positive,positive,positive,positive,positive
495301891,"Would you mind sharing why do you want to do these on the `mnist-keras.py` example? The same thing can be done much easier if using tensorpack layers or `tf.layers`, and without any of these catches and hacks (https://github.com/tensorpack/tensorpack/tree/master/examples/basics).

If you have a very large model which might be hard to migrate to tensorpack layers or `tf.layers`, chances are it may not work with the style of `mnist-keras.py` since you saw there are a lot of issues in the compatibility between the two libraries. My guess is that the `mnist-keras-v2` style may be less prone to such error.",would mind want example thing done much easier without large model might hard migrate may work style since saw lot compatibility two guess style may le prone error,issue,negative,positive,neutral,neutral,positive,positive
495299372,You can feed `{keras.backend.learning_phase():False}` in your custom callback when you evaluate a tensor.,feed false custom evaluate tensor,issue,negative,negative,negative,negative,negative,negative
495298009,"Thanks!

> What is this for?

I need a custom callback that is not an instance of `InferenceRunner` (like in FasterRCNN example). I think the `KerasPhaseCallback(True)` won't set the model in the inference mode. Using the learning phase as input will.",thanks need custom instance like example think true wo set model inference mode learning phase input,issue,positive,positive,positive,positive,positive,positive
495295717,Sorry I was using the wrong version of code. Now I can reproduce the error.,sorry wrong version code reproduce error,issue,negative,negative,negative,negative,negative,negative
495291581,"I do not see any error running `mnist-keras.py` with TF 1.13.1 on CPU. Could you post details following [the issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))?

> What do you think about these changes?

What is this for?",see error running could post following issue template think,issue,negative,neutral,neutral,neutral,neutral,neutral
495290280,"What do you think about these changes?
```
diff --git a/examples/keras/mnist-keras.py b/examples/keras/mnist-keras.py
index 2dcdd41..95a1af2 100755
--- a/examples/keras/mnist-keras.py
+++ b/examples/keras/mnist-keras.py
@@ -59,14 +59,14 @@ def get_keras_model():
 class Model(ModelDesc):
     def inputs(self):
         return [tf.TensorSpec((None, IMAGE_SIZE, IMAGE_SIZE), tf.float32, 'input'),
+                tf.TensorSpec((), tf.bool, 'learning_phase'),
                 tf.TensorSpec((None,), tf.int32, 'label')]
 
-    def build_graph(self, image, label):
+    def build_graph(self, image, learning_phase, label):
         image = tf.expand_dims(image, 3) * 2 - 1
         ctx = get_current_tower_context()
-
         M = get_keras_model()
-        logits = M(image)
+        logits = M(image, training=learning_phase)
         if ctx.is_main_training_tower:
             for op in M.updates:
                 tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, op)
@@ -97,7 +97,9 @@ class Model(ModelDesc):
 
 def get_data():
     train = BatchData(dataset.Mnist('train'), 128)
+    train = MapData(train, lambda ds: [ds[0], 1, ds[1]])
     test = BatchData(dataset.Mnist('test'), 256, remainder=True)
+    test = MapData(test, lambda ds: [ds[0], 0, ds[1]])
     return train, test
 
 
@@ -109,7 +111,6 @@ if __name__ == '__main__':
         model=Model(),
         dataflow=dataset_train,
         callbacks=[
-            KerasPhaseCallback(True),   # for Keras training
             ModelSaver(),
             InferenceRunner(
                 dataset_test,
```
One could use arbitrary callbacks, `InferenceRunner` is not required. ",think git index class model self return none none self image label self image label image image image image class model train train train lambda test test test lambda return train test true training one could use arbitrary,issue,negative,positive,positive,positive,positive,positive
495194260,"Oh... It started working.
The problem was in dataset. category/image id should start from 1 instead of 0.

Thanks.",oh working problem id start instead thanks,issue,negative,positive,positive,positive,positive,positive
495147545,It's not supported and that's why you'll need to first implement the graph for batch inference.,need first implement graph batch inference,issue,negative,positive,positive,positive,positive,positive
495144063,"InvalidArgumentError (see above for traceback): Can not squeeze dim[0], expected a dimension of 1, got 2
	 [[node rpn_2/Squeeze (defined at oneframe_inference_pb.py:31)  = Squeeze[T=DT_FLOAT, squeeze_dims=[0], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](rpn_2/transpose)]]
	 [[{{node generate_fpn_proposals/Lvl6/generate_rpn_proposals/nms_input_boxes/_105}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1138_generate_fpn_proposals/Lvl6/generate_rpn_proposals/nms_input_boxes"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

It seems generate_fpn_proposals don't support.",see squeeze dim dimension got node defined squeeze node support,issue,negative,positive,neutral,neutral,positive,positive
495111452,"You'll need to first implement the graph for batch inference.
Tensorpack is an interface for model training, and in general it does not provide extra support for fast inference. (https://tensorpack.readthedocs.io/tutorial/inference.html#inference-after-training-what-tensorpack-does)",need first implement graph batch inference interface model training general provide extra support fast inference,issue,negative,positive,positive,positive,positive,positive
495110817,"can you give some suggestions for batch inference? and make me freeze the model for batch inference in tf?

now, I can train the model use tensorpack and freeze the model for inference in tf one image by one image.",give batch inference make freeze model batch inference train model use freeze model inference one image one image,issue,negative,neutral,neutral,neutral,neutral,neutral
495091034,"I mean the detection, faster rcnn or mask rcnn, not classification",mean detection faster mask classification,issue,negative,negative,negative,negative,negative,negative
495038977,"@ppwwyyxx , yes, you are right. I have a misunderstanding about the training error. It is reported by a batch. Thanks a lot for your help! ",yes right misunderstanding training error batch thanks lot help,issue,negative,positive,positive,positive,positive,positive
495033187,"It seems to be a reasonable speed.
Tensorpack is a library for model training and does not provide optimization of inference speed.",reasonable speed library model training provide optimization inference speed,issue,negative,positive,positive,positive,positive,positive
495031482,"Hi, I'm using ResNet50 backbone and FasterRCNN. The inference time for one image (resized to 800x1333) in one GTX 1080Ti is about 200ms. And I'm using the NMS within Tensorflow. Is this reasonable? But I remembered that others can achieve to 70ms. Could anybody give some advice on how to speed up the inference? Thanks.",hi backbone inference time one image one ti within reasonable achieve could anybody give advice speed inference thanks,issue,negative,positive,positive,positive,positive,positive
494982591,https://github.com/tensorpack/tensorpack/commit/0bbec1e466b78794dbd22cbd70770cc55779e9fd packs the masks into bit array and should reduce the size of data by 8x. I'll run some more tests before merging it.,bit array reduce size data run,issue,negative,neutral,neutral,neutral,neutral,neutral
494872212,"OMG.
Then this is a real issue. For now you can unblock yourself by changing the size limit at:
https://github.com/tensorpack/tensorpack/blob/42416945c1e36a5f1d4350ee9c1ae8b134cbe841/tensorpack/utils/serialize.py#L19

I'll see what's a better way to fix this.",real issue unblock size limit see better way fix,issue,negative,positive,positive,positive,positive,positive
494871688,"The image I mentioned is this one
![12_Group_Large_Group_12_Group_Large_Group_12_31](https://user-images.githubusercontent.com/33063754/58190745-74b20e00-7cbd-11e9-9f0d-0c056f4c961d.jpg)

I suppose it could have over 1000 annotations, since it is face detection
",image one suppose could since face detection,issue,negative,neutral,neutral,neutral,neutral,neutral
494870089,"There is a few images with a lot of masks, for example an image with `size: 1151759322 mask shape (1139, 800, 1241)`. In this case, should I remove some of the masks, or how should I approach it?

Thank you!",lot example image size mask shape case remove approach thank,issue,negative,neutral,neutral,neutral,neutral,neutral
494865486,"This still looks like a reasonable size (158M). I was hoping you could find anything larger than 1G, e.g. 1675200000. 
Theoretically if there are data > 1G in when you use >1 workers, there should be something > 1G when you use 0 workers as well. Otherwise this is very strange. If you can't find anything>1G could you try upgrade msgpack and try again?",still like reasonable size could find anything theoretically data use something use well otherwise strange ca find anything could try upgrade try,issue,negative,positive,neutral,neutral,positive,positive
494864406,"> The training error and validation error don't change.

That is not a correct expectation. Training error depends on what images are sampled for training and what random augmentations they went through. So it will change although the weights are freezed.",training error validation error change correct expectation training error training random went change although,issue,negative,negative,negative,negative,negative,negative
494818612,"How large do you mean by extremely large data size? 

I ran the script again, and there is an image with the original size of 205,135 bytes, but after preprocessing, it prints `size: 158703939 mask shape (170, 800, 1028)`. Is that extremely large?",large mean extremely large data size ran script image original size size mask shape extremely large,issue,negative,positive,positive,positive,positive,positive
494776153,"@ppwwyyxx , 

**1. What I did:**

- I try to freeze the weights and batch norm parameter by using **tf.stop_gradients on logits and remove RunUpdateOps in callback operation**. 

- I try to add another fully connected layer + loss layer after **logits** layer of AlexNet. 
Here is the code
'
stoped_logits = tf.stop_gradient(logits)
output = FullyConnected('confidence', stoped_logits, 1)
add_param_summary(('.*/W', ['histogram', 'rms']))
outputt = (tf.sign(output) + 1) / 2
conf_correct = prediction_conf_correct(correct, outputt, name='conf-correct-top1')
add_moving_summary(tf.reduce_mean(conf_correct, name='conf-train-correct-top1'))
'

**2. What I observed**
The training error changes per epoch, but validation error doesn't change.

Here is the log
[0522 19:01:42 @monitor.py:459] QueueInput/queue_size: 1.6936e-38
[0522 19:01:42 @monitor.py:459] conf-train-correct-top1: 0.5078
[0522 19:01:42 @monitor.py:459] l2_regularize_loss: 2.263e-37
[0522 19:01:42 @monitor.py:459] learning_rate: 0.01
[0522 19:01:42 @monitor.py:459] param-summary/confidence/W-rms: 0
[0522 19:01:42 @monitor.py:459] param-summary/conv0/W-rms: 0.10412
[0522 19:01:42 @monitor.py:459] param-summary/conv0/W-rms_1: 0.10412
[0522 19:01:42 @monitor.py:459] param-summary/conv1/W-rms: 0.4558
[0522 19:01:42 @monitor.py:459] param-summary/conv1/W-rms_1: 0.4558
[0522 19:01:42 @monitor.py:459] param-summary/conv2/W-rms: 0.31383
[0522 19:01:42 @monitor.py:459] param-summary/conv2/W-rms_1: 0.31383
[0522 19:01:42 @monitor.py:459] param-summary/conv3/W-rms: 0.2774
[0522 19:01:42 @monitor.py:459] param-summary/conv3/W-rms_1: 0.2774
[0522 19:01:42 @monitor.py:459] param-summary/conv4/W-rms: 0.3697
[0522 19:01:42 @monitor.py:459] param-summary/conv4/W-rms_1: 0.3697
[0522 19:01:42 @monitor.py:459] param-summary/fc0/W-rms: 0.042924
[0522 19:01:42 @monitor.py:459] param-summary/fc0/W-rms_1: 0.042924
[0522 19:01:42 @monitor.py:459] param-summary/fc1/W-rms: 0.069286
[0522 19:01:42 @monitor.py:459] param-summary/fc1/W-rms_1: 0.069286
[0522 19:01:42 @monitor.py:459] param-summary/fct/W-rms: 0.12619
[0522 19:01:42 @monitor.py:459] param-summary/fct/W-rms_1: 0.12619
[0522 19:01:42 @monitor.py:459] train-error-top1: 0.5078
[0522 19:01:42 @monitor.py:459] train-error-top5: 0.27603
[0522 19:01:42 @monitor.py:459] val-conf-correct-top1: 0.51536
[0522 19:01:42 @monitor.py:459] val-error-top1: 0.51536
[0522 19:01:42 @monitor.py:459] val-error-top5: 0.27698
[0522 19:01:42 @monitor.py:459] xentropy-loss: 3.0478
[0522 19:01:42 @group.py:48] Callbacks took 56.032 sec in total. InferenceRunner: 55.7 seconds
[0522 19:01:42 @base.py:250] Start Epoch 3 ...
100%|##########|5000/5000[13:55<00:00, 5.98it/s]
[0522 19:15:38 @base.py:260] Epoch 3 (global_step 15000) finished, time:13 minutes 55 seconds.
[0522 19:15:38 @saver.py:77] Model saved to train_log/alexnet-dorefa-conf-1,1,32/model-15000.
100%|##########|782/782[00:55<00:00,14.09it/s]
[0522 19:16:33 @monitor.py:459] QueueInput/queue_size: 1.4608e-38
[0522 19:16:33 @monitor.py:459] conf-train-correct-top1: 0.49676
[0522 19:16:33 @monitor.py:459] l2_regularize_loss: 2.263e-37
[0522 19:16:33 @monitor.py:459] learning_rate: 0.01
[0522 19:16:33 @monitor.py:459] param-summary/confidence/W-rms: 0
[0522 19:16:33 @monitor.py:459] param-summary/conv0/W-rms: 0.10412
[0522 19:16:33 @monitor.py:459] param-summary/conv0/W-rms_1: 0.10412
[0522 19:16:33 @monitor.py:459] param-summary/conv1/W-rms: 0.4558
[0522 19:16:33 @monitor.py:459] param-summary/conv1/W-rms_1: 0.4558
[0522 19:16:33 @monitor.py:459] param-summary/conv2/W-rms: 0.31383
[0522 19:16:33 @monitor.py:459] param-summary/conv2/W-rms_1: 0.31383
[0522 19:16:33 @monitor.py:459] param-summary/conv3/W-rms: 0.2774
[0522 19:16:33 @monitor.py:459] param-summary/conv3/W-rms_1: 0.2774
[0522 19:16:33 @monitor.py:459] param-summary/conv4/W-rms: 0.3697
[0522 19:16:33 @monitor.py:459] param-summary/conv4/W-rms_1: 0.3697
[0522 19:16:33 @monitor.py:459] param-summary/fc0/W-rms: 0.042924
[0522 19:16:33 @monitor.py:459] param-summary/fc0/W-rms_1: 0.042924
[0522 19:16:33 @monitor.py:459] param-summary/fc1/W-rms: 0.069286
[0522 19:16:33 @monitor.py:459] param-summary/fc1/W-rms_1: 0.069286
[0522 19:16:33 @monitor.py:459] param-summary/fct/W-rms: 0.12619
[0522 19:16:33 @monitor.py:459] param-summary/fct/W-rms_1: 0.12619
[0522 19:16:33 @monitor.py:459] train-error-top1: 0.51508
[0522 19:16:33 @monitor.py:459] train-error-top5: 0.29268
[0522 19:16:33 @monitor.py:459] val-conf-correct-top1: 0.5228
[0522 19:16:33 @monitor.py:459] val-error-top1: 0.51536
[0522 19:16:33 @monitor.py:459] val-error-top5: 0.27698
[0522 19:16:33 @monitor.py:459] xentropy-loss: 3.1443
[0522 19:16:33 @group.py:48] Callbacks took 55.863 sec in total. InferenceRunner: 55.5 seconds
[0522 19:16:33 @base.py:250] Start Epoch 4 ...
100%|##########|5000/5000[13:56<00:00, 5.98it/s]
[0522 19:30:30 @base.py:260] Epoch 4 (global_step 20000) finished, time:13 minutes 56 seconds.
[0522 19:30:30 @saver.py:77] Model saved to train_log/alexnet-dorefa-conf-1,1,32/model-20000.
  0%|          |0/782[00:00<?,?it/s][0522 19:31:27 @monitor.py:459] QueueInput/queue_size: 1.7146e-38
[0522 19:31:27 @monitor.py:459] conf-train-correct-top1: 0.51858
[0522 19:31:27 @monitor.py:459] l2_regularize_loss: 2.263e-37
[0522 19:31:27 @monitor.py:459] learning_rate: 0.01
[0522 19:31:27 @monitor.py:459] param-summary/confidence/W-rms: 0
[0522 19:31:27 @monitor.py:459] param-summary/conv0/W-rms: 0.10412
[0522 19:31:27 @monitor.py:459] param-summary/conv0/W-rms_1: 0.10412
[0522 19:31:27 @monitor.py:459] param-summary/conv1/W-rms: 0.4558
[0522 19:31:27 @monitor.py:459] param-summary/conv1/W-rms_1: 0.4558
[0522 19:31:27 @monitor.py:459] param-summary/conv2/W-rms: 0.31383
[0522 19:31:27 @monitor.py:459] param-summary/conv2/W-rms_1: 0.31383
[0522 19:31:27 @monitor.py:459] param-summary/conv3/W-rms: 0.2774
[0522 19:31:27 @monitor.py:459] param-summary/conv3/W-rms_1: 0.2774
[0522 19:31:27 @monitor.py:459] param-summary/conv4/W-rms: 0.3697
[0522 19:31:27 @monitor.py:459] param-summary/conv4/W-rms_1: 0.3697
[0522 19:31:27 @monitor.py:459] param-summary/fc0/W-rms: 0.042924
[0522 19:31:27 @monitor.py:459] param-summary/fc0/W-rms_1: 0.042924
[0522 19:31:27 @monitor.py:459] param-summary/fc1/W-rms: 0.069286
[0522 19:31:27 @monitor.py:459] param-summary/fc1/W-rms_1: 0.069286
[0522 19:31:27 @monitor.py:459] param-summary/fct/W-rms: 0.12619
[0522 19:31:27 @monitor.py:459] param-summary/fct/W-rms_1: 0.12619
[0522 19:31:27 @monitor.py:459] train-error-top1: 0.51022
[0522 19:31:27 @monitor.py:459] train-error-top5: 0.27873
[0522 19:31:27 @monitor.py:459] val-conf-correct-top1: 0.5228
[0522 19:31:27 @monitor.py:459] val-error-top1: 0.51536
[0522 19:31:27 @monitor.py:459] val-error-top5: 0.27698
[0522 19:31:27 @monitor.py:459] xentropy-loss: 3.1212
[0522 19:31:27 @group.py:48] Callbacks took 56.623 sec in total. InferenceRunner: 56.3 seconds

**3. What I expected:**
The training error and validation error don't change. 

The additional fully connected layer's parameter being updated and loss layer I added changed. But in my log, it is not.

Please help, thanks! @ppwwyyxx 
",try freeze batch norm parameter remove operation try add another fully connected layer loss layer layer code output output correct training error per epoch validation error change log took sec total start epoch epoch finished time model saved took sec total start epoch epoch finished time model saved took sec total training error validation error change additional fully connected layer parameter loss layer added log please help thanks,issue,negative,negative,neutral,neutral,negative,negative
494709208,I could not understand what you did. Please post according to the issue template.,could understand please post according issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
494700611,"@ppwwyyxx The gradients look normal now! Thanks for your suggestions! However, after using **tf.stop_gradients on logits** and remove **RunUpdateOps**, the training error still changes after several epochs. In my codes, I only add another fully connected layer after logits, followed by a loss layer. It should not change the training/validation error on ImageNet. Can you provide some suggestions? I thought it would be some tensorpack trick i didn't know. Thanks in advance!",look normal thanks however remove training error still several add another fully connected layer loss layer change error provide thought would trick know thanks advance,issue,negative,positive,positive,positive,positive,positive
494655129,"It seems that you did not modify anything about weight decay, therefore the FC layer's weights are still trained by weight decay. You can change it at :https://github.com/tensorpack/tensorpack/blob/e97ce38dad5af583763f836ccc87ba14ea52f813/examples/DoReFa-Net/alexnet-dorefa.py#L63-L64",modify anything weight decay therefore layer still trained weight decay change,issue,negative,neutral,neutral,neutral,neutral,neutral
494654597,"If you have done something, please include them in:

### 1. What you did:

(1) **If you're using examples, what's the command you run:**

(2) **If you're using examples, have you made any changes to the examples? Paste `git status; git diff` here:**

(3) **If not using examples, tell us what you did:**

",done something please include command run made paste git status git tell u,issue,negative,neutral,neutral,neutral,neutral,neutral
494654302,"@ppwwyyxx ， yes, I have tried to use the tf.stop_gradient(), but it only freeze the first several conv layers' weights. The fully connected layers' weights are not freezed. Can you explain why? ",yes tried use freeze first several fully connected explain,issue,negative,positive,positive,positive,positive,positive
494629905,"Hi @ppwwyyxx , I have a question. Using `htop` command, I found many subprocesses attached one main process. Is this reasonable? ",hi question command found many attached one main process reasonable,issue,negative,positive,positive,positive,positive,positive
494625847,"@Oh ，thanks for your replay！
I’d love to know what you think  100000  images ， 300 classes, will enough for shufflenetV2?
Or do I need more training sets?",oh love know think class enough need training,issue,positive,positive,positive,positive,positive,positive
494612517,"@ppwwyyxx sorry, it seems I didn't install the package. Thanks a lot!",sorry install package thanks lot,issue,negative,negative,negative,negative,negative,negative
494611936,"My impression was that as long as you install `python-prctl`, there shouldn't be background processes left running after the training is killed.

If that doesn't work, what about changing this signal:
https://github.com/tensorpack/tensorpack/blob/e97ce38dad5af583763f836ccc87ba14ea52f813/tensorpack/utils/concurrency.py#L196
to `SIGKILL` instead?

Also, do you have a way to easily reproduce this issue?",impression long install background left running training work signal instead also way easily reproduce issue,issue,positive,positive,positive,positive,positive,positive
494548888,"I saw a similar incident recently that in one old environment, all training jobs get stuck at a certain epoch. But it was resolved after I upgrade cuda from 9.0 to 9.2 (and install the corresponding tensorflow). This may be related.",saw similar incident recently one old environment training get stuck certain epoch resolved upgrade install corresponding may related,issue,negative,positive,neutral,neutral,positive,positive
494505583,"It is possible to fix but it won't be fixed by them, because supporting variable scope is not useful in Keras.",possible fix wo fixed supporting variable scope useful,issue,positive,positive,positive,positive,positive,positive
494505127,"When you run it with NUM_WORKERS>1, it may not print the correct information which leads to the crash. 
Could you use NUM_WORKERS=0 instead, and see if there is any data that has extremely large data size (by `len(dumps(x))`?",run may print correct information crash could use instead see data extremely large data size,issue,negative,positive,positive,positive,positive,positive
494484508,"So I ran the dataloader alone, and the script crashed at this image 
![16_Award_Ceremony_Awards_Ceremony_16_411](https://user-images.githubusercontent.com/33063754/58117170-43bed400-7bfe-11e9-9b11-ff38dd8add0b.jpg)


In the terminal, the size and mask shape of the image is `size: 36678475 mask shape (17, 800, 1192)`, but it still prints out the error `ValueError: 1675200000 exceeds max_bin_len(1000000000)`

The data size and the mask shape seems reasonable. Do you have any insight into this? 
",ran alone script image terminal size mask shape image size mask shape still error data size mask shape reasonable insight,issue,negative,positive,positive,positive,positive,positive
494476473,"Thanks a lot! BTW I think it would be great to have full support for option 1:
> Write the tower function similar to a standard tensorpack program, but use some Keras layers in between. See mnist-keras.py on how to do this. It does not support all tensorpack trainers.

I am not really familiar with the keras variable scope issue. Is it something that can be fixed in the TensorFlow repo?",thanks lot think would great full support option write tower function similar standard program use see support really familiar variable scope issue something fixed,issue,positive,positive,positive,positive,positive,positive
494470806,This should be fixed in latest master now. Please note again that Keras support is experimental and I don't think there is a point for tensorpack to fully support Keras models except to demo a proof-of-concept of such possibility.,fixed latest master please note support experimental think point fully support except possibility,issue,positive,positive,positive,positive,positive,positive
494467921,"Keras updates batch norm by `model.updates`.
Therefore you need to add ops in `M.updates` into the collection `tf.GraphKeys.UPDATE_OPS` so that they can be known by tensorpack, like this:
https://github.com/tensorpack/tensorpack/blob/2981c5d4336ff36c158aa0fb5294975c80311c9c/tensorpack/contrib/keras.py#L75-L76",batch norm therefore need add collection known like,issue,negative,neutral,neutral,neutral,neutral,neutral
494466706,setting `DATA.NUM_WORKERS=0` will skip the codepath that throws the error and does not fix your issue. It will also make your training slower.,setting skip error fix issue also make training,issue,negative,neutral,neutral,neutral,neutral,neutral
494361501,"Sorry for the late reply.

Once I set `DATA.NUM_WORKERS=0`, the training works fine and there is no more error.

Thank you so much!",sorry late reply set training work fine error thank much,issue,negative,negative,neutral,neutral,negative,negative
494275188,"@ppwwyyxx 

> You have to implement the data loader for a different dataset, and change the number of classes in the last FC layer.

I will try it. Thank you very much",implement data loader different change number class last layer try thank much,issue,negative,positive,neutral,neutral,positive,positive
494274314,"> What do I have to modify

You have to implement the data loader for a different dataset, and change the number of classes in the last FC layer.

> Another question is whether I can use DoreFa to train AlexNet with (W, A) = ( 1, 8 )?

You can.",modify implement data loader different change number class last layer another question whether use train,issue,negative,neutral,neutral,neutral,neutral,neutral
494273108,"@ppwwyyxx  Thank you!
What do I have to modify to train AlexNet on **Tiny-ImageNet** instead of ImageNet?
`Tiny Imagenet has only 200 classes. Each class has 500 training images, 50 validation images, and 50 test images.`
Another question is whether I can use DoreFa to train AlexNet with  (W, A) = ( 1, 8 )?",thank modify train instead tiny class class training validation test another question whether use train,issue,negative,neutral,neutral,neutral,neutral,neutral
494224467,"That's a machine learning question, so we cannot answer. I'd assume that if you have <100 classes 100000 images would be enough.",machine learning question answer assume class would enough,issue,negative,neutral,neutral,neutral,neutral,neutral
494222971,"@ppwwyyxx Thank you very much, I've worked it out.
Now I only have almost 10,0000 pics for train, it is less for shufflenetV2?
And how many training sets are usually required for shufflenetV2?",thank much worked almost train le many training usually,issue,negative,positive,positive,positive,positive,positive
494072770,"The code will download some label files to `tensorpack_data` (full path shown in your picture) and it seems the download is broken. Please remove any files in the above directory, check your internet connection and try again.",code label full path shown picture broken please remove directory check connection try,issue,negative,negative,neutral,neutral,negative,negative
494072011,I did not know what you did so I could not reproduce your issue. Please make sure you are calling https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.MultiThreadMapData with correct arguments following its documentation.,know could reproduce issue please make sure calling correct following documentation,issue,positive,positive,positive,positive,positive,positive
494035685,"@ppwwyyxx 
When I eval on my own data, the following error appears, I don't know why
![image](https://user-images.githubusercontent.com/19808900/58033282-ca1fdb00-7b56-11e9-9aa0-3e2d6213c295.png)
",data following error know image,issue,negative,neutral,neutral,neutral,neutral,neutral
493944515,"@ppwwyyxx 
**When I try to run AlenNet pretrained model like this:**

![image](https://user-images.githubusercontent.com/33385095/58017839-cc247280-7b33-11e9-813d-ef04ffab3d18.png)

**I get this problem as follows:**
![image](https://user-images.githubusercontent.com/33385095/58017890-f0804f00-7b33-11e9-8408-6ad32c38dfee.png)

**What is wrong? Thank you**",try run model like image get problem image wrong thank,issue,negative,negative,negative,negative,negative,negative
493903772,The example at https://tensorpack.readthedocs.io/tutorial/inference.html#offlinepredictor shows how to test on any input array.,example test input array,issue,negative,neutral,neutral,neutral,neutral,neutral
493903522,"@ppwwyyxx Thanks you very much!
I still have a question,how can i test only one pic?",thanks much still question test one pic,issue,negative,positive,positive,positive,positive,positive
493895519,"0.2501563 and -0.2501563 are the binarized weights and this is what ""binarized"" means in all the binarized networks. You can use 1 and -1 as long as you compensate for the scale at a different place.",use long compensate scale different place,issue,negative,negative,neutral,neutral,negative,negative
493894147,"> No. It means 0.2501563 and -0.2501563. Although in a binarized implementation you'll want to use 1 and -1 to represent them.

So how can I get binarized weights like 1 and -1?  Or can I directly use 1 and -1 to represent 0.2501563 and -0.2501563?",although implementation want use represent get like directly use represent,issue,negative,positive,neutral,neutral,positive,positive
493891758,No. It means 0.2501563 and -0.2501563. Although in a binarized implementation you'll want to use 1 and -1 to represent them.,although implementation want use represent,issue,negative,neutral,neutral,neutral,neutral,neutral
493891181,"Thank you very much!
But I get another problem. I run fw to get the actual weights ( bitW = 1 )
I get -0.2501563 and 0.2501563 as follow:
![image](https://user-images.githubusercontent.com/33385095/58007556-1e0dce00-7b1d-11e9-8532-ece3c3c36fba.png)
I want to know whether 0.2501563  means 1 and -0.2501563  means -1?? @ppwwyyxx ",thank much get another problem run get actual get follow image want know whether,issue,negative,positive,neutral,neutral,positive,positive
493885066,"Use `tf.summary.xxx` on your tensor in the graph. Everything else is taken cared of and they will be summarized at the end of each epoch. 
See more options at https://tensorpack.readthedocs.io/tutorial/summary.html",use tensor graph everything else taken end epoch see,issue,negative,neutral,neutral,neutral,neutral,neutral
493877347,"See https://tensorpack.readthedocs.io/tutorial/inference.html#offlinepredictor

For shufflenet example, you can change the output names at https://github.com/tensorpack/tensorpack/blob/8879637399b665bfd11aa5ded663619a1a19bdb9/examples/ImageNetModels/imagenet_utils.py#L272-L284

You can `print(logits)` to see that its name is `linear/output`.",see example change output print see name,issue,negative,neutral,neutral,neutral,neutral,neutral
493835641,"If I understand it correctly, although time per epoch is already printed in the log, but what you need in this feature request is to multiply it by batch size and stored in a structured way (e.g. tensorboard)?",understand correctly although time per epoch already printed log need feature request multiply batch size structured way,issue,negative,neutral,neutral,neutral,neutral,neutral
493822036,@ppwwyyxx Where is the build grahp function?I can't find it,build function ca find,issue,negative,neutral,neutral,neutral,neutral,neutral
493727548,The `name` argument was there to mimic the API of other activation functions like `tf.nn.relu`. But I do feel that this argument is not useful and probably should better be removed.,name argument mimic activation like feel argument useful probably better removed,issue,positive,positive,positive,positive,positive,positive
493723334,"Thank you for the answer.
A bit misleading way to fail nevertheless",thank answer bit misleading way fail nevertheless,issue,negative,negative,negative,negative,negative,negative
493722450,Another tensorpack example that uses fp16 mixed precision training (though on fake data for benchmarking purposes): https://github.com/tensorpack/benchmarks/tree/master/ResNet-MultiGPU#performance--may-2019,another example mixed precision training though fake data,issue,negative,negative,negative,negative,negative,negative
493722352,"You need to either give it a different name if you don't want to share variable, or use `tf.variable_scope(tf.get_variable_scope(), reuse=True)` if you do want to share variables.

`name=` is not the variable scope name. It is the name of the output tensor, per tensorflow convention. 
See https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BNReLU.",need either give different name want share variable use want share variable scope name name output tensor per convention see,issue,positive,neutral,neutral,neutral,neutral,neutral
493695087,"The script does compute the mAP on `cfg.DATA.VAL` but not the loss.

As far as I know there is no definition on what is ""test loss"" for R-CNN. To some people this may mean exactly the ""training loss"" computed on validation dataset. But it may mean something else to others.
We can discuss at tensorpack gitter (https://gitter.im/tensorpack/users) if you like.",script compute map loss far know definition test loss people may mean exactly training loss validation may mean something else discus like,issue,negative,negative,neutral,neutral,negative,negative
493694308,"> There is no definition of a ""test loss"" in R-CNN unless you define it somehow first. You can compute a training loss on test dataset (if you have preprocessed its annotations in the same way as training).

This is very interesting. I'm indeed annotating the test dataset the same way I'm annotating train and val. But I'm a bit surprised you cannot compute a loss on the test set - I understood [a few papers reported the mAP](https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359) on COCO [Testing images](http://images.cocodataset.org/zips/test2014.zip). I thought I could compute mAP on a test dataset with

```
./predict.py --evaluate output.json --load /path/to/Trained-Model-Checkpoint \
    --config SAME-AS-TRAINING
```
but maybe this evaluates on the training set?

 Either I misunderstand something, or it's possible to compute the mAP but not the loss. This is curious. Let me know if you'd be available to discuss the topic more on a channel of your choice (Slack, email, Gitter, etc.) and I'll send you my contacts, otherwise don't worry and thanks a lot for your support.",definition test loss unless define somehow first compute training loss test way training interesting indeed test way train bit compute loss test set understood map coco testing thought could compute map test evaluate load maybe training set either misunderstand something possible compute map loss curious let know available discus topic channel choice slack send otherwise worry thanks lot support,issue,negative,positive,positive,positive,positive,positive
493682913,"> Since I want to compute the test loss on a test dataset with multiple images

There is no definition of a ""test loss"" in R-CNN unless you define it somehow first. You can compute a training loss on test dataset (if you have preprocessed its annotations in the same way as training).

If you just want to compute predictions on test images, it's in the README https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN#inference and support multiple images. You can change the names at https://github.com/tensorpack/tensorpack/blob/8879637399b665bfd11aa5ded663619a1a19bdb9/examples/FasterRCNN/predict.py#L140 to let it compute any other tensors defined in the graph, e.g.: https://github.com/tensorpack/tensorpack/blob/8879637399b665bfd11aa5ded663619a1a19bdb9/examples/FasterRCNN/predict.py#L40-L47 
(But again, there is no ""loss"" defined in the testing graph).",since want compute test loss test multiple definition test loss unless define somehow first compute training loss test way training want compute test support multiple change let compute defined graph loss defined testing graph,issue,negative,positive,neutral,neutral,positive,positive
493682607,Closing as this is by design. You can avoid this by not using the spawn method.,design avoid spawn method,issue,negative,neutral,neutral,neutral,neutral,neutral
493682516,"> How can I get the recognition of val data?

https://tensorpack.readthedocs.io/tutorial/inference.html#offlinepredictor
The above is used at:
https://github.com/tensorpack/tensorpack/blob/8879637399b665bfd11aa5ded663619a1a19bdb9/examples/ImageNetModels/imagenet_utils.py#L276-L284
You can change the output name to any tensor defined in the graph and it will be predicted below.",get recognition data used change output name tensor defined graph,issue,negative,neutral,neutral,neutral,neutral,neutral
493681173,"> There is no ""testing"" happen in the codebase (you would manually provide path to input images for testing).
If you want to add a new dataset for evaluation, it can be in the same format as ""val"" (i.e., a `instances_XXX.json` and `XXX/*.jpg`

Ok, you say I can manually provide a path to input images for testing. Since I want to compute the test loss on a test dataset with multiple images, is it possible to provide an entire folder for testing? Or do I have to test on a single image at a time? Where should I put this path?",testing happen would manually provide path input testing want add new evaluation format say manually provide path input testing since want compute test loss test multiple possible provide entire folder testing test single image time put path,issue,negative,positive,neutral,neutral,positive,positive
493676322,"@ppwwyyxx 
And I have anther question
1.What you did:
  $python shufflenet.py --eval --data ./data --load ShuffleNetV2-0.5x.npz --v2 -r=0.5
2.What you expected
  Now I can only get top-1 and top-5 error. 
  How can I get the recognition of val data?",anther question python data load get error get recognition data,issue,negative,neutral,neutral,neutral,neutral,neutral
493676122,"@ppwwyyxx Sorry to trouble you again! 
1.What you did:
    I train shuffleNetV2 with my own dataset, which contains 90,000 images for training the model
   And I don't use data aug
![image](https://user-images.githubusercontent.com/19808900/57970182-f00b7b00-79b0-11e9-9fe9-8ba5ddd72560.png)
2.What you expected
  I have trained almost 500 epoches, top*-errors are still high. 
 Is it because I train less, or  training data less, or something else?
![image](https://user-images.githubusercontent.com/19808900/57970168-be92af80-79b0-11e9-8ac3-e4b767f6ab34.png)


",sorry trouble train training model use data image trained almost top still high train le training data le something else image,issue,negative,negative,neutral,neutral,negative,negative
493624243,"The reason to use ""spawn"" is that it is safer and have lower memory usage. But ""spawn"" does not support lambda functions (this is a Python limitation) and this is exactly why I moved the preprocessor from a lambda to a class.
If you want to use lambda, remove the ""spawn"" method from `train.py`.",reason use spawn lower memory usage spawn support lambda python limitation exactly lambda class want use lambda remove spawn method,issue,negative,positive,positive,positive,positive,positive
493374714,Thanks for the report! The correct fix is `outputs = model.call(*input_tensors)`. I'll push it soon.,thanks report correct fix push soon,issue,negative,positive,positive,positive,positive,positive
493359522,"If you need to change the dataset, all you need to modify is `get_data` and the number of classes in the model.",need change need modify number class model,issue,negative,neutral,neutral,neutral,neutral,neutral
493358818,"@ppwwyyxx 
Oh,i see.
But  I still have two questions about train and eval:
Q1:about train
    If I train shuffleNetV2 with my own dataset, could I  use the code in shufflenet.py :
  
![image](https://user-images.githubusercontent.com/19808900/57911797-5615d680-78bb-11e9-9290-27249362c286.png)

Q2:about val
   If i eval myself model, could I  use the code in shufflenet.py  :
  
![image](https://user-images.githubusercontent.com/19808900/57911976-c1f83f00-78bb-11e9-8def-52dfe75e878e.png)


  
",oh see still two train train train could use code image model could use code image,issue,negative,neutral,neutral,neutral,neutral,neutral
493306485,"@ppwwyyxx You mean that I need to write a dataflow which replace dataset.ILSVRC12 function?
![image](https://user-images.githubusercontent.com/19808900/57901345-42587900-7897-11e9-9439-263bf1618edc.png)
",mean need write replace function image,issue,negative,negative,negative,negative,negative,negative
493211155,A quick shot is to `print(ret['gt_masks'].shape)` to see whether it's reasonable. This is most likely what's taking the space.,quick shot print ret see whether reasonable likely taking space,issue,negative,positive,positive,positive,positive,positive
493210417,"The log and config looks correct. I think there is still something wrong in your dataset. You can try the following:
1. Run `python data.py` directly (after removing the path in the end of `data.py`). This will run the dataloader alone without training, and should throw the same error.
2. If it does throw the error. Add:
```python
from tensorpack.utils import dumps
print(""size:"", len(dumps(ret)))
```
in the end of the preprocessor, in data.py, and set `DATA.NUM_WORKERS=0`. 
This will tell you the size of the data, and if it is too large, you can dump the data to a file to further understand what went wrong.",log correct think still something wrong try following run python directly removing path end run alone without training throw error throw error add python import print size ret end set tell size data large dump data file understand went wrong,issue,negative,negative,negative,negative,negative,negative
493206730,"Difference in config.py
```
88c88
< _C.DATA.TRAIN = ('WIDER_train',)   # i.e. trainval35k, AKA train2017
---
> _C.DATA.TRAIN = ('coco_train2014', 'coco_valminusminival2014')   # i.e. trainval35k, AKA train2017
90c90
< _C.DATA.VAL = ('WIDER_val', )  # AKA val2017
---
> _C.DATA.VAL = ('coco_minival2014', )  # AKA val2017
92c92
< _C.DATA.NUM_CATEGORY = 1  # without the background class (e.g., 80 for COCO)
---
> _C.DATA.NUM_CATEGORY = 80  # without the background class (e.g., 80 for COCO)
```

Difference in coco.py
```
28c28
<     COCO_id_to_category_id = {0:1}  # noqa
---
>     COCO_id_to_category_id = {13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30, 35: 31, 36: 32, 37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40, 46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48, 54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56, 62: 57, 63: 58, 64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64, 74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72, 82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}  # noqa
34c34,35
<     class_names = [""face""]  # noqa
---
>     class_names = [
>         ""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"", ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"", ""cat"", ""dog"", ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"", ""umbrella"", ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"", ""sports ball"", ""kite"", ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"", ""tennis racket"", ""bottle"", ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"", ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"", ""chair"", ""couch"", ""potted plant"", ""bed"", ""dining table"", ""toilet"", ""tv"", ""laptop"", ""mouse"", ""remote"", ""keyboard"", ""cell phone"", ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"", ""vase"", ""scissors"", ""teddy bear"", ""hair drier"", ""toothbrush""]  # noqa
213,214c214,216
<     for split in [""WIDER_train"", ""WIDER_val""]:
<         DatasetRegistry.register(split, lambda x=split: COCODetection(basedir, x))
---
>     for split in [""train2017"", ""val2017"", ""train2014"", ""val2014"",
>                   ""valminusminival2014"", ""minival2014""]:
>         DatasetRegistry.register(""coco_"" + split, lambda x=split: COCODetection(basedir, x))
218,219c220,221
<     basedir = 'coco'
<     c = COCODetection(basedir, 'WIDER_train')
---
>     basedir = '~/data/coco'
>     c = COCODetection(basedir, 'train2014')
```

The log file is attached below
[log.log](https://github.com/tensorpack/tensorpack/files/3188564/log.log)

Thank you!
",difference aka train aka train aka aka without background class coco without background class coco difference face person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe umbrella handbag tie suitcase sport ball kite baseball bat baseball glove surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza cake chair couch potted plant bed dining table toilet mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors bear hair drier toothbrush split split lambda split train train split lambda log file attached thank,issue,negative,positive,neutral,neutral,positive,positive
493156352,"From the paper:
> For BN layers, the learnable scaling coefficient γ is initialized to be 1, except for each residual block’s last BN where γ is initialized to be 0. Setting γ = 0 in the last BN of each residual block causes the forward/backward signal initially to propagate through the identity shortcut of ResNets, which we found to ease optimization at the start of training. This initialization improves all models but is particularly helpful for large minibatch training as we will show.",paper learnable scaling coefficient except residual block last setting last residual block signal initially propagate identity found ease optimization start training particularly helpful large training show,issue,negative,positive,neutral,neutral,positive,positive
493124495,"80 is a small number.
Could you post your changes and your entire logs, as the issue template requested?",small number could post entire issue template,issue,negative,negative,negative,negative,negative,negative
493030181,"Thank you for the quick reply!

I didn't modify anything in the data.py. In addition, I have checked the data input and its size, and the images seem to be fine.

The error occurs for different images also. Could it be the number of masks each image has? There are images which have over 80 masks. ",thank quick reply modify anything addition checked data input size seem fine error different also could number image,issue,negative,positive,positive,positive,positive,positive
492879663,"The name only needs to match the file name in the json. The prefix is not important.
The names for COCO 2014 have that prefix, but the names for COCO 2017 do not. I'll update the readme to avoid confusion.",name need match file name prefix important coco prefix coco update avoid confusion,issue,negative,positive,positive,positive,positive,positive
492854207,"> ValueError: 1916481600 exceeds max_bin_len(1000000000)


This indicates that the size of data for __one image__,  after the preprocessing, is around 2GB.
The data should only contain the image and some annotations (assuming you did not modify it) and should not have such a large size. This indicates you may have some erroneous data format.",size data around data contain image assuming modify large size may erroneous data format,issue,negative,negative,negative,negative,negative,negative
492761895,"Though we still can't reproduce or understand why it stucks after training for a while, there have been several recent issues about memory use in the Mask R-CNN example, which leads to weird issues. The memory issue mainly came from the use of `fork` in a multi threaded process, which is by itself unsafe and may lead to weird issues.
After https://github.com/tensorpack/tensorpack/commit/8f10b0f8580293721b07a40e3e9d4c6d82398790, the Mask R-CNN example now switch to ""spawn"" multiprocessing method which is safer and reduce CPU memory usage by about 2x. Hopefully this can resolve some of the issues.

Closing due to lack of activity, but feel free to comment if there is any follow ups about the issue.

Another potential reason is from bug in NCCL<=2.4.2. (fixed in https://docs.nvidia.com/deeplearning/sdk/nccl-release-notes/rel_2-4-7.html#rel_2-4-7)",though still ca reproduce understand training several recent memory use mask example weird memory issue mainly came use fork threaded process unsafe may lead weird mask example switch spawn method reduce memory usage hopefully resolve due lack activity feel free comment follow issue another potential reason bug fixed,issue,positive,negative,neutral,neutral,negative,negative
492723314,"1. They are variables saved by the momentum optimizer.
2. That's correct.
3. That's a machine learning question and not related to tensorpack.

As mentioned in https://tensorpack.readthedocs.io/tutorial/save-load.html#transfer-learning, if it is not easy to figure out the names to ignore, you can just rename the layer in the model.
To figure out what names are in a checkpoint, you can also use https://github.com/tensorpack/tensorpack/blob/master/scripts/ls-checkpoint.py",saved momentum correct machine learning question related easy figure ignore rename layer model figure also use,issue,negative,positive,positive,positive,positive,positive
492490892,"Thank you for your reply. The problem solved after upgrading to tensorflow1.12 and cudnn7.5.1. Here is the environment info now:
--------------------  -------------------------------------------------------------------
sys.platform          linux
Python                3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) [GCC 7.2.0]
Tensorpack            0.9.4
Numpy                 1.14.3
TensorFlow            1.12.0/v1.12.0-0-ga6d8ffae09
TF Compiler Version   4.8.5
TF CUDA support       True
TF MKL support        False
Nvidia Driver         /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.418.40.04
CUDA                  /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
CUDNN                 /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.1
NCCL                  /usr/lib/x86_64-linux-gnu/libnccl.so.2.1.2
CUDA_VISIBLE_DEVICES  None
GPU 0,1,2,3           Tesla K80
Free RAM              223.09/251.81 GB
CPU Count             40
cv2                   4.0.0
msgpack               0.6.1
python-prctl          True
--------------------  -------------------------------------------------------------------
",thank reply problem environment python default compiler version support true support false driver none free ram count true,issue,positive,positive,positive,positive,positive,positive
492307397,I just ran the same training for 1 epoch and could not observe the divergence.,ran training epoch could observe divergence,issue,negative,neutral,neutral,neutral,neutral,neutral
492306198,"One thing I forgot: register the name of your datasets here: https://github.com/tensorpack/tensorpack/blob/f0626dcb826e9027295bada03033bbfaef5008b3/examples/FasterRCNN/dataset/coco.py#L205-L212
This was recently added to map names into datasets.

Also, I realized that modifying `COCO_id_to_category_id` is confusing to people unfamiliar with COCO, so after https://github.com/tensorpack/tensorpack/commit/2902bfbe018a753543bc84ac2cf44f015c5aa1b0 the dict can be made an empty dict for your datasets.",one thing forgot register name recently added map also people unfamiliar coco made empty,issue,negative,negative,neutral,neutral,negative,negative
492302767,"> how should I modify my dataset structure, so that I can reuse the Faster R-CNN example code with minimal modifications? Would something like this work?

Yes. For minimal modifications to code, use COCO format.

> Where should I put the test data, in a folder test/ at the same level as train/ and val/?

There is no ""testing"" happen in the codebase (you would manually provide path to input images for testing).
If you want to add a new dataset for evaluation, it can be in the same format as ""val"" (i.e., a `instances_XXX.json` and `XXX/*.jpg`

> which is exactly the COCO format for the semantic segmentation task? The official page is http://cocodataset.org/#format-data . I guess it's Object Detection?

Yes. You can also download one of the json there (pick a small one) and take a look.

> what else should I modify, in order to make the Faster R-CNN example code run on a new dataset? For example, are there any ""features"" of the COCO datasets which are hard-coded in the Faster R-CNN example code, which I should be aware of?

All you need to change are:
1. `cfg.DATA.TRAIN/VAL/NUM_CATEGORY`
2. `COCO_id_to_category_id` and `class_names` in `COCODetection`.",modify structure reuse faster example code minimal would something like work yes minimal code use coco format put test data folder level testing happen would manually provide path input testing want add new evaluation format exactly coco format semantic segmentation task official page guess object detection yes also one pick small one take look else modify order make faster example code run new example coco faster example code aware need change,issue,positive,positive,neutral,neutral,positive,positive
492299845,"Theoretically, you can modify the txt files ( `test.txt` is not used) and the image files to use your own dataset. The structure of the train folder in your image is correct.

But it is better to [write a dataflow](https://tensorpack.readthedocs.io/tutorial/extend/dataflow.html) that loads your images.",theoretically modify used image use structure train folder image correct better write,issue,negative,positive,positive,positive,positive,positive
492298944,I suspect this is due to bugs in your old version of cudnn. Could you upgrade to cudnn 7.4 or 7.5?,suspect due old version could upgrade,issue,negative,negative,neutral,neutral,negative,negative
492180797,"@ppwwyyxx Thank you very much,I see!
Could I ask you anther question?
1. What you did:
 I download the files of ilsvrc_metadata
![image](https://user-images.githubusercontent.com/19808900/57690735-e8786900-7674-11e9-994f-2ffd230f9af0.png)

2. What you expected
Q1:
   If I  train shufflenetV2 with my own dataset, do I only need to provide my own train.txt 、val.txt and test.txt？
Q2:
And train folder should like this?
![image](https://user-images.githubusercontent.com/19808900/57690992-9d128a80-7675-11e9-9543-22903518d4dd.png)

",thank much see could ask anther question image train need provide train folder like image,issue,positive,positive,positive,positive,positive,positive
492128248,"> Does isSet = True means the thread is stopped?

Yes.",true thread stopped yes,issue,positive,positive,positive,positive,positive,positive
492066467,"You should have filled the issue template which will make your question more likely to be correctly answered. 

If you use a small amount of images, you'll see the error about buffer_size as you commented at https://github.com/tensorpack/tensorpack/issues/945#issuecomment-491750878. The solution is to use a small buffer_size at https://github.com/tensorpack/tensorpack/blob/aa1f82f7f1d9dded48b3bed08c2e40d998ba7fbd/examples/ImageNetModels/imagenet_utils.py#L102.",filled issue template make question likely correctly use small amount see error solution use small,issue,negative,negative,neutral,neutral,negative,negative
492047440,"@ppwwyyxx Thanks for your reply!
I've tried again without modifying any code, still  same logs.
The difference is that I only used 10 validation images, and val images and val.txt like below:
![image](https://user-images.githubusercontent.com/19808900/57665096-379aab80-762d-11e9-92d0-742197b62737.png)
![image](https://user-images.githubusercontent.com/19808900/57665097-38334200-762d-11e9-92e1-8f72db7dd2c5.png)
Does it matter?
",thanks reply tried without code still difference used validation like image image matter,issue,positive,positive,positive,positive,positive,positive
491900049,"Either you have added bugs to the code or you're not using latest version of the code.
The command you gave will not print logs like that.",either added code latest version code command gave print like,issue,negative,positive,positive,positive,positive,positive
491898954,There is nothing that stops you from doing mixed precision training in tensorpack because the model is implemented by you.,nothing mixed precision training model,issue,negative,neutral,neutral,neutral,neutral,neutral
491817092,"@ppwwyyxx Sorry to bother, I want to try to use DataFromList to read (image_name, label) from txt list , for replacing dataset.ILSVRC12, like this:
    image_names, labels = load_names(datadir, 'train.txt')  ## read lines and split
    ds = DataFromList(list(zip(input_data, target_data)), shuffle=True)
    #ds = dataset.ILSVRC12(datadir, 'train', shuffle=True)
    ds = AugmentImageComponent(ds, augmentors, copy=False)
    def mapf(dp):
            fname, cls = dp
            im = cv2.imread(fname, cv2.IMREAD_COLOR)
            im = np.flip(im, axis=2)
            # print(""fname={}"".format(fname))
            im = aug.augment(im)
            return im, cls
    ds = MultiThreadMapData(ds, parallel, mapf, buffer_size=2000, strict=True)
    ds = BatchData(ds, batch_size, remainder=True)
    ds = PrefetchDataZMQ(ds, 1)

But I find that ""list(zip(input_data, target_data))"" is not the right operation, so how can I do it to use tensorpack to read from txt and use augment and prefrechdata again? Thanks so much for your help!",sorry bother want try use read label list like read split list zip print return parallel find list zip right operation use read use augment thanks much help,issue,positive,positive,neutral,neutral,positive,positive
491750878,"@ywpkwon Hi, I encountered the same problem when ran shufflenetV2.
How could I modify the code ?
looking forward your replay,thx!",hi problem ran could modify code looking forward replay,issue,negative,neutral,neutral,neutral,neutral,neutral
491623902,This is now added as a special case in the script (in https://github.com/tensorpack/tensorpack/commit/eafe564b69f53f9c91485324a9673aed0afea3db#diff-d2a0332f6e8a3bb8156ef2bdd3d52aac).,added special case script,issue,negative,positive,positive,positive,positive,positive
491622731,run `from tensorflow.contrib.memory_stats import MaxBytesInUse` at the beginning of the script,run import beginning script,issue,negative,neutral,neutral,neutral,neutral,neutral
491597721,"The matterport/Mask_RCNN library gives an inference time of 1.7 seconds per 512x512 image (at least in this scenario). We need to detect RBCs in around 700 images to analyse an entire sample.

This library is much more efficient. Total execution time (inclusive of RBC localisation by hacking some components of the Faster R-CNN example) is 4-5 minutes. matterport/Mask_RCNN furnishes 15-17 minutes.

Thank you for making this amazing library open-source. It seems well polished in every aspect.",library inference time per image least scenario need detect around analyse entire sample library much efficient total execution time inclusive hacking faster example thank making amazing library well polished every aspect,issue,positive,positive,positive,positive,positive,positive
491535321,"> I plan to update moving statistics according to fakeIm = self.decoder(realz) and realIm = self.decoder(fakez)

I get that part, but you still have two choices in `self.decoder(outz)`. Even when you don't update moving statistics there, you can:
1. use the moving statistics (updated by other decoder()) to normalize
2. use the batch input statistics (i.e., statistics from outz) to normalize

Setting `training=False` will give you option 1.
Option 2 requires the latest commit https://github.com/tensorpack/tensorpack/commit/f6ede6124ef5414ee4761e0e4725b8c0cfc1319c, and it corresponds to `training=True, ema_update=""skip""`. I implement this just now and haven't tested enough on it. 
You can read more at the updated docs: https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm
 ",plan update moving statistic according get part still two even update moving statistic use moving statistic normalize use batch input statistic statistic normalize setting give option option latest commit skip implement tested enough read,issue,negative,positive,positive,positive,positive,positive
491531207,You can also use [argscope](https://tensorpack.readthedocs.io/tutorial/symbolic.html#argscope-and-linearwrap) around the last `decoder()` to achieve the same.,also use around last achieve,issue,negative,neutral,neutral,neutral,neutral,neutral
491514275,"Tuning `_C.RPN.TRAIN_PER_LEVEL_NMS_TOPK`, `_C.RPN.TEST_PER_LEVEL_NMS_TOPK` and `_C.TEST.FRCNN_NMS_THRESH` made a difference.

![output](https://user-images.githubusercontent.com/5787026/57570883-53246d00-7407-11e9-8390-5bde32c4a43d.png)
",tuning made difference output,issue,negative,neutral,neutral,neutral,neutral,neutral
491506625,"Config used

```
_C.TRAIN.NUM_GPUS = None         # by default, will be set from code
_C.TRAIN.WEIGHT_DECAY = 1e-4
_C.TRAIN.BASE_LR = 1e-3                # Originally 1e-2. Gives NaN loss
_C.TRAIN.WARMUP = 1000 
_C.TRAIN.WARMUP_INIT_LR = 1e-3 * 0.33         # Originally 1e-2 * 0.33
_C.TRAIN.STEPS_PER_EPOCH = 500
_C.TRAIN.STARTING_EPOCH = 1

_C.TRAIN.LR_SCHEDULE = [3000, 6400, 9600, 12800]
_C.TRAIN.EVAL_PERIOD = 25

_C.PREPROC.TRAIN_SHORT_EDGE_SIZE = [512, 512]  # [min, max] to sample from
_C.PREPROC.TEST_SHORT_EDGE_SIZE = 512
_C.PREPROC.MAX_SIZE = 512

_C.RPN.FG_RATIO = 0.5  # fg ratio among selected RPN anchors
_C.RPN.BATCH_PER_IM = 512  # total (across FPN levels) number of anchors that are marked valid
_C.RPN.MIN_SIZE = 0
_C.RPN.PROPOSAL_NMS_THRESH = 0.9

_C.RPN.TRAIN_PRE_NMS_TOPK = 12000
_C.RPN.TRAIN_POST_NMS_TOPK = 2000
_C.RPN.TEST_PRE_NMS_TOPK = 6000
_C.RPN.TEST_POST_NMS_TOPK = 2000  
# for FPN, #proposals per-level and #proposals after merging are (for now) the same
# if FPN.PROPOSAL_MODE = 'Joint', these options have no effect
_C.RPN.TRAIN_PER_LEVEL_NMS_TOPK = 2000
_C.RPN.TEST_PER_LEVEL_NMS_TOPK = 1000

# fastrcnn training ---------------------
_C.FRCNN.BATCH_PER_IM = 512
_C.FRCNN.BBOX_REG_WEIGHTS = [20., 20., 10., 10.]  # Detectron: 10, 10, 5, 5
_C.FRCNN.FG_THRESH = 0.5
_C.FRCNN.FG_RATIO = 0.25  # fg ratio in a ROI batch

# FPN -------------------------
_C.FPN.ANCHOR_STRIDES = (4, 8, 16, 32, 64) 
_C.FPN.PROPOSAL_MODE = 'Level' 
_C.FPN.NUM_CHANNEL = 256
_C.FPN.NORM = 'None' 
# The head option is only used in FPN. For C4 models, the head is C5
_C.FPN.FRCNN_HEAD_FUNC = 'fastrcnn_2fc_head'
# choices: fastrcnn_2fc_head, fastrcnn_4conv1fc_{,gn_}head
_C.FPN.FRCNN_CONV_HEAD_DIM = 256
_C.FPN.FRCNN_FC_HEAD_DIM = 1024
_C.FPN.MRCNN_HEAD_FUNC = 'maskrcnn_up4conv_head'   # choices: maskrcnn_up4conv_{,gn_}head

# Mask-RCNN
_C.MRCNN.HEAD_DIM = 256

# Cascade-RCNN, only available in FPN mode
_C.FPN.CASCADE = False

# testing -----------------------
_C.TEST.FRCNN_NMS_THRESH = 0.5

_C.TEST.RESULT_SCORE_THRESH = 0.0
_C.TEST.RESULT_SCORE_THRESH_VIS = 0.5   # only visualize confident results
_C.TEST.RESULTS_PER_IM = 400
```",used none default set code originally nan loss originally min sample ratio among selected total across number marked valid effect training ratio roi batch head option used head head head available mode false testing visualize confident,issue,negative,positive,positive,positive,positive,positive
491505575,"Thanks for your reply.
Emmm, I plan to update moving statistics according to `fakeIm = self.decoder(realz)` and `realIm = self.decoder(fakez)`.  According to your advise, I overwrite the logic, 
```
if is_training:
    fakeIm = self.decoder(realz, is_training) # realz from uniform distribution
    realIm = self.decoder(fakez, is_training) # fakez  = self.encoder(im)
    outIm = self.decoder(outz, False)   # outz from another distribution

    loss = self.classifier(realIm, outIm) #
```
It does work!  ",thanks reply plan update moving statistic according according advise overwrite logic uniform distribution false another distribution loss work,issue,negative,negative,negative,negative,negative,negative
491502695,I would guess there are other configs not correctly set.,would guess correctly set,issue,negative,neutral,neutral,neutral,neutral,neutral
491502502,"I get that you didn't want to update moving statistic in ` decoder(outz)`, but you didn't say what you want to do instead.
The batchnorm layer can be called with `training=False` and it won't update moving statistics and details in https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm
If this is not what you want to do, you can also implement your own variants of batchnorm instead.",get want update moving statistic say want instead layer wo update moving statistic want also implement instead,issue,negative,neutral,neutral,neutral,neutral,neutral
491492305,"So, turns out I messed with the `RPN.ANCHOR_SIZES` and `FPN.ANCHOR_STRIDES`

Faster R-CNN results

![output](https://user-images.githubusercontent.com/5787026/57567405-ea260080-73d8-11e9-8295-0e29aeb53448.png)

I do not understand why all of them aren't detected though. Config? Need to train more?",turn faster output understand though need train,issue,negative,neutral,neutral,neutral,neutral,neutral
491426063,"> So since you mentioned that 2GPU will train 2x more batch size, given that I have 4 GPUs, can I just reduce the batch size / 4 per GPU in this case to get the comparable accuracy and reduce the overall training time??

That's correct for large enough models, and depends on how you define your batch size in code. Not for this mnist example.

> I'm not sure where but from FasterRCNN example only batch size of 1 per GPU is available for tensorpack.

The FasterRCNN example in tensorpack only supports batch size 1. That does not mean Tensorpack only supports batch size 1.",since train batch size given reduce batch size per case get comparable accuracy reduce overall training time correct large enough define batch size code example sure example batch size per available example batch size mean batch size,issue,negative,positive,positive,positive,positive,positive
491425169,"Sorry. I'm completely new in this multi-gpu training area. I'm trying to train a custom Keras object detection model on a very large dataset. But the training is taking too much time on a single gpu, so I'm trying to see if I can speedup the training with multiple gpus and tensorpack.   

But from the MNIST example, it takes longer time overall with the multiple GPU training.

So since you mentioned that 2GPU will train 2x more batch size, given that I have 4 GPUs, can I just reduce the batch size / 4 per GPU in this case to get the comparable accuracy and reduce the overall training time?? 

I'm quite confused since I'm not sure where but from FasterRCNN example only batch size of 1 per GPU is available for tensorpack. 
",sorry completely new training area trying train custom object detection model large training taking much time single trying see training multiple example longer time overall multiple training since train batch size given reduce batch size per case get comparable accuracy reduce overall training time quite confused since sure example batch size per available,issue,negative,positive,neutral,neutral,positive,positive
491421651,"The number from my runs is a bit better: 1.3s for 1 GPU and 1.9s for 2 GPU, which means a 70% scaling efficiency (2GPU will train 2x more batch size).

Also note that using this model (no matter it is written in tensorpack or Keras) for benchmarking is not reasonable at all since it's way too small and the time to compute the training is totally not worth the time to communicate it.",number bit better scaling efficiency train batch size also note model matter written reasonable since way small time compute training totally worth time communicate,issue,positive,positive,positive,positive,positive,positive
491417370,"Your number does look strange, I'll take a look at that later. The keras ResNet example does have a good speedup last time I run.",number look strange take look later example good last time run,issue,negative,positive,positive,positive,positive,positive
491416375,"Like I said above, this example does not fully utilise tensorpack trainers to have good performance. To benchmark performance please use a native tensorpack example, or our official benchmark https://github.com/tensorpack/benchmarks/tree/master/other-wrappers",like said example fully good performance performance please use native example official,issue,positive,positive,positive,positive,positive,positive
491412909,"Thanks! The code with single gpu training is now working fine. But I have a concern. I haven't measured it super precisely, but from running the provided mnist example code on my 4 RTX 2080 Ti GPU station, the single gpu code seems to be a lot faster than the multi-gpu code.

Except for the first epoch, the single gpu code runs at around 1.3s per epoch, but the multi gpu code runs at around 3s per epoch. 

Do you think using a Horovod trainer with Tensorpack can mitigate this problem?? I think Tensorpack provides a wrapper for Horovod, but I haven't found good documentation directly from Horovod.",thanks code single training working fine concern measured super precisely running provided example code ti station single code lot faster code except first epoch single code around per epoch code around per epoch think trainer mitigate problem think wrapper found good documentation directly,issue,positive,positive,positive,positive,positive,positive
491410057,Sorry. You need `from contextlib import contextmanager` as well.,sorry need import well,issue,negative,negative,negative,negative,negative,negative
491407029,"Thank you for the quick response.
I updated tensorpack, downloaded the example codes, and ran the code again but had another error:

 C:\Users\dps42\Desktop\dps42\tensorpack\examples\keras>python mnist-keras.py
Traceback (most recent call last):
  File ""mnist-keras.py"", line 29, in <module>
    @contextmanager
NameError: name 'contextmanager' is not defined",thank quick response example ran code another error python recent call last file line module name defined,issue,negative,positive,positive,positive,positive,positive
491403663,"This was introduced yesterday in https://github.com/tensorpack/tensorpack/commit/6926d22a88547899e390ebe709913c38382ff361 and is now fixed.
Please note that [Keras support in tensorpack is experimental](https://github.com/tensorpack/tensorpack/tree/master/examples/keras#note) and there is no guarantee that your custom Keras model still works.
If your purpose is to benchmark, also note that using Keras inside tensorpack will not utilize full optimization in tensorpack.",yesterday fixed please note support experimental guarantee custom model still work purpose also note inside utilize full optimization,issue,positive,positive,positive,positive,positive,positive
491262276,"@ppwwyyxx , @armandmcqueen  - Thank you both for the info.

I am planning to test this with Faster R-CNN and Mask R-CNN. Will let you know how it goes and will ask for any help if required. You can close the issue if you want.",thank test faster mask let know go ask help close issue want,issue,positive,neutral,neutral,neutral,neutral,neutral
491023283,"I found that it's not hard to make ""SyncMultiGPUTrainerParameterServer"" to suport using Keras inside tensorpack model. See https://github.com/tensorpack/tensorpack/blob/6926d22a88547899e390ebe709913c38382ff361/examples/keras/mnist-keras.py#L27-L41 for a working example. Note that you still cannot create the Keras model inside the tensorpack function: it has to be a separate memoized function as the above example shows.
Supporting the more efficient replicated trainer will need more hacks due to how Keras works (and is one of the main reasons why Keras's multi gpu trainer is very slow).

Closing since Keras is not something we'd like to support anyway.",found hard make inside model see working example note still create model inside function separate function example supporting efficient replicated trainer need due work one main trainer slow since something like support anyway,issue,positive,negative,neutral,neutral,negative,negative
490991250,"Keras layers do not respect variable scope, which fundamentally conflicts with the design of tensorpack multigpu trainer. Your usage will work if you use a single-GPU trainer (`SimpleTrainer`).
Multi gpu training with Keras layers is doable if your whole model is in Keras (https://github.com/tensorpack/tensorpack/tree/master/examples/keras). But these are all experimental since Keras fundamentally conflicts with tensorflow scoping.",respect variable scope fundamentally design trainer usage work use trainer training doable whole model experimental since fundamentally,issue,negative,positive,positive,positive,positive,positive
490309116,"It could, in theory, be an issue with some batch size config (RPN.BATCH_PER_IM, FRCNN.BATCH_PER_IM, TEST.RESULTS_PER_IM, etc). Your data has a high number of boxes compared to COCO. Perhaps too many valid results get filtered before prediction or perhaps you are only training on a subset of the labels. 

But given the obvious spatial distribution of results, it seems more likely to be an issue related to cropping or resizing.",could theory issue batch size data high number coco perhaps many valid get prediction perhaps training subset given obvious spatial distribution likely issue related,issue,negative,positive,positive,positive,positive,positive
490301815,"I don't think when to drop LR will make such differences, though. There should be something larger that's different in your code or settings.",think drop make though something different code,issue,negative,neutral,neutral,neutral,neutral,neutral
489865855,"https://github.com/tensorpack/tensorpack/tree/master/examples/ImageNetModels:
> To train any of the models, just do ./{model}.py --data /path/to/ilsvrc. More options are available in ./{model}.py --help.

""More options"" for shufflenet is in the results table.",train model data available model help table,issue,negative,positive,positive,positive,positive,positive
489754406,"Ah. I see. Thanks for the info.
I am making some changes so as to get the LR to drop sooner.
Will test and update.

One question for now is, is there support for grayscale images? I'll have to load the images in RGB and train?",ah see thanks making get drop sooner test update one question support load train,issue,positive,positive,positive,positive,positive,positive
489740798,"From the results it does look like the model is doing something reasonable, but it is only doing it in a small region. So you might be using the model in a wrong way. But no one can debug by looking at the image. Any details you provide can be helpful.",look like model something reasonable small region might model wrong way one looking image provide helpful,issue,negative,negative,negative,negative,negative,negative
489738102,I now can successfully reproduce your issue. This is a tensorflow bug (in COND_V2) that has been workaround in latest tensorpack version. Upgrading tensorpack will solve your issue.,successfully reproduce issue bug latest version solve issue,issue,positive,positive,positive,positive,positive,positive
489737319,"I do not know how to reproduce this issue. Please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",know reproduce issue please post relevant following issue template click new issue unexpected visit link post unexpected,issue,negative,positive,positive,positive,positive,positive
489736490,"I review the code in [optimizer.py](https://github.com/tensorpack/tensorpack/commit/d8dd7ca93640c7299d2c2da064d57982fe56ff69#diff-11dc0f5801ffda9fc76942cf5ea4c217L222).
May it be caused by the following? 
```
op = tf.cond(pred, update_grad, tf.no_op, name=name).op	
```
The newest version has remove this line and use ```tf.identity```:
```
                op = tf.cond(pred, update_grad, tf.no_op)

             if global_step is not None:
                # Tensorpack maintains global_step by other means,
                # so this option is useless in tensorpack trainers.
                # But we include the implementation here for completeness
                global_step_increment = tf.assign_add(global_step, 1)
                op = tf.group(op, global_step_increment, name=name)
            else:
                op = tf.identity(op, name=name).op
```

",review code may following version remove line use none option useless include implementation completeness else,issue,negative,negative,negative,negative,negative,negative
489713498,"The log will print the time (epochs) the LR will drop. You can change the LR schedule in config.
The batch size is equal to the number of GPUs you use which is also printed in the log.",log print time drop change schedule batch size equal number use also printed log,issue,negative,neutral,neutral,neutral,neutral,neutral
489709205,"Alright. I will look into it.

I am not able to make the LR drop over time.
It starts at `0.00033` and increases for 1000 steps, as per the config, and then is a constant `0.001`

I read in the config file that LR_Schedule should decay the LR if `bs!=8`. Where can I modify the batch size? At some point, the loss hits a region after which it starts increasing again.",alright look able make drop time per constant read file decay modify batch size point loss region increasing,issue,negative,positive,positive,positive,positive,positive
489704350,"I think your code did something different in each framework (different configurations, different architectures, etc) or you may make a mistake in using it. But without more details there isn't much I can tell from the image.
The tensorpack implementation is able to reproduce performance of the models in Detectron model zoo (which matterport's cannot), as you can see from the README. It faithfully supports most features available in Detectron.",think code something different framework different different may make mistake without much tell image implementation able reproduce performance model zoo see faithfully available,issue,negative,positive,positive,positive,positive,positive
489198266,"Closing as there is no response. The most likely cause is out-of-memory on the host, since the example running with the default parameters may use around 16GB host memory. 
Feel free to reopen if you still have issues.",response likely cause host since example running default may use around host memory feel free reopen still,issue,positive,positive,positive,positive,positive,positive
489195569,"It is already included in the ""resume training"" section:

> “resume training” is mostly just “loading the last known checkpoint”. Therefore you should refer to the previous section on how to load a model.",already included resume training section resume training mostly loading last known therefore refer previous section load model,issue,negative,positive,positive,positive,positive,positive
489193630,"Thank you for the clarification. With 'session_init', it works fine with the following code:
""""""
    model_path = os.path.join('train_log', 'mnist-tfslim', 'model-4680')
    sess_init=SaverRestore( model_path) 

    config = TrainConfig(
        model=Model(),
        dataflow=dataset_train,
        callbacks=[
            ModelSaver(),
            InferenceRunner(
                dataset_test,
                ScalarStats(['cross_entropy_loss', 'accuracy'])),
        ],
        session_init= sess_init,
        starting_epoch=10,
        max_epoch=20,
    )
""""""
I would appreciate if you could include in the tensorpack documentation that 'session_init' should be used in addition to the 'starting_epoch' in 'Resume Training' section, for future users. Thank you!

",thank clarification work fine following code would appreciate could include documentation used addition training section future thank,issue,positive,positive,positive,positive,positive,positive
489185390,"Your best shot is to upgrade nvidia driver/cuda/cudnn or upgrade/downgrade tensorflow. You can search your error message in `tensorflow/issues` for related ones.
Closing as I believe this issue is not related to tensorpack.",best shot upgrade search error message related believe issue related,issue,negative,positive,positive,positive,positive,positive
489184453,"You need to load the checkpoint with `session_init` in addition to setting `starting_epoch`. The documentation already explained: https://tensorpack.readthedocs.io/tutorial/save-load.html#load-a-model-to-a-session
session_init is an argument of `TrainConfig`.",need load addition setting documentation already argument,issue,negative,neutral,neutral,neutral,neutral,neutral
489176902,"Hi. Thanks for the response. The log says that it is starting from epoch 10, but I meant that since when I stopped after epoch 10, the accuracy was above 0.99, so when I restart training from epoch 10, I should get above 0.99 accuracy too. So I meanted it appeared to have the same 'net-effect' of starting from epoch 1. Sorry for the confusion. AutoResumeTrainConfig works fine in this case, and it resumed with 0.99 accuracy just fine.

I just looked at the 'Resume Training' secton from the documentation. And the documentation suggested that I can adjust 'starting_epoch' to resume training, but it does not seem to give the desired effect. Is there an example how to restore the model with 'sess_init', and will I be able to see 0.99 accuracy for this case??",hi thanks response log starting epoch meant since stopped epoch accuracy restart training epoch get accuracy starting epoch sorry confusion work fine case accuracy fine training documentation documentation adjust resume training seem give desired effect example restore model able see accuracy case,issue,positive,positive,positive,positive,positive,positive
489173356,"CUDA error is most likely either a nvidia bug or a tensorflow bug. 
We cannot reproduce this issue to help you.",error likely either bug bug reproduce issue help,issue,negative,neutral,neutral,neutral,neutral,neutral
489172667,"I don't have short term plan to implement panoptic segmentation.
It shouldn't be very hard: on top of the Mask R-CNN example you'll just need to add some extra information in the data loader and a new branch in the FPN model.",short term plan implement panoptic segmentation hard top mask example need add extra information data loader new branch model,issue,negative,positive,neutral,neutral,positive,positive
489172367,"The training does start from epoch 10 as you can see from the log.
To load the model at epoch 10 you'll need to specify sess_init: https://tensorpack.readthedocs.io/tutorial/save-load.html",training start epoch see log load model epoch need specify,issue,negative,neutral,neutral,neutral,neutral,neutral
488971264,"> Thanks so much. Now I can get a NHWC *pb successfully with everything is correct. I modified shufflenet.py a lot, and imagenet_utils.py by one line change from NCHW to NHWC. I can contribute my code if needed.

need your code please",thanks much get successfully everything correct lot one line change contribute code need code please,issue,positive,positive,positive,positive,positive,positive
488161589,"It is better to fix the dependency issue first.
apt install libcap-dev
pip install python-prctl",better fix dependency issue first apt install pip install,issue,negative,positive,positive,positive,positive,positive
488044023,">  If I want to sample one thousand images, should this parameter be set to 1000 ?

No. See https://tensorpack.readthedocs.io/tutorial/extend/trainer.html#assumptions-of-base-trainer :
> steps_per_epoch can be any number you set and it only affects the schedule of callbacks

You should write a new dataflow which produces 1000 images.

> how does this __iter__ work?

Unaffected as mentioned above.

> Will self.shuffle only be used once when obtaining the iterator? If the yield data is not enough, will this function be recalled, then shuffle data and yield data again?

It will be called more than once. That's how people do shuffle.

> Must the __len__ function return a value which is number of times yield from __iter__ function ?

No for training. This is answered in https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.DataFlow

> If mutliple processors are used to prefetch data, do they have different random seeds for shuffling? 

They do. Otherwise it's not really shuffling

> And, how to ensure the mini-batch including nonredundant samples?

If you use `PrefetchData*`, it is not ensured. You're subject to birthday paradox. This is explained in https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.PrefetchDataZMQ

`MultiProcessMapData`/`MultiThreadMapData` can use multiple processes/threads and also ensure non-redundant samples.",want sample one thousand parameter set see number set schedule write new work unaffected used yield data enough function shuffle data yield data people shuffle must function return value number time yield function training used data different random shuffling otherwise really shuffling ensure use subject birthday paradox use multiple also ensure,issue,positive,negative,neutral,neutral,negative,negative
487672283,"I don't think batch norm can cause that much difference I got, I did not expect the exactly same result, but should be similar ones. There is no drop out in the net. uh... it's killing me, the model performs so well in the training and validation, but just goes completely off for the OfflinePredictor. 

Thanks anyway!",think batch norm cause much difference got expect exactly result similar drop net killing model well training validation go completely thanks anyway,issue,negative,positive,positive,positive,positive,positive
487355737,"Because this is not the right way to use it and the right way to use `export-model.py` is written in its code. It is not for ShuffleNet and you cannot use it for shufflenet.

See https://tensorpack.readthedocs.io/tutorial/inference.html",right way use right way use written code use see,issue,negative,positive,positive,positive,positive,positive
487352694,"Your model can do something different depends on what you write in the models. (e.g., batch norm layers, drop out layers both do something different in training/testing).

Just a guess. But without ways to reproduce it there isn't much I can tell.",model something different write batch norm drop something different guess without way reproduce much tell,issue,negative,positive,neutral,neutral,positive,positive
487340097,Your question is too general and not something I can answer directly. Please read some tutorials first to understand what your code is doing https://tensorpack.readthedocs.io/tutorial/index.html and then you'll know how to modify its behavior.,question general something answer directly please read first understand code know modify behavior,issue,negative,positive,positive,positive,positive,positive
486986218,"Thanks. To me it seems like they overdo with this cleanup. They should let people decide what ""style"" they wanna use. But yes, lets wait for what will happen, e.g. tf.estimators were slaughtered before community started to use it :) ",thanks like overdo cleanup let people decide style wan na use yes wait happen community use,issue,positive,neutral,neutral,neutral,neutral,neutral
486971868,"For now it's already possible to use most tensorpack features with TF 2.0 due to `tf.compat.v1`.
Most examples are written in 1.0 style but most of them are directly runnable after converting them with the `tf_upgrade_v2` tool.

As for running ""natively"" with 2.0 using eager mode and `tf.function` - it probably won't happen soon. The main issue is that `sess.run + hooks` is a more powerful system than what's now in TF2, and many important tensorpack features are hard to implement with `tf.function` or may need significant rewrite. We'll have better ideas when TF 2 is closer.",already possible use due written style directly runnable converting tool running natively eager mode probably wo happen soon main issue powerful system many important hard implement may need significant rewrite better closer,issue,positive,positive,positive,positive,positive,positive
486959000,"For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

Also read the docs https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.ModelSaver",anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected also read,issue,positive,positive,positive,positive,positive,positive
486833210,"The root cause of this issue is that the host runs out-of-memory and a worker process is forced killed by OS, causing the stuck.
The reason why `GPUUtilizationTracker` leads to increased memory use is due to how `fork` works - it duplicates the current process and therefore may increase memory consumption.

We have added a few improvements in recent commits:
1. A `HostMemoryTracker` callback to track memory consumption during training.
2. fork early in `GPUUtilizationTracker` to save some memory
3. Use less memory in the data loader.

A more fundamental solution to reduce memory usage is to use the `forkserver/spawn` context in Python's `multiprocessing`, available since Python 3.4.",root cause issue host worker process forced o causing stuck reason memory use due fork work current process therefore may increase memory consumption added recent track memory consumption training fork early save memory use le memory data loader fundamental solution reduce memory usage use context python available since python,issue,negative,positive,neutral,neutral,positive,positive
486740704,"> 1- I think any of these files correspond with the new model trained?, in other words, when the model file will be created? do I have to wait till a number of epochs?

See code below in the example:
https://github.com/tensorpack/tensorpack/blob/5982977058218f6aae47e74dad6deb898bbb3062/examples/FasterRCNN/train.py#L192-L194

> python train.py --predict input1.jpg --load C:/Users/GOM81687/Desktop/tensorpack/models/COCO-R101FPN-MaskRCNN-BetterParams.npz --config MODE_MASK=True MODE_FPN=True DATA.BASEDIR=C:/Users/GOM81687/Desktop/tensorpack/COCO/DIR BACKBONE.WEIGHTS=C:/Users/GOM81687/Desktop/tensorpack/models/ImageNet-R50-AlignPadding.npz

Your command is wrong.  See [README](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN#inference):
> Evaluation and prediction will need to be run with the corresponding configs used in training.",think correspond new model trained model file wait till number see code example python predict load command wrong see evaluation prediction need run corresponding used training,issue,negative,negative,negative,negative,negative,negative
486684067,"The second question is about inference. 

To check how inference works I check this command (this exercise is independent of Mapillary trainning)

C:\Users\GOM81687\Desktop\tensorpack\examples\FasterRCNN>python train.py --predict input1.jpg --load C:/Users/GOM81687/Desktop/tensorpack/models/COCO-R101FPN-MaskRCNN-BetterParams.npz --config MODE_MASK=True MODE_FPN=True  DATA.BASEDIR=C:/Users/GOM81687/Desktop/tensorpack/COCO/DIR BACKBONE.WEIGHTS=C:/Users/GOM81687/Desktop/tensorpack/models/ImageNet-R50-AlignPadding.npz

When COCO/DIR are my Mapillary images. Inference works but the output image is the same as the original, in other words nothing was detected: 
- Do I have to download COCO dataset to make inference?

Thanks.",second question inference check inference work check command exercise independent python predict load inference work output image original nothing coco make inference thanks,issue,positive,positive,positive,positive,positive,positive
486682386,"Hi, 

Thank you for the response. 

I installed latest version of tensorpack and now the training process works.

Every epoch takes 25 minutes approx. I interrupted the trainning running after the second epoch because I would like to test the inference before trying to train the entire Mapillary Dataset, and these files were created in a new folder called train_log:

- events.out.tfevents.1556184012.instance-1
- graph-0425-092053.meta
- log.txt
- stats.json

I have 2 questions: 

1- I think any of these files correspond with the new model trained?, in other words, when the model file will be created? do I have to wait till a number of epochs?

Thanks a lot.",hi thank response latest version training process work every epoch interrupted running second epoch would like test inference trying train entire new folder graph think correspond new model trained model file wait till number thanks lot,issue,positive,positive,positive,positive,positive,positive
486443315,">  if stats == -1:
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
MultiProcessMapDataZMQ successfully cleaned-up.

This issue was fixed a week ago in tensorpack. You can install latest tensorpack with `pip install -U git+https://github.com/ppwwyyxx/tensorpack.git`",truth value array one element ambiguous use successfully issue fixed week ago install latest pip install,issue,positive,positive,positive,positive,positive,positive
486425023,"Hi ppwwyyxx, 

Thanks for your answer.

I changed the CPU from 2 units to 16 and the MemoryError disappear!. However after finishing the first epoch I got this error: 

```
[0424 15:42:46 @input_source.py:556] Pre-filling StagingArea ...
[0424 15:42:50 @input_source.py:560] 1 element was put into StagingArea on each tower.
2019-04-24 15:45:30.227941: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
[0424 15:45:53 @param.py:161] [HyperParamSetter] At global_step=1, learning_rate changes from 0.003300 to 0.003307
  0%|2                                                                                                                                          |1/500[03:07<25:56:02, 0.01it/s]
100%|##########################################################################################################################################################################################|500/500[41:59<00:00, 0.02it/s]
[0424 16:24:46 @base.py:285] Epoch 1 (global_step 500) finished, time:41 minutes 59 seconds.
[0424 16:24:46 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
[0424 16:24:46 @misc.py:109] Estimated Time Left: 20 days 23 hours 27 minutes 50 seconds
2019-04-24 16:24:46.991885: W tensorflow/core/kernels/queue_base.cc:277] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed
Traceback (most recent call last):
  File ""train.py"", line 522, in <module>
    launch_train_with_config(traincfg, trainer)
  File ""/home/federicolondon2019/.local/lib/python3.5/site-packages/tensorpack/train/interface.py"", line 101, in launch_train_with_config
    extra_callbacks=config.extra_callbacks)
  File ""/home/federicolondon2019/.local/lib/python3.5/site-packages/tensorpack/train/base.py"", line 344, in train_with_defaults
    steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/federicolondon2019/.local/lib/python3.5/site-packages/tensorpack/train/base.py"", line 316, in train
    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/federicolondon2019/.local/lib/python3.5/site-packages/tensorpack/utils/argtools.py"", line 176, in wrapper
    return func(*args, **kwargs)
  File ""/home/federicolondon2019/.local/lib/python3.5/site-packages/tensorpack/train/base.py"", line 288, in main_loop
    self._callbacks.trigger_epoch()
  File ""/home/federicolondon2019/.local/lib/python3.5/site-packages/tensorpack/callbacks/base.py"", line 156, in trigger_epoch
    self._trigger_epoch()
  File ""/home/federicolondon2019/.local/lib/python3.5/site-packages/tensorpack/callbacks/group.py"", line 97, in _trigger_epoch
    cb.trigger_epoch()
  File ""/home/federicolondon2019/.local/lib/python3.5/site-packages/tensorpack/callbacks/base.py"", line 156, in trigger_epoch
    self._trigger_epoch()
  File ""/home/federicolondon2019/.local/lib/python3.5/site-packages/tensorpack/callbacks/prof.py"", line 85, in _trigger_epoch
    if stats == -1:
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
MultiProcessMapDataZMQ successfully cleaned-up.
```
Attached the full log if needed.
[log_16CPU_8GPUV100.pdf](https://github.com/tensorpack/tensorpack/files/3114142/log_16CPU_8GPUV100.pdf)


Let me know if you want me to open a new issue with this error or you think post it here is OK,

Thank you, 
Alberto.",hi thanks answer disappear however finishing first epoch got error element put tower successfully library locally epoch finished time running time left day skipping attempt queue closed recent call last file line module trainer file line file line file line train file line wrapper return file line file line file line file line file line truth value array one element ambiguous use successfully attached full log let know want open new issue error think post thank,issue,positive,positive,positive,positive,positive,positive
486346163,"The operation system might execute an out-of-memory kill since the amount of CPU memory on your system is small.
After the crash, could you check `dmesg` to see whether the kernel killed any processes due to out-of-memory?

If out-of-memory is indeed an issue, could you upgrade your FasterRCNN examples to [this commit](5982977058218f6aae47e74dad6deb898bbb3062)? With this commit, you'll have the option to reduce memory usage by setting `DATA.NUM_WORKERS` to 1 or 0.
When setting `NUM_WORKERS` to 0, the training consumes only about 8G CPU memory on my machine (although the training might get slower).",operation system might execute kill since amount memory system small crash could check see whether kernel due indeed issue could upgrade commit commit option reduce memory usage setting setting training memory machine although training might get,issue,negative,negative,negative,negative,negative,negative
486287110,"How large is the CPU memory and what's the amount of memory you observed (from `top -o %MEM`, screenshot will be helpful) that the code is taking before it crashed?",large memory amount memory top mem helpful code taking,issue,positive,positive,positive,positive,positive,positive
486230808,"Hi ppwwyyxx, 

Thanks for your recommendation. 

I increased the number of GPU to 8 Tesla V100 and I decreased the cfg.DATA.NUM_WORKERS to 1 but I  still get MemoryError when trainning with Mapillary Dataset, I am just using 5 images to train and 2 to validate. 

I didn't change anything from the last post, just cfg.DATA.NUM_WORKERS. 

The completed log is attached as pdf. 

Any idea what is happening?

Thanks for your support!
[Faster_RCNN_8GPU_MVD.pdf](https://github.com/tensorpack/tensorpack/files/3112474/Faster_RCNN_8GPU_MVD.pdf)

",hi thanks recommendation number still get train validate change anything last post log attached idea happening thanks support,issue,positive,positive,positive,positive,positive,positive
486002017,"Thanks for your suggestions! They make more sense indeed. 
I'll improve the code structure based on them.",thanks make sense indeed improve code structure based,issue,positive,positive,positive,positive,positive,positive
485320966,"> No because this is a Tensor, not a trainable variable, so it can have whatever name.
If it is a trainable variable, this is a valid name iff tf.get_variable_scope() == 'tower0'.

> Do you now have an answer already? 

Thank you very much.
I finally figured it out.",tensor trainable variable whatever name trainable variable valid name answer already thank much finally figured,issue,negative,neutral,neutral,neutral,neutral,neutral
485320000,"You seem to be following examples that are more than 1 years old.

If you look at any of the examples in this repo, you need to `return cost` in `build_graph` instead of setting `self.cost`.",seem following old look need return cost instead setting,issue,negative,positive,neutral,neutral,positive,positive
485319746,"Thanks. I'm not sure I understand this question correctly:

> What is the specific meaning of each component of the whole name?

Do you now have an answer already? The components comes from scopes and their meaning come from users who create the scopes, except for variable scopes like ""tower0"" which are created by tensorpack and are supposed to be respected.",thanks sure understand question correctly specific meaning component whole name answer already come meaning come create except variable like tower supposed,issue,positive,positive,positive,positive,positive,positive
485318851,"Thanks for the quick reply. 

It is mentioned in 
```
build_graph, train.py:88
```
in `compute_grad_from_inputs`.
I get the name through pycharm debugger through, not from tfdbg.
",thanks quick reply get name,issue,negative,positive,positive,positive,positive,positive
485318349,"> Is there any elaborate on the difference between layer_register and under_name_scope?

As far as names are concerned, `layer_register` opens a variable scope and `under_name_scope` opens a name scope.",elaborate difference far concerned variable scope name scope,issue,negative,positive,positive,positive,positive,positive
485317712,"> What is the specific meaning of each component of the whole name? 

e.g., `tower0`, `rpn_losses`, `label_loss` in your example. Where is this mentioned?

> Is this a violation of the naming convention?

No because this is a Tensor, not a trainable variable, so it can have whatever name.
If it is a trainable variable, this is a valid name iff `tf.get_variable_scope()` starts with 'tower0'`.",specific meaning component whole name tower example violation naming convention tensor trainable variable whatever name trainable variable valid name,issue,negative,positive,neutral,neutral,positive,positive
485317343,"Hi,

Is there any elaborate on the difference between layer_register and under_name_scope?

Also,
In the tutorial on tower function, the name of a variable is like 
> The name of any trainable variables created in the function must be like “variable_scope_name/custom/scopes/name”. Therefore, the name of any trainable variables must:
Not depend on name_scope’s name.
Not depend on some tensor’s name.
Not use the same variable_scope’s name twice.
Tensorpack layers create variables based on the name given to the layer (i.e., Conv2D('name', x)). So the name of the layer needs to follow the above rules as well.

What is the specific meaning of each component of the whole name? I think the meaning of ""name"" is clear, ""variable_scope_name"" refers to towers? In FasterRCNN, we get names like 
```
Tensor(""tower0/rpn_losses/label_loss:0"", shape=(), dtype=float32, device=/device:GPU:0)
```
Is this a violation of the naming convention? 

",hi elaborate difference also tutorial tower function name variable like name trainable function must like therefore name trainable must depend name depend tensor name use name twice create based name given layer name layer need follow well specific meaning component whole name think meaning name clear get like tensor violation naming convention,issue,positive,positive,positive,positive,positive,positive
485130358,"I rename it to Caltech101Silhouettes and split it in train, val and test as in BSDS500.
I also undo adding scipy in requirements.",rename split train test also undo,issue,negative,neutral,neutral,neutral,neutral,neutral
485054567,I suggest to either upgrade NCCL or upgrade TensorFlow (newer versions of TF will package NCCL) to see if that helps.,suggest either upgrade upgrade package see,issue,negative,neutral,neutral,neutral,neutral,neutral
485051348,"@ppwwyyxx , Yes, when I changed to mode='cpu', it works... Thanks so much! So what I need to do is to re-install NCCL? ",yes work thanks much need,issue,positive,positive,positive,positive,positive,positive
485039928,"<del> Please include your logs as requested in the issue template. </del> (didn't see that PDF contains logs)
You can change number of dataflow workers with `cfg.DATA.NUM_WORKERS` to reduce memory usage.",please include issue template see change number reduce memory usage,issue,negative,neutral,neutral,neutral,neutral,neutral
485038883,"If you want to know whether `train_op` is depended on by some given ops, use [dependency_of_fetches](https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.dependency.dependency_of_fetches)

For semantics of callback methods see https://tensorpack.readthedocs.io/tutorial/extend/callback.html

I'm not sure what it is you're asking. If the above does not help you, please clarify how do you define ""when training""  or ""training is going on"". Technically, you cannot do anything when `train_op` is being executed because Python needs to wait for it to finish.",want know whether given use semantics see sure help please clarify define training training going technically anything executed python need wait finish,issue,positive,positive,positive,positive,positive,positive
484933704,"Could yoy try commenting out `PrefetchData`? This may show you the correct error messages.

Also, what you need may be `MapData`.",could yoy try may show correct error also need may,issue,negative,neutral,neutral,neutral,neutral,neutral
484840873,"Hi @tmquan ,
Thanks for the script of ElasticDeformation.
While implementing it, I find your code may just suit for the square image, it works for me after changed the following lines:
```
DU = cv2.resize(du, (shape[-2], shape[-2])) 
DV = cv2.resize(dv, (shape[-2], shape[-2])) 
X, Y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))
```
to
```
DU = cv2.resize(du, (shape[1], shape[0])) 
DV = cv2.resize(dv, (shape[1], shape[0])) 
X, Y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))
```
",hi thanks script find code may suit square image work following shape shape shape shape shape shape shape shape shape shape shape shape,issue,negative,positive,neutral,neutral,positive,positive
484796627,"It prints when data is taken from the dataflow.
It does not print anything when you define the dataflow.
Now the docs are updated to clarify this.

See more at https://tensorpack.readthedocs.io/tutorial/dataflow.html",data taken print anything define clarify see,issue,negative,neutral,neutral,neutral,neutral,neutral
484791856,"This is answered in the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/feature-requests.md).

In brief, if you can train one network, then you can train many networks, then you can do NAS.",issue template brief train one network train many,issue,negative,positive,positive,positive,positive,positive
484785495,"@PatWie Hi, Thanks for reply and your script works.
@ppwwyyxx Hi, But I find the example in the latest docs shows like below:
> class dataflow.PrintData(ds, num=1, name=None, max_depth=3, max_list=3)[source]
> Bases: tensorpack.dataflow.base.ProxyDataFlow
> Example
> To enable this debugging output, you should place it somewhere in your dataflow like
> 
```
def __iter__():
    ds = SomeDataSource('path/to/lmdb')
    ds = SomeInscrutableMappings(ds)
    ds = PrintData(ds, num=2, max_list=2)
    return ds
ds = __iter__()
```
And it will return 
![image](https://user-images.githubusercontent.com/12379916/56411350-025f9f80-62b3-11e9-83e7-e11741d1abd2.png)

Instead of 
```
[0110 09:22:21 @common.py:589] DataFlow Info:
datapoint 0<2 with 4 components consists of
   0: float with value 0.0816501893251
   1: ndarray:int32 of shape (64,) in range [0, 10]
   2: ndarray:float32 of shape (64, 64) in range [-1.2248, 1.2177]
   3: list of len 50
      0: ndarray:int32 of shape (64, 64) in range [-128, 80]
      1: ndarray:float32 of shape (64, 64) in range [0.8400, 0.6845]
      ...
datapoint 1<2 with 4 components consists of
   0: float with value 5.88252075399
   1: ndarray:int32 of shape (64,) in range [0, 10]
   2: ndarray:float32 of shape (64, 64) with range [-0.9011, 0.8491]
   3: list of len 50
      0: ndarray:int32 of shape (64, 64) in range [-70, 50]
      1: ndarray:float32 of shape (64, 64) in range [0.7400, 0.3545]
      ...
```",hi thanks reply script work hi find example latest like class source base example enable output place somewhere like return return image instead float value shape range float shape range list shape range float shape range float value shape range float shape range list shape range float shape range,issue,positive,negative,neutral,neutral,negative,negative
484773858,"Thanks. It does sound like NCCL was stuck in your environment (it works fine in mine, though). You can verify this by using `mode='cpu'` in `SyncMultiGPUTrainerReplicated()` to see if that works.",thanks sound like stuck environment work fine mine though verify see work,issue,positive,positive,positive,positive,positive,positive
484737734,"@ppwwyyxx , Thanks for your reply. 

For your reference, yes, I haven't made modifications.

I have waited for about one hour and no training happens. I am not sure whether it is normal.

When I use ParameterServer, about 5mins later, it shows the progress, e.g., 5% percent, then finishes the first epoch in 30 mins and start next epoch. 

I thought is it the NCCL's problem? Could u provide some suggestions? Thanks!
",thanks reply reference yes made one hour training sure whether normal use later progress percent first epoch start next epoch thought problem could provide thanks,issue,positive,positive,positive,positive,positive,positive
484521502,"Thanks. The logs look normal to me. 
Can you confirm that no modifications to code were made?

How do you know the training was stuck? i.e., when the training was not stuck (when you use the ParamterServer trainer), what are the differences you observe?",thanks look normal confirm code made know training stuck training stuck use trainer observe,issue,negative,positive,positive,positive,positive,positive
484507480,@ppwwyyxx ， I have created another issue followed the template you sent.,another issue template sent,issue,negative,neutral,neutral,neutral,neutral,neutral
484486718,"@ppwwyyxx in your NOTES you also mentioned about op written for light-head rcnn:
https://github.com/zengarden/light_head_rcnn/tree/master/lib/lib_kernel/lib_roi_align

Are you planning to add it in near future? And in general are you planning to add their light-head?
",also written add near future general add,issue,negative,positive,neutral,neutral,positive,positive
484479452,"See rule 3: https://tensorpack.readthedocs.io/tutorial/trainer.html#rules-of-tower-function. In short, you should not let name of the variables (and also name of layers) depend on the name scope.",see rule short let name also name depend name scope,issue,negative,neutral,neutral,neutral,neutral,neutral
484418395,"Hi Yuxin Wu,

Sorry for the approximation directly by email but my GitHub account has been flagged, don't know why...

I posted an issue but as the account is flagged is not visible for the public to see so I though it may be a good idea to send it to you through this email, this is the issue:

Basically the problem is that when trying to train the network (Faster RCNN) with Mapillary dataset MemoryError come up. I am using 8 GPU Nvidia V100. I am a little bit desperate and tired as  I am not able to train this Dataset with any instance segmentation model (last was Mask-RCNN from matterplot which uses Resnet101 pre-trained with COCO dataset ) always getting MemoryError, even when I train the model with few pictures. I suspects I am doing something wrong.

Anyway here is what I get when trying to train your network with Mapillary Dataset (5 images train, 2 images val). Below your template and attached the log I got.

MemoryError when train FasterRCNN example with Mapillary Dataset, 4 x NVIDIA Tesla T4 #


If you're asking about an unexpected problem which you do not know the root cause,
use this template. PLEASE DO NOT DELETE THIS TEMPLATE, FILL IT:

If you already know the root cause to your problem,
feel free to delete everything in this template.

1. What you did:

TRAIN FasterRCNN network with Mapillary Dataset


(1) If you're using examples, what's the command you run:
python3 train.py --config MODE_MASK=True MODE_FPN=True 'DATA.BASEDIR=/home/federicolondon2019/tensorpack/COCO/DIR' 'BACKBONE.WEIGHTS
=/home/federicolondon2019/tensorpack/models/ImageNet-R50-AlignPadding.npz'


(2) If you're using examples, have you made any changes to the examples? Paste git status; git diff here:
I changed DATASET.PY and CONFIG.PY


DATASET.PY: (from line 17 to 32)

Original:

class COCODetection:
# handle the weird (but standard) split of train and val
_INSTANCE_TO_BASEDIR = {
'valminusminival2014': 'val2014',
'minival2014': 'val2014',
}

COCO_id_to_category_id = {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30, 35: 31, 36: 32, 37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40, 46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48, 54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56, 62: 57, 63: 58, 64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64, 74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72, 82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80} # noqa """""" Mapping from the incontinuous COCO category id to an id in [1, #category] For your own dataset, this should usually be an identity mapping. """""" # 80 names for COCO class_names = [ ""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"", ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"", ""cat"", ""dog"", ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"", ""umbrella"", ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"", ""sports ball"", ""kite"", ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"", ""tennis racket"", ""bottle"", ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"", ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"", ""chair"", ""couch"", ""potted plant"", ""bed"", ""dining table"", ""toilet"", ""tv"", ""laptop"", ""mouse"", ""remote"", ""keyboard"", ""cell phone"", ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"", ""vase"", ""scissors"", ""teddy bear"", ""hair drier"", ""toothbrush""]
Now:
class COCODetection:
# handle the weird (but standard) split of train and val
_INSTANCE_TO_BASEDIR = {
'valminusminival2014': 'val2017',
'minival2014': 'val2017',
}
COCO_id_to_category_id = **{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38}** # noqa """""" Mapping from the incontinuous COCO category id to an id in [1, #category] For your own dataset, this should usually be an identity mapping. """""" # 80 names for COCO class_names = ['BG', 'Bird', 'Ground_Animal', 'Crosswalk_Plain', 'Person', 'Bicyclist', 'Motorcyclist', 'Other_Rider', 'Lane_Marking_-_Crosswalk', 'Banner', 'Bench', 'Bike_Rack', 'Billboard', 'Catch_Basin', 'CCTV_Camera', 'Fire_Hydrant', 'Junction_Box', 'Mailbox', 'Manhole', 'Phone_Booth', 'Street_Light', 'Pole', 'Traffic_Sign_Frame', 'Utility_Pole', 'Traffic_Light', 'Traffic_Sign_(Back)', 'Traffic_Sign_(Front)', 'Trash_Can', 'Bicycle', 'Boat', 'Bus', 'Car', 'Caravan', 'Motorcycle', 'Other_Vehicle', 'Trailer', 'Truck', 'Wheeled_Slow']

CONFIG.PY (from 82 to 96)

Original:

dataset -----------------------

_C.DATA.BASEDIR = '/path/to/your/DATA/DIR'

All TRAIN dataset will be concatenated for training.

_C.DATA.TRAIN = ('train2014', 'valminusminival2014') # i.e. trainval35k, AKA train2017

Each VAL dataset will be evaluated separately (instead of concatenated)

_C.DATA.VAL = ('minival2014', ) # AKA val2017

This two config will be populated later by the dataset loader:

_C.DATA.NUM_CATEGORY = 0 # without the background class (e.g., 80 for COCO)
_C.DATA.CLASS_NAMES = [] # NUM_CLASS (NUM_CATEGORY+1) strings, the first is ""BG"".

whether the coordinates in the annotations are absolute pixel values, or a relative value in [0, 1]

_C.DATA.ABSOLUTE_COORD = True
_C.DATA.NUM_WORKERS = 5 # number of data loading workers

basemodel ----------------------

_C.BACKBONE.WEIGHTS = '' # /path/to/weights.npz





Now:

dataset -----------------------

_C.DATA.BASEDIR = '/home/federicolondon2019/tensorpack/COCO/DIR'

All TRAIN dataset will be concatenated for training.

_C.DATA.TRAIN = ('train2017') # i.e. trainval35k, AKA train2017

Each VAL dataset will be evaluated separately (instead of concatenated)

_C.DATA.VAL = ('val2017') # AKA val2017

This two config will be populated later by the dataset loader:

C.DATA.NUM_CATEGORY = 37 # without the background class (e.g., 80 for COCO)
C.DATA.CLASS_NAMES = **['BG', 'Bird', 'Ground_Animal', 'Crosswalk_Plain', 'Person', 'Bicyclist', 'Motorcyclist', 'Other_Rider', 'Lane_Marking-Crosswalk', 'Banner', 'Bench', 'Bike_Rack', 'Billboard', 'Catch_Basin', 'CCTV_Camera', 'Fire_Hydrant', 'Junction_Box', 'Mailbox', 'Manhole', 'Phone_Booth', 'Street_Light', 'Pole', 'Traffic_Sign_Frame', 'Utility_Pole', 'Traffic_Light', 'Traffic_Sign(Back)', 'Traffic_Sign(Front)', 'Trash_Can', 'Bicycle', 'Boat', 'Bus', 'Car', 'Caravan', 'Motorcycle', 'Other_Vehicle', 'Trailer', 'Truck', 'Wheeled_Slow'**] # NUM_CLASS (NUM_CATEGORY+1) strings, the first is ""BG"".

whether the coordinates in the annotations are absolute pixel values, or a relative value in [0, 1]

_C.DATA.ABSOLUTE_COORD = True
_C.DATA.NUM_WORKERS = 5 # number of data loading workers

basemodel ----------------------

_C.BACKBONE.WEIGHTS = **'/home/federicolondon2019/tensorpack/models/ImageNet-R50-AlignPadding.npz' # /path/to/weights.npz


**

(3) If not using examples, tell us what you did:

It's always better to copy-paste what you did than to describe them.

Please try to provide enough information to let other reproduce your issues.
Without reproducing the issue, we may not be able to investigate it.


2. What you observed:

(1) Include the ENTIRE logs here:
I get a MemoryError when running train.py in Google Cloud with 4 Tesla T4.  SEE PDF ATTACHED.



It's always better to copy-paste what you observed instead of describing them.

It's always better to paste as much as possible, although sometimes a partial log is OK.

Tensorpack typically saves stdout to its training log.
If stderr is relevant, you can run a command with CMD 2>&1 | tee logs.txt
to save both stdout and stderr to one file.

(2) Other observations, if any:
For example, CPU/GPU utilization, output images, tensorboard curves, if relevant to your issue.
I only included 5 images to train and 2 images to val

3. What you expected, if not obvious.

I expected to get a log

If you expect higher speed, please read
http://tensorpack.readthedocs.io/tutorial/performance-tuning.html
before posting.

If you expect certain accuracy, only in one of the two conditions can we help with it:
(1) You're unable to reproduce the accuracy documented in tensorpack examples.
(2) It appears to be a tensorpack bug.

Otherwise, how to train a model to certain accuracy is a machine learning question.
We do not answer machine learning questions and it is your responsibility to
figure out how to make your models more accurate.

4. Your environment:

  *   Python version: 3.5.3
  *   TF version: 1.13.1
  *   Tensorpack version: v0.9.4
You can install Tensorpack master by pip install -U git+https://github.com/ppwwyyxx/tensorpack.git
and see if your issue is already solved.
  *   If you're not using tensorpack under a normal command line shell (e.g.,
using an IDE or jupyter notebook), please retry under a normal command line shell.
  *   Hardware information, e.g. number of GPUs used.

You may often want to provide extra information related to your issue, but
at the minimum please try to provide the above information accurately to save effort in the investigation.

Attached the entire output when running.

Thanks


Thanks Yuxin
Alberto.
________________________________
De: Yuxin Wu <notifications@github.com>
Enviado: lunes, 15 de abril de 2019 16:20
Para: tensorpack/tensorpack
Cc: AlbertoMCS; Author
Asunto: Re: [tensorpack/tensorpack] ""ValueError: not enough values to unpack (expected 2, got 1)"" when running train.py in Mapillary Dataset. (#1146)


""DATA.BASEDIR=/content/drive/Colab Notebooks/otro/tensorpack/examples/FasterRCNN/COCO/DIR""

and similarly for the weights

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorpack%2Ftensorpack%2Fissues%2F1146%23issuecomment-483271409&data=02%7C01%7C%7C9d64c20c368f4649d7b208d6c1ad838b%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636909348305991542&sdata=AEfUS9%2FNZ5g0SENAlWeoEHU%2BQZbc8VsvAjZ36qgQVIY%3D&reserved=0>, or mute the thread<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAD77IGPTZVZ6LYOQF2J75XLPQSFK3ANCNFSM4HF7RAMA&data=02%7C01%7C%7C9d64c20c368f4649d7b208d6c1ad838b%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636909348306001553&sdata=jyzU2PboLHDV6Zupy4h1X9UsjTJ75VRFGsGyZbfiqc4%3D&reserved=0>.
",hi sorry approximation directly account know posted issue account visible public see though may good idea send issue basically problem trying train network faster come little bit desperate tired able train instance segmentation model last coco always getting even train model something wrong anyway get trying train network train template attached log got train example unexpected problem know root cause use template please delete template fill already know root cause problem feel free delete everything template train network command run python made paste git status git line original class handle weird standard split train incontinuous coco category id id category usually identity coco person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe umbrella handbag tie suitcase sport ball kite baseball bat baseball glove surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza cake chair couch potted plant bed dining table toilet mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors bear hair drier toothbrush class handle weird standard split train incontinuous coco category id id category usually identity coco back front original train training aka train separately instead aka two later loader without background class coco first whether absolute relative value true number data loading train training aka train separately instead aka two later loader without background class coco back front first whether absolute relative value true number data loading tell u always better describe please try provide enough information let reproduce without issue may able investigate include entire get running cloud see attached always better instead always better paste much possible although sometimes partial log typically training log relevant run command tee save one file example utilization output relevant issue included train obvious get log expect higher speed please read posting expect certain accuracy one two help unable reproduce accuracy bug otherwise train model certain accuracy machine learning question answer machine learning responsibility figure make accurate environment python version version version install master pip install see issue already normal command line shell ide notebook please retry normal command line shell hardware information number used may often want provide extra information related issue minimum please try provide information accurately save effort investigation attached entire output running thanks thanks de lunes de de para author enough unpack got running similarly thread reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
484411296,"> All tensorpack examples that have BatchNorm use BatchNorm correctly in testing phase.
> 
> For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

@ppwwyyxx Thanks for the repaid reply. I have checked my code again, and find that this issue is caused by the incorrect usage of tf.name_scope and tf.variable_scope. In the training phase, the ""tower0"" ""tower1""...seem to affect the variable scopes. I modify the code as follows and it works:

    scope = scope[scope.find('/') + 1:]
    with tf.name_scope(scope) as sc:
        cur_bnscope = sc  # different bn name within different transitions
        if is_training:
            cur_bnscope = cur_bnscope[cur_bnscope.find('/') + 1:]
	    with tf.name_scope(cur_bnscope):
		# print('bnscope', bnscope)
		preact = tf.nn.relu(BatchNorm(bnscope, inputs))
		# preact = tf.nn.relu(slim.batch_norm(inputs, fused=True))
		conv = slim.conv2d(preact, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)
		if dropout_p != 0.0:
		    conv = slim.dropout(conv, keep_prob=(1.0 - dropout_p))
		return conv",use correctly testing phase anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected thanks reply checked code find issue incorrect usage training phase tower tower seem affect variable modify code work scope scope scope different name within different print preact preact preact return,issue,positive,positive,positive,positive,positive,positive
484361810,"@ppwwyyxx , I have installed the latest tensorpack, with Tensorflow 1.9.0. I want to run the DoReFaNet on ImageNet using Tensorpack provided. has same issue. I have tried to change Tensorflow version, but still stuck at Prefilling staging... Can you provide some suggestions?",latest want run provided issue tried change version still stuck staging provide,issue,negative,positive,positive,positive,positive,positive
484354675,"All tensorpack examples that have BatchNorm use BatchNorm correctly in testing phase.

For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",use correctly testing phase anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected,issue,positive,positive,positive,positive,positive,positive
484119534,"Yes I find it. Thanks for the quick reply!
> We currently only support single image per GPU.
",yes find thanks quick reply currently support single image per,issue,positive,positive,positive,positive,positive,positive
484078012,"`group1/block1/conv1/bn/beta` does not have tower prefix because it corresponds to variables. This is by design in tower0, so they will have a clean name when saved as checkpoints.

Tower function is responsible for computing the gradients. Therefore `apply_gradients*` won't have the tower prefix because they are not part of the tower function.",tower prefix design tower clean name saved tower function responsible therefore wo tower prefix part tower function,issue,positive,positive,positive,positive,positive,positive
484009204,"The `PrintData` has been changed (just have a look at the date).

Something like

```python
from tensorpack import *

seq = FakeData([(10, 10, 3)], random=False)
seq = RepeatedDataPoint(seq, nr=5)
seq = AugmentImageComponents(seq, [imgaug.RandomCrop((4, 4))], index=[0], copy=False)
seq = PrintData(seq)
seq.reset_state()

next(seq.__iter__())

# or just do it yourself

seq.reset_state()
for dp in seq:
  print(dp[0].shape, dp[0].min(), dp[0].max())
```

will work. Not sure, if `PrintData` still supports the `num` parameter. I guess not.",look date something like python import next print work sure still parameter guess,issue,positive,positive,positive,positive,positive,positive
483910190,The batch size is 1 per GPU as said in NOTES.md and cannot be changed.,batch size per said,issue,negative,neutral,neutral,neutral,neutral,neutral
483909873,"Hi, 
I mean I can find it in the commented line like:
`# LR_SCHEDULE means equivalent steps when the total batch size is 8.
`
So I assume the default is 8, and I 'm wondering if the config 'batch_size' can be changed ?",hi mean find line like equivalent total batch size assume default wondering,issue,negative,negative,negative,negative,negative,negative
483805403,@zxpeter You can either randomly choose between different augmentations or add a field in the extra `roidbs` to indicate whether it needs flip or rotation and read the extra field out in `preprocess`.,either randomly choose different add field extra indicate whether need flip rotation read extra field,issue,negative,negative,negative,negative,negative,negative
483804284,"> I'm also using FRCNN example and I noticed default batch_size=8 is mentioned in config.py

No, there is no default batch_size=8 in config.py.

You can change any config in the config file.",also example default default change file,issue,negative,neutral,neutral,neutral,neutral,neutral
483771698,"
Hi，
I'm also using FRCNN example and I'm changing get_train_dataflow() in data.py to get augmented training data for my own dataset (400). And I want to use filp and rotation to increase data num to 3*400=1200(origin, filp, rotation).So instead of putting shuffled dataflow with random filp or rotation in, I want to use 1200 as initial input for each epoch.


def get_train_dataflow():
    
    roidbs = DetectionDataset().load_training_roidbs()
    
    print_class_histogram(roidbs)
    num = len(roidbs)

    roidbs = list(filter(lambda img: len(img['boxes'][img['is_crowd'] == 0]) > 0, roidbs))
    logger.info(""Filtered {} images which contain no non-crowd groudtruth boxes. Total #images for training: {}"".format(
        num - len(roidbs), len(roidbs)))

    ds = DataFromList(roidbs, shuffle=True)
    ds = RepeatedData(ds, nr=3)

    aug = imgaug.AugmentorList(
            [CustomResize(cfg.PREPROC.TRAIN_SHORT_EDGE_SIZE, cfg.PREPROC.MAX_SIZE),
             imgaug.Flip(horiz=True)])
    aug_2 = imgaug.AugmentorList(
            [CustomResize(cfg.PREPROC.TRAIN_SHORT_EDGE_SIZE, cfg.PREPROC.MAX_SIZE),
             imgaug.Rotation(90)])

    def preprocess(roidb):
       ... ...

    if cfg.TRAINER == 'horovod':
        ds = MultiThreadMapData(ds, 5, preprocess)
    else:
        # ds = MultiProcessMapDataZMQ(ds, 10, preprocess)
        ds = MapData(ds, preprocess)

    return ds

My plan is use RepeatedData(ds, nr=3) to get duplicated dataflow and  then flip the second part and rotate the third part, but can't get those parts. I think it may have better solution for my problem, any help will be appreciated.
",also example get augmented training data want use rotation increase data origin rotation instead random rotation want use initial input epoch list filter lambda contain total training else return plan use get flip second part rotate third part ca get think may better solution problem help,issue,positive,neutral,neutral,neutral,neutral,neutral
483767166,"Thanks! I'm training now!

I made the following script to truncate the output layer. 
https://gist.github.com/luxedo/73de2cc4dd69d6ab88aefd0aefabe9a8



> You do not want to load the weights that have 80 COCO classes so you need to remove them from the checkpoint (with `np.load` and `np.savez`). See more at https://tensorpack.readthedocs.io/tutorial/save-load.html

",thanks training made following script truncate output layer want load coco class need remove see,issue,negative,positive,neutral,neutral,positive,positive
483764082,"Hi,
How can I get that output?
When I excute that script in terminal it return nothing, when use jupter notebook it return <xxxx obejct>.
I'm using python3.",hi get output script terminal return nothing use notebook return python,issue,negative,neutral,neutral,neutral,neutral,neutral
483761955,"Hi, 
I'm also using FRCNN example and I noticed default batch_size=8 is mentioned in config.py.
And I didn't find the place to change batch_size param in config or other place, is that can be changed?
",hi also example default find place change param place,issue,negative,neutral,neutral,neutral,neutral,neutral
483683478,You do not want to load the weights that have 80 COCO classes so you need to remove them from the checkpoint (with `np.load` and `np.savez`). See more at https://tensorpack.readthedocs.io/tutorial/save-load.html,want load coco class need remove see,issue,negative,neutral,neutral,neutral,neutral,neutral
483679333,OK tested installing a wheel from the PR on both Linux and Mac and it works as expected.,tested wheel mac work,issue,negative,neutral,neutral,neutral,neutral,neutral
483678204,"I'm trying to train in my own data that have the COCO format but only two classes. I'm getting the following error:

`ValueError: Trying to load a tensor of shape (80,) into the variable 'maskrcnn/conv/b' whose shape is (2,).`",trying train data coco format two class getting following error trying load tensor shape variable whose shape,issue,negative,neutral,neutral,neutral,neutral,neutral
483677271,"Actually it has to be `""linux"" in sys_platform`, since the actual value is `""linux2""`.",actually since actual value,issue,negative,neutral,neutral,neutral,neutral,neutral
483670014,"Please include these information as requested in the issue template:


(1) **If you're using examples, what's the command you run:**

(2) **If you're using examples, have you made any changes to the examples? Paste `git status; git diff` here:**

(3) **If not using examples, tell us what you did:**

  It's always better to copy-paste what you did than to describe them.

  Please try to provide enough information to let other __reproduce__ your issues. 
  Without reproducing the issue, we may not be able to investigate it.

### 2. What you observed:

(1) **Include the ENTIRE logs here:**

It's always better to copy-paste what you observed instead of describing them.",please include information issue template command run made paste git status git tell u always better describe please try provide enough information let without issue may able investigate include entire always better instead,issue,positive,positive,positive,positive,positive,positive
483661613,"```diff
diff --git i/setup.py w/setup.py
index 25818221..7dcaa4a6 100644
--- i/setup.py
+++ w/setup.py
@@ -56,8 +56,8 @@ setup(
     ],
     tests_require=['flake8', 'scikit-image'],
     extras_require={
-        'all': ['pillow', 'scipy', 'h5py', 'lmdb>=0.92', 'matplotlib', 'scikit-learn'] +
-               ['python-prctl'] if platform.system() == 'Linux' else [],
+        'all': ['pillow', 'scipy', 'h5py', 'lmdb>=0.92', 'matplotlib', 'scikit-learn'],
+        'all: ""Linux"" in sys_platform': ['python-prctl'],
         'all: python_version < ""3.0""': ['tornado'],
     },
 )
```
you mean this? Could you check whether this works on mac?",git index setup else mean could check whether work mac,issue,negative,negative,negative,negative,negative,negative
483590647,"> In `train.SyncMultiGPUTrainerReplicated(gpus, average=True, mode=None, use_nccl=None)` , should I set `mode='nccl'` or `use_nccl=True` ? I am a little confused.

@ppwwyyxx @s7ev3n So what's the final options for SyncMultiGPUTrainerReplicated ? I have tried to set it as:
tensorpack.utils.gpu import get_nr_gpu
trainer = SyncMultiGPUTrainerReplicated(max(get_nr_gpu(), 1), mode=""nccl"") or
trainer = SyncMultiGPUTrainerReplicated(get_nr_gpu(), mode=""nccl"")

and the BN is set as @ppwwyyxx suggested, but it always failed to conduct synBN. Is that the issues from NVIDIA nccl2 library?
What's the best way to use nccl?  Should I use horovod instead?",set little confused final tried set import trainer trainer set always conduct library best way use use instead,issue,negative,positive,positive,positive,positive,positive
483283111,It does not use specialized CUDA  ops for roialign. This is still true.,use specialized still true,issue,negative,positive,positive,positive,positive,positive
483261358,"Hi, 

Sorry I think I don't understand well. What do I have to quote exactly? , what do you mean about ""command line arguments""?

Thanks a lot.
Alberto.",hi sorry think understand well quote exactly mean command line thanks lot,issue,positive,negative,negative,negative,negative,negative
483261280,"Ok, I'll make a PR changing point 4 of section **Speed** in [NOTES.md](https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md) to:

> This implementation does not use specialized CUDA ops (e.g. AffineChannel). Therefore it might be slower than other highly-optimized implementations. 

unless you have something against it.",make point section speed implementation use specialized therefore might unless something,issue,negative,neutral,neutral,neutral,neutral,neutral
483257580,"> Colab Notebooks

There is a space in between and therefore they end up being two arguments.
You need to quote your command line arguments.",space therefore end two need quote command line,issue,negative,neutral,neutral,neutral,neutral,neutral
483257035,Thanks! That location would be great.,thanks location would great,issue,positive,positive,positive,positive,positive,positive
483158589,"as @ppwwyyxx said, this may be an issue related to Phil's project. Try this version instead and let us know if it works:

`pip3 install git+https://github.com/waleedka/coco.git#subdirectory=PythonAPI`

You're training on COCO, right?",said may issue related project try version instead let u know work pip install training coco right,issue,negative,positive,positive,positive,positive,positive
483149384,"Yes, I think so too, thank you very much for the help!",yes think thank much help,issue,positive,positive,positive,positive,positive,positive
483122749,"It does not seem related to tensorpack, but I'm not sure what the warning means.

But if things are working for you I guess it doesn't matter.",seem related sure warning working guess matter,issue,negative,positive,positive,positive,positive,positive
483100789,I believe this is an issue of the project https://github.com/philferriere/cocoapi you're using and unrelated to tensorpack.,believe issue project unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
483099691,"To future users: the best way to use `tfdbg` is to add this callback to your training callbacks:
```
callbacks=[
    ...,
    TFLocalCLIDebugHook()
]
```
See docs at: https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.TFLocalCLIDebugHook

The tfdbg session wrapper will not work with tensorpack callbacks - this is a tensorflow bug. Don't use the session wrapper.",future best way use add training see session wrapper work bug use session wrapper,issue,positive,positive,positive,positive,positive,positive
483098300,"After debugger started, it shows:
```
Session.run() call #1:
Fetch(es): global_step:0
Feed dict: (Empty)
```
I then enter `run` and the result is:

t(ms)   |   Size(B)   |   Op type    |   Tensor name   |
---------|---------------|----------------|----------------------|
|[0.000]     |190          |VariableV2    |global_step:0    |

Then I enter `run` again and the error occurs.

On the other hand, the callback 

> callbacks=[
            HookToCallback(tf_debug.LocalCLIDebugHook())

 works, however, if running the debugger with the nan filter on:
```
run -f has_inf_or_nan
```
then for each step, there is a warning message basically says the same thing as the original problem, but it use the graph on the GPU instead.
```
WARNING:tensorflow:Failed to load partition graphs for device /job:localhost/replica:0/task:0/device:GPU:0 from disk. As a fallback, the client graphs will be used. This may cause mismatches in device names.
```
Maybe it is unrelated to tensorpack? 
",call fetch e feed empty enter run result size type tensor name enter run error hand work however running nan filter run step warning message basically thing original problem use graph instead warning load partition device disk fallback client used may cause device maybe unrelated,issue,negative,positive,positive,positive,positive,positive
483096226,"What did you enter in your debugger?

Could you launch debugger with this:
```python
 callbacks=[
            HookToCallback(tf_debug.LocalCLIDebugHook())
```
instead of the session wrapper?",enter could launch python instead session wrapper,issue,negative,neutral,neutral,neutral,neutral,neutral
483090511,"> I'm not sure I understand your question.
> 
> If you are running a pre-trained FasterRCNN model from tensorpack model zoo and expect to see more boxes, you can adjust the threshold in `cfg.TEST.RESULT_SCORE_THRESH_VIS`.

Thank you! I will have a try and come back later!",sure understand question running model model zoo expect see adjust threshold thank try come back later,issue,positive,positive,positive,positive,positive,positive
483089603,"I'm not sure I understand your question. 

If you are running a pre-trained FasterRCNN model from tensorpack model zoo and expect to see more boxes, you can adjust the threshold in `cfg.TEST.RESULT_SCORE_THRESH_VIS`.",sure understand question running model model zoo expect see adjust threshold,issue,negative,positive,positive,positive,positive,positive
482852857,"Thanks for your support.
If your goal is to use tensorpack, `tensorpack/` and `examples/` are enough.
`scripts/` contains some small scripts to work with checkpoints. They may be useful but they are quite simple so you may write your own as well.",thanks support goal use enough small work may useful quite simple may write well,issue,positive,positive,neutral,neutral,positive,positive
482821058,"Hi yx,

My question is similar and I'm using examples/FasterRCNN with my own dataset (600 images).
The model easily overfitting at around 50 epoch (100 steps per epoch).
![image](https://user-images.githubusercontent.com/12379916/56081626-fce10000-5e41-11e9-9cb7-1ab7e7ee7e00.png)

And frcnn loss keep going down.
![image](https://user-images.githubusercontent.com/12379916/56081712-f30bcc80-5e42-11e9-9aec-8d4778628c47.png)

I'm wondering if we got any solutions like cross validation/drop out/early stop in tensorpack for FRCNN? 
Can I assume that we don't need those methods for large dataset like COCO?


",hi question similar model easily around epoch per epoch image loss keep going image wondering got like cross stop assume need large like coco,issue,negative,positive,positive,positive,positive,positive
482699551,">  InferenceRunner(    # run inference(for validation) after every epoch
            val_dataflow,   # the DataFlow instance used for validation
            ScalarStats(    # produce `val_accuracy` and `val_cross_entropy_loss`
                ['accuracy'], prefix='val'))
    ]

This is correct, except that there is no tensor named ""accuracy"" in the graph

Also note that the other callback `EvalCallback` also runs evaluation. So you'll run evaluation twice, with different metrics. You can also modify `EvalCallback` to add your own metrics.",run inference validation every epoch instance used validation produce correct except tensor accuracy graph also note also evaluation run evaluation twice different metric also modify add metric,issue,negative,neutral,neutral,neutral,neutral,neutral
482692794,"I do not understand your question, but this will do multi-scale training:
```python
def resize(x):
    size = (np.random.randint(1, 6) * PATCH_SIZE, np.random.randint(1, 6) * PATCH_SIZE)
    x = [cv2.resize(a, size) for a in x]
    return np.asarray(x)

ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain, use_list=True)
ds = MapDataComponent(ds, resize, 0)
```",understand question training python resize size size return resize,issue,negative,neutral,neutral,neutral,neutral,neutral
482686628,"Thanks, I still have one thing I don't understand.

I am trying to do this:
**for ii in range(0, len(ds))**:
        print(ii)
        for i in range(1, BATCH_SIZE):#dp[0] [4,256,512,3] 
            **ds** = MapDataComponent(**ds**, lambda x: cv2.resize(x[i,:,:,:], (2 * PATCH_SIZE, 2 * 2 * PATCH_SIZE)), **ii**)
   
which does not work, I assume I pass the ds with index ii so that the x in lambda should be the image batch [batchnum, height, width, channel], but clearly it is not, since cv2.resize cannot process it.  But your MapDataComponent clearly states that ""input DataFlow which produces either list or dict."" and ""index (int or str) – index or key of the component."" 

But if i did sth like this:
ii=0
**for dp in ds.get_data()**:
        print(ii)
        for i in range(1, BATCH_SIZE):#dp[0] [4,256,512,3] 
            **dp[0]**= MapDataComponent(**dp[0]**, lambda x: cv2.resize(x[i,:,:,:], (2 * PATCH_SIZE, 2 * 2 * PATCH_SIZE)), **ii**)
        ii=ii+1
the code can run, but after i printing out the result, the dataflow size did not change at all, the image size is still the PATCH_SIZE.

I don't know what I should input into the MapDataComponent now.
     ",thanks still one thing understand trying range print range lambda work assume pas index lambda image batch height width channel clearly since process clearly input either list index index key component like print range lambda code run printing result size change image size still know input,issue,positive,positive,positive,positive,positive,positive
482518590,"Hi yx,

My question is similar and I'm using examples/FasterRCNN.
I didn't find any add_moving_summary() code in evaluation part, so I assume that's the reason why I can't see the Validation statistics on tensorboard.
![image](https://user-images.githubusercontent.com/12379916/56029348-c6758900-5d4c-11e9-99bb-b6ffac9333ac.png)
I also check up this issue #127, but not sure if it's right to modify like below.
`callbacks = [

            PeriodicCallback(
                ModelSaver(max_to_keep=10, keep_checkpoint_every_n_hours=1),
                every_k_epochs=20),
            # linear warmup
            ScheduledHyperParamSetter(
                'learning_rate', warmup_schedule, interp='linear', step_based=True),
            ScheduledHyperParamSetter('learning_rate', lr_schedule),
            PeakMemoryTracker(),
            EstimatedTimeLeft(median=True),
            SessionRunTimeout(60000).set_chief_only(True),   # 1 minute timeout
        ] + [
            EvalCallback(dataset, *MODEL.get_inference_tensor_names(), args.logdir)
            for dataset in cfg.DATA.VAL
        ] + [
            InferenceRunner(    # run inference(for validation) after every epoch
                val_dataflow,   # the DataFlow instance used for validation
                ScalarStats(    # produce `val_accuracy` and `val_cross_entropy_loss`
                    ['accuracy'], prefix='val'))
        ]
`

",hi question similar find code evaluation part assume reason ca see validation statistic image also check issue sure right modify like linear true minute run inference validation every epoch instance used validation produce,issue,positive,positive,positive,positive,positive,positive
482376838,">  after the batchdata, the ds has a different data format that opencv cannot process, but I don't know what exactly changed.

after `BatchData`, the data becomes a batch.

i.e. it has one more dimension.",different data format process know exactly data becomes batch one dimension,issue,negative,positive,positive,positive,positive,positive
482376242,"I simply change the order:

    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)
    ds = MapDataComponent(ds, lambda x: cv2.resize(x, (np.random.randint(1, 6) * PATCH_SIZE, np.random.randint(1, 6) * PATCH_SIZE)), 0)
    ds = MapData(ds, lambda dp: [split_input_2(dp[0])[0],split_input_2(dp[0])[1]])
it seems after the batchdata, the ds has a different data format that opencv cannot process, but I don't know what exactly changed. since if you put the MapDataComponent in front batchdata, opencv resize can work.",simply change order lambda lambda different data format process know exactly since put front resize work,issue,negative,positive,neutral,neutral,positive,positive
482375969,Sorry but I can't see how is this related to tensorpack.,sorry ca see related,issue,negative,negative,negative,negative,negative,negative
482375731,"I tried, then the opencv function cannot work:
cv2.error: OpenCV(4.0.0) /io/opencv/modules/imgproc/src/resize.cpp:3427: error: (-215:Assertion failed) !dsize.empty() in function 'resize'
",tried function work error assertion function,issue,negative,neutral,neutral,neutral,neutral,neutral
482375143,"You apparently cannot batch data of different shapes, right?
So you probably want to transform your images after batching them and transform all images in a batch with the same size.",apparently batch data different right probably want transform transform batch size,issue,negative,positive,positive,positive,positive,positive
482374843,"if I put them inside like this:
    ds = MapDataComponent(ds, lambda x: cv2.resize(x, (np.random.randint(1, 6) * PATCH_SIZE, np.random.randint(1, 6) * PATCH_SIZE)), 0)
    ds = MapData(ds, lambda dp: [split_input_2(dp[0])[0],split_input_2(dp[0])[1]])
    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)

 then here are the errors:

[0411 15:58:27 @common.py:145] ERR Shape of all arrays to be batched: [(768, 768, 3), (1280, 1280, 3), (1024, 1024, 3), (1280, 1280, 3)]
[0411 15:58:27 @common.py:142] ERR Cannot batch data. Perhaps they are of inconsistent shape?

where should I put the mapdatacomponent exactly?
",put inside like lambda lambda err shape err batch data perhaps inconsistent shape put exactly,issue,negative,positive,positive,positive,positive,positive
482354940,"Because you generate the random number only once.
If you want to have different sizes you need to generate the random number inside the lambda.",generate random number want different size need generate random number inside lambda,issue,negative,negative,negative,negative,negative,negative
482353358,"Sorry, have to reopen it, since I found out that the following does not work, the image size was fixed and did not change epoch by epoch.
   **rd = np.random.randint(1, 6)
   ds = MapDataComponent(ds, lambda x: cv2.resize(x, (rd * PATCH_SIZE, rd * PATCH_SIZE)), 0)**

I also tried sth like this:
    def myresize(ds):
        augmentors2 = [
            imgaug.RandomResize(xrange=[2,3], aspect_ratio_thres=0),
       ]   
       ds = AugmentImageComponent(ds, augmentors2) 
       return ds
    ds = MapDataComponent(ds, myresize, 0)
or this:
    def myresize(ds):
        rd = np.random.randint(1, 6) 
        ds = cv2.resize(ds, (rd * PATCH_SIZE, rd * PATCH_SIZE))
        return ds
    ds = MapDataComponent(ds, myresize, 0)
all did not work.
Can you please give an example on using augmentors in MapDataComponent? 
Thanks!",sorry reopen since found following work image size fixed change epoch epoch lambda also tried like return return work please give example thanks,issue,positive,negative,neutral,neutral,negative,negative
482313274,"If your data has different size, you cannot create a model with fixed input size. You need to use `None` for the input size.",data different size create model fixed input size need use none input size,issue,negative,positive,neutral,neutral,positive,positive
482311427,"I assume I can add sth like this into my code:
    ds = MapData(ds, lambda dp: [split_input_2(dp[0])[0],split_input_2(dp[0])[1]])
    +++++++
**ds = MapDataComponent(ds, lambda x: cv2.resize(x, (np.random.randint(2, 5) * PATCH_SIZE, np.random.randint(2, 5) * PATCH_SIZE)), 0)**
    +++++++++++
    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)

but then, because the model init used PATCH_SIZE, which is fixed at the beginning and cannot match the following resized data feed, how can I solve this issue?

class Model(ModelDesc):
    def __init__(self, **height=PATCH_SIZE, width=PATCH_SIZE**):
        super(Model, self).__init__()
        self.height = height
        self.width = width

    def inputs(self):
        return [InputDesc(tf.float32, (None, self.height, self.width, 3), 'input'),
                InputDesc(tf.float32, (None, self.height, self.width, 3), 'gt'),
                ]",assume add like code lambda lambda model used fixed beginning match following data feed solve issue class model self super model self height width self return none none,issue,positive,positive,positive,positive,positive,positive
482297880,You can use https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.MapDataComponent and do whatever to your data (including calling augmentors) in the mapping function.,use whatever data calling function,issue,negative,neutral,neutral,neutral,neutral,neutral
482077024,"Ok, great. I will have a look at this. Thanks for the help!",great look thanks help,issue,positive,positive,positive,positive,positive,positive
482054135,"Hi @ppwwyyxx , the problem lies in the memory  and my wrong setting. In my previous `config.py`: 

        _C.PREPROC.TRAIN_SHORT_EDGE_SIZE = [1024, 1024]  # [min, max] to sample from
        _C.PREPROC.TEST_SHORT_EDGE_SIZE = 1024
        _C.PREPROC.MAX_SIZE = 2048

which is the cause of the `out of memory`. 

And after I set the `config.py` as follows:

        _C.PREPROC.TRAIN_SHORT_EDGE_SIZE = [800, 800]  # [min, max] to sample from
        _C.PREPROC.TEST_SHORT_EDGE_SIZE = 800
        _C.PREPROC.MAX_SIZE = 1024

The problem gone. It seems the `Out of memory` will have effects on the internal process in `GPUUtilizationTracker` class and terminate it during training. Not sure why this happened.

And please check the following line: https://github.com/tensorpack/tensorpack/blob/6e0b509b5a010ab1ca14033999fa75cef39d579c/tensorpack/callbacks/prof.py#L78

The `stats` seems a list, so you may improve that. Thanks a lot!",hi problem memory wrong setting previous min sample cause memory set min sample problem gone memory effect internal process class terminate training sure please check following line list may improve thanks lot,issue,negative,positive,neutral,neutral,positive,positive
481991234,You can use `tf.control_dependencies` in `build_graph` to let tensors evaluate one at a time.,use let evaluate one time,issue,negative,neutral,neutral,neutral,neutral,neutral
481989571,"Thanks for the quick reply.

You suggest to combine the graphs of the teacher networks and the student into a single new graph? Then use ChainInit or similar to load the weights for each teacher network?

I can try this, but I am a bit worried about memory. I can have several high capacity networks in the teacher and the student could have a relatively high capacity too. Is there a good way to build, load the weights and evaluate each teacher network one at a time to be able to scale to many and large teacher networks?

Thanks,
Erik",thanks quick reply suggest combine teacher student single new graph use similar load teacher network try bit worried memory several high capacity teacher student could relatively high capacity good way build load evaluate teacher network one time able scale many large teacher thanks,issue,positive,positive,positive,positive,positive,positive
481951678,"Thanks for the investigation. So indeed some processes are killed. Not sure whether it is the `GPUUtilizationTracker` process -- but in any case a killed process may stuck your training.

I'm not sure why no processes are killed when `GPUUtilizationTracker` is not used, despite similar memory usage. I have some hypothesis but I'll need some time to try it.

For now, you need to lower your memory usage. You can achieve this by using fewer processes or smaller `buffer_size` in https://github.com/tensorpack/tensorpack/blob/8dd254be676d89000a7f4f6fd5cebe78b3499789/examples/FasterRCNN/data.py#L375. `buffer_size` is an option of `MultiProcessMapDataZMQ`.",thanks investigation indeed sure whether process case process may stuck training sure used despite similar memory usage hypothesis need time try need lower memory usage achieve smaller option,issue,positive,positive,positive,positive,positive,positive
481917032,"@yuyijie1995  Hi, Did you find any reason for this error? I am facing the simillar issue, but from my log, it seems before this error it is trying to run on CPU and the failure is due to that! ",hi find reason error facing issue log error trying run failure due,issue,negative,negative,negative,negative,negative,negative
481798614,"Thanks for the quick response!

I suppose I am asking whether 2-bit activations are really 2-bit. Which now sounds silly haha. 
",thanks quick response suppose whether really silly,issue,positive,positive,neutral,neutral,positive,positive
481794981,"> I am attempting to have different layers with different bitwidth activations. Can you please explain why this is a problem?

There is no problem anymore since TF1.7. 
I should remove the documentation.

> 2-bit activation would then be able to convey 4 distinct ""codes"" correct? As in, the activation function is quantized to become 4 distinct messages.

I do not know what ""code"" or ""message"" mean to you but this sound correct to me.",different different please explain problem problem since remove documentation activation would able convey distinct correct activation function become distinct know code message mean sound correct,issue,negative,positive,positive,positive,positive,positive
481766073,"```python
def build_model():
    a = tf.stop_gradient(teacher(inputs))
    b = student(inputs)
    return loss(a, b)
```",python teacher student return loss,issue,negative,neutral,neutral,neutral,neutral,neutral
481762893,We just mistakenly multiply the layer by 49 during training.,mistakenly multiply layer training,issue,negative,neutral,neutral,neutral,neutral,neutral
481693591,"@ppwwyyxx After I comment the two lines related to `GPUUtilizationTracker` in `train.py`, then the training is OK. And the memory usage is also high, but no stuck. So this may be not the main reason.
![8](https://user-images.githubusercontent.com/43327429/55883147-5e059b00-5bd8-11e9-9134-d5a0213b7015.jpg)
",comment two related training memory usage also high stuck may main reason,issue,negative,positive,positive,positive,positive,positive
481653648,"Sorry but, can you explain more about the operation ""tf.multiply(49)""?
Which layer has the bug in your model design?
Is it ok to use tensorflow's function directly?",sorry explain operation layer bug model design use function directly,issue,negative,negative,negative,negative,negative,negative
481595978,"I have update the `prof.py` with your latest commit. And check the outputs of `dmesg`.
Please find the `dmesg.txt`, it seems the `ZMQ` process was killed due to `Out of Memory`. 
![7](https://user-images.githubusercontent.com/43327429/55864109-66e17700-5bae-11e9-97ac-9de1f0f2a8a6.jpg)
![6](https://user-images.githubusercontent.com/43327429/55864118-6c3ec180-5bae-11e9-8689-2b447fd6c121.jpg)
[prof.py_new.txt](https://github.com/tensorpack/tensorpack/files/3062799/prof.py_new.txt)
[dmesg.txt](https://github.com/tensorpack/tensorpack/files/3062811/dmesg.txt)

@ppwwyyxx Thank you very much for helping checking this consistently! 
I'm sorry but I will be out for a few hours. I'll update this issue as soon as possible. Thanks.
",update latest commit check please find process due memory thank much helping consistently sorry update issue soon possible thanks,issue,positive,positive,neutral,neutral,positive,positive
481584367,"Thanks. It seems that this time it's stuck (or died) after ""worker 6"", different from last time. So perhaps it's not stuck in the nvidia driver and may be the cause is in somewhere else.

However this still feels very strange. It does seem like the process died rather than stuck.

You could take a look at `dmesg` command and see if it has printed any information about processes being killed by the operating system.

Perhaps also try removing the line `ensure_proc_terminate(self._proc)` from `_before_train` and see if it changes anything?",thanks time stuck worker different last time perhaps stuck driver may cause somewhere else however still strange seem like process rather stuck could take look command see printed information operating system perhaps also try removing line see anything,issue,negative,positive,neutral,neutral,positive,positive
481581051,"```python
 for k in range(num_gpu):
        predictors.append(lambda img,lbl: graph_funcs[k](img,lbl))
```
this line of code is wrong: `k` will be 3 for all 4 functions.

Closing as this is a Python question. You can find some help online such as  https://stackoverflow.com/questions/19837486/python-lambda-in-a-loop",python range lambda line code wrong python question find help,issue,negative,negative,negative,negative,negative,negative
481577515,"Hi @ppwwyyxx , thanks a lot for your suggestions! 
I tried to add more prints in `prof.py` and the log is attached. Please note in the video above, there is  a short term stuck from 2:10 to 4:10. It seems after this short term stuck, the `self._proc.is_alive()` seems to be `False`. I think the later stuck is much realted to this. But I cannot figure out why the `self._proc` is stopped during that short term stuck.
![5](https://user-images.githubusercontent.com/43327429/55860742-81fcb880-5ba7-11e9-88cf-1d3ef937a0f3.jpg)

Please find the detail `prof.py` and `log.log` as follows:
[log.log](https://github.com/tensorpack/tensorpack/files/3062565/log.log)

[prof.py.txt](https://github.com/tensorpack/tensorpack/files/3062576/prof.py.txt)


I'll update Tensorpack with your latest commit and test. Thanks!",hi thanks lot tried add log attached please note video short term stuck short term stuck false think later stuck much figure stopped short term stuck please find detail update latest commit test thanks,issue,positive,positive,neutral,neutral,positive,positive
481550814,"The latest commit (f831a46e7f37b1bf08b65e1f3549a8ee4f9921b2) reorganizes the above logic to make fewer calls to nvidia driver. It will only make calls to `utilization()` but not `device()`. It is a shot in the dark but if there is indeed an issue in the driver, this is something worth trying.

Other than this I don't have other suggestions except to upgrade the driver.",latest commit logic make driver make utilization device shot dark indeed issue driver something worth trying except upgrade driver,issue,negative,positive,positive,positive,positive,positive
481543831,"Thanks for the log. These are helpful.
As you can see, your FPN log was stuck between ""worker 5"" and ""worker 6"" however the C4 log was not.

This is quite strange, given that the code between ""worker 5"" and ""worker 6"" is essentially equivalent to this:
```python
import time
import numpy as np
from tensorpack.utils.nvml import NVMLContext

stats = np.zeros((2,), dtype='f4')
cnt = 0
with NVMLContext() as ctx:
    while True:
        time.sleep(1)
        data = [ctx.device(i).utilization()['gpu'] for i in [0, 1]]
        data = list(map(float, data))
        stats += data
        cnt += 1
        print(data)
```
which should never get stuck. It makes call to the nvidia driver, so if it ever gets stuck it sounds like a bug in the driver.",thanks log helpful see log stuck worker worker however log quite strange given code worker worker essentially equivalent python import time import import true data data list map float data data print data never get stuck call driver ever stuck like bug driver,issue,positive,positive,positive,positive,positive,positive
481522003,"@ppwwyyxx I've attached the logs. Please kindly check them. Thanks.
[c4_log.log](https://github.com/tensorpack/tensorpack/files/3061884/c4_log.log)
[fpn_log.log](https://github.com/tensorpack/tensorpack/files/3061885/fpn_log.log)
",attached please kindly check thanks,issue,positive,positive,positive,positive,positive,positive
481516116,"@ppwwyyxx Thanks for checking this problem! I've installed the latest master and also add the following in the `prof.py`:
![2](https://user-images.githubusercontent.com/43327429/55848642-2d921280-5b80-11e9-93d4-e354ad8fee66.jpg)

Then the log is as follows:
![1](https://user-images.githubusercontent.com/43327429/55848750-9e392f00-5b80-11e9-9535-6c95680ab559.jpg)

If convenient, please have a check. Thanks!



",thanks problem latest master also add following log convenient please check thanks,issue,positive,positive,positive,positive,positive,positive
481479220,"Thanks for the video. It seems there is an error in the GPUUtilizationTracker worker process but the error was somehow not printed. Could you uninstall and reinstall the [latest tensorpack](https://github.com/tensorpack/tensorpack/commit/6e0b509b5a010ab1ca14033999fa75cef39d579c) and see whether there are any error logs? If no extra information is printed, you might need to add some print statements in `GPUUtilizationTracker.worker` to see where it is stuck.",thanks video error worker process error somehow printed could reinstall latest see whether error extra information printed might need add print see stuck,issue,negative,positive,positive,positive,positive,positive
481175698,"Hi @ppwwyyxx , thanks for your suggestions. I have made a video recording the training process. Please find the video in https://pan.baidu.com/s/1PcJW4Aapt1W3iRGwcAfQ-w, the extraction code is `7kqr`. 

In this video, I run the experiments `without` and `with` the two line comment. 
First `without` the two line comment (from 0:00 to 9:40), please pay more attention the video from 2:10 to 4:10, which shows a short term stuck during the training, and after the first epoch, the training will stuck.

Then I comment the two line (from 9:40 to end), the training is OK. 

Please note that the experiments are with my modified code based on ResNetC4Model and ResNetFPNModel. I think the cause of the stuck lies in my modification to the ResNetFPNModel. Please allow me to check it again and check it carefully! Training ResNetFPNModel(modified) with one/two GPUs will have the stuck.

1. Could you reproduce this issue without modification to the model?
No. The original model run OK.

2. When it is stuck what would you observe if you press ctrl-C? (include logs if any)
Please see the video if convenient. It shows ""2019-04-09 17:24:03.079834: W tensorflow/core/kernels/queue_base.cc:277] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed""",hi thanks made video recording training process please find video extraction code video run without two line comment first without two line comment please pay attention video short term stuck training first epoch training stuck comment two line end training please note code based think cause stuck modification please allow check check carefully training stuck could reproduce issue without modification model original model run stuck would observe press include please see video convenient skipping attempt queue closed,issue,positive,positive,positive,positive,positive,positive
481147515,"In addition to the above requests:
1. Could you reproduce this issue without modification to the model?
2. When it is stuck what would you observe if you press `ctrl-C`? (include logs if any)",addition could reproduce issue without modification model stuck would observe press include,issue,negative,neutral,neutral,neutral,neutral,neutral
481137514,"This sounds like a bug. For anyone to better understand and diagnose your issue, please post relevant details following the issue template (you can find it by clicking ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

Also, please confirm whether you can reliably reproduce this bug every time or not, and whether the workaround you've found is effective every time or not..",like bug anyone better understand diagnose issue please post relevant following issue template find new issue unexpected visit link post unexpected also please confirm whether reliably reproduce bug every time whether found effective every time,issue,positive,positive,positive,positive,positive,positive
480664982,Yeah! I'll try to investigate my case as you suggested. Thank you! @ppwwyyxx ,yeah try investigate case thank,issue,positive,neutral,neutral,neutral,neutral,neutral
480575600,">  So maybe the bottleneck is in the data loading?

See https://tensorpack.readthedocs.io/tutorial/performance-tuning.html to know where your bottleneck is.

> Does Tensorpack support the data prefetching from the memory?

The example does this already.",maybe bottleneck data loading see know bottleneck support data memory example already,issue,negative,neutral,neutral,neutral,neutral,neutral
480574540,"Hi @ppwwyyxx , thanks a lot for your kind suggestions!
I've tried to modify the `os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'` and `os.environ['TF_AUTOTUNE_THRESHOLD'] = '0'` in `config.py`, and it has some speed up and the GPU memory usage seems decreased about ~2000M(from ~10000M to ~8000M). And the data loading seems costs `300%` CPU usage compared to the previous version.  Currently the log in the cluster is as follows:

     15%|#5        |77/500[03:02<16:39, 0.42it/s]
     17%|#7        |86/500[03:20<16:18, 0.42it/s]
     35%|###4      |174/500[06:02<11:28, 0.47it/s]
     37%|###6      |183/500[06:20<11:09, 0.47it/s]
     54%|#####4    |271/500[09:03<07:35, 0.50it/s]
     56%|#####6    |280/500[09:20<07:17, 0.50it/s]
     74%|#######3  |369/500[12:04<04:11, 0.52it/s]
     75%|#######5  |377/500[12:20<03:55, 0.52it/s]
     93%|#########3|467/500[15:05<01:02, 0.53it/s]
     95%|#########4|474/500[15:20<00:49, 0.53it/s]
    100%|##########|500/500[16:37<00:00, 0.50it/s][32m[0407 09:22:41 @base.py:285][0m Epoch 1 (global_step 500) finished, time:16 minutes 37 seconds.

And I've also tried the same code in local machine with 2 1080Ti GPUs, and also set the flags of CUDNN is `0`. The speed has ~`1.50it/s`. So maybe the bottleneck is in the data loading?

The dataset has about `50000` images and the size of each image is `800x800x3`. I saved the images into a python dictionary which has size of about 100G. I loaded the dictionary in the `get_train_dataflow()` and replace the image reading according to the original `roidb` information with the memory reading from the dictionary. The `free` command show the node has ~120G available memory, but when loading the dictionary into the memory, it said memory loading error. Does Tensorpack support the data prefetching from the memory?
",hi thanks lot kind tried modify speed memory usage data loading usage previous version currently log cluster epoch finished time also tried code local machine ti also set speed maybe bottleneck data loading size image saved python dictionary size loaded dictionary replace image reading according original information memory reading dictionary free command show node available memory loading dictionary memory said memory loading error support data memory,issue,positive,positive,positive,positive,positive,positive
480571044,"Have you seen: https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md ?

> If CuDNN warmup is on, the training will start very slowly, until about 10k steps (or more if scale augmentation is used) to reach a maximum speed. As a result, the ETA is also inaccurate at the beginning. CuDNN warmup is by default on when no scale augmentation is used.

You can set TF_AUTOTUNE_THRESHOLD to 0 in `config.py` to disable autotune.",seen training start slowly scale augmentation used reach maximum speed result eta also inaccurate beginning default scale augmentation used set disable,issue,negative,negative,negative,negative,negative,negative
480112066,"@ppwwyyxx I'm Sooooory! I forgot to resize the image with short side length. And in the config, I set the following parameters:

        # anchors -------------------------
        _C.RPN.ANCHOR_STRIDE = 16
        # _C.RPN.ANCHOR_SIZES = (32, 64, 128, 256, 512)   # sqrtarea of the anchor box
        # _C.RPN.ANCHOR_RATIOS = (0.5, 1., 2.)
        _C.RPN.ANCHOR_SIZES = (16, 32, 64, 128, 256)   # sqrtarea of the anchor box
        _C.RPN.ANCHOR_RATIOS = (0.125, 0.25, 0.5, 1., 2., 4., 8.)
        _C.RPN.POSITIVE_ANCHOR_THRESH = 0.7
        _C.RPN.NEGATIVE_ANCHOR_THRESH = 0.3

So the following script would work fine now.

      import numpy as np
        import cv2
        
        from tensorpack.dataflow import (
            DataFromList, MapDataComponent, MultiProcessMapDataZMQ, MultiThreadMapData, TestDataSpeed, imgaug)
        from common import (
            CustomResize, DataFromListOfDict, box_to_point8,
            filter_boxes_inside_shape, point8_to_box, segmentation_to_mask, np_iou)
        
        from config import finalize_configs, config as cfg
        finalize_configs(is_training=False)
        from data import get_rpn_anchor_input
        im = cv2.imread('2012_004314.jpg')
        boxes = np.array([[304,189,262,22]], dtype=np.float32)
        
        aug = imgaug.AugmentorList(
                [CustomResize(cfg.PREPROC.TRAIN_SHORT_EDGE_SIZE, cfg.PREPROC.MAX_SIZE)])
        # augmentation:
        im, params = aug.augment_return_params(im)
        points = box_to_point8(boxes)
        points = aug.augment_coords(points, params)
        boxes = point8_to_box(points)
        
        is_crowd = np.zeros((1,), dtype=np.int8)
        anchor_labels, anchor_boxes = get_rpn_anchor_input(im, boxes, is_crowd)
        anchors_pos = np.where(anchor_labels==1)
        print(anchors_pos)
        anchors_neg = np.where(anchor_labels==0)
        anchors_ign = np.where(anchor_labels==-1)
        print(len(anchors_neg[0]))
        print(len(anchors_ign[0]))

the result is:

        (array([16, 17, 18]), array([42, 42, 42]), array([29, 29, 29]))
        253
        573184

I'm sorry for bothering you for my misunderstanding and thanks for your time!",forgot resize image short side length set following anchor box anchor box following script would work fine import import import common import import data import augmentation print print print result array array array sorry misunderstanding thanks time,issue,negative,negative,neutral,neutral,negative,negative
480109302,"There are other reasons making the anchors negative, e.g. the iou is too small, etc.
This purely depends on your data, and without knowing what your data is like (e.g., the shape of it) and how you execute the code (e.g., the configs) there is not much I can tell.

If you expect others to investigate an unexpected issue, please include details for others to reproduce it.",making negative small purely data without knowing data like shape execute code much tell expect investigate unexpected issue please include reproduce,issue,negative,negative,neutral,neutral,negative,negative
480106244,"Hi @ppwwyyxx , in the above example, I set the `is_crowd=0`, and the ground truth box seems still be excluded. Maybe adding an `ADD_GTBOXES`  into the comment would be great!",hi example set ground truth box still maybe comment would great,issue,positive,positive,positive,positive,positive,positive
480104617,"@ppwwyyxx thanks a lot for your kind comments and suggestions!  
I think I need to read the Detectron. I must miss many things in the literature. Thanks!",thanks lot kind think need read must miss many literature thanks,issue,positive,positive,positive,positive,positive,positive
480103347,"`is_crowd=1` means the box is not a box of an instance so I exclude it from positive anchors. This follows the official implementation in Detectron.

Whether you want to include it is up to you.",box box instance exclude positive official implementation whether want include,issue,negative,positive,positive,positive,positive,positive
480102032,"Thanks, I updated the code with new interface, the error is gone.",thanks code new interface error gone,issue,negative,positive,positive,positive,positive,positive
480100414,"Hi @ppwwyyxx , thanks a lot for your kind reply! 
I'm not sure why the anchors are related to the `is_crowd` parameter. The `is_crowd` seems to be related to the `segmentation`. And the label generation of anchors for RPN is just related to the ground truth boxes, right? Why does the `is_crowd` have an impact to the anchors? And in the above example, the positive anchors is None. I may missed something, could you please give some advice? Thanks.",hi thanks lot kind reply sure related parameter related segmentation label generation related ground truth right impact example positive none may something could please give advice thanks,issue,positive,positive,positive,positive,positive,positive
479359671,"For anyone to be able to understand and diagnose your issue, please fill the issue template.",anyone able understand diagnose issue please fill issue template,issue,negative,positive,positive,positive,positive,positive
479189429,"It appears that you implemented `ModelDesc` with methods (probably `_get_inputs`) that are deprecated 1 year ago.

Please update your code following the interface of `ModelDesc` or the examples, e.g.:
https://github.com/tensorpack/tensorpack/blob/f9bf54073a8e1edc546e8a615fc02806d40c760a/examples/basics/mnist-convnet.py#L19-L25",probably year ago please update code following interface,issue,negative,neutral,neutral,neutral,neutral,neutral
479059588,"For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

As the [documentation said](https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.sessinit.SaverRestore.__init__), `ignore` should be ""a list of tensor names"" so why would you write ""loc:@linear/W""?",anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected documentation said ignore list tensor would write,issue,negative,positive,positive,positive,positive,positive
479058764,"Please use the function following its documentation.

If you cannot solve the issue please post issues following the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",please use function following documentation solve issue please post following issue template,issue,positive,neutral,neutral,neutral,neutral,neutral
478934775,"giving an error in sample function i.e Dimensions must be equal, but are 2 and 3 for 'clip_by_value_1/Minimum' (op: 'Minimum') with input shapes: [2], [3].",giving error sample function must equal input,issue,negative,neutral,neutral,neutral,neutral,neutral
478737182,"Keras' learning phase has nothing to do with the issue here. When you export a graph in tensorpack with PredictConfig, it already uses the testing phase.

The issue is tensorpack's implementation uses more types of ops in tensorflow, some of those unsupported by tensorrt.",learning phase nothing issue export graph already testing phase issue implementation unsupported,issue,negative,neutral,neutral,neutral,neutral,neutral
478735713,"I at all has not experience with this, just trying. In Keras has option `keras.backend.set_learning_phase` and it working.
If i set `keras.backend.set_learning_phase (1)` then i get a lot warning for unsupported operations.
If i set `keras.backend.set_learning_phase (0)` all warnings exclude `resize_nearest_neighbor` (and custom layers) disappear. Don't know how it working, and what to do this option, but i believe, it could somehow work, will try to found :-)",experience trying option working set get lot warning unsupported set exclude custom disappear know working option believe could somehow work try found,issue,negative,neutral,neutral,neutral,neutral,neutral
478731153,"OK. I have no experience with TensorRT and do not know how to interpret those warnings. 
If TensorRT does not support all these tensorflow ops, there is not much we can do about it.",experience know interpret support much,issue,negative,positive,positive,positive,positive,positive
478708256,"O! Very big, thank you for solution, it generate uff file!
But, `resize_nearest_neighbor`, operation does not supported TensorRT, so I need to replace or implement it. But, a lot, unsupported operations still in graph (if look on warnings). 
In Matterport solution only `resize_nearest_neighbor` (Keras layer UpSampling2D), and specific layers, for Mask-RCNN.",big thank solution generate file operation need replace implement lot unsupported still graph look solution layer specific,issue,positive,neutral,neutral,neutral,neutral,neutral
478403360,Closing since no clarification was provided on the question.,since clarification provided question,issue,negative,neutral,neutral,neutral,neutral,neutral
478403297,Closing now but feel free to follow up if the above links do not answer your questions.,feel free follow link answer,issue,positive,positive,positive,positive,positive,positive
478403103,The commit https://github.com/tensorpack/tensorpack/commit/d48cce3aa399db11a4e6fc35a5a896c96a92b641 linked above would also resolve the issue: it does not make the issue disappear (because I think it is an issue of jupyter notebooks / IDEs). But it will delay throwing the exception until you use `LinearWrap` on the layers. From the code you linked to it appears `LinerWrap` was not used so it will probably be safe.,commit linked would also resolve issue make issue disappear think issue ides delay throwing exception use code linked used probably safe,issue,positive,positive,positive,positive,positive,positive
478402595,"> These changes have not changed anything

You should include in your issue precisely what you did and what you observed, like what you did in the original issue.

I would on the other hand conclude that my suggestions were not correctly followed. 
In my test: the following diff does make the error message you've seen disappear:
```diff
--- i/examples/FasterRCNN/basemodel.py
+++ w/examples/FasterRCNN/basemodel.py
@@ -200,12 +200,7 @@ def resnet_fpn_backbone(image, num_blocks):
     with backbone_scope(freeze=freeze_at > 0):
         chan = image.shape[1]
         pad_base = maybe_reverse_pad(2, 3)
-        l = tf.pad(image, tf.stack(
-            [[0, 0], [0, 0],
-             [pad_base[0], pad_base[1] + pad_shape2d[0]],
-             [pad_base[0], pad_base[1] + pad_shape2d[1]]]))
-        l.set_shape([None, chan, None, None])
-        l = Conv2D('conv0', l, 64, 7, strides=2, padding='VALID')
+        l = Conv2D('conv0', image, 64, 7, strides=2, padding='SAME')
         l = tf.pad(l, [[0, 0], [0, 0], maybe_reverse_pad(0, 1), maybe_reverse_pad(0, 1)])
         l = MaxPooling('pool0', l, 3, strides=2, padding='VALID')
     with backbone_scope(freeze=freeze_at > 1):
```

Then I found there is another operation that TensorRT does not support. Another change would make TensorRT produce a UFF:
```diff
diff --git i/examples/FasterRCNN/model_fpn.py w/examples/FasterRCNN/model_fpn.py
index b7c776d8..46b49e26 100644
--- i/examples/FasterRCNN/model_fpn.py
+++ w/examples/FasterRCNN/model_fpn.py
@@ -32,17 +32,13 @@ def fpn_model(features):
     use_gn = cfg.FPN.NORM == 'GN'
 
     def upsample2x(name, x):
-        return FixedUnPooling(
-            name, x, 2, unpool_mat=np.ones((2, 2), dtype='float32'),
-            data_format='channels_first')
-
         # tf.image.resize is, again, not aligned.
-        # with tf.name_scope(name):
-        #     shape2d = tf.shape(x)[2:]
-        #     x = tf.transpose(x, [0, 2, 3, 1])
-        #     x = tf.image.resize_nearest_neighbor(x, shape2d * 2, align_corners=True)
-        #     x = tf.transpose(x, [0, 3, 1, 2])
-        #     return x
+        with tf.name_scope(name):
+            shape2d = tf.shape(x)[2:]
+            x = tf.transpose(x, [0, 2, 3, 1])
+            x = tf.image.resize_nearest_neighbor(x, shape2d * 2, align_corners=True)
+            x = tf.transpose(x, [0, 3, 1, 2])
+            return x
 
     with argscope(Conv2D, data_format='channels_first',
                   activation=tf.identity, use_bias=True,
```

Note that after the two changes, the model require the width & height of input images to be a multiple of 32. It may also produce slightly different results because it uses different operations (I've no idea how large the difference is and that's up to you to test).

I'm using TensorRT 5.0.2.6 + TF 1.13.1

Closing as this is not a tensorpack issue.",anything include issue precisely like original issue would hand conclude correctly test following make error message seen disappear image image none none none image found another operation support another change would make produce git index name return name name shaped shaped return name shaped shaped return note two model require width height input multiple may also produce slightly different different idea large difference test issue,issue,positive,positive,positive,positive,positive,positive
478336030,These changes have not changed anything. It seems that I am doing something wrong when exporting the graph.,anything something wrong graph,issue,negative,negative,negative,negative,negative,negative
478324539,"One thing that you can try:
in `basemodel.py`, replace all the 
```
tf.pad(.....)
Conv2D(..., padding='VALID')
```
By 
```
Conv2D(..., padding='SAME')
```

i.e., remove pad, and change valid to same.
This will make TensorRT happier. But it may have other bugs after this one.

This will also produce slightly different results, but probably negligible.",one thing try replace remove pad change valid make happier may one also produce slightly different probably negligible,issue,positive,neutral,neutral,neutral,neutral,neutral
478062300,"When you post this issue you've ignored the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md)), which have also instructed users to not trust jupyter / IDEs.",post issue issue template also instructed trust ides,issue,positive,neutral,neutral,neutral,neutral,neutral
478062060,"The code looks correct. Please DO NOT run it in jupyter notebooks or other notebooks/IDEs: they have bugs.

If you can reproduce the issue under a normal shell environment, please include details on how to reproduce it: just a link is far from enough.",code correct please run reproduce issue normal shell environment please include reproduce link far enough,issue,positive,positive,neutral,neutral,positive,positive
477912684,"Code link: https://github.com/tkuanlun350/3DUnet-Tensorflow-Brats18
Sir, above is the code which i'm trying and getting issue. Sir, the solution which you told I tried but I didn't get it. Can you please look into my code and tell where should I correct it?
Thanks in advance.",code link sir code trying getting issue sir solution told tried get please look code tell correct thanks advance,issue,positive,positive,positive,positive,positive,positive
477900117,"This is a tensorflow question and unrelated to tensorpack.
You can use functions like `tf.meshgrid`.",question unrelated use like,issue,negative,neutral,neutral,neutral,neutral,neutral
477897384,"Yes.
As the code says it is a tensor of shape `(B, H2, W2, 2)` which means it contains `B x H2 x W2` coordinates.",yes code tensor shape,issue,negative,neutral,neutral,neutral,neutral,neutral
477896108,"> what are the img,coords in sample(img,coords)?

`img` is an image.
`coords` are coordinates to sample from the image.

> And can I use this STN on a single image of my own?

I don't understand what you're asking.
You're allowed to use the code to do anything.
How to do them depends on what you want to do.",sample image sample image use single image understand use code anything want,issue,negative,negative,neutral,neutral,negative,negative
477892769,You have two `InstanceNorm5d` in your code with `@layer_register` (or you have one but execute it twice) and this is not allowed,two code one execute twice,issue,negative,neutral,neutral,neutral,neutral,neutral
477820654,"@ppwwyyxx Thanks a lot for your kind reply! I added the `tf.Print` in the graph of `ResNetC4Model`, but there are no the desired outputs when training. I used the `tf.py_func` to show the image tensor. Maybe I can also save the image values into the disk for checking. Thanks.",thanks lot kind reply added graph desired training used show image tensor maybe also save image disk thanks,issue,positive,positive,positive,positive,positive,positive
477597213,"Hi @ppwwyyxx , could you please give some advice on how to use the `tf.Print` to debug the Tensorpack? In my case, I use the FasterRCNN in Tensorpack and is there any way to print the value of `decoded_boxes`  tensor in the terminal log? Thanks.",hi could please give advice use case use way print value tensor terminal log thanks,issue,positive,positive,positive,positive,positive,positive
477407263,"From the log, it appears that you're using this project: https://github.com/DAI-Lab/TGAN

It does have loops that trains another model after finishing training one: https://github.com/DAI-Lab/TGAN/blob/25fd2384eb38d13c53cd0eb20bf2c3577b7ff878/src/launcher_simple.py#L124-L125

Closing as this is unrelated to tensorpack. If you have questions about the usage of a different project, please ask at that project.",log project another model finishing training one unrelated usage different project please ask project,issue,negative,neutral,neutral,neutral,neutral,neutral
477406608,"> Why it was training like a loop

The most likely explanation is that there is such a loop in the code you run.

Without code and other details that I asked for in the comment above, I am unable to understand your issue just from the partial logs.",training like loop likely explanation loop code run without code comment unable understand issue partial,issue,negative,negative,negative,negative,negative,negative
477406128,"No. Why it was training like a loop. I chose epoch = 2. After successfully finished training
```
[0328 09:13:48 @base.py:250] Start Epoch 2 ...
100%|#############################################################################################################################################################################|5000/5000[31:50<00:00, 2.62it/s]
[0328 09:45:38 @base.py:260] Epoch 2 (global_step 10000) finished, time:31 minutes 50 seconds.

```
It went back to Epoch 1 and started training again
```

Total #vars=94, #params=1675875, size=6.39MB
[0328 10:11:46 @base.py:187] Setup callbacks graph ...
[0328 10:11:46 @summary.py:38] Maintain moving average summary of 6 tensors in collection MOVING_SUMMARY_OPS.
[0328 10:11:46 @summary.py:75] Summarizing collection 'summaries' of size 9.
[0328 10:11:46 @graph.py:91] Applying collection UPDATE_OPS of 12 ops.
[0328 10:11:47 @base.py:205] Creating the session ...
2019-03-28 10:11:47.894942: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[0328 10:11:48 @base.py:211] Initializing the session ...
[0328 10:11:48 @base.py:218] Graph Finalized.
[0328 10:11:48 @concurrency.py:37] Starting EnqueueThread QueueInput/input_queue ...
[0328 10:11:49 @base.py:250] Start Epoch 1 ...
```

",training like loop chose epoch successfully finished training start epoch epoch finished time went back epoch training total setup graph maintain moving average summary collection collection size collection session binary use session graph starting start epoch,issue,positive,positive,positive,positive,positive,positive
477272585,`get_tf_version_number` does not make sense (because there is TF 1.10) and was deprecated half a year ago (https://github.com/tensorpack/tensorpack/commit/b673b24cf19fcfeda367dd023bc04c035e5b6d64#diff-3a128bdb78dfe138c82e2546d899de9f). Please use `get_tf_version_tuple` instead.,make sense half year ago please use instead,issue,negative,negative,negative,negative,negative,negative
476736726,I see. That works for me. Thank you very much.,see work thank much,issue,negative,positive,positive,positive,positive,positive
476705858,"The error is telling you that 'cost' does not exist in the graph, so there is no way to inference it. You can only inference tensors that exist.",error telling exist graph way inference inference exist,issue,negative,neutral,neutral,neutral,neutral,neutral
476397780,The latest release on pypi (0.9.4) is also new enough to run the examples.,latest release also new enough run,issue,negative,positive,positive,positive,positive,positive
476396551,"I fixed the issue based on my best guess of what happened. My guess is that you're running an old version (<= 0.9.1) of FasterRCNN examples with tensorpack > 0.9.1, which can trigger this bug.
Now it should be fixed, and if you're still seeing any issues, please complete the issue template so I don't have to guess.",fixed issue based best guess guess running old version trigger bug fixed still seeing please complete issue template guess,issue,positive,positive,positive,positive,positive,positive
476310183,"I'm unable to reproduce your issue, so please complete the issue template in the way described above.",unable reproduce issue please complete issue template way,issue,negative,negative,negative,negative,negative,negative
476108441,Thanks for reporting. You can checkout the examples at the  v0.9.1 tag AND reinstall tensorpack 0.9.1 for now to get it running. I'll fix this bug later.,thanks tag reinstall get running fix bug later,issue,negative,positive,neutral,neutral,positive,positive
475752582,"@Xiao-OMG If it is stuck at evaluation, it's probably a different issue: you need to check whether your evaluation data has the correct size. 
If your evaluation data returns `len(dataflow) == 4738`, but actually only has 4737 elements to produce, it will be stuck like this. See more at https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.DataFlow.__len__",stuck evaluation probably different issue need check whether evaluation data correct size evaluation data actually produce stuck like see,issue,negative,neutral,neutral,neutral,neutral,neutral
475682263,It is certainly possible if you implement something like the tensorpack trainer.,certainly possible implement something like trainer,issue,positive,neutral,neutral,neutral,neutral,neutral
475565991,"Emmm...
I used  **MultiThreadMapData** to generate train_data_flow, but **MultiProcessMapData** for eval_data_flow.  It stopped at the first step of the first evaluation........
Maybe it should use the same function",used generate stopped first step first evaluation maybe use function,issue,negative,positive,positive,positive,positive,positive
475560231,"@YJHMITWEB  I train a classification network on my own dataset and  have the same problems: 
 1. Memory gradually increases,
 2. Training stopped without any error raising. It stopped at the final step of 10th evaluation.  
![image](https://user-images.githubusercontent.com/18168264/54814792-63ec1880-4ccb-11e9-8715-4e4c0b09e663.png)

I will try MultiThreadMapData. Hope it help to solve problem 1

My TF is 1.9.0, maybe i should try upgrade TF
",train classification network memory gradually training stopped without error raising stopped final step th evaluation image try hope help solve problem maybe try upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
475109543,You're running the examples from github master. Therefore you need to install tensorpack from github master. Instructions to do so is in the template above.,running master therefore need install master template,issue,negative,neutral,neutral,neutral,neutral,neutral
475096405,".........I've tried a lot, replacing `MultiProcessMapData` in `data.py` by `MultiThreadMapData`, using less threads in reading data, the memory consumption is now normal, but still, every thing stops at every 220 epochs...
I think it's about my machine, since it works fine on every other's machine (no similar issue reported in this repo).
Maybe I will try it on another machine later..
",tried lot le reading data memory consumption normal still every thing every think machine since work fine every machine similar issue maybe try another machine later,issue,negative,positive,positive,positive,positive,positive
474943966,"I'm not sure what you mean by ""modify activation"".

But the graph is defined by yourself, so you can do whatever computation by defining the graph. I don't see how is that related to tensorpack.",sure mean modify activation graph defined whatever computation graph see related,issue,negative,positive,neutral,neutral,positive,positive
474521248,"Another thing to try is to run `python data.py`. It will just iterate over the the entire dataflow for 50000 iterations (which you can change to more iterations). You can use this to test whether the dataflow will grow the memory usage indefinitely. (in my test it does not grow, but perhaps this is different across systems).",another thing try run python iterate entire change use test whether grow memory usage indefinitely test grow perhaps different across,issue,negative,neutral,neutral,neutral,neutral,neutral
474435872,"As said in the issue template:


If you expect higher speed, please read
http://tensorpack.readthedocs.io/tutorial/performance-tuning.html 
before posting.

If you expect certain accuracy, only in one of the two conditions can we help with it:
(1) You're unable to match the accuracy documented in tensorpack examples.
(2) It appears to be a tensorpack bug.

Otherwise, how to train a model to certain accuracy is a machine learning question and is
not our responsibility to figure out.",said issue template expect higher speed please read posting expect certain accuracy one two help unable match accuracy bug otherwise train model certain accuracy machine learning question responsibility figure,issue,positive,positive,neutral,neutral,positive,positive
474208216,"> I expect everything goes right as before (when set FPN=True).
It's cost not nan with right evaluation results.

That was not a valid expectation. You've changed the hyper-parameters to bad ones and that may result in models that cannot train. 

Tensorpack examples are (obviously) not responsible for the accuracy of examples after arbitrary hyper-parameter changes by the user. It is your responsibility to figure out the right hyper parameter for your goal.

To perform ""quick"" training, refer to the ""quick"" config in the model zoo for a working set of hyper parameters that are cheaper to train than standard models.",expect everything go right set cost nan right evaluation valid expectation bad may result train obviously responsible accuracy arbitrary user responsibility figure right hyper parameter goal perform quick training refer quick model zoo working set hyper train standard,issue,negative,positive,positive,positive,positive,positive
474207060,"Ok, I will modify the `data.py` and test again.
But it's still weird, unlike #1097, in my case, there is no error raising.",modify test still weird unlike case error raising,issue,negative,negative,negative,negative,negative,negative
474206555,"Then your issue sounds similar to #1097 : memory gradually increases, and lead to a DeadlineExceedErrror (which is somewhat similar to getting stuck). However there, the issue does not appear for COCO dataset.

One possibility is to replace `MultiProcessMapData` in `data.py` by `MultiThreadMapData` or `MapData` (probably will be slow) -- this may make a difference, though I'm still not sure what the issue is at the moment.",issue similar memory gradually lead somewhat similar getting stuck however issue appear coco one possibility replace probably slow may make difference though still sure issue moment,issue,negative,positive,neutral,neutral,positive,positive
474205804,"Unfortunately, the problem still exists..
Every time after it runs abut 220 epochs, the whole process stops.
I start from scratch, 0 epoch, and it stops at 220 epochs,
then restart it, load the last checkpoint, start from 220, then it stops again at 439 epochs,
then 648 epochs...
Also, I have found that as the training processes, the memory usage gradually increases (about 0.3~0.4% per epoch of a total 256 GB memory). I'm not sure if it's because of some dynamic nodes defined in the graph, and too many of them are generated after a long-time graph running, which may consume too much memory, leading to the crash...",unfortunately problem still every time abut whole process start scratch epoch restart load last start also found training memory usage gradually per epoch total memory sure dynamic defined graph many graph running may consume much memory leading crash,issue,negative,positive,positive,positive,positive,positive
474192033,"Sorry for the late reply, the updated question as above.",sorry late reply question,issue,negative,negative,negative,negative,negative,negative
474007008,Closing since the question seems to be resolved. Feel free to reopen if otherwise.,since question resolved feel free reopen otherwise,issue,positive,positive,positive,positive,positive,positive
473760242,"> _C.TRAIN.EVAL_PERIOD = 0 # period (epochs) to run evaluation

To disable evaluation, set it to a large number, e.g.: 99999999",period run evaluation disable evaluation set large number,issue,negative,positive,positive,positive,positive,positive
473757947,"i try to train my own data 
this is my configs:
config = AttrDict()

_C = config     # short alias to avoid coding

# mode flags ---------------------
_C.TRAINER = 'replicated'  # options: 'horovod', 'replicated'
_C.MODE_MASK = False        # FasterRCNN or MaskRCNN
_C.MODE_FPN = True

# dataset -----------------------
_C.DATA.BASEDIR = '/home/zysz1/FasterRCNN/coco'
# All TRAIN dataset will be concatenated for training.
_C.DATA.TRAIN = ('train')   # i.e. trainval35k, AKA train2017
# Each VAL dataset will be evaluated separately (instead of concatenated)
_C.DATA.VAL = ('val', )  # AKA val2017
# This two config will be populated later by the dataset loader:
_C.DATA.NUM_CATEGORY = 5  # without the background class (e.g., 80 for COCO)
_C.DATA.CLASS_NAMES = [""BG"",""tiekedahuoji"",""heidingdahuoji"",""daoju"",""dianyuanhedianchi"",""jiandao""]  # NUM_CLASS (NUM_CATEGORY+1) strings, the first is ""BG"".
# whether the coordinates in the annotations are absolute pixel values, or a relative value in [0, 1]
_C.DATA.ABSOLUTE_COORD = False

# basemodel ----------------------
_C.BACKBONE.WEIGHTS = '/home/zysz1/FasterRCNN/weights/ImageNet-R50-AlignPadding.npz'   # /path/to/weights.npz
_C.BACKBONE.RESNET_NUM_BLOCKS = [3, 4, 6, 3]     # for resnet50
# RESNET_NUM_BLOCKS = [3, 4, 23, 3]    # for resnet101
_C.BACKBONE.FREEZE_AFFINE = False   # do not train affine parameters inside norm layers
_C.BACKBONE.NORM = 'None'  # options: FreezeBN, SyncBN, GN, None
_C.BACKBONE.FREEZE_AT = 2  # options: 0, 1, 2

# Use a base model with TF-preferred padding mode,
# which may pad more pixels on right/bottom than top/left.
# See https://github.com/tensorflow/tensorflow/issues/18213
# In tensorpack model zoo, ResNet models with TF_PAD_MODE=False are marked with ""-AlignPadding"".
# All other models under `ResNet/` in the model zoo are using TF_PAD_MODE=True.
# Using either one should probably give the same performance.
# We use the ""AlignPadding"" one just to be consistent with caffe2.
_C.BACKBONE.TF_PAD_MODE = False
_C.BACKBONE.STRIDE_1X1 = False  # True for MSRA models

# schedule -----------------------
_C.TRAIN.NUM_GPUS = 1         # by default, will be set from code
_C.TRAIN.WEIGHT_DECAY = 1e-4
_C.TRAIN.BASE_LR = 1e-2  # defined for total batch size=8. Otherwise it will be adjusted automatically
_C.TRAIN.WARMUP = 1000   # in terms of iterations. This is not affected by #GPUs
_C.TRAIN.WARMUP_INIT_LR = 1e-2 * 0.33  # defined for total batch size=8. Otherwise it will be adjusted automatically
_C.TRAIN.STEPS_PER_EPOCH = 500
_C.TRAIN.STARTING_EPOCH = 0  # the first epoch to start with, useful to continue a training

# LR_SCHEDULE means equivalent steps when the total batch size is 8.
# When the total bs!=8, the actual iterations to decrease learning rate, and
# the base learning rate are computed from BASE_LR and LR_SCHEDULE.
# Therefore, there is *no need* to modify the config if you only change the number of GPUs.

# _C.TRAIN.LR_SCHEDULE = [120000, 160000, 180000]      # ""1x"" schedule in detectron
_C.TRAIN.LR_SCHEDULE = [240000, 320000, 360000]      # ""2x"" schedule in detectron
# Longer schedules for from-scratch training (https://arxiv.org/abs/1811.08883):
# _C.TRAIN.LR_SCHEDULE = [960000, 1040000, 1080000]    # ""6x"" schedule in detectron
# _C.TRAIN.LR_SCHEDULE = [1500000, 1580000, 1620000]   # ""9x"" schedule in detectron
_C.TRAIN.EVAL_PERIOD = 0  # period (epochs) to run evaluation

# preprocessing --------------------
# Alternative old (worse & faster) setting: 600
_C.PREPROC.TRAIN_SHORT_EDGE_SIZE = [800, 800]  # [min, max] to sample from
_C.PREPROC.TEST_SHORT_EDGE_SIZE = 800
_C.PREPROC.MAX_SIZE = 1333
# mean and std in RGB order.
# Un-scaled version: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
_C.PREPROC.PIXEL_MEAN = [123.675, 116.28, 103.53]
_C.PREPROC.PIXEL_STD = [58.395, 57.12, 57.375]

# anchors -------------------------
_C.RPN.ANCHOR_STRIDE = 16
_C.RPN.ANCHOR_SIZES = (32, 64, 128, 256, 512)   # sqrtarea of the anchor box
_C.RPN.ANCHOR_RATIOS = (0.5, 1., 2.)
_C.RPN.POSITIVE_ANCHOR_THRESH = 0.7
_C.RPN.NEGATIVE_ANCHOR_THRESH = 0.3

# rpn training -------------------------
_C.RPN.FG_RATIO = 0.5  # fg ratio among selected RPN anchors
_C.RPN.BATCH_PER_IM = 256  # total (across FPN levels) number of anchors that are marked valid
_C.RPN.MIN_SIZE = 0
_C.RPN.PROPOSAL_NMS_THRESH = 0.7
# Anchors which overlap with a crowd box (IOA larger than threshold) will be ignored.
# Setting this to a value larger than 1.0 will disable the feature.
# It is disabled by default because Detectron does not do this.
_C.RPN.CROWD_OVERLAP_THRESH = 9.99
_C.RPN.HEAD_DIM = 1024      # used in C4 only

# RPN proposal selection -------------------------------
# for C4
_C.RPN.TRAIN_PRE_NMS_TOPK = 12000
_C.RPN.TRAIN_POST_NMS_TOPK = 2000
_C.RPN.TEST_PRE_NMS_TOPK = 6000
_C.RPN.TEST_POST_NMS_TOPK = 1000   # if you encounter OOM in inference, set this to a smaller number
# for FPN, #proposals per-level and #proposals after merging are (for now) the same
# if FPN.PROPOSAL_MODE = 'Joint', these options have no effect
_C.RPN.TRAIN_PER_LEVEL_NMS_TOPK = 2000
_C.RPN.TEST_PER_LEVEL_NMS_TOPK = 1000

# fastrcnn training ---------------------
_C.FRCNN.BATCH_PER_IM = 512
_C.FRCNN.BBOX_REG_WEIGHTS = [10., 10., 5., 5.]  # Better but non-standard setting: [20, 20, 10, 10]
_C.FRCNN.FG_THRESH = 0.5
_C.FRCNN.FG_RATIO = 0.25  # fg ratio in a ROI batch

# FPN -------------------------
_C.FPN.ANCHOR_STRIDES = (4, 8, 16, 32, 64)  # strides for each FPN level. Must be the same length as ANCHOR_SIZES
_C.FPN.PROPOSAL_MODE = 'Level'  # 'Level', 'Joint'
_C.FPN.NUM_CHANNEL = 256
_C.FPN.NORM = 'None'  # 'None', 'GN'
# The head option is only used in FPN. For C4 models, the head is C5
_C.FPN.FRCNN_HEAD_FUNC = 'fastrcnn_2fc_head'
# choices: fastrcnn_2fc_head, fastrcnn_4conv1fc_{,gn_}head
_C.FPN.FRCNN_CONV_HEAD_DIM = 256
_C.FPN.FRCNN_FC_HEAD_DIM = 1024
_C.FPN.MRCNN_HEAD_FUNC = 'maskrcnn_up4conv_head'   # choices: maskrcnn_up4conv_{,gn_}head

# Mask-RCNN
_C.MRCNN.HEAD_DIM = 256

# Cascade-RCNN, only available in FPN mode
_C.FPN.CASCADE = False
_C.CASCADE.IOUS = [0.5, 0.6, 0.7]
_C.CASCADE.BBOX_REG_WEIGHTS = [[10., 10., 5., 5.], [20., 20., 10., 10.], [30., 30., 15., 15.]]

# testing -----------------------
_C.TEST.FRCNN_NMS_THRESH = 0.5

# Smaller threshold value gives significantly better mAP. But we use 0.05 for consistency with Detectron.
# mAP with 1e-4 threshold can be found at https://github.com/tensorpack/tensorpack/commit/26321ae58120af2568bdbf2269f32aa708d425a8#diff-61085c48abee915b584027e1085e1043  # noqa
_C.TEST.RESULT_SCORE_THRESH = 0.05
_C.TEST.RESULT_SCORE_THRESH_VIS = 0.5   # only visualize confident results
_C.TEST.RESULTS_PER_IM = 100

_C.freeze()  # avoid typo / wrong config keys


then  i run train.py
that is all
thank you 
",try train data short alias avoid mode false true train training aka train separately instead aka two later loader without background class coco first whether absolute relative value false false train affine inside norm none use base model padding mode may pad see model zoo marked model zoo either one probably give performance use one consistent false false true schedule default set code defined total batch otherwise automatically affected defined total batch otherwise automatically first epoch start useful continue training equivalent total batch size total actual decrease learning rate base learning rate therefore need modify change number schedule schedule longer training schedule schedule period run evaluation alternative old worse faster setting min sample mean order version anchor box training ratio among selected total across number marked valid overlap crowd box threshold setting value disable feature disabled default used proposal selection encounter inference set smaller number effect training better setting ratio roi batch level must length head option used head head head available mode false testing smaller threshold value significantly better map use consistency map threshold found visualize confident avoid typo wrong run thank,issue,positive,negative,neutral,neutral,negative,negative
473757352,"Please include details following the issue template above:

> 1. What you did:
(1) If you're using examples, what's the command you run:
(2) If you're using examples, have you made any changes to the examples? Paste git diff here:
(3) If not using examples, tell us what you did:
It's always better to copy-paste what you did than to describe them.
Please try to provide enough information to let other reproduce your issues.
Without reproducing the issue, we may not be able to investigate it.",please include following issue template command run made paste git tell u always better describe please try provide enough information let reproduce without issue may able investigate,issue,positive,positive,positive,positive,positive,positive
473755226,"[0318 10:37:00 @logger.py:87] Argv: train.py
[0318 10:37:00 @config.py:279] Config: ------------------------------------------
{'BACKBONE': {'FREEZE_AFFINE': False,
              'FREEZE_AT': 2,
              'NORM': 'None',
              'RESNET_NUM_BLOCKS': [3, 4, 6, 3],
              'STRIDE_1X1': False,
              'TF_PAD_MODE': False,
              'WEIGHTS': '/home/zysz1/FasterRCNN/weights/ImageNet-R50-AlignPadding.npz'},
 'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0],
                                  [30.0, 30.0, 15.0, 15.0]],
             'IOUS': [0.5, 0.6, 0.7]},
 'DATA': {'ABSOLUTE_COORD': False,
          'BASEDIR': '/home/zysz1/FasterRCNN/coco',
          'CLASS_NAMES': ['BG', 'tiekedahuoji', 'heidingdahuoji', 'daoju', 'dianyuanhedianchi',
                          'jiandao'],
          'NUM_CATEGORY': 5,
          'NUM_CLASS': 6,
          'TRAIN': 'train',
          'VAL': ('val',)},
 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),
         'CASCADE': False,
         'FRCNN_CONV_HEAD_DIM': 256,
         'FRCNN_FC_HEAD_DIM': 1024,
         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',
         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',
         'NORM': 'None',
         'NUM_CHANNEL': 256,
         'PROPOSAL_MODE': 'Level',
         'RESOLUTION_REQUIREMENT': 32},
 'FRCNN': {'BATCH_PER_IM': 512,
           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],
           'FG_RATIO': 0.25,
           'FG_THRESH': 0.5},
 'MODE_FPN': True,
 'MODE_MASK': False,
 'MRCNN': {'HEAD_DIM': 256},
 'PREPROC': {'MAX_SIZE': 1344.0,
             'PIXEL_MEAN': [123.675, 116.28, 103.53],
             'PIXEL_STD': [58.395, 57.12, 57.375],
             'TEST_SHORT_EDGE_SIZE': 800,
             'TRAIN_SHORT_EDGE_SIZE': [800, 800]},
 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),
         'ANCHOR_SIZES': (32, 64, 128, 256, 512),
         'ANCHOR_STRIDE': 16,
         'BATCH_PER_IM': 256,
         'CROWD_OVERLAP_THRESH': 9.99,
         'FG_RATIO': 0.5,
         'HEAD_DIM': 1024,
         'MIN_SIZE': 0,
         'NEGATIVE_ANCHOR_THRESH': 0.3,
         'NUM_ANCHOR': 15,
         'POSITIVE_ANCHOR_THRESH': 0.7,
         'PROPOSAL_NMS_THRESH': 0.7,
         'TEST_PER_LEVEL_NMS_TOPK': 1000,
         'TEST_POST_NMS_TOPK': 1000,
         'TEST_PRE_NMS_TOPK': 6000,
         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,
         'TRAIN_POST_NMS_TOPK': 2000,
         'TRAIN_PRE_NMS_TOPK': 12000},
 'TEST': {'FRCNN_NMS_THRESH': 0.5,
          'RESULTS_PER_IM': 100,
          'RESULT_SCORE_THRESH': 0.05,
          'RESULT_SCORE_THRESH_VIS': 0.5},
 'TRAIN': {'BASE_LR': 0.01,
           'EVAL_PERIOD': 0,
           'LR_SCHEDULE': [240000, 320000, 360000],
           'NUM_GPUS': 1,
           'STARTING_EPOCH': 0,
           'STEPS_PER_EPOCH': 500,
           'WARMUP': 1000,
           'WARMUP_INIT_LR': 0.0033000000000000004,
           'WEIGHT_DECAY': 0.0001},
 'TRAINER': 'replicated'}
[0318 10:37:00 @train.py:472] Warm Up Schedule (steps, value): [(0, 0.0033000000000000004), (1000, 0.01)]
[0318 10:37:00 @train.py:473] LR Schedule (epochs, value): [(2, 0.01), (3840.0, 0.001), (5120.0, 0.00010000000000000002)]
loading annotations into memory...
Done (t=0.18s)
creating index...
index created!
[0318 10:37:00 @dataset.py:45] Instances loaded from /home/zysz1/FasterRCNN/coco/annotations/instances_train.json.
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 981/981 [00:00<00:00, 2638.87it/s]
[0318 10:37:00 @timer.py:48] Load Groundtruth Boxes for train finished, time:0.3739sec.
[0318 10:37:00 @data.py:49] Ground-Truth Boxes:
| class             |   #box |
|:------------------|-------:|
| BG                |      0 |
| tiekedahuoji      |    317 |
| heidingdahuoji    |   1729 |
| daoju             |    789 |
| dianyuanhedianchi |   1783 |
| jiandao           |    697 |
| total             |   5315 |
[0318 10:37:00 @data.py:294] Filtered 4 images which contain no non-crowd groudtruth boxes. Total #images for training: 977
[0318 10:37:00 @train.py:477] Total passes of the training set is: 2947.8
[0318 10:37:02 @input_source.py:220] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0318 10:37:02 @training.py:109] Building graph for training tower 0 on device /gpu:0 ...
[0318 10:37:02 @registry.py:125] conv0 input: [1, 3, None, None]
[0318 10:37:02 @registry.py:133] conv0 output: [1, 64, None, None]
[0318 10:37:02 @registry.py:125] pool0 input: [1, 64, None, None]
[0318 10:37:02 @registry.py:133] pool0 output: [1, 64, None, None]
[0318 10:37:02 @registry.py:125] group0/block0/conv1 input: [1, 64, None, None]
[0318 10:37:02 @registry.py:133] group0/block0/conv1 output: [1, 64, None, None]
[0318 10:37:02 @registry.py:125] group0/block0/conv2 input: [1, 64, None, None]
[0318 10:37:02 @registry.py:133] group0/block0/conv2 output: [1, 64, None, None]
[0318 10:37:02 @registry.py:125] group0/block0/conv3 input: [1, 64, None, None]
[0318 10:37:02 @registry.py:133] group0/block0/conv3 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group0/block0/convshortcut input: [1, 64, None, None]
[0318 10:37:02 @registry.py:133] group0/block0/convshortcut output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group0/block1/conv1 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group0/block1/conv1 output: [1, 64, None, None]
[0318 10:37:02 @registry.py:125] group0/block1/conv2 input: [1, 64, None, None]
[0318 10:37:02 @registry.py:133] group0/block1/conv2 output: [1, 64, None, None]
[0318 10:37:02 @registry.py:125] group0/block1/conv3 input: [1, 64, None, None]
[0318 10:37:02 @registry.py:133] group0/block1/conv3 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group0/block2/conv1 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group0/block2/conv1 output: [1, 64, None, None]
[0318 10:37:02 @registry.py:125] group0/block2/conv2 input: [1, 64, None, None]
[0318 10:37:02 @registry.py:133] group0/block2/conv2 output: [1, 64, None, None]
[0318 10:37:02 @registry.py:125] group0/block2/conv3 input: [1, 64, None, None]
[0318 10:37:02 @registry.py:133] group0/block2/conv3 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group1/block0/conv1 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group1/block0/conv1 output: [1, 128, None, None]
[0318 10:37:02 @registry.py:125] group1/block0/conv2 input: [1, 128, None, None]
[0318 10:37:02 @registry.py:133] group1/block0/conv2 output: [1, 128, None, None]
[0318 10:37:02 @registry.py:125] group1/block0/conv3 input: [1, 128, None, None]
[0318 10:37:02 @registry.py:133] group1/block0/conv3 output: [1, 512, None, None]
[0318 10:37:02 @registry.py:125] group1/block0/convshortcut input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group1/block0/convshortcut output: [1, 512, None, None]
[0318 10:37:02 @registry.py:125] group1/block1/conv1 input: [1, 512, None, None]
[0318 10:37:02 @registry.py:133] group1/block1/conv1 output: [1, 128, None, None]
[0318 10:37:02 @registry.py:125] group1/block1/conv2 input: [1, 128, None, None]
[0318 10:37:02 @registry.py:133] group1/block1/conv2 output: [1, 128, None, None]
[0318 10:37:02 @registry.py:125] group1/block1/conv3 input: [1, 128, None, None]
[0318 10:37:02 @registry.py:133] group1/block1/conv3 output: [1, 512, None, None]
[0318 10:37:02 @registry.py:125] group1/block2/conv1 input: [1, 512, None, None]
[0318 10:37:02 @registry.py:133] group1/block2/conv1 output: [1, 128, None, None]
[0318 10:37:02 @registry.py:125] group1/block2/conv2 input: [1, 128, None, None]
[0318 10:37:02 @registry.py:133] group1/block2/conv2 output: [1, 128, None, None]
[0318 10:37:02 @registry.py:125] group1/block2/conv3 input: [1, 128, None, None]
[0318 10:37:02 @registry.py:133] group1/block2/conv3 output: [1, 512, None, None]
[0318 10:37:02 @registry.py:125] group1/block3/conv1 input: [1, 512, None, None]
[0318 10:37:02 @registry.py:133] group1/block3/conv1 output: [1, 128, None, None]
[0318 10:37:02 @registry.py:125] group1/block3/conv2 input: [1, 128, None, None]
[0318 10:37:02 @registry.py:133] group1/block3/conv2 output: [1, 128, None, None]
[0318 10:37:02 @registry.py:125] group1/block3/conv3 input: [1, 128, None, None]
[0318 10:37:02 @registry.py:133] group1/block3/conv3 output: [1, 512, None, None]
[0318 10:37:02 @registry.py:125] group2/block0/conv1 input: [1, 512, None, None]
[0318 10:37:02 @registry.py:133] group2/block0/conv1 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block0/conv2 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block0/conv2 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block0/conv3 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block0/conv3 output: [1, 1024, None, None]
[0318 10:37:02 @registry.py:125] group2/block0/convshortcut input: [1, 512, None, None]
[0318 10:37:02 @registry.py:133] group2/block0/convshortcut output: [1, 1024, None, None]
[0318 10:37:02 @registry.py:125] group2/block1/conv1 input: [1, 1024, None, None]
[0318 10:37:02 @registry.py:133] group2/block1/conv1 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block1/conv2 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block1/conv2 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block1/conv3 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block1/conv3 output: [1, 1024, None, None]
[0318 10:37:02 @registry.py:125] group2/block2/conv1 input: [1, 1024, None, None]
[0318 10:37:02 @registry.py:133] group2/block2/conv1 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block2/conv2 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block2/conv2 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block2/conv3 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block2/conv3 output: [1, 1024, None, None]
[0318 10:37:02 @registry.py:125] group2/block3/conv1 input: [1, 1024, None, None]
[0318 10:37:02 @registry.py:133] group2/block3/conv1 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block3/conv2 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block3/conv2 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block3/conv3 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block3/conv3 output: [1, 1024, None, None]
[0318 10:37:02 @registry.py:125] group2/block4/conv1 input: [1, 1024, None, None]
[0318 10:37:02 @registry.py:133] group2/block4/conv1 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block4/conv2 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block4/conv2 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block4/conv3 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block4/conv3 output: [1, 1024, None, None]
[0318 10:37:02 @registry.py:125] group2/block5/conv1 input: [1, 1024, None, None]
[0318 10:37:02 @registry.py:133] group2/block5/conv1 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block5/conv2 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block5/conv2 output: [1, 256, None, None]
[0318 10:37:02 @registry.py:125] group2/block5/conv3 input: [1, 256, None, None]
[0318 10:37:02 @registry.py:133] group2/block5/conv3 output: [1, 1024, None, None]
[0318 10:37:02 @registry.py:125] group3/block0/conv1 input: [1, 1024, None, None]
[0318 10:37:02 @registry.py:133] group3/block0/conv1 output: [1, 512, None, None]
[0318 10:37:02 @registry.py:125] group3/block0/conv2 input: [1, 512, None, None]
[0318 10:37:03 @registry.py:133] group3/block0/conv2 output: [1, 512, None, None]
[0318 10:37:03 @registry.py:125] group3/block0/conv3 input: [1, 512, None, None]
[0318 10:37:03 @registry.py:133] group3/block0/conv3 output: [1, 2048, None, None]
[0318 10:37:03 @registry.py:125] group3/block0/convshortcut input: [1, 1024, None, None]
[0318 10:37:03 @registry.py:133] group3/block0/convshortcut output: [1, 2048, None, None]
[0318 10:37:03 @registry.py:125] group3/block1/conv1 input: [1, 2048, None, None]
[0318 10:37:03 @registry.py:133] group3/block1/conv1 output: [1, 512, None, None]
[0318 10:37:03 @registry.py:125] group3/block1/conv2 input: [1, 512, None, None]
[0318 10:37:03 @registry.py:133] group3/block1/conv2 output: [1, 512, None, None]
[0318 10:37:03 @registry.py:125] group3/block1/conv3 input: [1, 512, None, None]
[0318 10:37:03 @registry.py:133] group3/block1/conv3 output: [1, 2048, None, None]
[0318 10:37:03 @registry.py:125] group3/block2/conv1 input: [1, 2048, None, None]
[0318 10:37:03 @registry.py:133] group3/block2/conv1 output: [1, 512, None, None]
[0318 10:37:03 @registry.py:125] group3/block2/conv2 input: [1, 512, None, None]
[0318 10:37:03 @registry.py:133] group3/block2/conv2 output: [1, 512, None, None]
[0318 10:37:03 @registry.py:125] group3/block2/conv3 input: [1, 512, None, None]
[0318 10:37:03 @registry.py:133] group3/block2/conv3 output: [1, 2048, None, None]
[0318 10:37:03 @registry.py:125] fpn input: [1, 256, None, None],[1, 512, None, None],[1, 1024, None, None],[1, 2048, None, None]
[0318 10:37:03 @registry.py:125] fpn/lateral_1x1_c2 input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] fpn/lateral_1x1_c2 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/lateral_1x1_c3 input: [1, 512, None, None]
[0318 10:37:03 @registry.py:133] fpn/lateral_1x1_c3 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/lateral_1x1_c4 input: [1, 1024, None, None]
[0318 10:37:03 @registry.py:133] fpn/lateral_1x1_c4 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/lateral_1x1_c5 input: [1, 2048, None, None]
[0318 10:37:03 @registry.py:133] fpn/lateral_1x1_c5 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/upsample_lat5 input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] fpn/upsample_lat5 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/upsample_lat4 input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] fpn/upsample_lat4 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/upsample_lat3 input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] fpn/upsample_lat3 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/posthoc_3x3_p2 input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] fpn/posthoc_3x3_p2 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/posthoc_3x3_p3 input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] fpn/posthoc_3x3_p3 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/posthoc_3x3_p4 input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] fpn/posthoc_3x3_p4 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/posthoc_3x3_p5 input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] fpn/posthoc_3x3_p5 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] fpn/maxpool_p6 input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] fpn/maxpool_p6 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] fpn output: [1, 256, None, None],[1, 256, None, None],[1, 256, None, None],[1, 256, None, None],[1, 256, None, None]
[0318 10:37:03 @registry.py:125] rpn input: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] rpn/conv0 input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] rpn/conv0 output: [1, 256, None, None]
[0318 10:37:03 @registry.py:125] rpn/class input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] rpn/class output: [1, 3, None, None]
[0318 10:37:03 @registry.py:125] rpn/box input: [1, 256, None, None]
[0318 10:37:03 @registry.py:133] rpn/box output: [1, 12, None, None]
[0318 10:37:03 @registry.py:133] rpn output: [None, None, 3],[None, None, 3, 4]
[0318 10:37:08 @registry.py:125] fastrcnn input: [None, 256, 7, 7]
[0318 10:37:08 @registry.py:125] fastrcnn/fc6 input: [None, 256, 7, 7]
[0318 10:37:09 @registry.py:133] fastrcnn/fc6 output: [None, 1024]
[0318 10:37:09 @registry.py:125] fastrcnn/fc7 input: [None, 1024]
[0318 10:37:09 @registry.py:133] fastrcnn/fc7 output: [None, 1024]
[0318 10:37:09 @registry.py:133] fastrcnn output: [None, 1024]
[0318 10:37:09 @registry.py:125] fastrcnn/outputs input: [None, 1024]
[0318 10:37:09 @registry.py:125] fastrcnn/outputs/class input: [None, 1024]
[0318 10:37:09 @registry.py:133] fastrcnn/outputs/class output: [None, 6]
[0318 10:37:09 @registry.py:125] fastrcnn/outputs/box input: [None, 1024]
[0318 10:37:09 @registry.py:133] fastrcnn/outputs/box output: [None, 24]
[0318 10:37:09 @registry.py:133] fastrcnn/outputs output: [None, 6],[None, 6, 4]
[0318 10:37:09 @regularize.py:95] regularize_cost() found 57 variables to regularize.
[0318 10:37:09 @regularize.py:20] The following tensors will be regularized: group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/conv2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group2/block5/conv3/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, fpn/lateral_1x1_c2/W:0, fpn/lateral_1x1_c3/W:0, fpn/lateral_1x1_c4/W:0, fpn/lateral_1x1_c5/W:0, fpn/posthoc_3x3_p2/W:0, fpn/posthoc_3x3_p3/W:0, fpn/posthoc_3x3_p4/W:0, fpn/posthoc_3x3_p5/W:0, rpn/conv0/W:0, rpn/class/W:0, rpn/box/W:0, fastrcnn/fc6/W:0, fastrcnn/fc7/W:0, fastrcnn/outputs/class/W:0, fastrcnn/outputs/box/W:0
/home/zysz1/python3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
[0318 10:37:16 @training.py:347] 'sync_variables_from_main_tower' includes 0 operations.
[0318 10:37:16 @model_utils.py:64] Trainable Variables: 
name                            shape                    dim
------------------------------  ------------------  --------
group1/block0/conv1/W:0         [1, 1, 256, 128]       32768
group1/block0/conv2/W:0         [3, 3, 128, 128]      147456
group1/block0/conv3/W:0         [1, 1, 128, 512]       65536
group1/block0/convshortcut/W:0  [1, 1, 256, 512]      131072
group1/block1/conv1/W:0         [1, 1, 512, 128]       65536
group1/block1/conv2/W:0         [3, 3, 128, 128]      147456
group1/block1/conv3/W:0         [1, 1, 128, 512]       65536
group1/block2/conv1/W:0         [1, 1, 512, 128]       65536
group1/block2/conv2/W:0         [3, 3, 128, 128]      147456
group1/block2/conv3/W:0         [1, 1, 128, 512]       65536
group1/block3/conv1/W:0         [1, 1, 512, 128]       65536
group1/block3/conv2/W:0         [3, 3, 128, 128]      147456
group1/block3/conv3/W:0         [1, 1, 128, 512]       65536
group2/block0/conv1/W:0         [1, 1, 512, 256]      131072
group2/block0/conv2/W:0         [3, 3, 256, 256]      589824
group2/block0/conv3/W:0         [1, 1, 256, 1024]     262144
group2/block0/convshortcut/W:0  [1, 1, 512, 1024]     524288
group2/block1/conv1/W:0         [1, 1, 1024, 256]     262144
group2/block1/conv2/W:0         [3, 3, 256, 256]      589824
group2/block1/conv3/W:0         [1, 1, 256, 1024]     262144
group2/block2/conv1/W:0         [1, 1, 1024, 256]     262144
group2/block2/conv2/W:0         [3, 3, 256, 256]      589824
group2/block2/conv3/W:0         [1, 1, 256, 1024]     262144
group2/block3/conv1/W:0         [1, 1, 1024, 256]     262144
group2/block3/conv2/W:0         [3, 3, 256, 256]      589824
group2/block3/conv3/W:0         [1, 1, 256, 1024]     262144
group2/block4/conv1/W:0         [1, 1, 1024, 256]     262144
group2/block4/conv2/W:0         [3, 3, 256, 256]      589824
group2/block4/conv3/W:0         [1, 1, 256, 1024]     262144
group2/block5/conv1/W:0         [1, 1, 1024, 256]     262144
group2/block5/conv2/W:0         [3, 3, 256, 256]      589824
group2/block5/conv3/W:0         [1, 1, 256, 1024]     262144
group3/block0/conv1/W:0         [1, 1, 1024, 512]     524288
group3/block0/conv2/W:0         [3, 3, 512, 512]     2359296
group3/block0/conv3/W:0         [1, 1, 512, 2048]    1048576
group3/block0/convshortcut/W:0  [1, 1, 1024, 2048]   2097152
group3/block1/conv1/W:0         [1, 1, 2048, 512]    1048576
group3/block1/conv2/W:0         [3, 3, 512, 512]     2359296
group3/block1/conv3/W:0         [1, 1, 512, 2048]    1048576
group3/block2/conv1/W:0         [1, 1, 2048, 512]    1048576
group3/block2/conv2/W:0         [3, 3, 512, 512]     2359296
group3/block2/conv3/W:0         [1, 1, 512, 2048]    1048576
fpn/lateral_1x1_c2/W:0          [1, 1, 256, 256]       65536
fpn/lateral_1x1_c2/b:0          [256]                    256
fpn/lateral_1x1_c3/W:0          [1, 1, 512, 256]      131072
fpn/lateral_1x1_c3/b:0          [256]                    256
fpn/lateral_1x1_c4/W:0          [1, 1, 1024, 256]     262144
fpn/lateral_1x1_c4/b:0          [256]                    256
fpn/lateral_1x1_c5/W:0          [1, 1, 2048, 256]     524288
fpn/lateral_1x1_c5/b:0          [256]                    256
fpn/posthoc_3x3_p2/W:0          [3, 3, 256, 256]      589824
fpn/posthoc_3x3_p2/b:0          [256]                    256
fpn/posthoc_3x3_p3/W:0          [3, 3, 256, 256]      589824
fpn/posthoc_3x3_p3/b:0          [256]                    256
fpn/posthoc_3x3_p4/W:0          [3, 3, 256, 256]      589824
fpn/posthoc_3x3_p4/b:0          [256]                    256
fpn/posthoc_3x3_p5/W:0          [3, 3, 256, 256]      589824
fpn/posthoc_3x3_p5/b:0          [256]                    256
rpn/conv0/W:0                   [3, 3, 256, 256]      589824
rpn/conv0/b:0                   [256]                    256
rpn/class/W:0                   [1, 1, 256, 3]           768
rpn/class/b:0                   [3]                        3
rpn/box/W:0                     [1, 1, 256, 12]         3072
rpn/box/b:0                     [12]                      12
fastrcnn/fc6/W:0                [12544, 1024]       12845056
fastrcnn/fc6/b:0                [1024]                  1024
fastrcnn/fc7/W:0                [1024, 1024]         1048576
fastrcnn/fc7/b:0                [1024]                  1024
fastrcnn/outputs/class/W:0      [1024, 6]               6144
fastrcnn/outputs/class/b:0      [6]                        6
fastrcnn/outputs/box/W:0        [1024, 24]             24576
fastrcnn/outputs/box/b:0        [24]                      24
Total #vars=72, #params=41097261, size=156.77MB
[0318 10:37:16 @base.py:208] Setup callbacks graph ...
[0318 10:37:16 @argtools.py:146] WRN ""import prctl"" failed! Install python-prctl so that processes can be cleaned with guarantee.
[0318 10:37:17 @tower.py:130] Building graph for predict tower 'tower-pred-0' on device /gpu:0 ...
[0318 10:37:20 @collection.py:151] Size of these collections were changed in tower-pred-0: (tf.GraphKeys.MODEL_VARIABLES: 11->22)
loading annotations into memory...
Done (t=0.07s)
creating index...
index created!
[0318 10:37:20 @dataset.py:45] Instances loaded from /home/zysz1/FasterRCNN/coco/annotations/instances_val.json.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 981/981 [00:00<00:00, 57799.24it/s]
[0318 10:37:20 @timer.py:48] Load Groundtruth Boxes for val finished, time:0.0186sec.
loading annotations into memory...
Done (t=0.26s)
creating index...
index created!
[0318 10:37:20 @dataset.py:45] Instances loaded from /home/zysz1/FasterRCNN/coco/annotations/instances_val.json.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 981/981 [00:00<00:00, 70886.59it/s]
[0318 10:37:20 @timer.py:48] Load Groundtruth Boxes for val finished, time:0.0147sec.
[0318 10:37:20 @summary.py:46] [MovingAverageSummary] 69 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.
[0318 10:37:20 @summary.py:93] Summarizing collection 'summaries' of size 71.
[0318 10:37:24 @base.py:229] Creating the session ...
2019-03-18 10:37:24.298313: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-18 10:37:24.559692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:84:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2019-03-18 10:37:24.559834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2019-03-18 10:37:25.024130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-18 10:37:25.024187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2019-03-18 10:37:25.024198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2019-03-18 10:37:25.024542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11066 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)
[0318 10:37:31 @base.py:235] Initializing the session ...
[0318 10:37:31 @sessinit.py:204] Variables to restore from dict: group1/block2/conv3/W:0, group0/block2/conv1/W:0, group2/block2/conv1/W:0, group0/block0/conv2/W:0, group3/block1/conv2/W:0, group1/block0/convshortcut/W:0, group0/block0/conv3/W:0, group2/block1/conv3/W:0, group0/block1/conv2/W:0, group2/block4/conv3/W:0, group0/block1/conv3/W:0, group2/block0/conv2/W:0, group1/block0/conv2/W:0, group3/block2/conv3/W:0, group2/block1/conv1/W:0, group2/block2/conv2/W:0, group3/block0/conv1/W:0, group2/block4/conv2/W:0, group1/block1/conv2/W:0, group3/block0/conv3/W:0, group0/block2/conv2/W:0, group1/block1/conv1/W:0, group0/block0/convshortcut/W:0, group0/block2/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group3/block0/conv2/W:0, group1/block3/conv2/W:0, group3/block1/conv1/W:0, group3/block2/conv2/W:0, group2/block3/conv2/W:0, group2/block0/convshortcut/W:0, group2/block3/conv1/W:0, group1/block0/conv3/W:0, group2/block0/conv1/W:0, group2/block4/conv1/W:0, group3/block0/convshortcut/W:0, group2/block5/conv3/W:0, group1/block2/conv1/W:0, conv0/W:0, group1/block3/conv3/W:0, group1/block3/conv1/W:0, group0/block0/conv1/W:0, group3/block1/conv3/W:0, group2/block3/conv3/W:0, group1/block2/conv2/W:0, group1/block0/conv1/W:0, group2/block0/conv3/W:0, group1/block1/conv3/W:0, group0/block1/conv1/W:0, group2/block1/conv2/W:0, group3/block2/conv1/W:0, group2/block2/conv3/W:0
[0318 10:37:31 @sessinit.py:87] WRN The following variables are in the graph, but not found in the dict: fastrcnn/fc6/W, fastrcnn/fc6/b, fastrcnn/fc7/W, fastrcnn/fc7/b, fastrcnn/outputs/box/W, fastrcnn/outputs/box/b, fastrcnn/outputs/class/W, fastrcnn/outputs/class/b, fpn/lateral_1x1_c2/W, fpn/lateral_1x1_c2/b, fpn/lateral_1x1_c3/W, fpn/lateral_1x1_c3/b, fpn/lateral_1x1_c4/W, fpn/lateral_1x1_c4/b, fpn/lateral_1x1_c5/W, fpn/lateral_1x1_c5/b, fpn/posthoc_3x3_p2/W, fpn/posthoc_3x3_p2/b, fpn/posthoc_3x3_p3/W, fpn/posthoc_3x3_p3/b, fpn/posthoc_3x3_p4/W, fpn/posthoc_3x3_p4/b, fpn/posthoc_3x3_p5/W, fpn/posthoc_3x3_p5/b, global_step, learning_rate, rpn/box/W, rpn/box/b, rpn/class/W, rpn/class/b, rpn/conv0/W, rpn/conv0/b
[0318 10:37:31 @sessinit.py:87] WRN The following variables are in the dict, but not found in the graph: conv0/bn/beta, conv0/bn/gamma, conv0/bn/mean/EMA, conv0/bn/variance/EMA, group0/block0/conv1/bn/beta, group0/block0/conv1/bn/gamma, group0/block0/conv1/bn/mean/EMA, group0/block0/conv1/bn/variance/EMA, group0/block0/conv2/bn/beta, group0/block0/conv2/bn/gamma, group0/block0/conv2/bn/mean/EMA, group0/block0/conv2/bn/variance/EMA, group0/block0/conv3/bn/beta, group0/block0/conv3/bn/gamma, group0/block0/conv3/bn/mean/EMA, group0/block0/conv3/bn/variance/EMA, group0/block0/convshortcut/bn/beta, group0/block0/convshortcut/bn/gamma, group0/block0/convshortcut/bn/mean/EMA, group0/block0/convshortcut/bn/variance/EMA, group0/block1/conv1/bn/beta, group0/block1/conv1/bn/gamma, group0/block1/conv1/bn/mean/EMA, group0/block1/conv1/bn/variance/EMA, group0/block1/conv2/bn/beta, group0/block1/conv2/bn/gamma, group0/block1/conv2/bn/mean/EMA, group0/block1/conv2/bn/variance/EMA, group0/block1/conv3/bn/beta, group0/block1/conv3/bn/gamma, group0/block1/conv3/bn/mean/EMA, group0/block1/conv3/bn/variance/EMA, group0/block2/conv1/bn/beta, group0/block2/conv1/bn/gamma, group0/block2/conv1/bn/mean/EMA, group0/block2/conv1/bn/variance/EMA, group0/block2/conv2/bn/beta, group0/block2/conv2/bn/gamma, group0/block2/conv2/bn/mean/EMA, group0/block2/conv2/bn/variance/EMA, group0/block2/conv3/bn/beta, group0/block2/conv3/bn/gamma, group0/block2/conv3/bn/mean/EMA, group0/block2/conv3/bn/variance/EMA, group1/block0/conv1/bn/beta, group1/block0/conv1/bn/gamma, group1/block0/conv1/bn/mean/EMA, group1/block0/conv1/bn/variance/EMA, group1/block0/conv2/bn/beta, group1/block0/conv2/bn/gamma, group1/block0/conv2/bn/mean/EMA, group1/block0/conv2/bn/variance/EMA, group1/block0/conv3/bn/beta, group1/block0/conv3/bn/gamma, group1/block0/conv3/bn/mean/EMA, group1/block0/conv3/bn/variance/EMA, group1/block0/convshortcut/bn/beta, group1/block0/convshortcut/bn/gamma, group1/block0/convshortcut/bn/mean/EMA, group1/block0/convshortcut/bn/variance/EMA, group1/block1/conv1/bn/beta, group1/block1/conv1/bn/gamma, group1/block1/conv1/bn/mean/EMA, group1/block1/conv1/bn/variance/EMA, group1/block1/conv2/bn/beta, group1/block1/conv2/bn/gamma, group1/block1/conv2/bn/mean/EMA, group1/block1/conv2/bn/variance/EMA, group1/block1/conv3/bn/beta, group1/block1/conv3/bn/gamma, group1/block1/conv3/bn/mean/EMA, group1/block1/conv3/bn/variance/EMA, group1/block2/conv1/bn/beta, group1/block2/conv1/bn/gamma, group1/block2/conv1/bn/mean/EMA, group1/block2/conv1/bn/variance/EMA, group1/block2/conv2/bn/beta, group1/block2/conv2/bn/gamma, group1/block2/conv2/bn/mean/EMA, group1/block2/conv2/bn/variance/EMA, group1/block2/conv3/bn/beta, group1/block2/conv3/bn/gamma, group1/block2/conv3/bn/mean/EMA, group1/block2/conv3/bn/variance/EMA, group1/block3/conv1/bn/beta, group1/block3/conv1/bn/gamma, group1/block3/conv1/bn/mean/EMA, group1/block3/conv1/bn/variance/EMA, group1/block3/conv2/bn/beta, group1/block3/conv2/bn/gamma, group1/block3/conv2/bn/mean/EMA, group1/block3/conv2/bn/variance/EMA, group1/block3/conv3/bn/beta, group1/block3/conv3/bn/gamma, group1/block3/conv3/bn/mean/EMA, group1/block3/conv3/bn/variance/EMA, group2/block0/conv1/bn/beta, group2/block0/conv1/bn/gamma, group2/block0/conv1/bn/mean/EMA, group2/block0/conv1/bn/variance/EMA, group2/block0/conv2/bn/beta, group2/block0/conv2/bn/gamma, group2/block0/conv2/bn/mean/EMA, group2/block0/conv2/bn/variance/EMA, group2/block0/conv3/bn/beta, group2/block0/conv3/bn/gamma, group2/block0/conv3/bn/mean/EMA, group2/block0/conv3/bn/variance/EMA, group2/block0/convshortcut/bn/beta, group2/block0/convshortcut/bn/gamma, group2/block0/convshortcut/bn/mean/EMA, group2/block0/convshortcut/bn/variance/EMA, group2/block1/conv1/bn/beta, group2/block1/conv1/bn/gamma, group2/block1/conv1/bn/mean/EMA, group2/block1/conv1/bn/variance/EMA, group2/block1/conv2/bn/beta, group2/block1/conv2/bn/gamma, group2/block1/conv2/bn/mean/EMA, group2/block1/conv2/bn/variance/EMA, group2/block1/conv3/bn/beta, group2/block1/conv3/bn/gamma, group2/block1/conv3/bn/mean/EMA, group2/block1/conv3/bn/variance/EMA, group2/block2/conv1/bn/beta, group2/block2/conv1/bn/gamma, group2/block2/conv1/bn/mean/EMA, group2/block2/conv1/bn/variance/EMA, group2/block2/conv2/bn/beta, group2/block2/conv2/bn/gamma, group2/block2/conv2/bn/mean/EMA, group2/block2/conv2/bn/variance/EMA, group2/block2/conv3/bn/beta, group2/block2/conv3/bn/gamma, group2/block2/conv3/bn/mean/EMA, group2/block2/conv3/bn/variance/EMA, group2/block3/conv1/bn/beta, group2/block3/conv1/bn/gamma, group2/block3/conv1/bn/mean/EMA, group2/block3/conv1/bn/variance/EMA, group2/block3/conv2/bn/beta, group2/block3/conv2/bn/gamma, group2/block3/conv2/bn/mean/EMA, group2/block3/conv2/bn/variance/EMA, group2/block3/conv3/bn/beta, group2/block3/conv3/bn/gamma, group2/block3/conv3/bn/mean/EMA, group2/block3/conv3/bn/variance/EMA, group2/block4/conv1/bn/beta, group2/block4/conv1/bn/gamma, group2/block4/conv1/bn/mean/EMA, group2/block4/conv1/bn/variance/EMA, group2/block4/conv2/bn/beta, group2/block4/conv2/bn/gamma, group2/block4/conv2/bn/mean/EMA, group2/block4/conv2/bn/variance/EMA, group2/block4/conv3/bn/beta, group2/block4/conv3/bn/gamma, group2/block4/conv3/bn/mean/EMA, group2/block4/conv3/bn/variance/EMA, group2/block5/conv1/bn/beta, group2/block5/conv1/bn/gamma, group2/block5/conv1/bn/mean/EMA, group2/block5/conv1/bn/variance/EMA, group2/block5/conv2/bn/beta, group2/block5/conv2/bn/gamma, group2/block5/conv2/bn/mean/EMA, group2/block5/conv2/bn/variance/EMA, group2/block5/conv3/bn/beta, group2/block5/conv3/bn/gamma, group2/block5/conv3/bn/mean/EMA, group2/block5/conv3/bn/variance/EMA, group3/block0/conv1/bn/beta, group3/block0/conv1/bn/gamma, group3/block0/conv1/bn/mean/EMA, group3/block0/conv1/bn/variance/EMA, group3/block0/conv2/bn/beta, group3/block0/conv2/bn/gamma, group3/block0/conv2/bn/mean/EMA, group3/block0/conv2/bn/variance/EMA, group3/block0/conv3/bn/beta, group3/block0/conv3/bn/gamma, group3/block0/conv3/bn/mean/EMA, group3/block0/conv3/bn/variance/EMA, group3/block0/convshortcut/bn/beta, group3/block0/convshortcut/bn/gamma, group3/block0/convshortcut/bn/mean/EMA, group3/block0/convshortcut/bn/variance/EMA, group3/block1/conv1/bn/beta, group3/block1/conv1/bn/gamma, group3/block1/conv1/bn/mean/EMA, group3/block1/conv1/bn/variance/EMA, group3/block1/conv2/bn/beta, group3/block1/conv2/bn/gamma, group3/block1/conv2/bn/mean/EMA, group3/block1/conv2/bn/variance/EMA, group3/block1/conv3/bn/beta, group3/block1/conv3/bn/gamma, group3/block1/conv3/bn/mean/EMA, group3/block1/conv3/bn/variance/EMA, group3/block2/conv1/bn/beta, group3/block2/conv1/bn/gamma, group3/block2/conv1/bn/mean/EMA, group3/block2/conv1/bn/variance/EMA, group3/block2/conv2/bn/beta, group3/block2/conv2/bn/gamma, group3/block2/conv2/bn/mean/EMA, group3/block2/conv2/bn/variance/EMA, group3/block2/conv3/bn/beta, group3/block2/conv3/bn/gamma, group3/block2/conv3/bn/mean/EMA, group3/block2/conv3/bn/variance/EMA, linear/W, linear/b
[0318 10:37:31 @sessinit.py:217] Restoring 53 variables from dict ...
[0318 10:37:39 @base.py:242] Graph Finalized.
[0318 10:37:39 @concurrency.py:38] Starting EnqueueThread QueueInput/input_queue ...
[0318 10:37:39 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
 
it doesnt make any error  it just stoped  like this  ..
thank you 
",false false false false false true false warm schedule value schedule value loading memory done index index loaded load train finished time class box total contain total training total training set setting queue building graph training tower device input none none output none none pool input none none pool output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none none none none none none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none input none none output none none output none none none none none none none none none none input none none input none none output none none input none none output none none input none none output none none output none none none none input none input none output none input none output none output none input none input none output none input none output none output none none found regularize following converting sparse dense tensor unknown shape may consume large amount memory converting sparse dense tensor unknown trainable name shape dim total setup graph import install guarantee building graph predict tower device size loading memory done index index loaded load finished time loading memory done index index loaded load finished time collection run session collection size session binary use found device name ti major minor visible device interconnect strength edge matrix device memory physical device name ti bus id compute capability session restore following graph found following found graph graph starting running doesnt make error like thank,issue,positive,negative,neutral,neutral,negative,negative
473753890,"Since that is a different issue, please post details following the issue template.
Please include full logs whenever possible.",since different issue please post following issue template please include full whenever possible,issue,positive,positive,neutral,neutral,positive,positive
473745775,"![image](https://user-images.githubusercontent.com/42901810/54502705-07a19580-4967-11e9-8fc8-426aa8553d85.png)
sorry for bother you again.. it stopped  there for more than ten minutes ,what is the problem?",image sorry bother stopped ten problem,issue,negative,negative,negative,negative,negative,negative
473743614,"Sure, I just upgrade tensorflow to 1.12, and now testing whether the problem still exists or gone.",sure upgrade testing whether problem still gone,issue,negative,positive,positive,positive,positive,positive
473738973,"![0317-18:29:42](https://user-images.githubusercontent.com/1381301/54501464-a932d380-48e2-11e9-9a90-c75f94ccb841.png)
As said in the image, the data loader will fill out these two config items. This is implemented in `DetectionDataset.__init__` in `dataset.py`.

The correct way to train on your data is written in [NOTES.md](https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md#implementation-notes).",said image data loader fill two correct way train data written,issue,negative,neutral,neutral,neutral,neutral,neutral
473734539,"![image](https://user-images.githubusercontent.com/42901810/54500808-9dcfbe80-495b-11e9-9276-6bbd90def9ea.png)
![image](https://user-images.githubusercontent.com/42901810/54500813-b344e880-495b-11e9-8eb5-2b37c9b0b62f.png)
i changed configs like this  ,but you can see it doesnt changed",image image like see doesnt,issue,negative,neutral,neutral,neutral,neutral,neutral
473698066,"Latest tensorpack (353cd04fe81ea8920bc67f702bf492174b000768) now avoids the bug in `tf.layers`. If you use latest tensorpack, dilated conv should work now.",latest bug use latest dilated work,issue,negative,positive,positive,positive,positive,positive
473688775,It is worth trying to upgrade tensorflow. I've seen some weird errors (unlike yours) in the past but haven't seen any recently.,worth trying upgrade seen weird unlike past seen recently,issue,negative,negative,negative,negative,negative,negative
473683246,"I do not understand this question. If you're asking about an unexpected problem which you do not know the root cause, use the template you posted.",understand question unexpected problem know root cause use template posted,issue,negative,positive,neutral,neutral,positive,positive
473621711,Closing as this is not a tensorpack issue. Feel free to reopen if you have more questions.,issue feel free reopen,issue,positive,positive,positive,positive,positive,positive
473615756,"
### 1. What I did:

(1) This is the command I run: nohup ./train.py --config MODE_MASK=True MODE_FPN=True DATA.BASEDIR=/COCO/DIR BACKBONE.WEIGHTS=ImageNet-R50-AlignPadding.npz &

(2) **If you're using examples, have you made any changes to the examples? Paste `git diff` here:**
I'm using examples/FasterRcnn, and in the code I only changed one line as below: 
(mode_rpn.py in rpn_head() function)
change 
`hidden = Conv2D('conv0', featuremap, channel, 3, activation=tf.nn.relu)`
to
`hidden = Conv2D('conv0', featuremap, channel, 3, dilation_rate=2, activation=tf.nn.relu, padding='SAME')`
### 2. What you observed:

(1) **Include the ENTIRE logs here:**
Here is the logs that I observed that is different from before:
![image](https://user-images.githubusercontent.com/46680345/54485188-4d028c00-48af-11e9-8829-8c2d32624d5e.png)

![image](https://user-images.githubusercontent.com/46680345/54485190-69062d80-48af-11e9-91be-57d8a74aff65.png)

![image](https://user-images.githubusercontent.com/46680345/54485193-6f94a500-48af-11e9-9715-548af7a28973.png)

![image](https://user-images.githubusercontent.com/46680345/54485194-76bbb300-48af-11e9-81d0-caeff48d3c0d.png)

(2) **Other observations, if any:**
I used two GPUs(GTX 1080)
### 3. What you expected, if not obvious.
  What we want is to try to use atrous convolution to see anything change, we konw that atrous convlution has larger recption feild without needing redundant variables.

### 4. Your environment:
  + Python version: 3.6
  + TF version: 1.9
  + Tensorpack version: latest version.
      You can install Tensorpack master by `pip install -U git+https://github.com/ppwwyyxx/tensorpack.git`
      and see if your issue is already solved.
  + If you're not using tensorpack under a normal command line shell (e.g.,
    using an IDE or jupyter notebook), please retry under a normal command line shell. 
  + Hardware information: two GPUS
You may often want to provide extra information related to your issue, but
at the minimum please try to provide the above information __accurately__ to save effort in the investigation.
",command run made paste git code one line function change hidden channel hidden channel include entire different image image image image used two obvious want try use atrous convolution see anything change atrous without needing redundant environment python version version version latest version install master pip install see issue already normal command line shell ide notebook please retry normal command line shell hardware information two may often want provide extra information related issue minimum please try provide information save effort investigation,issue,positive,positive,neutral,neutral,positive,positive
473615682,"I did not make any other changes to any files under the `example` path.
It's weird.... Just that I have used tensorflow to develop for like more than 3 years, but haven't meet this kind of thing. 
Maybe there are some implicit problems in my machine, really have no idea...
But right now, I could just restart it every 220 epochs, it won't make any trouble.",make example path weird used develop like meet kind thing maybe implicit machine really idea right could restart every wo make trouble,issue,negative,positive,neutral,neutral,positive,positive
473614407,"That is weird. Did you make other changes (about trainer or data)?
I have not seen this with the default trainer and don't have an idea why this may happen.",weird make trainer data seen default trainer idea may happen,issue,negative,negative,negative,negative,negative,negative
473606313,"1. As I said: 
For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

2. No that's not how to print things in tensorflow. As I said, to print tensor values during training, the methods are in [the FAQ](https://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training). You cannot print tensor values before training because they have no values at that time.",said anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected print said print tensor training print tensor training time,issue,positive,positive,positive,positive,positive,positive
473605757,"Hi ppwwyyxx,
  1.In the training pharse, I only modified dilation_rate in model_rpn.py(rpn_head function): hidden = Conv2D('conv0', featuremap, channel, 3, dilation_rate=2, activation=tf.nn.relu, padding='SAME').But it went wrong in  ### valid_label_logits = tf.boolean_mask(label_logits, valid_mask)### in model_rpn.py(rpn_losses function),and the error message is InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 11400 values, but the requested shape has 196608.
  2.I try to use session to print the value of multilevel_anchors and multilevel_label_logits in model_fpn.py(multilevel_rpn_losses function),which are input into the rpn_losses.Here is my print process, but there is nothing printed:
with tf.device('/gpu:4'):
            #a = tf.Variable([1,2,3,4,5])
            print('在multilevel_rpn_losses中打印tensor.....................................')
            config = tf.ConfigProto(allow_soft_placement=True)
            config.gpu_options.allow_growth = True
            sess = tf.Session(config=config)
            print(sess.run(multilevel_anchors[0]))
            sess.close()
Thank you for your patience.",hi training function hidden channel went wrong function error message see input reshape tensor shape try use session print value function input print process nothing printed print true sess print thank patience,issue,negative,negative,negative,negative,negative,negative
473569166,"1. Without runnable code there is nothing I can tell from this error message. It's likely that your modified code produces incompatible shape with other tensors.

For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

2. I'm not sure I understand what that means. If you just want to print tensor values during training, it is in [the FAQ](https://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training)",without runnable code nothing tell error message likely code incompatible shape anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected sure understand want print tensor training,issue,positive,positive,positive,positive,positive,positive
473159261,"What we do is the same as what the paper and the official implementation do. As a result it certainly can work.

I have no knowledge of google's object detection API and why it cannot work is not something for us to answer. ",paper official implementation result certainly work knowledge object detection work something u answer,issue,negative,positive,positive,positive,positive,positive
472578182,"I see. It seems I haven't done my homework on Group Normalization. I had a fundamental misunderstanding that it could be ignored without implications to the rest of the model, similar to BN. Thank you for the clarification! I appreciate it.

I was a little surprised to see the difference in inference performance between GN and non-GN models. Moving from a standard FPN model to a GN-FPN model, I'm seeing an average increase of ~70% in inference time.  Further reading suggests this isn't outlandish.

For now I will predict with a GN config, until i have retrained without GN. 

Thanks again!",see done homework group normalization fundamental misunderstanding could without rest model similar thank clarification appreciate little see difference inference performance moving standard model model seeing average increase inference time reading outlandish predict without thanks,issue,negative,negative,neutral,neutral,negative,negative
472572037,">  I am left with the impression that the weights of the standard FPN model should still work when restored from a pre-trained checkpoint resulting from a GN config.... where the inputs to the GN layers would be redirected to the outputs of the GN layers, but maintain the same tensor names... effectively bypassing the GN layers, and ignoring the associated weights. Temporarily ignoring the discussion of how to properly load the weights, is this assumption correct? Or am I misunderstanding the implementation of the model architecture?

What you said about the architecture is correct. This means the model will load properly. This does not mean the model will predict properly.

> Moving on with the assumption that a standard FPN model will support weights trained with a GN config

Same as above. The weights will load like you did. The weights will not predict properly, and there is no way to make it so.

> Does tensorpack provides a means of restoring a model given the architecture defined by the model provided in the Config

In tensorpack, restoring a checkpoint means loading the variables in the checkpoint to the graph, and that's what it is.
I do not understand what ""restoring a model given the architecture defined by the model provided in the Config"" means exactly. It's better if you would clarify what is the desired behavior.

> hoping that the internal SessionUpdate is effectively restoring intersecting tensors by name. 

It is already ""restoring intersecting variables by name"", no matter whether you use `DictRestore` or `SaverRestore`.",left impression standard model still work resulting would maintain tensor effectively associated temporarily discussion properly load assumption correct misunderstanding implementation model architecture said architecture correct model load properly mean model predict properly moving assumption standard model support trained load like predict properly way make model given architecture defined model provided loading graph understand model given architecture defined model provided exactly better would clarify desired behavior internal effectively name already name matter whether use,issue,positive,positive,positive,positive,positive,positive
472497749,"The function does not contain mask visualization.
`--predict` contains it.",function contain mask visualization predict,issue,negative,neutral,neutral,neutral,neutral,neutral
472189559,"Also, if you're using your own models or datasets, then how to make your training converge is not our responsibility, unless there is a reason to think it is a tensorpack bug.

You can see these in the issue template when you file an issue:

From https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/unexpected-problems---bugs.md:
> If you expect higher accuracy, only in one of the two conditions can we help with it: (1) You're unable to match the accuracy documented in tensorpack examples. (2) It appears to be a tensorpack bug.
Otherwise, how to get high accuracy is a machine learning question and is not our responsibility to figure out.

From https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md
> ""The examples do not perform as expected after I change the models/dataset/parameters/etc.""
Tensorpack maintainers make sure the examples perform well without modifications. But it's your job to pick the model and parameters that are suitable for your own situation. We cannot help with such questions unless they appear to be a bug in tensorpack. ",also make training converge responsibility unless reason think bug see issue template file issue expect higher accuracy one two help unable match accuracy bug otherwise get high accuracy machine learning question responsibility figure perform change make sure perform well without job pick model suitable situation help unless appear bug,issue,positive,positive,positive,positive,positive,positive
472152041,"Had a recent issue of this nature, and did not find any direct explanation so leaving this brief comment in case it may help someone. I had an Anaconda 2018.12 Python 3.7.1 environment (not virtual) and attempting to import tensorpack 0.9.1 that was installed via pip but not using --user option. tensorpack would import in python started from command line, and via jupyter notebook, but not in VS2017 community or VS code. A virtual env with Python 3.6.8 was the answer, though not sure if either or both. ",recent issue nature find direct explanation leaving brief comment case may help someone anaconda python environment virtual import via pip user option would import python command line via notebook community code virtual python answer though sure either,issue,positive,positive,positive,positive,positive,positive
472099735,"If it is really stuck at this line:
https://github.com/tensorpack/tensorpack/blob/f42036acc675dbbff86e25f25506ccf4bff30f67/tensorpack/train/tower.py#L261
then it is a tensorflow issue.

As you can probably verify yourself, that using `tf.gradients` is OK in most normal cases (e.g., if you take a simple example and add gradient regularizer). As a result I cannot reproduce your issue.
Could you provide something that others can run and reproduce the same issue? This is needed no matter where (tensorflow or tensorpack) you report this bug to.",really stuck line issue probably verify normal take simple example add gradient regularizer result reproduce issue could provide something run reproduce issue matter report bug,issue,negative,positive,positive,positive,positive,positive
471959470,"@ppwwyyxx . I updated more information, please take a look",information please take look,issue,negative,neutral,neutral,neutral,neutral,neutral
471629340,"What happened in your log is that the data becomes slower in the 2nd epoch.
It's unlikely that batch size is the issue unless your batch size is different in each epoch. I would recommend you dig into it further.",log data becomes epoch unlikely batch size issue unless batch size different epoch would recommend dig,issue,negative,negative,negative,negative,negative,negative
471537122,"I found the problem of batch size. Previously, each gpu batch_size was 64, now it is 32. It can run very well.",found problem batch size previously run well,issue,negative,negative,negative,negative,negative,negative
471438283,There seems to be some setting issues with the website. please use https://tensorpack.readthedocs.io/tutorial/performance-tuning.html or https://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html: at least one of the two should work.,setting please use least one two work,issue,negative,negative,negative,negative,negative,negative
471433356,"As the issue template said:

> If you expect higher speed, please first read https://tensorpack.readthedocs.io/tutorial/performance-tuning.html

It can be seen from the log that your data gets slower in the 2nd epoch and you may want to investigate why.",issue template said expect higher speed please first read seen log data epoch may want investigate,issue,negative,positive,positive,positive,positive,positive
471275328,Opened an issue in tensorflow (https://github.com/tensorflow/tensorflow/issues/26278) since there is a chance it gets fixed in TF 2.,issue since chance fixed,issue,negative,positive,neutral,neutral,positive,positive
470785726,"Hello, the mnist example link is not available, can you reput the link?
I cannot find the example of this usage
Thanks in advance",hello example link available link find example usage thanks advance,issue,negative,positive,positive,positive,positive,positive
470738671,"Edit: added 4th case above, which I think verifies this is related to my system, so I'll close the issue again... ",edit added th case think related system close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
470727956,"Given that in all cases, testing the dataflow directly does not show rising memory usage, I would have assumed the same. However I'm witnessing the following, when testing on my own local machine:

#### 1) Default Example
Training from a fresh tensorpack repo using COCO data.
Results in stable memory consumption around ~40 GB or so during training. 

#### 2) Training with Custom dataflow, MultiProcessMapDataZMQ
Unmodified code, apart from a custom dataflow.
Results in memory overflow > 64GB, regardless of params supplied to `MultiProcessMapDataZMQ`

#### 3) Training with Custom dataflow, MapData
Unmodified code, apart from a custom dataflow. `MultiProcessMapDataZMQ` is replaced with `MapData`.
Results in staple memory consumption around ~17GB  or so during training.

#### 4) Training with Custom dataflow, MultiProcessMapDataZMQ, Docker
Repeat of **2** but running in a docker container.
Results in stable memory consumption < 20GB

Given the following:
- large difference in memory consumption between your experiment and my own using the default example (case **1** above) 
- only difference between the above cases **2** and **3**, is the map function, and yet they each yield dramatically different memory usage during training
- only difference between **2** and **4** is my local os vs fresh ubuntu in a docker container

I would think there could be something unexpected about how my system is running `MultiProcessMapDataZMQ`. Any recommendations for further debugging?",given testing directly show rising memory usage would assumed however following testing local machine default example training fresh coco data stable memory consumption around training training custom unmodified code apart custom memory overflow regardless training custom unmodified code apart custom staple memory consumption around training training custom docker repeat running docker container stable memory consumption given following large difference memory consumption experiment default example case difference map function yet yield dramatically different memory usage training difference local o fresh docker container would think could something unexpected system running,issue,positive,positive,positive,positive,positive,positive
470721144,"The memory usage should not rise during training and you can probably see that it does not rise (and in my experiment, stays <20GB with multiprocessing) with the examples unmodified.

If running the dataflow alone does not increase memory usage, dataflow is probably unrelated to the growth in memory you observed.",memory usage rise training probably see rise experiment stay unmodified running alone increase memory usage probably unrelated growth memory,issue,negative,neutral,neutral,neutral,neutral,neutral
470713999,"@ppwwyyxx  thanks for the suggestion.

I experimented with different params in `MultiProcessMapDataZMQ`. However, even with small values (`nr_proc=2` + `buffer_size=5`), I still saw rising memory usage until overflow during training.

I know referring to exact numbers is difficult with custom data... but do the following *relative* values make sense? That is to say, is it expected that a call like:

```python
ds = MultiProcessMapDataZMQ(ds, nr_proc=10, map_func=preprocess, buffer_size=5)
```
could yield > 64GB of memory usage during training, while the following call:

```python
ds = MapData(ds, preprocess)
```
yields a stable ~17Gb.

Additionally, why is it that memory usage would rise during training, but not when testing the dataflow in isolation, like so:
```python
ds = get_train_dataflow()

ds = PrintData(ds, 100)
TestDataSpeed(ds, 50000).start()
ds.reset_state()
for k in ds:
    pass
```",thanks suggestion experimented different however even small still saw rising memory usage overflow training know exact difficult custom data following relative make sense say call like python could yield memory usage training following call python stable additionally memory usage would rise training testing isolation like python pas,issue,positive,negative,neutral,neutral,negative,negative
470587639,"It can be used and there are examples that use it.
For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",used use anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected,issue,positive,positive,positive,positive,positive,positive
469832898,`MultiProcessMapDataZMQ` has a `buffer_size` option which can control the memory consumption.,option control memory consumption,issue,negative,neutral,neutral,neutral,neutral,neutral
469825691,"Training from a fresh tensorpack repo using COCO data results in stable memory consumption around ~40 GBs or so. With my own data, I could not avoid memory saturation when using `MultiProcessMapDataZMQ`. I switched to `MapData` instead which sits stable at ~17Gb. I'm closing for now as this is likely something specific to my own data preprocessing. Thanks again.",training fresh coco data stable memory consumption around data could avoid memory saturation switched instead stable likely something specific data thanks,issue,positive,positive,positive,positive,positive,positive
469486984,"Or could you please just simply tell me in tensorpack/example/fasterrcnn, do we use loss control instead of only use TRAIN.LR_SCHEDULE and TRAIN.STEPS_PER_EPOCH to control training step?",could please simply tell use loss control instead use control training step,issue,negative,neutral,neutral,neutral,neutral,neutral
469415428,"Thanks for the quick response. 

By ""trivial"" values i meant very small (to test that this wasn't an issue related to epoch transitions or epoch-based callbacks). More specifically I tried with `TRAIN.STEPS_PER_EPOCH=5` and `TRAIN.EVAL_PERIOD=3`, and observed the training session run for >40 epochs without issue before manually killing it.


My first thought would be that the issue is related to a rare data sample which wasn't encountered in that time. However I encountered no issues looping through the entirety of both training and eval dataflows. 

I tried removing callbacks to identify the culprit but no such luck.

Checking a resource monitor, it appears that main memory is filling up slowly during training, and hitting 100% (64GB) at the time of crash.... likely a slow down, which then causes a timeout via `SessionRunTimeout`. I'll investigate the source of the leak.
",thanks quick response trivial meant small test issue related epoch specifically tried training session run without issue manually killing first thought would issue related rare data sample time however looping entirety training tried removing identify culprit luck resource monitor main memory filling slowly training time crash likely slow via investigate source leak,issue,positive,positive,neutral,neutral,positive,positive
469327530,These are machine learning questions and are not related to tensorpack. As mentioned in the issue template we do not answer such questions.,machine learning related issue template answer,issue,negative,neutral,neutral,neutral,neutral,neutral
469062029,Thanks! Really appreciate the clearer doc!,thanks really appreciate clearer doc,issue,positive,positive,positive,positive,positive,positive
469061234,"Now the augmentors automatically reset their RNG after fork, in Python>=3.7.
Python's `random` module does the same (reseed after fork) in Python>=3.7, and does inefficient syncing in Python <3.7 (ref.: https://github.com/numpy/numpy/issues/9650#issuecomment-327144993).

Added some more docs to clarify this behavior.",automatically reset fork python python random module reseed fork python inefficient python ref added clarify behavior,issue,negative,negative,negative,negative,negative,negative
469054407,"Actually, numpy has the same issue. The child processes for whatever reason don't re-initialize themselves with a new seed. Python's `random` module, on the other hand, is completely fine though.",actually issue child whatever reason new seed python random module hand completely fine though,issue,positive,positive,neutral,neutral,positive,positive
469053905,"What you described is mentioned as Point(4) here: https://tensorpack.readthedocs.io/tutorial/extend/augmentor.html#design-of-tensorpack-s-imgaug-module. If you use the builtins like `AugmentImageComponent`, reset is done automatically for you. Otherwise you'll have to reset it yourself now.

I wanted to find a better place to mention this to users (or warn users), but haven't yet done it.
Or there might be a way to automatically do this (by some fork handler, for example).

Marking this issue as an enhancement.",point use like reset done automatically otherwise reset find better place mention warn yet done might way automatically fork handler example marking issue enhancement,issue,positive,positive,positive,positive,positive,positive
468900869,"I get where you come from. You're considering all parameters inside the `cv2.warpAffine` function as the transformation. I only consider the `mat` parameter as the transformation since it's the core of the rotation.

> Calling augment_with_params with params produced by a different augmentor instance is not the ""official"" usage and is also hacky

Seems like my goal is just not achievable by using `imgaug` normally lol. I'll close this issue.",get come considering inside function transformation consider mat parameter transformation since core rotation calling produced different instance official usage also hacky like goal achievable normally close issue,issue,positive,positive,neutral,neutral,positive,positive
468859245,"> If I limit the STEPS_PER_EPOCH and EVAL_PERIOD to trivial values,

what do you mean by trivial values?
Presumable changing these two values should not have an effect on failures.

For debugging I would start with (1) trying to run the dataflow in a loop to see if it fails after a while (2) train without custom callbacks",limit trivial mean trivial presumable two effect would start trying run loop see train without custom,issue,negative,negative,negative,negative,negative,negative
468824374,Closing and feel free to reopen or just ask if you have more questions.,feel free reopen ask,issue,positive,positive,positive,positive,positive,positive
468824272,Closing and feel free to reopen if the problem is not solved.,feel free reopen problem,issue,negative,positive,positive,positive,positive,positive
468823979,"My view of this is that you actually want to use two __different__ transformations on two different data (although they share some parameters), so it gets out of the abstraction of our ""imgaug"" module, so you need to do it manually in your dataflow.

Making the `Rotation` augmentor return less information in `get_augment_params` does not solve the problem of the imgaug abstraction. In your solution above you still need to construct two different transformations separately. The only difference after the change you proposed, is that you can then achieve what you want by writing less code, because the function would behave in a way that happen to be reusable by you. 

But I don't think this is a generic solution. It only makes the function more reusable if used it in a __non-standard__ way. Calling `augment_with_params` with `params` produced by a different augmentor instance is not the ""official"" usage and is also hacky: if you use different angles in the two `Rotation` instances, the same issue still arises. Because what you need is just fundamentally not supported by Augmentor.",view actually want use two two different data although share abstraction module need manually making rotation return le information solve problem abstraction solution still need construct two different separately difference change achieve want writing le code function would behave way happen think generic solution function used way calling produced different instance official usage also hacky use different two rotation issue still need fundamentally,issue,positive,neutral,neutral,neutral,neutral,neutral
468732022,"Like I said you need to use numpy arrays, not Tensor",like said need use tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
468648757,"Thanks for the quick reply! I tried `use_list = False`. I think the problem is that our `Preproc()` function returns a list of `Tensor`, but `BatchData` and `numpy`cannot undertand `Tensor` elements, rather than the inconstant shapes. 

````
def get_train_dataflow(batch_size):
   ....
    ds = MapData(ds, preprocess)
    ds = BatchData(ds, batch_size, use_list=True)
    return ds

batch_size = 32
ds.reset_state(batch_size)
for k in ds:
    print(k)
`````
````
[0301 12:06:49 @common.py:142] ERR Cannot batch data. Perhaps they are of inconsistent shape?
Traceback (most recent call last):
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/dataflow/common.py"", line 140, in _batch_numpy
    return np.asarray(data_list, dtype=dtype)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py"", line 492, in asarray
    return array(a, dtype, copy=False, order=order)
TypeError: data type not understood
Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19)
Type 'copyright', 'credits' or 'license' for more information
IPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.

````",thanks quick reply tried false think problem function list tensor tensor rather inconstant return print err batch data perhaps inconsistent shape recent call last file line return file line return array data type understood python custom default type information enhanced interactive python type help,issue,negative,positive,neutral,neutral,positive,positive
468422359,"More correctly, I think tensorflow may be able to accept lists, if the lists can be converted to a numpy array. But anyway it's still better for you to convert it (by `use_list=False`) so it fails early. Perhaps there are still certain field in the data that has different shapes and cannot be batched naively.",correctly think may able accept converted array anyway still better convert early perhaps still certain field data different naively,issue,positive,positive,positive,positive,positive,positive
468420656,"Hi, I managed to encode the inputs to the same shape by add some tensor operations in `preproc()`. And I batched them successfully using ``BatchData()`.
````
def get_train_dataflow(batch_size):
   ....
    ds = MapData(ds, preprocess)
    ds = BatchData(ds, batch_size, use_list=True)
    return ds
````

 As the result the dataflow will produce list of lists of tensors. 

````
batch_size = 32
ds.reset_state(batch_size)
for k in ds:
    print(k)

[0228 19:34:05 @common.py:816] DataFlow Info:
datapoint 0<100 with 4 components consists of
  0: list of len 32
    0: Tensor
    1: Tensor
    2: Tensor
    ...
  1: list of len 32
    0: Tensor
    1: Tensor
    2: Tensor
    ...
  2: list of len 32
    0: Tensor
    1: Tensor
    2: Tensor
    ...
  3: list of len 32
    0: Tensor
    1: Tensor
    2: Tensor
    ...

````
And maintained the ‘Inputs’ method returning a list of `tf.placeholder`. And feed the data with  `QueueInputs()`. But I am getting error like this
```
[0228 19:56:10 @base.py:242] Graph Finalized.
[0228 19:56:10 @concurrency.py:38] Starting EnqueueThread QueueInput/input_queue ...
[0228 19:56:10 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
[0228 19:56:11 @param.py:158] [HyperParamSetter] At global_step=0, learning_rate is set to 0.003300
[0228 19:56:11 @eval.py:219] [EvalCallback] Will evaluate every 25 epochs
[0228 19:56:12 @base.py:274] Start Epoch 1 ...
  0%|                                                                                                                                                                   |0/500[00:00<?,?it/s][0228 19:56:48 @input_source.py:170] ERR Exception in EnqueueThread QueueInput/input_queue:
Traceback (most recent call last):
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/input_source/input_source.py"", line 162, in run
    self.op.run(feed_dict=feed)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2377, in run
    _run_using_default_session(self, feed_dict, self.graph, session)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 5215, in _run_using_default_session
    session.run(operation, feed_dict)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1104, in _run
    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py"", line 492, in asarray
    return array(a, dtype, copy=False, order=order)
ValueError: setting an array element with a sequence.
[0228 19:56:48 @input_source.py:176] EnqueueThread QueueInput/input_queue Exited.
[0228 19:56:48 @base.py:290] Training was stopped by exception FIFOQueue '_0_QueueInput/input_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: QueueInput/input_deque = QueueDequeueV2[component_types=[DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](QueueInput/input_queue)]]
         [[Node: apply_gradients/AccumGradOptimizer/apply_grad_0/Momentum/update_group1/block0/conv1/W/ApplyMomentum/_624 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_7355_...lyMomentum"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

Caused by op 'QueueInput/input_deque', defined at:
  File ""train_ssd.py"", line 451, in <module>
    launch_train_with_config(traincfg, trainer)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/train/interface.py"", line 84, in launch_train_with_config
    model._build_graph_get_cost, model.get_optimizer)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/utils/argtools.py"", line 176, in wrapper
    return func(*args, **kwargs)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/train/tower.py"", line 214, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/train/trainers.py"", line 193, in _setup_graph
    grad_list = self._builder.call_for_each_tower(tower_fn)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 225, in call_for_each_tower
    use_vs=[False] + [True] * (len(self.towers) - 1))
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 121, in build_on_towers
    return DataParallelBuilder.call_for_each_tower(*args, **kwargs)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 116, in call_for_each_tower
    ret.append(func())
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/train/tower.py"", line 242, in get_grad_fn
    inputs = input.get_input_tensors()
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/input_source/input_source_base.py"", line 82, in get_input_tensors
    return self._get_input_tensors()
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/input_source/input_source.py"", line 267, in _get_input_tensors
    ret = self.queue.dequeue(name='input_deque')
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 440, in dequeue
    self._queue_ref, self._dtypes, name=name)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 3734, in queue_dequeue_v2
    timeout_ms=timeout_ms, name=name)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): FIFOQueue '_0_QueueInput/input_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: QueueInput/input_deque = QueueDequeueV2[component_types=[DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](QueueInput/input_queue)]]
         [[Node: apply_gradients/AccumGradOptimizer/apply_grad_0/Momentum/update_group1/block0/conv1/W/ApplyMomentum/_624 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_7355_...lyMomentum"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
.
```
I am not sure if this is caused by wrong usages of TensorPack functions. But it would be perfect if you can share some thoughts to figure out the bug.
Thanks very much! ",hi encode shape add tensor successfully return result produce list print list tensor tensor tensor list tensor tensor tensor list tensor tensor tensor list tensor tensor tensor method list feed data getting error like graph starting running set evaluate every start epoch err exception recent call last file line run file line run self session file line operation file line run file line file line return array setting array element sequence training stopped exception closed insufficient current size node node defined file line module trainer file line file line wrapper return file line input file line file line false true file line return file line file line file line return file line ret file line file line file line file line file line see closed insufficient current size node node sure wrong would perfect share figure bug thanks much,issue,positive,positive,positive,positive,positive,positive
467976346,Feel free to reopen if the above does not answer your questions.,feel free reopen answer,issue,positive,positive,positive,positive,positive,positive
467140415,"1. Find the input tensor in the code. In your case it's probably image: 
https://github.com/tensorpack/tensorpack/blob/274c754444ac35492e32b67764e2466b46f8a179/examples/ImageNetModels/shufflenet.py#L114
2. Add `print(image)`.
3. Find the output tensor in the code. In your case it may be logits:
https://github.com/tensorpack/tensorpack/blob/274c754444ac35492e32b67764e2466b46f8a179/examples/ImageNetModels/shufflenet.py#L153
4. Add `print(logits)`.
5. Use random strings as input names and output names and run the export code. It will print the names of image and logits.",find input tensor code case probably image add print image find output tensor code case may add print use random input output run export code print image,issue,negative,negative,negative,negative,negative,negative
467121207,"I tried the code in https://github.com/tensorpack/tensorpack/blob/master/examples/basics/export-model.py, but I didn't find where I should put ""print(tensor)""...",tried code find put print tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
467119044,The easiest way is to `print(tensor)`. It will tell you the name of the tensor.,easiest way print tensor tell name tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
467117733,Thanks. But how can I get the input names and output names? I am trying shufflenet V2. ,thanks get input output trying,issue,negative,positive,positive,positive,positive,positive
466557346,"You are totally right. I cannot believe I missed the simple thing..
Now the problem is completely solved. Thank you very much.
",totally right believe simple thing problem completely thank much,issue,negative,positive,positive,positive,positive,positive
466551558,"> In the original code, you've divided height and width with a value of 16 x 16 (on line 282).

This is not true. `a // 16 * 16` will divide `a` by 16 and multiply it by 16. See https://docs.python.org/3/reference/expressions.html#operator-precedence",original code divided height width value line true divide multiply see,issue,positive,positive,positive,positive,positive,positive
466550923,"I cannot understand what exactly you did and what you saw. If you're looking to solve an unexpected problem you met, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",understand exactly saw looking solve unexpected problem met please post relevant following issue template click new issue unexpected visit link post unexpected,issue,negative,positive,positive,positive,positive,positive
466550376,"Hey, 
Yeah, I tested again with the muliple of 16 as you mentioned but still having the same problem. 
Always has the mismatch in the end. 

In the original code, you've divided height and width with a value of 16 x 16 (on line 282). 
I don't think this is the problem of // or /. 
This line is really not changing the shape of an image, rather it just adds one dimension to fit it onto input tensor. But when I modify the code in a way that will actually change the values of width and height, then the error is there. Could you please check on this?

  ",hey yeah tested still problem always mismatch end original code divided height width value line think problem line really shape image rather one dimension fit onto input tensor modify code way actually change width height error could please check,issue,negative,positive,positive,positive,positive,positive
466475623,"`exploration` and `epoch` are reset because they are not tensorflow variables, and therefore cannot be saved in a tensorflow checkpoint.

Same thing in A3C: epoch will be reset.

You can start the training by providing a `starting_epoch`, as mentioned in the above logs. (See https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.TrainConfig).

You'll also need to modify `init_exploration=1.0` to a different initial exploration you need.",exploration epoch reset therefore saved thing epoch reset start training providing see also need modify different initial exploration need,issue,positive,neutral,neutral,neutral,neutral,neutral
466474218,"You will need to modify the `inputs` method of the model so that it matches whatever produced by the dataflow. And also write the model implementation which takes a list of `inputs`.
",need modify method model whatever produced also write model implementation list,issue,negative,neutral,neutral,neutral,neutral,neutral
466388449,"Thanks for the reply! 
Yes, I just realized the shape of elements in `ret`  returned by `preprocess` are different among images.
For the model, because we are using another model implementation that can handle images > 1, I think that would not be a problem.  And besides those two problems, would the usage of TensorPack dataflow and input source API be correct?

I will try to find some solutions to solve the shape issue of the input data. Since it is not a TensorPack problem, I will close the issue. But it would be perfect if you can also share some thoughts about it.

Thanks!",thanks reply yes shape ret returned different among model another model implementation handle think would problem besides two would usage input source correct try find solve shape issue input data since problem close issue would perfect also share thanks,issue,positive,positive,positive,positive,positive,positive
466245510,"> The one in FasterRCNN also uses activation:
> [tensorpack/examples/FasterRCNN/basemodel.py](https://github.com/tensorpack/tensorpack/blob/d8d35fb57fd8524ee53e030c062455c37b89474f/examples/FasterRCNN/basemodel.py#L76)
> 
> Line 76 in [d8d35fb](/tensorpack/tensorpack/commit/d8d35fb57fd8524ee53e030c062455c37b89474f)
> 
>  argscope(Conv2D, use_bias=False, activation=nonlin, 
> As to why it goes to NaN, it is a machine learning question and do not appear to be related to tensorpack.

",one also activation line go nan machine learning question appear related,issue,negative,neutral,neutral,neutral,neutral,neutral
466160918,Thanks for the confirmation. Placing changes in the mapped `preprocess` method worked just fine.,thanks confirmation method worked fine,issue,positive,positive,positive,positive,positive,positive
466128334,"Since the existing code already uses `MapData`, you can add your changes in `preprocess`.",since code already add,issue,negative,neutral,neutral,neutral,neutral,neutral
466127875,"It sounds like this goes out of the abstraction of augmentors because you need to make changes on the datapoint-level, not on the image-level. You need to use `MapData` or similar methods to modify the data points directly.",like go abstraction need make need use similar modify data directly,issue,negative,positive,neutral,neutral,positive,positive
466122749,"Closing due to lack of inactivity.
Feel free to reopen or add new issues if you have more questions.",due lack inactivity feel free reopen add new,issue,negative,positive,positive,positive,positive,positive
466122367,Closing as there is no response. Feel free to reopen and add the details about the issue.,response feel free reopen add issue,issue,positive,positive,positive,positive,positive,positive
466070858,"It would be incorrect because input data has different shapes and cannot be naively batched, and the model implementation does not handle >1 images.",would incorrect input data different naively model implementation handle,issue,negative,negative,negative,negative,negative,negative
465972124,"Thanks for the quick change - pulled the master again and all works well.

Regards,
James",thanks quick change master work well,issue,positive,positive,positive,positive,positive,positive
465963996,Thanks for the suggestions. Now `ale_python_interface` will not be required if you use gym environment.,thanks use gym environment,issue,negative,positive,positive,positive,positive,positive
465946248,Okay I see... and appreciate your patience greatly.,see appreciate patience greatly,issue,negative,positive,positive,positive,positive,positive
465945841,"The one in FasterRCNN also uses activation:
https://github.com/tensorpack/tensorpack/blob/d8d35fb57fd8524ee53e030c062455c37b89474f/examples/FasterRCNN/basemodel.py#L76

As to why it goes to NaN, it is a machine learning question and do not appear to be related to tensorpack.",one also activation go nan machine learning question appear related,issue,negative,neutral,neutral,neutral,neutral,neutral
465945317,"> https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet

thank you!
I have used this one also, but i found the code is different with examples/FasterRCNN backbone.
one use activation but another not. i use my trained resnet model to train FasterRCNN, after some epochs, the cost is NAN. but i use the pre-trained model:http://models.tensorpack.com/FasterRCNN/ImageNet-R50-AlignPadding.npz is ok.
![image](https://user-images.githubusercontent.com/18695918/53161608-444acd00-3605-11e9-98d5-77b576762bbd.png)
![image](https://user-images.githubusercontent.com/18695918/53161664-5dec1480-3605-11e9-9897-354ae035aedc.png)",thank used one also found code different backbone one use activation another use trained model train cost nan use model image image,issue,negative,neutral,neutral,neutral,neutral,neutral
465945030,"Clip need to come after BN so that the inputs to `Fa` can be within [0, 1].

In general, it's best to not trust third-party layer code, as mentioned in https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries.",clip need come fa within general best trust layer code,issue,positive,positive,positive,positive,positive,positive
465943826,"Another question brought by this...

I notice the tensorpack put the bn layer before activation (clip) layer while in tensorlayer after activation layer:
https://tensorlayercn.readthedocs.io/zh/latest/_modules/tensorlayer/layers/convolution/dorefa_conv.html#DorefaConv2d 

The order does matter as the negative convolved result can be converted to 0.0 to 1.0 if it go through bn layer before clip and thus kept after clip. It will not be kept if go through clip layer first.

I do not know what is more reasonable?",another question brought notice put layer activation clip layer activation layer order matter negative result converted go layer clip thus kept clip kept go clip layer first know reasonable,issue,negative,positive,neutral,neutral,positive,positive
465941244,"Maybe it will be less confusing if we just call it ""table"", not ""look up table"".
Yes multiple inputs map to the same output. And in terms of the actual implementation, I'm not sure what we used in actual deployment but at least you can use integer comparisons to achieve it.",maybe le call table look table yes multiple map output actual implementation sure used actual deployment least use integer achieve,issue,positive,positive,neutral,neutral,positive,positive
465940101,I kinda catch up with you now. So you mean that multiple indices in the LUT map to one value and the indexing should be like LUT[i>>weight_bitwidth] instead of LUT[i]. ,catch mean multiple index lut map one value indexing like lut instead lut,issue,positive,negative,negative,negative,negative,negative
465936411,"`Fa(a1)` only has `2^activation bitwidth` possible output values. So these are the number of elements needed in the lookup table. You don't need to cover every possible inputs in the range, because `Fa(clip(gamma / K * a0 + beta, 0, 1))` is weakly-monotonic w.r.t `a0`.",fa possible output number table need cover every possible range fa clip gamma beta,issue,negative,neutral,neutral,neutral,neutral,neutral
465908507,">The table size is 2^activation bitwidth;

I am not clear about this conclusion.

Suppose the weigth and activation bitwidths are both 2 and suppose we convolve weights 1/3 * [-3 to 3, ... -3 to 3] with activations 1/3 * [0 to 3, ... ,0 to 3]' and get 1/9 * [-9 to 9 + ... + -9 to 9] = 1/K * a0, where a0 is an integer in a rather large range. 

After the following bn and clip process we get a1 = clip(gamma * 1/K * a0 + beta, 0, 1). Therefore the interger a0 should be retained within these areas:

[int(-beta*K/gamma), int( (1-beta)*K/gamma )] if gamma >0

[int( (1-beta) * K/gamma), int( -beta * K/gamma )] if gamma <0 

Within these areas we perform Fa(a1), so the length of the LUT is calculated as (1-beta)*K/gamma - (-beta*K/gamma) = K/gamma = (2^(activation bitwidth) - 1) * (2^(weight bitwidth) - 1) / gamma

What is the problem about my understanding of the LUT?


",table size clear conclusion suppose activation suppose convolve get integer rather large range following clip process get clip gamma beta therefore within gamma gamma within perform fa length lut calculated activation weight gamma problem understanding lut,issue,negative,positive,positive,positive,positive,positive
465823274,"Yeah, understood. Thanks a lot! @ppwwyyxx .",yeah understood thanks lot,issue,positive,positive,positive,positive,positive,positive
465818021,"> If convenient, could you please give some advice on how to support training/testing in FasterRCNN in Tensorpack with batch>1 per GPU?

It's much more complicated to implement than single-batch in tensorflow and that's why it is not supported. And I have no recent plans adding it, for reasons below.

> I don't know how the engineers in big company achieve the large batch training in FasterRCNN.

You're correct that it takes a lot of memory. Even with batch>1 support, standard models typically can run at most 3 images per batch, which is not very interesting or super useful.
Sure you can also run smaller models / smaller images, but those are not very interesting to me, either.

And of course, if you count the batch size over all GPUs you can get much larger batch size.",convenient could please give advice support batch per much complicated implement recent know big company achieve large batch training correct lot memory even batch support standard typically run per batch interesting super useful sure also run smaller smaller interesting either course count batch size get much batch size,issue,positive,positive,positive,positive,positive,positive
465448945,For some reasons no int32 variables is allowed even as the temporary ones to pick the intermediate accumulated result... And I can not train the network using weights less than w4a4 bits. Actually sharp decreases usually appears once 3 or less bits are applied. Maybe it is limited to the structure of the network. I think I will try LUT in w4a4 anyway.   ,even temporary pick intermediate result train network le wa actually sharp usually le applied maybe limited structure network think try lut wa anyway,issue,negative,negative,negative,negative,negative,negative
465440620,"> 1. Is there any way to hide the floating point bn calculation? LUT maybe, but the size of the table could be very large if the bitwidth is long (>6)?

LUT. The table size is 2^activation bitwidth; And in practice we never use any bitwidth >4.

> 2. Even for the fixed point operation, how to deal with the potential accumulation overflow? It seems impossible to make the intermediate additon result in int16 if there are more channels with large kernel size.

You can save the addition result (the popcnt) into int16 or larger ints. They don't have to be stored, because the LUT is monotonic. Once you obtain the value, you immediately know which item in the LUT you need.",way hide floating point calculation lut maybe size table could large long lut table size practice never use even fixed point operation deal potential accumulation overflow impossible make intermediate result large kernel size save addition result lut monotonic obtain value immediately know item lut need,issue,positive,negative,neutral,neutral,negative,negative
465431442,"Sorry for omitting that I fuse the batch normal parameters into the weights. In the training phase, the float weights are like [0.3333 * bn_gamma, 0.6667 * bn_gamma, 0.3333 * bn_gamma, 0.3333 * bn_gamma]. The errors are generated after the fused weights fixed-point to int8 and multiplied with fixed-point feature values. Without the bn layer, I think everything fits the algorithm in the paper and goes without error. 

If my guess above and in my last post are correct about the fixed-point convolution, here comes two problems that puzzle me about the usage:    

1) Is there any way to hide the floating point bn calculation? LUT maybe, but the size of the table could be very large if the bitwidth is long (>6)?
2) Even for the fixed point operation, how to deal with the potential accumulation overflow? It seems impossible to make the intermediate additon result in int16 if there are more channels with large kernel size.",sorry fuse batch normal training phase float like fused feature without layer think everything algorithm paper go without error guess last post correct convolution come two puzzle usage way hide floating point calculation lut maybe size table could large long even fixed point operation deal potential accumulation overflow impossible make intermediate result large kernel size,issue,negative,negative,neutral,neutral,negative,negative
465402292,"Now I'm confused about what you're asking. 
I've probably misunderstood your  questions. There is no floating point operations involved in an actual deployment. And there won't be any extra quantization error. Why do you think the opposite?",confused probably misunderstood floating point involved actual deployment wo extra quantization error think opposite,issue,negative,negative,neutral,neutral,negative,negative
465386285,"I am not sure whether I understand what you mean. Suppose we use the w2a2 to perform the convolution 1/3 * 1/3 * [1, 2, 1, 3] * [2, 1, 0, 3]' and get the activated feature value 1/9 * [2, 7, 2, 10] and then clip to 1/9 * [2, 7, 2, 9] if using the clip(0, 1) after the convolution layer. Then it is convolved with the next weights 1/3 * [2, 2, 2, 2]' and we get 1/27 * [18, 18, 22, 22]. 

No floating values are involved in the example above but it sounds like ""float->quantize + activation + quantize ...... ->quantize->float (final output)"" rather than ""quantize->float + activataion + float->quantize"". And through LUT, do you directly match the [1, 2, 1, 3] [2, 1, 0, 3] combination to the result [2, 7, 2, 9]?",sure whether understand mean suppose use wa perform convolution get feature value clip clip convolution layer next get floating involved example like quantize activation quantize float final output rather float quantize lut directly match combination result,issue,positive,positive,neutral,neutral,positive,positive
465377576,"> In general, what's your recommendation on implementing hooks that are run after trainer.train_op in tensorpack, since the trainer.train_op is constructed after the callbacks are specified in the training config. (The example in the end also shows how to do this in general TF).

[launch_train_with_config](https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.launch_train_with_config) is almost equivalent to the following two lines of code:
```
    trainer.setup_graph(
        model.get_inputs_desc(), input,
        model.build_graph, model.get_optimizer)
    trainer.train_with_defaults(
        callbacks=config.callbacks,
        monitors=config.monitors,
        session_creator=config.session_creator,
        session_init=config.session_init,
        steps_per_epoch=config.steps_per_epoch,
        starting_epoch=config.starting_epoch,
        max_epoch=config.max_epoch,
        extra_callbacks=config.extra_callbacks)
```
You can run the first line first, which will create the train_op in trainer.
You can then construct your callbacks, and call the second line to start training.",general recommendation run since training example end also general almost equivalent following two code input run first line first create trainer construct call second line start training,issue,negative,positive,positive,positive,positive,positive
465252062,"Thanks! The preprocessing should follow `fb.resnet.torch` (which is also the preprocessing used by many papers). The correct version is to use `random_normal`. Now it's fixed.

This does not affect any models because I only used dataflow-based preprocessing and these TF-based pre-processing are only there for benchmark.",thanks follow also used many correct version use fixed affect used,issue,negative,positive,positive,positive,positive,positive
465249632,"Activation function on fixed point values (i.e., quantize->float + activataion + float->quantize) can be implemented as a lookup table.
Therefore in actual use, no floating point is involved and no extra quantization is needed.
",activation function fixed point float quantize table therefore actual use floating point involved extra quantization,issue,negative,positive,neutral,neutral,positive,positive
464498335,"I'm apparently not an estimator user so I may be wrong. By just looking at the documentation I can identify many flexibility issues in its design.

From https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator
> model_fn: Model function. Follows the signature:
Args:
features: This is the first item returned from the input_fn passed to train, evaluate, and predict. This should be a single tf.Tensor or dict of same.
labels: This is the second item returned from the input_fn passed to train, evaluate, and predict. This should be a single tf.Tensor or dict of same (for multi-head models). If mode is tf.estimator.ModeKeys.PREDICT, labels=None will be passed. If the model_fn's signature does not accept mode, the model_fn must still be able to handle labels=None.
mode: Optional. Specifies if this training, evaluation or prediction. See tf.estimator.ModeKeys.
params: Optional dict of hyperparameters. Will receive what is passed to Estimator in params parameter. This allows to configure Estimators from hyper parameter tuning.
config: Optional estimator.RunConfig object. Will receive what is passed to Estimator as its config parameter, or a default value. Allows setting up things in your model_fn based on configuration such as num_ps_replicas, or model_dir.
Returns: tf.estimator.EstimatorSpec

Bad abstraction: it assumes all your inputs are either ""features"" or ""labels"". I often have inputs that do something else and I would feel uncomfortable calling them either ""features"" or ""labels"". Of course you can still put them into one of the two buckets but it's just ugly.
Tensorpack: everything is just ""inputs""

> evaluate(
    input_fn,
    steps=None,
    hooks=None,
    checkpoint_path=None,
    name=None
)

Bad assumption: It takes an `input_fn` and will just use the existing `model_fn` with `mode=EVAL`.
So it assumes you'll want to evaluate the same `model_fn` with different `input_fn`. What if I want to evaluate also with different `model_fn` (which may still share all the trained parameters) ? Looks impossible. 
Tensorpack: support evaluation with different input + different tower function. The code release of our recent paper (https://github.com/facebookresearch/ImageNet-Adversarial-Training/) does exactly this type of evaluation.

Let's look at `EstimatorSpec` (https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec) which is what `model_fn` is supposed to return:

> Depending on the value of mode, different arguments are required. Namely
For mode == ModeKeys.TRAIN: required fields are loss and train_op.
For mode == ModeKeys.EVAL: required field is loss.
For mode == ModeKeys.PREDICT: required fields are predictions.

Strong assumption: assumes that the training ""runs one train_op in a loop"".
Tensorpack: assumes that the training is ""runs in a loop"".

Bad assumption: assumes you can compute a loss in evaluation. There are counter examples in tensorpack.

> eval_metric_ops: Dict of metric results keyed by name. The values of the dict can be one of the following: (1) instance of Metric class. (2) Results of calling a metric function, namely a (metric_tensor, update_op) tuple. metric_tensor should be evaluated without any impact on state (typically is a pure computation results based on variables.). For example, it should not trigger the update_op or requires any input fetching.

Bad assumption: It assumes that metrics can be computed by ops. What if I have to compute them in Python?
Maybe there are still ways to fetch tensors in the network and compute the metrics in Python. But then it gets unclear how to get those metrics to work with the rest of estimator (e.g., send them into tensorboard).
In general TF seems to assume you want to do all the work in TF (e.g., the entire dataset module), which is often just impractical.

These are just from a brief scan of the docs. I'm not an estimator user and these opinions may be subjective or wrong. 
And I would definitely agree if you say that estimator is good enough for 95% of use cases, or 100% good enough for your use cases. I think Keras is also 100% good enough for perhaps 90% of TF users.",apparently estimator user may wrong looking documentation identify many flexibility design model function signature first item returned train evaluate predict single second item returned train evaluate predict single mode signature accept mode must still able handle mode optional training evaluation prediction see optional receive estimator parameter configure hyper parameter tuning optional object receive estimator parameter default value setting based configuration bad abstraction either often something else would feel uncomfortable calling either course still put one two ugly everything evaluate bad assumption use want evaluate different want evaluate also different may still share trained impossible support evaluation different input different tower function code release recent paper exactly type evaluation let look supposed return depending value mode different namely mode loss mode field loss mode strong assumption training one loop training loop bad assumption compute loss evaluation counter metric keyed name one following instance metric class calling metric function namely without impact state typically pure computation based example trigger input fetching bad assumption metric compute python maybe still way fetch network compute metric python unclear get metric work rest estimator send general assume want work entire module often impractical brief scan estimator user may subjective wrong would definitely agree say estimator good enough use good enough use think also good enough perhaps,issue,negative,negative,neutral,neutral,negative,negative
464448579,"@yg320 It seems like the support for the estimators is going to be limited in TensorFlow 2.0. The custom estimators are discouraged in favor of the Keras models. Actually, they never appeared to have huge adoption in the community. It was always difficult to find any hints on how to use them. Note that since, TF is very popular there are a lot of low-effort tutorials which mostly paraphrase the original docs, but do not touch any details that might interest you in practice.

Source:
https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8",like support going limited custom favor actually never huge adoption community always difficult find use note since popular lot mostly paraphrase original touch might interest practice source,issue,positive,positive,positive,positive,positive,positive
464437919,"Hi!

I was wondering about your comment:

""I personally think `tf.Estimator` is not well designed and therefore not flexible enough. But I also think it does have something that tensorpack trainers can learn from.""

Can you please elaborate about the flexibility issues on tf.Estimator that you observed? 
And also, what do you think that Tensorpack trainers can learn from tf.Estimators?
 
I'm using Tensorpack for a while now, and was considering giving tf.Estimators a try (especially because of the community)

Thanks!
",hi wondering comment personally think well designed therefore flexible enough also think something learn please elaborate flexibility also think learn considering giving try especially community thanks,issue,positive,positive,positive,positive,positive,positive
464409105,"To avoid future confusion, after e79d74f85, loading a checkpoint with a learning_rate that's inconsistent to the schedule will lead to a warning message like this:
```python
logger.warn(""According to scheduler {}, parameter '{}' should become {} at the current point. ""
            ""However its current value is {}. ""
            ""If this is the only scheduler being used, you may want to check whether your ""
            ""initialization of the parameter is as expected"".format(
                self, self.param.readable_name, v, actual_value))

```",avoid future confusion loading inconsistent schedule lead warning message like python according parameter become current however current value used may want check whether parameter self,issue,negative,neutral,neutral,neutral,neutral,neutral
464132079,"You're responsible for setting a correct `steps_per_epoch` that works best for you.
In general, many trainers have no concept of ""number of GPUs"".",responsible setting correct work best general many concept number,issue,positive,positive,positive,positive,positive,positive
463918905,"I apologize that I did not find enough time to learn the paper and this code.
If you're still interested in adding this example, here are some high-level comments:
1. There is a significant amount of code duplication with CycleGAN, and ideally this should be avoided. I don't know yet what's the most beautiful way to do it though -- probably putting the shared part into a separate file and import it.
2. I need the instructions to run it (including getting the datasets). And some nice-looking sample outputs at certain steps (or when it finishes), so I know what to expect when I test the code.",apologize find enough time learn paper code still interested example significant amount code duplication ideally know yet beautiful way though probably part separate file import need run getting sample certain know expect test code,issue,positive,positive,positive,positive,positive,positive
463915880,"Should've been fixed after https://github.com/tensorpack/tensorpack/commit/c366eebf04f5e53022b1b266cc0f940bf6789f61
Docs (https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm) was also updated.

In general, the model will run faster with `internal_update=True` when some batchnorm are unused. This was also now mentioned in the docs.
The default was kept as `internal_update=False` to be consistent with `tf.layers`.",fixed also general model run faster unused also default kept consistent,issue,negative,positive,positive,positive,positive,positive
463911463,"Btw, it will not cause this issue if you use the `internal_update=True` option in `BatchNorm`.",cause issue use option,issue,negative,neutral,neutral,neutral,neutral,neutral
463911327,"Thanks a lot for finding this out! 
Indeed, unused batchnorm will also cause this. I'll fix this or add this note to documentation.",thanks lot finding indeed unused also cause fix add note documentation,issue,negative,positive,positive,positive,positive,positive
463909678,"Thank you for the suggestions. I think I have found the reason: I only used the first few layers of mobilenet backbone for training. The last several layers are not used for training, but were created by default when loading `mobilenet_base()`. After removing the batch_norm in these layers, the training works.",thank think found reason used first backbone training last several used training default loading removing training work,issue,negative,positive,neutral,neutral,positive,positive
463741844,"For anyone to better understand and diagnose your issue, please post relevant details following the issue template. Posting ""what you observed"" will let others have a better idea about the reason that causes your issue.",anyone better understand diagnose issue please post relevant following issue template posting let better idea reason issue,issue,positive,positive,positive,positive,positive,positive
463741420,"No it does not. The only thing `PrefetchOnGPUs` does is to change an environment variable.
And `PrefetchOnGPUs` has also been marked deprecated for a long time: it does not exist in documentation, and a user should not use it.",thing change environment variable also marked long time exist documentation user use,issue,negative,positive,neutral,neutral,positive,positive
463739800,"when I open the file: ~/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/dataflow/parallel.py
it has this in the beginning: 
from ..utils.gpu import change_gpu
and then:
__all__ = ['PrefetchData', 'MultiProcessPrefetchData',
           'PrefetchDataZMQ', 'PrefetchOnGPUs', 'MultiThreadPrefetchData']

I think this means in the newest version of dataflow, it actually asks for gpu resources",open file beginning import think version actually,issue,negative,neutral,neutral,neutral,neutral,neutral
463737782,"> In the newest version, it actually asks for GPU resources

It must be something else that's asking for GPU resources.

> Could you please double check about this or provide a link of the package that only uses CPU for dataflow?

The entire dataflow package uses CPU and has not one line of code about GPU.",version actually must something else could please double check provide link package entire package one line code,issue,negative,neutral,neutral,neutral,neutral,neutral
463737300,"In the newest version, it actually asks for GPU resources. Could you please double check about this or provide a link of the package that only uses CPU for dataflow? Thanks!",version actually could please double check provide link package thanks,issue,positive,positive,neutral,neutral,positive,positive
463531936,"You can `print(shared_name)` at https://github.com/tensorpack/tensorpack/blob/49675590da8a39c649b5f0f5ba522a22b90e2d69/tensorpack/models/batch_norm.py#L248
and see if there are any duplications within one GPU, and if different GPUs produce the same name.",print see within one different produce name,issue,negative,neutral,neutral,neutral,neutral,neutral
463529841,"> If a put batch_norm outside the `for` loop, then it works well.

That sounds like you may have used the same name for different batchnorm layers.",put outside loop work well like may used name different,issue,positive,neutral,neutral,neutral,neutral,neutral
463528563,"> I have put the batch_norm in a `for` loop ([ slim mobilenet v2](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py#L248) )

This is unrelated. What I meant is a symbolic loop (like `tf.while_loop`) which will execute the same layer in each iteration.
A loop in python side like this will create a new layer in each iteration.",put loop slim unrelated meant symbolic loop like execute layer iteration loop python side like create new layer iteration,issue,positive,positive,positive,positive,positive,positive
463527752,"Thank you for the suggestions. I think I have put the batch_norm in a `for` loop ([ slim mobilenet v2](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py#L248) ). If a put batch_norm outside the `for` loop, then it works well.

What I don't understand is that, in the `for` loop, layers all have different names (including batch_norm). Why would batch_norm hang? Thank you.",thank think put loop slim put outside loop work well understand loop different would thank,issue,positive,neutral,neutral,neutral,neutral,neutral
463439873,"> This is not true. As you can see in the code, when the current step is after the end of all scheduled point (1000), it will return `None`, and the parameter will not be changed.

Oh you are right. I didn't notice that `or laste == e` is behind the ` if laste is None`. After I ignore the `learning_rate` when loading the checkpoint, the problem solved. Thanks!",true see code current step end point return none parameter oh right notice behind none ignore loading problem thanks,issue,positive,positive,neutral,neutral,positive,positive
463328240,Closing due to no activity and it's unclear whether there is anything tensorpack should do about the issues being discussed.,due activity unclear whether anything,issue,negative,negative,negative,negative,negative,negative
463327209,"Related to that, the design choice that's made here can be reconsidered. 
Perhaps it's OK to let the scheduler set the parameter at the beginning of training as well, in addition to at the exact scheduled points.",related design choice made perhaps let set parameter beginning training well addition exact,issue,negative,positive,positive,positive,positive,positive
463288775,"> The warm-up schedule is `(steps, value): [(0, 0.0033000000000000004), (1000, 0.01)] ` and the self.global_step is 11000. So, the learning_rate will stuck in 0.01.

This is not true. As you can see in the code, when the current step is after the end of all scheduled point (1000), it will return `None`, and the parameter will not be changed.

The second parameter scheduler (the epoch-based one) is epoch base and without interpolation. As you can see in the code above, it is designed to work only at the exact schedule point. At other points, the parameter will not be changed.

The learning rate is 0.01 probably because that's the learning rate in your checkpoint. ",schedule value stuck true see code current step end point return none parameter second parameter one epoch base without interpolation see code designed work exact schedule point parameter learning rate probably learning rate,issue,negative,negative,neutral,neutral,negative,negative
463279723,You may also want to remove some custom callbacks to see if they affect this. Callbacks in general should not use `hooked_sess` and it may cause issues like this if you use `hooked_sess`.,may also want remove custom see affect general use may cause like use,issue,negative,positive,neutral,neutral,positive,positive
463279234,"> When would the BatchNorm be executed more than once?

For example when it is executed in a while loop",would executed example executed loop,issue,negative,neutral,neutral,neutral,neutral,neutral
463279054,"This question does not appear to be related to tensorpack.

Please read the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md).
",question appear related please read issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
463164950,"Thank you for your suggestion. I think the operations in each gpu are the same. I used SyncMultiGPUTrainerReplicated from tensorpack for multi-gpu training.

When would the BatchNorm be executed more than once? I tried to print the tensor name of the input at the beginning of BatchNorm, it looks like each BatchNorm is called only once.",thank suggestion think used training would executed tried print tensor name input beginning like,issue,positive,neutral,neutral,neutral,neutral,neutral
463098719,"Another possibility:

In every iteration, either all GPUs execute the BatchNorm once, or no GPUs execute the BatchNorm. If only a subset of GPUs execute the BatchNorm, or if the BatchNorm is executed more than once, this layer may hang because there is nothing to sync.
",another possibility every iteration either execute execute subset execute executed layer may nothing sync,issue,negative,neutral,neutral,neutral,neutral,neutral
463043465,"I tried one built-in example with sync bn, and it works fine. It should be my problem when replacing slim.batch_norm with tensorpack's batch norm. Will check it.",tried one example sync work fine problem batch norm check,issue,negative,positive,positive,positive,positive,positive
462661276,"https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm has mentioned the common reasons why it may hang.

If you cannot figure out the reason, please try running the built-in example with syncbn first -- if that fails, please fill the issue template.",common may figure reason please try running example first please fill issue template,issue,positive,negative,neutral,neutral,negative,negative
462459485,"Yeah, I'm pleased with how it's all looking, holding 3it/s and targeting a ~20min epoch. ",yeah looking holding epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
462454009,"This was a **LOT** harder than I expected, I feel like i've been install and uninstalling packages for hours. I'll paste a list of **all** colab library versions so that if anyone else has this problem hopefully this will solve it.
Currently running at ~3s/it (not 3it/s! haha). 
Appreciate all the help, this stuff is hard to learn as a beginner! 
```
Package                  Version              
------------------------ ---------------------
absl-py                  0.7.0                
alabaster                0.7.12               
albumentations           0.1.12               
altair                   2.3.0                
astor                    0.7.1                
astropy                  3.0.5                
atari-py                 0.1.7                
atomicwrites             1.3.0                
attrs                    18.2.0               
audioread                2.1.6                
autograd                 1.2                  
Babel                    2.6.0                
backports.tempfile       1.0                  
backports.weakref        1.0.post1            
batchglm                 0.4.1                
beautifulsoup4           4.6.3                
bleach                   1.5.0                
bokeh                    1.0.4                
boto                     2.49.0               
boto3                    1.9.89               
botocore                 1.12.89              
Bottleneck               1.2.1                
bs4                      0.0.1                
bz2file                  0.98                 
cachetools               3.1.0                
certifi                  2018.11.29           
cffi                     1.11.5               
cftime                   1.0.3.4              
chainer                  5.0.0                
chardet                  3.0.4                
Click                    7.0                  
cloudpickle              0.6.1                
cmake                    3.12.0               
colorlover               0.3.0                
community                1.0.0b1              
contextlib2              0.5.5                
convertdate              2.1.3                
coverage                 3.7.1                
coveralls                0.5                  
crcmod                   1.7                  
cufflinks                0.14.6               
cupy-cuda100             5.2.0                
cvxopt                   1.2.3                
cvxpy                    1.0.14               
cycler                   0.10.0               
cymem                    2.0.2                
Cython                   0.29.4               
cytoolz                  0.9.0.1              
daft                     0.0.4                
dask                     0.20.2               
dataclasses              0.6                  
datascience              0.10.6               
decorator                4.3.2                
defusedxml               0.5.0                
diffxpy                  0.4.2                
dill                     0.2.9                
distributed              1.25.3               
Django                   2.1.5                
dlib                     19.16.0              
dm-sonnet                1.23                 
docopt                   0.6.2                
docutils                 0.14                 
dopamine-rl              1.0.5                
easydict                 1.9                  
ecos                     2.0.7.post1          
editdistance             0.5.2                
en-core-web-sm           2.0.0                
entrypoints              0.3                  
enum34                   1.1.6                
ephem                    3.7.6.0              
et-xmlfile               1.0.1                
fa2                      0.3.5                
fancyimpute              0.4.2                
fastai                   1.0.42               
fastcache                1.0.2                
fastdtw                  0.3.2                
fastprogress             0.1.18               
fastrlock                0.4                  
fbprophet                0.4.post2            
featuretools             0.4.1                
filelock                 3.0.10               
fix-yahoo-finance        0.0.22               
Flask                    1.0.2                
folium                   0.2.1                
future                   0.16.0               
gast                     0.2.2                
GDAL                     2.2.2                
gdown                    3.6.4                
gensim                   3.6.0                
geographiclib            1.49                 
geopy                    1.17.0               
gevent                   1.4.0                
gin-config               0.1.2                
glob2                    0.6                  
google                   2.0.1                
google-api-core          1.7.0                
google-api-python-client 1.6.7                
google-auth              1.4.2                
google-auth-httplib2     0.0.3                
google-auth-oauthlib     0.2.0                
google-cloud-bigquery    1.8.1                
google-cloud-core        0.29.1               
google-cloud-language    1.0.2                
google-cloud-storage     1.8.0                
google-cloud-translate   1.3.3                
google-colab             0.0.1a1              
google-pasta             0.1.1                
google-resumable-media   0.3.2                
googleapis-common-protos 1.5.6                
googledrivedownloader    0.3                  
graph-nets               1.0.2                
graphviz                 0.10.1               
greenlet                 0.4.15               
grpcio                   1.15.0               
gspread                  3.0.1                
gspread-dataframe        3.0.2                
gunicorn                 19.9.0               
gym                      0.10.11              
h5netcdf                 0.6.2                
h5py                     2.8.0                
HeapDict                 1.0.0                
holidays                 0.9.9                
html5lib                 0.9999999            
httpimport               0.5.16               
httplib2                 0.11.3               
humanize                 0.5.1                
hyperopt                 0.1.1                
ideep4py                 2.0.0.post3          
idna                     2.6                  
image                    1.5.27               
imageio                  2.4.1                
imagesize                1.1.0                
imbalanced-learn         0.4.3                
imblearn                 0.0                  
imgaug                   0.2.6                
imutils                  0.5.2                
inflect                  2.1.0                
intel-openmp             2019.0               
intervaltree             2.1.0                
ipykernel                4.6.1                
ipython                  5.5.0                
ipython-genutils         0.2.0                
ipython-sql              0.3.9                
ipywidgets               7.4.2                
itsdangerous             1.1.0                
jdcal                    1.4                  
jieba                    0.39                 
Jinja2                   2.10                 
jmespath                 0.9.3                
joblib                   0.13.1               
jpeg4py                  0.1.4                
jsonschema               2.6.0                
jupyter                  1.0.0                
jupyter-client           5.2.4                
jupyter-console          6.0.0                
jupyter-core             4.4.0                
kaggle                   1.5.2                
kapre                    0.1.3.1              
Keras                    2.2.4                
Keras-Applications       1.0.7                
Keras-Preprocessing      1.0.9                
keras-vis                0.4.1                
kiwisolver               1.0.1                
knnimpute                0.1.0                
librosa                  0.6.2                
lightgbm                 2.2.3                
llvmlite                 0.27.0               
lmdb                     0.94                 
lucid                    0.3.8                
lunardate                0.2.0                
lxml                     4.2.6                
magenta                  0.3.19               
Markdown                 3.0.1                
MarkupSafe               1.1.0                
matplotlib               3.0.2                
matplotlib-venn          0.11.5               
mesh-tensorflow          0.0.5                
mido                     1.2.6                
mir-eval                 0.5                  
missingno                0.4.1                
mistune                  0.8.4                
mkl                      2019.0               
mlxtend                  0.14.0               
mock                     2.0.0                
more-itertools           5.0.0                
moviepy                  0.2.3.5              
mpi4py                   3.0.0                
mpmath                   1.1.0                
msgpack                  0.5.6                
msgpack-numpy            0.4.4.2              
multiprocess             0.70.7               
multitasking             0.0.7                
murmurhash               1.0.1                
music21                  5.5.0                
natsort                  5.5.0                
nbconvert                5.4.0                
nbformat                 4.4.0                
netCDF4                  1.4.2                
networkx                 2.2                  
nibabel                  2.3.3                
nltk                     3.2.5                
nose                     1.3.7                
notebook                 5.2.2                
np-utils                 0.5.9.0              
numba                    0.40.1               
numexpr                  2.6.9                
numpy                    1.14.6               
nvidia-ml-py3            7.352.0              
oauth2client             4.1.3                
oauthlib                 3.0.1                
okgrade                  0.4.3                
olefile                  0.46                 
opencv-contrib-python    3.4.3.18             
opencv-python            3.4.5.20             
openpyxl                 2.5.9                
osqp                     0.5.0                
packaging                19.0                 
pandas                   0.22.0               
pandas-datareader        0.7.0                
pandas-gbq               0.4.1                
pandas-profiling         1.4.1                
pandocfilters            1.4.2                
pathlib                  1.0.1                
patsy                    0.5.1                
pbr                      5.1.2                
pexpect                  4.6.0                
pickleshare              0.7.5                
Pillow                   4.0.0                
pip                      19.0.1               
plac                     0.9.6                
plotly                   1.12.12              
pluggy                   0.8.1                
portpicker               1.2.0                
prefetch-generator       1.0.1                
preshed                  2.0.1                
pretty-midi              0.2.8                
prettytable              0.7.2                
progressbar2             3.38.0               
promise                  2.2.1                
prompt-toolkit           1.0.15               
protobuf                 3.6.1                
psutil                   5.4.8                
psycopg2                 2.7.6.1              
ptyprocess               0.6.0                
py                       1.7.0                
pyasn1                   0.4.5                
pyasn1-modules           0.2.4                
pycocotools              2.0.0                
pycparser                2.19                 
pydot                    1.3.0                
pydot-ng                 2.0.0                
pydotplus                2.0.2                
pyemd                    0.5.1                
pyglet                   1.3.2                
Pygments                 2.1.3                
pygobject                3.26.1               
pymc3                    3.6                  
pymongo                  3.7.2                
pymystem3                0.2.0                
PyOpenGL                 3.1.0                
pyparsing                2.3.1                
pysndfile                1.3.2                
PySocks                  1.6.8                
pystache                 0.5.4                
pystan                   2.18.1.0             
pytest                   3.10.1               
python-apt               1.6.3+ubuntu1        
python-chess             0.23.11              
python-dateutil          2.5.3                
python-louvain           0.13                 
python-rtmidi            1.2.1                
python-slugify           2.0.1                
python-utils             2.3.0                
pytz                     2018.9               
PyWavelets               1.0.1                
PyYAML                   3.13                 
pyzmq                    17.0.0               
qtconsole                4.4.3                
regex                    2018.1.10            
requests                 2.18.4               
requests-oauthlib        1.2.0                
resampy                  0.2.1                
rpy2                     2.9.5                
rsa                      4.0                  
s3fs                     0.2.0                
s3transfer               0.2.0                
scikit-image             0.13.1               
scikit-learn             0.20.2               
scipy                    1.1.0                
screen-resolution-extra  0.0.0                
scs                      2.0.2                
seaborn                  0.7.1                
setuptools               40.8.0               
setuptools-git           1.2                  
simplegeneric            0.8.1                
six                      1.11.0               
sklearn                  0.0                  
smart-open               1.8.0                
snowballstemmer          1.2.1                
sortedcontainers         2.1.0                
spacy                    2.0.18               
Sphinx                   1.8.4                
sphinxcontrib-websupport 1.1.0                
SQLAlchemy               1.2.17               
sqlparse                 0.2.4                
stable-baselines         2.2.1                
statsmodels              0.8.0                
sympy                    1.1.1                
tables                   3.4.4                
tabulate                 0.8.3                
tb-nightly               1.13.0a20190211      
tblib                    1.3.2                
tensor2tensor            1.11.0               
tensorboard              1.12.2               
tensorboardcolab         0.0.22               
tensorflow               1.12.0               
tensorflow-estimator     1.10.12              
tensorflow-hub           0.2.0                
tensorflow-metadata      0.9.0                
tensorflow-probability   0.5.0                
tensorpack               0.9.1                
termcolor                1.1.0                
terminado                0.8.1                
testpath                 0.4.2                
textblob                 0.15.2               
textgenrnn               1.4.1                
tf-estimator-nightly     1.14.0.dev2019021101 
tf-nightly-gpu           1.13.0.dev20190208   
tfds-nightly             0.0.2.dev201902070013
tflearn                  0.3.2                
Theano                   1.0.4                
thinc                    6.12.1               
toolz                    0.9.0                
torch                    1.0.0                
torchsummary             1.5.1                
torchtext                0.3.1                
torchvision              0.2.1                
tornado                  4.5.3                
tqdm                     4.28.1               
traitlets                4.3.2                
tweepy                   3.6.0                
typing                   3.6.6                
tzlocal                  1.5.1                
ujson                    1.35                 
umap-learn               0.3.7                
Unidecode                1.0.23               
uritemplate              3.0.0                
urllib3                  1.22                 
vega-datasets            0.7.0                
wcwidth                  0.1.7                
webencodings             0.5.1                
Werkzeug                 0.14.1               
wheel                    0.32.3               
widgetsnbextension       3.4.2                
wordcloud                1.5.0                
wrapt                    1.11.1               
xarray                   0.11.3               
xgboost                  0.7.post4            
xkit                     0.0.0                
xlrd                     1.1.0                
xlwt                     1.3.0                
yellowbrick              0.9.1                
zict                     0.1.3                
zmq                      0.0.0 
```",lot harder feel like install paste list library anyone else problem hopefully solve currently running appreciate help stuff hard learn beginner package version alabaster astor post bleach bottleneck chainer click community coverage coverall cycler daft decorator dill distributed post fa post flask folium future gast greenlet gym humanize post image inflect jinja lucid magenta markdown mock music nose notebook pillow pip pluggy promise six spacy sphinx table tabulate dev dev dev torch tornado wheel post,issue,positive,negative,neutral,neutral,negative,negative
462442407,"I don't know whether the other packages affect this but you can remove them all, and they will be automatically installed if tensorflow actually depends on them. ",know whether affect remove automatically actually,issue,negative,neutral,neutral,neutral,neutral,neutral
462439334,"Ah ok, and I assume this means removing traces of non-gpu tensorflow libraries? eg/ removing 
```
tensorflow-estimator     1.10.12              
tensorflow-hub           0.2.0                
tensorflow-metadata      0.9.0                
tensorflow-probability   0.5.0                
tensorflow-tensorboard   1.5.1
tf-estimator-nightly     1.14.0.dev2019021101 
tf-nightly               1.13.0.dev20190208
```
?",ah assume removing removing dev dev,issue,negative,neutral,neutral,neutral,neutral,neutral
462439000,"Closing as it's a TF installation issue as discussed in #1077.
It would be easier to tell if full logs were provided, as suggested in the issue template.",installation issue would easier tell full provided issue template,issue,negative,positive,positive,positive,positive,positive
462437609,You need either `tensorflow-gpu` or `tf-nightly-gpu` (and only one of them but not two).,need either one two,issue,negative,neutral,neutral,neutral,neutral,neutral
462436550,"Ok thanks, I'll look to change tensorflow version. If anyone runs into this problem my current Colab library versions are;
```
tensor2tensor            1.11.0               
tensorboard              1.12.2               
tensorboardcolab         0.0.22               
tensorflow               1.13.0rc0            
tensorflow-estimator     1.10.12              
tensorflow-hub           0.2.0                
tensorflow-metadata      0.9.0                
tensorflow-probability   0.5.0                
tensorflow-tensorboard   1.5.1                
tensorpack               0.9.1   
tf-estimator-nightly     1.14.0.dev2019021101 
tf-nightly               1.13.0.dev20190208   
tfds-nightly             0.0.2.dev201902070013
```",thanks look change version anyone problem current library dev dev dev,issue,negative,positive,neutral,neutral,positive,positive
462434724,Apparently it's telling you that your tensorflow does not support CUDA. You need a different version of tensorflow and I'm not familiar with how it work in colab,apparently telling support need different version familiar work,issue,negative,positive,positive,positive,positive,positive
462433823,"welp, my bad. As for the GPU training it seems you're right if it's as long as 10s/it. Looking though the messages I do get this;
```[0211 18:01:46 @gpu.py:43] WRN Found non-empty CUDA_VISIBLE_DEVICES. But TensorFlow was not built with CUDA support!```
when I run it with;
`!python train-atari.py --env Breakout-v0 --gpu 1` or 
```!python train-atari.py --env Breakout-v0 --gpu 0```

Is there a different set up that is needed for training in tensorflow to allow GPU training? ",bad training right long looking though get found built support run python python different set training allow training,issue,negative,negative,negative,negative,negative,negative
462431965,"You have 10s/it, not 10it/s.

From your log it does not appear that you're using GPU. Normally tensorflow should print something GPU-related when creating the session.",log appear normally print something session,issue,negative,positive,positive,positive,positive,positive
462430899,"Yeah. that's exactly why i'm confused!

I just lowered it to 1000 STEPS_PER_EPOCH (to see if anything would change) and as you can see for them attached pic, I have 10 it/s but apparently, it will still take nearly 3 hours. 

![image](https://user-images.githubusercontent.com/35103224/52583366-51a6d100-2e27-11e9-88b3-24d40035561e.png)
",yeah exactly confused see anything would change see attached pic apparently still take nearly image,issue,negative,negative,neutral,neutral,negative,negative
462429774,"If it/s is really 10, and STEPS_PER_EPOCH is defined 6000 (https://github.com/tensorpack/tensorpack/blob/49675590da8a39c649b5f0f5ba522a22b90e2d69/examples/A3C-Gym/train-atari.py#L40 )

Then you should finish 1 epoch in 10 minutes, right?",really defined finish epoch right,issue,negative,positive,positive,positive,positive,positive
462423839,"Fixes that issue, thanks. Doesn't fix the fact that it's forecast to train 1 epoch in over 10 hours with an it/s speed of ~10. Any thoughts on why this might be?",issue thanks fix fact forecast train epoch speed might,issue,negative,positive,positive,positive,positive,positive
462173577,Then I must be doing something extremely wrong - I have it running on google colab in GPU accelerated mode and the time to complete 1 6000 step epoch is ~8 hours still. It sounds like it's still training on CPU somehow.,must something extremely wrong running accelerated mode time complete step epoch still like still training somehow,issue,negative,negative,negative,negative,negative,negative
462170199,"On a GPU it should take <20min per epoch.
As the README says, training without GPU may not result in a good model.

Changing the step number only has an effect on the parameter schedulers defined in the callbacks.",take min per epoch training without may result good model step number effect parameter defined,issue,negative,positive,positive,positive,positive,positive
462169714,"> The argument `--load` should take a path to model, not graph.
> If your directory does not contain a model, it may be just the training job haven't finished one epoch and saved one.

Ah I see, I didn't realise that a model was only saved after each epoch. It takes me an estimated 9 hours to train 1 epoch. If say I can only train maximum 4 hours continuously, would you recommend drastically lowering the num of steps per epoch? And would this effect training in any significant way?

Appreciate the very fast response! ",argument load take path model graph directory contain model may training job finished one epoch saved one ah see model saved epoch train epoch say train maximum continuously would recommend drastically lowering per epoch would effect training significant way appreciate fast response,issue,positive,positive,positive,positive,positive,positive
462169421,"The argument `--load` should take a path to model, not graph.
If your directory does not contain a model, it may be just the training job haven't finished one epoch and saved one.",argument load take path model graph directory contain model may training job finished one epoch saved one,issue,negative,neutral,neutral,neutral,neutral,neutral
462110325,"Related to (3), if it is `sess` not `hooked_sess`, does it mean `self.trainer.sess`, or a new sess like `sess = tf.Session()` in the callback class? Or is there another way to access an existing usable `sess`?",related sess mean new sess like sess class another way access usable sess,issue,negative,negative,neutral,neutral,negative,negative
462100239,"1. Yes
2. My bad. get_tensor_by_name is a method of a graph (self.graph.get_tensor_by_name). 
3. sess, not hooked_sess. See https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.Trainer.hooked_sess ",yes bad method graph sess see,issue,negative,negative,negative,negative,negative,negative
462099589,"Thanks for your advice!
Please let me make sure three things in your advice:
(1) the placeholders and all tensors will have `mypredict/` prefix in their names (e.g., `mypredict/image:0`), right?
(2) You mentioned `tf.get_tensor_by_name`, but more exactly, shouldn't I use `self.get_tensors_maybe_in_tower`? (I got an error that no `get_tensor_by_name` in `tf`.)
(3) Does ""use session.run to evaluate them in your callback."" mean I can use `self.trainer.hooked_sess.run` in my callback? Or is there another session?


",thanks advice please let make sure three advice prefix right exactly use got error use evaluate mean use another session,issue,positive,positive,positive,positive,positive,positive
462090698,"You can create arbitrary operations in the graph in your callback's `setup_graph` method. For example:

```
with tf.variable_scope(tf.get_variable_scope(), reuse=True), TowerContext('mypredict', is_training=False):
    placeholders = [ my new placeholders ]
    build_graph(placeholders)
```

And you can just use `tf.get_tensor_by_name` to get your input/output tensors and use session.run to evaluate them in your callback.",create arbitrary graph method example new use get use evaluate,issue,negative,positive,neutral,neutral,positive,positive
462078821,"Great, thanks for a quick reply!",great thanks quick reply,issue,positive,positive,positive,positive,positive,positive
462073946,"Thanks! Now it's disabled.

It was enabled it at first because it solves some bug in COND_V1.
But later I realized that V2 has some of its own bugs as well (e.g. https://github.com/tensorflow/tensorflow/issues/24517). So let's leave the decision to users.",thanks disabled first bug later well let leave decision,issue,positive,positive,neutral,neutral,positive,positive
461684539,"1 was fixed now.

For 2, an alternative way to perhaps simplify prediction is to do the mean subtraction in the graph, and just call the same `build_graph` function in inference.",fixed alternative way perhaps simplify prediction mean subtraction graph call function inference,issue,negative,negative,negative,negative,negative,negative
461681743,"It seems that while TF is been moved to TF2, the official model repo will be unmaintained and rely on deprecated slim. Therefore closing this issue.
It's likely that TF will create a new set of pre-trained models/code for TF 2 and we can asses it then.",official model unmaintained rely slim therefore issue likely create new set,issue,negative,positive,neutral,neutral,positive,positive
461681091,"(I think there was some bug in github and I couldn't see this issue a couple of days ago, thus the late reply)

I do not see why this issue is related to tensorpack. The log says that `tf.gather` on that device does not support indexing with -1, and this is a tensorflow issue.

Feel free to reopen if you have any evidence to believe otherwise.",think bug could see issue couple day ago thus late reply see issue related log device support indexing issue feel free reopen evidence believe otherwise,issue,positive,positive,neutral,neutral,positive,positive
461534208,"Tensorpack is a training interface and we do not implement any particular deep learning algorithm as a feature in tensorpack. For now at least I did not see anything that stops you from optimizing model/parameters, but this certainly depends on what exactly you want to do.",training interface implement particular deep learning algorithm feature least see anything certainly exactly want,issue,negative,positive,neutral,neutral,positive,positive
461533298,"1. That makes sense. I don't believe it will have visible effect to accuracy since the difference is < 0.5 out of image range of 255. But using the training mean is the legitimate thing to do.

2. Tensorpack saves with tensorflow's saver, which saves variables in the graph only. If you want to save anything else to the checkpoint, you can create a variable in the graph with this value. This is a tensorflow issue and unrelated to tensorpack.",sense believe visible effect accuracy since difference image range training mean legitimate thing saver graph want save anything else create variable graph value issue unrelated,issue,positive,negative,negative,negative,negative,negative
461530949,"1 is a python issue
2 NHWC is a tensorflow issue
2 remove GPU tracking is the natural thing to do if you don't use GPU

So I did not see anything that tensorpack should do regarding your issues. Do you have any proposals?",python issue issue remove natural thing use see anything regarding,issue,negative,positive,neutral,neutral,positive,positive
461102793,"To load a model to do fine tune: https://tensorpack.readthedocs.io/tutorial/save-load.html#load-a-model-to-a-session

To train on a new dataset: https://tensorpack.readthedocs.io/tutorial/dataflow.html
https://tensorpack.readthedocs.io/tutorial/extend/dataflow.html",load model fine tune train new,issue,negative,positive,positive,positive,positive,positive
460990347,I didnt got the answer which is desired. KIndly let s know how can i finetune flownet algo with new dataset? Kindly suggest,didnt got answer desired kindly let know new kindly suggest,issue,positive,positive,positive,positive,positive,positive
460761641,Should be fixed now. In general I recommend `logger.set_logger_dir(path)` to have a full control over the file name. ,fixed general recommend path full control file name,issue,negative,positive,positive,positive,positive,positive
460030590,"As the documentation said:
```
    """""" Use :func:`xla.compile` to compile the tower function.
    Note that XLA has very strong requirements on the tower function, e.g.:

    1. limited op support
    2. inferrable shape
    3. no summary support

    and many tower functions cannot be compiled by XLA.
    Don't use it if you don't understand it.
    """"""
```

To optimize mask rcnn you may need to manually pick a subgraph and use `xla.compile` yourself on it.

This question is unrelated to tensorpack and just due to tensorflow's limitation.",documentation said use compile tower function note strong tower function limited support shape summary support many tower use understand optimize mask may need manually pick use question unrelated due limitation,issue,positive,positive,positive,positive,positive,positive
459129375,"Ah, I was misunderstanding how `box_logits` were parameterized. That's much clearer now - thank you for the help!",ah misunderstanding much clearer thank help,issue,negative,positive,positive,positive,positive,positive
459113965,"1. No. Box logits are not defined on any particular coordinates. It only encodes the ratio w.r.t anchors and does not have its own absolute scale.
Refer to the fast rcnn paper for details.

2. Yes. Anchors are defined on coordidates of the input image.

3. 

4. See 1. ",box defined particular ratio absolute scale refer fast paper yes defined input image see,issue,negative,positive,positive,positive,positive,positive
459110189,"Okay, that's helpful, thank you. And am I correct that: 
- The `rpn_head` outputs `box_logits` which are in the coordinate system of the given FPN layer feature map. 
- Then `anchor.decode_logits(box_logits)` uses the anchors for that layer to convert from `box_logits` to the coordinate system of the input image. 
- This works because each FPN layer is associated with a given anchor size so as you move up in the FPN, the feature space is smaller, but the anchor boxes are larger. 
- The `rpn_head` isn't aware of which FPN layer the input comes from, but because the anchor boxes are larger in higher layers, the same (x1, y1, h1, w1) output of `rpn_head` will lead to a larger proposal when output on FPN_LVL_4 than when output on FPN_LVL_3.",helpful thank correct system given layer feature map layer convert system input image work layer associated given anchor size move feature space smaller anchor aware layer input come anchor higher output lead proposal output output,issue,positive,positive,positive,positive,positive,positive
458770312,"> Won't the proposals be using the coordinate system of the feature map, 

No. The proposals are generated from the feature map but their values are in the coordinate system of the input image.",wo system feature map feature map system input image,issue,negative,neutral,neutral,neutral,neutral,neutral
458387118,"Oh, I see. It's a historical commit. Thanks.",oh see historical commit thanks,issue,positive,positive,neutral,neutral,positive,positive
458356076,"For the record, this was the code I used in `train.py` to reproduce the issue:
```python
    from tensorpack.tfutils.export import ModelExporter
    finalize_configs(is_training=False)
    pred_config = PredictConfig(
                model=MODEL,
                session_init=get_model_loader(""/PATH/models/FasterRCNN/COCO-R50FPN-MaskRCNN-Standard.npz""),
                input_names=MODEL.get_inference_tensor_names()[0],
                output_names=['output/boxes', 'output/scores', 'output/labels'])
 
    pbfile = ""./compact_graph.pb""
    ModelExporter(pred_config).export_compact(pbfile, optimize=False)
 
    with tf.gfile.FastGFile(pbfile, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
 
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    sess = tf.Session(config=config)
    sess.graph.as_default()
    with tf.Graph().as_default():
        tf.import_graph_def(graph_def, name='')
    import sys; sys.exit()
```

Closing since it's a tensorflow issue. Will add an option in exporter to skip `optimize_for_inference` to avoid this.",record code used reproduce issue python import true sess import since issue add option exporter skip avoid,issue,negative,positive,positive,positive,positive,positive
458338006,"I do not have other recommendations except to manually write a faster graph (which I sometimes do). Inference is not the focus of this project, so we only do what tensorflow can do in this project.",except manually write faster graph sometimes inference focus project project,issue,negative,neutral,neutral,neutral,neutral,neutral
458336583,"Thanks for the reply!
Do you have any recommendations for other ways of optimizing the frozen graph?",thanks reply way frozen graph,issue,negative,positive,positive,positive,positive,positive
458332946,"Thanks for the details!
I can reproduce your issue, and removing the `optimize_for_inference` function call at  https://github.com/tensorpack/tensorpack/blob/21c494697faa40db0e280a3c53abad2521b2e1f6/tensorpack/tfutils/export.py#L74-L79 makes it work, which agrees with your findings.
So it appears to be a bug in tensorflow. It optimizes a good graph to something wrong.",thanks reproduce issue removing function call work bug good graph something wrong,issue,negative,positive,positive,positive,positive,positive
457900686,"> I didn't find any anchors in this line

Because anchors are not needed in this particular line and is needed later.

I recommend you to read the series of object detection papers instead. In general we do not answer machine learning questions in tensorpack issues. This is a place to learn to use the tensorpack library, not a place to learn machine learning models.",find line particular line later recommend read series object detection instead general answer machine learning place learn use library place learn machine learning,issue,negative,positive,neutral,neutral,positive,positive
457899774,"Which line of code?
Let me explain a little bit more, I've spent couple of days read your code and I have two questions:

1.  Does RPN only use resnet's conv4 when calculating label and box in this function? I didn't find any anchors in this line 'rpn_label_logits, rpn_box_logits = rpn_head('rpn', featuremap, cfg.RPN.HEAD_DIM, cfg.RPN.NUM_ANCHOR) '. And I thought this line constructed the basic model of RPN head.

def rpn_head(featuremap, channel, num_anchors): #featuremap 1024 15
    """"""
    Returns:
        label_logits: fHxfWxNA
        box_logits: fHxfWxNAx4
    """"""
    with argscope(Conv2D, data_format='channels_first',
                  kernel_initializer=tf.random_normal_initializer(stddev=0.01)):
        hidden = Conv2D('conv0', featuremap, channel, 3, activation=tf.nn.relu) #1024个3*3的卷积核对resnet的输出conv4进行卷积

        label_logits = Conv2D('class', hidden, num_anchors, 1) #创建分类的卷积网络
        box_logits = Conv2D('box', hidden, 4 * num_anchors, 1) #创建回归的卷积网络
        # 1, NA(*4), im/16, im/16 (NCHW)

        label_logits = tf.transpose(label_logits, [0, 2, 3, 1])  # 1xfHxfWxNA
        label_logits = tf.squeeze(label_logits, 0)  # fHxfWxNA

        shp = tf.shape(box_logits)  # 1x(NAx4)xfHxfW
        box_logits = tf.transpose(box_logits, [0, 2, 3, 1])  # 1xfHxfWx(NAx4)
        box_logits = tf.reshape(box_logits, tf.stack([shp[2], shp[3], num_anchors, 4]))  # fHxfWxNAx4
    return label_logits, box_logits

2. I'm also wondering the function of the code below.
Is this code only make a little adjustment to the RPN bounding box?

pred_boxes_decoded = anchors.decode_logits(rpn_box_logits)

I'm a 菜鸟 and thanks for your time!",line code let explain little bit spent couple day read code two use calculating label box function find line thought line basic model head channel hidden channel hidden hidden na return also wondering function code code make little adjustment bounding box thanks time,issue,negative,negative,neutral,neutral,negative,negative
457898023,Closing due to lack of activity. Feel free to reopen if the problem is not solved.,due lack activity feel free reopen problem,issue,negative,positive,positive,positive,positive,positive
457897843,"RPN does use anchors and you can find the word ""anchors"" in the code.",use find word code,issue,negative,neutral,neutral,neutral,neutral,neutral
457315077,"Ah I see. I'd be happy to contribute, but that does seem complicated while supporting Python 2.",ah see happy contribute seem complicated supporting python,issue,positive,positive,positive,positive,positive,positive
457049284,"You're using new version of the command line arguments with old version of code.
The new version of code uses `RESNET_NUM_BLOCKS` only and has no issues.",new version command line old version code new version code,issue,negative,positive,positive,positive,positive,positive
457000997,It would be great to have. But I'm not sure how elegant it will be given that we still support Python 2. Contributions/suggestions are very welcome. ,would great sure elegant given still support python welcome,issue,positive,positive,positive,positive,positive,positive
456520927,Works! Thanks for the quick reply!,work thanks quick reply,issue,negative,positive,positive,positive,positive,positive
455755981,"You can convert all `DT_FLOAT` from a float32 model to `DT_HALF`, resulting a float16 model. See my https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/graph.py#L147 for example.",convert float model resulting float model see example,issue,negative,neutral,neutral,neutral,neutral,neutral
455726138,dict is still not supported in many dataflows so I haven't touched these codepath yet. Thanks for finding the bug!,still many touched yet thanks finding bug,issue,negative,positive,positive,positive,positive,positive
455480564,"Good to know. 
Probably dask is too old and require an old version of numpy.
Closing.",good know probably old require old version,issue,negative,positive,positive,positive,positive,positive
455459692,"Since the issue comes from import tensorflow.contrib, this is not a tensorpack issue.
Since the stack trace eventually come from dask, you should try either upgrading dask or uninstalling dask",since issue come import issue since stack trace eventually come try either,issue,negative,neutral,neutral,neutral,neutral,neutral
455457759,"I reinstalled the tensorpack and reboot, then the process aborted only before **import tensorpack**
**>>> import tensorpack**
/opt/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorpack/__init__.py"", line 17, in <module>
    from tensorpack.models import *
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorpack/models/__init__.py"", line 48, in <module>
    _global_import(module_name)
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorpack/models/__init__.py"", line 29, in _global_import
    p = __import__(name, globals(), locals(), level=1)
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorpack/models/regularize.py"", line 24, in <module>
    l2_regularizer = tf.contrib.layers.l2_regularizer
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
    module = self._load()
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
    module = importlib.import_module(self.__name__)
  File ""/opt/anaconda2/lib/python2.7/importlib/__init__.py"", line 37, in import_module
    __import__(name)
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/__init__.py"", line 40, in <module>
    from tensorflow.contrib import distributions
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distributions/__init__.py"", line 38, in <module>
    from tensorflow.contrib.distributions.python.ops.estimator import *
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py"", line 21, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/__init__.py"", line 95, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/__init__.py"", line 28, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/__init__.py"", line 30, in <module>
    from tensorflow.contrib.learn.python.learn import estimators
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py"", line 302, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py"", line 35, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 36, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 52, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py"", line 26, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data
  File ""/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py"", line 33, in <module>
    import dask.dataframe as dd
  File ""/opt/anaconda2/lib/python2.7/site-packages/dask/dataframe/__init__.py"", line 3, in <module>
    from .core import (DataFrame, Series, Index, _Frame, map_partitions,
  File ""/opt/anaconda2/lib/python2.7/site-packages/dask/dataframe/core.py"", line 19, in <module>
    from .. import array as da
  File ""/opt/anaconda2/lib/python2.7/site-packages/dask/array/__init__.py"", line 7, in <module>
    from .routines import (take, choose, argwhere, where, coarsen, insert,
  File ""/opt/anaconda2/lib/python2.7/site-packages/dask/array/routines.py"", line 253, in <module>
    @wraps(np.matmul)
  File ""/opt/anaconda2/lib/python2.7/functools.py"", line 33, in update_wrapper
    setattr(wrapper, attr, getattr(wrapped, attr))
AttributeError: 'numpy.ufunc' object has no attribute '__module__'


>>> import numpy
>>> numpy.__version__
'1.16.0'
>>> 
",process aborted import import conversion second argument float future float import recent call last file line module file line module import file line module file line name file line module file line module file line module file line name file line module import file line module import file line module import file line module import file line module import file line module import file line module import file line module import file line module import estimator file line module import file line module import file line module import file line module import series index file line module import array da file line module import take choose coarsen insert file line module file line wrapper wrapped object attribute import,issue,negative,neutral,neutral,neutral,neutral,neutral
455449474,"You did not follow the issue template to post the output of `python -c 'import tensorpack; print(tensorpack.__version__);'`. 
Please do so and also print `tensorpack.__file__` which will help you find your installation problems.",follow issue template post output python print please also print help find installation,issue,positive,neutral,neutral,neutral,neutral,neutral
455448527,"You're not importing a correct (latest) version of tensorpack. And it's likely you have multiple versions of tensorpack installed in your system. Please remove tensorpack by running `pip2 uninstall tensorpack`, `pip uninstall tensorpack` each for several times and clean your PYTHONPATH environment variables. Then install tensorpack following the issue template you posted above.

",correct latest version likely multiple system please remove running pip pip several time clean environment install following issue template posted,issue,positive,positive,positive,positive,positive,positive
455445864,"and when I exec the code **python mnist-visualizations.py** 
the following errors dump:
/opt/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[0118 14:52:56 @logger.py:94] WRN Log directory train_log/mnist-visualizations exists! Please either backup/delete it, or use a new directory.
[0118 14:52:56 @logger.py:96] WRN If you're resuming from a previous run you can choose to keep it.
[0118 14:52:56 @logger.py:97] Select Action: k (keep) / b (backup) / d (delete) / n (new) / q (quit):
d
[0118 14:52:58 @logger.py:74] Argv: mnist-visualizations.py
[0118 14:52:58 @fs.py:89] WRN Env var $TENSORPACK_DATASET not set, using /home/lthpc/tensorpack_data for datasets.
Traceback (most recent call last):
  File ""mnist-visualizations.py"", line 128, in <module>
    model=Model(),
TypeError: Can't instantiate abstract class Model with abstract methods _get_inputs
",code python following dump conversion second argument float future float import log directory please either use new directory previous run choose keep select action keep backup delete new quit set recent call last file line module ca abstract class model abstract,issue,negative,positive,neutral,neutral,positive,positive
455425915,"In all processes:
`starting_epoch=[the next epoch]`

In main processes or all processes (doesn't matter):
`sessinit=[load your model]`

These will do what `AutoResumeTrainConfig` is supposed to.",next epoch main matter load model supposed,issue,negative,positive,neutral,neutral,positive,positive
455421653,"Being able to automatically resume requires logging directory. Using horovod requires different logging directory for each process. So it appears there is no simple solution to this.

What I did is to manually set the epoch and load from a given checkpoint",able automatically resume logging directory different logging directory process simple solution manually set epoch load given,issue,negative,positive,positive,positive,positive,positive
455081834,"I solved the problem. You are rights, problem was  with  not correct matching  `COCO_id_to_category_id` and  `class_names`.

Thank you for the help.",problem problem correct matching thank help,issue,negative,neutral,neutral,neutral,neutral,neutral
455077193,After looking closer it appears that you probably have made changes to the code and the COCO id mapping was messed up.,looking closer probably made code coco id,issue,negative,neutral,neutral,neutral,neutral,neutral
455074702,"It does look like you may have some issues with your dataset, or the data loading code. My run prints the following in the log:
```
| class          |   #box |                                                                                                                                                                         |:---------------|-------:|                                                                       
| BG             |      0 |                                                                                                                                                                         | person         | 257221 |                                                                       
| bicycle        |   7056 |                                                                                                                                                                         | car            |  43532 |                                                                       
| motorcycle     |   8654 |                                                                       
| airplane       |   5129 |                                                                                                                                                                         | bus            |   6061 |                                                                                                                                                                         | train          |   4570 |                                                                       
| truck          |   9970 |                                                                       
| boat           |  10575 |                                                                       
| traffic light  |  12834 |                      
| fire hydrant   |   1865 |                      
| stop sign      |   1983 |                      
| parking meter  |   1283 |                      
| bench          |   9820 |                      
| bird           |  10512 |                      
| cat            |   4766 |                      
| dog            |   5500 |                      
| horse          |   6567 |                      
| sheep          |   9223 |                      
| cow            |   8014 |                      
| elephant       |   5484 |                      
| bear           |   1294 |                      
| zebra          |   5269 |                      
| giraffe        |   5128 |                      
| backpack       |   8713 |                      
| umbrella       |  11265 |                      
| handbag        |  12340 |                      
| tie            |   6445 |
| suitcase       |   6112 |
| frisbee        |   2681 |
| skis           |   6620 |
| snowboard      |   2679 |
| sports ball    |   6291 |
| kite           |   8792 |
| baseball bat   |   3273 |
| baseball glove |   3742 |
| skateboard     |   5536 |
| surfboard      |   6093 |
| tennis racket  |   4803 |
| bottle         |  24070 |
| wine glass     |   7839 |
| cup            |  20574 |
| fork           |   5474 |
| knife          |   7759 |
| spoon          |   6159 |
| bowl           |  14323 |
```
which is quite different from what's printed in your logs. Yours have many classes with no training examples.",look like may data loading code run following log class box person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe umbrella handbag tie suitcase sport ball kite baseball bat baseball glove surfboard tennis racket bottle wine glass cup fork knife spoon bowl quite different printed many class training,issue,negative,positive,positive,positive,positive,positive
455073121,Closing due to lack of activity. If you have more questions you can open a new issue.,due lack activity open new issue,issue,negative,positive,neutral,neutral,positive,positive
455072665,"Closing due to no response.
Feel free to reopen if you have any updates to share",due response feel free reopen share,issue,positive,positive,positive,positive,positive,positive
454993958,"Without a GPU, inference can also run in NCHW format if tensorflow is built with MKL support.",without inference also run format built support,issue,negative,neutral,neutral,neutral,neutral,neutral
454987617,"Made another run with TF1.12, cuda9.0, cudnn7.4.1, latest tensorpack mater on Jan 14, unmodified example code. Observed the same results. Closing as the issue cannot be reproduced and is likely an environment issue.",made another run latest mater unmodified example code issue likely environment issue,issue,negative,positive,positive,positive,positive,positive
454898773,Closing as this issue is about COCO format and not very related to tensorpack.,issue coco format related,issue,negative,neutral,neutral,neutral,neutral,neutral
454877616,"Will push a fix soon. Note that `Deconv2D` is not a public interface and you should use `Conv2DTranspose` instead: https://tensorpack.readthedocs.io/modules/models.html, which will not have such issues.",push fix soon note public interface use instead,issue,negative,neutral,neutral,neutral,neutral,neutral
454556635,"I cannot reproduce your issue when running with the same config on 2 GPUs. It's best to include your cuda version, cudnn version as well and try upgrading cudnn to a newer version. Also it may help to check that your dataset looks reasonable, your opencv is able correctly read images.

Also note that running with 2 GPUs is not how the models in the model zoo or in the paper are trained. It may require some parameter tuning and we cannot help with tuning parameters. You may try to use a smaller initial learning rate (`WARMUP_INIT_LR`), for example.",reproduce issue running best include version version well try version also may help check reasonable able correctly read also note running model zoo paper trained may require parameter tuning help tuning may try use smaller initial learning rate example,issue,positive,positive,positive,positive,positive,positive
454249250,"It is mentioned in the pull request template https://github.com/tensorpack/tensorpack/blob/master/.github/pull_request_template.md, which you can see when opening a pull request, that we do not add layers into tensorpack.",pull request template see opening pull request add,issue,negative,neutral,neutral,neutral,neutral,neutral
453878616,"The formula you wrote in your original question requires second order gradient of `f`.
`f` contains `CropAndResize` because `f` is a faster r-cnn.
This is a tensorflow question and unrelated to tensorpack. You can solve it by implementing the tensorflow operation that computes the second order gradient.",formula wrote original question second order gradient faster question unrelated solve operation second order gradient,issue,negative,positive,positive,positive,positive,positive
453878412,"Thank you,  I've been searching for a while still not found any useful solutions.

Is there a way that I can fix this error ?

cause I am not quite sure why does it contain the` CropAndResizeGradImage` operation when I use `tf.gradients` and why doesn't it contained when not using `tf.gradients`
 ",thank searching still found useful way fix error cause quite sure contain operation use,issue,positive,positive,positive,positive,positive,positive
453877584,If you didn't use `tf.gradients` then your forward pass won't contain the `CropAndResizeGradImage` operation.,use forward pas wo contain operation,issue,negative,neutral,neutral,neutral,neutral,neutral
453877487,"thank you for your reply,
Do you have any ideas why does this error pop-up after I add tf.gradients in my method?
cause it doesn't happen before I did this
",thank reply error add method cause happen,issue,negative,neutral,neutral,neutral,neutral,neutral
453852633,`CropAndResizeGradImage` is the gradient of `CropAndResize` and the error is saying tensorflow does not know how to compute the gradient of `CropAndResizeGradImage` which is required by your method,gradient error saying know compute gradient method,issue,negative,neutral,neutral,neutral,neutral,neutral
453824831,"Thanks for your kindness reply, 

yes, `tf.gradients` is what I used here
`grads = tf.gradients(total_cost, list(weights.values()))`

but it ends up with error :
> LookupError: No gradient defined for operation 'tower0/gradients/tower0/roi_align/crop_and_resize/CropAndResize_grad/CropAndResizeGradImage' (op type: CropAndResizeGradImage)

please help",thanks kindness reply yes used list error gradient defined operation type please help,issue,positive,positive,positive,positive,positive,positive
453823035,"Please print the the keys and their shape of var_to_dump, Maybe you don't import the meta and load ckpt  correctly.",please print shape maybe import meta load correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
453806467,"If you want to obtain the gradients you can just call `tf.gradients`. The optimizer interface seems not useful to you in this case.
The name is generated automatically by tensorflow and is you don't have to care about that.",want obtain call interface useful case name automatically care,issue,positive,positive,positive,positive,positive,positive
453797363,"I tried computing gradients before final cost, but having error:
> Traceback (most recent call last):
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 551, in gradients
    grad_fn = ops.get_gradient_function(op)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2134, in get_gradient_function
    return _gradient_registry.lookup(op_type)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/framework/registry.py"", line 93, in lookup
    ""%s registry has no entry for: %s"" % (self._name, name))
LookupError: gradient registry has no entry for: CropAndResizeGradImage

> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""train.py"", line 558, in <module>
>     launch_train_with_config(traincfg, trainer)
>   File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/interface.py"", line 85, in launch_train_with_config
>     model._build_graph_get_cost, model.get_optimizer)
>   File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/utils/argtools.py"", line 181, in wrapper
>     return func(*args, **kwargs)
>   File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/tower.py"", line 203, in setup_graph
>     train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
>   File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/trainers.py"", line 172, in _setup_graph
>     self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)
>   File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 222, in build
>     use_vs=[False] + [True] * (len(self.towers) - 1))
>   File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 118, in build_on_towers
>     ret.append(func())
>   File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/tower.py"", line 246, in get_grad_fn
>     aggregation_method=self.AGGREGATION_METHOD)
>   File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 414, in compute_gradients
>     colocate_gradients_with_ops=colocate_gradients_with_ops)
>   File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 555, in gradients
>     (op.name, op.type))
> LookupError: No gradient defined for operation 'tower0/gradients/tower0/roi_align/crop_and_resize/CropAndResize_grad/CropAndResizeGradImage' (op type: CropAndResizeGradImage)

why is the name 'tower' after 'tower' such as  'tower0/gradients/tower0/***' ",tried final cost error recent call last file line file line return file line registry entry name gradient registry entry handling exception another exception recent call last file line module trainer file line file line wrapper return file line input file line input file line build false true file line file line file line file line gradient defined operation type name,issue,negative,negative,neutral,neutral,negative,negative
453793592,"sorry, I did not make my question  clear.
let me define it again.

My target is to update weight two times in an iteration. So here's what I do
I'm using a function that feeds inputs and weights and  return cost  `f(inputs, weights) -> cost` 
computes the gradients as ∇θ , learning rate as `lr`
Therefore, my final cost would be `f(inputs,  θ - lr*∇θ )` 

So, I have to access the accumulation  and the learning rate for my final cost.
Since the optimizer is defined in `class DetectionModel(ModelDesc):   def optimizer(self):` ,
is there any way I can access the slots and learning rate of optimizer through the tensorpack interface?",sorry make question clear let define target update weight two time iteration function return cost cost learning rate therefore final cost would access accumulation learning rate final cost since defined class self way access learning rate interface,issue,negative,negative,neutral,neutral,negative,negative
453718694,"> Hi, I'd like to know how to implement an Update_Weights function :
weights = Update_Weights( loss, weights)

What does this function do and how is it related to tensorpack?",hi like know implement function loss function related,issue,negative,neutral,neutral,neutral,neutral,neutral
453715770,"Hi, I'd like to know how to implement an Update_Weights function : 
`weights = Update_Weights( loss, weights)` 
base on the optimizer of FasterRCNN below
`# optimizer function

     def optimizer(self):

        lr = tf.get_variable('learning_rate', initializer=0.003, trainable=False)
        tf.summary.scalar('learning_rate-summary', lr)

        # The learning rate is set for 8 GPUs, and we use trainers with average=False.
        lr = lr / 8.
        opt = tf.train.MomentumOptimizer(lr, 0.9)
        if cfg.TRAIN.NUM_GPUS < 8:
            opt = optimizer.AccumGradOptimizer(opt, 8 // cfg.TRAIN.NUM_GPUS)

        return opt`

thank you",hi like know implement function loss base function self learning rate set use opt opt opt return opt thank,issue,negative,negative,negative,negative,negative,negative
453634378,"For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

Preliminary suggestions:
Check your checkpoint by `tf.train.NewCheckpointReader`
Check what your code has loaded by printing",anyone better understand diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected preliminary check check code loaded printing,issue,positive,positive,positive,positive,positive,positive
453569413,You don't need to call `warpaffine` yourself. You'll need to call `augment_return_params` and `augment_with_params` yourself.,need call need call,issue,negative,neutral,neutral,neutral,neutral,neutral
453444695,"1. the image augmentation methods do not accept batch dimension, and do not implement augment_with_params. so i would need to call get_params and then apply warpaffine myself",image augmentation accept batch dimension implement would need call apply,issue,negative,neutral,neutral,neutral,neutral,neutral
453434329,"1. apply them after batching by calling the augmentor's methods yourself

2. ""inverse"" is not well-defined and impossible for many augmentors. So you'll need to do that yourself. The parameters an augmentor used is available when you call its methods yourself.

See docs at https://tensorpack.readthedocs.io/modules/dataflow.imgaug.html#tensorpack.dataflow.imgaug.Augmentor",apply calling inverse impossible many need used available call see,issue,negative,positive,neutral,neutral,positive,positive
453320892,"Segmentation expects a different format (x1, y1, x2, y2 ,x3, y3 ...). The documentation should have the details",segmentation different format documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
452992203,"Also, just found that the HED example uses the layer with channel=1 only, so there is nothing to fix. But I'll add some notes to the code.
Closing as resolved. Thanks again!",also found example layer nothing fix add code resolved thanks,issue,positive,positive,positive,positive,positive,positive
452991728,"> Might be helpful as a reference. But it will have the same issue: personally I would not trust any non-standard layers written by others.

I agree. Someone should at least read the code before using it.

> `tf.image.resize_bilinear` has its own issues as well and it certainly is not equivalent to this layer in tensorpack during upsampling. That's why they're called non-standard.

Got it. I'll dig harder on TF's code.

All in all, pretty happy to contribute. I love tensorpack much more than keras personally lol",might helpful reference issue personally would trust written agree someone least read code well certainly equivalent layer got dig harder code pretty happy contribute love much personally,issue,positive,positive,positive,positive,positive,positive
452985134,"> A separate non-standard layer codebase might be helpful though.

Might be helpful as a reference. But it will have the same issue: personally I would not trust any non-standard layers written by others.

`tf.image.resize_bilinear` has its own issues as well and it certainly is not equivalent to this layer in tensorpack during upsampling. That's why they're called non-standard.",separate layer might helpful though might helpful reference issue personally would trust written well certainly equivalent layer,issue,positive,positive,positive,positive,positive,positive
452983361,"> Thanks a lot for finding this!
> A great example of what I wrote here: https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries
> 
> > It’s best to not trust others’ layers!
> > For non-standard layers that’s not included in TensorFlow or Tensorpack, it’s best to implement them yourself. Non-standard layers often do not have a mathematical definition that people all agree on, and different people can implement it differently. Also, deep learning models on github often have bugs, especially when there is no reproduced experiments with the code.
> > For your own good, it’s best to implement the layers yourself. This is also why Tensorpack does not contain non-standard layers.
> 
> `BilinearUpSample` was made un-public and removed from documentation (https://tensorpack.readthedocs.io/modules/models.html) a while ago because it's a non-standard layer and we should not provide an implementation for it. But it's good to know the bug!

I see, but some of the non-standard layers like upsampling are pretty common. A separate non-standard layer codebase might be helpful though.
The reason I hesitate to use TF's implementation is that the image.resize_* functions are messy (at least in resize_area) and might cause some weird things to happen. tf.image.resize_bilinear works fine though.",thanks lot finding great example wrote best trust included best implement often mathematical definition people agree different people implement differently also deep learning often especially code good best implement also contain made removed documentation ago layer provide implementation good know bug see like pretty common separate layer might helpful though reason hesitate use implementation messy least might cause weird happen work fine though,issue,positive,positive,positive,positive,positive,positive
452941460,"> So, will it shuffle again after it produce everything in the list?

Yes. That's literally what the code says. You can find tutorial online if you don't understand what ""yield"" means.

> such as, after training an epoch, will it shuffle the list again

That depends on your definition of ""epoch"". Dataflow knows nothing about training and nothing about ""epoch"".",shuffle produce everything list yes literally code find tutorial understand yield training epoch shuffle list definition epoch nothing training nothing epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
452940592,"So, will it shuffle again after it produce everything in the list?
such as, after training an epoch, will it shuffle the list again",shuffle produce everything list training epoch shuffle list,issue,negative,neutral,neutral,neutral,neutral,neutral
452932759,"```python
        idxs = np.arange(len(self.lst))
        self.rng.shuffle(idxs)
        for k in idxs:
            yield self.lst[k]`
```
You can see from the code above that it will produce everything in the list after one shuffle.

This has nothing to do with training iteration because dataflow is unrelated to training.",python yield see code produce everything list one shuffle nothing training iteration unrelated training,issue,negative,neutral,neutral,neutral,neutral,neutral
452930468,"Thank you helps a lot
 I'd like to know according to code below:

 `       class DataFromList(RNGDataFlow):

    def __init__(self, lst, shuffle=True):
        """"""
        Args:
            lst (list): input list. Each element is a datapoint.
            shuffle (bool): shuffle data.
        """"""
        super(DataFromList, self).__init__()
        self.lst = lst
        self.shuffle = shuffle

    def __len__(self):
        return len(self.lst)

    def __iter__(self):
        if not self.shuffle:
            for k in self.lst:
                yield k
        else:
            idxs = np.arange(len(self.lst))
            self.rng.shuffle(idxs)
            for k in idxs:
                yield self.lst[k]`

Will the data list shuffle every training iteration?
",thank lot like know according code class self list input list element shuffle bool shuffle super self shuffle self return self yield else yield data list shuffle every training iteration,issue,positive,positive,positive,positive,positive,positive
452886653,"No. You should make sure the size is reasonable and make sure your dataflow does go back to the beginning and start producing the beginning of your dataset after `size()` number of datapoints have been produced.
Otherwise the inference results will be meaningless.",make sure size reasonable make sure go back beginning start beginning size number produced otherwise inference meaningless,issue,negative,positive,positive,positive,positive,positive
452877494,Is it possible to change the default value for validation_steps if I want to keep the validation_data option? Because my dataflow.size() is just a large value and meaningless.,possible change default value want keep option large value meaningless,issue,negative,negative,neutral,neutral,negative,negative
452873137,"[fit](https://tensorpack.readthedocs.io/modules/contrib.html#tensorpack.contrib.keras.KerasModel.fit) takes the same keyword arguments as [Trainer.train_with_defaults](https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.Trainer.train_with_defaults), which includes the number of steps in training. It's by default `dataflow.size()`.
The number of steps in inference is determined by your inference dataflow which you can obtain by `dataflow.size()`. ",fit number training default number inference determined inference obtain,issue,positive,positive,positive,positive,positive,positive
452871980,"If the example works, then your model may have some behavior that's not allowed in tensorpack. I cannot tell without a runnable code to reproduce the issue. Again, note in https://github.com/tensorpack/tensorpack/tree/master/examples/keras#note that keras support in tensorpack is experimental and may not work for all types of models.",example work model may behavior tell without runnable code reproduce issue note support experimental may work,issue,negative,positive,neutral,neutral,positive,positive
452851571,"Plus, how was the number of iterations determined? I have tried to change my training set but still result in the same iterations. In my case, it is simply too large, taking 60526112 hours to finish using single gpu. How can I modify this value?

[0109 14:00:14 @inference_runner.py:97] [InferenceRunner] Will eval 62500000000 iterations",plus number determined tried change training set still result case simply large taking finish single modify value,issue,positive,positive,neutral,neutral,positive,positive
452849805,"Upgrading again. Still the same version.
pip install -U git+https://github.com/ppwwyyxx/tensorpack.git
...
Successfully installed tensorpack-0.9.0.1

Yes. I have tested the example with fake_data using the two gpu setting. It worked.
Here, I only need training on multi-gpus without hyperparamter tuning of learning rate.",still version pip install successfully yes tested example two setting worked need training without tuning learning rate,issue,positive,positive,positive,positive,positive,positive
452845048,"> Tensorpack version: python -c 'import tensorpack; print(tensorpack.__version__);'.
You can install Tensorpack master by pip install -U git+https://github.com/ppwwyyxx/tensorpack.git
and see if your issue is already solved. 0.9

0.9 is not latest. The command to upgrade is above.

Could you verify whether the example without modification works on multiple gpu first?",version python print install master pip install see issue already latest command upgrade could verify whether example without modification work multiple first,issue,negative,positive,positive,positive,positive,positive
452821269,I trained `res101_cascade_fpn_gn` from scratch and `replicated` trainer works perfectly for me! Closing the issue now. Will post here if I find anything.,trained scratch replicated trainer work perfectly issue post find anything,issue,positive,positive,positive,positive,positive,positive
452814071,"upgrading does not fix the issue.

If you're asking about an unexpected problem you met, use this template.
__PLEASE DO NOT DELETE THIS TEMPLATE, FILL IT__:

### 1. What you did:
I was trying to use multi-gpu for model training

(1) **If you're using examples, what's the command you run:**
following example: https://github.com/tensorpack/tensorpack/blob/master/examples/keras/imagenet-resnet-keras.py

(2) **If you're using examples, have you made any changes to the examples? Paste them here:**
change the optimizer to Adam:
  model = KerasModel(build_model_resnet50,
                                    inputs_desc=[InputDesc(tf.float32, (None,) + img_shape, 'images')],
                                    targets_desc=[InputDesc(tf.float32, (None, len(target_name)), 'labels')],
                                    input=train_gen, trainer=SyncMultiGPUTrainerReplicated(num_GPU)
                                    )
 model.compile(optimizer=tf.train.AdamOptimizer(learning_rate), loss='categorical_crossentropy', metrics='categorical_accuracy')

(3) **If not using examples, tell us what you did here:**

Note that we may not be able to investigate it if there is no reproducible code.
It's always better to paste what you did instead of describing them.

### 2. What you observed:

(1) **Include the ENTIRE logs here:**
[0109 12:20:48 @collection.py:151] Size of these collections were changed in tower1: (tf.GraphKeys.UPDATE_OPS: 68->136)
/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py:320: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  ""Reason: {} "".format(name, reason))
[0109 12:20:52 @training.py:320] WRN [ReplicatedTrainer] Do not know how to sync variable 'tower1/conv2d_34/kernel:0' across GPUs. Reason: Cannot find conv2d_34/kernel:0 in the graph! 
Traceback (most recent call last):
  File ""./HCP_task_fmri_cnn_tensorpack_changesize_bk4_wm_test.py"", line 822, in <module>
    model_test_GPU_new.compile(optimizer=tf.train.AdamOptimizer(learning_rate), loss='categorical_crossentropy', metrics='categorical_accuracy')
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py"", line 267, in compile
    metrics=metrics)
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py"", line 212, in setup_keras_trainer
    lambda: optimizer)
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/utils/argtools.py"", line 176, in wrapper
    return func(*args, **kwargs)
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/tower.py"", line 214, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/trainers.py"", line 194, in _setup_graph
    self.train_op, post_init_op = self._builder.build(grad_list, get_opt_fn)
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 303, in build
    post_init_op = SyncMultiGPUReplicatedBuilder.get_post_init_ops()
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 345, in get_post_init_ops
    log_failure(v.name, ""Cannot find {} in the graph!"".format(realname))
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 322, in log_failure
    ""The aforementioned variable is trainable, so this is probably a fatal error.""
AssertionError: The aforementioned variable is trainable, so this is probably a fatal error.

It's always better to paste what you observed instead of describing them.

A part of logs is sometimes enough, but it's always better to paste as much as possible.

You can run a command with `CMD 2>&1 | tee logs.txt` to save all stdout & stderr logs to one file.

(2) **Other observations, if any:**
For example, CPU/GPU utilization, output images, tensorboard curves, if relevant to your issue.
[0109 12:20:38 @training.py:49] [DataParallel] Training a model of 2 towers.
[0109 12:20:38 @interface.py:31] Automatically applying QueueInput on the DataFlow.
[0109 12:20:38 @interface.py:43] Automatically applying StagingInput on the DataFlow.
[0109 12:20:38 @input_source.py:220] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0109 12:20:38 @training.py:109] Building graph for training tower 0 on device /gpu:0 ...
[0109 12:20:43 @training.py:109] Building graph for training tower 1 on device /gpu:1 ...

### 3. What you expected, if not obvious.

If you expect higher accuracy, only in one of the two conditions can we help with it:
(1) You're unable to match the accuracy documented in tensorpack examples.
(2) It appears to be a tensorpack bug.

Otherwise, how to get high accuracy is a machine learning question and is
not our responsibility to figure out.

### 4. Your environment:
  + Python version: python3.6
  + TF version: `python -c 'import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)'`. 1.12.0
  + Tensorpack version: `python -c 'import tensorpack; print(tensorpack.__version__);'`.
      You can install Tensorpack master by `pip install -U git+https://github.com/ppwwyyxx/tensorpack.git`
      and see if your issue is already solved. 0.9
  + If you're not using tensorpack under a normal command line shell (e.g.,
    using an IDE or jupyter notebook), please retry under a normal command line shell.  in shell
  + Hardware information, e.g. number of GPUs used. 2

About efficiency issues, PLEASE first read http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html
",fix issue unexpected problem met use template delete template fill trying use model training command run following example made paste change model none none tell u note may able investigate reproducible code always better paste instead include entire size tower method use instead reason name reason know sync variable across reason find graph recent call last file line module file line compile file line lambda file line wrapper return file line input file line file line build file line find graph file line variable trainable probably fatal error variable trainable probably fatal error always better paste instead part sometimes enough always better paste much possible run command tee save one file example utilization output relevant issue training model automatically automatically setting queue building graph training tower device building graph training tower device obvious expect higher accuracy one two help unable match accuracy bug otherwise get high accuracy machine learning question responsibility figure environment python version python version python print version python print install master pip install see issue already normal command line shell ide notebook please retry normal command line shell shell hardware information number used efficiency please first read,issue,positive,positive,positive,positive,positive,positive
452798180,"New implementation:
```python
def BilinearUpSample(x, shape):
    """"""
    Deterministic bilinearly-upsample the input images.
    It is implemented by deconvolution with ""BilinearFiller"" in Caffe.
    It is aimed to mimic caffe behavior.
    Args:
        x (tf.Tensor): a NHWC tensor
        shape (int): the upsample factor
    Returns:
        tf.Tensor: a NHWC tensor.
    """"""
    #log_deprecated(""BilinearUpsample"", ""Please implement it in your own code instead!"", ""2019-03-01"")
    inp_shape = x.shape.as_list()
    ch = inp_shape[3]
    assert ch is not None

    shape = int(shape)
    filter_shape = 2 * shape

    def bilinear_conv_filler(s):
        """"""
        s: width, height of the conv filter
        https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/filler.hpp#L219-L268
        """"""
        f = np.ceil(float(s) / 2)
        c = float(2 * f - 1 - f % 2) / (2 * f)
        ret = np.zeros((s, s), dtype='float32')
        for x in range(s):
            for y in range(s):
                ret[x, y] = (1 - abs(x / f - c)) * (1 - abs(y / f - c))
        return ret
    w = bilinear_conv_filler(filter_shape)
    w = np.repeat(w, ch * 1).reshape((filter_shape, filter_shape, ch, 1))

    weight_var = tf.constant(w, tf.float32,
                             shape=(filter_shape, filter_shape, ch, 1),
                             name='bilinear_upsample_filter')
    x = tf.pad(x, [[0, 0], [shape - 1, shape - 1], [shape - 1, shape - 1], [0, 0]], mode='SYMMETRIC')
    out_shape = tf.shape(x) * tf.constant([1, shape, shape, 1], tf.int32)

    @tf.custom_gradient
    def depthwise_deconv(x):
        ret = tf.nn.depthwise_conv2d_native_backprop_input(
            out_shape, weight_var, x, [1, shape, shape, 1], padding='SAME')
        def grad(dy):
            return tf.nn.depthwise_conv2d(dy, weight_var, [1, shape, shape, 1], padding='SAME')
        return ret, grad

    deconv = depthwise_deconv(x)

    edge = shape * (shape - 1)
    deconv = deconv[:, edge:-edge, edge:-edge, :]

    if inp_shape[1]:
        inp_shape[1] *= shape
    if inp_shape[2]:
        inp_shape[2] *= shape
    deconv.set_shape(inp_shape)
    return deconv
```
I haven't verified the backward is correct, but forward seems right now.",new implementation python shape deterministic input mimic behavior tensor shape factor please implement code instead assert none shape shape shape width height filter float float ret range range ret return ret shape shape shape shape shape shape ret shape shape grad return shape shape return ret grad edge shape shape edge edge shape shape return backward correct forward right,issue,negative,positive,positive,positive,positive,positive
452790026,"Your issue can probably fixed by upgrading tensorpack to github's master.
If not, please also verify whether the example runs on multiple gpus, and provide details of failure following the issue template.

Please also note what's written in the example: https://github.com/tensorpack/tensorpack/tree/master/examples/keras#note",issue probably fixed master please also verify whether example multiple provide failure following issue template please also note written example,issue,negative,negative,neutral,neutral,negative,negative
452788682,"Thanks a lot for finding this!
A great example of what I wrote here: https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries

> It’s best to not trust others’ layers!
For non-standard layers that’s not included in TensorFlow or Tensorpack, it’s best to implement them yourself. Non-standard layers often do not have a mathematical definition that people all agree on, and different people can implement it differently. Also, deep learning models on github often have bugs, especially when there is no reproduced experiments with the code.
For your own good, it’s best to implement the layers yourself. This is also why Tensorpack does not contain non-standard layers.

`BilinearUpSample` was made un-public and removed from documentation (https://tensorpack.readthedocs.io/modules/models.html) a while ago because it's a non-standard layer and we should not provide an implementation for it. But it's good to know the bug!",thanks lot finding great example wrote best trust included best implement often mathematical definition people agree different people implement differently also deep learning often especially code good best implement also contain made removed documentation ago layer provide implementation good know bug,issue,positive,positive,positive,positive,positive,positive
452776814,"Have solved the problem by replacing ""input0=Input(shape=input_shape)"" with ""input0=Input(tensor=image)""

The script works now when I only use one-gpu following the mnist example.
But I have another issue when trying to use the multi-gpu for training and validation.
I was following this example: https://github.com/tensorpack/tensorpack/blob/master/examples/keras/imagenet-resnet-keras.py

Here is the new error message:

[0109 12:01:46 @collection.py:151] Size of these collections were changed in tower1: (tf.GraphKeys.UPDATE_OPS: 68->136)
/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py:320: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  ""Reason: {} "".format(name, reason))
[0109 12:01:51 @training.py:320] WRN [ReplicatedTrainer] Do not know how to sync variable 'tower1/conv2d_34/kernel:0' across GPUs. Reason: Cannot find conv2d_34/kernel:0 in the graph! 
Traceback (most recent call last):
  File ""./HCP_task_fmri_cnn_tensorpack_changesize_bk4_wm_test.py"", line 822, in <module>
    model_test_GPU_new.compile(optimizer=tf.train.AdamOptimizer(learning_rate), loss='categorical_crossentropy', metrics='categorical_accuracy')
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py"", line 267, in compile
    metrics=metrics)
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py"", line 212, in setup_keras_trainer
    lambda: optimizer)
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/utils/argtools.py"", line 176, in wrapper
    return func(*args, **kwargs)
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/tower.py"", line 214, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/trainers.py"", line 194, in _setup_graph
    self.train_op, post_init_op = self._builder.build(grad_list, get_opt_fn)
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 303, in build
    post_init_op = SyncMultiGPUReplicatedBuilder.get_post_init_ops()
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 345, in get_post_init_ops
    log_failure(v.name, ""Cannot find {} in the graph!"".format(realname))
  File ""/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 322, in log_failure
    ""The aforementioned variable is trainable, so this is probably a fatal error.""
AssertionError: The aforementioned variable is trainable, so this is probably a fatal error.
",problem script work use following example another issue trying use training validation following example new error message size tower method use instead reason name reason know sync variable across reason find graph recent call last file line module file line compile file line lambda file line wrapper return file line input file line file line build file line find graph file line variable trainable probably fatal error variable trainable probably fatal error,issue,negative,positive,neutral,neutral,positive,positive
452622726,"( btw, this change is not needed:
```
20 _INSTANCE_TO_BASEDIR = {
21 'train2017': 'train2017',
22 'val2017': 'val2017',
23 }
```
because by default it will use the same name",change default use name,issue,negative,neutral,neutral,neutral,neutral,neutral
452622433,Thanks for reporting! The code uses the `thread_name_prefix` argument which is available only after Python 3.6. Removing that argument will work,thanks code argument available python removing argument work,issue,negative,positive,positive,positive,positive,positive
452587714,"See examples at https://github.com/tensorpack/tensorpack/blob/master/examples/keras/mnist-keras-v2.py

You're not calling KerasModel with correct argument types.",see calling correct argument,issue,negative,neutral,neutral,neutral,neutral,neutral
452571992,"Thanks for your quick reply.
I actually used a keras model, with the function listed below:

def build_cnn_model_test(input_shape, Nlabels, filters=16, convsize=3, convsize2=5, poolsize=2, hidden_size=256, conv_layers=4):
    input0 = Input(shape=input_shape)
    drop1 = input0
    ####quickly reducing image dimension first
    for li in range(1):
        conv1 = Conv2D(filters, (convsize, convsize), strides=2, padding='same',activation='relu',
                    kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(l2_reg))(drop1)
        conv1 = BatchNormalization()(conv1)
        drop1 = Dropout(0.25)(conv1)
        filters *= 2
    for li in range(conv_layers-1):
        conv1 = Conv2D(filters, convsize, padding='same',activation='relu',
                    kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(l2_reg))(drop1)
        conv1 = BatchNormalization()(conv1)
        conv1 = Conv2D(filters, convsize2, padding='same',activation='relu',
                    kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(l2_reg))(conv1)
        conv1 = BatchNormalization()(conv1)
        conv1 = Conv2D(filters, (convsize, convsize), strides=2, padding='same',activation='relu',
                    kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(l2_reg))(conv1)
        conv1 = BatchNormalization()(conv1)
        drop1 = Dropout(0.25)(conv1)
        if (li+1) % 2 == 0:
            filters *= 2
    drop2 = drop1
    avg1 = AveragePooling2D(pool_size=(5, 5))(drop2)
    flat = Flatten()(avg1)
    hidden = Dense(hidden_size, kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(l2_reg), activation='relu')(flat)
    drop3 = Dropout(0.4)(hidden)
    out = Dense(Nlabels, activation='softmax')(drop3)
    model = Model(inputs=input0, outputs=out)
    model.summary()
    return model

model_test_GPU = build_cnn_model_test(img_shape, nb_class)",thanks quick reply actually used model function listed input input drop input quickly reducing image dimension first li range drop drop dropout li range drop drop dropout drop drop drop flat flatten hidden dense flat drop dropout hidden dense drop model model return model,issue,negative,positive,neutral,neutral,positive,positive
452505322,"If you're using a custom dataset in COCO format, modify the class `COCODetection` to make it work. You may need to change the dict `COCO_id_to_category_id` to an identity mapping.",custom coco format modify class make work may need change identity,issue,negative,neutral,neutral,neutral,neutral,neutral
452474807,"`KerasModel` takes a function which returns a keras model:
https://github.com/tensorpack/tensorpack/blob/f5d1714a13002a7f04755a1c1afe7d70c2b71ef1/tensorpack/contrib/keras.py#L223-L224

See examples at https://github.com/tensorpack/tensorpack/blob/master/examples/keras/mnist-keras-v2.py

It appears you're not calling `KerasModel` with correct argument types.",function model see calling correct argument,issue,negative,neutral,neutral,neutral,neutral,neutral
452473719,"Thanks for your help. Problem solved.

I have another errors when trying compile the CNN model use the KerasModel class.
    model_test_GPU_new.compile(optimizer=tf.train.AdamOptimizer(), loss='categorical_crossentropy', metrics='categorical_accuracy')
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py"", line 267, in compile
    metrics=metrics)
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py"", line 212, in setup_keras_trainer
    lambda: optimizer)
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/utils/argtools.py"", line 176, in wrapper
    return func(*args, **kwargs)
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/tower.py"", line 214, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/trainers.py"", line 193, in _setup_graph
    grad_list = self._builder.call_for_each_tower(tower_fn)
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 225, in call_for_each_tower
    use_vs=[False] + [True] * (len(self.towers) - 1))
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 121, in build_on_towers
    return DataParallelBuilder.call_for_each_tower(*args, **kwargs)
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py"", line 116, in call_for_each_tower
    ret.append(func())
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/tower.py"", line 266, in get_grad_fn
    return compute_grad_from_inputs(*inputs)
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/tower.py"", line 245, in compute_grad_from_inputs
    cost = get_cost_fn(*inputs)
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/tfutils/tower.py"", line 286, in __call__
    output = self._tower_fn(*args)
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py"", line 165, in get_cost
    outputs = model_caller(input_tensors)
  File ""/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py"", line 106, in __call__
    outputs = model.outputs
AttributeError: 'Tensor' object has no attribute 'outputs'


**Here is my code:**

        train_gen = data_pipe_3dcnn_block(fmri_data_train, confounds_train, label_train,
                                          target_name=target_name, flag_cnn=Flag_CNN_Model,block_dura=block_dura,
                                          batch_size=batch_size, data_type='train', nr_thread=nr_thread, buffer_size=buffer_size)

        #########################################
        def one_hot(label):
            return np.eye(len(target_name))[label]

        train_gen = dataflow.MapDataComponent(train_gen, one_hot, 1)

        model_test_GPU_new = KerasModel(model_test_GPU,
                                        inputs_desc=[InputDesc(tf.float32, (None,) + img_shape, 'images')],
                                        targets_desc=[InputDesc(tf.uint8, (None, len(target_name)), 'labels')],
                                        input=train_gen, trainer=SyncMultiGPUTrainerReplicated(num_GPU))

        model_test_GPU_new.compile(optimizer=tf.train.AdamOptimizer(), loss='categorical_crossentropy', metrics='categorical_accuracy')
        callbacks = [ModelSaver(), GPUUtilizationTracker() ]

        ######start training the model
        model_test_GPU_new.fit(steps_per_epoch=100, max_epoch=100, callbacks=callbacks)",thanks help problem another trying compile model use class file line compile file line lambda file line wrapper return file line input file line file line false true file line return file line file line return file line cost file line output file line file line object attribute code label return label none none start training model,issue,positive,positive,neutral,neutral,positive,positive
452188009,"> It's will load whole datasets just one time  when  launch_train_with_config() ?

That depends on how your dataflow is implemented.",load whole one time,issue,negative,positive,positive,positive,positive,positive
452171294,"      Ok, I will check it more detail, and I am still confused about data load mechanism，It's will load whole datasets just one time  when  launch_train_with_config() ?
      In training phase,   deploying batch_size = 32, thread_process number = 4, prefetch = 1000,  use QueueInput(), and dataset contain  5000 feature files ,  get  OOM,   but  down to 100 feature files, it's  work well, all the feature file shape is: [601 ,880] ,dtype np.float32.

_define:_
class DataFlow(RNGDataFlow):

    def __init__(self, data_path, batch_size):
        self.batch_size = batch_size
        self.wav_files = glob.glob(data_path)

    def __call__(self, n_prefetch=1000, n_thread=1):
        df = self
        df = BatchData(df, self.batch_size)
        df = PrefetchData(df, n_prefetch, n_thread)
        return df

class Net2DataFlow(DataFlow):
    def get_data(self):
        for xx in xxx:
             yiled（read *npy file）

df = Net2DataFlow(xx, bastch_size = 32 )
data=QueueInput(df(n_prefetch=1000, n_thread=4))
            ",check detail still confused data load load whole one time training phase number use contain feature get feature work well feature file shape class self self self return class self,issue,negative,negative,neutral,neutral,negative,negative
452081083,"I cannot see evidence that this is a tensorpack issue. From the error log:
```
TypeError: 'float' object cannot be interpreted as an integer
```
It appears that your dataflow returns a float as its length, not an integer. You may want to check that.",see evidence issue error log object integer float length integer may want check,issue,negative,neutral,neutral,neutral,neutral,neutral
452076522,"Thanks so much. Now I can get a NHWC *pb successfully with everything is correct. I modified shufflenet.py a lot, and imagenet_utils.py by one line change from NCHW to NHWC.  I can contribute my code if needed. 
",thanks much get successfully everything correct lot one line change contribute code,issue,positive,positive,positive,positive,positive,positive
452057841,"```python
class ProcessingDataFlow(DataFlow):
  def __init__(self, ds):
    self.ds = ds
    
  def reset_state(self):
    self.ds.reset_state()

  def __iter__(self):
    a = {}
    for datapoint in self.ds:
      if a:
          a.update({k + '-2': v for k, v in datapoint.items()})
          yield a
          a = {}
      else:
          a.update(datapoint)

```",python class self self self yield else,issue,negative,neutral,neutral,neutral,neutral,neutral
452054250,Can you tell me how to modify the dataflow please. I have no idea.,tell modify please idea,issue,negative,neutral,neutral,neutral,neutral,neutral
451997493,"Thank you , it works!

I  have another question,
In meta-learning, there's` input_a `and `inputs_b` in each training iterations.
computing losses such as `loss( inputs_a, label_a)` and  `loss( inputs_b, label_b)` 
but  `get_train_dataflow` in FasterRCNN pass one image in each training iterations.

Thus, how can I get two or more data and labels from dataflow in each iterations
",thank work another question training loss loss pas one image training thus get two data,issue,negative,neutral,neutral,neutral,neutral,neutral
451779875,"The above was the standard way to load checkpoints, which is used by all the examples. If you like you can also just do `get_variable(initializer=tf.constant(...))`.",standard way load used like also,issue,negative,neutral,neutral,neutral,neutral,neutral
451777937,"As the Rules of Tower Function has mentioned:
> The creation of any trainable variables must respect reuse variable scope. To respect variable reuse (i.e. sharing), use tf.get_variable instead of tf.Variable in the function.

So, I am not able to use tf.Variable in the case. 
however, how can I initial my training variable  `weights[f'group{g}/block{blc}/conv{cv}/W']` with ResNet pre-trained weights using tf.get_variable  ?",tower function creation trainable must respect reuse variable scope respect variable reuse use instead function able use case however initial training variable,issue,positive,positive,positive,positive,positive,positive
451771014,">  Do not know how to sync variable 'tower0/Meta_FasterRCNN/conv0/beta:0' across GPUs. Reason: Name should not have prefix 'tower0' in this trainer! This variable is trainable, so this is probably a fatal error.

As the error log says, you're not allowed to create a variable whose name starts with ""tower0"". This name is reserved for tensorpack to use. https://tensorpack.readthedocs.io/tutorial/trainer.html#rules-of-tower-function

> File ""train.py"", line 422, in _setup_graph
self.predictors = [self._build_coco_predictor(k % num_gpu) for k in range(self.num_predictor)]
File ""train.py"", line 422, in 
self.predictors = [self._build_coco_predictor(k % num_gpu) for k in range(self.num_predictor)]
File ""train.py"", line 436, in _build_coco_predictor
...
KeyError: ""The name 'tower-pred-0/output/boxes:0' refers to a Tensor which does not exist. The operation, 'tower-pred-0/output/boxes', does not exist in the graph.

As the error log says, your callbacks needs the tensor ""output/boxes:0"" to do inference but this tensor does not exist in your graph.",know sync variable across reason name prefix trainer variable trainable probably fatal error error log create variable whose name tower name reserved use file line range file line range file line name tensor exist operation exist graph error log need tensor inference tensor exist graph,issue,negative,neutral,neutral,neutral,neutral,neutral
451770237,"Hi, I've got further questions.
Now, I'm writing my own functions. 
using `tf.nn.conv2D` instead of tensorpack` Conv2D`, so I can forwardly pass weights into layers.

here's what I do in tensorpack/example/FasterRCNN/train.py ,

`class ResNetC4Model(DetectionModel): `

    def construct_weights( self, backbone_dir='/project/project-mira6/mcs/fasterRCNNmodels/ImageNet-ResNet50.npz'):
        '''must call after '''
        # resnet variable 
        backbone_weights = np.load( backbone_dir ) 
        weights = {}
        weights['conv0/W'] = tf.Variable( backbone_weights['conv0/W:0'], name='conv0/W')
        weights['conv0/beta'] = tf.Variable( backbone_weights['conv0/bn/beta:0'], name='conv0/beta')
        weights['conv0/gamma'] = tf.Variable( backbone_weights['conv0/bn/gamma:0'], name='conv0/gamma')
        RESNET_NUM_BLOCK = [3, 4, 6, 3] 
        for g in [0,1,2,3]:
            for blc in range( RESNET_NUM_BLOCK[g]):
                for cv in np.arange(1,4):
                    weights[f'group{g}/block{blc}/conv{cv}/W'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/conv{cv}/W:0'],name=f'group{g}/block{blc}/conv{cv}/W' )
                    weights[f'group{g}/block{blc}/conv{cv}/gamma'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/conv{cv}/bn/gamma:0'],name=f'group{g}/block{blc}/conv{cv}/gamma' )
                    weights[f'group{g}/block{blc}/conv{cv}/beta'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/conv{cv}/bn/beta:0'], name=f'group{g}/block{blc}/conv{cv}/beta' )

                if blc == 0:
                    weights[f'group{g}/block{blc}/convshortcut/W'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/convshortcut/W:0'],name=f'group{g}/block{blc}/convshortcut/W' )
                    weights[f'group{g}/block{blc}/convshortcut/gamma'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/convshortcut/bn/gamma:0'],name=f'group{g}/block{blc}/convshortcut/gamma' )
                    weights[f'group{g}/block{blc}/convshortcut/beta'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/convshortcut/bn/beta:0'],name=f'group{g}/block{blc}/convshortcut/beta' )
        # rpn
        weights['rpn/conv0/W']=tf.get_variable('rpn/conv0/W', shape=[3,3,1024,1024], initializer=tf.random_normal_initializer(stddev=0.01))
        weights['rpn/conv0/b']=tf.get_variable('rpn/conv0/b', shape=[1024], initializer=tf.zeros_initializer() )
        weights['rpn/class/W']=tf.get_variable('rpn/class/W', shape=[1,1,1024,15], initializer=tf.random_normal_initializer(stddev=0.01))
        weights['rpn/class/b']=tf.get_variable('rpn/class/b', shape=[15], initializer=tf.zeros_initializer() )
        weights['rpn/box/W']=tf.get_variable('rpn/box/W', shape=[1,1,1024,60], initializer=tf.random_normal_initializer(stddev=0.01))
        weights['rpn/box/b']=tf.get_variable('rpn/box/b', shape=[60], initializer=tf.zeros_initializer() )

        # frcnn FC layer
        weights['fastrcnn/class/W']=tf.get_variable('fastrcnn/class/W', shape=[2048, 81], initializer=tf.random_normal_initializer(stddev=0.01))
        weights['fastrcnn/class/b']=tf.get_variable('fastrcnn/class/b', shape=[81], initializer=tf.zeros_initializer() )
        weights['fastrcnn/box/W']=tf.get_variable('fastrcnn/box/W', shape=[2048, 324], initializer=tf.random_normal_initializer(stddev=0.01))
        weights['fastrcnn/box/b']=tf.get_variable('fastrcnn/box/b', shape=[324], initializer=tf.zeros_initializer() )
    
        return weights

    def build_graph(self, *inputs):
        inputs = dict(zip(self.input_names, inputs))
        is_training = get_current_tower_context().is_training
        image = self.preprocess(inputs['image'])     # 1CHW

        with tf.variable_scope('Meta_FasterRCNN', reuse=None) as training_scope:
            if 'weights' in dir(self):
                training_scope.reuse_variables()
                weights = self.weights
            else:
                # Define the weights
                self.weights = weights = self.construct_weights()
        
            
            featuremap = resnet_c4_backbone(image, cfg.BACKBONE.RESNET_NUM_BLOCK[:3], weights)`

I load the ResNet50.npz into tf.Variables and save it in a dictionary name `weights` , and pass it through my model in order to accomplish the function `f(inputs, weights) -> cost`

but, I'm have fatal errors such as : 

> [0107 03:39:27 @training.py:296] WRN [ReplicatedTrainer] Do not know how to sync variable 'tower0/Meta_FasterRCNN/conv0/W:0' across GPUs. Reason: Name should not have prefix 'tower0' in this trainer! This variable is trainable, so this is probably a fatal error.
[0107 03:39:27 @training.py:296] WRN [ReplicatedTrainer] Do not know how to sync variable 'tower0/Meta_FasterRCNN/conv0/beta:0' across GPUs. Reason: Name should not have prefix 'tower0' in this trainer! This variable is trainable, so this is probably a fatal error.

at the end I've got error:

> Traceback (most recent call last):
  File ""train.py"", line 592, in <module>
    launch_train_with_config(traincfg, trainer)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/interface.py"", line 95, in launch_train_with_config
    extra_callbacks=config.extra_callbacks)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/base.py"", line 319, in train_with_defaults
    steps_per_epoch, starting_epoch, max_epoch)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/base.py"", line 289, in train
    self.setup_callbacks(callbacks, monitors)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/utils/argtools.py"", line 181, in wrapper
    return func(*args, **kwargs)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/base.py"", line 189, in setup_callbacks
    self._callbacks.setup_graph(weakref.proxy(self))
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/callbacks/base.py"", line 52, in setup_graph
    self._setup_graph()
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/callbacks/group.py"", line 70, in _setup_graph
    cb.setup_graph(self.trainer)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/callbacks/base.py"", line 52, in setup_graph
    self._setup_graph()
  File ""train.py"", line 422, in _setup_graph
    self.predictors = [self._build_coco_predictor(k % num_gpu) for k in range(self.num_predictor)]
  File ""train.py"", line 422, in <listcomp>
    self.predictors = [self._build_coco_predictor(k % num_gpu) for k in range(self.num_predictor)]
  File ""train.py"", line 436, in _build_coco_predictor
    graph_func = self.trainer.get_predictor(self._in_names, self._out_names, device=idx)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/tower.py"", line 135, in get_predictor
    output_tensors = tower.get_tensors(output_names)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/tfutils/tower.py"", line 405, in get_tensors
    return [self.get_tensor(name) for name in names]
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/tfutils/tower.py"", line 405, in <listcomp>
    return [self.get_tensor(name) for name in names]
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/tfutils/tower.py"", line 388, in get_tensor
    ret = get_op_or_tensor_by_name(name_with_ns)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/tfutils/common.py"", line 134, in get_op_or_tensor_by_name
    return f(name)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/tfutils/common.py"", line 129, in f
    return G.get_tensor_by_name(n)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3207, in get_tensor_by_name
    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3035, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3077, in _as_graph_element_locked
    ""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'tower-pred-0/output/boxes:0' refers to a Tensor which does not exist. The operation, 'tower-pred-0/output/boxes', does not exist in the graph.""
MultiProcessMapDataZMQ successfully cleaned-up.

please help",hi got writing instead forwardly pas class self call variable range layer return self zip image self else define image load save dictionary name pas model order accomplish function cost fatal know sync variable across reason name prefix trainer variable trainable probably fatal error know sync variable across reason name prefix trainer variable trainable probably fatal error end got error recent call last file line module trainer file line file line file line train file line wrapper return file line self file line file line file line file line range file line range file line file line file line return name name file line return name name file line ret file line return name file line return file line return name file line return file line graph name name tensor exist operation exist graph successfully please help,issue,negative,positive,positive,positive,positive,positive
451720272,"If I remember it correctly, was running 0.5.8, which produced the error, upgrade to 0.6.0 resolved the issue.",remember correctly running produced error upgrade resolved issue,issue,negative,neutral,neutral,neutral,neutral,neutral
451714667,"Which versions did you upgrade it from and to? There is an assertion on msgpack version at 
https://github.com/tensorpack/tensorpack/blob/4eaeed3f6f4651aece29181266194beb4960774f/tensorpack/utils/serialize.py#L77
It should assert the version during import and I wonder why it wasn't functional",upgrade assertion version assert version import wonder functional,issue,negative,neutral,neutral,neutral,neutral,neutral
451697360,"ok, we did a fresh pull from tensorpack, then applies our changes on top, the issue now goes away. ",fresh pull top issue go away,issue,negative,positive,positive,positive,positive,positive
451696156,"all codes are directly from tensorpack, our changes are limited to dataset configuration only (our own dataset are coco format too, so changes are only to COCO_id_to_category_id and class_names). 

RESNET_NUM_BLOCKS in config.py is intact as latest tensorpack config.py file.",directly limited configuration coco format intact latest file,issue,negative,positive,positive,positive,positive,positive
451695707,"> A previously working training (on own dataset) 

I assume this means your own old code (and own config)?

Does the code in tensorpack by itself (without modifications, trained on COCO) work? I cannot be responsible for the modifications you made.

From the error log it seems that your config file does not have the key `RESNET_NUM_BLOCKS`, however the latest config file does have it.",previously working training assume old code code without trained coco work responsible made error log file key however latest file,issue,negative,positive,positive,positive,positive,positive
451695433,"Thx for quick response, all files in examples/FasterRCNN were updated/refreshed (have followed this code repository for past a couple of years).",quick response code repository past couple,issue,negative,positive,neutral,neutral,positive,positive
451695182,"`examples/FasterRCNN` is not a library. `examples/FasterRCNN` is an example, with multiple files. 
You cannot expect things to work if you combine an old version of one file with a newer version of another file.",library example multiple expect work combine old version one file version another file,issue,negative,positive,neutral,neutral,positive,positive
451588130,"```python
@under_name_scope()
def channel_shuffle(l, group):
    in_shape = l.get_shape().as_list()
    H, W, in_channel = in_shape[1:]  
    assert in_channel % group == 0, in_channel
    l = tf.reshape(l, [-1, H, W, in_channel // group, group]) //guess the reshape is critical
    l = tf.transpose(l, [0, 1, 2, 4, 3]) //guess, this tranpose need to change 
    l = tf.reshape(l, [-1, H, W, in_channel])
    return l
```",python group assert group group group reshape critical need change return,issue,negative,neutral,neutral,neutral,neutral,neutral
451573496,"I think I fixed most of the places in shufflent.py in order to generate NHWC graph and thus an NHWC format *pb. Now it proceeds to end a *pb, but the dimension starts to be wrong after the 1st channel_shuffle. So I think this is the last obstacle. Can you help to refactor it? After this, I can contribute my changed code to the repo, so everyone can see if OK.   Thank you. 

```

@under_name_scope()
def channel_shuffle(l, group):
    in_shape = l.get_shape().as_list()
    in_channel = in_shape[1]  // (changed to in_shape[3])
    assert in_channel % group == 0, in_channel
    l = tf.reshape(l, [-1, in_channel // group, group] + in_shape[-2:]) //guess the reshape is critical
    l = tf.transpose(l, [0, 2, 1, 3, 4]) //guess, this tranpose need to change 
    l = tf.reshape(l, [-1, in_channel] + in_shape[-2:])
    return l
```",think fixed order generate graph thus format proceeds end dimension wrong st think last obstacle help contribute code everyone see thank group assert group group group reshape critical need change return,issue,negative,negative,neutral,neutral,negative,negative
451553830,It's not a bug. It should be changed according to the data format.,bug according data format,issue,negative,neutral,neutral,neutral,neutral,neutral
451552581,"
Thanks, but is` line 36 shufflenet.py [1, 1, stride, stride] `a bug ?
according to the API (https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d), should it be `[1, stride, stride, 1]`?
",thanks line stride stride bug according stride stride,issue,negative,positive,positive,positive,positive,positive
451548449,"I very occasionally saw strange crashes when running with horovod, but never when using the ""replicated"" trainer. My guess is this is horovod-specific, but I cannot tell because I can never reproduce such errors. https://github.com/uber/horovod/issues/404 is one such example and I've seen other crashes with different behaviors. Unless you want to do distributed training, using the ""replicated"" trainer is the same as horovod.

You can also check more logs: in addition to the logs saved by tensorpack, `mpirun` also saves logs for you (if you use the `-output-filename` option) and may contain more information.
Also, your job can crash if CPU memory is not enough.",occasionally saw strange running never replicated trainer guess tell never reproduce one example seen different unless want distributed training replicated trainer also check addition saved also use option may contain information also job crash memory enough,issue,negative,negative,neutral,neutral,negative,negative
451541414,"The modification you need to do is completely in the tensorflow domain and not related to tensorpack much. So that's not something I can offer much help. You can just print the shapes of tensors to see where it goes wrong. From your log it appears this layer is already wrong:
```
[0104 13:06:37 @registry.py:121] stage2/block0/dconv input: [None, 56, 56, 58]
> [0104 13:06:37 @registry.py:129] stage2/block0/dconv output: [None, 56, 28, 58]
```
",modification need completely domain related much something offer much help print see go wrong log layer already wrong input none output none,issue,negative,negative,neutral,neutral,negative,negative
451539002,"Thanks, I changed the `imagenet_utility.py` data_format from NCHW and NHWC. I also change a couple of hard coded location of `in_channel = in_shape[1] to in_shape[3] in shufflenet.py. ` I can proceed a little bit but died in shortcut_channel.


```
> 0104 13:06:37 @shufflenet.py:144] #Channels: [24, 116, 232, 464]
> [0104 13:06:37 @registry.py:121] conv1 input: [None, 224, 224, 3]
> [0104 13:06:37 @registry.py:129] conv1 output: [None, 112, 112, 24]
> [0104 13:06:37 @registry.py:121] pool1 input: [None, 112, 112, 24]
> [0104 13:06:37 @registry.py:129] pool1 output: [None, 56, 56, 24]
> [0104 13:06:37 @registry.py:121] stage2 input: [None, 56, 56, 24]
> [0104 13:06:37 @registry.py:121] stage2/block0/conv1 input: [None, 56, 56, 24]
> [0104 13:06:37 @registry.py:129] stage2/block0/conv1 output: [None, 56, 56, 58]
> [0104 13:06:37 @registry.py:121] stage2/block0/dconv input: [None, 56, 56, 58]
> [0104 13:06:37 @registry.py:129] stage2/block0/dconv output: [None, 56, 28, 58]
> [0104 13:06:37 @registry.py:121] stage2/block0/conv2 input: [None, 56, 28, 58]
> [0104 13:06:37 @registry.py:129] stage2/block0/conv2 output: [None, 56, 28, 60]
> [0104 13:06:37 @registry.py:121] stage2/block0/shortcut_dconv input: [None, 56, 56, 24]
> Traceback (most recent call last):
>   File ""./shufflenet.py"", line 263, in <module>
>     ModelExporter(pred_config).export_compact('./new_compact_graph-nhwc.pb')
>   File ""/usr/local/lib/python2.7/dist-packages/tensorpack/tfutils/export.py"", line 48, in export_compact
>     self.config.tower_func(*input.get_input_tensors())
>   File ""/usr/local/lib/python2.7/dist-packages/tensorpack/tfutils/tower.py"", line 286, in __call__
>     output = self._tower_fn(*args)
>   File ""/home/tim/tensorpack/examples/ImageNetModels/imagenet_utils.py"", line 333, in build_graph
>     logits = self.get_logits(image)
>   File ""./shufflenet.py"", line 149, in get_logits
>     l = shufflenet_stage('stage2', l, channels[0], 4, group)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorpack/models/registry.py"", line 124, in wrapped_func
>     outputs = func(*args, **actual_args)
>   File ""./shufflenet.py"", line 109, in shufflenet_stage
>     l = shufflenet_unit_v2(name, l, channel, 2 if i == 0 else 1)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorpack/models/registry.py"", line 124, in wrapped_func
>     outputs = func(*args, **actual_args)
>   File ""./shufflenet.py"", line 95, in shufflenet_unit_v2
>     shortcut = DepthConv('shortcut_dconv', shortcut, shortcut_channel, 3, stride=2)
>   File ""/usr/local/lib/python2.7/dist-packages/tensorpack/models/registry.py"", line 124, in wrapped_func
>     outputs = func(*args, **actual_args)
>   File ""./shufflenet.py"", line 29, in DepthConv
>     assert out_channel % in_channel == 0, (out_channel, in_channel)
> AssertionError: (56, 24)
> ```
```

For shortcut is complicated for me, as it is tangled with shape transposes",thanks also change couple hard location proceed little bit input none output none pool input none pool output none stage input none input none output none input none output none input none output none input none recent call last file line module file line file line output file line image file line group file line file line name channel else file line file line file line file line assert complicated shape,issue,negative,negative,negative,negative,negative,negative
451517773,"weights do not have NHWC or NCHW format.
The ops are in NCHW format as written in the code.",format format written code,issue,negative,neutral,neutral,neutral,neutral,neutral
451509334,"Simply change shufflenet.py line  37, line 117 to NHWC and ""channle_last"" won't work as it will report Dimension not equal error.  ",simply change line line wo work report dimension equal error,issue,negative,neutral,neutral,neutral,neutral,neutral
451484998,I see the trained *npz (and thus converted *pb) store weights in NCHW format. Is there anyway to change from NCHW into NHWC format when converting from *npz to *pb? ,see trained thus converted store format anyway change format converting,issue,negative,neutral,neutral,neutral,neutral,neutral
451318584,"Hi,

Thanks for the quick reply! So in my annotations json file, `segmentation` field is always set to be empty, which caused the error. I am struggling to convert the dataset to coco format properly right now.

Thanks,
Xin

",hi thanks quick reply file segmentation field always set empty error struggling convert coco format properly right thanks,issue,negative,positive,positive,positive,positive,positive
451315870,"one more question, is the obtained *pb file is a frozen one (frozen with trained weights in *ckpt) or it is merely a graph without weights in it?  Thanks again",one question file frozen one frozen trained merely graph without thanks,issue,negative,negative,negative,negative,negative,negative
451313695,Still many many thanks!,still many many thanks,issue,negative,positive,positive,positive,positive,positive
451313467,"This is a tflite question that's unrelated to tensorpack.

It asks you to provide input shape so you should do it. Google tells me that you can do it with command line: https://github.com/tensorflow/tensorflow/blob/6932e795621a76a7e520bee529d915265b40e9bd/tensorflow/lite/python/tflite_convert.py#L310-L313",question unrelated provide input shape command line,issue,negative,neutral,neutral,neutral,neutral,neutral
451313083,"By ,input_names=['input'], it works, and I can get ./compact_graph.pb. 

Yet, I am not able to use TOCO (which is tflite_convert, the same thing just renamed)

```
tflite_convert --output_file=./shufflenet.tflite --graph_def_file=./compact_graph.pb --input_arrays=input --output_arrays=linear/output

2019-01-03 17:21:35.382450: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
Traceback (most recent call last):
  File ""/home/tim/tensorflow/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 412, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 408, in run_main
    _convert_model(tflite_flags)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 162, in _convert_model
    output_data = converter.convert()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/lite.py"", line 404, in convert
    ""'{0}'."".format(_tensor_name(tensor)))
ValueError: Provide an input shape for input array 'input'.

```
I am able to visualize *.pb by tensorboard, which shows that input is a placeholder with shapes not defined.  

",work get yet able use toco thing binary use recent call last file line module main file line main file line run main file line file line file line convert tensor provide input shape input array able visualize input defined,issue,negative,positive,positive,positive,positive,positive
451307110,"Maybe you did not provide the `input_names`. By default the inputs are the one used by your model, which is both ""input"" and ""label"", as declared here: https://github.com/tensorpack/tensorpack/blob/860f7a382f8dc245e46c5f637866ef6384db1733/examples/ImageNetModels/imagenet_utils.py#L323-L325

However, when converting to toco, it appears that the conversion tool does not like it when an input is not used. In this case since you're asking for `linear/output`, this output does not depend on 'label' so you should not add 'label' to the `input_names`.",maybe provide default one used model input label declared however converting toco conversion tool like input used case since output depend add,issue,negative,neutral,neutral,neutral,neutral,neutral
451304603,"I uninstall and reinstall again
sudo pip show tensorflow 
Name: tensorflow
Version: 1.12.0

Now, I have different errors

```
Traceback (most recent call last):
  File ""./shufflenet.py"", line 260, in <module>
    ModelExporter(pred_config).export_compact('./compact_graph.pb')
  File ""/usr/local/lib/python2.7/dist-packages/tensorpack/tfutils/export.py"", line 74, in export_compact
    False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/optimize_for_inference_lib.py"", line 111, in optimize_for_inference
    placeholder_type_enum)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/strip_unused_lib.py"", line 83, in strip_unused
    raise KeyError(""The following input nodes were not found: %s\n"" % not_found)
KeyError: ""The following input nodes were not found: set([u'label'])\n""

```

Can you upload *.ckpt & *pb file somewhere? Maybe it is more easier. 
Thank you. 
",reinstall pip show name version different recent call last file line module file line false file line file line raise following input found following input found set file somewhere maybe easier thank,issue,positive,negative,neutral,neutral,negative,negative
451285616,"TensorFlow's inference optimization tool only works for toco after TF 1.8: https://github.com/tensorflow/tensorflow/commit/c8e3d2b43e4cbf9a9e32567a2e59597916f5b0b9

The above commit adds support for exporting a TOCO-compatible graph if using TF>=1.8.",inference optimization tool work toco commit support graph,issue,positive,neutral,neutral,neutral,neutral,neutral
451283392,"The code runs well on my machine, which suggests that it may require certain version of tensorflow to work. I'm using TensorFlow 1.12. cc @PatWie who wrote the export function",code well machine may require certain version work wrote export function,issue,negative,positive,positive,positive,positive,positive
451278369,"Thanks, linear/output works. But this time, it dies in line 259. 

```

[0103 15:08:42 @sessinit.py:217] Restoring from dict ...
Converted 282 variables to const ops.
Traceback (most recent call last):
  File ""./shufflenet.py"", line 259, in <module>
    ModelExporter(pred_config).export_compact('/tmp/compact_graph.pb')
  File ""/usr/local/lib/python2.7/dist-packages/tensorpack/tfutils/export.py"", line 74, in export_compact
    False)
TypeError: optimize_for_inference() takes exactly 4 arguments (5 given)

```

",thanks work time line converted recent call last file line module file line false exactly given,issue,negative,positive,neutral,neutral,positive,positive
451276932,"See the tutorial on https://tensorpack.readthedocs.io/tutorial/symbolic.html#access-relevant-tensors
The name of the output tensor of the 'linear' layer will be 'linear/output'.
In general you can print a tensor to see its name by `print(x)`.",see tutorial name output tensor layer general print tensor see name print,issue,negative,positive,neutral,neutral,positive,positive
451276316,"Thanks so much. I updated my local repo by checking in your new commits. 
I worked on linux machine which blocked the github.com so it is hard to paste all my log. But that is exactly the same error like you pasted here. 

I tried several words in the ['  '], like 'linear' (from line  153, fully connected operation), 'gap' (line 153, for Pooling), , 'linear output', or 'output', etc.   from the shufflenet.py.

`, output_names=[' ']`

but They all gives me similar error:

`KeyError: ""The name 'linear:0' refers to a Tensor which does not exist. The operation, 'linear', does not exist in the graph. 
`
So I do not know what is the output_names? 




",thanks much local new worked machine blocked hard paste log exactly error like pasted tried several like line fully connected operation line output similar error name tensor exist operation exist graph know,issue,negative,positive,neutral,neutral,positive,positive
451267858,"> (1) Include the ENTIRE logs here:

Please include the entire logs which will help with the investigation. Otherwise I'll have to modify the code and run it myself.  The logs is like this:
```
Traceback (most recent call last):
  File ""./shufflenet.py"", line 252, in <module>
    model = model,
  File ""/home//tensorpack/tensorpack/predict/config.py"", line 85, in __init__
    assert_type(self.output_names, list)
  File ""/home//tensorpack/tensorpack/predict/config.py"", line 57, in assert_type
    assert isinstance(v, tp), v.__class__
AssertionError: <class 'NoneType'>
```
which means you have to provide `output_names`.",include entire please include entire help investigation otherwise modify code run like recent call last file line module model model file line list file line assert class provide,issue,positive,neutral,neutral,neutral,neutral,neutral
451265346,"I want to do the reverse, from *npz to *ckpt. @sharpstill , Do you know is there any tool? ",want reverse know tool,issue,negative,neutral,neutral,neutral,neutral,neutral
451250661,"It's indeed much faster than matterport's one for standard models, and slightly faster than the one in this repo as well.

UPDATE: now we're as fast as maskrcnn-benchmark (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md)",indeed much faster one standard slightly faster one well update fast,issue,negative,positive,neutral,neutral,positive,positive
451248565,"@John1231983 @letuantang   Facebook AI Research released Maskrcnn_benchmark, it's allegedly a much faster version of Matterport's Mask_RCNN.  Would recommend trying that out:  https://github.com/facebookresearch/maskrcnn-benchmark",ai research allegedly much faster version would recommend trying,issue,negative,positive,positive,positive,positive,positive
451246397,"> It may not be interesting, but it's clearly a bug and therefore should be addressed

By ""not interesting"", I'm not referring to the crash. I'm referring to (3) in the original post which is about custom dataset.

> Also the original poster made a great request for

As noted in the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/feature-requests.md), generally we only take feature request on the tensorpack library, not on the examples. ""Examples"" mean that it's something that a user should read, understand and be able to implement. I can understand how demanding such feature can be but it's not on my list for the near future.

",may interesting clearly bug therefore interesting crash original post custom also original poster made great request noted issue template generally take feature request library mean something user read understand able implement understand demanding feature list near future,issue,positive,positive,positive,positive,positive,positive
451245020,"> I can confirm getting this error message as well.

Your error message is different from the original one in this issue and I don't understand how they are related. Your issue appears to come from a bad installation of horovod and can probably be solved by uninstall and reinstall horovod with the `--no-cache-dir` option. Please open a new issue following the issue template if it still exists.",confirm getting error message well error message different original one issue understand related issue come bad installation probably reinstall option please open new issue following issue template still,issue,negative,negative,neutral,neutral,negative,negative
451244061,"I can confirm getting this error message as well.

> tensorflow.python.framework.errors_impl.NotFoundError: /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/mpi_lib.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeENS_11StringPieceE

I would disagree about this issue not being super interesting.  It may not be interesting, but it's clearly a bug and therefore should be addressed.  Also the original poster made a great request for ""some instructions on how to prepare custom datasets. Also how to do transfer learning with the pretrained official tensorpack models.""

I'm going to try creating a new conda environment and do a fresh install of tensorflow-gpu, opencv-python and tensorpack.",confirm getting error message well undefined symbol would disagree issue super interesting may interesting clearly bug therefore also original poster made great request prepare custom also transfer learning official going try new environment fresh install,issue,positive,positive,positive,positive,positive,positive
451218866,"Your question is unrelated to this issue. Please open a new one and follow the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md)).
`input_names` and `output_names` can be any tensor defined in the graph. If you want to input an image, the name of that tensor is ""input"".",question unrelated issue please open new one follow issue template click new issue unexpected visit link post unexpected tensor defined graph want input image name tensor input,issue,negative,positive,neutral,neutral,positive,positive
451186926,"Thanks so much for quick reply and patience. I insert the following code after line 249 of 
https://github.com/tensorpack/tensorpack/blob/master/examples/ImageNetModels/shufflenet.py

    pred_config = PredictConfig(
        session_init=get_model_loader(args.load),
        model = Model()
        #, input_names=['input']
        #, output_names=['output']
        )
    ModelExporter(pred_config).export_compact('/tmp/compact_graph.pb')

I used command"" python ./shufflenet.py --load ShuffleNetV2-1x.npz --v2 -r=1""

It gvies me AssertionError: <type 'NoneType'> which is model=Model() on line 252 , 
However, line 249 is exactly the model=Model() which passed before this block. 

Another question is what is the input_names & and output_names should I put? I can view the *npz file by 
```
x.np.load()
print name = x.files 
```
It gives me such names like ""stage/4/block3/dconv_bn/variance/EMA:0"", etc 
but I do not know who are input names and who are output names. 




",thanks much quick reply patience insert following code line model model used command python load type line however line exactly block another question put view file print name like know input output,issue,negative,positive,positive,positive,positive,positive
451042035,"1. For the crash, I recommend you to `try..except` the relevant lines and save the data that causes `cocomask.frPyObjects` to fail. It's likely that certain data in your dataset can make the function crash, or maybe your data is in an unexpected format. This function is from `pycocotools` so I have no idea what types of inputs can make it crash. But it seems to be quite robust from what I tested just now:
```
In [22]: cocomask.frPyObjects([[0, 0, 32.2, 100, 100, 100, 100, 34.5], [3.2, -3.3, -2.2, 0, 10, 10]], 50, 300)
Out[22]: 
[{'counts': b'01a14M2M3N2M3M3N2M3M3N3L3M3N2M3M3N1N0001O00001O00001O00001O001O00001O00001O00001O00001O00001O00001O00001O00001O001O00001O00001O00001O00001O00001O00001O00001O00001O001O00001O00001O00001O00001O00001O0000^g9',
  'size': [50, 300]},
 {'counts': b'02`11O1O1O0101O1O1O1OjT>', 'size': [50, 300]}]
```
The cocoapi does work with Python 3.

2. For opencv: I use `pip install opencv-python` and it seems to work well for me. But I don't know what happens to you and this is a separate issue (and probably irrelevant to tensorpack anyway).

3. This is a TODO item in [NOTES.md](https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md) but it's unlikely it will be implemented any time soon since this is not super interesting to me. ",crash recommend try except relevant save data fail likely certain data make function crash maybe data unexpected format function idea make crash quite robust tested work python use pip install work well know separate issue probably irrelevant anyway item unlikely time soon since super interesting,issue,positive,positive,neutral,neutral,positive,positive
451011914,"> Which export.py it refers to (maybe export-model.py)?
Yes. `export-model.py`.

You can copy the `export_compact` function into your shufflenet code:
```python
    pred_config = PredictConfig(
        session_init=get_model_loader(model_path),
        model=MyShuffleNetModel(),
        input_names=['my input names'],
        output_names=['output names I need'])
    ModelExporter(pred_config).export_compact('/tmp/compact_graph.pb')
```
This gives you a serialized `tf.Graph` as a pb file but you need to figure out whether this pb file is in the same format that your tool needs. ""pb file"" is not a format.",maybe yes copy function code python input need file need figure whether file format tool need file format,issue,negative,neutral,neutral,neutral,neutral,neutral
451010082,"Thanks so much. I read the file: https://github.com/tensorpack/tensorpack/blob/master/examples/basics/export-model.py

But I do not quite get the comments

1. train the model by
    python export.py
2. export the model by
    python export.py --export serving --load train_log/export/checkpoint
    python export.py --export compact --load train_log/export/checkpoint

Which export.py it refers to (maybe export-model.py)?  
Suppose I have already download the ""ShuffleNetV2-1x.npz"" ( I think I can skip step 1 training, right? but I do not have dir train_log), what should I do next in order to obtain *.pb ?  Can you give more instructions. Thank you. 

 

",thanks much read file quite get train model python export model python export serving load python export compact load maybe suppose already think skip step training right next order obtain give thank,issue,positive,positive,positive,positive,positive,positive
450998721,"The [npz format](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.savez.html) contains key-value pairs that stores the trained parameters. With the example code that builds the graph, it is sufficient to restore and use the model, because now you have both the graph and its parameters.

However, if you want to use the model with a particular tool, you'll need to figure out what format it needs and how to construct a model which the tool can use. Maybe the `export_compact` function in `examples/basics/export-model.py` can help, but it all depends on what your tool needs. I'm not familiar with TOCO and inference tools are out of the scope of tensorpack.",format trained example code graph sufficient restore use model graph however want use model particular tool need figure format need construct model tool use maybe function help tool need familiar toco inference scope,issue,positive,positive,positive,positive,positive,positive
450995581,"We want to convert TF format to TF Lite format. According to TOCO, *npz is not supported yet, as I know. ",want convert format lite format according toco yet know,issue,negative,neutral,neutral,neutral,neutral,neutral
450995306,"Why the pretrained model here is *npz format (http://models.tensorpack.com/)
While, TF hosted ones are *pb (https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). 

Is there anyway convert from *npz to *pb format? Thankyou.",model format anyway convert format,issue,negative,neutral,neutral,neutral,neutral,neutral
450806315,"@roadcode In fact, @ppwwyyxx has posted one solution in https://github.com/tensorpack/tensorpack/tree/fpn_cpu_inference/examples/FasterRCNN branch.",fact posted one solution branch,issue,negative,neutral,neutral,neutral,neutral,neutral
450797061,@Remember2018 how do yo modify the FasterRCNN with NHWC format?,remember yo modify format,issue,negative,neutral,neutral,neutral,neutral,neutral
450717287,"@ppwwyyxx thanks to your kind help, I have modified the FasterRCNN with NHWC format, and now the CPU inference of FasterRCNN is OK. 
But the speed seems slow. For testing one resized image with short edge size of 800 and max size of 1333， the time of ResNet-50-FPN is about 2.5 seconds.  For the tf-faster-rcnn with VGG16 network, the time is about 1 second. ",thanks kind help format inference speed slow testing one image short edge size size time network time second,issue,positive,positive,neutral,neutral,positive,positive
450705216,Look at the output images and see what makes most sense to you.,look output see sense,issue,negative,neutral,neutral,neutral,neutral,neutral
450702784,"Dear,

How I can select a suitable parameters for theses:

dataflow.imgaug.Brightness(delta??, clip=True)
(i.e. delta = ?)
dataflow.imgaug.Contrast(factor_range??, rgb=True, clip=True)
(i.e. factor_range??)

dataflow.imgaug.GaussianBlur(max_size=3)
dataflow.imgaug.Gamma(range=(-0.5, 0.5))

Best regards,
Canh Nguyen",dear select suitable thesis delta delta best,issue,positive,positive,positive,positive,positive,positive
450681185,"As mentioned above I do not want to comment more. I cannot understand your question because terms like ""an origin wrt the current cell"", ""relative cell from the original position"" are not mathematically defined and I do not know what they mean. This is why the issue is complicated: I do not believe it can be explained well with plain words.

The source code is CropAndResizePerBox.",want comment understand question like origin current cell relative cell original position mathematically defined know mean issue complicated believe well plain source code,issue,positive,negative,neutral,neutral,negative,negative
450654030,"Could you please comment on ""this is how tensorflow works"" ? 
I understood you want to take a relative floating point coordinate with an origin wrt the current cell, but how does tf know the relative cell from the original position in the grid ?  Or just direct me to the part of the source code. Do you mean the `void CropAndResizePerBox()` in the C src ? 
I'm a bit confused on how the geometry works in calculating the `y_in` and `x_in` [values in the nested loop](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/crop_and_resize_op.cc#L240).",could please comment work understood want take relative floating point origin current cell know relative cell original position grid direct part source code mean void bit confused geometry work calculating loop,issue,negative,negative,neutral,neutral,negative,negative
450402588,"The link is offline for a good reason: There are many changes in Tensor flow and I do not want to contribute another deprecated guide.

The file
https://github.com/tensorpack/tensorpack/blob/35beb43c4b30f703f885358fd9eb2776b47378ee/examples/basics/export-model.py

Should be pretty self-explanatory. If something changes, I will provide another PR to update this file whenever necessary. The serving part (Serverside) is up-to-date and covered as well under
https://github.com/PatWie/tensorflow-cmake/tree/master/serving

",link good reason many tensor flow want contribute another guide file pretty something provide another update file whenever necessary serving part covered well,issue,positive,positive,positive,positive,positive,positive
450360924,@PatWie hi ，Can you re-deliver a link for **more details (why and how)** ?The previous link is no longer accessible.,hi link previous link longer accessible,issue,negative,positive,positive,positive,positive,positive
450360424,"sorry to bother , I did not read the relevant issues carefully ，Now I found some references [here](https://github.com/tensorpack/tensorpack/pull/290).
",sorry bother read relevant carefully found,issue,negative,negative,neutral,neutral,negative,negative
450311984,"Your error log is about convolution, not max pooling. Only replacing maxpooling is certainly not enough.
You never need to retrain the model because both data_format uses the weights in the same format.",error log convolution certainly enough never need retrain model format,issue,negative,neutral,neutral,neutral,neutral,neutral
450311531,"Although this might be unrelated to TensorPack, please allow me to post what I have tried here. I tried the following methods, but both failed:

(1) compile the tensorflow and the tensorflow_model_server with Intel-MKL support, then using the tensorflow-serving-api to serve the FasterRCNN model trained with TensorPack. But the following error occurs:

    raise _Rendezvous(state, None, None, deadline)
    grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.ABORTED, Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file external/org_tensorflow/tensorflow/core/kernels/mkl_conv_ops.cc:1125
         [[{{node conv0/Conv2D}}]])>

(2) replace all the `MaxPooling2D` and `tf.nn.avg_pool` with NCHW in the FasterRCNN model with the corresponding NHWC versions (plus the extra two `tf.transpose` operations).  But the following error occurs:

    raise _Rendezvous(state, None, None, deadline)
    grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNIMPLEMENTED, Generic conv implementation only supports NHWC tensor format for now.
         [[{{node conv0/Conv2D}} = Conv2D[T=DT_FLOAT, _output_shapes=[[1,64,?,?]], data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Pad, conv0/W/read)]])>

So the final way is, retrain the ImageNet-Pretrained ResNet models with NHWC data format, and then modify all the NCHW operations to the NHWC version? 

Is it possible to re-create the graph with all operations in NHWC format, and then change all the variables in NCHW format to the NHWC format?",although might unrelated please allow post tried tried following compile support serve model trained following error raise state none none deadline operation received exception status message could create dilated convolution forward file node replace model corresponding plus extra two following error raise state none none deadline generic implementation tensor format node valid pad final way retrain data format modify version possible graph format change format format,issue,negative,neutral,neutral,neutral,neutral,neutral
450142123,"It seems the tensorflow also used the tranpose to fit the maxpool op with NCHW format. Please see the following lines:

https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/ops/nn_ops.py#L904-L913",also used fit format please see following,issue,positive,positive,positive,positive,positive,positive
449957068,"Thanks a lot! @ppwwyyxx , I just used the pip install from official website.",thanks lot used pip install official,issue,negative,positive,positive,positive,positive,positive
449934849,"It's more likely that your tensorflow is built with MKL which includes support for NCHW layout, however ours are the default build without MKL.

In any case such issues do not seem to be related to tensorpack and I don't have any other comments on your questions.",likely built support layout however default build without case seem related,issue,negative,neutral,neutral,neutral,neutral,neutral
449932934,"Thank you @ppwwyyxx . I used the tf-1.10.1. I'm still figuring out why the saved model trained with tensorpack cannot be deployed in tensorflow-serving CPU environment. 

I checked the tf.layers.MaxPooling2D API in 

https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/keras/layers/pooling.py#L304-L308

it seems the function used the `tf.nn.max_pool`, if the code snippts above can be run in CPU mode, why not the tf.layers.MaxPooling2D? I probably missed something.",thank used still saved model trained environment checked function used code run mode probably something,issue,positive,neutral,neutral,neutral,neutral,neutral
449930744,"The code does not work for me or @susueah on CPU. 
Maybe it works in some version.

If NCHW works for you already, sounds good and you may have less work to do then.",code work maybe work version work already good may le work,issue,negative,positive,positive,positive,positive,positive
449928305,"@ppwwyyxx I tested the code, it seems the code can be run in CPU mode. Please check the following screenshots.

![1](https://user-images.githubusercontent.com/43327429/50438794-91266600-092a-11e9-9942-542eb24ba7e8.png)
![2](https://user-images.githubusercontent.com/43327429/50438795-91266600-092a-11e9-8c9e-3e946a988969.png)

In addition, please check the following line, https://github.com/tensorflow/tensorflow/blob/8855b0c12433b1bdebfa1ea72b966a35fb0925bf/tensorflow/core/kernels/pooling_ops_common.h#L299
it seems the TensorFlow has more data formats (e.g., the `NCHW_VECT_C`), not only the `NHWC` and `NCHW`. I don't know if the `tf.layers` can support the new formats.
",tested code code run mode please check following addition please check following line data know support new,issue,positive,positive,neutral,neutral,positive,positive
449922226,"Hi @ppwwyyxx , it seems the `tf.nn.max_pool` support the NCHW format. Please check this issue https://github.com/tensorflow/tensorflow/issues/15364#issuecomment-394766712 

Does this mean the tf.layers APIs used in tensorpack have some inconsistency with the tf.nn APIs?",hi support format please check issue mean used inconsistency,issue,positive,negative,negative,negative,negative,negative
449894746,"Yeah, thanks a lot. I checked the `FasterRCNN/model_fpn.py`, it seems you have shown the solution from https://github.com/tensorpack/tensorpack/blob/b097e7d5acefa4eec13873f257035ac0c3db595b/examples/FasterRCNN/model_fpn.py#L40-L45

And I think we may modify all the backbone graph and the FasterRCNN graph to replace the MaxPooling(channel_first) to MaxPooling(channel_last).

So I think maybe the easist way is to implement the Maxpooling(NCHW) in CPU mode. I'm not sure why this is not implemented in TensorFlow.


",yeah thanks lot checked shown solution think may modify backbone graph graph replace think maybe way implement mode sure,issue,positive,positive,positive,positive,positive,positive
449890213,"Easiest way to make it run on CPU is to replace all the NCHW layers (conv, pooling, maybe batchnorm) by  transpose + NHWC layers + transpose back.
But this will apparently going to be very slow and you'll want to rewrite the graph in a way that's more efficient on CPU.",easiest way make run replace maybe transpose transpose back apparently going slow want rewrite graph way efficient,issue,positive,negative,neutral,neutral,negative,negative
449889096,@ppwwyyxx thanks a lot for your reply! Could you please give some advice on how to do this? Do we need to replace all the `maxpool(NCHW)` with `maxpool(NHWC)`? Thanks. ,thanks lot reply could please give advice need replace thanks,issue,positive,positive,positive,positive,positive,positive
449874382,"Yes. The interface for ""layer"" in any libraries is `f(input) -> output`, not `f(input, weights) -> output`.",yes interface layer input output input output,issue,negative,neutral,neutral,neutral,neutral,neutral
449872964,"I'm trying to adapt model-agnostic meta learning to tensorpack example FasterRCNN.
So, you mean that I can not use the ""layer""  in example, I should rewrite the model on my own ? ",trying adapt meta learning example mean use layer example rewrite model,issue,negative,negative,negative,negative,negative,negative
449871597,"Tensorpack is unrelated to how you implement your model. You can use any tensorflow symbolic functions that computes the output from the input.

You'll not be able to use ""layer""-like abstractions because they do not provide you an interface to use different weights. You'll need to write the model on your own.",unrelated implement model use symbolic output input able use layer provide interface use different need write model,issue,negative,positive,positive,positive,positive,positive
449871454,"Thanks for your reply. 
Yes, It can be formulated as a single-cost optimization problem as you mentioned.
 To implement function `f(inputs, weights) -> cost` is what i want to achieve here.

But, My Technical difficulty is that how to implement the function practically.
such as  accessed all the weights through tensorpack interface,
and pass it down to my model so that `f(inputs, weights) -> cost` could work.",thanks reply yes optimization problem implement function cost want achieve technical difficulty implement function practically interface pas model cost could work,issue,positive,positive,neutral,neutral,positive,positive
449868525,"Please see https://tensorpack.readthedocs.io/tutorial/training-interface.html, https://tensorpack.readthedocs.io/tutorial/extend/trainer.html.

To answer your question: `ModelDesc` and the existing trainers are all for single-cost optimization. So you can either (1): formulate your task as a single-cost optimization or (2) write your own trainer (see tutorial above).

It appears to me that your problem can be formulated as a single-cost optimization problem. You just need to implement the function `f(inputs, weights) -> cost` in your equation, and your final cost is just f(inputs, θ - α∇θfθ), as your equation suggests. ",please see answer question optimization either formulate task optimization write trainer see tutorial problem optimization problem need implement function cost equation final cost equation,issue,positive,neutral,neutral,neutral,neutral,neutral
449825050,Training code for the GN model is now available at https://github.com/ppwwyyxx/GroupNorm-reproduce/,training code model available,issue,negative,positive,positive,positive,positive,positive
449806898,"""A small change is not enough"" implies that with a large change this is possible.

It can be deployed if you write the model in a way that tensorflow supports on CPU.",small change enough large change possible write model way,issue,negative,negative,neutral,neutral,negative,negative
449806563,"Hi @ppwwyyxx , thanks a lot for your great tensorpack! I have  a question: do you mean the Faster-RCNN model trained with tensorpack cannot be depolyed in CPU environment? Thanks.",hi thanks lot great question mean model trained environment thanks,issue,positive,positive,positive,positive,positive,positive
449553563,"https://tensorpack.readthedocs.io/_modules/tensorpack/predict/dataset.html#SimpleDatasetPredictor

If you can't figure out, please post an issue following the issue template with a reproducible example.",ca figure please post issue following issue template reproducible example,issue,negative,neutral,neutral,neutral,neutral,neutral
449546593,"Even if i run SimpleDatasetPredictor with batch_size=1 from dataset, it give different result compared with sess.run (the same result as OfflinePredictor) in Tensorflow. ",even run give different result result,issue,negative,neutral,neutral,neutral,neutral,neutral
449545781,There is no difference. They will give you the same result under the same inputs provided if your model is deterministic.,difference give result provided model deterministic,issue,negative,neutral,neutral,neutral,neutral,neutral
449545735,"hello, what is the difference between OfflinePredictor and SimpleDatasetPredictor, because when i run inference on Imagenet with these functions, it give me different result.",hello difference run inference give different result,issue,negative,neutral,neutral,neutral,neutral,neutral
449226892,"See https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md:

> ""The examples do not perform as expected after I change the models/dataset/parameters/etc."" Tensorpack maintainers make sure the examples perform well without modifications. But it's your job to pick the model and parameters that are suitable for your own situation. We cannot help with such questions unless they appear to be a bug in tensorpack.

",see perform change make sure perform well without job pick model suitable situation help unless appear bug,issue,positive,positive,positive,positive,positive,positive
449226127,"I found that your version of training is very fast compared to the version which link is 
 https://github.com/matterport/Mask_RCNN and can be flexibly configured like MODE_MASK=False.
I guess the reason for the slow convergence is because the number of graphics card is too small and the batch is very small.
Please also give me some advice Thanks！.",found version training fast version link flexibly like guess reason slow convergence number graphic card small batch small please also give advice,issue,positive,negative,negative,negative,negative,negative
449222607,"I reinstall the environment by using conda.
pip install tensorflow-gpu==1.9.0
",reinstall environment pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
449198559,"Thank you very much. I will try it.

On Thu, Dec 20, 2018 at 4:54 PM Yuxin Wu <notifications@github.com> wrote:

> The easiest way to debug is to remove your model and replace it with a
> simple model of several FullyConnected layers and see if the wrong behavior
> still exists.
>
> If the error is gone, that means your model may have override the device
> setting somewhere and you can debug from there by adding parts of your
> model back.
>
> If the wrong behavior is still there, remove the data and replace it with
> FakeData. Remove all the custom callbacks.
> If the wrong behavior is still there then, you should have already created
> a small enough example that you can share.
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/1014#issuecomment-449161913>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AGGcrwHOteR-eSp44o2gxAks6CuOLzDGks5u7BUZgaJpZM4ZVjp9>
> .
>
-- 

Hongyang
",thank much try wrote easiest way remove model replace simple model several see wrong behavior still error gone model may override device setting somewhere model back wrong behavior still remove data replace remove custom wrong behavior still already small enough example share state reply directly view mute thread,issue,negative,negative,negative,negative,negative,negative
449182794,"Sorry, I tried creating a minimal example with some fake data and couldn't reproduce it. I'll close the issue for now and re-open when I have more time to investigate.",sorry tried minimal example fake data could reproduce close issue time investigate,issue,negative,negative,negative,negative,negative,negative
449161913,"The easiest way to debug is to remove your model and replace it with a simple model of several FullyConnected layers and see if the wrong behavior still exists. 

If the error is gone, that means your model may have override the device setting somewhere and you can debug from there by adding parts of your model back.

If the wrong behavior is still there, remove the data and replace it with `FakeData`. Remove all the custom callbacks. 
If the wrong behavior is still there then, you should have already created a small enough example that you can share.",easiest way remove model replace simple model several see wrong behavior still error gone model may override device setting somewhere model back wrong behavior still remove data replace remove custom wrong behavior still already small enough example share,issue,negative,negative,negative,negative,negative,negative
449160955,"Nope, I debugged several days but didn't find any clue. Since the deadline
is comming soom, I may try to fix it later. I am not sure it's the problem
of tensoeflow or cuda.

On Thu, Dec 20, 2018, 16:46 Yuxin Wu <notifications@github.com> wrote:

> Have you solved this issue?
>
> —
> You are receiving this because you modified the open/close state.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/1014#issuecomment-449160177>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AGGcrw-pgKb9J_h2XqHGTh_-gRAqQWjSks5u7BMugaJpZM4ZVjp9>
> .
>
-- 

Hongyang
",nope several day find clue since deadline may try fix later sure problem wrote issue state reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
449160892,Do you have a minimal reproducible example that demonstrates the bug? Otherwise it's hard to tell why it happens or whether it is related to tensorpack or not.,minimal reproducible example bug otherwise hard tell whether related,issue,negative,negative,negative,negative,negative,negative
448888602,"@

> Your batch size will be 1.

OK. Thank you very much",batch size thank much,issue,negative,positive,positive,positive,positive,positive
448887726,@ppwwyyxx That's OK. I just want to figure out the detailed principle of the code. But does this mean that my batch is 8 or 1?Because my machine has become very stuck。,want figure detailed principle code mean batch machine become,issue,negative,positive,neutral,neutral,positive,positive
448887090,"You can.

You do not need to modify any parameters. However you might get very different results in the end, even if you're willing to wait for that long.",need modify however might get different end even willing wait long,issue,negative,positive,neutral,neutral,positive,positive
448841818,"I  have check this:
./tensorpack-master/examples/FasterRCNN/train.py --predict person.jpg --load ./tensorpack-master/examples/FasterRCNN/model_cascade.py --config MODE_MASK=True MODE_FPN=True FPN.CASCADE=True BACKBONE.RESNET_NUM_BLOCKS=[3,4,23,3] TEST.RESULT_SCORE_THRESH=1e-4 PREPROC.TRAIN_SHORT_EDGE_SIZE=[640,800] TRAIN.LR_SCHEDULE=[420000,500000,540000] BACKBONE.WEIGHTS=./tensorpack-master/COCO-R101FPN-MaskRCNN-BetterParams.npz


./tensorpack-master/examples/FasterRCNN/train.py --predict person.jpg --load ./tensorpack-master/examples/FasterRCNN/model_mrcnn.py --config MODE_MASK=True MODE_FPN=True FPN.CASCADE=True BACKBONE.RESNET_NUM_BLOCKS=[3,4,23,3] TEST.RESULT_SCORE_THRESH=1e-4 PREPROC.TRAIN_SHORT_EDGE_SIZE=[640,800] TRAIN.LR_SCHEDULE=[420000,500000,540000] BACKBONE.WEIGHTS=./tensorpack-master/COCO-R101FPN-MaskRCNN-BetterParams.npz

the error info is same.",check predict load predict load error,issue,negative,neutral,neutral,neutral,neutral,neutral
448835949,"See https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN#inference

The `--load` has to be a model you downloaded in the ""results"" table.",see load model table,issue,negative,neutral,neutral,neutral,neutral,neutral
448376229,"Perfect. Feel free to close the PR, then! :)

On 18 Dec 2018 21:24, Yuxin Wu <notifications@github.com> wrote:

For logging and argscope, see #778<https://github.com/tensorpack/tensorpack/pull/778> by @PatWie<https://github.com/PatWie>
You can use:

enable_argscope_for_module(tf.layers)

to get the ""tensorpack-style"" logging and argscope for tf.layers.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/tensorpack/tensorpack/pull/1017#issuecomment-448375779>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AOeHoAGV6ta0vqWbDUgd0X4C1jojaEYjks5u6V0AgaJpZM4ZYzDm>.
",perfect feel free close wrote logging see use get logging thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
448375779,"For logging and argscope, see #778 by @PatWie 
You can use:
```python
enable_argscope_for_module(tf.layers)
```
to get the ""tensorpack-style"" logging and argscope for tf.layers.

https://github.com/tensorpack/tensorpack/blob/master/examples/basics/mnist-tflayers.py",logging see use python get logging,issue,negative,neutral,neutral,neutral,neutral,neutral
448372347,"I know. That is exactly what I had. But then I wrote the wrapper for the nice logging and argscope. I thought might have been useful. Probably the split implementation is a bit useless though.


On 18 Dec 2018 18:08, Yuxin Wu <notifications@github.com> wrote:

Please see the following from https://tensorpack.readthedocs.io/tutorial/symbolic.html:

These layers were written only because there were no alternatives when tensorpack was first developed. Nowadays, these implementation actually call tf.layers directly. Tensorpack will not add any more layers into its core library because this is not the focus of tensorpack, and there are many other alternative symbolic libraries today.

You can use tf.layers.Conv3D directly in tensorpack. You do not need to add a tensorpack wrapper for them.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/tensorpack/tensorpack/pull/1017#issuecomment-448315050>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AOeHoJxysJcFMMAkRUypXwhrn7Ux4fLnks5u6S8FgaJpZM4ZYzDm>.
",know exactly wrote wrapper nice logging thought might useful probably split implementation bit useless though wrote please see following written first nowadays implementation actually call directly add core library focus many alternative symbolic today use directly need add wrapper thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
448315050,"Please see the following from https://tensorpack.readthedocs.io/tutorial/symbolic.html:

> These layers were written only because there were no alternatives when tensorpack was first developed. Nowadays, these implementation actually call tf.layers directly. Tensorpack will not add any more layers into its core library because this is not the focus of tensorpack, and there are many other alternative symbolic libraries today.

You can use `tf.layers.Conv3D` directly in tensorpack. You do not need to add a tensorpack wrapper for them.",please see following written first nowadays implementation actually call directly add core library focus many alternative symbolic today use directly need add wrapper,issue,negative,positive,positive,positive,positive,positive
448134408,Thanks for finding this! The links were fixed now and should be effective in several minutes,thanks finding link fixed effective several,issue,positive,positive,positive,positive,positive,positive
448102913,"> To modify the pre-trained model you can load it with `np.load` and save it with `np.savez`. Refer to numpy documentation for details.

Thank you. I'll try your method.",modify model load save refer documentation thank try method,issue,positive,neutral,neutral,neutral,neutral,neutral
448101940,To modify the pre-trained model you can load it with `np.load` and save it with `np.savez`. Refer to numpy documentation for details.,modify model load save refer documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
448101800,"Oh I misread your issue. You're also loading a pre-trained model.
You can remove `fastrcnn/outputs/*`, `maskrcnn/conv/*` from the pre-trained model because it's impossible to load them when your dataset has different number of outputs.
Alternatively, you can rename them in the graph. If the names are different, these values won't be loaded.",oh misread issue also loading model remove model impossible load different number alternatively rename graph different wo loaded,issue,negative,negative,negative,negative,negative,negative
448100572,"> That's the only place you need to modify.

The problem still exists. Do you have any other suggestions?",place need modify problem still,issue,negative,neutral,neutral,neutral,neutral,neutral
448098673,"> The config file currently has NUM_CATEGORY=80

I have changed the config.py file.Where else should I modify it?
_C.DATA.NUM_CATEGORY = 46 ",file currently else modify,issue,negative,neutral,neutral,neutral,neutral,neutral
447724178,"> In that case, gpu 6 was allocated with all tensors while the other three remained 215MiB. It seems the tensor allocation only allocate tensors on the first gpu.

And I assume this behavior should happen on a different machine as well.

It's hard to comment anything without a reproducible code. Perhaps you override the device setting of tensorpack in your model in some way.",case three mib tensor allocation allocate first assume behavior happen different machine well hard comment anything without reproducible code perhaps override device setting model way,issue,negative,negative,neutral,neutral,negative,negative
447723481,"Actually I tried to reduce the batch size such that OOM does not happen. In that case, gpu 6 was allocated with all tensors while the other three remained 215MiB. It seems the tensor allocation only allocate tensors on the first gpu.",actually tried reduce batch size happen case three mib tensor allocation allocate first,issue,negative,positive,positive,positive,positive,positive
447716993,"I don't know how much this picture actually tells. Perhaps it shows this only because the program encounters an OOM even before starting to work on other tensors on the other 3 GPUs. I wouldn't assume there are bugs just because of seeing this picture.

From public data it seems that Titan XP does have 1GB more memory than 1080Ti.",know much picture actually perhaps program even starting work would assume seeing picture public data memory ti,issue,negative,positive,neutral,neutral,positive,positive
447707013,"Thanks. I have been using tensorpack for more than one year. It's a really good one for multi-gpu training. The thing confuses me is this:

![image](https://user-images.githubusercontent.com/6397103/50063911-0a2bfd80-0174-11e9-8b6b-ac5f9f5be2c0.png)

I train on 4 gpus. But it seems all are allocated to one gpu instead of 4. Is it possible for some configuration. On the server where code runs well, the gpus are Titan XP. But on this trouble server, they are GTX 1080 ti. I mean does the multi-gpu allocation depend on something? Thank you very much.
",thanks one year really good one training thing image train one instead possible configuration server code well trouble server ti mean allocation depend something thank much,issue,positive,positive,neutral,neutral,positive,positive
447703340,"It looks just like a normal OOM to me. I cannot see whether ""the gpu allocator allocates all operations to one gpu instead of all gpus"" or not from your observations, and cannot see whether your issue is related to tensorpack or not.

Note that even if the same code works on a different machine before, there could be many factors that affect memory allocation, such as TF version,  gpu numbers and model, cuda version, etc.",like normal see whether allocator one instead see whether issue related note even code work different machine could many affect memory allocation version model version,issue,negative,positive,positive,positive,positive,positive
447617488,"We have no plans to do so, especially at this early stage when ""Mesh TF"" is still premature. I have found no evidence that this library works well for real-life problems on GPUs.",especially early stage mesh still premature found evidence library work well,issue,negative,positive,positive,positive,positive,positive
447173778,"Then this appears to be a tensorflow bug. But still it's best if you could create a minimal reproducible example that demonstrates the bug (i.e. the crash), not to demonstrate that it creates two graphs (perhaps there is a reason for it to do so).",bug still best could create minimal reproducible example bug crash demonstrate two perhaps reason,issue,positive,positive,positive,positive,positive,positive
447173210,The `_GetGlobalDefaultGraph` call creates the default one - I verified with `tf.get_default_graph()`. The second one created in `func_graph_from_py_func` is different.,call default one second one different,issue,negative,neutral,neutral,neutral,neutral,neutral
447172712,Maybe that's because it's actually creating the default one (if no one has been created yet) ,maybe actually default one one yet,issue,negative,neutral,neutral,neutral,neutral,neutral
447172408,"I put a breakpoint in the constructor of `tf.Graph`, and it turns out that the `tf.data.TFRecordDataset` is creating another graph instead of using the default one. 

```
import tensorflow as tf
TRAIN_FILE = '...'
ds = tf.data.TFRecordDataset(TRAIN_FILE, compression_type='GZIP')
```

```
-> ds = tf.data.TFRecordDataset(TRAIN_FILE, compression_type='GZIP')
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py(194)__init__()
-> filenames = ops.convert_to_tensor(filenames, dtype=dtypes.string)
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(998)convert_to_tensor()
-> as_ref=False)
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(1094)internal_convert_to_tensor()
-> ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py(217)_constant_tensor_conversion_function()
-> return constant(v, dtype=dtype, name=name)
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py(192)constant()
-> g = ops.get_default_graph()
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(5327)get_default_graph()
-> return _default_graph_stack.get_default()
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(5002)get_default()
-> ret = self._GetGlobalDefaultGraph()
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(5011)_GetGlobalDefaultGraph()
-> self._global_default_graph = Graph()
> /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(2677)__init__()
-> self._lock = threading.RLock()
(Pdb) c
-> ds = tf.data.TFRecordDataset(TRAIN_FILE, compression_type='GZIP')
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py(207)__init__()
-> self._impl = filenames.flat_map(read_one_file)
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py(1001)flat_map()
-> return FlatMapDataset(self, map_func)
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py(2257)__init__()
-> experimental_nested_dataset_support=True)
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py(1454)__init__()
-> self._function.add_to_graph(ops.get_default_graph())
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py(481)add_to_graph()
-> self._create_definition_if_needed()
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py(337)_create_definition_if_needed()
-> self._create_definition_if_needed_impl()
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py(346)_create_definition_if_needed_impl()
-> self._capture_by_value, self._caller_device)
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py(845)func_graph_from_py_func()
-> func_graph = _FuncGraph(name, capture_by_value)
  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py(631)__init__()
-> super(_FuncGraph, self).__init__(*args, **kwargs)
> /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(2677)__init__()
-> self._lock = threading.RLock()
```",put constructor turn another graph instead default one import ret value return constant constant return ret graph return self name super self,issue,positive,positive,positive,positive,positive,positive
447142656,"It is the correct way. Tensorpack does not create new `tf.Graph` so I don't know why they appear to be in two different graphs. It's better if you could come up with a minimal reproducible example to demonstrate the error.
Also please also make sure you did not create any graph in your code.",correct way create new know appear two different better could come minimal reproducible example demonstrate error also please also make sure create graph code,issue,positive,positive,positive,positive,positive,positive
446940729,">     1. Loading a pre-trained model is just simply `--load model` handled in most examples. `--load` will try to load all parameters with the same name and print warning about the ummatched weights in the model, so to re-train some layers you should change their names before loading. [HED](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/HED) is such an example which loads pre-trained vgg..
> 
>     2. [FAQ](http://tensorpack.readthedocs.io/en/latest/tutorial/faq.html#how-to-freeze-some-variables-in-training) about freezing variables.

can't find the FAQ page",loading model simply load model handled load try load name print warning model change loading example freezing ca find page,issue,negative,neutral,neutral,neutral,neutral,neutral
446833788,The `npz` file contains no information about network architecture.,file information network architecture,issue,negative,neutral,neutral,neutral,neutral,neutral
446833643,Thanks for spotting this! This line has no effect and has been removed now.,thanks spotting line effect removed,issue,negative,positive,positive,positive,positive,positive
446827926,"Thus, we need to add a subclass of DetectionModel.
Then, it may need to get the input/output tensor name.
Is there a way to check from the npz file?",thus need add subclass may need get tensor name way check file,issue,negative,neutral,neutral,neutral,neutral,neutral
446777508,"As quoted above:

> ""The examples do not perform well after I change the models/dataset/parameters/etc."" Tensorpack maintainers make sure the examples perform well without modification. But it's your job to pick the model and parameters that are suitable for your own situation. We cannot help with such questions unless they appear to be a bug in tensorpack.

The implemented SE is for training resnet on ImageNet and it is effective there.",perform well change make sure perform well without modification job pick model suitable situation help unless appear bug se training effective,issue,positive,positive,positive,positive,positive,positive
446769797,"This issue is to test the effectiveness of your implemented Squeeze-and-Excitation operation in 'resnet_model.py'

  It seems has no improvement on your 'cifar10-resnet.py'. ",issue test effectiveness operation improvement,issue,negative,neutral,neutral,neutral,neutral,neutral
446768728,"Tensorpack issues are for questions/bug reports about the library. We do not answer general machine learning questions such as ""why my model doesn't work"".

From https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md:

> Some typical questions that we DO NOT answer:
""Could you improve/implement an example/paper ?"" -- We have no plans to do so. We don't consider feature requests for examples or implement a paper for you. If you don't know how to do something yourself, you may ask a usage question.
""The examples do not perform well after I change the models/dataset/parameters/etc."" Tensorpack maintainers make sure the examples perform well without modification. But it's your job to pick the model and parameters that are suitable for your own situation. We cannot help with such questions unless they appear to be a bug in tensorpack.
""Why my model doesn't work?"", ""I don't understand this paper you implement."", ""How should I change the examples for my own dataset?"" We do not answer machine learning questions.
",library answer general machine learning model work typical answer could consider feature implement paper know something may ask usage question perform well change make sure perform well without modification job pick model suitable situation help unless appear bug model work understand paper implement change answer machine learning,issue,positive,positive,positive,positive,positive,positive
446681415,`saver` is not a tensorflow `tf.Operation` therefore it's not possible to use it inside `build_graph`. You need to use a callback.,saver therefore possible use inside need use,issue,negative,neutral,neutral,neutral,neutral,neutral
446680969,No that's not how things works. You'll need to modify the code to use a different base model.,work need modify code use different base model,issue,negative,negative,negative,negative,negative,negative
446597869,"您好，我也遇到了同样的问题，可以分享下您最终的解决办法吗？
Hello, I have encountered the same problem, can you share your final solution?",hello problem share final solution,issue,negative,neutral,neutral,neutral,neutral,neutral
446500887,"@ppwwyyxx 
what if I want to apply two different operations , according to global_step, but not just two returned values of two functions? Like this: 
```
if global_step>100:
   saver = tf.train.Saver(vars_1)
else:
   saver = tf.train.Saver(vars_2)
```
`tf.cond `function seems not to work here.",want apply two different according two returned two like saver else saver function work,issue,negative,neutral,neutral,neutral,neutral,neutral
446493627,"In general you can use the `RunOp` callback to run any ops.
However the table initializers should be run by default by tensorpack. The above commit 168f729 should resolve this.",general use run however table run default commit resolve,issue,positive,positive,neutral,neutral,positive,positive
446431775,"Yes, I also have found that mutli thread implementation is not much faster than single thread implementation.

Thanks for kind reply! It really helps a lot.",yes also found thread implementation much faster single thread implementation thanks kind reply really lot,issue,positive,positive,positive,positive,positive,positive
446287687,"""thread"" does not ""fork"". The data integrity of `MultiThreadMapData` was explained in: https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.MultiThreadMapData. You'll not see duplicates

Also, threads in python are unlikely to improve the performance substantially",thread fork data integrity see also python unlikely improve performance substantially,issue,positive,negative,negative,negative,negative,negative
446225703,"Thanks a lot! I finally understand that.

Unfortunately, I found that LMDB deserialization is the bottleneck.
So, I try to implement the dataflow in a parallel way.

For example:

            from tensorpack.utils.compatible_serialize import loads

            ds = LMDBData(datadir, shuffle=False)
            ds = MultiThreadMapData(ds, nr_thread=25,
                                    map_func = lambda dp: loads(dp[1]))
            ds = LocallyShuffleData(ds, 50000)
            ds = PrefetchData(ds, 5000, 1)
            ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
            ds = AugmentImageCoordinates(ds, augmentors, coords_index=2, copy=False)
            ds = PrefetchDataZMQ(ds, 40)
            ds = BatchData(ds, 256, remainder=not isTrain)

At this time, I am worried about this multithread implementation. 
If the MultiThreadMapData forks the LMDBData, then all threads are expected to output the same datapoint. 
Is this true? 

Sorry for my poor background about parallel implementation.
",thanks lot finally understand unfortunately found bottleneck try implement parallel way example import lambda lambda time worried implementation output true sorry poor background parallel implementation,issue,negative,negative,negative,negative,negative,negative
446088131,"Thank you for reply!

I have another question.

I am confused with 'LocallyShuffleData'. What does 'buffer_size' mean?
Does it mean the number of images? 

So, If I set the 'buffer_size' of LocallyShuffleData to the number of training data (e.g. 1,281,167 for ImageNet), then the shuffle occurs every one epoch?
",thank reply another question confused mean mean number set number training data shuffle every one epoch,issue,negative,negative,negative,negative,negative,negative
445846848,@junsukchoe 'shuffle' will do random access on the file and will therefore put more pressure on the hard disk.,random access file therefore put pressure hard disk,issue,negative,negative,negative,negative,negative,negative
445845793,"As the error log suggests, this is a CUDA installation issue.
See also https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html",error log installation issue see also,issue,negative,neutral,neutral,neutral,neutral,neutral
445737791,"> It can run now.Thank you very much!

I face the same error ,could you tell me how to solve it?",run much face error could tell solve,issue,negative,positive,positive,positive,positive,positive
445512165,"> The translation happens at this line:
> [tensorpack/tensorpack/models/conv2d.py](https://github.com/tensorpack/tensorpack/blob/82aa4b2bed83f52c5886e7f0e19378f907657f7f/tensorpack/models/conv2d.py#L77)
> 
> Line 77 in [82aa4b2](/tensorpack/tensorpack/commit/82aa4b2bed83f52c5886e7f0e19378f907657f7f)
> 
>  data_format = get_data_format(data_format, tfmode=False)

Oh! I got it. Thanks very much!",translation line line oh got thanks much,issue,negative,positive,positive,positive,positive,positive
445508815,The code will behave normally. The data format will be translated automatically to a uniform style.,code behave normally data format automatically uniform style,issue,negative,positive,positive,positive,positive,positive
445508791,"> You can use either style and it will not cause error.

I'm quite confused about it. If  _data_format_ is 'channels_last', then _channel_axis_ will be 1 instead of 3 (which I think it should be) according to the [if statement](https://github.com/tensorpack/tensorpack/blob/82aa4b2bed83f52c5886e7f0e19378f907657f7f/tensorpack/models/conv2d.py#L79) . Do you mean the following codes will behave normally? Or it doesn't matter whether _channel_axis_ is right?",use either style cause error quite confused instead think according statement mean following behave normally matter whether right,issue,negative,negative,neutral,neutral,negative,negative
445508559,You can use either style and it will not cause error.,use either style cause error,issue,negative,neutral,neutral,neutral,neutral,neutral
445482838,"So `InferenceRunner(QueueInput(dataset_val), [ScalarStats('total_cost')])` this line will go to the condition of ""is.training=False""? Therefore there was no `total_lost` calculation, am I right?",line go condition therefore calculation right,issue,negative,positive,positive,positive,positive,positive
445479837,"There is no existing definition of ""validation loss"" for FasterRCNN anywhere as far as I know. The definition of such a concept is a machine learning question, so I'll not discuss it here.

If you have a definition of such a loss, you can implement it yourself in the `build_graph` function when `is_training=False`. Implementing a graph will only rely on tensorflow knowledge and is unrelated to tensorpack.

Please also note that the current evaluation only depends on the input image, therefore the validation dataflow only produces the input image. If your definition of such a loss depends on other input data (such as labels), you'll need to also define a dataflow to produce them.",definition validation loss anywhere far know definition concept machine learning question discus definition loss implement function graph rely knowledge unrelated please also note current evaluation input image therefore validation input image definition loss input data need also define produce,issue,negative,positive,neutral,neutral,positive,positive
445477396,"Thanks for reply, Yuxin. Can we get some advice for achieving the validation loss during the training, such as adding some coding lines somewhere? It seems challenging to borrow some ideas from the codes from other example repositories. The available prediction or evaluation features seem to specifically generate the COCO metrics. Instead, we would like to monitor the validation loss during the training if possible. Thanks again! ",thanks reply get advice validation loss training somewhere borrow example available prediction evaluation seem specifically generate coco metric instead would like monitor validation loss training possible thanks,issue,positive,positive,positive,positive,positive,positive
445476187,"In validation, the graph does not compute the cost, as can be seen in `build_graph` (`if is_training: ...`).",validation graph compute cost seen,issue,negative,neutral,neutral,neutral,neutral,neutral
445430584,"> You can downgrade msgpack to 0.5.6

Great! It works. Thank you!",downgrade great work thank,issue,positive,positive,positive,positive,positive,positive
444946744,"The mode was now changed to ""nccl"".
Actually I don't have such machine to test if ""nccl"" works for 10 GPUs. Let me know if anything goes wrong.",mode actually machine test work let know anything go wrong,issue,negative,negative,negative,negative,negative,negative
444657115,"Wow, everything is already there. Sorry, I thought I went through quite much, but still, there are more! Thank you so much!",wow everything already sorry thought went quite much still thank much,issue,positive,neutral,neutral,neutral,neutral,neutral
444584268,"The solution is to write your function in a way that's pickleable (no closures, etc).
If the unpickleable function is from tensorpack, please report it as an issue.

The original issue was fixed already because now the function `MapDataComponent.__init__.<locals>.f` was turned into a member function rather than a closure.",solution write function way function please report issue original issue fixed already function turned member function rather closure,issue,positive,positive,positive,positive,positive,positive
444582755,@vqdang Were you able to find a way around this problem?,able find way around problem,issue,negative,positive,positive,positive,positive,positive
444386136,"Your code never changes the learning rate. `self.lr = xxx` does not modify the value of learning rate variable. It only modifies the attribute ""lr"" of the `ModelDesc`. `tf.assign` and similar functions need to be used to modify a tensorflow variable.

Also, this code is not safe for multi-gpu training because the tower function will be called multiple times: https://tensorpack.readthedocs.io/tutorial/trainer.html


Closing since this is a tensorflow usage problem and therefore unrelated to tensorpack. ",code never learning rate modify value learning rate variable attribute similar need used modify variable also code safe training tower function multiple time since usage problem therefore unrelated,issue,negative,positive,positive,positive,positive,positive
444379334,Thanks. Then commit 7158eed should resolve this issue.,thanks commit resolve issue,issue,positive,positive,positive,positive,positive,positive
444379020,"Thanks a lot. It could be this reason. I am tuning the code and set the epoch as 5 steps which takes only 1 second. When the step_per_epoch set to 250, the issues disappear.",thanks lot could reason tuning code set epoch second set disappear,issue,negative,positive,neutral,neutral,positive,positive
444377414,"It would be better if you could post more logs according to the issue template.
In particular, I'm wondering whether your epoch is too short (<3 seconds) and the `GPUUtilizationTracker` hasn't collected enough samples. 
",would better could post according issue template particular wondering whether epoch short collected enough,issue,negative,positive,positive,positive,positive,positive
444279358,"For those who have the similar issue, the simplest and final solution for me was
```
pip uninstall msgpack-numpy
pip uninstall msgpack-numpy   ( to make sure.. )
pip install msgpack-numpy
```
It gave me `msgpack-numpy-0.4.4.2` and it works.",similar issue final solution pip pip make sure pip install gave work,issue,positive,positive,positive,positive,positive,positive
444276382,"Thanks a lot.  

Actually, I editted my question (sorry, I didn't know if your response would be this quick..). Now the problem doesn't have the nested `PrefetchDataZMQ`s (btw, good to know, thanks.). It seemed to relate to `RandomCrop`.

**However, I tried to make a new clean `virtualenv` and pip-installed packages from the working machine's pip status, and now it works!** As you said, although pip looks the same, some packages seemed to have problems.  Thank you!",thanks lot actually question sorry know response would quick problem good know relate however tried make new clean working machine pip status work said although pip thank,issue,positive,positive,positive,positive,positive,positive
444266230,"It is mentioned in the [docs](https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.PrefetchDataZMQ) that nesting two `PrefetchDataZMQ` is not allowed. I actually don't know what will happen if you do so.

Sometimes people install multiple versions of the same library by mistake and pip only shows one version. This might explain. But anyway, to downgrade, the safest option is to first uninstall it twice (in case there are more than one version of it installed).

No need to downgrade msgpack.",two actually know happen sometimes people install multiple library mistake pip one version might explain anyway downgrade option first twice case one version need downgrade,issue,negative,positive,neutral,neutral,positive,positive
444262031,"Hi,  I had two almost-the-same machines, but **only one** has the `BufferError: memoryview: underlying buffer is not C-contiguous` bug. Interestingly, **both** machines has 
```
numpy==1.14.3
msgpack==0.5.6
msgpack-numpy==0.4.4.1
tensorflow-gpu==1.8.0
pyzmq==17.0.0
```

Could you give me some idea why this may happen? I tried to minimize the code:
```
from tensorpack.utils import logger
from tensorpack.utils.timer import timed_operation
from tensorpack.utils.argtools import log_once
from tensorpack import *
from tensorpack.dataflow import dataset
import cv2

ds = FakeData([[320, 1024, 3], [320, 1024]], random=False,
              dtype=['uint8', 'uint8'], domain=[(0, 255), (0, 81)])

aug_geo = [imgaug.ResizeShortestEdge(350, interp=cv2.INTER_CUBIC),
           imgaug.RandomCrop(320),                          # <-- This line has the issue.
           imgaug.Flip(horiz=True)]
ds = AugmentImageComponents(ds, aug_geo, (0, 1), copy=False)
ds = PrefetchDataZMQ(ds, nr_proc=5)    
ds = BatchData(ds, 1)
TestDataSpeed(ds).start()
```
**Very interestingly, if I remove `RandomCrop`, everything works fine. `RandomCrop` + `PrefetchDataZMQ` brings the issue.** Errors look like:
```
Process _Worker-1:5:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/home/paul/projects/tensorpack/tensorpack/dataflow/parallel.py"", line 290, in run
    socket.send(dumps(dp), copy=False)
  File ""/home/paul/projects/tensorpack/tensorpack/utils/serialize.py"", line 19, in dumps_msgpack
    return msgpack.dumps(obj, use_bin_type=True)
  File ""/home/paul/.virtualenvs/tf/lib/python3.5/site-packages/msgpack_numpy.py"", line 196, in packb
    return Packer(**kwargs).pack(o)
  File ""msgpack/_packer.pyx"", line 284, in msgpack._packer.Packer.pack
  File ""msgpack/_packer.pyx"", line 290, in msgpack._packer.Packer.pack
  File ""msgpack/_packer.pyx"", line 287, in msgpack._packer.Packer.pack
  File ""msgpack/_packer.pyx"", line 263, in msgpack._packer.Packer._pack
  File ""msgpack/_packer.pyx"", line 234, in msgpack._packer.Packer._pack
  File ""msgpack/_packer.pyx"", line 266, in msgpack._packer.Packer._pack
BufferError: memoryview: underlying buffer is not C-contiguous
  0%|                                                                                                                                               |0/5000[00:00<?,?it/s]  0%|1                                                                                                                                      |5/5000[00:02<36:13, 2.30it/s]
Traceback (most recent call last):
  File ""why.py"", line 27, in <module>
    TestDataSpeed(ds).start()
  File ""/home/paul/projects/tensorpack/tensorpack/dataflow/common.py"", line 61, in start
    for idx, dp in enumerate(itr):
  File ""/home/paul/projects/tensorpack/tensorpack/dataflow/parallel.py"", line 331, in __iter__
    yield self._recv()
  File ""/home/paul/projects/tensorpack/tensorpack/dataflow/parallel.py"", line 321, in _recv
    return loads(self.socket.recv(copy=False))
  File ""zmq/backend/cython/socket.pyx"", line 790, in zmq.backend.cython.socket.Socket.recv
  File ""zmq/backend/cython/socket.pyx"", line 828, in zmq.backend.cython.socket.Socket.recv
  File ""zmq/backend/cython/socket.pyx"", line 172, in zmq.backend.cython.socket._recv_frame
  File ""zmq/backend/cython/checkrc.pxd"", line 12, in zmq.backend.cython.checkrc._check_rc
KeyboardInterrupt
PrefetchDataZMQ successfully cleaned-up.
```

Plus, how to downgrade `msgpack-numpy` if I have to downgrade for one machine? `pip install msgpack-numpy==0.4.3.1`? What about `msgpack`? Would you recommend downgrade together?
",hi two one underlying buffer bug interestingly could give idea may happen tried minimize code import logger import import import import import line issue interestingly remove everything work fine issue look like process recent call last file line file line run file line return file line return packer file line file line file line file line file line file line underlying buffer recent call last file line module file line start enumerate file line yield file line return file line file line file line file line successfully plus downgrade downgrade one machine pip install would recommend downgrade together,issue,positive,positive,positive,positive,positive,positive
443533080,"> So, if I have to use AugmentImageComponents (because it has index=(0, 1) argument so I can apply same augmentations to images and labels equally), then is there no way to use MultiThreadMapData?

Correct. There are also `MultiThreadPrefetchData` which works similarly to `PrefetchDataZMQ`. And `MultiProcessMapData` which works similarly to `MultiThreadMapData`.

> An example I can think of is something like map_func=[augmentor.augment(dp[0]), augmentor.augment(dp[1])], but then, I imagine two different augment will apply to dp[0] and dp[1] respectively (due to random factors).

Yes. The map function needs to written with `augmentor.augment_return_params`. In fact the implementation of `AugmentImageComponents` uses map function as well: https://tensorpack.readthedocs.io/_modules/tensorpack/dataflow/image.html#AugmentImageComponents

I'll add more documentation about these methods of augmentors.

> I'm curious if one can use a multithread way when using AugmentImageComponent(ds, augmentors, (0, 1)).

You can use a correct map function (as mentioned above), or `MultiThreadPrefetchData`.",use argument apply equally way use correct also work similarly work similarly example think something like imagine two different augment apply respectively due random yes map function need written fact implementation map function well add documentation curious one use way use correct map function,issue,positive,negative,negative,negative,negative,negative
443289495,"Sorry; just tried to reproduce your benchmark accuracy with my limited GPU power. 

I will look into how you synchronize the results of the 8 GPUs and how you distribute the 256 samples in the batch to the 8 GPUs in your implementation. Thanks!
",sorry tried reproduce accuracy limited power look synchronize distribute batch implementation thanks,issue,negative,negative,negative,negative,negative,negative
443285653,"As said in the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md), we do not answer __machine learning__ questions.",said issue template answer,issue,negative,neutral,neutral,neutral,neutral,neutral
443277181,"would ""8 GPUs and a total batch size of 256 "" be similar to  ""1 GPUs and a total batch size of 32 ""?",would total batch size similar total batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
443237908,"on https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet
you highlight one of the baselines:
ResNet18 | 10.50% | 29.66%
which was obtained with 8 GPUs and a total batch size of 256.

Do you have a benchmark accuracy for only one GPU? 
Also, I wonder if you have an idea on what the optimal batchsize will be for only one GPU on ResNet18?

When I ran with one GPU/ batch size of 64. I got: 13.1% and 33.8% for the top5 and top1, respectively. Those numbers are 3% lower than what you got. 


",highlight one total batch size accuracy one also wonder idea optimal one ran one batch size got top top respectively lower got,issue,positive,positive,positive,positive,positive,positive
442929737,"You can use small images, small networks, small batch size, etc...",use small small small batch size,issue,negative,negative,negative,negative,negative,negative
442892266,"I see. so, it may be caused by the image size? 
when I crop the image to smaller size, say 200 instead of 224 as your default value, the whole net can fit into my GPU. But with smaller image size, all the Resnet parameters such as learning rate may need to be retuned. ",see may image size crop image smaller size say instead default value whole net fit smaller image size learning rate may need,issue,positive,positive,positive,positive,positive,positive
442873496,"By memory do you mean CPU memory?
You can use fewer prefetching processes in that case.",memory mean memory use case,issue,negative,negative,negative,negative,negative,negative
442867320,"I did use some codes like below, but it seemed to use too much memory to load the data. I wonder if there is a way using less memory?

---
def get_data(train_or_test, fake=False):
    if fake:
        return FakeData([[64, INPUT_SHAPE, INPUT_SHAPE, 3], [64]], 1000, random=False, dtype='uint8')
    isTrain = train_or_test == 'train'
    datadir = args.data
    ds = dataset.ILSVRC12(datadir, train_or_test,
                          shuffle=True if isTrain else False, dir_structure='original')

    ds = FixedSizeData(ds,100000)

---",use like use much memory load data wonder way le memory fake return else false,issue,negative,negative,negative,negative,negative,negative
442800221,"I think the reason is that there is a default ProgressBar callback. You can change it instead of adding another one. The default callbacks are in `extra_callbacks` so you can override it.

I dig into the source code to find out that. Maybe it's better to show it in the document.",think reason default change instead another one default override dig source code find maybe better show document,issue,negative,positive,positive,positive,positive,positive
442774756,"Thanks.  I tried to add `ProgressBar`
````
return AutoResumeTrainConfig(
        dataflow=dataset_train,
        callbacks=[
            ModelSaver(keep_checkpoint_every_n_hours=5),
            GPUUtilizationTracker(),
            EstimatedTimeLeft(),
            ProgressBar(names=['cost']),
        ],
        model=Model(num_classes=NUM_CLASSES),
        steps_per_epoch=steps_per_epoch,
        max_epoch=500,
    )
````

Now, it shows two progress bars, maybe one is the default one.
````
 88%|###########################6           |88/100[00:32<00:04, 2.44it/s], cost=0.156
 88%|#######################################         |88/100[00:32<00:04, 2.44it/s]
````
How can I make the default one to print out `cost`?

",thanks tried add return two progress maybe one default one make default one print cost,issue,positive,positive,positive,positive,positive,positive
442746033,@erdollar hello! I want to know how does your net work with prelu? can you share some results?,hello want know net work share,issue,negative,neutral,neutral,neutral,neutral,neutral
442670680,"1. 
See ""When to log"" in https://tensorpack.readthedocs.io/tutorial/summary.html

2.  https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.ProgressBar has such an option. You can also use https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.TensorPrinter.

Note that both 1 and 2 involves modifying the default callback list: therefore you'll need to overwrite the `extra_callbacks` option in https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.TrainConfig.",see log option also use note default list therefore need overwrite option,issue,negative,neutral,neutral,neutral,neutral,neutral
442322493,"Thanks for your reply, this error is fixed automatically,but another error happened .
 ERR [MultiProcessMapDataZMQ] buffer_size cannot be larger than the size of the DataFlow!
![2018-11-28 13-13-45](https://user-images.githubusercontent.com/30818917/49130548-7652d880-f30f-11e8-9d89-93330c3d2b91.png)
Can you give me some suggestions ? ",thanks reply error fixed automatically another error err size give,issue,negative,positive,positive,positive,positive,positive
442157653,"I would recommend you to uninstall all msgpack related packages in your system and reinstall it.
How to install msgpack is unrelated to tensorpack so I'm closing this issue.",would recommend related system reinstall install unrelated issue,issue,negative,neutral,neutral,neutral,neutral,neutral
442153946,"
so，please, how can I solve it?




    
    
        
            
                
                    
                        
                    
                    
                        
                            454232091
                            
                            
                                
                                    邮箱：454232091@qq.com
                                
                        
                    
                
            
        
    
Signature is customized by Netease Mail Master


On 11/28/2018 01:28, Yuxin Wu wrote: msgpack 0.5.6 has the strict_types option: https://github.com/msgpack/msgpack-python/blob/0.5.6/msgpack/_packer.pyx#L117-L119
Therefore you're not using msgpack 0.5.6. You may have different versions installed at different places.

—You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or mute the thread.
{""api_version"":""1.0"",""publisher"":{""api_key"":""05dde50f1d1a384dd78767c55493e4bb"",""name"":""GitHub""},""entity"":{""external_key"":""github/tensorpack/tensorpack"",""title"":""tensorpack/tensorpack"",""subtitle"":""GitHub repository"",""main_image_url"":""https://assets-cdn.github.com/images/email/message_cards/header.png"",""avatar_image_url"":""https://assets-cdn.github.com/images/email/message_cards/avatar.png"",""action"":{""name"":""Open in GitHub"",""url"":""https://github.com/tensorpack/tensorpack""}},""updates"":{""snippets"":[{""icon"":""PERSON"",""message"":""@ppwwyyxx in #991: msgpack 0.5.6 has the `strict_types` option: https://github.com/msgpack/msgpack-python/blob/0.5.6/msgpack/_packer.pyx#L117-L119\r\n\r\nTherefore you're not using msgpack 0.5.6. You may have different versions installed at different places.""}],""action"":{""name"":""View Issue"",""url"":""https://github.com/tensorpack/tensorpack/issues/991#issuecomment-442146143""}}}
[
{
""@context"": ""http://schema.org"",
""@type"": ""EmailMessage"",
""potentialAction"": {
""@type"": ""ViewAction"",
""target"": ""https://github.com/tensorpack/tensorpack/issues/991#issuecomment-442146143"",
""url"": ""https://github.com/tensorpack/tensorpack/issues/991#issuecomment-442146143"",
""name"": ""View Issue""
},
""description"": ""View this Issue on GitHub"",
""publisher"": {
""@type"": ""Organization"",
""name"": ""GitHub"",
""url"": ""https://github.com""
}
},
{
""@type"": ""MessageCard"",
""@context"": ""http://schema.org/extensions"",
""hideOriginalBody"": ""false"",
""originator"": ""AF6C5A86-E920-430C-9C59-A73278B5EFEB"",
""title"": ""Re: [tensorpack/tensorpack] Train faster rcnn  (#991)"",
""sections"": [
{
""text"": """",
""activityTitle"": ""**Yuxin Wu**"",
""activityImage"": ""https://assets-cdn.github.com/images/email/message_cards/avatar.png"",
""activitySubtitle"": ""@ppwwyyxx"",
""facts"": [

]
}
],
""potentialAction"": [
{
""name"": ""Add a comment"",
""@type"": ""ActionCard"",
""inputs"": [
{
""isMultiLine"": true,
""@type"": ""TextInput"",
""id"": ""IssueComment"",
""isRequired"": false
}
],
""actions"": [
{
""name"": ""Comment"",
""@type"": ""HttpPOST"",
""target"": ""https://api.github.com"",
""body"": ""{\n\""commandName\"": \""IssueComment\"",\n\""repositoryFullName\"": \""tensorpack/tensorpack\"",\n\""issueId\"": 991,\n\""IssueComment\"": \""{{IssueComment.value}}\""\n}""
}
]
},
{
""name"": ""Close issue"",
""@type"": ""HttpPOST"",
""target"": ""https://api.github.com"",
""body"": ""{\n\""commandName\"": \""IssueClose\"",\n\""repositoryFullName\"": \""tensorpack/tensorpack\"",\n\""issueId\"": 991\n}""
},
{
""targets"": [
{
""os"": ""default"",
""uri"": ""https://github.com/tensorpack/tensorpack/issues/991#issuecomment-442146143""
}
],
""@type"": ""OpenUri"",
""name"": ""View on GitHub""
},
{
""name"": ""Unsubscribe"",
""@type"": ""HttpPOST"",
""target"": ""https://api.github.com"",
""body"": ""{\n\""commandName\"": \""MuteNotification\"",\n\""threadId\"": 416716672\n}""
}
],
""themeColor"": ""26292E""
}
]",solve signature mail master wrote option therefore may different different directly view mute thread publisher name entity title subtitle repository option may different different action name view issue context type type target name view issue description view issue publisher type organization name type context false originator title train faster text name add comment type true type id false name comment type target body name close issue type target body o default type name view name type target body,issue,positive,negative,neutral,neutral,negative,negative
442146143,"msgpack 0.5.6 has the `strict_types` option: https://github.com/msgpack/msgpack-python/blob/0.5.6/msgpack/_packer.pyx#L117-L119

Therefore you're not using msgpack 0.5.6. You may have different versions installed at different places.",option therefore may different different,issue,negative,neutral,neutral,neutral,neutral,neutral
442131311,"I'm sorry, Next time I will pay attention.
I use pip to install tensorpack, pip list includes msgpack (0.5.6) moudle.",sorry next time pay attention use pip install pip list,issue,negative,negative,negative,negative,negative,negative
442108073,Tensorflow cannot access your GPU. That is your own environment problem and others' solution is unlikely to be helpful to you.,access environment problem solution unlikely helpful,issue,negative,negative,negative,negative,negative,negative
442106809,"For anyone to better understand and diagnose your issue, please post relevant details following the issue template. See the first line of your issue.

Tensorpack requires:
https://github.com/tensorpack/tensorpack/blob/c4a68c6c5b4e6c7dc7b73014870edeee6e1cfe76/setup.py#L31
but you do not have it.


",anyone better understand diagnose issue please post relevant following issue template see first line issue,issue,positive,positive,positive,positive,positive,positive
442015060,"@xtanitfy 
I face the same error ,could you tell me how to solve it??thanks!",face error could tell solve thanks,issue,negative,positive,positive,positive,positive,positive
441614618,"Shufflenet is highly bound by the speed of data (because the compute cost is very small), therefore the speed is mainly determined by your CPU and disk, not GPUs. On my machine it's about 10~15 minutes every epoch.",highly bound speed data compute cost small therefore speed mainly determined disk machine every epoch,issue,negative,positive,neutral,neutral,positive,positive
441530084,The error is unrelated to tensorpack because `os.path.isdir` is a python function.,error unrelated python function,issue,negative,neutral,neutral,neutral,neutral,neutral
440924737,"Please always include entire logs.

It looks like it's an environment issue of your environment and your tensorflow cannot use your GPUs. You can debug by, e.g. runing the mnist examples and see if the GPU is used.",please always include entire like environment issue environment use see used,issue,positive,neutral,neutral,neutral,neutral,neutral
440492143,"> cudnn version is 9.0

9.0 is cuda version.

Made another run with TF 1.10, cuda 9.0, cudnn 7.1.1 and still able to reproduce results.
I recommend you to take a look at https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/index.html which does mention some bugs in certain cudnn versions.",version version made another run still able reproduce recommend take look mention certain,issue,positive,positive,positive,positive,positive,positive
439740262,"thanks, after many times of debugging , now I can be sure that the problem is not caused by faster-rcnn, the error log confuses me，the problem is the shape of our dataset，not somewhere else.
thanks a lot ,now it's not a faster-rcnn exalples issue",thanks many time sure problem error log problem shape somewhere else thanks lot issue,issue,negative,positive,positive,positive,positive,positive
439719791,Your code here https://github.com/stoensin/IC/blob/755fb3e8b2bb8698ad76e93c17bd5c6a6be20559/dense2p/dense2p.py#L421-L427 looks very wrong and please double check their shapes and dimensions. ,code wrong please double check,issue,negative,negative,negative,negative,negative,negative
439717880,"> but the only part of the network that uses GroupNorm is the code for faster r-cnn, but I have not made any changes to it

The error does come from faster r-cnn. However this does not imply that it is due to a problem of faster r-cnn.
As you can probably verify yourself (and you'd better do so in case this is a bug of tensorflow), that things will work if you train faster r-cnn with the above configurations.

Another option that may help is that perhaps you can distill a [MCVE](https://stackoverflow.com/help/mcve) that anyone can easily run and see the errors. It's almost impossible to debug something that may not be a tensorpack issue at all, just from the logs. ",part network code faster made error come faster however imply due problem faster probably verify better case bug work train faster another option may help perhaps distill anyone easily run see almost impossible something may issue,issue,negative,positive,neutral,neutral,positive,positive
439695061,"Thank you for your patience, I have rewritten the format of the issue.
The model I am working on now refactors the model I implemented with tensorflow. The original model is already working properly. For the part of image feature extraction, I will now use the faster r-cnn example, the other part from the original Some models are migrated;
The error that now appears looks like a dimension mismatch in GroupNorm, but the only part of the network that uses GroupNorm is the code for faster r-cnn, but I have not made any changes to it.",thank patience format issue model working model original model already working properly part image feature extraction use faster example part original error like dimension mismatch part network code faster made,issue,positive,positive,positive,positive,positive,positive
439671313,"> Could you provide your environment info (especially TF version and cudnn version)?

My TF version is 1.10.0 and the cudnn version is 9.0.",could provide environment especially version version version version,issue,negative,neutral,neutral,neutral,neutral,neutral
439668145,"Running the FasterRCNN example with the configuration you provided does not produce any errors in my test.

Since you're making nontrivial modifications to the code. If your modifications to the code causes errors, it's likely not a tensorpack issue and not what we'll help with.",running example configuration provided produce test since making code code likely issue help,issue,negative,neutral,neutral,neutral,neutral,neutral
439667788,"For anyone to be able to understand your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"").",anyone able understand issue please post relevant following issue template click new issue unexpected,issue,negative,positive,positive,positive,positive,positive
439663739,Just made the above changes to this PR. Let me know if you found anything is incorrect,made let know found anything incorrect,issue,negative,neutral,neutral,neutral,neutral,neutral
439662262,">  Isn't true positives inside crowd box are usually small? Like a person in a crowd of people?

Crowd is labeled as boxes. In coco the labeled boxes are sometimes unnecessarily big due to lazy labelers. The crowd box may as as result overlap meaningful objects, especially objects of a different category (e.g. a car in a crowd of people).

What you said about crowd boxes all make sense -- that's why there is crowd box handling code, though disabled. And they are disabled because there is importance in maintaining consistency with official code.

Therefore how about let's add your improvement but at the same time still keep things consistent by default.
Could you change the default `CROWD_OVERLAP_THRESH` to something like 1.0 or larger (since it's not used at all for now) and add a comment that this is by default disabled? ",true inside crowd box usually small like person crowd people crowd coco sometimes unnecessarily big due lazy crowd box may result overlap meaningful especially different category car crowd people said crowd make sense crowd box handling code though disabled disabled importance consistency official code therefore let add improvement time still keep consistent default could change default something like since used add comment default disabled,issue,positive,negative,neutral,neutral,negative,negative
439656816,"I think it is wrong not to handle crowd label even Detectron did not. The network would be penalized for doing true detection inside the crowd label region.
 
I have not tried to train on Coco but for my private dataset which includes about 3% crowd label, it improve about 2 points on overall mAP.

If it did not make a difference for your former training, is it possible that former code based on iou did not properly ignore anchors in crowd box region?
Another possible theory is the crowd region would be marked as bgs if not taken cared of. But later the code samples fgs and bgs and there are many bg labels so some mistakes in crowd label region is may be statistically not significant. 

I am curious why ignoring small anchors inside large crowd box is arguable? Isn't true positives inside crowd box are usually small? Like a person in a crowd of people?",think wrong handle crowd label even network would true detection inside crowd label region tried train coco private crowd label improve overall map make difference former training possible former code based properly ignore crowd box region another possible theory crowd region would marked taken later code many crowd label region may statistically significant curious small inside large crowd box arguable true inside crowd box usually small like person crowd people,issue,positive,positive,neutral,neutral,positive,positive
439652337,Then you'll need to modify the model to support arbitrary size (i.e. None size),need modify model support arbitrary size none size,issue,negative,negative,neutral,neutral,negative,negative
439634534,"This would work if all input images have the same size. In other words, LR_SIZE_H and LR_SIZE_W are different for each input image.",would work input size different input image,issue,negative,neutral,neutral,neutral,neutral,neutral
439533121,"You only have to initialize it once by:
```python
predict_func = OfflinePredictor(PredictConfig(
         model=Model(LR_SIZE_H, LR_SIZE_W),
         session_init=get_model_loader(model_path),
         input_names=['Ilr'],
         output_names=['prediction']))
```
in the code. Then just use `predict_func` as many times as you want. That's what I meant by writing it inside the exiting `apply` function.",initialize python code use many time want meant writing inside apply function,issue,negative,positive,positive,positive,positive,positive
439530064,Hi @ppwwyyxx thanks for your response. But what about the Model(...) constructed every time for each image with the image original resolution? Is there any way to have it initialized once?,hi thanks response model every time image image original resolution way,issue,positive,positive,positive,positive,positive,positive
439518567,"Do you still wish to add this functionality?

It's easier to just modify on top of the existing `apply` function. Make it take a list of `lowres` instead of a single one, and evaluate them one by one. Python's argparse can take a list of arguments. This way the usage of the script for single / multiple images will also be compatible.",still wish add functionality easier modify top apply function make take list instead single one evaluate one one python take list way usage script single multiple also compatible,issue,positive,positive,neutral,neutral,positive,positive
439479129,"As said in the code, we did not do this filtering because the official implementation (Detectron) did not do this.
And in practice I've found no differences.
Did you find any accuracy difference by using your filtering?

> For example, small anchors inside large crowd box would have small iou so not being ignored.

It's arguable whether small anchors inside large crowd box should be ignored.",said code filtering official implementation practice found find accuracy difference filtering example small inside large crowd box would small arguable whether small inside large crowd box,issue,negative,negative,neutral,neutral,negative,negative
438947739,"Again I don't know what you did so I don't know how I can help.
Since you made modifications on top of working code, it's easier for you to figure out what is causing training to get stuck.",know know help since made top working code easier figure causing training get stuck,issue,positive,positive,positive,positive,positive,positive
438946594,"this is a imagenet training task, and using tf.case to split the graph into 3 splits, like this:
```
def f1(x): return tf.identity(x+1)
r = case({tf.less(global_step, y): f1(3), tf.greater(global_step, z): f1(4)},
         default=f1(5), exclusive=True)
```
but the code just got stuck in :    
```
[1115 07:27:24 @sessinit.py:220] Restoring from dict ...
[1115 07:33:05 @base.py:240] Graph Finalized.
[1115 07:33:09 @concurrency.py:37] Starting EnqueueThread QueueInput/input_queue ...
[1115 07:33:09 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
[1115 07:33:37 @concurrency.py:37] Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...
[1115 07:33:37 @inference_runner.py:101] [InferenceRunner] Will eval 782 iterations
[1115 07:33:39 @base.py:272] Start Epoch 1 ...
  0%|                                                                                                                                                                                                                      |0/5000[00:00<?,?it/s][1115 07:33:39 @input_source.py:551] Pre-filling StagingArea ...
[1115 07:33:44 @input_source.py:555] 1 element was put into StagingArea on each tower.
```
@ppwwyyxx 
tensorpack version is 0.9.0.1",training task split graph like return case code got stuck graph starting running starting start epoch element put tower version,issue,negative,neutral,neutral,neutral,neutral,neutral
438906909,"Thanks! Wu,

I have fixed it, However, I don't find any method to tap the trained
weights. I want the values of train weights and biases. How to do that?


",thanks fixed however find method tap trained want train,issue,negative,positive,positive,positive,positive,positive
438905213,"@ppwwyyxx 
issue solved. Due to my tensorflow version too low.",issue due version low,issue,negative,negative,neutral,neutral,negative,negative
438902323,"#### yeah, i removed some 'reuse_variables' in my model, it's working, thanks a lot.
",yeah removed model working thanks lot,issue,positive,positive,positive,positive,positive,positive
438902211,"Added it as another rule at https://tensorpack.readthedocs.io/tutorial/trainer.html#rules-of-tower-function

> Do not modify the reuse option (e.g., by `scope.reuse_variables()`) of a variable
     scope that is not created by you. This affects other's code.",added another rule modify reuse option variable scope code,issue,negative,neutral,neutral,neutral,neutral,neutral
438900906,"Please format the post so that it's easier to read.
The error is 
```
ValueError: Input 0 of layer conv2d_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 40, 40]
```
which says the convolution layer needs 4D inputs but you're giving a 3D inputs. This is a bug in your graph code which you need to fix.",please format post easier read error input layer incompatible layer found full shape received none convolution layer need giving bug graph code need fix,issue,positive,positive,positive,positive,positive,positive
438900248,"I'm not familiar with any similar work that does SSD with our quantization. We've done semantic segmentation which seems to work fine.

Closing as this is not a tensorpack issue. I would in general recommend you to not quantize too many layers (some layers should not be quantized), and use pre-training on imagenet.",familiar similar work quantization done semantic segmentation work fine issue would general recommend quantize many use,issue,positive,positive,positive,positive,positive,positive
438899336,Do not use `reuse_variables()` in the root scope. It affects all other code. This sounds like the issue you're having.,use root scope code like issue,issue,negative,neutral,neutral,neutral,neutral,neutral
438898021,"Again if i want to change the dataset in svhn-digit.py code to MNIST and i change the code as following 

`#!/usr/bin/env python
# -*- coding: utf-8 -*-
# File: svhn-digit-dorefa.py
# Author: Yuxin Wu

import os
import argparse
import tensorflow as tf

from tensorpack import *
from tensorpack.tfutils.summary import add_moving_summary, add_param_summary
from tensorpack.dataflow import dataset
from tensorpack.tfutils.varreplace import remap_variables

from dorefa import get_dorefa

""""""
This is a tensorpack script for the SVHN results in paper:
DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients
http://arxiv.org/abs/1606.06160

The original experiements are performed on a proprietary framework.
This is our attempt to reproduce it on tensorpack.

Accuracy:
    With (W,A,G)=(1,1,4), can reach 3.1~3.2% error after 150 epochs.
    With (W,A,G)=(1,2,4), error is 3.0~3.1%.
    With (W,A,G)=(32,32,32), error is about 2.3%.

Speed:
    With quantization, 60 batch/s on 1 1080Ti. (4721 batch / epoch)

To Run:
    ./svhn-digit-dorefa.py --dorefa 1,2,4
""""""

BITW = 1
BITA = 2
BITG = 4


class Model(ModelDesc):
    def inputs(self):
        return [tf.placeholder(tf.float32, [None, 40, 40], 'input'),
                tf.placeholder(tf.int32, [None], 'label')]

    def build_graph(self, image, label):
        is_training = get_current_tower_context().is_training

        fw, fa, fg = get_dorefa(BITW, BITA, BITG)

        # monkey-patch tf.get_variable to apply fw
        def binarize_weight(v):
            name = v.op.name
            # don't binarize first and last layer
            if not name.endswith('W') or 'conv0' in name or 'fc' in name:
                return v
            else:
                logger.info(""Binarizing weight {}"".format(v.op.name))
                return fw(v)

        def nonlin(x):
            if BITA == 32:
                return tf.nn.relu(x)
            return tf.clip_by_value(x, 0.0, 1.0)

        def activate(x):
            return fa(nonlin(x))

        image = image / 256.0

        with remap_variables(binarize_weight), \
                argscope(BatchNorm, momentum=0.9, epsilon=1e-4), \
                argscope(Conv2D, use_bias=False):
            logits = (LinearWrap(image)
                      .Conv2D('conv0', 48, 5, padding='VALID', use_bias=True)
                      .MaxPooling('pool0', 2, padding='SAME')
                      .apply(activate)
                      # 18
                      .Conv2D('conv1', 64, 3, padding='SAME')
                      .apply(fg)
                      .BatchNorm('bn1').apply(activate)

                      .Conv2D('conv2', 64, 3, padding='SAME')
                      .apply(fg)
                      .BatchNorm('bn2')
                      .MaxPooling('pool1', 2, padding='SAME')
                      .apply(activate)
                      # 9
                      .Conv2D('conv3', 128, 3, padding='VALID')
                      .apply(fg)
                      .BatchNorm('bn3').apply(activate)
                      # 7

                      .Conv2D('conv4', 128, 3, padding='SAME')
                      .apply(fg)
                      .BatchNorm('bn4').apply(activate)

                      .Conv2D('conv5', 128, 3, padding='VALID')
                      .apply(fg)
                      .BatchNorm('bn5').apply(activate)
                      # 5
                      .tf.nn.dropout(0.5 if is_training else 1.0)
                      .Conv2D('conv6', 512, 5, padding='VALID')
                      .apply(fg).BatchNorm('bn6')
                      .apply(nonlin)
                      .FullyConnected('fc1', 10)())
        tf.nn.softmax(logits, name='output')

        # compute the number of failed samples
        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='wrong_tensor')
        # monitor training error
        add_moving_summary(tf.reduce_mean(wrong, name='train_error'))

        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
        cost = tf.reduce_mean(cost, name='cross_entropy_loss')
        # weight decay on all W of fc layers
        wd_cost = regularize_cost('fc.*/W', l2_regularizer(1e-7))

        add_param_summary(('.*/W', ['histogram', 'rms']))
        total_cost = tf.add_n([cost, wd_cost], name='cost')
        add_moving_summary(cost, wd_cost, total_cost)
        return total_cost

    def optimizer(self):
        lr = tf.train.exponential_decay(
            learning_rate=1e-3,
            global_step=get_global_step_var(),
            decay_steps=4721 * 100,
            decay_rate=0.5, staircase=True, name='learning_rate')
        tf.summary.scalar('lr', lr)
        return tf.train.AdamOptimizer(lr, epsilon=1e-5)


# def get_config():
#     logger.set_logger_dir(os.path.join('train_log', 'svhn-dorefa-{}'.format(args.dorefa)))


def get_config():
    logger.set_logger_dir(os.path.join('train_log', 'svhn-MNIST-{}'.format(args.mnist)))

    # prepare dataset
    # d1 = dataset.SVHNDigit('train')
    # d2 = dataset.SVHNDigit('extra')
    # data_train = RandomMixData([d1, d2])
    # data_test = dataset.SVHNDigit('test')
    d1 = dataset.Mnist('train')
    #d2 = dataset.SVHNDigit('extra')
    data_train = d1 #RandomMixData([d1, d2])
    data_test = dataset.Mnist('test')

    augmentors = [
        imgaug.Resize((40, 40)),
        imgaug.Brightness(30),
        imgaug.Contrast((0.5, 1.5)),
    ]
    data_train = AugmentImageComponent(data_train, augmentors)
    data_train = BatchData(data_train, 128)
    data_train = PrefetchDataZMQ(data_train, 5)

    augmentors = [imgaug.Resize((40, 40))]
    data_test = AugmentImageComponent(data_test, augmentors)
    data_test = BatchData(data_test, 128, remainder=True)

    return TrainConfig(
        data=QueueInput(data_train),
        callbacks=[
            ModelSaver(),
            InferenceRunner(data_test,
                            [ScalarStats('cost'), ClassificationError('wrong_tensor')])
        ],
        model=Model(),
        max_epoch=20,
    )


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--dorefa',
                        help='number of bits for W,A,G, separated by comma. Defaults to \'1,2,4\'',
                        default='1,2,4')
    args = parser.parse_args()

    BITW, BITA, BITG = map(int, args.dorefa.split(','))
    config = get_config()
    launch_train_with_config(config, SimpleTrainer())
`

the Error is like this

`[1115 02:14:47 @logger.py:108] WRN Log directory train_log/svhn-dorefa-1,2,4 exists! Use 'd' to delete it. 
[1115 02:14:47 @logger.py:111] WRN If you're resuming from a previous run, you can choose to keep it.
Press any other key to exit. 
Select Action: k (keep) / d (delete) / q (quit):d
[1115 02:14:58 @logger.py:73] Argv: gdrive/My Drive/DoReFa-Net/svhn-digit-dorefa.py --dorefa 1,2,4
[1115 02:14:58 @fs.py:100] WRN Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.
[1115 02:14:58 @parallel.py:293] [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[1115 02:14:58 @input_source.py:220] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[1115 02:14:59 @trainers.py:52] Building graph for a single training tower ...
[1115 02:14:59 @registry.py:121] conv0 input: [None, 40, 40]
Traceback (most recent call last):
  File ""gdrive/My Drive/DoReFa-Net/svhn-digit-dorefa.py"", line 182, in <module>
    launch_train_with_config(config, SimpleTrainer())
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/train/interface.py"", line 87, in launch_train_with_config
    model._build_graph_get_cost, model.get_optimizer)
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/utils/argtools.py"", line 176, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/train/tower.py"", line 204, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/train/trainers.py"", line 54, in _setup_graph
    grads = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)()
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/train/tower.py"", line 232, in get_grad_fn
    cost = get_cost_fn(*input.get_input_tensors())
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/tfutils/tower.py"", line 284, in __call__
    output = self._tower_fn(*args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/graph_builder/model_desc.py"", line 246, in _build_graph_get_cost
    ret = self.build_graph(*inputs)
  File ""gdrive/My Drive/DoReFa-Net/svhn-digit-dorefa.py"", line 76, in build_graph
    .Conv2D('conv0', 48, 5, padding='VALID', use_bias=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/models/linearwrap.py"", line 47, in layer_func
    ret = layer(name, self._t, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/models/registry.py"", line 124, in wrapped_func
    outputs = func(*args, **actual_args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/models/tflayer.py"", line 66, in decorated_func
    return func(inputs, **ret)
  File ""/usr/local/lib/python3.6/dist-packages/tensorpack/models/conv2d.py"", line 68, in Conv2D
    ret = layer.apply(inputs, scope=tf.get_variable_scope())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 817, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py"", line 374, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 730, in __call__
    self._assert_input_compatibility(inputs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 1477, in _assert_input_compatibility
    str(x.shape.as_list()))
ValueError: Input 0 of layer conv2d_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 40, 40]`

I dont know where to change so that i can feed [None, 40 40]",want change code change code following python file author import o import import import import import import import script paper training low convolutional neural low original proprietary framework attempt reproduce accuracy reach error error error speed quantization ti batch epoch run class model self return none none self image label fa apply name first last layer name name return else weight return return return activate return fa image image image activate activate activate activate activate activate else compute number wrong label monitor training error wrong cost cost cost weight decay cost cost return self return prepare return parser comma map error like log directory use delete previous run choose keep press key exit select action keep delete quit set fork one time setting queue building graph single training tower input none recent call last file line module file line file line wrapper return file line input file line input file line cost file line output file line ret file line file line ret layer name file line file line return ret file line ret file line apply return file line super layer self file line file line input layer incompatible layer found full shape received none dont know change feed none,issue,negative,positive,neutral,neutral,positive,positive
438897852,"> In this implementation, quantized operations are all performed through tf.float32. They don't make your network faster.

From the readme",implementation make network faster,issue,negative,neutral,neutral,neutral,neutral,neutral
438897473,I do not know what you did. I suppose you'll need to learn more about variable scopes and tf.AUTO_REUSE can solve issues like this.,know suppose need learn variable solve like,issue,positive,neutral,neutral,neutral,neutral,neutral
438897086,"Thanks! Wu, that means you are using standard convolution method instead of
bitwise operation?

",thanks standard convolution method instead bitwise operation,issue,negative,positive,neutral,neutral,positive,positive
438889529,"@ppwwyyxx 
and if I use two tf.cond function, the error is like:
```
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 503, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 443, in __call__
    self.build(input_shapes[0])
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py"", line 189, in build
    trainable=True)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 383, in add_variable
    trainable=trainable and self.trainable)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1065, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 962, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 360, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack-0.8.6-py2.7.egg/tensorpack/models/tflayer.py"", line 86, in custom_getter
    return getter(name, *args, **kwargs)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 352, in _true_getter
    use_resource=use_resource)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 682, in _get_single_variable
    ""VarScope?"" % name)
ValueError: Variable conv0/bn/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
```
I have already use` tf.get_variable_scope().reuse_variables() ` in the code to avoid error on `conv0/W:0`, **but this error on `conv0/bn/beta` seems to be in the source code of tensorpack, because it uses the BNReLU function in tensorpack** ",use two function error like file line apply return file line file line build file line file line file line file line file line return getter name file line file line name variable exist mean set already use code avoid error error source code function,issue,negative,negative,negative,negative,negative,negative
438887792,"@ppwwyyxx 
I tried to change the `tf.cond `function to `tf.case` function to get a 3-pipeline graph like this:
```
def f1(x): return tf.identity(x+1)
r = case({tf.less(global_step, y): f1(3), tf.greater(global_step, z): f1(4)},
         default=f1(5), exclusive=True)
```
but the code is keeping giveing this error:
```
  File ""imagenet.py"", line 78, in get_logits
    group_func, self.block_func, self.qw, self.pretrained_weights)
  File ""/data/projects/resnet_model.py"", line 155, in resnet_backbone
    .Conv2DQuant('conv0', 64, 7, stride=2, nl=BNReLUQuant, is_quant=True)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack-0.8.6-py2.7.egg/tensorpack/models/linearwrap.py"", line 47, in layer_func
    ret = layer(name, self._t, *args, **kwargs)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack-0.8.6-py2.7.egg/tensorpack/models/registry.py"", line 124, in wrapped_func
    outputs = func(*args, **actual_args)
  File ""/data/learned_quantization.py"", line 549, in Conv2DQuant
    quantized_weight = QuantizedWeight('weight_quant', kernel_in, n, nbit=nbit,pretrained_weights=pretrained_weights)
  File ""/data/learned_quantization.py"", line 477, in QuantizedWeight
    default=Son_QuantizedWeight(name,x, n,nbit=32,pretrained_weights=pretrained_weights),exclusive=False)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3074, in case
    raise TypeError(""fn for pred %s must be callable."" % pred.name)
TypeError: fn for pred tower0/conv0/Greater:0 must be callable.

```

",tried change function function get graph like return case code keeping error file line file line file line ret layer name file line file line file line name file line case raise must callable must callable,issue,negative,neutral,neutral,neutral,neutral,neutral
438743833,Could you provide your environment info (especially TF version and cudnn version)?,could provide environment especially version version,issue,negative,neutral,neutral,neutral,neutral,neutral
437567895,"Your code has a large amount of duplication with the existing code, and is very inefficient because it builds a predictor for each image. Therefore it cannot be accepted.",code large amount duplication code inefficient predictor image therefore accepted,issue,negative,positive,positive,positive,positive,positive
437431539,Closing as there is no update. Feel free to reopen if you have more information to share.,update feel free reopen information share,issue,positive,positive,positive,positive,positive,positive
437431345,Feel free to reopen if you have other information to share.,feel free reopen information share,issue,positive,positive,positive,positive,positive,positive
437266147,"If I understand your questions correctly,
`round((2^k-1) r_i)` is a k-bit fixed-point integer.
`r_o` is a k-bit fixed point number, but not an integer. 
But because `r_o * (2^k-1)` is fixed-point integer, the dot product can still be done with integer convolution kernel because you can ignore a coefficient and apply it back later.",understand correctly round integer fixed point number integer integer dot product still done integer convolution kernel ignore coefficient apply back later,issue,negative,negative,neutral,neutral,negative,negative
436991562,"As the docs said: https://www.tensorflow.org/api_docs/python/tf/contrib/summary, the mechanism of writing summary is different between `tf.summary` and `tf.contrib.summary`. 
The `tf.contrib.summary` mechanism is not included in tensorpack and will need a new callback to do so.",said mechanism writing summary different mechanism included need new,issue,negative,positive,neutral,neutral,positive,positive
436926392,"@ppwwyyxx 
I will give it a try after the current experiment ends.
Besides, If I replace `tf.summary` with` tf.contrib.summary`, no error occurs, but after every epoch, the printed information actually does not contain the summary value I have included in the `tf.contrib.summary`. Why this happens?",give try current experiment besides replace error every epoch printed information actually contain summary value included,issue,negative,neutral,neutral,neutral,neutral,neutral
436912333,"Yes. Both weights will be saved.

Could you also try cond_v2 following the suggestions in https://github.com/tensorflow/tensorflow/issues/15874#issuecomment-436833266. I don't know if your issues is similar to that one but it's worth a try. Cond_v2 is available only in very recent tensorflow versions. ",yes saved could also try following know similar one worth try available recent,issue,positive,positive,positive,positive,positive,positive
436910947,"@ppwwyyxx 
I solved this issue by either removing tf.smmary or replace tf.summary with tf.contrib.summary",issue either removing replace,issue,negative,neutral,neutral,neutral,neutral,neutral
436899073,"@ppwwyyxx 
1. 
in your  example code:
```
+        def f(x):
+            return tf.cond(get_global_step_var() < 100,
+                           lambda: Conv2D('convtrue', x, filters=64),
+                           lambda: Conv2D('convfalse', x, filters=64))
```
if global_step<100, the computation graph contains the layer 'convture', else  it contains the layer 'convfalse'
. So in the end,  will the computation graph be a little weird? Both the weight of convtrue and convfalse will be saved?

2. Besides, if this is an issue related to summary, if I convert to a higher tensorflow version (currently 1.3.0), will this issue be also solved, without removing the tf.summary operation? If converting to a higher tensorflow version does not work, will exist any other valid solutions, except removing summary operation?

",example code return lambda lambda computation graph layer else layer end computation graph little weird weight saved besides issue related summary convert higher version currently issue also without removing operation converting higher version work exist valid except removing summary operation,issue,negative,negative,neutral,neutral,negative,negative
436722944,"Closing as a duplicate of #977 
A complete bug report can often easily lead to the solution.",duplicate complete bug report often easily lead solution,issue,positive,positive,positive,positive,positive,positive
436718374,Looks like an issue with msgpack-numpy. Could you downgrade msgpack-numpy from 0.4.4.1 to 0.4.3.1 and retry?,like issue could downgrade retry,issue,negative,neutral,neutral,neutral,neutral,neutral
436564118,You may be using the examples from the master branch but the tensorpack library from an older commit (which you may have previously installed).,may master branch library older commit may previously,issue,negative,neutral,neutral,neutral,neutral,neutral
436562984,"Thanks for your fast response!
I am on a master branch actually:
```
root@tower:~/a/tensorpack# git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
```
will try older examples now",thanks fast response master branch actually root tower git master already branch try older,issue,negative,positive,positive,positive,positive,positive
436560229,"It was added yesterday. Please upgrade to github's master branch. Alternatively, you can use the examples from an older commit.",added yesterday please upgrade master branch alternatively use older commit,issue,positive,positive,positive,positive,positive,positive
436536462,"@ppwwyyxx 
It worked! Thank you very much ~
I have made a mistake on this `is_training` param. After modified it to `get_current_tower_context().is_training` in `build_graph`, everything's back to normal.",worked thank much made mistake param everything back normal,issue,negative,positive,positive,positive,positive,positive
436259175,"Since you know the `is_training` parameter is the difference, why would you assume that the `is_training` parameter does not matter?

In tensorpack's tower function, use `get_current_tower_context().is_training` to get the correct mode: https://tensorpack.readthedocs.io/tutorial/trainer.html#what-you-can-do-inside-tower-function
",since know parameter difference would assume parameter matter tower function use get correct mode,issue,negative,neutral,neutral,neutral,neutral,neutral
436256867,"Tensorpack will not summarize anything in the graph if you did not add things by `tf.summary` or other tensorpack summary functions in the graph.

To avoid the summary callbacks, see documentation about `extra_callbacks`: https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.TrainConfig",summarize anything graph add summary graph avoid summary see documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
436256137,"@ppwwyyxx 
Besides, If I simply remove my own `tf.summary `operation, it seems that the tensorpack itself will trigger `tf.summary` after each epoch, is this true?",besides simply remove operation trigger epoch true,issue,negative,positive,positive,positive,positive,positive
436255208,"this is my callbacks:
```
callbacks = [PeriodicTrigger(ModelSaver(), every_k_epochs=15)
]
```
So I think the problem comes from the tf.summary callback , which is  a default tensorpack operation after each epoch. In the function `Son_QuantizedActive ` there is tf.summary operation, indeed. So how can I avoid this issue? Any advice?
@ppwwyyxx ",think problem come default operation epoch function operation indeed avoid issue advice,issue,negative,neutral,neutral,neutral,neutral,neutral
436253261,"This is likely a tensorflow bug. If you cannot provide a minimal reproducible code, there isn't much I can do. I cannot reproduce the same issue by making some simple changes to an existing example. For example, this works fine:
```diff
diff --git i/examples/basics/cifar-convnet.py w/examples/basics/cifar-convnet.py
index 81e59e2c..cb482fc7 100755
--- i/examples/basics/cifar-convnet.py
+++ w/examples/basics/cifar-convnet.py
@@ -43,12 +43,18 @@ class Model(ModelDesc):
         else:
             data_format = 'channels_last'
 
+        def f(x):
+            return tf.cond(get_global_step_var() < 100,
+                           lambda: Conv2D('convtrue', x, filters=64),
+                           lambda: Conv2D('convfalse', x, filters=64))
+
         image = image / 4.0     # just to make range smaller
         with argscope(Conv2D, activation=BNReLU, use_bias=False, kernel_size=3), \
                 argscope([Conv2D, MaxPooling, BatchNorm], data_format=data_format):
             logits = LinearWrap(image) \
                 .Conv2D('conv1.1', filters=64) \
                 .Conv2D('conv1.2', filters=64) \
+                .apply(f) \
                 .MaxPooling('pool1', 3, stride=2, padding='SAME') \
                 .Conv2D('conv2.1', filters=128) \
                 .Conv2D('conv2.2', filters=128) \
```

From the log, the error happened at the last iteration of the epoch (not at inference). Maybe you have some callbacks that are enabled at the last iteration of the epoch. 

By default the [summary callback](https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.MergeAllSummaries) is enabled at the last iteration of the epoch. And in tensorflow, summary ops cannot be used inside `tf.cond`. This may also cause your issues.",likely bug provide minimal reproducible code much reproduce issue making simple example example work fine git index class model else return lambda lambda image image make range smaller image log error last iteration epoch inference maybe last iteration epoch default summary last iteration epoch summary used inside may also cause,issue,negative,positive,neutral,neutral,positive,positive
436251595,"@ppwwyyxx 
Thanks for your response~
My preprocessing and postprocessing codes between `eval_callback` and `offlinepredictor` are almost the same. The only difference is the `batchnorm` param `is_training`: 

In the training scripts:
```python
    model_config = model_factory.get_default_config(args.model_name)
    model_config.update({
        'lr': args.lr,
        'weight_decay': args.weight_decay,
        'backbone_name': 'mobilefacenet',
        'backbone_config':{'weight_decay': args.weight_decay, 'is_training': True},
        'image_shape': args.img_size,
        'train_stage': args.train_stage,
        'num_classes': args.num_classes,
        })
    model = model_factory.get_model(args.model_name, model_config)
```
While in the offlinepredictor scripts:
```python
    model_config = model_factory.get_default_config('arcface_model')
    model_config.update({
            'lr': 0.001,
            'weight_decay': 0.00004,
            'backbone_name': args.net_name,
            'backbone_config':{'weight_decay': 0.00004, 'is_training': False},
            'image_shape': img_size,
            'train_stage': 'softmax',
            'num_classes': 50841,
        })
    model = model_factory.get_model('arcface_model', model_config)
    
    test_dataset, num_samples = FaceRecognDataflow.get_dataflow(args.data_dir, img_size, batch_size, is_training=False)
    
    test_predictor = SimpleDatasetPredictor(config=PredictConfig(
        model=model,
        session_init=get_model_loader(os.path.join(args.model_dir, args.model_prefix)),
        input_names=['input'],
        output_names=['embedding']
        ),
        dataset=test_dataset)
```
And I am using `tf.contrib.layers.*` functions to build the network like this way:
```python
    self.batch_norm_param = {
                'is_training': is_training,
                'scale':True,
                'center':True,
                'epsilon':0.001,
                'decay':0.995,
                'updates_collections':None,
                'variables_collections': [tf.GraphKeys.TRAINABLE_VARIABLES],
                }
    net = tf.contrib.layers.conv2d(
                inp,
                num_outputs=64,
                kernel_size=3,
                stride=2,
                biases_initializer=None,
                activation_fn=functools.partial(self.activation_fn, name='{}_{}'.format(self.net_name, layer_count)),
                normalizer_fn=tf.contrib.layers.batch_norm,
                normalizer_params=self.batch_norm_param,
                weights_regularizer=self.regularizer, 
                weights_initializer=self.initializer,
                scope=layer_name 
                )
    net = ...
```
And I strill get different results between `eval_callback` and `offlinepredictor`
",thanks almost difference param training python true model python false model build network like way python true true none net net get different,issue,positive,positive,positive,positive,positive,positive
436250811,There is a bug in the first commit that was fixed by a later commit.,bug first commit fixed later commit,issue,positive,positive,positive,positive,positive,positive
436249755,"@PacteraOliver hello, i have the same problem now, and solve it thanks to your advice. But i can't understand, do u know why it need root? Thanks~",hello problem solve thanks advice ca understand know need root,issue,negative,positive,positive,positive,positive,positive
436246699,Thanks for the quick fix! Works for me now,thanks quick fix work,issue,negative,positive,positive,positive,positive,positive
436245051,"I wrote a PR, but get `remote: Permission to tensorpack/tensorpack.git denied to sytham`.

The fix is (tested, works for me)
```
if validation_data is not None:
            callbacks.insert(0, InferenceRunner(
                validation_data, ScalarStats(self._stats_to_inference)))
```

",wrote get remote permission fix tested work none,issue,negative,negative,neutral,neutral,negative,negative
436244827,"@ppwwyyxx 
I have pulled a new issue about this problem , related to tensorpack.  https://github.com/tensorpack/tensorpack/issues/974",new issue problem related,issue,negative,positive,neutral,neutral,positive,positive
436211758,You're using tensorpack correctly. Maybe check whether you're using the same preprocessing or postprocessing between the callback and the offline evaluation?,correctly maybe check whether evaluation,issue,negative,neutral,neutral,neutral,neutral,neutral
436193235,"After the above fix, your code can now print the ""successfully cleaned-up"" message before the sleep finished.",fix code print successfully message sleep finished,issue,negative,positive,positive,positive,positive,positive
436186141,"Ah, I see. You're saying if it appears after sleep it's just because the whole kernel is shutting down.
It appears after sleep.",ah see saying sleep whole kernel shutting sleep,issue,negative,positive,positive,positive,positive,positive
436184101,"Do you see the message before sleep or after sleep? It should appear before the sleep, otherwise it's still a bug.",see message sleep sleep appear sleep otherwise still bug,issue,negative,neutral,neutral,neutral,neutral,neutral
436172406,"I think I've found the problem. I'm running everything from a Jupyter notebook. When I run the above minimal example from script, I do see the ""successfully cleaned-up"" message.

I understand this has now basically ceased to be your problem :) but do you have any suggestions for alternative solutions that don't involve not using a notebook? Could there be another way of shutting down the processes without using `del`? I can move large parts of code to library/script, but our users mostly use notebooks to set up training runs, especially for collecting the right dataset and configuring the model.",think found problem running everything notebook run minimal example script see successfully message understand basically problem alternative involve notebook could another way shutting without move large code mostly use set training especially right model,issue,negative,positive,positive,positive,positive,positive
436164644,Thanks for the short repro! There might be a bug somewhere that mistakenly keeps reference to the dataflow. I'll take a look at that!,thanks short might bug somewhere mistakenly reference take look,issue,negative,positive,neutral,neutral,positive,positive
436160615,"Thanks, I do indeed see the ""successfully cleaned-up"" message in your example.
However, expanding your example to the minimal one that represents my use case, I don't see it:
```python
import gc
from tensorpack import *
from tensorpack.contrib.keras import KerasModel
import tensorflow as tf
from tensorflow import keras
KL = keras.layers

def create_dataflow():
    x = FakeData([[3], [1]], 1000)
    x = BatchData(x, 3)
    x = PrefetchDataZMQ(x, 3)
    return x

def model_func(x):
    M = keras.models.Sequential()
    M.add(KL.InputLayer(input_tensor=x))
    M.add(KL.Dense(1))
    return M

x = create_dataflow()
Q = QueueInput(x)
M = KerasModel(
        model_func,
        inputs_desc=[InputDesc(tf.float32, [None, 3], 'inputs')],
        targets_desc=[InputDesc(tf.float32, [None, 1], 'labels')],
        input=Q)
M.compile(optimizer=tf.train.AdamOptimizer(), loss='binary_crossentropy')
M.fit(steps_per_epoch=len(x), max_epoch=2)

del M
del Q
del x
gc.collect()

import time; time.sleep(10)
```
I'm not sure what else I should delete here?",thanks indeed see successfully message example however expanding example minimal one use case see python import import import import import return return none none import time sure else delete,issue,positive,positive,positive,positive,positive,positive
436156134,These questions you've been asking are unrelated to @minhson 's issues (in fact this one is unrelated to tensorpack either). Please use a new issue thread for different questions.,unrelated fact one unrelated either please use new issue thread different,issue,negative,positive,neutral,neutral,positive,positive
436148733,"@ppwwyyxx 
thanks for your advice.
Actually, I want to do this:
after **every 2 epoches**, I add my newly defined operation on one layer. So in the 1 to 2 epoch, only the first layer is added this operation, in the 3 to 4 epoch, the first and the second layer is added this operation. etc. until all the layers are added this operation. So I need to include the global_step value into the layers's definition (for example resnet). So did you have any advice on how to use this global_step_var tensor?",thanks advice actually want every add newly defined operation one layer epoch first layer added operation epoch first second layer added operation added operation need include value definition example advice use tensor,issue,positive,positive,positive,positive,positive,positive
436145378,"Yes.
Also you cannot use `if step == 100` in graph definition because when you define the graph, there is no value and the comparison is impossible. You need to use `tf.cond` and other related functions.
I recommend you to read some tutorials about the concept of tensorflow graph. ",yes also use step graph definition define graph value comparison impossible need use related recommend read concept graph,issue,positive,negative,negative,negative,negative,negative
436144444,"@ppwwyyxx 
If so, if I am using `get_global_step_var` **in a newly defined layer function**, 
 after the training has started, **the value** of this tensor will keep changing with the training process, right? And if I am using this tensor as the parameter of this function, it is valid, right?",newly defined layer function training value tensor keep training process right tensor parameter function valid right,issue,negative,positive,positive,positive,positive,positive
436143522,You cannot use `get_global_step_value` when you define a graph because the training has not started and there is no value. You can only use `get_global_step_var`,use define graph training value use,issue,negative,neutral,neutral,neutral,neutral,neutral
436143199,"@ppwwyyxx 
so given an imagenet-resnet training task, here :https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py
, in this python script, where can I get the global_step_value, and feed this value as a parameter to the definition of a layer definition , for example ,feed it as a parameter of this function:https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/resnet_model.py#L39
or a newly defined layer function.",given training task python script get feed value parameter definition layer definition example feed parameter function newly defined layer function,issue,negative,positive,positive,positive,positive,positive
436142106,"You cannot use ""value"" of tensor in a layer. Inside a layer you're defining the graph therefore you have to use symbolic tensor. When you define the graph there is no actual value of global step at all.",use value tensor layer inside layer graph therefore use symbolic tensor define graph actual value global step,issue,positive,neutral,neutral,neutral,neutral,neutral
436139224,"@ppwwyyxx 
I am using tensorpack-0.8.6, and when I run `global_step = get_global_step_value()`, the code gives me an error:
```
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack-0.8.6-py2.7.egg/tensorpack/tfutils/common.py"", line 76, in get_global_step_value
    get_global_step_var())
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/training_util.py"", line 58, in global_step
    return int(sess.run(global_step_tensor))
AttributeError: 'NoneType' object has no attribute 'run'
```
I just use it in a newly defined layer function.
Is there any method that can make me directly use this global_step value in a newly defined layer function? In the standard tensorpack-imagenet training code, **I can not find where the tensorflow-session is. How to use this  `get_global_step_value()` function?**",run code error file line file line return object attribute use newly defined layer function method make directly use value newly defined layer function standard training code find use function,issue,negative,positive,neutral,neutral,positive,positive
436127292,"Yes. You can use overwrite the train_op, as mentioned in https://tensorpack.readthedocs.io/tutorial/extend/trainer.html#write-a-trainer. You can also take a look at how the `GANTrainer` is implemented at https://github.com/tensorpack/tensorpack/blob/master/examples/GAN/GAN.py.

However, writing a custom trainer means you'll have to reimplement multi-gpu data-parallel logic.
Instead, if you only want to make modifications to the gradient computation process or the computed gradients, it's better to use `tf.custom_gradient` in your graph, or use a different (possibly a custom) optimizer (e.g., https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.optimizer.apply_grad_processors) so that the existing code 
```
grads = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)()
```
continues to work.",yes use overwrite also take look however writing custom trainer logic instead want make gradient computation process better use graph use different possibly custom code input work,issue,positive,positive,positive,positive,positive,positive
436126206,https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.get_global_step_var if you want to use it in the graph and https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.get_global_step_value if you want to use it in python.,want use graph want use python,issue,negative,neutral,neutral,neutral,neutral,neutral
436124947,"Here is the log 4 days ago that reproduced the experiments, with no modification to the code.
[log.log](https://github.com/tensorpack/tensorpack/files/2551563/log.log)

I'm using TF 1.11, cuda 9.0.176, cudnn 7.2.1
Are you using newer version of TF?",log day ago modification code version,issue,negative,neutral,neutral,neutral,neutral,neutral
436114656,"> The original models were trained without modifications to the code and on 8 GPUs. Since you've made changes to the code, getting different results is not surprising.

Hi, I conduct the experiments with the original code on 8 GPUs. The top-1 accuracy is still 67.5% (3% lower than reported). ",original trained without code since made code getting different surprising hi conduct original code accuracy still lower,issue,positive,positive,positive,positive,positive,positive
436111467,"@ppwwyyxx 
what I need to do is get the current global_step by `tf.train.get_or_create_global_step` and manipulate the graph variables according to the current global_steps., for example, 
```
if global_step==100, 
variable_1= variable_1 * 2, 
if global_step==200, 
variable_1=variable_1 + variable_2
if global_step==300, 
variable_1=variable_1 * variable_3
```
**(every 100 steps,** **the operations are not the same**) **But how to get the exact value of global_step in the code, and use this value instantly?**",need get current manipulate graph according current example every get exact value code use value instantly,issue,positive,positive,neutral,neutral,positive,positive
435999437,Just add the name of the cost tensor to `output_names`.,add name cost tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
435959574,"`del` does free the dataflow. And you can see ""successfully cleaned-up"" if you run the following code:
```python
from tensorpack import *

def create():
    x = FakeData([[3,3,3,3], [2]], 1000)
    x = BatchData(x, 3)
    x = PrefetchDataZMQ(x, 3)
    return x

x = create()
x.reset_state()
for idx, dp in enumerate(x):
    print(idx)
del x

import time; time.sleep(10)
```

So I assume there are other objects in your code that holds a reference to the dataflow so it never gets cleaned.",free see successfully run following code python import create return create enumerate print import time assume code reference never,issue,positive,positive,positive,positive,positive,positive
435938757,"Thank you for the quick response and for pointing me to [the dist-strategy rfc](https://github.com/tensorflow/community/blob/dab4be7e8e17e8ebb9ffee949e9aa4834c9aecbc/rfcs/20181016-replicator.md).

Since the dataflow is already independent, it seems callbacks are the place to focus. Perhaps they could be framework agnostic as well -- with implementations on top of [Keras callback](https://keras.io/callbacks/) and [Pytorch ignite handlers](https://pytorch.org/ignite/handlers.html).

After I finish the project I am working on using Tensorpack, I can try to adapt the training loop / hooks I use to Keras callback and see if there are any gaps.",thank quick response pointing since already independent place focus perhaps could framework agnostic well top ignite finish project working try adapt training loop use see,issue,positive,positive,positive,positive,positive,positive
435920726,"Hi, thanks for your thoughts! In general I doubt google will seriously consider significant TF2 RFCs from outside google, partially because someone is going to be responsible for implementing and maintaining it. 
About the features you talk about:
1. DataFlow is pure-python, framework-agnostic. I even tried to split it to a separate project (but was too lazy to do so). It is not very reasonable to integrate it into TF since it's not strongly tied to any parts of TF. Just think of DataFlow as a third-party library, what you need is just a good way to connect it to TF, (which may be `tf.data.Dataset.from_generator`?).

2. TF2 plans to have dist-strategy work naturally with Keras. If things goes well, this will do an equivalent work of tensorpack trainers, and make Keras models train faster.

3. I actually don't know. I'm not very familiar with Keras callback API. I think the main thing to care is ""what you can do"" inside a callback. For tensorpack this was roughly documented [here](https://tensorpack.readthedocs.io/tutorial/extend/callback.html#what-you-can-do-in-the-callback). If all these can be done in Keras callbacks, then there should be no issues converting among them ",hi thanks general doubt seriously consider significant outside partially someone going responsible talk even tried split separate project lazy reasonable integrate since strongly tied think library need good way connect may work naturally go well equivalent work make train faster actually know familiar think main thing care inside roughly done converting among,issue,positive,positive,positive,positive,positive,positive
435854570,Probably not a big deal if your model's performance does not variate too much epoch to epoch :-),probably big deal model performance variate much epoch epoch,issue,negative,positive,neutral,neutral,positive,positive
435822498,"Nice catch. This should be conveyed more clearly in the docs, and detected automatically.",nice catch clearly automatically,issue,positive,positive,positive,positive,positive,positive
435811307,"(1), (2), (3):
https://tensorpack.readthedocs.io/tutorial/save-load.html#transfer-learning
As mentioned in the docs, you only need to change the name in your code and don't need to do anything to the checkpoint, (or you can change the checkpoint as well). Missing tensor will be printed as warning and will not be loaded.
On the other hand, `same-name-but-different-shape` tensors will become an error.",need change name code need anything change well missing tensor printed warning loaded hand become error,issue,negative,negative,negative,negative,negative,negative
435808071,The `global_step` is saved in the checkpoint. You can use the `ignore` option when you load a model: https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.sessinit.SaverRestore,saved use ignore option load model,issue,negative,neutral,neutral,neutral,neutral,neutral
435806673,"One more!  Please see `tensorpack/scripts/checkpoint-manipulate.py`. There could be a bug?

In line 7 & 19--24,
```
from tensorpack.tfutils.varmanip import dump_chkpt_vars
...
    if args.model.endswith('.npy'):
        params = np.load(args.model, encoding='latin1').item()
    elif args.model.endswith('.npz'):
        params = dict(np.load(args.model))
    else:
        params = dump_chkpt_vars(args.model)    # <---- weird?
```
1. There is no `dump_chkpt_vars` in `tensorpack.tfutils.varmanip`.
2. This parts are for loading, so `dump_chkpt_vars` should be `load_chkpt_vars`, maybe?",one please see could bug line import else weird loading maybe,issue,negative,negative,negative,negative,negative,negative
435673731,Closing as there is no response. Feel free to reopen if you have more specific questions about what you attempt to do.,response feel free reopen specific attempt,issue,positive,positive,positive,positive,positive,positive
435360744,"For anyone to better diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",anyone better diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected,issue,positive,positive,positive,positive,positive,positive
435286993,"The original models were trained without modifications to the code and on 8 GPUs. Since you've made changes to the code, getting different results is not surprising.",original trained without code since made code getting different surprising,issue,positive,positive,positive,positive,positive,positive
435262683,"For anyone to better diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

In particular, I do not know what you did and what you've observed. Please do not describe what you did and what you saw, paste them.",anyone better diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected particular know please describe saw paste,issue,positive,positive,positive,positive,positive,positive
435092987,"Forgot to post the update, sorry about that!

Turns out there was a bug in the code for building the model for regular tf trainer (missed a line) so that's why it could fit much larger training batches. Turns out that in the end it's just the fact that the model is massive, and nothing was wrong on the side of tensorpack.

Thanks for the tips and sorry for the trouble!",forgot post update sorry turn bug code building model regular trainer line could fit much training turn end fact model massive nothing wrong side thanks sorry trouble,issue,negative,negative,neutral,neutral,negative,negative
435033676,"> Would you say it is typical behavio

It's typical that GPU utilization becomes lower because no systems can scale linearly.

> (maybe due to Python's multi-threading/processing inefficiency at this moment?

To know what is the reason you'll need to benchmark the training and data separately to see whether the data is fast enough.",would say typical typical utilization becomes lower scale linearly maybe due python inefficiency moment know reason need training data separately see whether data fast enough,issue,negative,negative,neutral,neutral,negative,negative
434977646,"What do you mean by FPN? Setting the parameter to true in the following command:

**./train.py --config MODE_MASK=False MODE_FPN=False DATA.BASEDIR=/media/ext-drive/datasets/coco/coco BACKBONE.WEIGHTS=/media/ext-drive/faster_rcnn/faster_rcnn_new/tensorpack-master/examples/FasterRCNN/models/ImageNet-R50-AlignPadding.npz**

Thanks",mean setting parameter true following command thanks,issue,positive,positive,neutral,neutral,positive,positive
434975707,I think 8G is probably enough if you use FPN and a small BATCH_PER_IM,think probably enough use small,issue,negative,negative,negative,negative,negative,negative
434971364,"I reduced the parameter `FRCNN.BATCH_PER_IM` to 128 but still is giving me the same error.

I want to train COCO on FasterRCNN. Hence, I need a model pretrained on Imagenet. 
So, you are suggesting it is impossible to train such case on my GPU.

Regards.",reduced parameter still giving error want train coco hence need model suggesting impossible train case,issue,negative,negative,negative,negative,negative,negative
434964997,The FasterRCNN example does not include small models. For large models you can also try: 1. use FPN and 2. use a smaller `FRCNN.BATCH_PER_IM`.,example include small large also try use use smaller,issue,negative,negative,neutral,neutral,negative,negative
434962775,Your GPU does not have enough memory to run it. You'll need to change your models.,enough memory run need change,issue,negative,neutral,neutral,neutral,neutral,neutral
434962153,"Thank you very much. Using `pip install -U git+https://github.com/ppwwyyxx/tensorpack.git` solved the issue.

However, now I am facing another error.
**tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[512,1024,14,14]**

Eventhough I pulled down the **_C.RPN.TEST_POST_NMS_TOPK** parameter from 1000 to 100 but still I am getting the same error.

The GPU I am using is Geforce GTX1080 with 8GB.

",thank much pip install issue however facing another error tensor shape parameter still getting error,issue,negative,positive,positive,positive,positive,positive
434951544,"I am sorry for the wrong editing of the code above. The code for reading data is shown as follows:

```
    lmdb_data = os.path.join(datadir, 'ILSVRC-%s.lmdb'%name)
    ds = LMDBData(lmdb_data, shuffle=False)
    if isTrain:
        ds = LocallyShuffleData(ds, 50000)
    ds = PrefetchData(ds, 5000, 1)
    ds = LMDBDataPoint(ds)
    ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
    ds = AugmentImageComponent(ds, augmentors, copy=False)
    if parallel < 16:
        logger.warn(""DataFlow may become the bottleneck when too few processes are used."")
    ds = BatchData(ds, batch_size, remainder=False)
    ds = PrefetchDataZMQ(ds, parallel)
    ds.reset_state()
```

The val accuracy:
![image](https://user-images.githubusercontent.com/15543999/47839118-851b9d80-ddec-11e8-9bad-53d42284e11b.png)
",sorry wrong code code reading data shown name lambda parallel may become bottleneck used parallel accuracy image,issue,negative,negative,negative,negative,negative,negative
434927777,"That means you're still using the old tensorpack.
The command to install latest tensorpack is in the [README](https://github.com/tensorpack/tensorpack/) as well as the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md):
`pip install -U git+https://github.com/ppwwyyxx/tensorpack.git`
The command to see current tensorpack version is also in the [issue template](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md):
`python -c 'import tensorpack; print(tensorpack.__version__)'`.
",still old command install latest well issue template pip install command see current version also issue template python print,issue,negative,positive,positive,positive,positive,positive
434803376,"I just downloaded the master-file from github. But still getting the same error. 

After downloading the package, I run the setup file through ""./setup.py build"" and then I go to ""examples/faster-rcnn"" directory and execute  the following:

**./train.py --config MODE_MASK=False MODE_FPN=False DATA.BASEDIR=/media/ext-drive/datasets/coco/coco BACKBONE.WEIGHTS=/media/ext-drive/faster_rcnn/faster_rcnn_new/tensorpack-master/examples/FasterRCNN/models/ImageNet-R50-AlignPadding.npz**

Any advice?",still getting error package run setup file build go directory execute following advice,issue,negative,neutral,neutral,neutral,neutral,neutral
434789687,"tensorpack 0.9 was just pushed to pypi: https://pypi.org/project/tensorpack/
Reinstall from pypi or github master should solve the issue.",reinstall master solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
434620219,"I copied `export.py` to my model export script folder, and modified importing ,such as `from ..utils import logger` to `from tensorpack.utils import logger`. It did not report importing issue. 
However, new issue is below:
`TypeError: optimize_for_inference() takes exactly 4 arguments (5 given)`
The script is below:
```
from export import ModelExporter
sess_init = SaverRestore(model_path=restore_ckpt, ignore=ignore_list)
pred_config = PredictConfig(model=model, 
                           session_init=sess_init,
                           input_names=['images'], 
                           output_names=['fullconv/output'])
ModelExporter(pred_config).export_compact('/compact_graph.pb')
```

It is ok now, I deleted `False` in:
```
pruned_graph_def = optimize_for_inference_lib.optimize_for_inference(
                frozen_graph_def,
                [n.name[:-2] for n in input_tensors],
                [n.name[:-2] for n in output_tensors],
                [dtype.as_datatype_enum for dtype in dtypes],
                False)
```",copied model export script folder import logger import logger report issue however new issue exactly given script export import false false,issue,negative,negative,negative,negative,negative,negative
434613881,"The importing issue still exists after `pip install --upgrade git+https://github.com/tensorpack/tensorpack.git`. 

tensorpack version is 0.8.9",issue still pip install upgrade version,issue,negative,neutral,neutral,neutral,neutral,neutral
434610087,Then you probably need to upgrade tensorpack. The upgrade method is in the README and also in the issue template.,probably need upgrade upgrade method also issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
434609626,"For anyone to better diagnose your issue, please always post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md)).

Your issue is a duplicate of #919.",anyone better diagnose issue please always post relevant following issue template click new issue unexpected visit link post unexpected issue duplicate,issue,positive,positive,positive,positive,positive,positive
434580817,"What I did:

I simply tried to run `export-model.py`, and the error is `ImportError: cannot import name ModelExporter`. The `ModelExporter` just can not be imported. While importing other tensorpack modules has no problem.


The link in the tutorial is missing, because 'basic' in tutorial link:
 'https://github.com/tensorpack/tensorpack/blob/master/examples/basic/export-model.py' should be 'basics'.",simply tried run error import name problem link tutorial missing tutorial link,issue,negative,negative,neutral,neutral,negative,negative
434576731,"> What you did:
I would like to export my model to .pb file, following this link: https://tensorpack.readthedocs.io/tutorial/inference.html#exporter

This is what you want to do, not what you did. I still don't know what you did.

> What you observed, including but not limited to the entire logs.
I can not import ModelExporter, ImportError: cannot import name ModelExporter, also examples/basic/export-model.py in the tutorial is missing, maybe should update the docs.

This is not the entire logs. Please post full error message.

The example `export-model.py` can run with latest tensorpack (github master).",would like export model file following link want still know limited entire import import name also tutorial missing maybe update entire please post full error message example run latest master,issue,negative,positive,neutral,neutral,positive,positive
434305790,"super quick means quick training, not quick inference.",super quick quick training quick inference,issue,positive,positive,positive,positive,positive,positive
434290941,"The partial logs suggest that you're using one GPU.
If not, please post issues following the issue template: including the command you run and the entire logs.",partial suggest one please post following issue template command run entire,issue,negative,negative,neutral,neutral,negative,negative
434282432,"> For anyone to better diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))
> 
> I assume you're running one of the standard settings in the README. When evaluated with only 1 GPU, the evaluation time you saw is normal.

i used four gpu",anyone better diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected assume running one standard evaluation time saw normal used four,issue,positive,positive,positive,positive,positive,positive
434276452,"For anyone to better diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

I assume you're running one of the standard settings in the README. When evaluated with only 1 GPU, the evaluation time you saw is normal.",anyone better diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected assume running one standard evaluation time saw normal,issue,positive,positive,positive,positive,positive,positive
434160895,Thanks man! The tf.Print method works! Thanks for your help.,thanks man method work thanks help,issue,positive,positive,positive,positive,positive,positive
434149813,"https://github.com/tensorpack/tensorpack/blob/eb408ed0e45204059a938b6fae46f8db7205fd15/examples/DoReFa-Net/dorefa.py#L61-L62
Add `x = tf.Print(x, [x])`.

Alternatively, `print(x)` to see the name of the gradient tensor, and use other methods in https://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training to print the tensor.",add alternatively print see name gradient tensor use print tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
434149086,Yes the bottleneck is the data but https://tensorpack.readthedocs.io/tutorial/performance-tuning.html#investigate-dataflow has more information on figuring out __which part of data__ is the bottleneck. Before doing that it does not make sense to do any optimization.,yes bottleneck data information part bottleneck make sense optimization,issue,positive,neutral,neutral,neutral,neutral,neutral
434148507,"Is there any update?

StagingArea should not be an issue because the input is not very large. I would assume something (either other processes or a initialized session in the same process) took the memory before tensorpack starts training.",update issue input large would assume something either session process took memory training,issue,negative,positive,positive,positive,positive,positive
434147941,i am going to use ImageNet-R101-AlignPadding.npz this pre-trained model and submit task to try again. thank you very much.,going use model submit task try thank much,issue,negative,positive,positive,positive,positive,positive
434147631,"And if you change your `config.py` to use ResNet101, you need to load a ResNet101 pre-trained model.",change use need load model,issue,negative,neutral,neutral,neutral,neutral,neutral
434147252,"so, your mean is that i should set up the backbone anyway? thanks very much for your patient!",mean set backbone anyway thanks much patient,issue,negative,positive,neutral,neutral,positive,positive
434146996,You need to set `BACKBONE.WEIGHTS` in either `config.py` or the command line.,need set either command line,issue,negative,neutral,neutral,neutral,neutral,neutral
434146585,"./train.py --config \
    MODE_MASK=True MODE_FPN=True \
    DATA.BASEDIR=/path/to/COCO/DIR \
    BACKBONE.WEIGHTS=/path/to/ImageNet-R50-Pad.npz \
this is your example .But i have change the config in the config.py other than the backnoe.weights. Why can't I run python train.py directly?",example change ca run python directly,issue,negative,positive,neutral,neutral,positive,positive
434146199,"The pre-trained weights to load for coco training is trained on imagenet and has nothing to do with the 80 categories.

I did not say that there must be a pre-trained backbone. There are ways to train without a pre-trained backbone, but `python train.py` is just the wrong command.
",load coco training trained nothing say must backbone way train without backbone python wrong command,issue,negative,negative,negative,negative,negative,negative
434145553,Do you mean that there  must have a backbone on the imagenet when training?,mean must backbone training,issue,negative,negative,negative,negative,negative,negative
434143347,"> `python train.py` is not the correct command to train. You need to use a pre-trained weights. Read the README for details.

but if  the dataset don't have 80 NUM_CATEGORY,the pre-trained weights is wrong in the last .what should i do.",python correct command train need use read wrong last,issue,negative,negative,negative,negative,negative,negative
434142699,`python train.py` is not the correct command to train. You need to use a pre-trained weights. Read the README for details.,python correct command train need use read,issue,negative,neutral,neutral,neutral,neutral,neutral
434142182,"> Please post the details following the issue template in the link above, i.e.:
> 
> 1. What you did:
> 
> * If you're using examples:
>   
>   * What's the command you run:
>   * Have you made any changes to the examples? Paste them if any:
> * If not, tell us what you did that may be relevant.
>   But we may not investigate it if there is no reproducible code.
> * Better to paste what you did instead of describing them.
> 
> 1. What you observed, including but not limited to the **entire** logs.
> 
> * Better to paste what you observed instead of describing them.
> 
> 1. What you expected, if not obvious.
> 
> Since you mentioned both training and prediction, I assume that's two separate commands. Therefore please post two group of relevant information (what you did and what you observed) for the two commands, respectively.

ok,thanks! i have pasted the question https://github.com/tensorpack/tensorpack/issues/955",please post following issue template link command run made paste tell u may relevant may investigate reproducible code better paste instead limited entire better paste instead obvious since training prediction assume two separate therefore please post two group relevant information two respectively thanks pasted question,issue,positive,positive,positive,positive,positive,positive
433964914,"Please post the details following the issue template in the link above, i.e.:

1. What you did:
  + If you're using examples:
    + What's the command you run:
    + Have you made any changes to the examples? Paste them if any:
  + If not, tell us what you did that may be relevant.
    But we may not investigate it if there is no reproducible code.
  + Better to paste what you did instead of describing them.
2. What you observed, including but not limited to the __entire__ logs.
  + Better to paste what you observed instead of describing them.
3. What you expected, if not obvious.

Since you mentioned both training and prediction, I assume that's two separate commands. Therefore please post two group of relevant information (what you did and what you observed) for the two commands, respectively.",please post following issue template link command run made paste tell u may relevant may investigate reproducible code better paste instead limited better paste instead obvious since training prediction assume two separate therefore please post two group relevant information two respectively,issue,positive,positive,positive,positive,positive,positive
433937971,"> Please post relevant details following the issue template in the link above. In particular, I do not understand what you did, what you observed and what you expected.

i used the tensorpack to train coco dataset and used the default configuration. i can train the model success. But when i predict the model , but the predict image is the same as original image,in other wors, the model don't find any box and segmentation.",please post relevant following issue template link particular understand used train coco used default configuration train model success predict model predict image original image model find box segmentation,issue,positive,positive,positive,positive,positive,positive
433931350,"Please post relevant details following the issue template in the link above. In particular, I do not understand what you did, what you observed and what you expected.",please post relevant following issue template link particular understand,issue,negative,positive,positive,positive,positive,positive
433894938,"> For anyone to better diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))

i used four gpu to train and used one gpu to inference!",anyone better diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected used four train used one inference,issue,positive,positive,positive,positive,positive,positive
433859184,"> > 1. ""some variables are used in training but not found (i.e. not used) in inference"", and in the models I provided, I have manually removed such variables, so you won't see any warnings.
> >    As for why your model does not perform well, it is a machine learning/computer vision question that we'll not be able to answer.
> > 2. You do not need to use that format because tensorpack recognize both. To make the conversion, load and save the variables with https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.varmanip.save_chkpt_vars
> 
> OK，Thanks.
Has your problem been solved? I have encountered the same problem.",used training found used inference provided manually removed wo see model perform well machine vision question able answer need use format recognize make conversion load save problem problem,issue,negative,positive,positive,positive,positive,positive
433807670,"Hi， 

The bottleneck is the data.

```
QueueInput/queue_size: 2.0649e-38
```

I know that [Efficitent DataFlow/Sequential Read](https://tensorpack.readthedocs.io/tutorial/efficient-dataflow.html#sequential-read) should solve that problem, as the codes I wrote above, but it seems didn't do what I thought.

I tried to write the whole numpy array in **lmdb generating** part to reduce the time of **lmdb decoding** cost. Besides I got a large lmdb file, however the queus_size is similar (near to zero), and GPU utilization is still low. training 30% val 10%. 

Do you have any suggestion?",bottleneck data know read solve problem wrote thought tried write whole array generating part reduce time cost besides got large file however similar near zero utilization still low training suggestion,issue,negative,positive,positive,positive,positive,positive
433761574,This sounds like a machine learning question rather than a tensorpack usage question. We do not answer general machine learning question in issues.,like machine learning question rather usage question answer general machine learning question,issue,negative,positive,neutral,neutral,positive,positive
433671202,"Thank you for your reply. I think I can get the gradient value form tf.Print() method. I got another question, where should I use tf.Print() method? In the dorefa.py, under the gradient quantization method? Otherwise, I need to use this inside the tensorpack trainer? Sorry for some trivial questions, I am just new for tensorflow and tensorpack.",thank reply think get gradient value form method got another question use method gradient quantization method otherwise need use inside trainer sorry trivial new,issue,positive,negative,negative,negative,negative,negative
433668940,"See FAQ: https://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training

You can get the name of the gradient by `tf.gradient(cost, some_tensor)`.",see get name gradient cost,issue,negative,neutral,neutral,neutral,neutral,neutral
433668877,"Is there any way I can print the gradient in tensorpack? My purpose for
this question is to see how gradient change in gradient quzntization.

Yuxin Wu <notifications@github.com>于2018年10月27日 周六21:28写道：

> If you want to use session.run, don't use tensorpack trainer.
> You can copy the graph building code to somewhere else and use sessions
> yourself.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/951#issuecomment-433668732>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/Ap7TUvbkTRCBrJHXf_pSHsytaW2h-xv4ks5upQg9gaJpZM4X9rCH>
> .
>
",way print gradient purpose question see gradient change gradient want use use trainer copy graph building code somewhere else use session thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
433668732,"If you want to use session.run, don't use tensorpack trainer.
You can copy the graph building code to somewhere else and use sessions yourself.",want use use trainer copy graph building code somewhere else use session,issue,negative,neutral,neutral,neutral,neutral,neutral
433606064,"Hi, 

It is indeed the problem of `LocallyShuffleData `. Now it works. Thank you.

I followed this [document **Sequential Read**](https://tensorpack.readthedocs.io/tutorial/efficient-dataflow.html#sequential-read). It would be better if the Error mention about it or there has a warning.

emmm, there is another GPU utilization question refer to **Sequential Read**. I am not sure if I need to raise another issue. Could I write it here?

When I training with the lmdb decode codes I written before(commented out `LocallyShuffleData`, fixed problem), The GPU utilization is almost 30% all training epoch time and about 10% validation epoch.

It seems that `ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)` cost too much CPU time.

At before, I use `tf.image.decode_jpeg()` function to decode raw jpeg data in **tfrecord** file, it looks well and my tensorflow codes can achieve almost 90% utilization.

But I tried here, seems that `lambda` works not well with it.

As I am not well understood tensorpack, is there any example that could use gpu to decode jpeg raw data? Otherwise, how can I achieve better GPU utilization mentioned in this [document](https://tensorpack.readthedocs.io/tutorial/input-source.html)?",hi indeed problem work thank document sequential read would better error mention warning another utilization question refer sequential read sure need raise another issue could write training decode written fixed problem utilization almost training epoch time validation epoch lambda cost much time use function decode raw data file well achieve almost utilization tried lambda work well well understood example could use decode raw data otherwise achieve better utilization document,issue,positive,positive,positive,positive,positive,positive
433597762,"The issue is that `LocallyShuffleData` for now does not handle the case when the buffer size (50000) is even larger than the whole dataflow -- this case does not make sense at all. I'll add a warning for this.

Also, using LocallyShuffleData for inference is wrong at the first place because it randomly mixes datapoints.",issue handle case buffer size even whole case make sense add warning also inference wrong first place randomly,issue,negative,negative,negative,negative,negative,negative
433591619,"This it the code that how I generate lmdb file.

I read raw jpeg data so that the lmdb file will not be too much larger than origin images' size.

I designed a class, designed `__len__` and `__iter__`, it looks no problem.

```python
def generate_multi_label_lmdb(save_lmdb_path, image_to_labels):
    """"""
    Generate lmdb file contain raw jpg format images, and a numpy array of multi-labels.
    :param save_lmdb_path: path to save lmdb file
    :param image_to_labels: a dict, key is image path, value is string labels, eg. ""1 0 1 0 0 1""
    :return:
    """"""

    from tensorpack.dataflow import PrefetchDataZMQ, LMDBSerializer, DataFlow

    class ImageLabel(DataFlow):
        def __init__(self, dict_image_labels):
            self.dict = dict_image_labels

        def __len__(self):
            return len(self.dict)

        def __iter__(self):
            for img_path in self.dict:
                # img = np.array(Image.open(img_path))
                image_raw = open(img_path).read()
                labels = str(image_to_labels[img_path])
                labels_list = labels.split("" "")
                labels_list = [float(i) for i in labels_list]
                labels_array = np.array(labels_list)
                yield [image_raw, labels_array]

    ds0 = ImageLabel(image_to_labels)
    ds1 = PrefetchDataZMQ(ds0, nr_proc=1)
    LMDBSerializer.save(ds1, save_lmdb_path)
```

The codes below is for decoding lmdb file.

```python
def decode_multi_label_lmdb(load_lmdb_path, batchsize=48,  remainder=False):
    """"""
    Decode lmdb file and returan a BatchData
    :param load_lmdb_path: lmdb file path
    :param batchsize: batchsize of BatchData
    :param remainder: according BatchData reminder
    :return: BatchData [images_array, labels_array]
    """"""
    import cv2
    from tensorpack.dataflow import BatchData, LMDBSerializer, MapDataComponent, LocallyShuffleData, AugmentImageComponent
    ds = LMDBSerializer.load(load_lmdb_path, shuffle=False)
    ds = LocallyShuffleData(ds, 50000)
    ds = MapDataComponent(ds, lambda x: np.asarray(bytearray(x), dtype=""uint8""), 0)
    ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
    ds = MapDataComponent(ds, lambda x: x/float(255), 0)
    # ds = AugmentImageComponent(ds, lots_of_augmentors)
    ds = BatchData(ds, batchsize, remainder=remainder)
    return ds
```",code generate file read raw data file much origin size designed class designed problem python generate file contain raw format array param path save file param key image path value string return import class self self return self open float yield file python decode file param file path param param remainder according reminder return import import lambda lambda lambda return,issue,negative,negative,neutral,neutral,negative,negative
433591169,"
`dataset_test.__len__()` returns the number of batches.

```
[1027 13:13:51 @format.py:92] Found 1213 entries in D:\infor\dataset\lmdb\val.lmdb
38
```
my batch size is 32,  1213 entries in my lmdb file,  so 1213/32=37.90625

I use `reminder=True`, so I will got 38 batches. Is that correct?

But when I run the  `for loop` you wrote above, both my codes and your mnist example codes got error. Those errors are different.

For my codes, it returns an error:

```
Traceback (most recent call last):
  File ""D:\PyCharm 2018.2.2\helpers\pydev\pydevd.py"", line 1664, in <module>
    main()
  File ""D:\PyCharm 2018.2.2\helpers\pydev\pydevd.py"", line 1658, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""D:\PyCharm 2018.2.2\helpers\pydev\pydevd.py"", line 1068, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""D:\PyCharm 2018.2.2\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""D:/infor/train_with_tensorpack.py"", line 118, in <module>
    for cnt, data in enumerate(dataset_test):
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorpack\dataflow\common.py"", line 116, in __iter__
    for data in self.ds:
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorpack\dataflow\common.py"", line 274, in __iter__
    for dp in self.ds:
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorpack\dataflow\common.py"", line 274, in __iter__
    for dp in self.ds:
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorpack\dataflow\common.py"", line 274, in __iter__
    for dp in self.ds:
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorpack\dataflow\common.py"", line 589, in __iter__
    self._add_data()
  File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorpack\dataflow\common.py"", line 581, in _add_data
    dp = next(self.ds_itr)
AttributeError: 'LocallyShuffleData' object has no attribute 'ds_itr'

```

For your [example](https://github.com/tensorpack/tensorpack/blob/master/examples/keras/mnist-keras.py), I test it, return a different error:
```
AttributeError: 'Mnist' object has no attribute 'rng'
```",number found batch size file use got correct run loop wrote example got error different error recent call last file line module main file line main setup none none file line run file execute script file line compile file file line module data enumerate file line data file line file line file line file line file line next object attribute example test return different error object attribute,issue,negative,positive,neutral,neutral,positive,positive
433588471,"Please check whether `dataset_test.__len__()` is correct and see if `for cnt, data in enumerate(dataset_test): print(cnt)` can produce this many of data.",please check whether correct see data enumerate print produce many data,issue,negative,positive,positive,positive,positive,positive
433372031,"@ppwwyyxx 
if using PeriodicCallback, will still the tensors of a total epoch be saved to disk? Can I save the tensors of only 10 iters ?",still total epoch saved disk save,issue,positive,neutral,neutral,neutral,neutral,neutral
433361979,"@ppwwyyxx 
how to save dump the tensors into npz file every 100 iters?
currently, directly writing callbacks= [DumpTensors(*****)] will save the tensors every iteration, which is too much regarding to the disk storage. How to save it every 100 or 1000 iters?",save dump file every currently directly writing save every iteration much regarding disk storage save every,issue,positive,positive,positive,positive,positive,positive
433195304,"Closing as there is no response, and how to get gradients w.r.t. each layer's output is a tensorflow's question unrelated to tensorpack.",response get layer output question unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
432790445,"My mistake: On my `DataFlow`, I was generating data as `[[input_a, input_b], y]`, but changing that to `[input_a, input_b, y]` solved the case.",mistake generating data case,issue,negative,neutral,neutral,neutral,neutral,neutral
432768824,"If I remove the `None` from the `input_a` input, it passes, but then I got an issue from my model, because it was expecting `ndim=3` and received `ndim=2`.

```
ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=2
```",remove none input got issue model received input incompatible layer found,issue,negative,neutral,neutral,neutral,neutral,neutral
432615137,"Every tensor has a name.
You can see the name of a tensor by `print(a)`. Or you can give it a new name by `a = tf.identity(a, name='name')`.",every tensor name see name tensor print give new name,issue,negative,positive,positive,positive,positive,positive
432546311,"@ppwwyyxx 
does the callbacks.DumpTensors() only support dump the tensors with names? 
what if an intermmediate variable calculated by other tensors with names?
suppose `a=b*2`, and I want to dump the 'a' variable into npz file, but a has no names. How to dump it into npz file?",support dump variable calculated suppose want dump variable file dump file,issue,negative,neutral,neutral,neutral,neutral,neutral
432221188,"Please post relevant details following the issue template. 

> What you observed, including but not limited to the entire logs.
ImportError: cannot import name 'FeedfreePredict'

This is not the entire logs.

With a correct installation, this command:
```bash
python -c 'from tensorpack.predict import FeedfreePredictor'
```
will exit without errors.",please post relevant following issue template limited entire import name entire correct installation command bash python import exit without,issue,negative,positive,neutral,neutral,positive,positive
432220150,"@I tried install using pip install -U git+https://github.com/ppwwyyxx/tensorpack.git. However, it is still the version 0.8.9 and sitll can not import name 'FeedfreePredict', please check it.",tried install pip install however still version import name please check,issue,negative,neutral,neutral,neutral,neutral,neutral
432214729,It needs to be the latest version (github master branch). You've posted the installation instruction above.,need latest version master branch posted installation instruction,issue,negative,positive,positive,positive,positive,positive
432213124,"@ppwwyyxx so, which version should I use?
and how about if I use from tensorpack.predict.feedfree import FeedfreePredictor instead?",version use use import instead,issue,negative,neutral,neutral,neutral,neutral,neutral
432194582,"from tensorpack.predict.feedfree import FeedfreePredictor
I use this line instead.",import use line instead,issue,negative,neutral,neutral,neutral,neutral,neutral
431939742,I experienced a similar issue. I guess https://github.com/tensorpack/tensorpack/issues/945 might be related..?,experienced similar issue guess might related,issue,negative,positive,positive,positive,positive,positive
431937853,"Oh, I figured out my mistake. 

The `__iter__` function of DataFlow should be self-refreshable. I.e, initialization part should also be in the `__iter__` function when using python generator.

Instead of 
````
class MyDataFlow(RNGDataFlow): 
    def __init__(self):
        ... initialize self.generator here ...
    def __iter__(self):
        for _ in range(len(self.num_samples))
            example = next(self.generator)
            ....
            yield example
````
it should be
````
class MyDataFlow(RNGDataFlow): 
    def __init__(self):
        ...
    def __iter__(self):
        ... initialize self.generator here ...
        for _ in range(len(self.num_samples))
            example = next(self.generator)
            ....
            yield example
````
This is because the queueing code uses something like `ds = self.df.__iter__()` in order for refreshing.

* Btw, sorry for duplicated same issue postings. Last night, any of the postings didn't appear, so I tried several times thinking as network issue.",oh figured mistake function part also function python generator instead class self initialize self range example next yield example class self self initialize range example next yield example code something like order refreshing sorry issue last night appear tried several time thinking network issue,issue,negative,neutral,neutral,neutral,neutral,neutral
431886061,"Reopen because #942 is empty. Closing #942 instead.

For anyone to better diagnose your issue, please post relevant details following the issue template (click ""New Issue"" -> ""Unexpected Problems / Bugs"", or visit [this link to post issues about unexpected problems](https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md))",reopen empty instead anyone better diagnose issue please post relevant following issue template click new issue unexpected visit link post unexpected,issue,positive,positive,positive,positive,positive,positive
431652419,"I don't currently have access to the workstation, I'll update the issue with logs on monday. 

Meanwhile, could you tell me how to disable stagingarea? When I didn't specify staginginput tensorpack used a default one instead.",currently access update issue meanwhile could tell disable specify used default one instead,issue,negative,neutral,neutral,neutral,neutral,neutral
431609333,"Also, to report any unexpected issues please always post __full log__ rather than describing what you saw. Even you don't want to post code, logs can still be helpful. Another hypothesis is that you've manually created a session that takes all memory before starting tensorpack trainer. With logs this can be easily seen.

See https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md on how to write an issue.",also report unexpected please always post rather saw even want post code still helpful another hypothesis manually session memory starting trainer easily seen see write issue,issue,positive,positive,positive,positive,positive,positive
431609098,"I would assume you've made some mistakes in the model code so it actually is something else. But without enough details I cannot help. You can easily verify that the official examples can all run with a large batch size (e.g. ResNet50 can run with a batch size of 128 on 16GB GPU).

Also try removing stagingarea to see if it changes anything. ",would assume made model code actually something else without enough help easily verify official run large batch size run batch size also try removing see anything,issue,negative,positive,positive,positive,positive,positive
431473370,"Yup, pyarrow was the issue and `pip uninstall pyarrow` resolved it. Thank you for the quick response!",issue pip resolved thank quick response,issue,negative,positive,positive,positive,positive,positive
431461653,"Well I'm not sure if ""your own version"" will avoid `import pyarrow` after setting that environment variable.
Maybe just `pip uninstall pyarrow` and see what happened afterwards.",well sure version avoid import setting environment variable maybe pip see afterwards,issue,negative,positive,positive,positive,positive,positive
431461255,"Could you try `export TENSORPACK_SERIALIZE=msgpack`?
There is a bug in pyarrow that conflicts with horovod.",could try export bug,issue,negative,neutral,neutral,neutral,neutral,neutral
431380951,"My apology that did not make the issue clear. I will rephrase it, and perhaps open a new issue. Thanks.",apology make issue clear rephrase perhaps open new issue thanks,issue,positive,positive,positive,positive,positive,positive
431350926,">  I tried to build multiple models using model.get_logits(image) under TowerContext, and feed the scaled input image which is a tf queue tensor, then restore the model variables using SaverRestore(), and use sess.run(final_pred) to get the final prediction. 

I'm not completely sure what you did and not even sure if any steps above (except for building the model) involves tensorpack. You need to set variable scope to reuse for multiple calls to `get_logits`, and if possible use placeholders to find bugs easier. Print `tf.global_variables` to see what variables you've created, etc.

To report any unexpected issues please use the issue template.",tried build multiple image feed scaled input image queue tensor restore model use get final prediction completely sure even sure except building model need set variable scope reuse multiple possible use find easier print see report unexpected please use issue template,issue,positive,positive,positive,positive,positive,positive
431326413,"Hi, @ppwwyyxx 
I would like to do inference with multiple scales inputs and add those model outputs, i.e. logits, to get final prediction. I tried to build multiple models using `model.get_logits(image)` under `TowerContext`, and feed the scaled input image which is a tf queue tensor, then restore the model variables using `SaverRestore()`, and use `sess.run(final_pred)` to get the final prediction. However, the prediction image is messy like random variables restore. I check the variable names in the saved model, it is the same as building the model, variables are auto-reuse.
The saved model has no problem, as I have tried to do inference using `OfflinePredictor` which is in the tutorial, and it is ok.
Many thanks!",hi would like inference multiple scale add model get final prediction tried build multiple image feed scaled input image queue tensor restore model use get final prediction however prediction image messy like random restore check variable saved model building model saved model problem tried inference tutorial many thanks,issue,positive,positive,neutral,neutral,positive,positive
431235341,You can also wrap a `tf.identity` function with `tf.custom_gradient`. This will let you access to the gradients in the backward pass and is more efficient in runtime. You can refer to tensorflow documentation to use it or `tf.gradients`.,also wrap function let access backward pas efficient refer documentation use,issue,negative,neutral,neutral,neutral,neutral,neutral
430897957,"I see. Thanks for the advice!
I'll leave a few random related links:
https://stackoverflow.com/questions/42394585/how-to-inspect-a-tensorflow-tfrecord-file
https://www.kaggle.com/mpekalski/reading-tfrecord",see thanks advice leave random related link,issue,negative,negative,neutral,neutral,negative,negative
430891922,"No such conversion exists. In general you can read TFRecord from pure python, with `tf.python_io.tf_record_iterator`. This will give you serialized bytes from the TFRecord and you'll need to decode the bytes into data points.",conversion general read pure python give need decode data,issue,negative,positive,positive,positive,positive,positive
430567439,"The method is correct. As for why the value is periodic, maybe your validation data is not written to produce the same data each time. Or it may be just how your training works: such thing does happen in certain types of training (e.g. when you don't shuffle the training data).",method correct value periodic maybe validation data written produce data time may training work thing happen certain training shuffle training data,issue,positive,positive,positive,positive,positive,positive
430557728,"Hi, @ppwwyyxx 
May I ask how I save the best validation result model during training?
I tried `infs = [ScalarStats(names='mean_iou', prefix='val')]` and `InferenceRunner(dataset_val, infs)` in callbacks, and also `MaxSaver(monitor_stat='val_mean_iou')`.  While the summary of `val_mean_iou` does not seem right, its value much larger than `mean_iou` and has periodic oscillation.

I guess my method is not right. Thanks !",hi may ask save best validation result model training tried also summary seem right value much periodic oscillation guess method right thanks,issue,positive,positive,positive,positive,positive,positive
430475588,"Could you please post relevant details following the issue template so that I understand what you did and what you observed. (click ""New Issue"" -> ""Unexpected Problems / Bugs""). ",could please post relevant following issue template understand click new issue unexpected,issue,negative,positive,positive,positive,positive,positive
429713409,"Wow, thanks for the quick answer! I got it.
So, for those who may be wondering, the two options below worked very well.

### option 1
```
    def _trigger_epoch(self):
        tensor = tf.get_default_graph().get_tensor_by_name(""tower0/in_image:0"")
        val = self.trainer.hooked_sess.run(tensor)
        print(val)
```

### option 2
```
    def _need_run(self):
        if self.local_step == self.trainer.steps_per_epoch - 1:
            return True
        return False

    def _before_run(self, run_context):
        if self._need_run():
            tensor = tf.get_default_graph().get_tensor_by_name(""tower0/in_image:0"")
            return tf.train.SessionRunArgs(tensor)
        return None

    def _after_run(self, run_context, run_values):
        values = run_values.results
        if values is None: return
        print(values)
```
Although I was not that critical in increasing one `global_step`, I chose the second option. 
",wow thanks quick answer got may wondering two worked well option self tensor tensor print option self return true return false self tensor return tensor return none self none return print although critical increasing one chose second option,issue,positive,positive,neutral,neutral,positive,positive
429703617,"Many functionality in tensorpack requires some extra ops to be run together with each step. For example, in your case the use of StagingArea requires the stage&unstage ops to be run to fill the data buffer. Otherwise the training will hang due to empty data buffer.

As a result there is a `trainer.hooked_sess` that will automatically do the above for you and that's what you should use.

Note that it will also do the following: (1) waste a data point for this sess.run instead of for training. (2) increase global_step. This may not be what you want. But this is also inevitable if you want to make a new `sess.run` call. That's why it's usually better to use `before_run/after_run` callback to run things __along with__ the training iteration instead of after the iteration. (https://tensorpack.readthedocs.io/tutorial/extend/callback.html).",many functionality extra run together step example case use stage run fill data buffer otherwise training due empty data buffer result automatically use note also following waste data point instead training increase may want also inevitable want make new call usually better use run training iteration instead iteration,issue,negative,positive,neutral,neutral,positive,positive
429315653,That's why I am asking. Having something like `tensorpack.hub` would be indeed really helpful. ,something like would indeed really helpful,issue,positive,positive,positive,positive,positive,positive
428721915,"That's not how the code is supposed to be used. You only need to call
```python
    predictor = tp.OfflinePredictor(tp.PredictConfig(
        model=Model(),
        session_init=tp.get_model_loader(model_path),
        input_names=['image'],
        output_names=['saliency']))
```
once because it builds the graph and load the models. Calling it many times under the same graph will make tensorflow throw the above error.

The predictor, after being built, can be used many times. ",code supposed used need call python predictor graph load calling many time graph make throw error predictor built used many time,issue,negative,positive,positive,positive,positive,positive
428449115,"It seems it's supported since TF1.8: https://github.com/tensorflow/tensorflow/commit/5c469e6bafb479ef110b2f02f070507a3711664d#diff-280d77a571d7e6e2e5c11563762014b2
Please `from tensorpack.tfutils.common import get_tf_version_tuple` and only enable this feature when the version matches.",since please import enable feature version,issue,negative,neutral,neutral,neutral,neutral,neutral
428421852,"A ""tower"" is a tensorflow terminology which roughly means the model's forward function. More in the tutorial: https://tensorpack.readthedocs.io/tutorial/trainer.html#what-is-tower-function

The tower function can be called many times for different purposes (e.g. multi-gpu training, inference). Tensorflow collection is a per-graph global data storage, and modifying global state (especially for multiple times) can lead to bugs that are hard to find. Therefore tensorpack prints such messages as info.
In your case it can be safely ignored.",tower terminology roughly model forward function tutorial tower function many time different training inference collection global data storage global state especially multiple time lead hard find therefore case safely,issue,negative,positive,neutral,neutral,positive,positive
428417822,Please remove BNLeakyReLU. Tensorpack will not add any symbolic layers. The existing layers exists only because there were no alternatives at the beginning. See more at the tutorial: https://tensorpack.readthedocs.io/tutorial/symbolic.html,please remove add symbolic beginning see tutorial,issue,negative,neutral,neutral,neutral,neutral,neutral
427727852,"OK, I get it now. Thanks !
I was considering to modify learning rate compared to on single GPU.",get thanks considering modify learning rate single,issue,negative,positive,neutral,neutral,positive,positive
427710955,"Nothing special happens to losses (if you don't use the collections). It is the gradients that are averaged, not the loss, as said in the above docs you quote.

When you use the collections, the losses in one GPU may contain losses from other GPUs, depend on what you've put in the collection.",nothing special use loss said quote use one may contain depend put collection,issue,negative,positive,positive,positive,positive,positive
427710673,"I did not notice that until now...

I still do not quite understand what happens to losses after reading the docs.
`Data-parallel training in ‘ParameterServer’ mode. It builds one tower on each GPU with shared variable scope. It synchronizes the gradients computed from each tower, averages them and applies to the shared variables.` 
If loss from each tower have shared variable scope, does it mean that it will collect and sum losses from each tower, and then **average** them ? 
Thanks.
",notice still quite understand reading training mode one tower variable scope tower loss tower variable scope mean collect sum tower average thanks,issue,negative,negative,neutral,neutral,negative,negative
427707210,"Adding things to such collections is unsafe, as mentioned in the tutorial: https://tensorpack.readthedocs.io/tutorial/trainer.html

> It may get called multiple times for data-parallel training or inference. As a result:
You’ll need to be careful when modifying global states, e.g. adding ops to collections, setting attributes of a model instance.

So your loss in the last GPU will contain the loss tensor or previous GPUs, that may be the issue.

Your questions:
1. Per-step. They are mathematically equivalent.
2. The docs of these two class has explained it.",unsafe tutorial may get multiple time training inference result need careful global setting model instance loss last contain loss tensor previous may issue mathematically equivalent two class,issue,negative,negative,neutral,neutral,negative,negative
427706501,"Hi, @ppwwyyxx I manage to run sync bn with `SyncMultiGPUTrainerParameterServer`.
The issue is probably caused by computing loss part. I used slim's weight decay loss, an auxiliary loss and obtain all losses using `tf.losses.get_total_loss()`, while using tensorpack I added regularize loss to loss collection and  `tf.losses.get_total_loss()`. I now use `tf.add_n()` to add all the losses. I have no idea why this makes sync bn hang.
BTW, I have two little question about the two SyncMulti Trainers: 
1.Does `SyncMultiGPUTrainerParameterServer` synchronize variables per step and `SyncMultiGPUTrainerReplicated` per epoch ? 
2.Does the two trainers sum losses from each tower or average losses across each tower ?
I am new to this cross gpu training. Thanks very much for your answer.",hi manage run sync issue probably loss part used slim weight decay loss auxiliary loss obtain added regularize loss loss collection use add idea sync two little question two synchronize per step per epoch two sum tower average across tower new cross training thanks much answer,issue,negative,positive,neutral,neutral,positive,positive
427554456,"> 1. ""some variables are used in training but not found (i.e. not used) in inference"", and in the models I provided, I have manually removed such variables, so you won't see any warnings.
>    As for why your model does not perform well, it is a machine learning/computer vision question that we'll not be able to answer.
> 2. You do not need to use that format because tensorpack recognize both. To make the conversion, load and save the variables with https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.varmanip.save_chkpt_vars

OK，Thanks.",used training found used inference provided manually removed wo see model perform well machine vision question able answer need use format recognize make conversion load save,issue,positive,positive,positive,positive,positive,positive
427553553,"> The warning said ""The following variables are in the checkpoint, but not found in the graph"". It means some variables are used in training but not found (i.e. not used) in inference, which is OK.
> It would not be OK if it's the other way around.

1 、I didn't use BACKBONE. WEIGHTS. I trained 189 classes of models, - config and training parameters are consistent, but - pridict can't detect the target, but using the model you provided can detect the target. I'm not sure what the problem is? Do you have any suggestions? Thank you
2、Another question, how do I save the npz format model? It is consistent with the model you provide. I am not sure whether this question is related to format.",warning said following found graph used training found used inference would way around use backbone trained class training consistent ca detect target model provided detect target sure problem thank question save format model consistent model provide sure whether question related format,issue,positive,positive,positive,positive,positive,positive
427539205,"get_current_tower_context can only be called in the tower function (i.e. the forward function you wrote.)

trigger_step is only called for each training step. (See the callbacks tutorial). So what you need is already done.",tower function forward function wrote training step see tutorial need already done,issue,negative,neutral,neutral,neutral,neutral,neutral
427538971,"Since the cifar example is naive enough and it works on your machine as well, your code probably is not naive and may have some interesting parts that make it hang. I think that's where  you want to look at.",since example naive enough work machine well code probably naive may interesting make think want look,issue,negative,negative,neutral,neutral,negative,negative
427538893,"Many thanks! I fixed this bug.

Another question.
For my schduler, 
```python
class Schdule_Relax():
       def ident(self, x):
           return x
```
Now, the schduler is just a test version. I want to make it behave differently for training and inference. 
For example, the hyperparameter is called `relax`.  I want to increase `relax` every batch by 1. After each epoch, we will do inference. During inference, I want to keep its value fixed.  Then, when we move into next epoch, I want to increase its value every batch by 1 again.

I try to use `get_current_tower_context().is_training`, but I got `AttributeError: 'NoneType' object has no attribute 'is_training'`. It seem that this function cannot find the tower context. 

Could you please give some suggestions?",many thanks fixed bug another question python class self return test version want make behave differently training inference example relax want increase relax every batch epoch inference inference want keep value fixed move next epoch want increase value every batch try use got object attribute seem function find tower context could please give,issue,positive,positive,positive,positive,positive,positive
427538348,"@ppwwyyxx Sorry for the delay, the cifar example code could run successfully run on my machine. Does it mean that my tensorpack code still has some issues ?
Since I use `SyncMultiGPUTrainerReplicated` trained my model for a few days, the result is much worse than the model I wrote using slim. I first thought the hyper parameter needs modification, but I have tried a few, still no improvement.",sorry delay example code could run successfully run machine mean code still since use trained model day result much worse model wrote slim first thought hyper parameter need modification tried still improvement,issue,negative,negative,neutral,neutral,negative,negative
427523404,"In case you want to do training twice in one script, you need to create a new graph for each training. That's one possibility of your bug.",case want training twice one script need create new graph training one possibility bug,issue,negative,positive,positive,positive,positive,positive
427522620,The code you posted is correct. I believe you've deleted the buggy part when you simplify your code for posting.,code posted correct believe buggy part simplify code posting,issue,negative,neutral,neutral,neutral,neutral,neutral
427519299,"I followed your suggestion.
```python
class Model(ModelDesc):
    def inputs(self):
        return [tf.placeholder(tf.float32, [None, 32, 32, 3], 'input'),
                tf.placeholder(tf.int32, [None], 'label')]

    def build_graph(self, image, label):
        relax = tf.get_variable('relax', initializer=1.0, trainable=False)
        logits = (LinearWrap(image)
                  .layerone(image, relax)
                  .layertwo( relax))
def get_config():
   class Schdule_Relax():
       def ident(self, x):
           return x

    relax_schduler = Schdule_Relax()
    class RelaxSetter(Callback):
        def _setup_graph(self):
            self._relax = [k for k in tf.global_variables() if k.name == 'relax:0'][0]
        def _trigger_step(self):
            self._relax.load(relax_schduler.ident(1.))

    model = Model()
    return TrainConfig(
        dataflow=dataset_train,
        callbacks=[
            ModelSaver(),
            InferenceRunner(dataset_test,
                            [ScalarStats('cost'), ClassificationError('wrong_tensor')]),
            ScheduledHyperParamSetter('learning_rate',
                                      [(1, 0.01), (82, 0.001), (123, 0.0002), (200, 0.0001)]),
            RelaxSetter(),
        ],
        model=model,
        max_epoch=args.epoches,
    ), model

if __name__ == '__main__':
    config, model = get_config()
    launch_train_with_config(config, SimpleTrainer())
```

Then I got this error,
```
[1005 22:45:54 @logger.py:109] WRN Log directory ../../../runs/trash/ exists! Use 'd' to delete it. 
[1005 22:45:54 @logger.py:112] WRN If you're resuming from a previous run, you can choose to keep it.
Press any other key to exit. 
Select Action: k (keep) / d (delete) / q (quit):d
[1005 22:45:55 @logger.py:74] Argv: cifar_simon.py
[1005 22:45:55 @cifar.py:32] Found cifar10 data in ../../../cifar10_data/.
[1005 22:45:56 @parallel.py:185] [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[1005 22:45:56 @argtools.py:152] WRN Install python-prctl so that processes can be cleaned with guarantee.
[1005 22:45:56 @cifar.py:32] Found cifar10 data in ../../../cifar10_data/.
[1005 22:45:56 @argtools.py:152] WRN Install python-prctl so that processes can be cleaned with guarantee.
check...................
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/codes/tensorpack/examples/DoReFa-Net/cifar_simon.py in <module>()
    274     config, model = get_config()
    275     print('check...................')
--> 276     launch_train_with_config(config, SimpleTrainer())
    277 

/usr/local/lib/python3.6/dist-packages/tensorpack/train/interface.py in launch_train_with_config(config, trainer)
     83     trainer.setup_graph(
     84         inputs_desc, input,
---> 85         model._build_graph_get_cost, model.get_optimizer)
     86     _check_unused_regularization()
     87     trainer.train_with_defaults(

/usr/local/lib/python3.6/dist-packages/tensorpack/utils/argtools.py in wrapper(*args, **kwargs)
    179         _FUNC_CALLED.add(key)
    180 
--> 181         return func(*args, **kwargs)
    182 
    183     return wrapper

/usr/local/lib/python3.6/dist-packages/tensorpack/train/tower.py in setup_graph(self, inputs_desc, input, get_cost_fn, get_opt_fn)
    200 
    201         # TODO setup may want to register monitor as well??
--> 202         input_callbacks = self._setup_input(inputs_desc, input)
    203         train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
    204         self.register_callback(input_callbacks + train_callbacks)

/usr/local/lib/python3.6/dist-packages/tensorpack/train/tower.py in _setup_input(self, inputs_desc, input)
    216     def _setup_input(self, inputs_desc, input):
    217         assert not input.setup_done()
--> 218         return input.setup(inputs_desc)
    219 
    220     def _make_get_grad_fn(self, input, get_cost_fn, get_opt_fn):

/usr/local/lib/python3.6/dist-packages/tensorpack/utils/argtools.py in wrapper(*args, **kwargs)
    179         _FUNC_CALLED.add(key)
    180 
--> 181         return func(*args, **kwargs)
    182 
    183     return wrapper

/usr/local/lib/python3.6/dist-packages/tensorpack/input_source/input_source_base.py in setup(self, inputs_desc)
     95             callbacks of InputSource cannot use any `trigger*()` method.
     96         """"""
---> 97         self._setup(inputs_desc)
     98         self._setup_done = True
     99         return self.get_callbacks()

/usr/local/lib/python3.6/dist-packages/tensorpack/input_source/input_source.py in _setup(self, inputs)
    110     def _setup(self, inputs):
    111         # placeholders as input are always safe to reuse.
--> 112         self._all_placehdrs = [v.build_placeholder_reuse() for v in inputs]
    113         self._cb = self._FeedCallback(self._iter_ds, self._all_placehdrs)
    114 

/usr/local/lib/python3.6/dist-packages/tensorpack/input_source/input_source.py in <listcomp>(.0)
    110     def _setup(self, inputs):
    111         # placeholders as input are always safe to reuse.
--> 112         self._all_placehdrs = [v.build_placeholder_reuse() for v in inputs]
    113         self._cb = self._FeedCallback(self._iter_ds, self._all_placehdrs)
    114 

/usr/local/lib/python3.6/dist-packages/tensorpack/graph_builder/model_desc.py in build_placeholder_reuse(self)
     64             return self._cached_placeholder[g]
     65         else:
---> 66             return self.build_placeholder()
     67 
     68     def _register_cached_placeholder(self, placeholder):

/usr/local/lib/python3.6/dist-packages/tensorpack/graph_builder/model_desc.py in build_placeholder(self)
     48         with tf.name_scope(None):   # clear any name scope it might get called in
     49             ret = tf.placeholder(
---> 50                 self.type, shape=self.shape, name=self.name)
     51         self._register_cached_placeholder(ret)
     52         return ret

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in placeholder(dtype, shape, name)
   1733                        ""eager execution."")
   1734 
-> 1735   return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
   1736 
   1737 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in placeholder(dtype, shape, name)
   4923     shape = _execute.make_shape(shape, ""shape"")
   4924     _, _, _op = _op_def_lib._apply_op_helper(
-> 4925         ""Placeholder"", dtype=dtype, shape=shape, name=name)
   4926     _result = _op.outputs[:]
   4927     _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    786                          input_types=input_types, attrs=attr_protos,
--> 787                          op_def=op_def)
    788       return output_structure, op_def.is_stateful, op
    789 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    452                 'in a future version' if date is None else ('after %s' % date),
    453                 instructions)
--> 454       return func(*args, **kwargs)
    455     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    456                                        _add_deprecated_arg_notice_to_docstring(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(***failed resolving arguments***)
   3125     del compute_shapes
   3126 
-> 3127     self._check_not_finalized()
   3128     for idx, a in enumerate(inputs):
   3129       if not isinstance(a, Tensor):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _check_not_finalized(self)
   2798     """"""
   2799     if self._finalized:
-> 2800       raise RuntimeError(""Graph is finalized and cannot be modified."")
   2801 
   2802   def _add_op(self, op):

RuntimeError: Graph is finalized and cannot be modified.

```",suggestion python class model self return none none self image label relax image image relax relax class self return class self self model model return model model got error log directory use delete previous run choose keep press key exit select action keep delete quit found data fork one time install guarantee found data install guarantee check recent call last module model print trainer input wrapper key return return wrapper self input setup may want register monitor well input input self input self input assert return self input wrapper key return return wrapper setup self use trigger true return self self input always safe reuse self input always safe reuse self return else return self self none clear name scope might get ret ret return ret shape name eager execution return shape name shape shape shape self name return future version date none else date return return enumerate tensor self raise graph self graph,issue,positive,positive,positive,positive,positive,positive
427512331,"They can both do (1). But in tensorflow you can do:
```
@tf.custom_gradient
def func(x, W, QW):
  o1 = f1(x, QW)
  o2 = f2(x, W)
  def grad(dy):
      return tf.gradients(o2, [x, W, QW], grad_ys=[dy])
  return o1, grad
```
I don't know if this is easy to do in pytorch or not.",grad return return grad know easy,issue,negative,positive,positive,positive,positive,positive
427511284,"Thanks for your input. I agree with you that backward pass of TF can become much flexible with tf.gradients and tf.custom_gradient. 

Regarding one of your comments:
> On this point I think TF is more flexible than any other frameworks. Others only allow you to do (1).

How do you think of Pytorch? I saw that some people also implement quantization schemes on Pytorch by manipulating the back-prop computation (e.g., [link](https://github.com/eladhoffer/quantized.pytorch)). My naive understanding is that one can explicitly define ""backward()"" function for any op, which is somewhat similar to tf.custom_gradients. I was wondering which one fundamentally allows more flexibility (or maybe they are equally good)?



",thanks input agree backward pas become much flexible regarding one point think flexible allow think saw people also implement quantization computation link naive understanding one explicitly define backward function somewhat similar wondering one fundamentally flexibility maybe equally good,issue,positive,positive,positive,positive,positive,positive
427506447,"I do not know how to achieve this without any super ugly path hacking, because `examples` is not under `tensorpack`.",know achieve without super ugly path hacking,issue,negative,negative,negative,negative,negative,negative
427503178,And it does not require a separate instantiation of variables. Variables can be created in side the decorated function and the documentation has mentioned how to do this.,require separate side decorated function documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
427502574,"In terms of the backward, you can either  (1) implement it yourself by calling `conv2d_transpose`, or (2) implement a new forward pass and call `tf.gradients`.

On this point I think TF is more flexible than any other frameworks. Others only allow you to do (1).",backward either implement calling implement new forward pas call point think flexible allow,issue,negative,positive,positive,positive,positive,positive
427501778,"
Yeah... I think the above pseudo code still has several problems. For example, it is not clear how to handle weight gradient computation. The above pseudo code does not compute weight gradients (dW), whereas typically instantiation of Conv2D would automatically call Conv2D_backpropInput and Conv2D_backpropFilter. So unless handled separately, the training would not work properly. Also, it would require separate instantiation of variables, which is tricky unless being very careful.

So, it seems to me that Tensorflow might have fundamental limitations with respect to flexibility in manipulating backward-pass computation. I was curious about your thoughts on this point. Would there be a good workaround in Tensorflow to enhance such flexibility?





",yeah think pseudo code still several example clear handle weight gradient computation pseudo code compute weight whereas typically would automatically call unless handled separately training would work properly also would require separate tricky unless careful might fundamental respect flexibility computation curious point would good enhance flexibility,issue,positive,positive,neutral,neutral,positive,positive
427497825,"Yes, something like that should work. These kind of things are usually not very easy to implement correctly so be careful.",yes something like work kind usually easy implement correctly careful,issue,positive,positive,positive,positive,positive,positive
427496463,"Then you need to also mark ""what"" to summarize following the tutorial.
When you create the summaries with `tf.summary.xxx`, tag it with a different collection. See https://www.tensorflow.org/api_docs/python/tf/summary/scalar.
Then call `MergeAllSummaries` with the non-default collection name.

The tutorial reads:
> MergeAllSummaries callback is in the default callbacks. It runs ops in the SUMMARIES collection (by default) every epoch (by default), and writes results to the monitors.

""by default"" implies that you can change it.",need also mark summarize following tutorial create tag different collection see call collection name tutorial default collection default every epoch default default change,issue,negative,neutral,neutral,neutral,neutral,neutral
427495890,The code was updated to use the same activation as alexnet to avoid confusion. It has similar performance.,code use activation avoid confusion similar performance,issue,negative,neutral,neutral,neutral,neutral,neutral
427494358,"Hi Yuxin,

Thanks for your input. Just to confirm my understanding, I think what you suggested would be similar to the following pseudo code?

@tf.custom_gradient
def convquant(x,w,wq):
  y = conv2d(x,wq) #  Use quantized weight in the forward pass
  def convquant_grad(dy):
    dx = conv2d_backpropInput(dy,w) #  Use original weight in the backward pass
    return dx
  return y, convquant_grad
",hi thanks input confirm understanding think would similar following pseudo code use weight forward pas use original weight backward pas return return,issue,positive,positive,positive,positive,positive,positive
427493446,"The thing is that I only want to monitor one tensor every batch. 
`MergeAllSummaries.period=1` will make all the other monitors called every batch too? 
I want to keep monitors like train_cost, val_cot called every epoch.",thing want monitor one tensor every batch make every batch want keep like every epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
427488132,"As a tutorial said: https://tensorpack.readthedocs.io/tutorial/summary.html, the logging frequency is controlled by the `MergeAllSummaries` callback.",tutorial said logging frequency,issue,negative,neutral,neutral,neutral,neutral,neutral
427417340,It will not be an input. It can be any tensor that you've previously defined.,input tensor previously defined,issue,negative,negative,negative,negative,negative,negative
427413544,"Thanks a lot.

If I use logic like `class A`, do I still have to claim a placeholder in `Model.inputs` like this ?
```python
class Model(ModelDesc):
    def inputs(self):
        return [tf.placeholder(tf.float32, [None, 32, 32, 3], 'input'),
                tf.placeholder(tf.int32, [None], 'label'),
                tf.placeholder(tf.float32, (), 'extra_para')]
```
I plan to implement it by 
```python
class Get_Extra_Para(object):
    def __init__(self):
          ....
    def step(self):
          some rule....
          return some value

extra_source = Get_Extra_Para()

class A(Callback):
    def _before_run(self, _):
        return tf.train.SessionRunArgs(fetches=[], 
                                     feed_dict={'extra_para:0': extra_source.step()})
```
Is above codes enough if I want to change `extra_para` every iteration ?",thanks lot use logic like class still claim like python class model self return none none plan implement python class object self step self rule return value class self return enough want change every iteration,issue,positive,positive,neutral,neutral,positive,positive
427264569,"Anything other than the training is done in the callback. Looking at https://tensorpack.readthedocs.io/tutorial/extend/callback.html, there are two ways you can add this logic:
```python
class A(Callback):
    def _before_run(self, _):
        return tf.train.SessionRunArgs(fetches=[], feed_dict={'learning_rate:0': np.random.rand()})

class B(Callback):
    def _setup_graph(self):
        self._lr = [k for k in tf.global_variables() if k.name == 'learning_rate:0'][0]
    def _trigger_step(self):
        self._lr.load(np.random.rand())
```",anything training done looking two way add logic python class self return class self self,issue,negative,neutral,neutral,neutral,neutral,neutral
427187792,"Thanks for the quick reply. 
close this question.",thanks quick reply close question,issue,negative,positive,positive,positive,positive,positive
427187233,"I don't remember any specific reasons we use abs() for SVHN. I think clip(abs(x), 0, 1) and clip(x, 0, 1) should both work well as an activation function.

IIRC it's just one of the things we tried and happened to use on SVHN but not AlexNet, for no particular reasons. ",remember specific use think clip clip work well activation function one tried use particular,issue,negative,positive,neutral,neutral,positive,positive
427081853,"Thanks for a quick response!! Let us also try TF 1.10. 

",thanks quick response let u also try,issue,negative,positive,positive,positive,positive,positive
427080740,"I can reproduce the issue, however the same thing works for TF 1.10.
This could be a bug in tensorflow and I'll investigate. For the moment please use 1.10 instead.",reproduce issue however thing work could bug investigate moment please use instead,issue,negative,neutral,neutral,neutral,neutral,neutral
427079688,"Tensorpack does not and will not provide any non-standard layers like this. Whatever models it supports will be implemented in tensorflow anyway so there is nothing you need from tensorpack.

The thing you described can be implemented with `custom_gradient`. It allows you to do any computation with any tensors in the backward. And in fact it's best to not involve tensorpack layers and implement your customized version of layers in this case, because you'll need to access the full-precision tensors in the gradient function. ",provide like whatever anyway nothing need thing computation backward fact best involve implement version case need access gradient function,issue,positive,positive,positive,positive,positive,positive
427063919,"I think the case I illustrated is **different** from simply customizing ops in the back-prop.

`tf.custom_gradient` allows change of **computation** in back-prop given the **same** input/output Tensors as arguments. The case I illustrated requires **different** input argument (i.e., full-prec weights)  to be used for the **same** computation in the back-prop.

I was wondering if there's any tensorpack functions / wrapper that can be used/extended to achieve the required change? 
",think case different simply change computation given case different input argument used computation wondering wrapper achieve change,issue,negative,neutral,neutral,neutral,neutral,neutral
427052148,"Locally I have a lot of copies of ""imagenet_utils.py"" around as well. Isn't there a time you want to modify GAN.py very quickly but don't want to affect the other code? That's why I prefer copying myself.",locally lot around well time want modify quickly want affect code prefer,issue,negative,positive,positive,positive,positive,positive
427050758,"`find . -name ""GAN.py"" | wc -l` is **23**. And that's definitely not the best solution yet. 

I could life with a big read blinking warning my the terminal saying that "" import tensorpack.examples is highly unstable"" and using it ""will initiate the end of the world.""
",find definitely best solution yet could life big read blinking warning terminal saying import highly unstable initiate end world,issue,positive,positive,positive,positive,positive,positive
427046499,"I would recommend you to copy them because those code can change in any incompatible ways. This is how I work now. I would not want to see a new user starting to import these.
What about a huge warning at import saying these code can break?
Or an alternative: allow the import, but don't include any example files in `pip install`. In this way a normal user will have no chance relying on these code with no compatibility. And only developers (which use tensorpack locally and hopefully know the risks) are able to import it. ",would recommend copy code change incompatible way work would want see new user starting import huge warning import saying code break alternative allow import include example pip install way normal user chance code compatibility use locally hopefully know able import,issue,positive,positive,positive,positive,positive,positive
427045260,"I believe this is a tensorflow question (i.e. how to build such a graph). You'll need to customize the gradients of quantized convolution, just like how the gradient of quantization is customized now in `dorefa.py`. You can refer to `tf.custom_gradient` to learn more.",believe question build graph need convolution like gradient quantization refer learn,issue,negative,neutral,neutral,neutral,neutral,neutral
427040855,"The warning said  ""The following variables are in the checkpoint, but not found in the graph"". It means some variables are used in training but not found (i.e. not used) in inference, which is OK.
It would not be OK if it's the other way around.",warning said following found graph used training found used inference would way around,issue,negative,neutral,neutral,neutral,neutral,neutral
427004940,"@ppwwyyxx based on this code, i trained my own data.the framework used cascade-rcnn,but the model i trained could not test my data,the returned result was empty. the hint was missing variables. i did not know if it was a problem with the model or a problem with the training code.there was a hint on it.
this is my config:./train.py --config \
    MODE_MASK=True MODE_FPN=True FPN.CASCADE=True BACKBONE.RESNET_NUM_BLOCK=[3,4,23,3] \
    DATA.BASEDIR=./data/coco/ \
    BACKBONE.WEIGHTS=./data/models/COCO-R101FPN-MaskRCNN-BetterParams.npz \
    TEST.RESULT_SCORE_THRESH=1e-4 \
    PREPROC.TRAIN_SHORT_EDGE_SIZE=[640,800] \
    TRAIN.LR_SCHEDULE=[420000,500000,540000]",based code trained framework used model trained could test data returned result empty hint missing know problem model problem training hint,issue,negative,negative,negative,negative,negative,negative
426887730,"I'm not sure what you're asking. If you're asking about why the model does not perform well on your data, we do not answer such questions because it's a general machine learning / computer vision question and not necessarily related to tensorpack.",sure model perform well data answer general machine learning computer vision question necessarily related,issue,positive,positive,positive,positive,positive,positive
426720126,"k bits can represent 2^k different numbers.
2 bits can represent 4 different numbers.
And we choose those numbers to be {0, 1/3, 2/3, 1}.",represent different represent different choose,issue,negative,neutral,neutral,neutral,neutral,neutral
426711371,"Hi  @ppwwyyxx .

Sorry , I confused about  2 bit would return values in {0,1/3,2/3,1}.
in readme file , I saw the text below , However in hardware implementation it can't use 2 bits two represent 1/3,2/3(binary format can't use 2 bits to represent)

![image](https://user-images.githubusercontent.com/31586393/46425618-b877f780-c76e-11e8-8a70-95472ffc069a.png)

take 2.34669849e-01 for example:
using 2^k-1 :
weight_bit=2 the value is equal to 0.3333333,but actually 0.33 can't use two bits to represent
using 2^k :
weight_bit=2 the value is equal to 0.25, it can use two bits (0.11) to represent
So why don't use 2^k? Did I make a mistake? thanks.



",hi sorry confused bit would return file saw text however hardware implementation ca use two represent binary format ca use represent image take example value equal actually ca use two represent value equal use two represent use make mistake thanks,issue,negative,negative,negative,negative,negative,negative
426115716,"Maybe you should try to check whether you have installed `cv2`. 
If not, image augmentation is not available.",maybe try check whether image augmentation available,issue,negative,positive,positive,positive,positive,positive
425555909,"@ppwwyyxx : Thank you for the answer. Yes, by *.pb I mean the pn files created by tensorflow. But the API of the tools is really bad :( especially in tripping out all the unused nodes in inference modes. For example, I don't know how to remove (not turn off) the drop outs to make the graph size smaller.",thank answer yes mean really bad especially tripping unused inference example know remove turn drop make graph size smaller,issue,negative,negative,negative,negative,negative,negative
425534922,"You should in general read a file by referring to the library that creates the file.
Specifically, if you mean ""pb files"" created by tensorflow, then tensorflow will have the tool to read it, not tensorpack.",general read file library file specifically mean tool read,issue,negative,negative,negative,negative,negative,negative
425503982,"@ppwwyyxx : Does tensorpack support reading the *.pb files? If not, could you recommend other tools that can also extract the nets' weights to numpy arrays? Many thanks in advance!",support reading could recommend also extract many thanks advance,issue,positive,positive,positive,positive,positive,positive
425442550,"```diff
diff --git i/examples/basics/cifar-convnet.py w/examples/basics/cifar-convnet.py
index 81e59e2c..2339c098 100755
--- i/examples/basics/cifar-convnet.py
+++ w/examples/basics/cifar-convnet.py
@@ -45,7 +45,8 @@ class Model(ModelDesc):
 
         image = image / 4.0     # just to make range smaller
         with argscope(Conv2D, activation=BNReLU, use_bias=False, kernel_size=3), \
-                argscope([Conv2D, MaxPooling, BatchNorm], data_format=data_format):
+                argscope([Conv2D, MaxPooling, BatchNorm], data_format=data_format), \
+                argscope([BatchNorm], sync_statistics='nccl'):
             logits = LinearWrap(image) \
                 .Conv2D('conv1.1', filters=64) \
                 .Conv2D('conv1.2', filters=64) \
```
The above change on the cifar example (which uses `SyncMultiGPUTrainerParameterServer`) can run well. Does it run on your machine?",git index class model image image make range smaller image change example run well run machine,issue,negative,neutral,neutral,neutral,neutral,neutral
425340926,"Hi, @ppwwyyxx 
I tried `SyncMultiGPUTrainerParameterServer` with a naive model and constant layername like you said, the code is showed below. It just stuck (I waited for like 5 minutes) and when changed to `SyncMultiGPUTrainerReplicated`, the training process is smoothly running. This seems mysterious.
```
    def get_fake_logits(self, images):
        '''
        Fake logits for testing.
        '''  
        net = Conv2D('fake_conv', images, filters=19, kernel_size=1, strides=1)
        net = BatchNorm('fake_bn', net,  sync_statistics=""nccl"")
        logits = PReLU('fake_prelu', net)
               
        return logits
```
While all variables in my model is under variable scope 'xxnetTensorpack', training now could run without any warning about syncBN using `SyncMultiGPUTrainerReplicated`, should I delete this variable scope ? 
Or should I try deleting variable scope and `ParameterServer`? Just tried, did not work.",hi tried naive model constant like said code stuck like training process smoothly running mysterious self fake net net net net return model variable scope training could run without warning delete variable scope try variable scope tried work,issue,negative,negative,neutral,neutral,negative,negative
425156860,"It would still be interesting to know why it does not work for you when using `ParameterServer`.
Can you try `ParameterServer` with a naive model?
Can you make sure that the layername in your model does not depend on other factors such as variable scope? Different GPUs have to have those BatchNorm layers under the same name to be able to sync statistics.",would still interesting know work try naive model make sure model depend variable scope different name able sync statistic,issue,negative,positive,positive,positive,positive,positive
425061628,"I tested `SyncMultiGPUTrainerReplicated` with a naive model and `sync_statistics`, it works ! And my model also works ! 
Thanks so much, I may close this issue now !",tested naive model work model also work thanks much may close issue,issue,negative,positive,neutral,neutral,positive,positive
425002260,"I started to see the same issue after an upgrade of tensorflow. The fix is :
```python
from tensorflow.contrib.nccl.python.ops.nccl_ops import _validate_and_load_nccl_so
_validate_and_load_nccl_so()
```
",see issue upgrade fix python import,issue,negative,neutral,neutral,neutral,neutral,neutral
425001292,"It doesn't matter.
With `ParameterServer` I'm still able to use `sync_statistics`.

Could you try a naive model with only one batchnorm layer and please make sure you're using a constant layer name (`BatchNorm('layername', inputs)`) ?",matter still able use could try naive model one layer please make sure constant layer name,issue,negative,positive,positive,positive,positive,positive
424992517,"Hi, @ppwwyyxx , I debug my model and find that the dataflow stuck in `BatchNorm()`, in which I use `sync_statistics='nccl'` with `SyncMultiGPUTrainerParameterServer`. When I set `sync_statistics=None`, the training works. 
SyncBN is the most attracting feature for me, what can I do to make it work with SyncBN? Thanks !!",hi model find stuck use set training work feature make work thanks,issue,negative,positive,positive,positive,positive,positive
424926649,"Confirmed that this is not a tensorpack issue. Closing now.
We are sorry for your problems but this is not the place for tensorflow questions.",confirmed issue sorry place,issue,negative,negative,neutral,neutral,negative,negative
424925348,"I am referring to the source code of tensorpack, wrote some code, may have some errors about NCCL, I hope someone can answer.",source code wrote code may hope someone answer,issue,negative,neutral,neutral,neutral,neutral,neutral
424924698,This issue seems unrelated to tensorpack because it sounds like an issue running your own code.,issue unrelated like issue running code,issue,negative,neutral,neutral,neutral,neutral,neutral
424923912,"PS:
my nccl version:

apt list --installed | grep nccl

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

libnccl-dev/unknown,now 2.2.13-1+cuda9.0 amd64 [installed,upgradable to: 2.2.13-1+cuda9.2]
libnccl2/unknown,now 2.2.13-1+cuda9.0 amd64 [installed,upgradable to: 2.2.13-1+cuda9.2]
",version apt list warning apt stable interface use caution,issue,negative,positive,positive,positive,positive,positive
424913329,After image_dtype set to float32 and using ResNet50 I observed about 15% improvement for imagenet evaluation.,set float improvement evaluation,issue,negative,neutral,neutral,neutral,neutral,neutral
424912378,"How much faster is the new `FeedfreePredictor`?

```
    # This does not have a visible improvement over naive predictor,
    # but will have an improvement if image_dtype is set to float32.
```",much faster new visible improvement naive predictor improvement set float,issue,positive,positive,neutral,neutral,positive,positive
424630398,"1.RealData, RealLogits --> Stuck
2.`FakeData`, RealLogits --> Stuck
3.RealData, FakeLogits --> Work !
The model logits part should have bugs ! I will check it ! 
Thanks so much !",stuck stuck work model part check thanks much,issue,negative,positive,positive,positive,positive,positive
424589615,"If `FakeData` has the same issue, next step is to replace your model with a trivial model (e.g. `return tf.reduce_sum(input + tf.get_variable('var', shape=[], dtype=tf.float32))`.
If the problem still exists, please post significant parts in the rest of code because I assume there isn't much code left after removing data and model.

Also, please make sure to try with the latest tensorpack.",issue next step replace model trivial model return input problem still please post significant rest code assume much code left removing data model also please make sure try latest,issue,positive,positive,positive,positive,positive,positive
424582228,"@ppwwyyxx Thank you so much for the quick reply !! I will check my code, and come back bringing more details. 
A little question: What does `print(next(ds.get_data()))` printing out numpy array mean ? My understanding is that the dataflow is running correctly.

I just use `FakeData` to test, the issue still exists.",thank much quick reply check code come back little question print next printing array mean understanding running correctly use test issue still,issue,negative,negative,neutral,neutral,negative,negative
424578092,"Without code I can't tell much about the issue. A stuck can be caused by issues with your data, your model, or some unexpected use of the library (e.g. wrong use of StagingInput). I would recommend to replace your dataflow by `FakeData` to see if your issues still exist or start from a working example to see where things went wrong.

For your dataflow testing: it should run until the end of your dataflow (which is the length of your list divide by the batch size). You can make a dataflow infinite by `RepeatedData(ds, -1)`, which is automatically done if you use a dataflow for training.",without code ca tell much issue stuck data model unexpected use library wrong use would recommend replace see still exist start working example see went wrong testing run end length list divide batch size make infinite automatically done use training,issue,negative,negative,negative,negative,negative,negative
424407223,"thanks , I'll share if I get better results while playing with parameters",thanks share get better,issue,positive,positive,positive,positive,positive,positive
424394457,"According to the [tutorial](https://tensorpack.readthedocs.io/tutorial/summary.html), the quickest way is to use [this callback](https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.MergeAllSummaries).
Note that by default this callback controls all the summaries. If you only want to customize certain summaries, you'll need to add those summaries into a different collection: https://www.tensorflow.org/api_docs/python/tf/summary/scalar",according tutorial way use note default want certain need add different collection,issue,negative,positive,positive,positive,positive,positive
424386518,What parameter to use is up to you. You're welcome to share new results with different parameters.,parameter use welcome share new different,issue,positive,positive,positive,positive,positive,positive
424384954,See the tutorial on summary and logging.,see tutorial summary logging,issue,negative,neutral,neutral,neutral,neutral,neutral
424039860,"Examples that use `augment` to augment image and segmentation: #214 
Examples that use `augment_coords` to augment image and segmentation: https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN",use augment augment image segmentation use augment image segmentation,issue,negative,neutral,neutral,neutral,neutral,neutral
424023744,Or you can just use `augment` if your label is image..,use augment label image,issue,negative,neutral,neutral,neutral,neutral,neutral
423835552,@ppwwyyxx Thanks. I think it might be easier by directly loading binary masks and changing the augmentation code.,thanks think might easier directly loading binary augmentation code,issue,positive,positive,positive,positive,positive,positive
423835256,"And again, if your data is originally in the form of masks, you can just augment all the pixels directly without converting to polygons.",data originally form augment directly without converting,issue,negative,positive,positive,positive,positive,positive
423835179,That again is just a problem with the dataset. You can apparently represent a donut by two polygons: the left part and the right part or the donut. If your data is not annotated like that there is nothing I can do.,problem apparently represent two left part right part data like nothing,issue,negative,positive,positive,positive,positive,positive
423835002,"@ppwwyyxx Take a donut as an example, it has two polygons(inner and outer contours found by opencv or skimage). So segmentation_to_mask would fail. Do you mean I need to change the polygons to contain ones only(for example by adding a virtual line between inner and outer contour)? That would be really hacky for complicated objects. Do you have any idea about it?",take example two inner outer found would fail mean need change contain example virtual line inner outer contour would really hacky complicated idea,issue,negative,negative,negative,negative,negative,negative
423834306,"If you use only one polygon for the object with holes then that's a problem of the dataset.
Like I said any subset of pixels in an image can be represented by a list of polygons ",use one polygon object problem like said subset image list,issue,negative,neutral,neutral,neutral,neutral,neutral
423834231,"@ppwwyyxx I mean the function segmentation_to_mask will merge the polygon of the hole, so in the resulted mask, the hole part is filled by 1 but 0 is expected. That’s my problem.",mean function merge polygon hole mask hole part filled problem,issue,negative,positive,neutral,neutral,positive,positive
423833818,"Any subset of pixels in an image can be represented by a list of polygons (in the extreme case you can represent each pixel by one polygon), so there should be no issues with the use of polygon. I do not see how the line of code you refer to is related to the discussion.

If you want to use masks for augmentation you can still augment each pixel in the mask.",subset image list extreme case represent one polygon use polygon see line code refer related discussion want use augmentation still augment mask,issue,negative,negative,neutral,neutral,negative,negative
423792607,"Like it said you cannot do multiple processing within a MPI process.
You can start separate processes that run data processing and send the data to the training process. This is done in the example here https://github.com/tensorpack/benchmarks/tree/master/ResNet-Horovod.",like said multiple within process start separate run data send data training process done example,issue,negative,neutral,neutral,neutral,neutral,neutral
423566348,"The readme has a mistake.

I cannot find the number 18000 in DDQN paper, but let's take the code as ground truth.",mistake find number paper let take code ground truth,issue,negative,neutral,neutral,neutral,neutral,neutral
423565730,"This is an issue I fixed before but forgot to take a note about it. Unfortunately it was introduced again lately..
I will think about how to properly address this. ",issue fixed forgot take note unfortunately lately think properly address,issue,negative,negative,neutral,neutral,negative,negative
423554873,You can let weight decay be a tensor that's computed from global step (tf.train.get_or_create_global_step IIRC). This is unrelated to tensorpack.,let weight decay tensor global step unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
423446087,"@ppwwyyxx 
if I want to create a variable with L2-regularizer, but I want to let the weight_decay to decreases over global_steps (not simply a constant number value):

```
new_v = tf.get_variable(
                'new_v', [1,1], tf.float32,
                initializer=new_v_init,
                trainable=True)
                regularizer=tf.contrib.layers.l2_regularizer(weight_decay))
```
Is this supported in tensorpack?",want create variable want let simply constant number value,issue,positive,neutral,neutral,neutral,neutral,neutral
423444443,Merge both classes into `ModelExporter` and renamed *mobile* to *compact* to represent a compressed and self-contained graph. The example is updated as well to reflect those changes.,merge class mobile compact represent compressed graph example well reflect,issue,negative,neutral,neutral,neutral,neutral,neutral
423410168,"I ran some tests with the `cifar-convnet.py` example. After those changes I'm not able to get deterministic results either, and observed similar gap in validation error.
However running the same code on CPU gave me deterministic results (up to the very last digit).
So I guess it's just how the ops work on GPU and there isn't much we can do in tensorpack.",ran example able get deterministic either similar gap validation error however running code gave deterministic last digit guess work much,issue,negative,positive,positive,positive,positive,positive
423404828,"The ':0' is probably not an issue (although the warnings are indeed confusing, and should be improved). The issue is you have ""-"" in the graph but ""_"" in the checkpoint as you can see from the warnings.

As for the shapes, I recommend you to check more carefully, and copy your old model code as much as possible.",probably issue although indeed issue graph see recommend check carefully copy old model code much possible,issue,negative,neutral,neutral,neutral,neutral,neutral
423218199,"The features to be added to the library by this PR seems to be just: exporting the model to saved_model format (for serving) or a pruned graph format.
I think the features make sense as part of a training framework because the output can be seen as a result of the training. After exporting to standard format a user can then do other fancy stuff such as toco or tensorrt conversion which I don't want to include in tensorpack.
For implementation I think it's simpler to just have the old name (`ModelExporter`) and two methods for two different formats. Also the pruned graph does not seem to be very related to ""mobile"" so maybe a better name is needed?",added library model format serving graph format think make sense part training framework output seen result training standard format user fancy stuff toco conversion want include implementation think simpler old name two two different also graph seem related mobile maybe better name,issue,negative,positive,positive,positive,positive,positive
423163043,To repeat: This does not apply the TOCO (which indeed does *not* work). It mainly focuses on how the TensorFlow-Serving export does work. The tf-mobile is just there because it was only a matter of a few minutes.,repeat apply toco indeed work mainly export work matter,issue,negative,positive,positive,positive,positive,positive
422328192,"`MultiGPUGANTrainer` automatically adds `StagingInput`. You have to remove `StagingInput` in your data because `StagingInput` cannot be nested together.
After the above commit it should now throw an error about this situation.",automatically remove data together commit throw error situation,issue,negative,neutral,neutral,neutral,neutral,neutral
422315612,Please post any problems following the issue template.,please post following issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
422311500,"1. It is from personal communication with the author. It helps a bit.
2. https://github.com/tensorpack/tensorpack/blob/0dbcbac7ae5dc62f89ecdbe0961731513c11f2ec/examples/FasterRCNN/config.py#L181",personal communication author bit,issue,negative,neutral,neutral,neutral,neutral,neutral
422310334,"@ppwwyyxx 
after changing that to 32, this error happens:
```
2018-09-18 16:50:16.397043: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_1_DataParallelInferenceRunner/QueueInput/input_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: DataParallelInferenceRunner/QueueInput/input_deque_7 = QueueDequeueV2[component_types=[DT_UINT8, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](DataParallelInferenceRunner/QueueInput/input_queue)]]
```",error range closed insufficient current size node,issue,negative,negative,neutral,neutral,negative,negative
422055328,"The last one is correct. If something fails, please post the issue following the issue template.",last one correct something please post issue following issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
421938026,"If weights in your model are fp16 then you certainly need to load fp16 weights.

> Do I have to load the numpy dict in python and then cast to numpy float 16?

yes.",model certainly need load load python cast float yes,issue,positive,positive,positive,positive,positive,positive
421356807,You can use inputs with known batch size. The batch size of the tensor is the batch size of your dataflow.,use known batch size batch size tensor batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
421339867,"@ppwwyyxx 
Given this custom layer:
```
@layer_register()
def CustomOp(x):
   ........
```
and x is the activations between convolutional layers. But if I print the shape of x, the results are usually
something like :(none, 64,56,56) , which means the first dimension--batch_size dimension is unknown. But in my code, I need to reshape x to dimension[-1,1] and do other related operations. But after reshaping, the first dimension is still ""none"" and this causes error in my following operations due to slicing. So is there any methods that can solve my issues? If total_batch_size is 256, and I am using 8 gpus, is this ""none""--batch_size-dimension 256 or 256/8=32? 

################################################
I noticed that this problem is cause by https://github.com/Microsoft/LQ-Nets/blob/master/imagenet_utils.py#L194
```
    def _get_inputs(self):
        return [InputDesc(self.image_dtype, [None, self.image_shape, self.image_shape, 3], 'input'),
                InputDesc(tf.int32, [None], 'label')]
```
So if my batch_size is 256 and I am using 8 gpus,  what number should I set in the above ""None""? 
256 or 32?",given custom layer convolutional print shape usually something like none first dimension dimension unknown code need reshape dimension related first dimension still none error following due slicing solve none problem cause self return none none number set none,issue,positive,positive,neutral,neutral,positive,positive
421287209,"learning rate is a variable defined in `optimizer()`. You can define it in `build_graph()` instead and access it in `optimizer()`.

The current exploration prob is not a tensorflow variable so there is no way to access it in the graph. You need to modify many code to do that.",learning rate variable defined define instead access current exploration prob variable way access graph need modify many code,issue,negative,positive,positive,positive,positive,positive
420804230,"There is no concept of image or batch in dataflow. It just gives you ""datapoints"", whatever it is.",concept image batch whatever,issue,negative,neutral,neutral,neutral,neutral,neutral
420803602,"Yeah, now I realized this logic. It would be nice to write this down somewhere in the documentation. I did not expect that the it/s changes the meaning from images/sec into batches/sec.",yeah logic would nice write somewhere documentation expect meaning,issue,positive,positive,positive,positive,positive,positive
420802599,"Considering that 150/512=0.3, 460/512=0.9, your speed numbers look just reasonable. What do you expect?",considering speed look reasonable expect,issue,negative,positive,positive,positive,positive,positive
420518256,"> I found the same error.
> If install tensorpack by `pip install tensorpack` this will return this error.
> So to solve this error, must use github code.

thanks! it works!",found error install pip install return error solve error must use code thanks work,issue,negative,positive,positive,positive,positive,positive
420448209,"Well, now it works. Thanks for your help.",well work thanks help,issue,positive,positive,positive,positive,positive,positive
420442706,Did you try Ctrl+F5. I double checked it. I can see the page.,try double checked see page,issue,negative,neutral,neutral,neutral,neutral,neutral
420414992,"@PatWie  This link still does not work for me  http://models.tensorpack.com/caffe/ 
Anyway, I find the one cached by google search engine working for me. http://webcache.googleusercontent.com/search?q=cache:ox2CCzCHrPkJ:models.tensorpack.com/+&cd=1&hl=en&ct=clnk&gl=be",link still work anyway find one search engine working,issue,negative,neutral,neutral,neutral,neutral,neutral
420315843,This link seems not work now. Could you help to address this issue? Thanks.,link work could help address issue thanks,issue,positive,positive,positive,positive,positive,positive
419688245,"(tensorpack) C:\Users\cynth\Documents\Mini3\Scripts>pip3 --version
pip 18.0 from c:\users\cynth\documents\mini3\envs\tensorpack\lib\site-packages\pip (python 3.6)

(tensorpack) C:\Users\cynth\Documents\Mini3\Scripts>pip3 --verbose install --upgrade git+https://github.com/tensorpack/tensorpack.git
Config variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect
Config variable 'WITH_PYMALLOC' is unset, Python ABI tag may be incorrect
Created temporary directory: C:\Users\cynth\AppData\Local\Temp\pip-ephem-wheel-cache-_cvvlhug
Created temporary directory: C:\Users\cynth\AppData\Local\Temp\pip-req-tracker-i80u1q2z
Created requirements tracker 'C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-tracker-i80u1q2z'
Created temporary directory: C:\Users\cynth\AppData\Local\Temp\pip-install-b5w7vwrp
Collecting git+https://github.com/tensorpack/tensorpack.git
  Created temporary directory: C:\Users\cynth\AppData\Local\Temp\pip-req-build-_x5nasqf
  Cloning https://github.com/tensorpack/tensorpack.git to c:\users\cynth\appdata\local\temp\pip-req-build-_x5nasqf
  Running command git clone -q https://github.com/tensorpack/tensorpack.git C:\Users\cynth\AppData\Local\Temp\pip-req-build-_x5nasqf
  Added git+https://github.com/tensorpack/tensorpack.git to build tracker 'C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-tracker-i80u1q2z'
  Running setup.py (path:C:\Users\cynth\AppData\Local\Temp\pip-req-build-_x5nasqf\setup.py) egg_info for package from git+https://github.com/tensorpack/tensorpack.git
    Running command python setup.py egg_info
    running egg_info
    creating pip-egg-info\tensorpack.egg-info
    writing pip-egg-info\tensorpack.egg-info\PKG-INFO
    writing dependency_links to pip-egg-info\tensorpack.egg-info\dependency_links.txt
    writing requirements to pip-egg-info\tensorpack.egg-info\requires.txt
    writing top-level names to pip-egg-info\tensorpack.egg-info\top_level.txt
    writing manifest file 'pip-egg-info\tensorpack.egg-info\SOURCES.txt'
    error: package directory 'find:                             # will call find_packages()' does not exist
Cleaning up...
  Removing source in C:\Users\cynth\AppData\Local\Temp\pip-req-build-_x5nasqf
Removed git+https://github.com/tensorpack/tensorpack.git from build tracker 'C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-tracker-i80u1q2z'
Removed build tracker 'C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-tracker-i80u1q2z'
Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\cynth\AppData\Local\Temp\pip-req-build-_x5nasqf\
Exception information:
Traceback (most recent call last):
  File ""c:\users\cynth\documents\mini3\envs\tensorpack\lib\site-packages\pip\_internal\basecommand.py"", line 141, in main
    status = self.run(options, args)
  File ""c:\users\cynth\documents\mini3\envs\tensorpack\lib\site-packages\pip\_internal\commands\install.py"", line 299, in run
    resolver.resolve(requirement_set)
  File ""c:\users\cynth\documents\mini3\envs\tensorpack\lib\site-packages\pip\_internal\resolve.py"", line 102, in resolve
    self._resolve_one(requirement_set, req)
  File ""c:\users\cynth\documents\mini3\envs\tensorpack\lib\site-packages\pip\_internal\resolve.py"", line 256, in _resolve_one
    abstract_dist = self._get_abstract_dist_for(req_to_install)
  File ""c:\users\cynth\documents\mini3\envs\tensorpack\lib\site-packages\pip\_internal\resolve.py"", line 209, in _get_abstract_dist_for
    self.require_hashes
  File ""c:\users\cynth\documents\mini3\envs\tensorpack\lib\site-packages\pip\_internal\operations\prepare.py"", line 298, in prepare_linked_requirement
    abstract_dist.prep_for_dist(finder, self.build_isolation)
  File ""c:\users\cynth\documents\mini3\envs\tensorpack\lib\site-packages\pip\_internal\operations\prepare.py"", line 126, in prep_for_dist
    self.req.run_egg_info()
  File ""c:\users\cynth\documents\mini3\envs\tensorpack\lib\site-packages\pip\_internal\req\req_install.py"", line 636, in run_egg_info
    command_desc='python setup.py egg_info')
  File ""c:\users\cynth\documents\mini3\envs\tensorpack\lib\site-packages\pip\_internal\utils\misc.py"", line 701, in call_subprocess
    % (command_desc, proc.returncode, cwd))
pip._internal.exceptions.InstallationError: Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\cynth\AppData\Local\Temp\pip-req-build-_x5nasqf\

(tensorpack) C:\Users\cynth\Documents\Mini3\Scripts>",pip version pip python pip verbose install upgrade variable unset python tag may incorrect variable unset python tag may incorrect temporary directory temporary directory tracker temporary directory temporary directory running command git clone added build tracker running path package running command python running writing writing writing writing writing manifest file error package directory call exist cleaning removing source removed build tracker removed build tracker command python error code exception information recent call last file line main status file line run file line resolve file line file line file line finder file line file line file line command python error code,issue,negative,positive,neutral,neutral,positive,positive
419687961,"(tensorpack) C:\Users\cynth\Documents\Mini3\Scripts>pip --verbose install --upgrade git+https://github.com/tensorpack/tensorpack.git
Config variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect
Config variable 'WITH_PYMALLOC' is unset, Python ABI tag may be incorrect
Created temporary directory: C:\Users\cynth\AppData\Local\Temp\pip-ephem-wheel-cache-67gp1d2g
Created temporary directory: C:\Users\cynth\AppData\Local\Temp\pip-install-1pqi0g_r
Collecting git+https://github.com/tensorpack/tensorpack.git
  Created temporary directory: C:\Users\cynth\AppData\Local\Temp\pip-req-build-2c09im7c
  Cloning https://github.com/tensorpack/tensorpack.git to c:\users\cynth\appdata\local\temp\pip-req-build-2c09im7c
  Running command git clone -q https://github.com/tensorpack/tensorpack.git C:\Users\cynth\AppData\Local\Temp\pip-req-build-2c09im7c
  Running setup.py (path:C:\Users\cynth\AppData\Local\Temp\pip-req-build-2c09im7c\setup.py) egg_info for package from git+https://github.com/tensorpack/tensorpack.git
    Running command python setup.py egg_info
    running egg_info
    creating pip-egg-info\tensorpack.egg-info
    writing pip-egg-info\tensorpack.egg-info\PKG-INFO
    writing dependency_links to pip-egg-info\tensorpack.egg-info\dependency_links.txt
    writing requirements to pip-egg-info\tensorpack.egg-info\requires.txt
    writing top-level names to pip-egg-info\tensorpack.egg-info\top_level.txt
    writing manifest file 'pip-egg-info\tensorpack.egg-info\SOURCES.txt'
    error: package directory 'find:                             # will call find_packages()' does not exist
Cleaning up...
  Removing source in C:\Users\cynth\AppData\Local\Temp\pip-req-build-2c09im7c
Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\cynth\AppData\Local\Temp\pip-req-build-2c09im7c\
Exception information:
Traceback (most recent call last):
  File ""C:\Users\cynth\Documents\Mini3\lib\site-packages\pip\_internal\basecommand.py"", line 228, in main
    status = self.run(options, args)
  File ""C:\Users\cynth\Documents\Mini3\lib\site-packages\pip\_internal\commands\install.py"", line 291, in run
    resolver.resolve(requirement_set)
  File ""C:\Users\cynth\Documents\Mini3\lib\site-packages\pip\_internal\resolve.py"", line 103, in resolve
    self._resolve_one(requirement_set, req)
  File ""C:\Users\cynth\Documents\Mini3\lib\site-packages\pip\_internal\resolve.py"", line 257, in _resolve_one
    abstract_dist = self._get_abstract_dist_for(req_to_install)
  File ""C:\Users\cynth\Documents\Mini3\lib\site-packages\pip\_internal\resolve.py"", line 210, in _get_abstract_dist_for
    self.require_hashes
  File ""C:\Users\cynth\Documents\Mini3\lib\site-packages\pip\_internal\operations\prepare.py"", line 324, in prepare_linked_requirement
    abstract_dist.prep_for_dist(finder, self.build_isolation)
  File ""C:\Users\cynth\Documents\Mini3\lib\site-packages\pip\_internal\operations\prepare.py"", line 154, in prep_for_dist
    self.req.run_egg_info()
  File ""C:\Users\cynth\Documents\Mini3\lib\site-packages\pip\_internal\req\req_install.py"", line 486, in run_egg_info
    command_desc='python setup.py egg_info')
  File ""C:\Users\cynth\Documents\Mini3\lib\site-packages\pip\_internal\utils\misc.py"", line 698, in call_subprocess
    % (command_desc, proc.returncode, cwd))
pip._internal.exceptions.InstallationError: Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\cynth\AppData\Local\Temp\pip-req-build-2c09im7c\
You are using pip version 10.0.1, however version 18.0 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.

(tensorpack) C:\Users\cynth\Documents\Mini3\Scripts>",pip verbose install upgrade variable unset python tag may incorrect variable unset python tag may incorrect temporary directory temporary directory temporary directory running command git clone running path package running command python running writing writing writing writing writing manifest file error package directory call exist cleaning removing source command python error code exception information recent call last file line main status file line run file line resolve file line file line file line finder file line file line file line command python error code pip version however version available consider via pip install upgrade pip command,issue,negative,positive,positive,positive,positive,positive
419687647,"(tensorpack) C:\Users\cynth\Documents\Mini3\Scripts>python -m pip install --upgrade pip
Collecting pip
  Using cached https://files.pythonhosted.org/packages/5f/25/e52d3f31441505a5f3af41213346e5b6c221c9e086a166f3703d2ddaf940/pip-18.0-py2.py3-none-any.whl
Installing collected packages: pip
  Found existing installation: pip 10.0.1
    Uninstalling pip-10.0.1:
      Successfully uninstalled pip-10.0.1
Successfully installed pip-18.0

(tensorpack) C:\Users\cynth\Documents\Mini3\Scripts>pip install --upgrade git+https://github.com/tensorpack/tensorpack.git
Collecting git+https://github.com/tensorpack/tensorpack.git
  Cloning https://github.com/tensorpack/tensorpack.git to c:\users\cynth\appdata\local\temp\pip-req-build-4qxs6zaz
    Complete output from command python setup.py egg_info:
    running egg_info
    creating pip-egg-info\tensorpack.egg-info
    writing pip-egg-info\tensorpack.egg-info\PKG-INFO
    writing dependency_links to pip-egg-info\tensorpack.egg-info\dependency_links.txt
    writing requirements to pip-egg-info\tensorpack.egg-info\requires.txt
    writing top-level names to pip-egg-info\tensorpack.egg-info\top_level.txt
    writing manifest file 'pip-egg-info\tensorpack.egg-info\SOURCES.txt'
    error: package directory 'find:                             # will call find_packages()' does not exist

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\cynth\AppData\Local\Temp\pip-req-build-4qxs6zaz\
You are using pip version 10.0.1, however version 18.0 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.

(tensorpack) C:\Users\cynth\Documents\Mini3\Scripts>python --version
Python 3.6.6 :: Anaconda, Inc.
",python pip install upgrade pip pip collected pip found installation pip successfully uninstalled successfully pip install upgrade complete output command python running writing writing writing writing writing manifest file error package directory call exist command python error code pip version however version available consider via pip install upgrade pip command python version python anaconda,issue,negative,positive,positive,positive,positive,positive
419613441,"You're right. To be more precise it should handle the case of zero. But the probability the weight become exactly zero is very small, so in practice it does not make a big difference.",right precise handle case zero probability weight become exactly zero small practice make big difference,issue,negative,positive,positive,positive,positive,positive
418753182,"Last time I tried those graph transform, it does not do what I want (fuse BN into conv layers) and I end up rewriting the graph and transformed the weights manually.",last time tried graph transform want fuse end graph manually,issue,negative,neutral,neutral,neutral,neutral,neutral
418652430,"I do not really refer to tf-lite or tf-mobile. I refer to a frozen graph (everything is constant) and pruned graph (only nodes necessary for inference). They do not need to be deployed on mobile devices.
In all my cases this worked perfectly and I can restore even graphs which I trained several months ago without any issues.

So far, the documentation is missing but the minimal working example probably shows the most important things.",really refer refer frozen graph everything constant graph necessary inference need mobile worked perfectly restore even trained several ago without far documentation missing minimal working example probably important,issue,positive,positive,positive,positive,positive,positive
418435389,Honestly I don't like the tf-lite and mobile thing in general because my impression is that they are premature and won't work well for any actual model I care about..,honestly like mobile thing general impression premature wo work well actual model care,issue,positive,positive,positive,positive,positive,positive
418010959,"I found the same error.
If install tensorpack by `pip install tensorpack` this will return this error.
So to solve this error, must use github code.",found error install pip install return error solve error must use code,issue,negative,neutral,neutral,neutral,neutral,neutral
417906115,"The code works well for its original purpose (training on COCO dataset).
Since you're using it for a different task you'll be responsible for making any necessary changes, including changing hyperparameters or data processing, to make it work. 
For example, if many of your image does not have a ground truth box, it could easily make training unstable. And you'll need to figure out the best setting to train it (e.g. change the fg/bg sampe strategy, etc).

Such problem is not considered a tensorpack issue, unless you have reasons to believe it's actually caused by a bug in the code rather than the change of dataset.

",code work well original purpose training coco since different task responsible making necessary data make work example many image ground truth box could easily make training unstable need figure best setting train change strategy problem considered issue unless believe actually bug code rather change,issue,positive,positive,positive,positive,positive,positive
417905184,"Forgot to mention that I used a custom dataset, which only have a little  fg , but I don't think thats where the bug comes from

when trained with tf1.10.0 for 20 epoch

>  fastrcnn_losses/box_loss: 0.0022282
>  [32m[0902 01:57:11 @monitor.py:435] [0m fastrcnn_losses/label_loss: 0.028814
>  [32m[0902 01:57:11 @monitor.py:435] [0m fastrcnn_losses/label_metrics/accuracy: 0.9955
>  [32m[0902 01:57:11 @monitor.py:435] [0m fastrcnn_losses/label_metrics/false_negative: 1
>  [32m[0902 01:57:11 @monitor.py:435] [0m fastrcnn_losses/label_metrics/fg_accuracy: 2.2959e-37
>  [32m[0902 01:57:11 @monitor.py:435] [0m learning_rate: 0.01
>  [32m[0902 01:57:11 @monitor.py:435] [0m maskrcnn_loss/accuracy: 0.49791
>  [32m[0902 01:57:11 @monitor.py:435] [0m maskrcnn_loss/fg_pixel_ratio: 0.50209
>  [32m[0902 01:57:11 @monitor.py:435] [0m maskrcnn_loss/maskrcnn_loss: nan
>  [32m[0902 01:57:11 @monitor.py:435] [0m maskrcnn_loss/pos_accuracy: 2.3443e-37
>  [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level2: 511.92
>  [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level3: 0.080237
>  [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level4: 0.0023035
>  [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level5: 2.3035e-37
>  [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level2: 2.222
>  [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level3: 0.080237
>  [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level4: 0.0023035
>  [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level5: 0
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/box_loss: 0.015912
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/label_loss: 0.12846
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/box_loss: 0.0099081
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_loss: 0.085681
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.1: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.2: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.5: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.1: 0.26406
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.2: 0.26406
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.5: 0.26406
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/num_pos_anchor: 4.6261
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/num_valid_anchor: 200.4
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/box_loss: 0.0043817
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_loss: 0.034303
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.1: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.2: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.5: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.1: 0.31281
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.2: 0.31281
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.5: 0.31281
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/num_pos_anchor: 2.068
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/num_valid_anchor: 45.1
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/box_loss: 0.001617
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_loss: 0.0082804
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.1: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.2: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.5: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.1: 0.41653
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.2: 0.41653
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.5: 0.41653
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/num_pos_anchor: 0.55376
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/num_valid_anchor: 9.2134
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/box_loss: 5.1986e-06
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_loss: 0.00018782
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.1: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.2: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.5: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.1: 0.49879
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.2: 0.49879
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.5: 0.49879
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/num_pos_anchor: 0.0049376
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/num_valid_anchor: 1.2441
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/box_loss: 0
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_loss: 6.2402e-06
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.1: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.2: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.5: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.1: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.2: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.5: 0.5
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/num_pos_anchor: 0
>  [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/num_valid_anchor: 0.04577
>  [32m[0902 01:57:11 @monitor.py:435] [0m sample_fast_rcnn_targets/num_bg: 509.7
>  [32m[0902 01:57:11 @monitor.py:435] [0m sample_fast_rcnn_targets/num_fg: 2.3046
>  [32m[0902 01:57:11 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/best_iou_per_gt: nan
>  [32m[0902 01:57:11 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.3: 0.16727
>  [32m[0902 01:57:11 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.5: 0.051163
>  [32m[0902 01:57:11 @monitor.py:435] [0m total_cost: nan
>  [32m[0902 01:57:11 @monitor.py:435] [0m wd_cost: nan

when trained with tf1.8.0 for 20 epoch

> [32m[0902 04:44:37 @monitor.py:435] [0m fastrcnn_losses/box_loss: 0.076984
>  [32m[0902 04:44:37 @monitor.py:435] [0m fastrcnn_losses/label_loss: 0.03472
>  [32m[0902 04:44:37 @monitor.py:435] [0m fastrcnn_losses/label_metrics/accuracy: 0.98665
>  [32m[0902 04:44:37 @monitor.py:435] [0m fastrcnn_losses/label_metrics/false_negative: 0.16149
>  [32m[0902 04:44:37 @monitor.py:435] [0m fastrcnn_losses/label_metrics/fg_accuracy: 0.83851
>  [32m[0902 04:44:37 @monitor.py:435] [0m learning_rate: 0.01
>  [32m[0902 04:44:37 @monitor.py:435] [0m maskrcnn_loss/accuracy: 0.88644
>  [32m[0902 04:44:37 @monitor.py:435] [0m maskrcnn_loss/fg_pixel_ratio: 0.44969
>  [32m[0902 04:44:37 @monitor.py:435] [0m maskrcnn_loss/maskrcnn_loss: 0.25286
>  [32m[0902 04:44:37 @monitor.py:435] [0m maskrcnn_loss/pos_accuracy: 0.39374
>  [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level2: 440.01
>  [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level3: 53.552
>  [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level4: 17.493
>  [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level5: 0.94497
>  [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level2: 23.713
>  [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level3: 3.6506
>  [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level4: 0.13955
>  [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level5: 0
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/box_loss: 0.0092716
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/label_loss: 0.0047935
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/box_loss: 0.0081492
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_loss: 0.0037865
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.1: 0.58309
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.2: 0.59838
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.5: 0.61808
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.1: 0.70027
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.2: 0.70025
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.5: 0.69964
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/num_pos_anchor: 5.3862
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/num_valid_anchor: 199.5
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/box_loss: 0.00070578
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_loss: 0.00034172
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.1: 0.59589
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.2: 0.61341
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.5: 0.61765
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.1: 0.63041
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.2: 0.63041
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.5: 0.63041
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/num_pos_anchor: 1.5428
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/num_valid_anchor: 44.827
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/box_loss: 0.0003572
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_loss: 0.00065178
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.1: 0.60173
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.2: 0.60216
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.5: 0.60358
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.1: 0.59635
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.2: 0.59635
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.5: 0.59635
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/num_pos_anchor: 0.92755
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/num_valid_anchor: 10.267
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/box_loss: 5.9436e-05
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_loss: 1.3451e-05
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.1: 0.51215
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.2: 0.51215
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.5: 0.51215
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.1: 0.51215
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.2: 0.51215
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.5: 0.51214
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/num_pos_anchor: 0.10742
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/num_valid_anchor: 1.4042
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/box_loss: 0
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_loss: 2.7921e-08
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.1: 0.5
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.2: 0.5
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.5: 0.5
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.1: 0.5
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.2: 0.5
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.5: 0.5
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/num_pos_anchor: 0
>  [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/num_valid_anchor: 0.0015343
>  [32m[0902 04:44:37 @monitor.py:435] [0m sample_fast_rcnn_targets/num_bg: 484.5
>  [32m[0902 04:44:37 @monitor.py:435] [0m sample_fast_rcnn_targets/num_fg: 27.504
>  [32m[0902 04:44:37 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/best_iou_per_gt: 0.67628
>  [32m[0902 04:44:37 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.3: 0.86692
>  [32m[0902 04:44:37 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.5: 0.76383
>  [32m[0902 04:44:37 @monitor.py:435] [0m total_cost: 0.84116
>  [32m[0902 04:44:37 @monitor.py:435] [0m wd_cost: 0.46254",forgot mention used custom little think thats bug come trained epoch nan nan nan nan trained epoch,issue,negative,negative,negative,negative,negative,negative
417903526,"The issue template has something more. This is still missing:
> 2. What you observed, including but not limited to the __entire__ logs.
  Better to paste what you observed instead of describing them.",issue template something still missing limited better paste instead,issue,negative,positive,neutral,neutral,positive,positive
417903272,"sorry about the confusing,

I clone the FasterRcnn example from 

https://github.com/tensorpack/tensorpack/tree/01245d68c30c223bb598d91b390d41abc7f94640

run it with
 `
python train.py
` 
the only change in config is I load a resnet50 backbone weights. and FPN = True

I also made some change in data.py from:
`
return ret
`
to
`
names = ['image', 'anchor_labels_lvl2', 'anchor_boxes_lvl2', 'anchor_labels_lvl3', 'anchor_boxes_lvl3', 'anchor_labels_lvl4', 'anchor_boxes_lvl4', 'anchor_labels_lvl5', 'anchor_boxes_lvl5', 'anchor_labels_lvl6', 'anchor_boxes_lvl6', 'gt_boxes', 'gt_labels', 'gt_masks']
return [ret[name] for name in names]
`

",sorry clone example run python change load backbone true also made change return ret return ret name name,issue,negative,negative,neutral,neutral,negative,negative
417899134,"Please provide more details following the issue template.
In particular, please provide the command you use to:
1. get the source code to run
2. run",please provide following issue template particular please provide command use get source code run run,issue,positive,positive,neutral,neutral,positive,positive
417899072,"Since your model is not originally defined by tensorpack's layer, you should not use tensorpack's layer to define it when you load it. You should refer to whoever provides the model for the code that defines the model.",since model originally defined layer use layer define load refer whoever model code model,issue,negative,positive,positive,positive,positive,positive
417898481,"Sorry about my unsuitable issue...
I define my graph using tensorpack layer and load a tensorflow model. I change some tensor names to load it correctly, such as: change `W` to `weights` and `b` to `biases`. Actually, I don't know why the tensor names in tensorpack's Conv2D is not corresponding to the tensorflow's. Even I go through the entire tensorpack code, I can not find anything about `ExponentialMovingAverage`, and there are nothing in tensorpack corresponding to it, so I cannot adapt it through change the code. ",sorry unsuitable issue define graph layer load model change tensor load correctly change actually know tensor corresponding even go entire code find anything nothing corresponding adapt change code,issue,negative,negative,negative,negative,negative,negative
417832521,"You are correct that this is not a high priority subject or even one to consider at all. But what I'm trying to convey here is not copy-pasting the code from an old repository to a newer one, but rather providing a base with more efficient implementation matters.
To me it makes no sense I spend twice or 5x the time on training something while I can easily have a massive speedup using tensorpack! this is currently the issue. 
In order to address that, one way would be to port everything to tensorpack and training them to show that everything works as expected. Another way that's much suitable I guess would be to specify the bottlenecks and explain in the documentation that how one can overcome them. to be more exact, how to make an existing TF implementation run faster. (what would you do to make them run faster? what would you change and why?) 
I saw an example of this concerning datasets in the documentation. an explanation of how to go about such things would be enough and would have the same outcome of porting everything to tensorboard except that the contributors will be doing it. 

",correct high priority subject even one consider trying convey code old repository one rather providing base efficient implementation sense spend twice time training something easily massive currently issue order address one way would port everything training show everything work another way much suitable guess would specify explain documentation one overcome exact make implementation run faster would make run faster would change saw example concerning documentation explanation go would enough would outcome everything except,issue,positive,positive,neutral,neutral,positive,positive
417799429,"I was thinking only one or two files, that construct the network by importing their code and do the inference. That provides an easy access to those pretrained models. 
I would not want to train them again or copy-paste their code.",thinking one two construct network code inference easy access would want train code,issue,negative,positive,positive,positive,positive,positive
417796992,"Porting these models is merely a copy paste / import action (tensorpack already supports TF.slim). I wonder if it is worth spending time in porting these imagenet-based models over.
Do you really want to compete with a repo which is apparently the first choice of people -- independent of the code-readability? You cannot convince people to make a wise choice (have a look at the keras fanbase).

The work would be redundant and is first and foremost time consuming work (when we talk about training them).
And I am not sure if one needs to retrain them (ilsvr classification) anyway again and again. Loading the graph from there though might be interesting when fine-tuning in tensorpack.

The [CaffeModel Zoo](
https://github.com/BVLC/caffe/wiki/Model-Zoo) is IMHO much more interesting and diverse: DeepHand, VQA, some face-recognition stuff, semantic segmentation. If someone has too much spare time, I guess this would be more helpful for people (they can copy the suggested TF.models and fine-tune from there). This would touch a much wider audience.

There are bigger opportunities:
My concern is that there is currently a huge gap in the tensorpack documentation of how to bring a trained model in tensorpack to production (tensorflow-serving, tf-mobile, freeze graph stuff). If I remember correctly there is not even the undocumented npz export mentioned.
Since tensorpack already uses ModelDescr and has a PredictConfig being able to build the inference-only graph, it would be much more straightforward to export models from tensorpack than the way described in the official TF docs (including pruning the graph). The ""apply"" function in the examples is almost doing it.

I can check if the tfutils/export.py still works and phrase a first documentation draft. It would be huge plus if the entire pipeline is described and supported rather than adding more image classification models.

But in the end it is: Training takes time and a proper implementation as well. I do have a mobilenet implementation and I can contribute my cityscape data reader. However, we agree that it is most of the time tedious work to just get the model correct.

This is not a wishlist of new examples (time is limited). It is a slightly different (and more broad) view on the topic.",merely copy paste import action already wonder worth spending time really want compete apparently first choice people independent convince people make wise choice look work would redundant first foremost time consuming work talk training sure one need retrain classification anyway loading graph though might interesting zoo much interesting diverse stuff semantic segmentation someone much spare time guess would helpful people copy would touch much audience bigger concern currently huge gap documentation bring trained model production freeze graph stuff remember correctly even undocumented export since already able build graph would much straightforward export way official pruning graph apply function almost check still work phrase first documentation draft would huge plus entire pipeline rather image classification end training time proper implementation well implementation contribute cityscape data reader however agree time tedious work get model correct new time limited slightly different broad view topic,issue,positive,positive,positive,positive,positive,positive
417765040,"@ppwwyyxx you are right I also agree with that. Starting off with the imagenet models would be a good starting port and hopefully as the whole project gets momentum, other sections will be added/ported by contributors. ",right also agree starting would good starting port hopefully whole project momentum,issue,positive,positive,positive,positive,positive,positive
417755825,"I think most code in [tensorflow/models/research](https://github.com/tensorflow/models/tree/master/research) are too specific (therefore only useful to a small group of people) or poorly maintained.

The imagenet models there https://github.com/tensorflow/models/tree/master/research/slim are worth porting. Some of the translation models also look good but I'm not very familiar with them.",think code specific therefore useful small group people poorly worth translation also look good familiar,issue,positive,positive,positive,positive,positive,positive
417737580,"I revert the change of interface on InputSource. I think __len__ doesn't apply very well since InputSource is not really iterable. The size is more like an attribute of it.

Also, from https://docs.python.org/3/library/functions.html#len:


> len(s)
Return the length (the number of items) of an object. The argument may be a sequence (such as a string, bytes, tuple, list, or range) or a collection (such as a dictionary, set, or frozen set).
",revert change interface think apply well since really iterable size like attribute also return length number object argument may sequence string list range collection dictionary set frozen set,issue,negative,positive,positive,positive,positive,positive
417603064,"@ppwwyyxx 
did you mean directly insert the following code into the training sciprt?
 `ProcessTensors(['tensor1', 'tensor2'], lambda tensor1, tensor2: print(tensor1, tensor2, tensor1 + tensor2))`",mean directly insert following code training lambda tensor tensor print tensor tensor tensor tensor,issue,negative,negative,neutral,neutral,negative,negative
417580305,"If you reduce the loss and compute gradients on the reduced loss, tensorflow will copy the gradients among GPUs.",reduce loss compute reduced loss copy among,issue,negative,neutral,neutral,neutral,neutral,neutral
417579736,"Please post issues following the issue template (New Issue -> Unexpected Problems).

It looks like you're loading a model that does not match your graph definition, and from the logs it looks like the graph definition is not using any tensorpack layers. So the issue does not seem to be related to tensorpack.",please post following issue template new issue unexpected like loading model match graph definition like graph definition issue seem related,issue,positive,positive,neutral,neutral,positive,positive
417579027,"you mean that  reduce the loss is equal to reduce the gradient?
And the reduce of gradient is faster than reduce of loss?
Size of gradient  is same with the shape of tranning variable , bigger than the loss,which is just scalar in common model. How can reduce of gradient is faster?

",mean reduce loss equal reduce gradient reduce gradient faster reduce loss size gradient shape variable bigger loss scalar common model reduce gradient faster,issue,negative,negative,negative,negative,negative,negative
417576206,"It's faster ,but may be wrong.
If reduce the loss is equal to reduce the gradient, why all other distribute trainning do the reduce of gradient?",faster may wrong reduce loss equal reduce gradient distribute reduce gradient,issue,negative,negative,negative,negative,negative,negative
417570793,"Where do you think it is not right?

It's faster to reduce the gradients.",think right faster reduce,issue,negative,positive,positive,positive,positive,positive
417566914,"I make a mistake in question 3, the MultiGPUGANTrainer just add and mean of all the loss ,not the variable gradient. I think it's a dangerous trick ,if this is right, why we allreduce the gradient instead of loss?

",make mistake question add mean loss variable gradient think dangerous trick right gradient instead loss,issue,negative,negative,negative,negative,negative,negative
417438363,"It works!
I use the 
```
samples = input_generator.get(
          dataset,
          FLAGS.train_crop_size,
          FLAGS.train_batch_size,
          min_resize_value=FLAGS.min_resize_value,
          max_resize_value=FLAGS.max_resize_value,
          resize_factor=FLAGS.resize_factor,
          min_scale_factor=FLAGS.min_scale_factor,
          max_scale_factor=FLAGS.max_scale_factor,
          scale_factor_step_size=FLAGS.scale_factor_step_size,
          dataset_split=FLAGS.train_split,
          is_training=True,
          model_variant=FLAGS.model_variant)
```
to generate input tensor list.

And it is wrong to just put the `samples` into the `TensorInput`, which is `data=TensorInput(samples)`.

It should be a lambda function to make the function callable:
`data=TensorInput(lambda: samples, iter_steps)`

We can see more details in the `tensorpack/examples/PennTreebank`.

BTW, tensorpack is really a powerful tool! Thank you for your contribution! And I think the document about the use of tensorflow tensor input should be easier to access. ",work use generate input tensor list wrong put lambda function make function callable lambda see really powerful tool thank contribution think document use tensor input easier access,issue,positive,negative,neutral,neutral,negative,negative
417433536,Alright. I find an example in the PTB training example. I should read the document more carefully. Thank you for your kind help! And I will reply later after I solve it.,alright find example training example read document carefully thank kind help reply later solve,issue,positive,positive,positive,positive,positive,positive
417428610,"As the [document](https://tensorpack.readthedocs.io/modules/input_source.html#tensorpack.input_source.TensorInput) of TensorInput said, it takes __a function which returns a list__, not a list directly.",document said function list directly,issue,negative,positive,neutral,neutral,positive,positive
417427642,"It doesn't work...
I change the samples to `list` with the format:
```
===========samples: [<tf.Tensor 'cond/Merge:0' shape=(513, 513, 3) dtype=float32>, <tf.Tensor 'cond/Merge_1:0' shape=(513, 513, 1) dtype=int32>]
```
But I get the error:
```
Traceback (most recent call last):
  File ""deeplab/hed.py"", line 495, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""deeplab/hed.py"", line 489, in main
    SyncMultiGPUTrainer(max(get_num_gpu(), 1)))
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/train/interface.py"", line 85, in launch_train_with_config
    model._build_graph_get_cost, model.get_optimizer)
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 181, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/train/tower.py"", line 194, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/train/trainers.py"", line 93, in _setup_graph
    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/graph_builder/training.py"", line 160, in build
    grad_list = DataParallelBuilder.build_on_towers(self.towers, get_grad_fn, devices)
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/graph_builder/training.py"", line 118, in build_on_towers
    ret.append(func())
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/train/tower.py"", line 222, in get_grad_fn
    cost = get_cost_fn(*input.get_input_tensors())
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/input_source/input_source_base.py"", line 81, in get_input_tensors
    return self._get_input_tensors()
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/input_source/input_source.py"", line 352, in _get_input_tensors
    ret = self.get_tensor_fn()
TypeError: 'list' object is not callable
```",work change list format get error recent call last file line module file line run main file line main file line file line wrapper return file line input file line input file line build file line file line cost file line return file line ret object callable,issue,negative,positive,neutral,neutral,positive,positive
417426124,"`InputSource` has to produce __list__ as inputs that match the order you declared in your model.
You can make a list from a dict by `[dic['image'], dic['label']]`, assuming that's the order you declared them.",produce match order declared model make list assuming order declared,issue,negative,neutral,neutral,neutral,neutral,neutral
417423940,"Alright, is there any `InputSource` suitable for the `FIFOQueue`?

I want to handle the `samples` which has the data format: `{'image': <tf.Tensor 'batch:0' shape=(8, 513, 513, 3) dtype=float32>, 'label': <tf.Tensor 'batch:1' shape=(8, 513, 513, 1) dtype=int32>}`. It is a dict, and cannot be processed by the `TensorInput`.

Or I can handle the `dataset_train` which has the data format: `<class 'tensorflow.python.ops.data_flow_ops.FIFOQueue'>`.
It seems that there is not suitable `InputSource` for my data.",alright suitable want handle data format handle data format class suitable data,issue,negative,positive,positive,positive,positive,positive
417421898,"No. As the [document](https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.TrainConfig) said, `data=` takes a [InputSource](https://tensorpack.readthedocs.io/modules/input_source.html#tensorpack-input-source-package). FIFOQueue is not an `InputSource`. Please use the functions according to the docs.

If you have created a `tf.data.Dataset`, you can use [TFDatasetInput](https://tensorpack.readthedocs.io/modules/input_source.html#tensorpack.input_source.TFDatasetInput). If you have a function that creates `tf.Tensors` directly, use [TensorInput](https://tensorpack.readthedocs.io/modules/input_source.html#tensorpack.input_source.TensorInput).",document said please use according use function directly use,issue,negative,positive,neutral,neutral,positive,positive
417417957,"Sorry, it is my fault... 
But when I use the `data=inputs_queue`, where the `inputs_queue=prefetch_queue.prefetch_queue(
          samples, capacity=128 * config.num_clones)`
I got the error:
```
Traceback (most recent call last):
  File ""deeplab/hed.py"", line 495, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""deeplab/hed.py"", line 484, in main
    config = get_config()
  File ""deeplab/hed.py"", line 440, in get_config
    max_epoch=100,
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/train/config.py"", line 101, in __init__
    assert_type(data, InputSource)
  File ""/usr/local/lib/python2.7/site-packages/tensorpack/train/config.py"", line 94, in assert_type
    assert isinstance(v, tp), v.__class__
AssertionError: <class 'tensorflow.python.ops.data_flow_ops.FIFOQueue'>
``` 
Can the `data=` accept the `FIFOQueue` as inputs?

The code is 
```
    dataset = segmentation_dataset.get_dataset(
      FLAGS.dataset, FLAGS.train_split, dataset_dir=FLAGS.dataset_dir)

    samples = input_generator.get(
          dataset,
          FLAGS.train_crop_size,
          FLAGS.train_batch_size,
          min_resize_value=FLAGS.min_resize_value,
          max_resize_value=FLAGS.max_resize_value,
          resize_factor=FLAGS.resize_factor,
          min_scale_factor=FLAGS.min_scale_factor,
          max_scale_factor=FLAGS.max_scale_factor,
          scale_factor_step_size=FLAGS.scale_factor_step_size,
          dataset_split=FLAGS.train_split,
          is_training=True,
          model_variant=FLAGS.model_variant)
    dataset_train = prefetch_queue.prefetch_queue(
          samples, capacity=128)

    steps_per_epoch = 100 #dataset_train.size() * 40
    print '===========dataset_train:', dataset_train
    print '===========samples:', samples
    #dataset_val = get_data('val')

    return TrainConfig(
        data=dataset_train,
        callbacks=[
            ModelSaver(),
            ScheduledHyperParamSetter('learning_rate', [(30, 6e-6), (45, 1e-6), (60, 8e-7)]),
            HumanHyperParamSetter('learning_rate') #,
            #InferenceRunner(dataset_val,
            #                BinaryClassificationStats('prediction', 'edgemap4d'))
        ],
        model=Model(),
        steps_per_epoch=steps_per_epoch,
        max_epoch=100,
    )
```
Where the `samples` is a `tf.train.batch` with the format of `{'image': <tf.Tensor 'batch:0' shape=(8, 513, 513, 3) dtype=float32>, 'label': <tf.Tensor 'batch:1' shape=(8, 513, 513, 1) dtype=int32>}`.

",sorry fault use got error recent call last file line module file line run main file line main file line file line data file line assert class accept code print print return format,issue,negative,negative,neutral,neutral,negative,negative
417376363,"1. contributions are welcome
2. No. all trainers in tensorpack are single-cost trainers, as you can see here: https://tensorpack.readthedocs.io/modules/train.html
3. both are ok
4. Would you elaborate?",welcome see would elaborate,issue,negative,positive,positive,positive,positive,positive
417201907,"Next time please post full error. I do not know how you get those errors but it feels like you did not use the correct API: https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.TrainConfig

You should use `dataflow=` if using a dataflow, or `data=` if using an InputSource.",next time please post full error know get like use correct use,issue,negative,positive,positive,positive,positive,positive
417177822,"But it seems that other data formats can't be used in tensorpack trainer:
```
AssertionError: <tensorflow.python.ops.data_flow_ops.FIFOQueue object at 0x7f374c4a1310>
AssertionError: <class 'tensorflow.python.ops.data_flow_ops.FIFOQueue'>
AssertionError: <class 'tensorpack.input_source.input_source.TensorInput'>

```",data ca used trainer object class class,issue,negative,neutral,neutral,neutral,neutral,neutral
417167194,"Dataflow is unrelated to tensorflow, it should produce numpy arrays.

To use tensorflow tensors as inputs, you can check the ""input pipeline""
tutorial for options other than dataflow.

On Wed, Aug 29, 2018, 6:30 PM Qi Fan <notifications@github.com> wrote:

> Thank you for your kind reply!
> When run
>
> ds.reset_state()
> print(next(ds.get_data()))
> TestDataSpeed(ds).start()
> ``
> The print is :
>
> =================
> [<tf.Tensor 'cond/Merge:0' shape=(513, 513, 3) dtype=float32>, <tf.Tensor
> 'Squeeze_2:0' shape=(513, 513) dtype=int32>]
> 0%| |0/5000[00:00<?,?it/s]
> [0830 01:19:38 @param.py:194] Use train_log/hed/hyper.txt to set
> hyperparam: 'learning_rate'.
> [0830 01:19:42 @training.py:51] [DataParallel] Training a model of 4
> towers.
>
>
> My generator is:
>
> slim = tf.contrib.slim
>
> dataset_data_provider = slim.dataset_data_provider
> def _get_data(data_provider, dataset_split):
> if common.LABELS_CLASS not in data_provider.list_items():
> raise ValueError('Failed to find labels.')
>
> image, height, width = data_provider.get(
> [common.IMAGE, common.HEIGHT, common.WIDTH])
> Some datasets do not contain image_name.
>
> if common.IMAGE_NAME in data_provider.list_items():
> image_name, = data_provider.get([common.IMAGE_NAME])
> else:
> image_name = tf.constant('')
>
> label = None
> if dataset_split != common.TEST_SET:
> label, = data_provider.get([common.LABELS_CLASS])
>
> return image, label, image_name, height, width
>
> def get(dataset,
> crop_size,
> batch_size,
> min_resize_value=None,
> max_resize_value=None,
> resize_factor=None,
> min_scale_factor=1.,
> max_scale_factor=1.,
> scale_factor_step_size=0,
> num_readers=1,
> num_threads=1,
> dataset_split=None,
> is_training=True,
> model_variant=None):
>
> if dataset_split is None:
> raise ValueError('Unknown dataset split.')
> if model_variant is None:
> tf.logging.warning('Please specify a model_variant. See '
> 'feature_extractor.network_map for supported model '
> 'variants.')
>
> data_provider = dataset_data_provider.DatasetDataProvider(
> dataset,
> num_readers=num_readers,
> num_epochs=None if is_training else 1,
> shuffle=is_training)
> image, label, image_name, height, width = _get_data(data_provider,
> dataset_split)
> if label is not None:
> if label.shape.ndims == 2:
> label = tf.expand_dims(label, 2)
> elif label.shape.ndims == 3 and label.shape.dims[2] == 1:
> pass
> else:
> raise ValueError('Input label shape must be [height, width], or '
> '[height, width, 1].')
>
> label.set_shape([None, None, 1])
>
> original_image, image, label = input_preprocess.preprocess_image_and_label(
> image,
> label,
> crop_height=crop_size[0],
> crop_width=crop_size[1],
> min_resize_value=min_resize_value,
> max_resize_value=max_resize_value,
> resize_factor=resize_factor,
> min_scale_factor=min_scale_factor,
> max_scale_factor=max_scale_factor,
> scale_factor_step_size=scale_factor_step_size,
> ignore_label=dataset.ignore_label,
> is_training=is_training,
> model_variant=model_variant)
> sample = {
> common.IMAGE: image,
> common.IMAGE_NAME: image_name,
> common.HEIGHT: height,
> common.WIDTH: width
> }
> if label is not None:
> sample[common.LABEL] = tf.squeeze(label, -1)
>
> if not is_training:
> # Original image is only used during visualization.
> sample[common.ORIGINAL_IMAGE] = original_image,
> num_threads = 1
> yield [sample[common.IMAGE], sample[common.LABEL]]
>
> And I generate my dataflow through:
>
> def get_data():
> dataset = segmentation_dataset.get_dataset(
> FLAGS.dataset, FLAGS.train_split, dataset_dir=FLAGS.dataset_dir)
>
> #tf.gfile.MakeDirs(FLAGS.train_logdir)
> #tf.logging.info('Training on %s set', FLAGS.train_split)
>
> #samples = MyDataFlow(
> samples = input_generator_sync.get(
>       dataset,
>       FLAGS.train_crop_size,
>       FLAGS.train_batch_size,
>       min_resize_value=FLAGS.min_resize_value,
>       max_resize_value=FLAGS.max_resize_value,
>       resize_factor=FLAGS.resize_factor,
>       min_scale_factor=FLAGS.min_scale_factor,
>       max_scale_factor=FLAGS.max_scale_factor,
>       scale_factor_step_size=FLAGS.scale_factor_step_size,
>       dataset_split=FLAGS.train_split,
>       is_training=True,
>       model_variant=FLAGS.model_variant)
>
> ds = DataFromGenerator(samples)
> #ds = BatchDataByShape(ds, 8, idx=0) # when remove it and the PrefecthDataZMQ, the test works.
>
> #ds = PrefetchDataZMQ(ds, 1) #
> return ds
>
> and
>
> dataset_train = get_data() #get_data('train')
> #dataset_train = input_generator_sync()
> steps_per_epoch = 100 # dataset_train.size() * 40
> #dataset_val = get_data('val')
> print '================='
> dataset_train.reset_state()
> print(next(dataset_train.get_data()))
> TestDataSpeed(dataset_train).start()
>
> And the dataset is generated by:
>
> def get_dataset(dataset_name, split_name, dataset_dir):
> """"""Gets an instance of slim Dataset.
>
> Args:
> dataset_name: Dataset name.
> split_name: A train/val Split name.
> dataset_dir: The directory of the dataset sources.
>
> Returns:
> An instance of slim Dataset.
>
> Raises:
> ValueError: if the dataset_name or split_name is not recognized.
> """"""
> if dataset_name not in _DATASETS_INFORMATION:
> raise ValueError('The specified dataset is not supported yet.')
>
> splits_to_sizes = _DATASETS_INFORMATION[dataset_name].splits_to_sizes
>
> if split_name not in splits_to_sizes:
> raise ValueError('data split name %s not recognized' % split_name)
> Prepare the variables for different datasets.
>
> num_classes = _DATASETS_INFORMATION[dataset_name].num_classes
> ignore_label = _DATASETS_INFORMATION[dataset_name].ignore_label
>
> file_pattern = _FILE_PATTERN
> file_pattern = os.path.join(dataset_dir, file_pattern % split_name)
> Specify how the TF-Examples are decoded.
>
> keys_to_features = {
> 'image/encoded': tf.FixedLenFeature(
> (), tf.string, default_value=''),
> 'image/filename': tf.FixedLenFeature(
> (), tf.string, default_value=''),
> 'image/format': tf.FixedLenFeature(
> (), tf.string, default_value='jpeg'),
> 'image/height': tf.FixedLenFeature(
> (), tf.int64, default_value=0),
> 'image/width': tf.FixedLenFeature(
> (), tf.int64, default_value=0),
> 'image/segmentation/class/encoded': tf.FixedLenFeature(
> (), tf.string, default_value=''),
> 'image/segmentation/class/format': tf.FixedLenFeature(
> (), tf.string, default_value='png'),
> }
> items_to_handlers = {
> 'image': tfexample_decoder.Image(
> image_key='image/encoded',
> format_key='image/format',
> channels=3),
> 'image_name': tfexample_decoder.Tensor('image/filename'),
> 'height': tfexample_decoder.Tensor('image/height'),
> 'width': tfexample_decoder.Tensor('image/width'),
> 'labels_class': tfexample_decoder.Image(
> image_key='image/segmentation/class/encoded',
> format_key='image/segmentation/class/format',
> channels=1),
> }
>
> decoder = tfexample_decoder.TFExampleDecoder(
> keys_to_features, items_to_handlers)
>
> return dataset.Dataset(
> data_sources=file_pattern,
> reader=tf.TFRecordReader,
> decoder=decoder,
> num_samples=splits_to_sizes[split_name],
> items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,
> ignore_label=ignore_label,
> num_classes=num_classes,
> name=dataset_name,
> multi_label=True)
>
>  —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/877#issuecomment-417160022>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABUTtTue9JzC-0Jzj1nK68tged5D6Irwks5uV0A9gaJpZM4WReTN>
> .
>
",unrelated produce use check input pipeline tutorial wed fan wrote thank kind reply run print next print use set training model generator slim raise find image height width contain else label none label return image label height width get none raise split none specify see model else image label height width label none label label pas else raise label shape must height width height width none none image label image label sample image height width label none sample label original image used visualization sample yield sample sample generate set remove test work return print print next instance slim name split name directory instance slim raise yet raise split name prepare different specify return reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
417160022,"Thank you for your kind reply!
When run 

``` 
ds.reset_state()
print(next(ds.get_data()))
TestDataSpeed(ds).start()
```

The print is :

```
=================
[<tf.Tensor 'cond/Merge:0' shape=(513, 513, 3) dtype=float32>, <tf.Tensor 'Squeeze_2:0' shape=(513, 513) dtype=int32>]
  0%|                                                                                                                                                                                                                   |0/5000[00:00<?,?it/s]
[0830 01:19:38 @param.py:194] Use train_log/hed/hyper.txt to set hyperparam: 'learning_rate'.
[0830 01:19:42 @training.py:51] [DataParallel] Training a model of 4 towers.
```

My generator is:

```
slim = tf.contrib.slim

dataset_data_provider = slim.dataset_data_provider
def _get_data(data_provider, dataset_split):
  if common.LABELS_CLASS not in data_provider.list_items():
    raise ValueError('Failed to find labels.')

  image, height, width = data_provider.get(
      [common.IMAGE, common.HEIGHT, common.WIDTH])

  # Some datasets do not contain image_name.
  if common.IMAGE_NAME in data_provider.list_items():
    image_name, = data_provider.get([common.IMAGE_NAME])
  else:
    image_name = tf.constant('')

  label = None
  if dataset_split != common.TEST_SET:
    label, = data_provider.get([common.LABELS_CLASS])

  return image, label, image_name, height, width


def get(dataset,
        crop_size,
        batch_size,
        min_resize_value=None,
        max_resize_value=None,
        resize_factor=None,
        min_scale_factor=1.,
        max_scale_factor=1.,
        scale_factor_step_size=0,
        num_readers=1,
        num_threads=1,
        dataset_split=None,
        is_training=True,
        model_variant=None):

  if dataset_split is None:
    raise ValueError('Unknown dataset split.')
  if model_variant is None:
    tf.logging.warning('Please specify a model_variant. See '
                       'feature_extractor.network_map for supported model '
                       'variants.')

  data_provider = dataset_data_provider.DatasetDataProvider(
      dataset,
      num_readers=num_readers,
      num_epochs=None if is_training else 1,
      shuffle=is_training)
  image, label, image_name, height, width = _get_data(data_provider,
                                                      dataset_split)
  if label is not None:
                                                                                                                                                                                                                                if label.shape.ndims == 2:
      label = tf.expand_dims(label, 2)
    elif label.shape.ndims == 3 and label.shape.dims[2] == 1:
      pass
    else:
      raise ValueError('Input label shape must be [height, width], or '
                       '[height, width, 1].')

    label.set_shape([None, None, 1])
  original_image, image, label = input_preprocess.preprocess_image_and_label(
      image,
      label,
      crop_height=crop_size[0],
      crop_width=crop_size[1],
      min_resize_value=min_resize_value,
      max_resize_value=max_resize_value,
      resize_factor=resize_factor,
      min_scale_factor=min_scale_factor,
      max_scale_factor=max_scale_factor,
      scale_factor_step_size=scale_factor_step_size,
      ignore_label=dataset.ignore_label,
      is_training=is_training,
      model_variant=model_variant)
  sample = {
      common.IMAGE: image,
      common.IMAGE_NAME: image_name,
      common.HEIGHT: height,
      common.WIDTH: width
  }
  if label is not None:
    sample[common.LABEL] = tf.squeeze(label, -1)

  if not is_training:
    # Original image is only used during visualization.
    sample[common.ORIGINAL_IMAGE] = original_image,
    num_threads = 1
  yield [sample[common.IMAGE], sample[common.LABEL]]
```
And I generate my dataflow through:
```
def get_data():
    dataset = segmentation_dataset.get_dataset(
      FLAGS.dataset, FLAGS.train_split, dataset_dir=FLAGS.dataset_dir)

    #tf.gfile.MakeDirs(FLAGS.train_logdir)
    #tf.logging.info('Training on %s set', FLAGS.train_split)

    #samples = MyDataFlow(
    samples = input_generator_sync.get(
          dataset,
          FLAGS.train_crop_size,
          FLAGS.train_batch_size,
          min_resize_value=FLAGS.min_resize_value,
          max_resize_value=FLAGS.max_resize_value,
          resize_factor=FLAGS.resize_factor,
          min_scale_factor=FLAGS.min_scale_factor,
          max_scale_factor=FLAGS.max_scale_factor,
          scale_factor_step_size=FLAGS.scale_factor_step_size,
          dataset_split=FLAGS.train_split,
          is_training=True,
          model_variant=FLAGS.model_variant)

    ds = DataFromGenerator(samples)
    #ds = BatchDataByShape(ds, 8, idx=0) # when remove it and the PrefecthDataZMQ, the test works.

    #ds = PrefetchDataZMQ(ds, 1) #
    return ds
```
and 
```
    dataset_train = get_data() #get_data('train')
    #dataset_train = input_generator_sync()
    steps_per_epoch = 100 # dataset_train.size() * 40
    #dataset_val = get_data('val')
    print '================='
    dataset_train.reset_state()
    print(next(dataset_train.get_data()))
    TestDataSpeed(dataset_train).start()
```
And the dataset is generated by:
```
def get_dataset(dataset_name, split_name, dataset_dir):
  """"""Gets an instance of slim Dataset.

  Args:
    dataset_name: Dataset name.
    split_name: A train/val Split name.
    dataset_dir: The directory of the dataset sources.

  Returns:
    An instance of slim Dataset.

  Raises:
    ValueError: if the dataset_name or split_name is not recognized.
  """"""
  if dataset_name not in _DATASETS_INFORMATION:
    raise ValueError('The specified dataset is not supported yet.')

  splits_to_sizes = _DATASETS_INFORMATION[dataset_name].splits_to_sizes

  if split_name not in splits_to_sizes:
    raise ValueError('data split name %s not recognized' % split_name)

  # Prepare the variables for different datasets.
  num_classes = _DATASETS_INFORMATION[dataset_name].num_classes
  ignore_label = _DATASETS_INFORMATION[dataset_name].ignore_label

  file_pattern = _FILE_PATTERN
  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)

  # Specify how the TF-Examples are decoded.
  keys_to_features = {
      'image/encoded': tf.FixedLenFeature(
          (), tf.string, default_value=''),
      'image/filename': tf.FixedLenFeature(
          (), tf.string, default_value=''),
      'image/format': tf.FixedLenFeature(
          (), tf.string, default_value='jpeg'),
      'image/height': tf.FixedLenFeature(
          (), tf.int64, default_value=0),
      'image/width': tf.FixedLenFeature(
          (), tf.int64, default_value=0),
      'image/segmentation/class/encoded': tf.FixedLenFeature(
          (), tf.string, default_value=''),
      'image/segmentation/class/format': tf.FixedLenFeature(
          (), tf.string, default_value='png'),
  }
  items_to_handlers = {
      'image': tfexample_decoder.Image(
          image_key='image/encoded',
          format_key='image/format',
          channels=3),
      'image_name': tfexample_decoder.Tensor('image/filename'),
      'height': tfexample_decoder.Tensor('image/height'),
      'width': tfexample_decoder.Tensor('image/width'),
      'labels_class': tfexample_decoder.Image(
          image_key='image/segmentation/class/encoded',
          format_key='image/segmentation/class/format',
          channels=1),
  }

  decoder = tfexample_decoder.TFExampleDecoder(
      keys_to_features, items_to_handlers)

  return dataset.Dataset(
      data_sources=file_pattern,
      reader=tf.TFRecordReader,
      decoder=decoder,
      num_samples=splits_to_sizes[split_name],
      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,
      ignore_label=ignore_label,
      num_classes=num_classes,
      name=dataset_name,
      multi_label=True)
```

",thank kind reply run print next print use set training model generator slim raise find image height width contain else label none label return image label height width get none raise split none specify see model else image label height width label none label label pas else raise label shape must height width height width none none image label image label sample image height width label none sample label original image used visualization sample yield sample sample generate set remove test work return print print next instance slim name split name directory instance slim raise yet raise split name prepare different specify return,issue,positive,positive,positive,positive,positive,positive
417019366,"Can you make sure your dataflow __can actually run__? You can check it by `TestDataSpeed(ds).start()`.
If it does, what does the elements look like? Can you print the first element?
You can print it by:
```python
ds.reset_state()
print(next(ds.get_data()))
```
If you don't know how to write a dataflow, read https://tensorpack.readthedocs.io/tutorial/extend/dataflow.html .
Data does not have to come from files. It can be arbitrary python code.",make sure actually check look like print first element print python print next know write read data come arbitrary python code,issue,positive,positive,positive,positive,positive,positive
416990282,"Hi!
How can i resume training after interrupted it?
Thanks you!",hi resume training interrupted thanks,issue,negative,positive,positive,positive,positive,positive
416979545,"What I really want to do is just using the dataflow to handle my data.
The data format is 
`
{'width': <tf.Tensor 'batch:4' shape=(16,) dtype=int64>, 'image': <tf.Tensor 'batch:1' shape=(16, 513, 513, 3) dtype=float32>, 'label': <tf.Tensor 'batch:3' shape=(16, 513, 513, 1) dtype=int32>, 'image_name': <tf.Tensor 'batch:2' shape=(16,) dtype=string>, 'height': <tf.Tensor 'batch:0' shape=(16,) dtype=int64>}
` which is processed by `tf.train.batch`
or
`
{'width': <tf.Tensor 'Reshape_6:0' shape=() dtype=int64>, 'image': <tf.Tensor 'cond/Merge:0' shape=(513, 513, 3) dtype=float32>, 'label': <tf.Tensor 'cond/Merge_1:0' shape=(513, 513, 1) dtype=int32>, 'image_name': <tf.Tensor 'Reshape_1:0' shape=() dtype=string>, 'height': <tf.Tensor 'Reshape_3:0' shape=() dtype=int64>}
` which is not processed by `tf.train.batch`

And I don't know how to convert it to dataflow, because in your tutorials, the data are usually loaded totally first and are then processed.

Could you help me fix it? 

Thanks!
",really want handle data data format know convert data usually loaded totally first could help fix thanks,issue,positive,positive,neutral,neutral,positive,positive
416971597,"I use my own dataset and use a generator to generate dataflow:

```
ds = input_generator.get(**args)
ds = DataFromGenerator(ds)
ds = BatchData(ds, 8)
ds = PrefetchDataZMQ(ds, 1)
...
TrainConfig(
dataflow = ds
...
)

launch_train_with_config(config, SyncMultiGPUTrainerReplicated(4, average=False, mode='nccl')
```

And it stucks...

[0829 14:08:18 @base.py:211] Initializing the session ...
[0829 14:08:18 @sessinit.py:207] Variables to restore from dict: conv3_1/W:0, conv1_2/W:0, conv5_1/W:0, conv5_2/W:0, conv4_2/W:0, conv2_2/W:0, conv2_1/W:0, conv1_1/W:0, conv3_2/W:0, conv5_3/W:0, conv3_3/W:0, conv4_1/W:0, conv4_3/W:0
[0829 14:08:18 @sessinit.py:90] WRN The following variables are in the graph, but not found in the dict: branch1/convfc/W:0, branch1/convfc/b:0, branch2/convfc/W:0, branch2/convfc/b:0, branch3/convfc/W:0, branch3/convfc/b:0, branch4/convfc/W:0, branch4/convfc/b:0, branch5/convfc/W:0, branch5/convfc/b:0, conv1_1/bn/beta:0, conv1_1/bn/gamma:0, conv1_1/bn/mean/EMA:0, conv1_1/bn/variance/EMA:0, conv1_2/bn/beta:0, conv1_2/bn/gamma:0, conv1_2/bn/mean/EMA:0, conv1_2/bn/variance/EMA:0, conv2_1/bn/beta:0, conv2_1/bn/gamma:0, conv2_1/bn/mean/EMA:0, conv2_1/bn/variance/EMA:0, conv2_2/bn/beta:0, conv2_2/bn/gamma:0, conv2_2/bn/mean/EMA:0, conv2_2/bn/variance/EMA:0, conv3_1/bn/beta:0, conv3_1/bn/gamma:0, conv3_1/bn/mean/EMA:0, conv3_1/bn/variance/EMA:0, conv3_2/bn/beta:0, conv3_2/bn/gamma:0, conv3_2/bn/mean/EMA:0, conv3_2/bn/variance/EMA:0, conv3_3/bn/beta:0, conv3_3/bn/gamma:0, conv3_3/bn/mean/EMA:0, conv3_3/bn/variance/EMA:0, conv4_1/bn/beta:0, conv4_1/bn/gamma:0, conv4_1/bn/mean/EMA:0, conv4_1/bn/variance/EMA:0, conv4_2/bn/beta:0, conv4_2/bn/gamma:0, conv4_2/bn/mean/EMA:0, conv4_2/bn/variance/EMA:0, conv4_3/bn/beta:0, conv4_3/bn/gamma:0, conv4_3/bn/mean/EMA:0, conv4_3/bn/variance/EMA:0, conv5_1/bn/beta:0, conv5_1/bn/gamma:0, conv5_1/bn/mean/EMA:0, conv5_1/bn/variance/EMA:0, conv5_2/bn/beta:0, conv5_2/bn/gamma:0, conv5_2/bn/mean/EMA:0, conv5_2/bn/variance/EMA:0, conv5_3/bn/beta:0, conv5_3/bn/gamma:0, conv5_3/bn/mean/EMA:0, conv5_3/bn/variance/EMA:0, convfcweight/W:0, global_step:0, learning_rate:0
[0829 14:08:18 @sessinit.py:90] WRN The following variables are in the dict, but not found in the graph: conv1_1/b:0, conv1_2/b:0, conv2_1/b:0, conv2_2/b:0, conv3_1/b:0, conv3_2/b:0, conv3_3/b:0, conv4_1/b:0, conv4_2/b:0, conv4_3/b:0, conv5_1/b:0, conv5_2/b:0, conv5_3/b:0, fc6/W:0, fc6/b:0, fc7/W:0, fc7/b:0, fc8/W:0, fc8/b:0
[0829 14:08:18 @sessinit.py:220] Restoring from dict ...
[0829 14:08:20 @base.py:218] Graph Finalized.
[0829 14:08:21 @concurrency.py:37] Starting EnqueueThread QueueInput/input_queue ...
[0829 14:08:21 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
[0829 14:08:23 @base.py:250] Start Epoch 1 ...
  0%|                                                                                                                                                                                                                    |0/100[00:00<?,?it/s][0829 14:08:23 @input_source.py:520] Pre-filling StagingArea ...",use use generator generate session restore following graph found following found graph graph starting running start epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
416872939,"@ppwwyyxx 
in tensorpack , how to print  the values of  a specific tensor?
in standard tensorflow, it is like this:
```
with tf.Session() as sess:     
           print(sess.run(product))
           print (product.eval())
```
or
`sess.run(tf.Print(.....,[....]))`
but in tensorpack, I can not find where the current session is.",print specific tensor standard like sess print product print find current session,issue,negative,neutral,neutral,neutral,neutral,neutral
416656941,Please post issues following the issue template (new issues -> unexpected problems).,please post following issue template new unexpected,issue,negative,positive,neutral,neutral,positive,positive
416467928,"Thank you. When i call TrainConfig,  I can write it as TrainConfig(data=StagingInput(QueueInput(...))). Is it right?  ",thank call write right,issue,negative,positive,positive,positive,positive,positive
416313683,"Something like this would be cleaner:
```python
import six

class MyMeta(type):
    def __new__(meta, name, bases, dct):
        print(""New class"", name)
        if '__iter__' not in dct:
            print(""Rename!"")
            dct['__iter__'] = dct.pop('get_data')
        return super(MyMeta, meta).__new__(meta, name, bases, dct)
    def __init__(cls, name, bases, dct):
        print(""Initializing class"", name)
        super(MyMeta, cls).__init__(name, bases, dct)


@six.add_metaclass(MyMeta)
class MyBase(object):
    def __iter__(self):
        raise NotImplementedError

    def get_data(self):
        return self.__iter__()

class MyDerived(MyBase):
    def get_data(self):
        yield ""Derive""

x = MyDerived()
print(list(x.get_data()))
print(list(iter(x)))
```",something like would cleaner python import six class type meta name base print new class name print rename return super meta meta name base name base print class name super name base class object self raise self return class self yield derive print list print list iter,issue,positive,negative,negative,negative,negative,negative
416163900,"With the latest commit the following works:

```python
from tensorpack import *

class SomeOldDf(DataFlow):
    def size(self):
        return 2

    def get_data(self):
        for dp in [1, 2]:
            yield [dp]


class SomeNewDf(DataFlow):
    def __len__(self):
        return 2

    def __iter__(self):
        for dp in [1, 2]:
            yield [dp]


ds = SomeOldDf()
ds = BatchData(ds, 2)
print(next(ds.__iter__()))
print(len(ds))

ds = SomeOldDf()
ds = BatchData(ds, 2)
print(next(ds.get_data()))
print(ds.size())

ds = SomeNewDf()
ds = BatchData(ds, 2)
print(next(ds.__iter__()))
print(len(ds))

ds = SomeNewDf()
ds = BatchData(ds, 2)
print(next(ds.get_data()))
print(ds.size())

ds = SomeOldDf()
for dp in ds:
  print(dp)

ds = SomeNewDf()
for dp in ds:
  print(dp)

ds = SomeOldDf()
for dp in ds.get_data():
  print(dp)

ds = SomeNewDf()
for dp in ds.get_data():
  print(dp)
```

This gracefully handles old **and** new code **without** any changes, but takes the liberty to throw a deprecation warning.",latest commit following work python import class size self return self yield class self return self yield print next print print next print print next print print next print print print print print gracefully old new code without liberty throw deprecation warning,issue,negative,positive,neutral,neutral,positive,positive
416149851,"I do hope nothing breaks.. I want to learn if there are any magic alternatives, before adopting a breaking change and worrying about version numbers..",hope nothing want learn magic breaking change worrying version,issue,negative,positive,positive,positive,positive,positive
416060565,"Then the only way is to use get_data() and just add `__iter__()`. Certainly, we do not want to call reflection magic during pre-fetching. What about introducing a ""(RNG)Dataflow_v2"" class (similar to what TensorFlow is doing as a workaround to not break semver promises eg. concat_v2 vs concat back then).

For each technical problem there is a legal solution: just bump the major version ;-)

**Edit** Do you really care about this compatibility? Do you care about SemVer? I mean TP is v0.8.8.",way use add certainly want call reflection magic class similar break back technical problem legal solution bump major version edit really care compatibility care mean,issue,positive,positive,neutral,neutral,positive,positive
416060087,"> I guess overwrite get_data in the base class is sufficient:

This does not solve the case to call old code (user-writter dataflow) with new method (__iter__).",guess overwrite base class sufficient solve case call old code new method,issue,negative,negative,negative,negative,negative,negative
416054227,"If refactoring the dataflow is ok, there are some other aspects I would like to discuss or even rebel against:

- `reset_state` seems to be unnecessary long. Why not simply `reset`? There is no ambiguity but a high chance for typos when your mind is already pre-paring the code line.
- There should be a consistent naming: Currently, we have: `dp` for data point (which is ok) and we have `df` and `ds` for data-flow (I guess the latter is data-stream?). If would do another pass and just rename `ds` to `df`.

And the last point (not this PR) would be a guarantee to sample each data-point per epoch. As far as I understand, `Prefetch<someSuffix>` allows me to hog the entire CPU power and pre-process data. But having 12 worker processes gives me a [really high chance](https://en.wikipedia.org/wiki/Birthday_problem) to miss some examples in one epoch when each worker shuffles data.",would like discus even rebel unnecessary long simply reset ambiguity high chance mind already code line consistent naming currently data point guess latter would another pas rename last point would guarantee sample per epoch far understand hog entire power data worker really high chance miss one epoch worker data,issue,positive,positive,neutral,neutral,positive,positive
416052979,"I guess overwrite `get_data` in the base class is sufficient:

1. base-class will take care of `get_data` and correctly forward to `__iter__`
2. `__iter__` will be called and everything is ok.",guess overwrite base class sufficient take care correctly forward everything,issue,negative,negative,negative,negative,negative,negative
416017453,"Build the graph yourself (by calling build_graph, for example) in inference mode only.
https://tensorpack.readthedocs.io/tutorial/inference.html
Then follow tensorflow documentation on how to use tensorboard.",build graph calling example inference mode follow documentation use,issue,negative,neutral,neutral,neutral,neutral,neutral
416016615,"Hi!
How can i see the graph of Inference only in Tensorboard?
Thanks you!",hi see graph inference thanks,issue,negative,positive,positive,positive,positive,positive
415554651,"Weights are now fixed (the missing weights for flownet-c) and the model is fixed
Sometimes a single character makes a difference.  :unamused:

I uploaded a new version of (flownet-c.npz). I tested (downloaded the weights  and started the run) FlowNet-S and FlowNet-C (results are below):

| Model|  AEE(sintel clean) |
| ------ | ------ |
| FlowNet-S | 3.82|
| FlowNet-C | 3.08|
| FlowNet2 | 2.10 (authors report 2.03) |",fixed missing model fixed sometimes single character difference unamused new version tested run clean report,issue,negative,positive,neutral,neutral,positive,positive
415329233,"Yes, for FlowNet-S there are some waaaaaaay out of range. However, on a few examples against the reference implementation I get:

```
actual (1, 2, 96, 128)  expected  (1, 2, 96, 128)
sum-error  0.0009913198
mean-error  3.323065428514557e-14
```

I will test those

![flow0264](https://user-images.githubusercontent.com/6756603/44512767-de17d880-a6bb-11e8-9c6d-d031777c4884.jpg)
![flow0314](https://user-images.githubusercontent.com/6756603/44512772-e1ab5f80-a6bb-11e8-9929-802574a11e29.jpg)
![flow0248](https://user-images.githubusercontent.com/6756603/44512775-e40db980-a6bb-11e8-8509-33ed3b6e15d7.jpg)
![flow0002](https://user-images.githubusercontent.com/6756603/44512792-ecfe8b00-a6bb-11e8-9db3-c07f1b4d8fd5.jpg)
![flow0061](https://user-images.githubusercontent.com/6756603/44512799-f12aa880-a6bb-11e8-897b-66ddbd4b62f3.jpg)

This is a little bit intriguing as all these modules are used in FlowNet2 itself. But you are right according to the paper the numbers should be somewhere around ~4.50. 

> It seems that these two models are not doing anything useful.

For some inputs, it is the case.",yes range however reference implementation get actual test flow flow flow flow flow little bit intriguing used right according paper somewhere around two anything useful case,issue,positive,positive,positive,positive,positive,positive
415312369,"Setting the capacity to 1 is enough to achieve what you said because the staging op will block if capacity is reached.
That sounds like a reasonable thing to do. However some benchmarks will be needed to see how it affects speed or memory.",setting capacity enough achieve said staging block capacity like reasonable thing however see speed memory,issue,negative,positive,neutral,neutral,positive,positive
415311604,"why  not add a control dependence between Stage and Unstage  to make unstage run before stage .
so we can set the StagingArea  capacity to 1, it can work as the same way using smaller GPU memory",add control dependence stage make run stage set capacity work way smaller memory,issue,negative,neutral,neutral,neutral,neutral,neutral
415196199,"Thanks! I can reproduce the 2.1 results. But with flownet2-s:
```
python3 flownet2.py --load flownet2-s.npz  --model flownet2-s --sintel_path ~/data/Sintel/training/
```
I got an error of 13.365433.

And with flownet2-c, I saw:
```
WRN The following variables are in the graph, but not found in the dict: upsampled_flow3_to_2/bias:0, upsampled_flow4_to_3/bias:0, upsampled_flow5_to_4/bias:0, upsampled_flow6_to_5/bias:0
```
which also gives me an error 13.36 in the end. It seems that these two models are not doing anything useful.",thanks reproduce python load model got error saw following graph found also error end two anything useful,issue,negative,positive,positive,positive,positive,positive
415147240,I hope I cherry picked all commits during rebasing on master.,hope cherry picked master,issue,negative,neutral,neutral,neutral,neutral,neutral
415143799,"I add the note about the mysterious 20 (which is essentially the max_displacement the network can handle). I not gonna touch any operation or change them. The version works fine. Feel free to change anything.

AEE(train) on Sintel-Clean of *2.104513* (authors report 2.03). 
Despite all effort, there must be a different between Caffe and TensorFlow. But I guess this difference is ok. For the evaluation, I followed [this implementation](https://github.com/NVIDIA/flownet2-pytorch/blob/6827b4c59d1b5d58bcce492e2d5ec2d6cd1e42de/datasets.py#L68-L70) and simply crop the input images.",add note mysterious essentially network handle gon na touch operation change version work fine feel free change anything train report despite effort must different guess difference evaluation implementation simply crop input,issue,positive,positive,positive,positive,positive,positive
415139958,Actually with TF 1.4 the code should be able to run most of the times. It may crash for some inputs due to TF bugs.,actually code able run time may crash due,issue,negative,positive,positive,positive,positive,positive
415139100,"I run some tests with today's master code and `--config MODE_MASK=True MODE_FPN=True` on 1 & 8 V100s, with `export TF_CUDNN_USE_AUTOTUNE=0` so the speed is faster and more stable at the beginning.

Starting from the 2nd epoch, 1 V100 takes 2min29s per epoch and 8 V100s take 2min51s per epoch.",run today master code export speed faster stable beginning starting epoch per epoch take per epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
415129175,Closing as the crash does not seem to be a tensorpack issue. Uninstall and reinstall horovod is worth trying.,crash seem issue reinstall worth trying,issue,negative,positive,positive,positive,positive,positive
415010522,"@ppwwyyxx 
I want to modify dorefa-net, but  I met a problem on how to define a learned variable which is like : `[alpha, a_1*alpha,a_2*alpha,...,a_k*alpha]`, in which `alpha`  and` a_1 ~ a_k` are all learned numbers in this variable.  Is this kind of variable supported in tensorpack?",want modify met problem define learned variable like alpha alpha alpha alpha alpha learned variable kind variable,issue,positive,positive,positive,positive,positive,positive
414768267,"`__iter__` and `__len__` sounds good!

For compatibility, we need to consider 

1. calling old code with new method
2. calling new code with old method.

Having `get_data` wrap `__iter__` would only solve 2. Having them wrap each other sound weird.",good compatibility need consider calling old code new method calling new code old method wrap would solve wrap sound weird,issue,negative,positive,positive,positive,positive,positive
414765070,It was added. But in general most dataflows still assume list instead of dict. #768 aims to make dataflow work for dicts.,added general still assume list instead make work,issue,negative,positive,neutral,neutral,positive,positive
414180858,"My TF version is limited to 1.4.0, so that choosing 0.8.5 version fasterrcnn code...  And i also evaluated the model, the ap is also almost 0. And i verified the model is loaded.
Do u  have the pretrained model of 0.8.5 version model?  if convenient, can u share it with me.
Thanks for your apply!",version limited choosing version code also model also almost model loaded model version model convenient share thanks apply,issue,negative,positive,neutral,neutral,positive,positive
414138009,"Evaluating (instead of finetuning) the model will give you the correct performance. (If not, upgrade fasterrcnn code as well)

If you are asking why finetuning performance is not what you expected, that's a machine learning question so we don't answer it. A smaller learning rate is definitely needed. ",instead model give correct performance upgrade code well performance machine learning question answer smaller learning rate definitely,issue,positive,neutral,neutral,neutral,neutral,neutral
414127127,"Thank you for your apply, here is the issue:

1. What you did:
[train command, in order to transfer/finetune] 
python train.py --gpu 0,1,2,3 --datadir /coco --load /pretrained_models/COCO-R50C4-MaskRCNN-Standard.npz 

2. What you observed, including but not limited to the __entire__ logs.
  The AP performance on COCO val2014 dataset is almost 0. And the training precision/recall almost improves, it seems that the pretrained model on COCO is same as the ImageNet-R50.npz.

4. Your environment:
  + Python python3.6
  + TF version: 1.4.0.
  + Tensorpack version: 0.8.8 (But using 0.8.5 version of FastRCNN code)
",thank apply issue train command order python load limited performance coco almost training almost model coco environment python python version version version code,issue,negative,negative,neutral,neutral,negative,negative
414099733,"Then it's normal.
TF sometimes prints unharmful error.",normal sometimes unharmful error,issue,negative,positive,positive,positive,positive,positive
414099530,"The training is continuing, but quite slow ` 40%|###9      |199/500[03:10<04:49, 1.04it/s]`.
I have used this [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn) implementation before. It doesn't report any error even with `ResNet-101` backbone. I have set `MODEL_MASK=False` but I still got this error.
Can you tell me what caused this error? Thanks.",training quite slow used implementation report error even backbone set still got error tell error thanks,issue,negative,negative,neutral,neutral,negative,negative
414099158,"Did the training stop or not?

If yes, either upgrade your hardware or use a less memory-consuming model (e.g. FPN).",training stop yes either upgrade hardware use le model,issue,negative,neutral,neutral,neutral,neutral,neutral
413924510,"The model is a standard TF checkpoint so you can read it with `tf.train.NewCheckpointReader`, or use https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.varmanip.load_chkpt_vars.

After loading a model you can refer to numpy documentation on how to save it into numpy format.",model standard read use loading model refer documentation save format,issue,negative,neutral,neutral,neutral,neutral,neutral
413792376,"@ppwwyyxx 
after training and getting the quantized model, how to print all the weights values of a specific layer, given the quantized model? (or save them into a .npy file)",training getting model print specific layer given model save file,issue,negative,neutral,neutral,neutral,neutral,neutral
413746217,I have no context of what you're doing or how you met the error so I can't help. The error is unrelated to tensorpack so you'd better solve it by learning tensorflow.,context met error ca help error unrelated better solve learning,issue,negative,positive,positive,positive,positive,positive
413737033,"Hi!
When i try to run `print((sess.run(w)))`, i got the error as below:
`FailedPreconditionError (see above for traceback): Attempting to use uninitialized value res1.0/conv1/Wn
	 [[Node: res1.0/conv1/Wn/read = Identity[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](res1.0/conv1/Wn)]]
	 [[Node: mul_4/_1 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_49_mul_4"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]`

I tried to initialize the variables: `sess.run(tf.initialize_variables([v for v in tf.all_variables()]))` but i still get that error.
Thanks you!",hi try run print got error see use value node identity node tried initialize still get error thanks,issue,negative,positive,positive,positive,positive,positive
413585295,"Hi!
I am sorry if i asked some questions which isn't related to Tensorpack.
Could you show me how to change the value of variable individually?
For example: i only want to change some weights in a variable with shape = (3, 3, 16, 16)
Thanks you very much!",hi sorry related could show change value variable individually example want change variable shape thanks much,issue,positive,negative,neutral,neutral,negative,negative
413417200,"Use https://www.tensorflow.org/api_docs/python/tf/train/NewCheckpointReader to read your checkpoint.
Or with tensorpack predictor, add the name of variables to `output_names`.
Or use tf.Print.",use read predictor add name use,issue,negative,neutral,neutral,neutral,neutral,neutral
413416233,"Hi!
How can i print value of weight variable during Inference. 
Thanks you!",hi print value weight variable inference thanks,issue,positive,positive,positive,positive,positive,positive
413406706,"Hi!
Can i have 2 **remap_variables()** in code. Because i have 2 quantization function (one is normal TTQ, another is to change the weight again).
Thanks you!
",hi code quantization function one normal another change weight thanks,issue,negative,positive,positive,positive,positive,positive
413111791,"If you still see multi-gpu running faster than 1 gpu, please run it with no modification to the code and post your command and full logs like what's in the issue template.",still see running faster please run modification code post command full like issue template,issue,positive,positive,positive,positive,positive,positive
413111317,"<del>The export will not work because tensorpack enable autotune when import.</del>

I'm sure multi-gpu is not faster than 1 gpu if you wait for like 50 epochs.",export work enable sure faster wait like,issue,positive,positive,positive,positive,positive,positive
413110247,"I disable warmup by export TF_CUDNN_USE_AUTOTUNE=0 
the problem is still here.
How can multi gpu with allreduce run faster than 1 gpu",disable export problem still run faster,issue,negative,neutral,neutral,neutral,neutral,neutral
413083279,"As a reference, when trained from scratch with standard settings, the time for the first several epochs on 8 V100s should be: 17min, 9min, 7min, 6min, 5min, for the same R50C4 model.",reference trained scratch standard time first several min min min min min model,issue,negative,positive,neutral,neutral,positive,positive
413081767,Your third column is way too fast. I believe you've made mistakes in your code or setup or command and it's actually doing something other than what you think.,third column way fast believe made code setup command actually something think,issue,negative,positive,neutral,neutral,positive,positive
413080960,"If you're asking about what each class means in cifar10, that's not a feature tensorpack provided, although it will make sense to add it. Now please figure that out yourself.",class feature provided although make sense add please figure,issue,negative,neutral,neutral,neutral,neutral,neutral
413076704,"How can i change `meta = dataset.ILSVRCMeta()` `words = meta.get_synset_words_1000()` to Cifar10 dataset. Because there is no that kind of function in dataflow.dataset.Cifar10.

Thanks you!
",change meta kind function thanks,issue,positive,positive,positive,positive,positive,positive
413064219,"my question is  2 gpu  run faster  than 1 gpu without gradient allreduce
I can not find some way to debug it.
CUDA_VISIBLE_DEVICES=0,1 python train.py ,I run many times and speed up from about 60 step ,then come to 4 it/s.
```
  (global_step 137) finished at 1534294900.3835046, time:0.11500763893127441.
[0815 01:01:40 @base.py:247]  (global_step 138) begin at 1534294900.3839097
 66%|###############################################################                                 |138/210[01:33<00:15, 4.53it/s][0815 01:01:40 @base.py:254]  (global_step 138) finished at 1534294900.504368, time:0.12045836448669434.
[0815 01:01:40 @base.py:247]  (global_step 139) begin at 1534294900.5047371
[0815 01:01:40 @base.py:254]  (global_step 139) finished at 1534294900.6229901, time:0.11825299263000488.
[0815 01:01:40 @base.py:247]  (global_step 140) begin at 1534294900.6234102
[0815 01:01:40 @base.py:254]  (global_step 140) finished at 1534294900.7403178, time:0.11690759658813477.
[0815 01:01:40 @base.py:247]  (global_step 141) begin at 1534294900.7407358
[0815 01:01:40 @base.py:254]  (global_step 141) finished at 1534294900.8584886, time:0.1177527904510498.
[0815 01:01:40 @base.py:247]  (global_step 142) begin at 1534294900.8589025
 68%|################################################################9                               |142/210[01:34<00:16, 4.12it/s][0815 01:01:41 @base.py:254]  (global_step 142) finished at 1534294901.5618732, time:0.7029707431793213.
[0815 01:01:41 @base.py:247]  (global_step 143) begin at 1534294901.5622423
[0815 01:01:41 @base.py:254]  (global_step 143) finished at 1534294901.6763937, time:0.1141514778137207.
[0815 01:01:41 @base.py:247]  (global_step 144) begin at 1534294901.676972
[0815 01:01:42 @base.py:254]  (global_step 144) finished at 1534294902.0140057, time:0.337033748626709.
[0815 01:01:42 @base.py:247]  (global_step 145) begin at 1534294902.0145013
 69%|##################################################################2                             |145/210[01:34<00:14, 4.58it/s][0815 01:01:42 @base.py:254]  (global_step 145) finished at 1534294902.143973, time:0.1294717788696289.
[0815 01:01:42 @base.py:247]  (global_step 146) begin at 1534294902.1443706
[0815 01:01:42 @base.py:254]  (global_step 146) finished at 1534294902.8292904, time:0.6849198341369629.
[0815 01:01:42 @base.py:247]  (global_step 147) begin at 1534294902.829791
[0815 01:01:43 @base.py:254]  (global_step 147) finished at 1534294903.529799, time:0.7000079154968262.
[0815 01:01:43 @base.py:247]  (global_step 148) begin at 1534294903.5302541
 70%|###################################################################6                            |148/210[01:36<00:22, 2.78it/s][0815 01:01:43 @base.py:254]  (global_step 148) finished at 1534294903.6478417, time:0.11758756637573242.
[0815 01:01:43 @base.py:247]  (global_step 149) begin at 1534294903.6482408
[0815 01:01:43 @base.py:254]  (global_step 149) finished at 1534294903.772947, time:0.12470626831054688.
[0815 01:01:43 @base.py:247]  (global_step 150) begin at 1534294903.7733433
[0815 01:01:43 @base.py:254]  (global_step 150) finished at 1534294903.88845, time:0.11510658264160156.
[0815 01:01:43 @base.py:247]  (global_step 151) begin at 1534294903.8888514
 72%|#####################################################################                           |151/210[01:36<00:15, 3.75it/s][0815 01:01:44 @base.py:254]  (global_step 151) finished at 1534294904.1664782, time:0.2
```

CUDA_VISIBLE_DEVICES=0 python train.py 

```
 66%|###############################################################5                                |139/210[03:49<01:43, 0.69it/s][0815 01:09:38 @base.py:254]  (global_step 139) finished at 1534295378.650176, time:2.45257830619812.
[0815 01:09:38 @base.py:247]  (global_step 140) begin at 1534295378.650516
 67%|################################################################                                |140/210[03:51<02:08, 0.55it/s][0815 01:09:40 @base.py:254]  (global_step 140) finished at 1534295380.852188, time:2.201672077178955.
[0815 01:09:40 @base.py:247]  (global_step 141) begin at 1534295380.8525262
[0815 01:09:41 @base.py:254]  (global_step 141) finished at 1534295381.1603434, time:0.3078172206878662.
[0815 01:09:41 @base.py:247]  (global_step 142) begin at 1534295381.1607199
 68%|################################################################9                               |142/210[03:54<01:46, 0.64it/s][0815 01:09:43 @base.py:254]  (global_step 142) finished at 1534295383.447481, time:2.2867610454559326.
[0815 01:09:43 @base.py:247]  (global_step 143) begin at 1534295383.447814
[0815 01:09:43 @base.py:254]  (global_step 143) finished at 1534295383.7949831, time:0.3471691608428955.
[0815 01:09:43 @base.py:247]  (global_step 144) begin at 1534295383.7953558
 69%|#################################################################8                              |144/210[03:55<01:02, 1.05it/s][0815 01:09:44 @base.py:254]  (global_step 144) finished at 1534295384.1309144, time:0.3355586528778076.
[0815 01:09:44 @base.py:247]  (global_step 145) begin at 1534295384.1312623
[0815 01:09:44 @base.py:254]  (global_step 145) finished at 1534295384.4696224, time:0.338360071182251.
[0815 01:09:44 @base.py:247]  (global_step 146) begin at 1534295384.4699774
 70%|##################################################################7                             |146/210[03:57<01:08, 0.93it/s][0815 01:09:46 @base.py:254]  (global_step 146) finished at 1534295386.5130866, time:2.043109178543091.
[0815 01:09:46 @base.py:247]  (global_step 147) begin at 1534295386.5134234
[0815 01:09:47 @base.py:254]  (global_step 147) finished at 1534295387.00269, time:0.48926663398742676.
[0815 01:09:47 @base.py:247]  (global_step 148) begin at 1534295387.0030763
 70%|###################################################################6                            |148/210[03:58<00:46, 1.34it/s][0815 01:09:47 @base.py:254]  (global_step 148) finished at 1534295387.3571424, time:0.3540661334991455.
[0815 01:09:47 @base.py:247]  (global_step 149) begin at 1534295387.3574877
 71%|####################################################################1                           |149/210[04:00<01:39, 0.62it/s][0815 01:09:49 @base.py:254]  (global_step 149) finished at 1534295389.8615568, time:2.5040690898895264.
[0815 01:09:49 @base.py:247]  (global_step 150) begin at 1534295389.8618948
[0815 01:09:50 @base.py:254]  (global_step 150) finished at 1534295390.2188885, time:0.3569936752319336.
[0815 01:09:50 @base.py:247]  (global_step 151) begin at 1534295390.219254
 72%|#####################################################################                           |151/210[04:03<01:25, 0.69it/s]
```",question run faster without gradient find way python run many time speed step come finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time python finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin finished time begin,issue,negative,positive,positive,positive,positive,positive
412974857,"1. quantize the saved weights by running fw() on them, as mentioned in https://github.com/tensorpack/tensorpack/issues/573
2. change the model so it does not run fw() on the weights.


The 1. and 2. cancel each other so there is actually no point doing so.",quantize saved running change model run cancel actually point,issue,negative,neutral,neutral,neutral,neutral,neutral
412972488,"Thanks you very much! 
But as i know, the trained model that is saved in checkpoint doesn’t have quantized weight.
So if i want to run Inference on quantized weight model, i have to change it manually?
If i have to do that, how can i do that?
Thanks you!",thanks much know trained model saved weight want run inference weight model change manually thanks,issue,positive,positive,positive,positive,positive,positive
412971397,The NaN issue turns out to because the learning rate wasn't correctly scaled for <8 GPUs. The code was only tested with 8+ GPUs. I'll push a fix for that one later.,nan issue turn learning rate correctly scaled code tested push fix one later,issue,negative,neutral,neutral,neutral,neutral,neutral
412967222,"There is an inference function in the TTQ example code already:
https://github.com/tensorpack/tensorpack/blob/17955e8955284d9f91f079dd515ebf1b1a29ec9b/examples/DoReFa-Net/alexnet-dorefa.py#L169-L193",inference function example code already,issue,negative,neutral,neutral,neutral,neutral,neutral
412961776,Thanks. The code works for newer version of tensorflow. But I'll adopt your fix.,thanks code work version adopt fix,issue,positive,positive,positive,positive,positive,positive
412852775,"When I use  SyncMultiGPUTrainerReplicated trainer by the command
CUDA_VISIBLE_DEVICES=0,1 python train.py (all the default config)

then it come to 3~4 it/s after 100 step, 
it's incredible becase when I run CUDA_VISIBLE_DEVICES=0  python train.py it always come to about 0.5 it/s
then I print the step time cost ,:
```
[0814 12:15:17 @base.py:247] hvd rank 0, (global_step 61) begin at 1534248917.7014177
 29%|############################1                                                                    |61/210[01:37<01:42, 1.46it/s][0814 12:15:18 @base.py:254] hvd rank 0, (global_step 61) finished at 1534248918.3272123, time:0.6257946491241455.
[0814 12:15:18 @base.py:247] hvd rank 0, (global_step 62) begin at 1534248918.3275385
 30%|############################6                                                                    |62/210[01:38<01:55, 1.28it/s][0814 12:15:19 @base.py:254] hvd rank 0, (global_step 62) finished at 1534248919.20141, time:0.8738715648651123.
[0814 12:15:19 @base.py:247] hvd rank 0, (global_step 63) begin at 1534248919.2017226
 30%|#############################1                                                                   |63/210[01:39<01:48, 1.35it/s][0814 12:15:19 @base.py:254] hvd rank 0, (global_step 63) finished at 1534248919.9030836, time:0.7013609409332275.
[0814 12:15:19 @base.py:247] hvd rank 0, (global_step 64) begin at 1534248919.9034119
[0814 12:15:20 @base.py:254] hvd rank 0, (global_step 64) finished at 1534248920.0661807, time:0.16276884078979492.
[0814 12:15:20 @base.py:247] hvd rank 0, (global_step 65) begin at 1534248920.0665429
[0814 12:15:20 @base.py:254] hvd rank 0, (global_step 65) finished at 1534248920.2433767, time:0.1768338680267334.
[0814 12:15:20 @base.py:247] hvd rank 0, (global_step 66) begin at 1534248920.243729
 31%|##############################4                                                                  |66/210[01:40<01:15, 1.91it/s][0814 12:15:20 @base.py:254] hvd rank 0, (global_step 66) finished at 1534248920.8267455, time:0.5830163955688477.
[0814 12:15:20 @base.py:247] hvd rank 0, (global_step 67) begin at 1534248920.8270683
[0814 12:15:20 @base.py:254] hvd rank 0, (global_step 67) finished at 1534248920.9875398, time:0.16047143936157227.
[0814 12:15:20 @base.py:247] hvd rank 0, (global_step 68)
```",use trainer command python default come step incredible run python always come print step time cost rank begin rank finished time rank begin rank finished time rank begin rank finished time rank begin rank finished time rank begin rank finished time rank begin rank finished time rank begin rank finished time rank,issue,negative,negative,negative,negative,negative,negative
412647020,It may also help to set `NCCL_DEBUG=INFO` to see if there is any issue in communication. Also make sure horovod was rebuilt and reinstalled after each time you install tensorflow.,may also help set see issue communication also make sure rebuilt time install,issue,positive,positive,positive,positive,positive,positive
412608235,"I've run into the same issue (not NaN, but segfault) with horovod before, not sure which library is causing it.
The replicated trainer should be more stable to use.",run issue nan sure library causing replicated trainer stable use,issue,positive,positive,positive,positive,positive,positive
412441818,"Did you pull some magic tricks here? Your code is fine! :+1: 
Still the network is not fully convolutional anymore.",pull magic code fine still network fully convolutional,issue,negative,positive,positive,positive,positive,positive
412438167,What you did look similar to my code. Mine was specialized for stride_1=1 and stride_2=2.,look similar code mine specialized,issue,negative,neutral,neutral,neutral,neutral,neutral
412436006,"Not sure, what you are doing, but the CPU version has 8 nested for-loops. You might be able to get away with the loop over `B`. 

Anyway, a working and correct pure TensorFlow implemenation is:

```python
def correlation(A, B, kernel_size, max_displacement, stride_1, stride_2, pad, data_format):
    import numpy as np
    """""" This is a fallback option for the correlation cost layer
    """"""
    assert kernel_size == 1
    assert data_format == 'NCHW'

    b, c, h, w = A.shape.as_list()

    r = max_displacement / stride_2
    d = 2 * r + 1
    border = max_displacement
    dr = int(max_displacement / stride_2)

    Cout = int(d * d)
    Hout = int(np.ceil((h + 2 * (pad - border)) / stride_1))
    Wout = int(np.ceil((w + 2 * (pad - border)) / stride_1))

    Apad = tf.pad(A, [[0, 0], [0, 0], [pad, pad], [pad, pad]])
    Bpad = tf.pad(B, [[0, 0], [0, 0], [pad, pad], [pad, pad]])

    res = []

    for tj in range(-dr, dr + 1):
        for ti in range(-dr, dr + 1):
            res_h = []
            for h in range(0, Hout):
                h1 = int(h * stride_1 + max_displacement)
                res_w = []
                for w in range(0, Wout):
                    w1 = int(w * stride_1 + max_displacement)

                    patchA = Apad[:, :, h1:h1+1, w1:w1+1]

                    w2 = w1 + ti * stride_2
                    h2 = h1 + tj * stride_2

                    patchB = Bpad[:, :, h2:h2+1, w2:w2+1]

                    ans = tf.reduce_mean(patchA * patchB, axis=[1], keepdims=True)
                    res_w.append(ans)

                res_h.append(tf.concat(res_w, axis=3))
            res.append(tf.concat(res_h, axis=2))
    res = tf.concat(res, axis=1)
    return res
```

But it takes years until the graph is built (input shape [1, 256, 48, 64]). I cancelled the process after a few minutes.
And this requires H and W dimension to be known beforehand (not `None`)",sure version might able get away loop anyway working correct pure python correlation pad import fallback option correlation cost layer assert assert border pad border pad border pad pad pad pad pad pad pad pad range ti range range range ti return graph built input shape process dimension known beforehand none,issue,negative,positive,positive,positive,positive,positive
412420008,"But a CPU version would be slower than a python version running on a GPU. I made an implementation although I'm not very sure about its correctness. It's about 3~5x slower on GPU
```python
from __init__ import correlation_cost

h = 64
w = 64
CHAN = 256
BATCH = 4

shape = [BATCH, CHAN, h, w]
ina = tf.random_normal(shape, dtype=tf.float32)
inb = tf.random_normal(shape, dtype=tf.float32)

d = 20
D = 21
assert d % 2 == 0

out = correlation_cost(
        ina, inb, 1, d, stride_1=1, stride_2=2, pad=d,
        data_format='NCHW')

inb = tf.pad(inb, [[0, 0], [0, 0], [d, d], [d, d]])

res = []
for k1 in range(0, D):
    start_h = k1 * 2
    for k2 in range(0, D):
        start_w = k2 * 2
        s = tf.slice(inb, [0, 0, start_h, start_w], [-1, -1, h, w])
        ans = tf.reduce_mean(ina * s, axis=1, keepdims=True)
        res.append(ans)
res = tf.concat(res, axis=1)   # ND^2HW


def bench(op, warmup, iter):
    for k in range(warmup):
        op.run()
    start = time.perf_counter()
    for k in range(iter):
        op.run()
    duration = time.perf_counter() - start
    return duration

sess = tf.Session()
with sess.as_default():
    o1, o2 = sess.run([out, res])
    print(np.abs(o1 - o2).max())

    print(bench(out.op, 10, 20))
    print(bench(res.op, 10, 20))
```",version would python version running made implementation although sure correctness python import batch shape batch shape shape assert range range bench iter range start range iter duration start return duration sess print print bench print bench,issue,negative,positive,positive,positive,positive,positive
412418269,But I still can not train right now ...The same error still in there .What shoud I do @ppwwyyxx ,still train right error still,issue,negative,positive,positive,positive,positive,positive
412417514,It is a bug introduced two days ago and it's now fixed. Thanks!,bug two day ago fixed thanks,issue,negative,positive,positive,positive,positive,positive
412417391,"Wow, what an incredible smart move on my side. I didn't saw the CMakeLists.txt being in the .gitignore file :joy:

I started a rudimentary python implementation but gave it up in favor of the CPU version. 

1. That's only required when using CUDA >=9.1 (in an incompatible combination with Ubuntu18.04)
2. TF 1.9, 1.10 stable versions work (was a TensorFlow issue before)
3. is now written in the Readme.md
4. I don't see this point. It works fine even on a GTX 960.

Another workaround would be just the CPU version, which reduces all your points to ""2"". If I find some free time, I could hack a fallback option as pure TensorFlow.",wow incredible smart move side saw file joy rudimentary python implementation gave favor version incompatible combination stable work issue written see point work fine even another would version find free time could hack fallback option pure,issue,positive,positive,positive,positive,positive,positive
412410254,"So they call it a ""patch"" but only use `kernel_size=1`?
In this case it's not too bad to write it in python with a for loop over D^2 slices, compute each slice and concat them in the end. The op is only used once so the speed may not be a huge concern.",call patch use case bad write python loop compute slice end used speed may huge concern,issue,negative,negative,negative,negative,negative,negative
412371834,"I hope we don't include any ops. I agree with https://github.com/tensorflow/tensorflow/pull/21392#issuecomment-412354516 that it's not easy to maintain external ops and let it build automatically. After copying the missing CMakeLists.txt from your repo it still takes me about 10 minutes to get things running. I have to deal with:
1. Set the compiler that nvcc uses to gcc <= 5
2. TF version must match exactly. Did some hacks but seems unable to make it work with my TF 1.9.0rc1. Reinstall TF 1.10
3. download cub and set path
4. change ``--gpu-architecture``.

There shouldn't be so many manual steps to get an example running.",hope include agree easy maintain external let build automatically missing still get running deal set compiler version must match exactly unable make work reinstall cub set path change many manual get example running,issue,positive,positive,neutral,neutral,positive,positive
412334825,"I think everything is complete for now. The training protocol seems to be just a tedious task when re-implementing. But this would be another PR. If this PR ok, we should copy the weights to models.tensorpack.com as well. I can do this on the server-side (no need to re-upload them)",think everything complete training protocol tedious task would another copy well need,issue,negative,negative,negative,negative,negative,negative
412247972,"@ppwwyyxx So sorry I didn't notice that when readling the link, thanks a lot",sorry notice link thanks lot,issue,negative,negative,negative,negative,negative,negative
412247648,The link above explicitly said do not use `tf.Variable`,link explicitly said use,issue,negative,neutral,neutral,neutral,neutral,neutral
412247232,"@ppwwyyxx Here's the layer implementation
```
def InstanceNorm(net, train=True):
    batch, rows, cols, channels = [i.value for i in net.get_shape()]
    var_shape = [channels]
    mu, sigma_sq = tf.nn.moments(net, [1,2], keep_dims=True)
    shift = tf.Variable(tf.zeros(var_shape))
    scale = tf.Variable(tf.ones(var_shape))
    epsilon = 1e-3
    normalized = (net-mu)/(sigma_sq + epsilon)**(.5)
    return scale * normalized + shift
@layer_register()
def conv_layer(net, filters, kernel_size, strides, relu=True):
    net = Conv2D('conv', net, filters = filters, kernel_size = kernel_size, strides = strides)
    net = InstanceNorm(net)
    if relu:
        net = tf.nn.relu(net)
    return net
```
```
class Transform(ModelDesc):
    def __init__(self, data_format='NHWC'):
        pass
    def get_inputs(self):
        pass
    def get_logits(self, image):
        logits = (LinearWrap(image)
                     .conv_layer('transfer_conv1', filters=32, kernel_size=9, strides=1)
                     .conv_layer('transfer_conv2', 64)
                     .conv_layer('transfer_conv3', 128)
```
I was writing a style transfer model, `get_logits` used for transform input image to another style, and I also defined a vgg model in `class Transform` to extract features of styled_image and transformed_image.
I assume the problem is from the variables defined in `InstanceNorm`, but I can't see why.",layer implementation net batch mu net shift scale epsilon epsilon return scale shift net net net net net net net return net class transform self pas self pas self image image writing style transfer model used transform input image another style also defined model class transform extract assume problem defined ca see,issue,negative,neutral,neutral,neutral,neutral,neutral
412245616,"http://tensorpack.readthedocs.io/tutorial/trainer.html#tower-trainer

If you can't figure out what's wrong after reading the above tutorial, post your layer implementation.",ca figure wrong reading tutorial post layer implementation,issue,negative,negative,negative,negative,negative,negative
412167576,"Added FlowNet2-C. ~~Until they merged this [PR](https://github.com/tensorflow/tensorflow/pull/21392) somehow, tensorpack needs to ship the custom operation. To keep this C++ stuff reasonable small, I just included a 1:1 refactoring of the pytorch version with giving credits (which is basically a refactored version of the original version) instead of a complete re-implementation GPU, CPU and some modifications (see PR).~~

The latest commit adds the full implementation.",added somehow need ship custom operation keep stuff reasonable small included version giving basically version original version instead complete see latest commit full implementation,issue,positive,positive,positive,positive,positive,positive
412139076,"1. warmup mainly means autotune
2. the number is a rough estimate and has no mathematical meaning
3. (1) you can write a callback and put a timer in `trigger_step`
    (2) I don't know how to get time spent of a specific op in tensorflow except to enable tracing and parse the result",mainly number rough estimate mathematical meaning write put timer know get time spent specific except enable tracing parse result,issue,negative,negative,neutral,neutral,negative,negative
412024502,"I  can only find this in fastrcnn notes
```
The training will start very slow due to convolution warmup, until about 10k steps to reach a maximum speed. Then the training speed will slowly decrease due to more accurate proposals.
```

1,.You mean AUTOTUNE mainly do the convolution warmup?
2.why 10K would reach a maximum speed , there are about 10k kinds of image shape in the COCO?
3.If I want to check the performence of trainning by step cost time and performence  of gradient allreduce , do you have some suggestion?",find training start slow due convolution reach maximum speed training speed slowly decrease due accurate mean mainly convolution would reach maximum speed image shape coco want check step cost time gradient suggestion,issue,negative,negative,neutral,neutral,negative,negative
412002988,"The weights of FlowNet-S are used. FlowNet-s is a thin version.

> For the stack configuration we append upper- or lower-case letters to indicate the original
FlowNet or the thin version with 3/8 of the channels.",used thin version stack configuration append indicate original thin version,issue,negative,negative,negative,negative,negative,negative
411993141,"It will improve performance at the beginning as mentioned in the fastrcnn notes.
I don't know whether it improves overall performance or not.",improve performance beginning know whether overall performance,issue,negative,neutral,neutral,neutral,neutral,neutral
411992855,"If each data has different size,  , export TF_CUDNN_USE_AUTOTUNE=0 may improve train performence
It would be very nice to let others know that  disable cudnn autotune in  FasterRCNN  example",data different size export may improve train would nice let know disable example,issue,positive,positive,positive,positive,positive,positive
411990541,"Thx
and I get wrong result after TF_CUDNN_USE_AUTOTUNE=0 above
I retry set shuffle=False in get_train_dataflow ,then print the image shape and obeject number in the image and step time cost
I just try one epoch of 210 step ,
the epoch cost time are 4min 54s(enable autotune) and 1min 44s(disable autotune)
then I get the log below:

unset TF_CUDNN_USE_AUTOTUNE
```
image shape:[1201 800 3]
obeject number:[2]
[32m[0810 06:27:14 @base.py:254][0m hvd rank 0, (global_step 86)  time:0.2963137626647949.
image shape:[800 1199 3]
obeject number:[8]
[32m[0810 06:27:14 @base.py:254][0m hvd rank 0, (global_step 87)  time:0.33693432807922363.
image shape:[800 1261 3]
obeject number:[10]
[32m[0810 06:27:17 @base.py:254][0m hvd rank 0, (global_step 88)  time:2.603098154067993.
image shape:[800 808 3]
obeject number:[2]
[32m[0810 06:27:17 @base.py:254][0m hvd rank 0, (global_step 89)  time:0.5877795219421387.
image shape:[800 1278 3]obeject number:[4]

[32m[0810 06:27:20 @base.py:254][0m hvd rank 0, (global_step 90)  time:2.456442356109619.
image shape:[800 1199 3]
obeject number:[1]
[32m[0810 06:27:22 @base.py:254][0m hvd rank 0, (global_step 91)  time:2.151698350906372.
```
export TF_CUDNN_USE_AUTOTUNE=0

```
image shape:[1201 800 3]
obeject number:[2]
[32m[0810 06:16:50 @base.py:254][0m hvd rank 0, (global_step 86)  time:0.379549503326416.
image shape:[800 1199 3]obeject number:[8]

[32m[0810 06:16:51 @base.py:254][0m hvd rank 0, (global_step 87)  time:0.40617823600769043.
image shape:[800 1261 3]
obeject number:[10]
[32m[0810 06:16:51 @base.py:254][0m hvd rank 0, (global_step 88)  time:0.36014246940612793.
image shape:[800 808 3]obeject number:[2]

[32m[0810 06:16:52 @base.py:254][0m hvd rank 0, (global_step 89)  time:0.44893598556518555.
image shape:[800 1278 3]
obeject number:[4]
[32m[0810 06:16:52 @base.py:254][0m hvd rank 0, (global_step 90)  time:0.32912302017211914.
image shape:[800 1199 3]
obeject number:[1]
[32m[0810 06:16:52 @base.py:254][0m hvd rank 0, (global_step 91)  time:0.3986361026763916.
```
",get wrong result retry set print image shape number image step time cost try one epoch step epoch cost time min enable min disable get log unset image shape number rank time image shape number rank time image shape number rank time image shape number rank time image shape number rank time image shape number rank time export image shape number rank time image shape number rank time image shape number rank time image shape number rank time image shape number rank time image shape number rank time,issue,negative,negative,negative,negative,negative,negative
411974690,"monitors only take care of where to put logging data. It does not manage what to log and when they are produced. See http://tensorpack.readthedocs.io/tutorial/summary.html#tensorflow-summaries

For your use case it's better to read http://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training",take care put logging data manage log produced see use case better read,issue,positive,positive,positive,positive,positive,positive
411967733,"I want to show op output shape  so I 

```
image_shape2d = tf.shape(image,name='imageshape')[2:] 
```
and I add monitors in TrainConfig
```
monitors=[ScalarPrinter(enable_step=True,whitelist=['.*/Conv2D','imageshape'])],
```
But I get noting to print",want show output shape image add get print,issue,negative,neutral,neutral,neutral,neutral,neutral
411962389,"> if this means the most time cost are in backbone which has many Conv2D

I don't know if this is true. You'll need more detailed benchmarks.

>  How would I improve the train speed

You can let the dataloader produce images of more similar sizes together.

> When I export TF_CUDNN_USE_AUTOTUNE=0 as you said in the link ,the step cost is still very different and no improve of performence

The image sizes are still different so this option does nothing useful for this issue.",time cost backbone many know true need detailed would improve train speed let produce similar size together export said link step cost still different improve image size still different option nothing useful issue,issue,positive,positive,positive,positive,positive,positive
411928885,"I'm missing some contexts on what you're doing. I downloaded `flownet2-models.tar.gz` at https://github.com/lmb-freiburg/flownet2, and saw the following:
```
FlowNet2/    FlowNet2-C/   FlowNet2-CS/   FlowNet2-CSS/        FlowNet2-CSS-ft-sd/  FlowNet2-S/   FlowNet2-ss/  FlowNet2-sss/
FlowNet2-c/  FlowNet2-cs/  FlowNet2-css/  FlowNet2-css-ft-sd/  FlowNet2-s/          FlowNet2-SD/  FlowNet2-SS/
```
Did you port the weights of `FlowNet2-s` or `FlowNet2-S`? What's the difference?",missing saw following port difference,issue,negative,negative,neutral,neutral,negative,negative
411121813,"Keras is much more popular than tensorpack, so what they did is quite reasonable.",much popular quite reasonable,issue,negative,positive,positive,positive,positive,positive
410962169,"Thanks @ppwwyyxx . It's quite odd that they didn't simply consider integrating the tensorpack with tensorflow as they did with keras (although the keras integration is kind of broken IMHO). They didn't even mention the tensorpack in [their whitepaper](https://terrytangyuan.github.io/data/papers/tf-estimators-kdd-paper.pdf).

The biggest problem with estimators seems to be their lifecycle. It seems like they add few features, make it big on Google I/O and then forget about it for the rest of the year. E.g. I was recently thinking about implementing a run hook for printing the progress bar but it seems that there already is such thing in tensorpack.

Btw. I just found out that keras also use a similar concept of callbacks [https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/keras/callbacks/Callback](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/keras/callbacks/Callback).

I guess that I'll just give the tensorpack a try.",thanks quite odd simply consider although integration kind broken even mention biggest problem like add make big forget rest year recently thinking run hook printing progress bar already thing found also use similar concept guess give try,issue,negative,positive,neutral,neutral,positive,positive
410914316,"It was handled because `get_input_tensors` will be called on each device, 8 times in total.",handled device time total,issue,negative,neutral,neutral,neutral,neutral,neutral
410889578,"Regarding Callbacks vs SessionRunHooks:
> Why aren't Callback and tf.train.SessionRunHook compatible?

They are compatible: you can convert between them with http://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.HookToCallback .

Tensorpack callbacks exist mainly because it is written long before `SessionRunHook` ever exists.
It has one small extra feature about scheduling -- it has the concept of epoch which allows you to do something once a while. But you can easily implement this with SessionRunHook as well. 

Regarding trainer vs tf.estimator

I personally think `tf.Estimator` is not well designed and therefore not flexible enough. But I also think it does have something that tensorpack trainers can learn from.

Also, `tf.Estimator` does not support efficient data-parallel training, until very recently (still in contrib). However tensorpack trainer supports it from the very beginning.",regarding compatible compatible convert exist mainly written long ever one small extra feature concept epoch something easily implement well regarding trainer personally think well designed therefore flexible enough also think something learn also support efficient training recently still however trainer beginning,issue,positive,positive,neutral,neutral,positive,positive
410497803,"What's inside `build_graph` is your own TensorFlow code and has nothing to do with tensorpack. As a result we don't provide support on how to implement a TensorFlow model.

Despite of that, you can refer to tensorpack examples at http://dorefa.net for a correct implementation of TTQ.",inside code nothing result provide support implement model despite refer correct implementation,issue,negative,neutral,neutral,neutral,neutral,neutral
410456157,"I know, this is only the inference part (like the VGG, AlexNet examples). The links to the ported Caffe-Model probably need to change towards the model zoo.

I consider to re-implement the FlowNet2 or PWC. This is a good start to do a verifiable implementation for TensorFlow (part of FlowNet2 is FlowNet2-S).",know inference part like link ported probably need change towards model zoo consider good start verifiable implementation part,issue,positive,positive,positive,positive,positive,positive
410425591,"@ppwwyyxx 
but why did the resnet-dorefa have a `tf.multiply(49)` operation before the last full connected layer, in your implementation?",operation last full connected layer implementation,issue,negative,positive,positive,positive,positive,positive
410308683,"Oh, I see
Thanks for your patience.",oh see thanks patience,issue,negative,positive,positive,positive,positive,positive
410305897,"Each step is a sgd step on a batch of training data.
The relationship between this and frames is answered in the readme already.",step step batch training data relationship already,issue,negative,neutral,neutral,neutral,neutral,neutral
410305669,We do not help people tune the models. It's out of the scope of tensorpack.,help people tune scope,issue,negative,neutral,neutral,neutral,neutral,neutral
410238192,"@ppwwyyxx 
I tried to modify the resnet-dorefa.py to train the resnet18-dorefanet, and before train the quantization model, I just want to pretrain a full-precision model. So firstly I run `python resnet-dorefa.py --dorefa 32,32,32 ...`, but the loss just do not drop after several epoches.  Any advice is appreciated. The modified scripts are: 
resnet-dorefa.py: 
```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# File: resnet-dorefa.py

import cv2
import tensorflow as tf
import argparse
import numpy as np
import os

from tensorpack import *
from tensorpack.dataflow import dataset
from tensorpack.tfutils.varreplace import remap_variables
from tensorpack.tfutils.summary import add_param_summary
from imagenet_utils import get_imagenet_dataflow, fbresnet_augmentor, ImageNetModel

from imagenet_utils import ImageNetModel, eval_on_ILSVRC12, fbresnet_augmentor
from dorefa import get_dorefa
from tensorpack.utils.gpu import get_num_gpu
""""""
This script loads the pre-trained ResNet-18 model with (W,A,G) = (1,4,32)
It has 59.2% top-1 and 81.5% top-5 validation error on ILSVRC12 validation set.

To run on images:
    ./resnet-dorefa.py --load ResNet-18-14f.npz --run a.jpg b.jpg

To eval on ILSVRC validation set:
    ./resnet-dorefa.py --load ResNet-18-14f.npz --eval --data /path/to/ILSVRC
""""""

BITW = 1
BITA = 4
BITG = 32
TOTAL_BATCH_SIZE = 256

class Model(ImageNetModel):
    weight_decay = 0.0001
    weight_decay_pattern = 'fc.*/W'

    def get_logits(self, image):

        if BITW == 't':
            fw, fa, fg = get_dorefa(32, 32, 32)
            fw = ternarize
        else:
            fw, fa, fg = get_dorefa(BITW, BITA, BITG)

        # monkey-patch tf.get_variable to apply fw
        def new_get_variable(v):
            name = v.op.name
            # don't binarize first and last layer
            if not name.endswith('W'): #or 'conv0' in name or 'fct' in name:
                return v
            else:
                logger.info(""Quantizing weight {}"".format(v.op.name))
                return fw(v)

        def nonlin(x):
            if BITA == 32:
                return tf.nn.relu(x)    # still use relu for 32bit cases
            return tf.clip_by_value(x, 0.0, 1.0)

        def activate(x):
            return fa(nonlin(x))
        def resblock(x, channel, stride):
            def get_stem_full(x):
                return (LinearWrap(x)
                        .Conv2D('c3x3a', channel, 3)
                        .BatchNorm('stembn')
                        .apply(activate)
                        .Conv2D('c3x3b', channel, 3)())
            channel_mismatch = channel != x.get_shape().as_list()[3]
            if stride != 1 or channel_mismatch or 'pool1' in x.name:
                # handling pool1 is to work around an architecture bug in our model
                if stride != 1 or 'pool1' in x.name:
                    x = AvgPooling('pool', x, stride, stride)
                x = BatchNorm('bn', x)
                x = activate(x)
                shortcut = Conv2D('shortcut', x, channel, 1)
                stem = get_stem_full(x)
            else:
                shortcut = x
                x = BatchNorm('bn', x)
                x = activate(x)
                stem = get_stem_full(x)
            return shortcut + stem

        def group(x, name, channel, nr_block, stride):
            with tf.variable_scope(name + 'blk1'):
                x = resblock(x, channel, stride)
            for i in range(2, nr_block + 1):
                with tf.variable_scope(name + 'blk{}'.format(i)):
                    x = resblock(x, channel, 1)
            return x
        with remap_variables(new_get_variable), \
                argscope([Conv2D, BatchNorm, MaxPooling], data_format='channels_first'), \
                argscope(BatchNorm, momentum=0.9, epsilon=1e-4), \
                argscope(Conv2D, use_bias=False):
            logits = (LinearWrap(image)
                      # use explicit padding here, because our private training framework has
                      # different padding mechanisms from TensorFlow
                      .tf.pad([[0, 0], [3, 2], [3, 2], [0, 0]])
                      .Conv2D('conv1', 64, 7, stride=2, padding='VALID', use_bias=True)
                      .tf.pad([[0, 0], [1, 1], [1, 1], [0, 0]], 'SYMMETRIC')
                      .MaxPooling('pool1', 3, 2, padding='VALID')
                      .apply(group, 'conv2', 64, 2, 1)
                      .apply(group, 'conv3', 128, 2, 2)
                      .apply(group, 'conv4', 256, 2, 2)
                      .apply(group, 'conv5', 512, 2, 2)
                      .BatchNorm('lastbn')
                      .apply(nonlin)
                      .GlobalAvgPooling('gap')
                      .tf.multiply(49)  # this is due to a bug in our model design
                      .FullyConnected('fct', 1000)())
        add_param_summary(('.*/W', ['histogram', 'rms']))
        tf.nn.softmax(logits, name='output')  # for prediction
        return logits
    #def optimizer(self):
        #lr = tf.get_variable('learning_rate', initializer=0.001, trainable=False)
        #return tf.train.AdamOptimizer(lr, epsilon=1e-5)        
    def optimizer(self):
        lr = tf.get_variable('learning_rate', initializer=0.01, trainable=False)
        return tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=False)

def get_config():
    data_train = get_data('train')
    data_test = get_data('val')

    return TrainConfig(
        dataflow=data_train,
        callbacks=[
            PeriodicTrigger(ModelSaver(), every_k_epochs=15),
            ScheduledHyperParamSetter(
                'learning_rate', [(30, 0.001), (60, 0.0001)]),
            InferenceRunner(data_test,
                            [ClassificationError('wrong-top1', 'val-error-top1'),
                             ClassificationError('wrong-top5', 'val-error-top5')])
        ],
        model=Model(),
        steps_per_epoch=1280000 // TOTAL_BATCH_SIZE,
        max_epoch=91,
    )


def get_data(dataset_name):
    isTrain = dataset_name == 'train'
    augmentors = fbresnet_augmentor(isTrain)
    return get_imagenet_dataflow(
        args.data, dataset_name, BATCH_SIZE, augmentors)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--gpu', help='the physical ids of GPUs to use')
    parser.add_argument('--load', help='load a checkpoint, or a npz (given as the pretrained model)')
    parser.add_argument('--data', help='ILSVRC dataset dir')
    parser.add_argument('--dorefa', required=True,
                        help='number of bits for W,A,G, separated by comma. W=""t"" means TTQ')
    parser.add_argument('--run', help='run on a list of images with the pretrained model', nargs='*')
    args = parser.parse_args()

    dorefa = args.dorefa.split(',')
    if dorefa[0] == 't':
        assert dorefa[1] == '32' and dorefa[2] == '32'
        BITW, BITA, BITG = 't', 32, 32
    else:
        BITW, BITA, BITG = map(int, dorefa)

    if args.gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu

    if args.run:
        assert args.load.endswith('.npz')
        run_image(Model(), DictRestore(dict(np.load(args.load))), args.run)
        sys.exit()

    nr_tower = max(get_num_gpu(), 1)
    BATCH_SIZE = TOTAL_BATCH_SIZE // nr_tower
    logger.set_logger_dir(os.path.join(
        'train_log', 'resnet18-dorefa-{}'.format(args.dorefa)))
    logger.info(""Batch per tower: {}"".format(BATCH_SIZE))

    config = get_config()
    if args.load:
        config.session_init = SaverRestore(args.load)
    launch_train_with_config(config, SyncMultiGPUTrainerReplicated(nr_tower))
```
imagenet_utils.py:

```
import cv2
import numpy as np
import multiprocessing
import tensorflow as tf
from abc import abstractmethod

from tensorpack import imgaug, dataset, ModelDesc
from tensorpack.dataflow import (
    AugmentImageComponent, PrefetchDataZMQ,
    BatchData, MultiThreadMapData)
from tensorpack.predict import PredictConfig, SimpleDatasetPredictor
from tensorpack.utils.stats import RatioCounter
from tensorpack.models import regularize_cost
from tensorpack.tfutils.summary import add_moving_summary
from tensorpack.utils import logger


class GoogleNetResize(imgaug.ImageAugmentor):
    """"""
    crop 8%~100% of the original image
    See `Going Deeper with Convolutions` by Google.
    """"""
    def __init__(self, crop_area_fraction=0.08,
                 aspect_ratio_low=0.75, aspect_ratio_high=1.333,
                 target_shape=224):
        self._init(locals())

    def _augment(self, img, _):
        h, w = img.shape[:2]
        area = h * w
        for _ in range(10):
            targetArea = self.rng.uniform(self.crop_area_fraction, 1.0) * area
            aspectR = self.rng.uniform(self.aspect_ratio_low, self.aspect_ratio_high)
            ww = int(np.sqrt(targetArea * aspectR) + 0.5)
            hh = int(np.sqrt(targetArea / aspectR) + 0.5)
            if self.rng.uniform() < 0.5:
                ww, hh = hh, ww
            if hh <= h and ww <= w:
                x1 = 0 if w == ww else self.rng.randint(0, w - ww)
                y1 = 0 if h == hh else self.rng.randint(0, h - hh)
                out = img[y1:y1 + hh, x1:x1 + ww]
                out = cv2.resize(out, (self.target_shape, self.target_shape), interpolation=cv2.INTER_CUBIC)
                return out
        #out = imgaug.ResizeShortestEdge(self.target_shape, interp=cv2.INTER_CUBIC).augment(img)
        out = imgaug.Resize(self.target_shape,interp=cv2.INTER_CUBIC).augment(img)

        out = imgaug.CenterCrop(self.target_shape).augment(out)
        return out


def fbresnet_augmentor(isTrain):
    """"""
    Augmentor used in fb.resnet.torch, for BGR images in range [0,255].
    """"""
    if isTrain:
        augmentors = [
            #GoogleNetResize(),
            #imgaug.ResizeShortestEdge(256, cv2.INTER_CUBIC),
            imgaug.Resize((256,256)),
            imgaug.RandomCrop((224, 224)),
            # It's OK to remove these augs if your CPU is not fast enough.
            # Removing brightness/contrast/saturation does not have a significant effect on accuracy.
            # Removing lighting leads to a tiny drop in accuracy.

            imgaug.Flip(horiz=True),
        ]
    else:
        augmentors = [
            imgaug.Resize((256,256)),
            imgaug.CenterCrop((224, 224)),
        ]
    return augmentors


def get_imagenet_dataflow(
        datadir, name, batch_size,
        augmentors, parallel=None):
    """"""
    See explanations in the tutorial:
    http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html
    """"""
    assert name in ['train', 'val', 'test']
    assert datadir is not None
    assert isinstance(augmentors, list)
    isTrain = name == 'train'
    if parallel is None:
        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading
    if isTrain:
        ds = dataset.ILSVRC12(datadir, name, shuffle=True)
        ds = AugmentImageComponent(ds, augmentors, copy=False)
        if parallel < 16:
            logger.warn(""DataFlow may become the bottleneck when too few processes are used."")
        ds = PrefetchDataZMQ(ds, parallel)
        ds = BatchData(ds, batch_size, remainder=False)
    else:
        ds = dataset.ILSVRC12Files(datadir, name, shuffle=False)
        aug = imgaug.AugmentorList(augmentors)

        def mapf(dp):
            fname, cls = dp
            im = cv2.imread(fname, cv2.IMREAD_COLOR)
            im = aug.augment(im)
            return im, cls
        ds = MultiThreadMapData(ds, parallel, mapf, buffer_size=2000, strict=True)
        ds = BatchData(ds, batch_size, remainder=True)
        ds = PrefetchDataZMQ(ds, 1)
    return ds


def eval_on_ILSVRC12(model, sessinit, dataflow):
    pred_config = PredictConfig(
        model=model,
        session_init=sessinit,
        input_names=['input', 'label'],
        output_names=['wrong-top1', 'wrong-top5']
    )
    pred = SimpleDatasetPredictor(pred_config, dataflow)
    acc1, acc5 = RatioCounter(), RatioCounter()
    for top1, top5 in pred.get_result():
        batch_size = top1.shape[0]
        acc1.feed(top1.sum(), batch_size)
        acc5.feed(top5.sum(), batch_size)
    print(""Top1 Error: {}"".format(acc1.ratio))
    print(""Top5 Error: {}"".format(acc5.ratio))


class ImageNetModel(ModelDesc):
    image_shape = 224

    """"""
    uint8 instead of float32 is used as input type to reduce copy overhead.
    It might hurt the performance a liiiitle bit.
    The pretrained models were trained with float32.
    """"""
    image_dtype = tf.uint8

    """"""
    Either 'NCHW' or 'NHWC'
    """"""
    data_format = 'NCHW'

    """"""
    Whether the image is BGR or RGB. If using DataFlow, then it should be BGR.
    """"""
    image_bgr = True

    weight_decay = 1e-4

    """"""
    To apply on normalization parameters, use '.*/W|.*/gamma|.*/beta'
    """"""
    weight_decay_pattern = '.*/W'

    """"""
    Scale the loss, for whatever reasons (e.g., gradient averaging, fp16 training, etc)
    """"""
    loss_scale = 1.

    def inputs(self):
        return [tf.placeholder(self.image_dtype, [None, self.image_shape, self.image_shape, 3], 'input'),
                tf.placeholder(tf.int32, [None], 'label')]

    def build_graph(self, image, label):
        image = ImageNetModel.image_preprocess(image, bgr=self.image_bgr)
        assert self.data_format in ['NCHW', 'NHWC']
        if self.data_format == 'NCHW':
            image = tf.transpose(image, [0, 3, 1, 2])

        logits = self.get_logits(image)
        loss = ImageNetModel.compute_loss_and_error(logits, label)

        if self.weight_decay > 0:
            wd_loss = regularize_cost(self.weight_decay_pattern,
                                      tf.contrib.layers.l2_regularizer(self.weight_decay),
                                      name='l2_regularize_loss')
            add_moving_summary(loss, wd_loss)
            total_cost = tf.add_n([loss, wd_loss], name='cost')
        else:
            total_cost = tf.identity(loss, name='cost')
            add_moving_summary(total_cost)

        if self.loss_scale != 1.:
            logger.info(""Scaling the total loss by {} ..."".format(self.loss_scale))
            return total_cost * self.loss_scale
        else:
            return total_cost

    @abstractmethod
    def get_logits(self, image):
        """"""
        Args:
            image: 4D tensor of ``self.input_shape`` in ``self.data_format``

        Returns:
            Nx#class logits
        """"""

    def optimizer(self):
        lr = tf.get_variable('learning_rate', initializer=0.1, trainable=False)
        tf.summary.scalar('learning_rate-summary', lr)
        return tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)

    @staticmethod
    def image_preprocess(image, bgr=True):
        with tf.name_scope('image_preprocess'):
            if image.dtype.base_dtype != tf.float32:
                image = tf.cast(image, tf.float32)
            image = image * (1.0 / 255)

            mean = [0.485, 0.456, 0.406]    # rgb
            std = [0.229, 0.224, 0.225]
            
            if bgr:
                mean = mean[::-1]
                std = std[::-1]
            image_mean = tf.constant(mean, dtype=tf.float32)
            image_std = tf.constant(std, dtype=tf.float32)
            image = (image - image_mean) / image_std
            return image

    @staticmethod
    def compute_loss_and_error(logits, label):
        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
        loss = tf.reduce_mean(loss, name='xentropy-loss')

        def prediction_incorrect(logits, label, topk=1, name='incorrect_vector'):
            with tf.name_scope('prediction_incorrect'):
                x = tf.logical_not(tf.nn.in_top_k(logits, label, topk))
            return tf.cast(x, tf.float32, name=name)

        wrong = prediction_incorrect(logits, label, 1, name='wrong-top1')
        add_moving_summary(tf.reduce_mean(wrong, name='train-error-top1'))

        wrong = prediction_incorrect(logits, label, 5, name='wrong-top5')
        add_moving_summary(tf.reduce_mean(wrong, name='train-error-top5'))
        return loss


if __name__ == '__main__':
    import argparse
    from tensorpack.dataflow import TestDataSpeed
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', required=True)
    parser.add_argument('--batch', type=int, default=32)
    parser.add_argument('--aug', choices=['train', 'val'], default='val')
    args = parser.parse_args()

    if args.aug == 'val':
        augs = fbresnet_augmentor(False)
    elif args.aug == 'train':
        augs = fbresnet_augmentor(True)
    df = get_imagenet_dataflow(
        args.data, 'train', args.batch, augs)
    # For val augmentor, Should get >100 it/s (i.e. 3k im/s) here on a decent E5 server.
    TestDataSpeed(df).start()


```",tried modify train train quantization model want pretrain model firstly run python loss drop several advice python file import import import import import o import import import import import import import import script model validation error validation set run load run validation set load data class model self image fa else fa apply name first last layer name name return else weight return return still use bit return activate return fa channel stride return channel activate channel channel stride handling pool work around architecture bug model stride stride stride activate channel stem else activate stem return stem group name channel stride name channel stride range name channel return image use explicit padding private training framework different padding group group group group due bug model design prediction return self return self return return return parser physical use load given model data comma run list model assert else map assert model batch per tower import import import import import import import import import import import import logger class crop original image see going self self area range area else else return return used range remove fast enough removing significant effect accuracy removing lighting tiny drop accuracy else return name see tutorial assert name assert none assert list name parallel none parallel min assuming name parallel may become bottleneck used parallel else name return parallel return model top top print top error print top error class instead float used input type reduce copy overhead might hurt performance bit trained either whether image true apply normalization use scale loss whatever gradient training self return none none self image label image image assert image image image loss label loss loss else loss scaling total loss return else return self image image tensor class self return image image image image image mean mean mean mean image image return image label loss loss loss label label return wrong label wrong wrong label wrong return loss import import parser data batch false true get decent server,issue,negative,positive,neutral,neutral,positive,positive
410158206,"Yes I kind of misunderstood the quantization mechanism until I walked through the discussions in #27. I now know the weights should still be float32 even after quantization whatsoever while most of the multiplications and additions in Cov and FC layers are bitwisely operated according to the equation (3) of your paper. 

Another thought: I think it is no need of weights quantization if I use the provided dorefa-alexnet.py with the pre-trained npz model to test as the quantization layers are already provided between Conv layers in dorefa-alexnet.py. And the accuracy should be almost the same but the speed is slower because the Conv2d in it are float32-based. I do not know if my thought is correct as I am to test the float32 version net anyway.",yes kind misunderstood quantization mechanism know still float even quantization whatsoever according equation paper another thought think need quantization use provided model test quantization already provided accuracy almost speed know thought correct test float version net anyway,issue,positive,positive,positive,positive,positive,positive
410150087,"1. Yes
2. 
> Although the quantized activate function are applied, the conv1 still perform float32-based convolution calculations in your open implementation.

Yes

> And the npz model is also trained under this environment which explains the float32 weights it contains.

Using float32 weights for training is by design of the listed quantization methods. It is not about this specific implementation.",yes although activate function applied still perform convolution open implementation yes model also trained environment float float training design listed quantization specific implementation,issue,positive,neutral,neutral,neutral,neutral,neutral
410143993,"Thank you for your reply. However, two things are still not very clear for me:

1. Suppose I have realized all the necessary quantized operations in the forwarded propagation, does it mean the downloaded pre-trained 1-2-6 or 8-8-8 model is testable after I manually quantized every weight elements using fw() as you have suggested in #573?

2. By claiming ""In this implementation, quantized operations are all performed through tf.float32"", my understanding is that all the data format is still float32 in the python scripts you provide. For example: 
""
......
.Conv2D('conv0', 96, 12, strides=4, padding='VALID', use_bias=True)
.apply(activate)
.Conv2D('conv1', 256, 5, padding='SAME', split=2)
......
""
Although the quantized activate function are applied, the conv1 still perform float32-based convolution calculations in your open implementation. And the npz model is also trained under this environment which explains the float32 weights it contains. 
I do not know if it is correct.",thank reply however two still clear suppose necessary propagation mean model testable manually every weight implementation understanding data format still float python provide example activate although activate function applied still perform convolution open implementation model also trained environment float know correct,issue,positive,negative,neutral,neutral,negative,negative
410141880,It's my fault rather than tensorpack's. Close this issue. ,fault rather close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
410101726,"It's answered in the issue template:
> ""Could you improve/implement an example/paper ?"" 
  -- the answer is: we have no plans to do so and we don't take feature requests for
  examples. If you don't know how to do it, you may ask a usage question.",issue template could answer take feature know may ask usage question,issue,negative,neutral,neutral,neutral,neutral,neutral
409821621,"The input occupy the same amount of memory, but the computation is different and therefore will need different amount of memory.

Because it's usually faster this way.",input occupy amount memory computation different therefore need different amount memory usually faster way,issue,negative,negative,neutral,neutral,negative,negative
409819676,"@ppwwyyxx 
Thanks your answer. Take following cases for example.
case 1. use  ```images=tf.transpose(images, [0, 3, 1, 2])``` 
case 2. don't use the tf.transpose code snippet.
for both cases, tf.transpose only changes the order of axis, images will occupy same size memory. If you are right, case 1 should fail too. In fact the train code works right and gives me right result for case 1. 
Another question, In [cifar-convnet](https://github.com/tensorpack/tensorpack/blob/32e41776094d075ffd4089e5f450e52634b36d46/examples/basics/cifar-convnet.py#L41),  we have following code snippet
```
        if tf.test.is_gpu_available():
            image = tf.transpose(image, [0, 3, 1, 2])
            data_format = 'channels_first'
        else:
            data_format = 'channels_last'
```
why we have to make channels first when using GPU in training? some reasons? 


Thanks again.",thanks answer take following example case use case use code snippet order axis occupy size memory right case fail fact train code work right right result case another question following code snippet image image else make first training thanks,issue,negative,positive,positive,positive,positive,positive
409802152,How is this issue related to tensorpack? It sounds like you just don't have enough memory.,issue related like enough memory,issue,negative,neutral,neutral,neutral,neutral,neutral
409779341,"@ppwwyyxx 
Sure, that's a good idea. Thanks!",sure good idea thanks,issue,positive,positive,positive,positive,positive,positive
409616622,"Please post issues following the issue template.

1. Depend on what trainer you're using. Some trainer make variables local by design. Is there any reason you want them global?

2. You're recommended to use `HorovodTrainer` for distributed training. TensorFlow is not actively supporting its native distributed training features.",please post following issue template depend trainer trainer make local design reason want global use distributed training actively supporting native distributed training,issue,positive,positive,neutral,neutral,positive,positive
409460670,"Sounds like you want to do something on numpy arrays. If that's the case this question is unrelated to tensorpack. 
You can implement the function with whatever tools you like, with or without tensorflow. You can use the corresponding function in tensorflow by creating a graph, a session, and run the op.",like want something case question unrelated implement function whatever like without use corresponding function graph session run,issue,positive,neutral,neutral,neutral,neutral,neutral
409459348,"Sounds like it was solved.
In general, modifying the config file is more robust than setting everything from the command line. You can also add your own command line options for the config you want to change. For example `--load` can be used for `BACKBONE.WEIGHTS`.",like general file robust setting everything command line also add command line want change example load used,issue,positive,positive,neutral,neutral,positive,positive
409440499,"This problem is just caused by  Windows10 dos window cannot realize 'xxx' string, need convert to ""xxx"".
Solved.",problem do window realize string need convert,issue,negative,neutral,neutral,neutral,neutral,neutral
408329994,"Can I join two different optimizer to one in ModelDesc?
opt1 = tf.train.AdamOptimizer(1e-4)
opt2 = tf.train.GradientDescentOptimizer(1e-3)",join two different one opt opt,issue,negative,neutral,neutral,neutral,neutral,neutral
408327122,"varlist2 is wout and bout in fact ,so I use apply_grad_processors and ScaleGradient as below:

```
 def optimizer(self):
        opt = tf.train.AdamOptimizer(1e-4)
        return optimizer.apply_grad_processors(
            opt, [gradproc.ScaleGradient(
                [('.*/wout', 10), ('.*/bout', 10)],log=True)])
```

I found this print in log
```
[0727 06:38:03 @gradproc.py:251] Apply lr multiplier 10 for var_name/wout
[0727 06:38:03 @gradproc.py:251] Apply lr multiplier 10 for var_name/bout
```
I think it works~
thx",bout fact use self opt return opt found print log apply multiplier apply multiplier think,issue,negative,neutral,neutral,neutral,neutral,neutral
408321387,"Use http://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.optimizer.apply_grad_processors with http://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.gradproc.ScaleGradient.

More generally, you can implement a `tf.train.Optimizer` yourself by inheriting an existing optimizer such as Adam and making some modifications.",use generally implement making,issue,negative,positive,neutral,neutral,positive,positive
407521079,@ppwwyyxx Have you ever considered to write a tool to transfer other pre-trained models such as [here](https://github.com/facebookresearch/Detectron/blob/master/MODEL_ZOO.md) to npz model that tensorpack could use?,ever considered write tool transfer model could use,issue,negative,neutral,neutral,neutral,neutral,neutral
407282826,The provided models are trained on COCO dataset and are just about as good as the official detectron models on this dataset. ,provided trained coco good official,issue,negative,positive,positive,positive,positive,positive
407273916,@PacteraOliver The model provided is very awful， purchase Titan or Tesla to train yourself,model provided purchase train,issue,negative,neutral,neutral,neutral,neutral,neutral
407007444,"@ppwwyyxx ah right that's what's missing! 
Did a `.astype(np.uint8)` and now both envs matches, thx a lot!",ah right missing lot,issue,negative,positive,neutral,neutral,positive,positive
406811785,"If the original examples themselves are working, it's unlikely to be a tensorpack issue.
We don't help people debug their code and model. If you think you found a tensorpack bug, please post more details following the issue template.

My suggestion is to add some `tf.check_numerics` ops around some suspicious ops. Use smaller reward if they are dense. And use smaller learning rate.",original working unlikely issue help people code model think found bug please post following issue template suggestion add around suspicious use smaller reward dense use smaller learning rate,issue,positive,negative,neutral,neutral,negative,negative
406755259,"@ppwwyyxx 

I fixed this problem by run as root.",fixed problem run root,issue,negative,positive,neutral,neutral,positive,positive
406743751,"@ppwwyyxx 
Hi, thank you for your update.

I could successfully run the [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn) before or other models which need cuda/cudnn. 

I will try with docker to build the environment. Hope it could fix the environment problem.",hi thank update could successfully run need try docker build environment hope could fix environment problem,issue,positive,positive,positive,positive,positive,positive
406740988,The log is quite clear that you have environment problems with your cuda/cudnn.,log quite clear environment,issue,negative,positive,positive,positive,positive,positive
406535354,"@ppwwyyxx Thx for the reply.

I used the above `variable_summaries` function only for TensorBoard visualization purpose, sorry for the confusion.
So within `alexnet-dorefa.py`'s `get_logits` I added these (since TensorPack callbacks/monitors are for training but not inference...) to visualize the input `image` tensor's values:
```Python
variable_summaries(image, '-image')
self.merged = tf.summary.merge_all(tf.GraphKeys.SUMMARIES)
```

Some findings:
If I comment out the ""minus mean divide std"" part within `image_preprocess` like this:
```Python
    @staticmethod
    def image_preprocess(image, bgr=True):
        with tf.name_scope('image_preprocess'):
            if image.dtype.base_dtype != tf.float32:
                image = tf.cast(image, tf.float32)
            image = image * (1.0 / 255)

            mean = [0.485, 0.456, 0.406]    # rgb
            std = [0.229, 0.224, 0.225]
            if bgr:
                mean = mean[::-1]
                std = std[::-1]
            image_mean = tf.constant(mean, dtype=tf.float32)
            image_std = tf.constant(std, dtype=tf.float32)
            #image = (image - image_mean) / image_std
            #image = (image - image_mean)
            return image
```
With the same picture as input, I get the mean and std values of the picture after resize+centerCrop from both TensorPack and pure Python env almost matched to 2 or 3 decimal place;
However, in TensorPack the min and max value of the `image` tensor was shown as 0. and 1. on TensorBoard;
But in `run_image` I also printed the image's stats after `augement`, before launching `OfflinePredictor`,
```Python
def run_image(model, sess_init, inputs):
    pred_config = PredictConfigMod(
        model=model,
        session_init=sess_init,
        input_names=['input'],
        output_names=['output']
    )
    predictor = OfflinePredictorMod(pred_config)
    meta = dataset.ILSVRCMeta()
    words = meta.get_synset_words_1000()
    print('type(predictor)={}'.format(type(predictor)))

    writer = tf.summary.FileWriter(logger.get_logger_dir())
    
    transformers = imgaug.AugmentorList(fbresnet_augmentor(isTrain=False))
    for f in inputs:
        assert os.path.isfile(f), f
        img = cv2.imread(f).astype('float32')
        assert img is not None

        print('img.shape ={}'.format(img.shape))
        print('img mean={}, var={}, stddev={}'.format(np.mean(img), np.var(img), np.std(img)))
        img = transformers.augment(img)[np.newaxis, :, :, :]
        print('img.shape ={}'.format(img.shape))
        print('img mean={}, var={}, stddev={}'.format(np.mean(img), np.var(img), np.std(img)))
        print('img min={} max={}'.format(np.min(img), np.max(img)))
        predicted = predictor(img)
        outputs = predicted[0]
        prob = outputs[0]
        ret = prob.argsort()[-10:][::-1]

        names = [words[i] for i in ret]
        print(f + "":"")
        print(list(zip(names, prob[ret])))
        print(ret)
        
        writer.add_summary(predicted[1])
```
and got the image min and max as: `img min=-10.28548526763916 max=275.43719482421875`,
which is the same value as I got in pure Python env too, 
but then things doesn't add up coz if all that's done within `image_preprocess` now is 
`image = image * (1.0 / 255)`, 
we really shouldn't have got min=0.0, max=1.0 on the `image` tensor, no !?

Sorry for the long story and thx for any opinions in advance.",reply used function visualization purpose sorry confusion within added since training inference visualize input image tensor python image comment minus mean divide part within like python image image image image image mean mean mean mean image image image image return image picture input get mean picture pure python almost decimal place however min value image tensor shown also printed image python model predictor meta print predictor type predictor writer assert assert none print print print print print predictor prob ret ret print print list zip prob ret print ret got image min value got pure python add coz done within image image really got image tensor sorry long story advance,issue,positive,negative,negative,negative,negative,negative
406494636,"As said in the issue template, we don't take feature requests on examples. ",said issue template take feature,issue,negative,neutral,neutral,neutral,neutral,neutral
406396808,"`ScalarStats` uses prefix, not inference runner. You can probably do this for ScalarStats automatically. But it will be hard to do this automatically for other Inferencer.",prefix inference runner probably automatically hard automatically,issue,negative,negative,negative,negative,negative,negative
406392771,"The inference runner uses the prefix validation, no? Hence, the namings are clear by ScalarStats
https://github.com/tensorpack/tensorpack/blob/1554550de5322362af52b198587ee832b17f223b/examples/basics/mnist-tflayers.py#L125

These would be good candidates for the custom scalars. This would be a neat way to combine validation and training metrics in one chart without any ugly hacks.",inference runner prefix validation hence clear would good custom would neat way combine validation training metric one chart without ugly,issue,positive,positive,neutral,neutral,positive,positive
406391272,"It looks like what's needed from a user is just to:
1. create a layout summary
2. call `trainer.monitors.put_summary()` somewhere (e.g. in before_train, or manually outside the training loop.

There seems to be many many options in creating a layout:  https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/custom_scalar/custom_scalar_demo.py#L60-L97. So I'm not sure if tensorpack should do it and make those decisions in step 1. But I'm not familiar with this feature, maybe those options are not important at all.",like user create layout summary call somewhere manually outside training loop many many layout sure make step familiar feature maybe important,issue,positive,positive,positive,positive,positive,positive
406181979,C for Conv. I'd recommend you to read the papers listed in the README first. They have all the answers.,recommend read listed first,issue,negative,positive,positive,positive,positive,positive
406181191,"Thank you for your quick reply :)

I am relatively new to deep learning, so can I ask what ""C"" stands for in R50-C4 ? I could guess R stands for ResNet only. Thanks for your kindness.


",thank quick reply relatively new deep learning ask could guess thanks kindness,issue,positive,positive,positive,positive,positive,positive
406180010,"You can use `.ckpt` as `BACKBONE.WEIGHTS`.
You'll need to build a graph that can correctly use the weights you load.
",use need build graph correctly use load,issue,negative,neutral,neutral,neutral,neutral,neutral
406142988,"It corresponds to the TF_PAD_MODE option. What it does can be found in the fastrcnn basemodel.
It does not matter much.",option found matter much,issue,negative,positive,positive,positive,positive,positive
406139176,You can modify the resnet training code following what's in the fasterrcnn basemodel.,modify training code following,issue,negative,neutral,neutral,neutral,neutral,neutral
406008604,"If you're asking about whether it affects accuracy, I don't know but I think not.

To be consistent with the paper.",whether accuracy know think consistent paper,issue,negative,positive,positive,positive,positive,positive
405914990,"I really cant feature out why first crop `2*resolusion` then avg pool, is there reason specifically for `tf.image.crop_and_resize` ?",really cant feature first crop pool reason specifically,issue,negative,positive,positive,positive,positive,positive
405869334,"Is it safe to change 
```
        ret = crop_and_resize(
            featuremap, boxes,
            tf.zeros([tf.shape(boxes)[0]], dtype=tf.int32),
            resolution * 2)

        ret = tf.nn.avg_pool(ret, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME', data_format='NHWC')

```
to
```
        ret = crop_and_resize(
            featuremap, boxes,
            tf.zeros([tf.shape(boxes)[0]], dtype=tf.int32),
            resolution)
```",safe change ret resolution ret ret ret resolution,issue,negative,positive,positive,positive,positive,positive
405784937,"@PatWie Sorry, I forget to fill the issue template. I will follow your guide if I meet issue again. 
After root my machine, mnist-tfslim.py runs successfully. it's my system's issue indeed. 

Thanks very much. ",sorry forget fill issue template follow guide meet issue root machine successfully system issue indeed thanks much,issue,positive,positive,positive,positive,positive,positive
405544927,"Hi all, i have run the example code(from flyers) above, the result is not normal just because the QueueInput/queue_size has data while the QueueInput_1/queue_size is almost empty, the log is as below:
[0717 18:49:04 @base.py:272] Epoch 90 (global_step 18000) finished, time:1.47 seconds.
[0717 18:49:05 @monitor.py:428] QueueInput/queue_size: 34.267
[0717 18:49:05 @monitor.py:428] QueueInput_1/queue_size: 1.5595e-38
[0717 18:49:05 @monitor.py:428] net1_loss: 10.731
[0717 18:49:05 @monitor.py:428] net2_loss: 0.004886
[0717 18:49:05 @monitor.py:428] pred1: 0.94957
[0717 18:49:05 @monitor.py:428] pred2: 0.015798
[0717 18:49:05 @base.py:262] Start Epoch 91 ...
100%|##########|100/100[00:01<00:00,63.74it/s]
[0717 18:49:06 @base.py:272] Epoch 91 (global_step 18200) finished, time:1.57 seconds.
[0717 18:49:06 @monitor.py:428] QueueInput/queue_size: 35.001
[0717 18:49:06 @monitor.py:428] QueueInput_1/queue_size: 1.5303e-38
[0717 18:49:06 @monitor.py:428] net1_loss: 10.888
[0717 18:49:06 @monitor.py:428] net2_loss: 0.0055581
[0717 18:49:06 @monitor.py:428] pred1: 0.95158
[0717 18:49:06 @monitor.py:428] pred2: 0.0138
[0717 18:49:06 @base.py:262] Start Epoch 92 ...
100%|##########|100/100[00:01<00:00,68.13it/s]
[0717 18:49:08 @base.py:272] Epoch 92 (global_step 18400) finished, time:1.47 seconds.
[0717 18:49:08 @monitor.py:428] QueueInput/queue_size: 34.065
[0717 18:49:08 @monitor.py:428] QueueInput_1/queue_size: 1.4937e-38
[0717 18:49:08 @monitor.py:428] net1_loss: 10.866
[0717 18:49:08 @monitor.py:428] net2_loss: 0.0048307
[0717 18:49:08 @monitor.py:428] pred1: 0.92544
[0717 18:49:08 @monitor.py:428] pred2: 0.015397
[0717 18:49:08 @base.py:262] Start Epoch 93 ...
100%|##########|100/100[00:01<00:00,63.75it/s]
[0717 18:49:09 @base.py:272] Epoch 93 (global_step 18600) finished, time:1.57 seconds.
[0717 18:49:09 @monitor.py:428] QueueInput/queue_size: 38.195
[0717 18:49:09 @monitor.py:428] QueueInput_1/queue_size: 1.42e-38
[0717 18:49:09 @monitor.py:428] net1_loss: 10.45
[0717 18:49:09 @monitor.py:428] net2_loss: 0.0047563
[0717 18:49:09 @monitor.py:428] pred1: 0.94389
[0717 18:49:09 @monitor.py:428] pred2: 0.014586
[0717 18:49:09 @base.py:262] Start Epoch 94 ...
  0%|          |0/100[00:00<?,?it/s][0717 18:49:11 @base.py:272] Epoch 94 (global_step 18800) finished, time:1.57 seconds.
100%|##########|100/100[00:01<00:00,63.74it/s]
[0717 18:49:11 @monitor.py:428] QueueInput/queue_size: 36.5
[0717 18:49:11 @monitor.py:428] QueueInput_1/queue_size: 1.5501e-38
[0717 18:49:11 @monitor.py:428] net1_loss: 10.427
[0717 18:49:11 @monitor.py:428] net2_loss: 0.004415
[0717 18:49:11 @monitor.py:428] pred1: 0.95424
[0717 18:49:11 @monitor.py:428] pred2: 0.014196
[0717 18:49:11 @base.py:262] Start Epoch 95 ...
  0%|          |0/100[00:00<?,?it/s][0717 18:49:13 @base.py:272] Epoch 95 (global_step 19000) finished, time:1.47 seconds.
100%|##########|100/100[00:01<00:00,68.15it/s]
[0717 18:49:13 @monitor.py:428] QueueInput/queue_size: 39.004
[0717 18:49:13 @monitor.py:428] QueueInput_1/queue_size: 1.2729e-38
[0717 18:49:13 @monitor.py:428] net1_loss: 10.679
[0717 18:49:13 @monitor.py:428] net2_loss: 0.004388
[0717 18:49:13 @monitor.py:428] pred1: 0.94648
[0717 18:49:13 @monitor.py:428] pred2: 0.014553
[0717 18:49:13 @base.py:262] Start Epoch 96 ...
  0%|          |0/100[00:00<?,?it/s][0717 18:49:14 @base.py:272] Epoch 96 (global_step 19200) finished, time:1.47 seconds.
100%|##########|100/100[00:01<00:00,68.10it/s]
[0717 18:49:14 @monitor.py:428] QueueInput/queue_size: 43.999
[0717 18:49:14 @monitor.py:428] QueueInput_1/queue_size: 1.5625e-38
[0717 18:49:14 @monitor.py:428] net1_loss: 10.688
[0717 18:49:14 @monitor.py:428] net2_loss: 0.0045032
[0717 18:49:14 @monitor.py:428] pred1: 0.95173
[0717 18:49:14 @monitor.py:428] pred2: 0.013164
[0717 18:49:14 @base.py:262] Start Epoch 97 ...
  0%|          |0/100[00:00<?,?it/s][0717 18:49:16 @base.py:272] Epoch 97 (global_step 19400) finished, time:1.47 seconds.
100%|##########|100/100[00:01<00:00,68.08it/s]
[0717 18:49:16 @monitor.py:428] QueueInput/queue_size: 46.185
[0717 18:49:16 @monitor.py:428] QueueInput_1/queue_size: 2.1594e-38
[0717 18:49:16 @monitor.py:428] net1_loss: 10.591
[0717 18:49:16 @monitor.py:428] net2_loss: 0.0041702
[0717 18:49:16 @monitor.py:428] pred1: 0.93017
[0717 18:49:16 @monitor.py:428] pred2: 0.012632
[0717 18:49:16 @base.py:262] Start Epoch 98 ...
  0%|          |0/100[00:00<?,?it/s][0717 18:49:17 @base.py:272] Epoch 98 (global_step 19600) finished, time:1.52 seconds.
100%|##########|100/100[00:01<00:00,65.84it/s]
[0717 18:49:17 @monitor.py:428] QueueInput/queue_size: 46.516
[0717 18:49:17 @monitor.py:428] QueueInput_1/queue_size: 1.6361e-38
[0717 18:49:17 @monitor.py:428] net1_loss: 10.789
[0717 18:49:17 @monitor.py:428] net2_loss: 0.0041644
[0717 18:49:17 @monitor.py:428] pred1: 0.95202
[0717 18:49:17 @monitor.py:428] pred2: 0.01548
[0717 18:49:17 @base.py:262] Start Epoch 99 ...
  0%|          |0/100[00:00<?,?it/s][0717 18:49:19 @base.py:272] Epoch 99 (global_step 19800) finished, time:1.52 seconds.
100%|##########|100/100[00:01<00:00,65.88it/s]
[0717 18:49:19 @monitor.py:428] QueueInput/queue_size: 48.972
[0717 18:49:19 @monitor.py:428] QueueInput_1/queue_size: 1.5584e-38
[0717 18:49:19 @monitor.py:428] net1_loss: 11.04
[0717 18:49:19 @monitor.py:428] net2_loss: 0.0040718
[0717 18:49:19 @monitor.py:428] pred1: 0.96595
[0717 18:49:19 @monitor.py:428] pred2: 0.015411
[0717 18:49:19 @base.py:262] Start Epoch 100 ...
  0%|          |0/100[00:00<?,?it/s][0717 18:49:21 @base.py:272] Epoch 100 (global_step 20000) finished, time:1.47 seconds.
100%|##########|100/100[00:01<00:00,68.09it/s]
[0717 18:49:21 @monitor.py:428] QueueInput/queue_size: 50
[0717 18:49:21 @monitor.py:428] QueueInput_1/queue_size: 1.5499e-38
[0717 18:49:21 @monitor.py:428] net1_loss: 10.796
[0717 18:49:21 @monitor.py:428] net2_loss: 0.0042789
[0717 18:49:21 @monitor.py:428] pred1: 0.96815
[0717 18:49:21 @monitor.py:428] pred2: 0.015265
[0717 18:49:21 @base.py:276] Training has finished!
[0717 18:49:21 @input_source.py:149] EnqueueThread QueueInput/input_queue Exited.
2018-07-17 18:49:21.051904: W tensorflow/core/kernels/queue_base.cc:295] _1_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed


Anyone can give me some suggest ? @flyers @ppwwyyxx ",hi run example code result normal data almost empty log epoch finished time start epoch epoch finished time start epoch epoch finished time start epoch epoch finished time start epoch epoch finished time start epoch epoch finished time start epoch epoch finished time start epoch epoch finished time start epoch epoch finished time start epoch epoch finished time start epoch epoch finished time training finished skipping attempt queue closed anyone give suggest,issue,negative,negative,neutral,neutral,negative,negative
405519657,rebased to current HEAD and remove local documentation (mistake),current head remove local documentation mistake,issue,negative,neutral,neutral,neutral,neutral,neutral
405315201,"You need to do similar things for `mask_rcnn_head_func` as well. I'm not sure if there are other places you need to also hack around.

Also, the `ff_false` part should return `[0, ncls, 4]` as the second element, although it was `ncls-1` in an earlier version.

I'm closing since the reason I drop TF 1.4 support is exactly because it's annoying and I don't want to deal with it more, especially for FPN. Hope you can solve it and get it running with my above comments. You can install a new version of TF with old cuda by compiling from source.",need similar well sure need also hack around also part return second element although version since reason drop support exactly annoying want deal especially hope solve get running install new version old source,issue,positive,negative,neutral,neutral,negative,negative
405312644,"> is that all?

That is all.

If you're wondering what you did wrong, I can not tell much because I can only guess what you did, based on the limited description you've given. From the code snippet it looks like you're subtracting the mean of the image somehow, but in the code you can see that the mean/std are constants.",wondering wrong tell much guess based limited description given code snippet like mean image somehow code see,issue,negative,negative,negative,negative,negative,negative
405245085,"I have added the tf.cond() in the ResNetFPNModel
function as follows, but  still  has this problem, how can I fix it? Thank you.
![_20180716211806](https://user-images.githubusercontent.com/37856730/42760746-fafa8ad8-893d-11e8-9aeb-5fdd8e6e36b1.png)
",added function still problem fix thank,issue,negative,neutral,neutral,neutral,neutral,neutral
405223988,"This is a cuDNN issue and not related to tensorpack at all. 
You are using a cuDNN version which is not compatible to your TensorFlow version.

Btw: I am convinced the Issue Template is not intended to be a joke and was meant to be filled out, especially the TensorFlow version is important.",issue related version compatible version convinced issue template intended joke meant filled especially version important,issue,positive,positive,positive,positive,positive,positive
405219264,"Hi @ppwwyyxx, one question.

When doing `run_image`,  the input image goes thru 
`ResizeShortestEdge` and `CenterCrop` provided by `fbresnet_augmentor`, 
and then the returned `img` is passed to the `OfflinePredictor`, 
and since the model is an `ImageNetModel`,
eventually inside the graph, due to `build_graph` of `ImageNetModel` the `img` goes thru `image_preprocess` (and NCHW/NHWC handling) before being passed into `get_logits`, 
and what `image_preprocess` does is divide values by 255, minus per channel mean and then divides std,
is that all !?

So what I'm doing is similar to replicating the inference result in say pure python+numpy environment,
the `image` after the `ResizeShortestEdge` and `CenterCrop` does match in both env, 
but then if I just do ""div 255, minus mean, div std"" 
(I copied the exact mean and std used in `ImageNetModel` ), 
the resulting image has different mean and std values than that given by TensorBoard.
On the pure env I used numpy's np.mean and np.std, on TensorPack/TensorFlow I added `variable_summaries(image, '-image')` to the model, where
```Python
def variable_summaries(var, name):
  """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""
  with tf.name_scope('summaries' + name):
    mean = tf.reduce_mean(var)
    tf.summary.scalar('mean', mean)
    with tf.name_scope('stddev'):
      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
    tf.summary.scalar('stddev', stddev)
    tf.summary.scalar('max', tf.reduce_max(var))
    tf.summary.scalar('min', tf.reduce_min(var))
    tf.summary.histogram('histogram', var)
```
Am I missing anything ? 
On both envs the image is read using cv2.imread, so both are in BGR, and they both still match after resize + crop so no problem there...",hi one question input image go provided returned since model eventually inside graph due go handling divide minus per channel mean similar inference result say pure environment image match div minus mean div copied exact mean used resulting image different mean given pure used added image model python name attach lot tensor visualization name mean mean mean missing anything image read still match resize crop problem,issue,negative,negative,negative,negative,negative,negative
405172074,Not if you ask me. But it is overly commonly used.,ask overly commonly used,issue,negative,negative,negative,negative,negative,negative
405159912,"@ppwwyyxx oh my god... It's all my fault...... I just clicked a wrong link.....the vgg16.ckpt is from google drive uploaded by totally an another author(endernewton). I'm so stupid and close this issue......

I'm so sorry and really thank you.",oh god fault wrong link drive totally another author stupid close issue sorry really thank,issue,negative,negative,negative,negative,negative,negative
405158414,"> 'your model' is vgg16.ckpt from google drive

Could you give more details? I have not put any vgg16.ckpt on google drive IIRC.",model drive could give put drive,issue,negative,neutral,neutral,neutral,neutral,neutral
405158081,"@ppwwyyxx  Thanks for your review. Here my model is generated by [gennet](https://github.com/zxqcreations/gennet), its a tool to generate a CNN.

yes, differences for some weights,  'your model' is vgg16.ckpt from google drive while mine is generated by gennet.",thanks review model tool generate yes model drive mine,issue,positive,positive,positive,positive,positive,positive
405157194,"I don't understand your question completely but it seems like you want to know why ""my model"" has different shape for some weights from your models.
However I don't know which of the ""my model"" are you talking about, and how did you obtain ""my model"". Could you answer these questions with more details?",understand question completely like want know model different shape however know model talking obtain model could answer,issue,positive,positive,neutral,neutral,positive,positive
405133171,"btw, I can understand that numpy is good for its simplicity, but is there any advantage of HDF5, compared to lmdb?",understand good simplicity advantage,issue,positive,positive,positive,positive,positive,positive
405009725,"Use http://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.PeriodicTrigger .

Or just set the epoch size to a smaller number..",use set epoch size smaller number,issue,negative,neutral,neutral,neutral,neutral,neutral
405002989,"I did not see any tensorpack issue in this thread.
If you meet unexpected problems, please post details according to the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md).",see issue thread meet unexpected please post according issue template,issue,negative,positive,neutral,neutral,positive,positive
404999535,@ppwwyyxx Have you solve this problems? I encounter a similar issues..... Waiting for your answers. Thank you very much!,solve encounter similar waiting thank much,issue,positive,positive,neutral,neutral,positive,positive
404726035,All the configurations are written in the last column. I don't quite understand what you're asking about.,written last column quite understand,issue,negative,neutral,neutral,neutral,neutral,neutral
404629441,Closing. Please track the corresponding issue in tensorflow.,please track corresponding issue,issue,negative,neutral,neutral,neutral,neutral,neutral
404567879,"I'm closing this now since it's not a tensorpack problem. Maybe `cv2.getBuildInformation()` can tell something, e.g., you can check whether CUDA or opencl is enabled in `cv2.getBuildInformation()`.",since problem maybe tell something check whether,issue,negative,neutral,neutral,neutral,neutral,neutral
404478681,"It is indeed a problem with opencv. More precisely it crashes when calling the following function:
`ret = cv2.flip(img, self.code)
`            
in line 46 of the image augmentation package: https://github.com/tensorpack/tensorpack/blob/master/tensorpack/dataflow/imgaug/misc.py

I have changed the code to the numpy version of flip which works correctly. 
`ret = np.flip(img, self.code)
`
Altough, I guess I might encounter some reduction of speed and performance using numpy instead of opencv to compute the augmentation ?

Well, if not perfect, at least it works. If somebody has any idea how i could try to debug the problem with the opencv call to **cv2.flip** it would still be great. Otherwise the issue can be closed.

Thanks all for the help",indeed problem precisely calling following function ret line image augmentation package code version flip work correctly ret guess might encounter reduction speed performance instead compute augmentation well perfect least work somebody idea could try problem call would still great otherwise issue closed thanks help,issue,positive,positive,positive,positive,positive,positive
404427556,"16G is too small. You can modify `data.py` to use smaller `nr_proc` or `buffer_size` to reduce memory use. http://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.MultiProcessMapDataZMQ

Also, I don't know what does ""on init"" means exactly. Please post unexpected issues following the issue template.",small modify use smaller reduce memory use also know exactly please post unexpected following issue template,issue,negative,positive,neutral,neutral,positive,positive
404426867,"For me it only takes 1 min, so it's certainly not something I would want to spend time improving. Not sure if the use of python2 contributes to the low efficiency.",min certainly something would want spend time improving sure use python low efficiency,issue,positive,positive,positive,positive,positive,positive
404423273,"everytime for running, it stucks at 
```
loading annotations into memory...
Done (t=10.44s)
creating index...
index created!
[0712 15:45:35 @coco.py:77] Instances loaded from /home/wangtao/prj/mscoco/annotations/instances_train2014.json.
```
for so long(more than 10 mins), then val data for 260 secs ,can we mitigate this issue?
",running loading memory done index index loaded long data mitigate issue,issue,negative,negative,neutral,neutral,negative,negative
404409166,It's not right. Please understand what the code does before changing it. You can find the syntax here: https://www.python.org/dev/peps/pep-3132/,right please understand code find syntax,issue,negative,positive,positive,positive,positive,positive
404408667,"https://github.com/tensorpack/tensorpack/blob/ccda379035165c1192f253b04af1fb3e73f1faf4/examples/FasterRCNN/eval.py#L75
one modification is change this line to `boxes, probs, labels, masks = model_func(resized_img)` dont know if it is right for py2
",one modification change line dont know right,issue,negative,positive,positive,positive,positive,positive
404408248,"I modify some thing and got it work in py2, but it stucked here:
```
[0712 14:41:04 @logger.py:74] Argv: /home/user/prj/tensorpack-fpn/train.py --config  MODE_MASK=True MODE_FPN=True  DATA.BASEDIR=/home/user/prj/mscoco  BACKBONE.WEIGHTS=./ImageNet-ResNet50.npz
[0712 14:41:06 @config.py:195] Config: ------------------------------------------
{'BACKBONE': {'FREEZE_AFFINE': False,
              'NORM': 'FreezeBN',
              'RESNET_NUM_BLOCK': [3, 4, 6, 3],
              'STRIDE_1X1': False,
              'TF_PAD_MODE': False,
              'WEIGHTS': './ImageNet-ResNet50.npz'},
 'DATA': {'BASEDIR': '/home/user/prj/mscoco',
          'CLASS_NAMES': [],
          'NUM_CATEGORY': 80,
          'NUM_CLASS': 81,
          'TRAIN': ['train2014', 'valminusminival2014'],
          'VAL': 'minival2014'},
 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),
         'FRCNN_CONV_HEAD_DIM': 256,
         'FRCNN_FC_HEAD_DIM': 1024,
         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',
         'NUM_CHANNEL': 256,
         'PROPOSAL_MODE': 'Level',
         'RESOLUTION_REQUIREMENT': 32},
 'FRCNN': {'BATCH_PER_IM': 512,
           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],
           'FG_RATIO': 0.25,
           'FG_THRESH': 0.5},
 'MODE_FPN': True,
 'MODE_MASK': True,
 'MRCNN': {'HEAD_DIM': 256},
 'PREPROC': {'MAX_SIZE': 1344.0,
             'PIXEL_MEAN': [123.675, 116.28, 103.53],
             'PIXEL_STD': [58.395, 57.12, 57.375],
             'SHORT_EDGE_SIZE': 800},
 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),
         'ANCHOR_SIZES': (32, 64, 128, 256, 512),
         'ANCHOR_STRIDE': 16,
         'BATCH_PER_IM': 256,
         'CROWD_OVERLAP_THRES': 0.7,
         'FG_RATIO': 0.5,
         'HEAD_DIM': 1024,
         'MIN_SIZE': 0,
         'NEGATIVE_ANCHOR_THRES': 0.3,
         'NUM_ANCHOR': 15,
         'POSITIVE_ANCHOR_THRES': 0.7,
         'PROPOSAL_NMS_THRESH': 0.7,
         'TEST_PER_LEVEL_NMS_TOPK': 1000,
         'TEST_POST_NMS_TOPK': 1000,
         'TEST_PRE_NMS_TOPK': 6000,
         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,
         'TRAIN_POST_NMS_TOPK': 2000,
         'TRAIN_PRE_NMS_TOPK': 12000},
 'TEST': {'FRCNN_NMS_THRESH': 0.5,
          'RESULTS_PER_IM': 100,
          'RESULT_SCORE_THRESH': 0.05,
          'RESULT_SCORE_THRESH_VIS': 0.3},
 'TRAIN': {'BASE_LR': 0.01,
           'LR_SCHEDULE': [240000, 320000, 360000],
           'NUM_GPUS': 4,
           'STEPS_PER_EPOCH': 500,
           'WARMUP': 1000,
           'WEIGHT_DECAY': 0.0001},
 'TRAINER': 'replicated'}
[0712 14:41:06 @train.py:566] Warm Up Schedule (steps, value): [(0, 0.0033333333333333335), (2000.0, 0.01)]
[0712 14:41:06 @train.py:567] LR Schedule (epochs, value): [(4, 0.01), (960.0, 0.001), (1280.0, 0.00010000000000000002)]
[0712 14:41:07 @prof.py:45] WRN [GPUUtilizationTracker] Both devices and CUDA_VISIBLE_DEVICES are None! Will monitor all 4 visible GPUs!
loading annotations into memory...
Done (t=10.54s)
creating index...
index created!
[0712 14:41:24 @coco.py:77] Instances loaded from /home/user/prj/mscoco/annotations/instances_train2014.json.
```
",modify thing got work false false false true true warm schedule value schedule value none monitor visible loading memory done index index loaded,issue,positive,positive,neutral,neutral,positive,positive
404386629,There shouldn't be much to modify. You can just run it and fix whatever exceptions you meet.,much modify run fix whatever meet,issue,negative,positive,positive,positive,positive,positive
404383181,"Interesting.. I was able to reproduce it. Will investigate it a bit.
Typically I only run GraphProfiler for about 10 steps (as mentioned in is docs).",interesting able reproduce investigate bit typically run,issue,negative,positive,positive,positive,positive,positive
404367635,"For any unexpected problems, __PLEASE ALWAYS INCLUDE__:
1. What you did:
	+ If you're using examples:
        ` examples/ResNet/imagenet-resnet.py `
		+ What's the command you run:
        ` ./imagenet-resnet.py --data path/to/data --gpu 3,4 --batch 64 `
		+ Have you made any changes to code? Paste them if any:
        just add GraphProfiler to callbacks in line 76 :
        ``` 
        callbacks = [
            ModelSaver(),
            EstimatedTimeLeft(),
            ScheduledHyperParamSetter(
                'learning_rate', [
                    (0, min(START_LR, BASE_LR)), (30, BASE_LR * 1e-1), (60, BASE_LR * 1e-2),
                    (90, BASE_LR * 1e-3), (100, BASE_LR * 1e-4)]),
            **GraphProfiler()**
        ]
        ```
    https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py#L76
2. What you observed, including but not limited to the __entire__ logs.
    
    I found this in chrome-trace-50.json, normal nodestats start timestamp are all startwith 15xxx while some nodestats start timestamp are startwith 19xxx,the timeline duration are so many year!
    '''
           {
                ""args"": {
                    ""op"": ""MEMCPYDtoH"",
                    ""name"": ""edge_7572_tower0/linear/BiasAdd""
                },
                ""tid"": 0,
                ""name"": ""MEMCPYDtoH"",
                ""ph"": ""X"",
                ""pid"": 5,
                ""dur"": 359,
                ""ts"": 19978045970716470,
                ""cat"": ""Op""
            },
            {
                ""args"": {
                    ""op"": ""MEMCPYDtoH"",
                    ""name"": ""edge_1079_tower0/gradients/tower0/gap/output_grad/DynamicStitch""
                },
                ""tid"": 1,
                ""name"": ""MEMCPYDtoH"",
                ""ph"": ""X"",
                ""pid"": 5,
                ""dur"": 2,
                ""ts"": 1531302016698591,
                ""cat"": ""Op""
            },
            {
                ""args"": {
                    ""op"": ""Sub"",
                    ""name"": ""tower0/group2/block0/conv3/bn/AssignMovingAvg/sub_1""
                },
                ""tid"": 0,
                ""name"": ""Sub"",
                ""ph"": ""X"",
                ""pid"": 17,
                ""dur"": 4,
                ""ts"": 19978045970621550,
                ""cat"": ""Op""
            },
            {
                ""args"": {
                    ""op"": ""Sub"",
                    ""name"": ""tower0/group2/block0/conv3/bn/AssignMovingAvg_1/sub_1""
                },
                ""tid"": 0,
                ""name"": ""Sub"",
                ""ph"": ""X"",
                ""pid"": 17,
                ""dur"": 67,
                ""ts"": 19978045970621559,
                ""cat"": ""Op""
            },
    '''
3. Your environment:
	+ Python version.
    'python3.5'
	+ TF version: `python -c 'import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)'`.
    'tensorflow 1.8.0'
	+ Tensorpack version: `python -c 'import tensorpack; print(tensorpack.__version__)'`.
    '0.8.6(I download the master source code and install tensorpack by python setup.py install)'
    ",unexpected always command run data batch made code paste add line min limited found normal start start duration many year name tid name cat name tid name cat sub name tid name sub cat sub name tid name sub cat environment python version version python print version python print master source code install python install,issue,negative,positive,positive,positive,positive,positive
404365771,Could you post more information following the issue template? If I could reproduce your problem I can file an issue to tensorflow. Otherwise there is nothing I can do.,could post information following issue template could reproduce problem file issue otherwise nothing,issue,negative,neutral,neutral,neutral,neutral,neutral
404359166,"yes ,it can be reproducible.
the problem  happened in one step of about 40~50 step,
the trace of step before it  are normal, after it the trace are abnormal.",yes reproducible problem one step step trace step normal trace abnormal,issue,negative,positive,positive,positive,positive,positive
404039323,"Function: Counter.

From my below code, How can I add a counter for count multiple class of multiple categories?
results = tfnet.return_predict(frame)
for color, result in zip(colors, results):
tl = (result['topleft']['x'], result['topleft']['y'])
br = (result['bottomright']['x'], result['bottomright']['y'])
label = result['label']",function counter code add counter count multiple class multiple frame color result zip color result result result result label result,issue,negative,neutral,neutral,neutral,neutral,neutral
404037764,"Yes. I am using tensorpack.
Sorry for my lack of knowledge.

Regards,


MURAD AL QURISHEE
Teaching Assistant  University of Tennessee at Chattanooga
Dept. of Civil and Chemical Engineering

m: (423)-316-8547 a: 862 Oak St, Chattanooga, TN 37403
w: http://muradalqurishee.wixsite.com/muradalqurishee
e: mgp216@ <muradalqurishee@gmail.com>mocs.utc.edu s: Murad Al Qurishee

Follow me

<https://newoldstamp.com/editor/?utm_source=Free_Editor&utm_medium=social_icon&utm_campaign=nos>
. <https://web.facebook.com/muradal.qurishee.7>.
<https://www.linkedin.com/in/muradal-qurishee>.

Get your own signature
<https://newoldstamp.com/editor/?utm_source=Free_Editor&utm_medium=email_banner&utm_campaign=new>





On 10 July 2018 at 20:59, Yuxin Wu <notifications@github.com> wrote:

> ""Using tensorflow"" does not imply you're using tensorpack. Tensorflow is a
> dependency of tensorpack.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/818#issuecomment-404037616>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AkIKm7pWKQ9FK-ibEH1G6r9BWfU2tXT6ks5uFXgagaJpZM4VKbyj>
> .
>
",yes sorry lack knowledge al teaching assistant university civil chemical engineering oak st al follow get signature wrote imply dependency thread reply directly view mute thread,issue,negative,negative,negative,negative,negative,negative
404037246,"Yes, its related to tensorpack. I am using tensorflow.

Regards,


MURAD AL QURISHEE
Teaching Assistant  University of Tennessee at Chattanooga
Dept. of Civil and Chemical Engineering

m: (423)-316-8547 a: 862 Oak St, Chattanooga, TN 37403
w: http://muradalqurishee.wixsite.com/muradalqurishee
e: mgp216@ <muradalqurishee@gmail.com>mocs.utc.edu s: Murad Al Qurishee

Follow me

<https://newoldstamp.com/editor/?utm_source=Free_Editor&utm_medium=social_icon&utm_campaign=nos>
. <https://web.facebook.com/muradal.qurishee.7>.
<https://www.linkedin.com/in/muradal-qurishee>.

Get your own signature
<https://newoldstamp.com/editor/?utm_source=Free_Editor&utm_medium=email_banner&utm_campaign=new>





On 10 July 2018 at 20:55, Yuxin Wu <notifications@github.com> wrote:

> How is ""keras tensorflow backend"" related to tensorpack?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorpack/tensorpack/issues/818#issuecomment-404037088>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AkIKmyZKW35kRtxqgmXMfNjSpd1MU0G3ks5uFXcugaJpZM4VKbyj>
> .
>
",yes related al teaching assistant university civil chemical engineering oak st al follow get signature wrote related thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
403893050,"Probably an issue with opencv. When tensorflow first released it has some bad interaction with certain build of opencv, on Linux. Maybe the problem still exists on windows.
For example, if your opencv is built with cuda support or opencl support, it's very likely to cause crash when used together with tensorflow. There are other possible causes as well.",probably issue first bad interaction certain build maybe problem still example built support support likely cause crash used together possible well,issue,negative,negative,neutral,neutral,negative,negative
403755838,"Hey, thanks for your answers,

- About it being an OOM : I tried reducing the depth / number of parameters to very low amounts and it still not run so I don't think this is the problem
- About testing other official code: actually the cifar-convnet (from basic) does not run either with the same crash (crash at the first iteration). So it might come from a method added by cifar that is not used by mnist code.

UPDATE: after investigation, the code does not crash anymore when removing the augmentation part and commenting
`    #ds = AugmentImageComponent(ds, augmentors)
`
so it seems to be a problem with the augmentation component",hey thanks tried reducing depth number low still run think problem testing official code actually basic run either crash crash first iteration might come method added used code update investigation code crash removing augmentation part problem augmentation component,issue,negative,positive,positive,positive,positive,positive
403719777,"If you merge this, I would use it as a starting point for the HDF5 saver/loader and adapt/add the unit tests later.",merge would use starting point unit later,issue,negative,neutral,neutral,neutral,neutral,neutral
403692869,"I did it in this way:
init an _profile_context in GraphProfiler  in __init__
`self._profile_context = tf.contrib.tfprof.ProfileContext(self._dir, debug=True)`
then add the run metadata of this step to the profiler in _after_run
`self._profile_context.profiler.add_step(self.global_step, meta)`
finally dump the profiler on specify step of first epoch
`self._profile_context.profiler._write_profile(filename)`

it runs ok with QueueInput and dump the profiler
",way add run step profiler meta finally dump profiler specify step first epoch dump profiler,issue,negative,positive,positive,positive,positive,positive
403688726,It's common to see this since you've removed `GoogleNetResize()`.,common see since removed,issue,negative,negative,negative,negative,negative,negative
403684049,"For unexpected problems, please post issues following the [issue template](https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md)",unexpected please post following issue template,issue,negative,positive,neutral,neutral,positive,positive
403674126,"@ppwwyyxx
I run a --dorefa 8,8,8 experiment on imagenet, but why the val-error-top1 is still >90% after 45 epoches? I just set the default settings in the alexnet_dorefa.py file

```
[0710 01:38:19 @base.py:237] Start Epoch 45 ...
100%|################################################################################################|5000/5000[16:57<00:00, 4.92it/s]
[0710 01:55:16 @base.py:247] Epoch 45 (global_step 225000) finished, time:16 minutes 57 seconds.
[0710 01:55:16 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...
[0710 01:55:28 @saver.py:84] Model saved to train_log/alexnet-dorefa-8,8,8/model-225000.
100%|##################################################################################################|782/782[01:53<00:00, 6.90it/s]
[0710 01:57:21 @monitor.py:440] QueueInput/queue_size: 49.05
[0710 01:57:21 @monitor.py:440] l2_regularize_loss: 1.0566
[0710 01:57:21 @monitor.py:440] param-summary/conv0/W-rms: 0.093548
[0710 01:57:21 @monitor.py:440] param-summary/conv1/W-rms: 0.10669
[0710 01:57:21 @monitor.py:440] param-summary/conv2/W-rms: 0.1022
[0710 01:57:21 @monitor.py:440] param-summary/conv3/W-rms: 0.1015
[0710 01:57:21 @monitor.py:440] param-summary/conv4/W-rms: 0.10491
[0710 01:57:21 @monitor.py:440] param-summary/fc0/W-rms: 0.0076856
[0710 01:57:21 @monitor.py:440] param-summary/fc1/W-rms: 0.0070158
[0710 01:57:21 @monitor.py:440] param-summary/fct/W-rms: 0.016896
[0710 01:57:21 @monitor.py:440] train-error-top1: 0.46866
[0710 01:57:21 @monitor.py:440] train-error-top5: 0.2167
[0710 01:57:21 @monitor.py:440] val-error-top1: 0.95656
[0710 01:57:21 @monitor.py:440] val-error-top5: 0.87892
[0710 01:57:21 @monitor.py:440] xentropy-loss: 2.0036
```
",run experiment still set default file start epoch epoch finished time running model saved,issue,negative,neutral,neutral,neutral,neutral,neutral
403577883,"The latest commit has also fixed it, since #802 will need some improvement before it can be merged.",latest commit also fixed since need improvement,issue,positive,positive,positive,positive,positive,positive
403198920,I think so. The paper was >2 years old and I did not perform this experiment at that time so I do not remember what was the setting.,think paper old perform experiment time remember setting,issue,negative,positive,neutral,neutral,positive,positive
403198601,"
@ppwwyyxx 
If I train a dorefa-32-32-32 model, can I regard this model as the full-precision model, as described in your paper?",train model regard model model paper,issue,negative,neutral,neutral,neutral,neutral,neutral
403196630,"You wrote `DictRestore(args.load)`.
`args.load` is a filename of type `str`, as printed in the error message.",wrote type printed error message,issue,negative,neutral,neutral,neutral,neutral,neutral
403196562,"@ppwwyyxx 
But this weights.npz file is created from 
`python dump-model-params.py --meta graph.meta model-2925 weights.npz`.... 
This should be a dict, right?",file python meta right,issue,negative,positive,positive,positive,positive,positive
403196360,"I don't know what is the ""original tensorflow way"".

This does not give you any prefetch. You can use `tf.data` API to add prefetch yourself, e.g.: `TFDatasetInput.dataflow_to_dataset(dataset_train, [tf.float32, tf.int32]).prefetch(100)`.",know original way give use add,issue,negative,positive,positive,positive,positive,positive
403196244,"TFDatasetInput works, and profiler runs ok.
TFDatasetInput coverts dataflow to tf.dataset and enable prefetch from disk to cpu using orignal tensorflow way？
Can I say TFDatasetInput do the same work with QueueInput just in different way by different speed.",work profiler enable disk orignal say work different way different speed,issue,negative,neutral,neutral,neutral,neutral,neutral
403196217,"@ppwwyyxx 
--load ""weights.npz""
changing to DictRestore() still error......
```
Traceback (most recent call last):
  File ""alexnet-dorefa.py"", line 279, in <module>
    config.session_init = DictRestore(args.load)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack-0.8.6-py2.7.egg/tensorpack/tfutils/sessinit.py"", line 195, in __init__
    assert isinstance(variable_dict, dict), type(variable_dict)
AssertionError: <type 'str'>
```",load still error recent call last file line module file line assert type type,issue,negative,neutral,neutral,neutral,neutral,neutral
403195814,"@ppwwyyxx 
I have successfully generate this weights.npz, but when I load this for finetuning, error is like this:
```
tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file ~/projects/tensorpack/examples/DoReFa-Net/float32/cifar100/train_log/alexnet-dorefa-
32,32,32/model-7800-weights.npz: Data loss: not an sstable (bad magic number): 
perhaps your file is in a different file format and you need to use a different restore operator?
PrefetchDataZMQ successfully cleaned-up.
```",successfully generate load error like unable open table file data loss bad magic number perhaps file different file format need use different restore operator successfully,issue,negative,positive,positive,positive,positive,positive
403195576,"@ppwwyyxx 
a little confused 
dump-model-params.py [-h] --meta META input output
I just run
`python dump-model-params.py --meta graph.meta model-2925 weights.npz`
not correct?",little confused meta meta input output run python meta correct,issue,negative,negative,negative,negative,negative,negative
403194847,"Conceptually yes. It loads whatever in `weights.npz`.
But your command line arguments are incorrect. Please read the docs `dump-model-params.py -h`",conceptually yes whatever command line incorrect please read,issue,positive,neutral,neutral,neutral,neutral,neutral
403194840,"as try way 1: the training runs ok,but no profiler file generated .
then I try it on PennTreebank example ,it works ok.
so it's about the QueueInput ,not the tensorpack source code.
I would try TFDatasetInput


",try way training profiler file try example work source code would try,issue,negative,neutral,neutral,neutral,neutral,neutral
403194726,"@ppwwyyxx 
so did you mean I can simply run the 
`python dump-model-params.py --input model-2925 --output weights.npz --meta graph.meta`
and then I can load this weights.npz file **for only loading the weights?**",mean simply run python input output meta load file loading,issue,negative,negative,negative,negative,negative,negative
403194542,"You can use StagingInput.
I think originally they have not seriously tested such features, just like many other ""fancy"" features in tensorflow that does not work.
You can use GraphProfiler to profile. It works for all types of input.
You can use ProfileContext in your own code. You do not need to touch tensorpack source code.",use think originally seriously tested like many fancy work use profile work input use code need touch source code,issue,negative,positive,neutral,neutral,positive,positive
403194312,"can it  be used with StagingInput? I think yes
and How orignal tensorflow profiler prefetch data

And must I change the tensorpack source code tensorpack/train/base.py to add profiler?",used think yes orignal profiler data must change source code add profiler,issue,negative,neutral,neutral,neutral,neutral,neutral
403194298,"You can use alternative to `QueueInput` such as `data=TFDatasetInput(TFDatasetInput.dataflow_to_dataset(dataset_train, [tf.float32, tf.int32]))` which does not use threads.

In general I don't think ProfileContext is a good thing to use. It's implemented in a very very hacky way and that's doomed to cause trouble.",use alternative use general think good thing use hacky way cause trouble,issue,negative,positive,positive,positive,positive,positive
403194098,"@ppwwyyxx 
I run --load 2925, and
the log has the following hints:
[0707 06:52:15 @steps.py:126] Start training with global_step=2925

so this restore just load the weights, not any more information loaded, right? (for example load checkpoint  **learning rate stored in the model-2925**)",run load log following start training restore load information loaded right example load learning rate,issue,negative,positive,positive,positive,positive,positive
403193861,"Some debugging shows that `ProfileContext` is not thread safe.
Therefore it cannot be used with `QueueInput`. `QueueInput` will start a new thread to feed data into queues.
This is a tensorflow issue so there isn't much I can do.",thread safe therefore used start new thread feed data issue much,issue,negative,positive,positive,positive,positive,positive
403192866,"the profile_6 up is imcompeleted.
I change the PennTreebank example in the same way ,and try as third way
it runs ok and the profile file generated right.
so this problem may be between the QueueInput and ProfileContext",change example way try third way profile file right problem may,issue,negative,positive,positive,positive,positive,positive
403192694,"@ppwwyyxx 
I have trained a model and save it like this:
![image](https://user-images.githubusercontent.com/13804492/42407567-b290a460-81f1-11e8-8f7c-341ef596ce22.png)
**But how to load this for finetuning?** The alexnet-dorefa.py seems not to generate the ckpt model files.",trained model save like image load generate model,issue,positive,neutral,neutral,neutral,neutral,neutral
403188783,"I have changed the mnist-convnet example trainer to the horovodtrainer and mpirun it on 2 gpus .
change the TrainConfig as:
`        model=Model(),`
`        dataflow=dataset_train, `
`        callbacks=[`
`            GPUUtilizationTracker(),`
`            GraphProfiler(dump_metadata=True, dump_tracing=True, dump_event=True),`
`        ],`
`        steps_per_epoch=10,`
`        max_epoch=3,`

it works fine. Then I try profiler
try 1:
add profiler in the train script as 
`    with tf.contrib.tfprof.ProfileContext(os.path.join('./train_log', str(hvd.rank())),trace_steps=range(3, 6),
                              dump_steps=[6]) as pctx:`
 `           launch_train_with_config(config, HorovodTrainer())`
it runs ok,but no profiler file generated .

try2:
add profiler in the tensorpack source tensorpack/train/base.py as
`    def main_loop(self, steps_per_epoch, starting_epoch, max_epoch):`
`            with tf.contrib.tfprof.ProfileContext(logget_logger_dir()) as pctx:`
 `                with self.sess.as_default():`
it generate profiler file but an exception 
`tensorflow.python.framework.errors_impl.UnavailableError: CUPTI subcriber limit reached.`

try3:
add profiler in the tensorpack source tensorpack/train/base.py as
`    def main_loop(self, steps_per_epoch, starting_epoch, max_epoch):`
`        with tf.contrib.tfprof.ProfileContext(logger.get_logger_dir(),trace_steps=range(3, 6), dump_steps=[6]) as pctx:`
 `           with self.sess.as_default():`

no exception found, but hold stuck when fisrt epoch of rank=0 end .
rank=0 logdir has 10 chrome-trace-*.json and 10 runmetadata-*.pb and one events file and one log.log
rank=1 logdir has only one events file and one log.log and the profile_6!
",example trainer change work fine try profiler try add profiler train script profiler file try add profiler source self generate profiler file exception limit try add profiler source self exception found hold stuck epoch end one file one one file one,issue,negative,positive,positive,positive,positive,positive
403184052,Adding this to the mnist example works fine. So please provide more details on what you did and what you observed.,example work fine please provide,issue,negative,positive,positive,positive,positive,positive
403136130,"Check your logs carefully. There is probably a warning.

Next time please post issues following the issue template. In particular, include the entire logs.",check carefully probably warning next time please post following issue template particular include entire,issue,negative,positive,neutral,neutral,positive,positive
403086613,This is the official implementation of the paper. It never has the so-called de-normalization process.,official implementation paper never process,issue,negative,neutral,neutral,neutral,neutral,neutral
403086434,"@ppwwyyxx 
So did you mean that this implementation does not have the de-normalization process? This does not influence the accuracy？",mean implementation process influence,issue,negative,negative,negative,negative,negative,negative
403085561,"@ppwwyyxx 
But what I am refering to is the weights of convolutions, not activations. Why can batchnorm bring the data distribution of convolution weights back?",bring data distribution convolution back,issue,negative,neutral,neutral,neutral,neutral,neutral
403083548,"1. I don't know what you mean by ""zero the gradients"". TensorFlow has correct gradients for the clip operation.
2. It's correct. But there is also a BatchNorm layer that will bring the distribution back.",know mean zero correct clip operation correct also layer bring distribution back,issue,negative,negative,negative,negative,negative,negative
403082893,Maybe also try some larger official code (e.g. `ResNet/cifar10-resnet.py`). Perhaps you have something like an out-of-memory error but windows does not tell you.,maybe also try official code perhaps something like error tell,issue,negative,neutral,neutral,neutral,neutral,neutral
403081904,"@ppwwyyxx 
1. Given that there is clip operation to [0,1] on the activations, we need to zero the gradients corresponding to these 0s and 1s. But I can not find this zero-gradient operation in the code. Is this due to the auto-grad mechanism of tensorflow?
2. It seems that there is no ""de-normalization"" process in the code, since there is the following formula to normalize the weight to range[-1,1]
![image](https://user-images.githubusercontent.com/13804492/42389616-dbcaa02c-817b-11e8-9c34-9103a975f453.png)
If there is no de-normalization (or call it de-quantization), the data distribution seems to be different from the original weights. Is this correct? Or have I understood something wrong?

",given clip operation need zero corresponding find operation code due mechanism process code since following formula normalize weight range image call data distribution different original correct understood something wrong,issue,negative,negative,neutral,neutral,negative,negative
403080969,"Your modification still runs well on Linux. I believe this is an environment problem (software incompatibilities among each other, etc). Sorry but I cannot help.",modification still well believe environment problem among sorry help,issue,negative,negative,negative,negative,negative,negative
403061033,"Hey,

Thanks a lot for your answer. I pasted below the code with the changes.

To answer : ""what makes you think it crashed""? Well code stops running at this point + output error message in windows : ""Python has stopped working, A problem caused the program to stop working correctly""

I ran the MNIST examples in the basics and it worked correctly

```
import numpy as np
import tensorflow as tf
import argparse
import os

from tensorpack import *
from tensorpack.tfutils.symbolic_functions import *
from tensorpack.tfutils.summary import *

BATCH_SIZE = 64

class Model(ModelDesc):
    def __init__(self, depth):
        super(Model, self).__init__()
        self.N = int((depth - 4)  / 3)
        self.growthRate =12

    def _get_inputs(self):
        return [InputDesc(tf.float32, [None, 32, 32, 3], 'input'),
                InputDesc(tf.int32, [None], 'label')
               ]

    def _build_graph(self, input_vars):
        image, label = input_vars
        image = image / 128.0 - 1

        def conv(name, l, channel, stride):
            return Conv2D(name, l, channel, 3, stride=stride,
                          nl=tf.identity, use_bias=False,
                          W_init=tf.random_normal_initializer(stddev=np.sqrt(2.0/9/channel)))
        def add_layer(name, l):
            shape = l.get_shape().as_list()
            in_channel = shape[3]
            with tf.variable_scope(name) as scope:
                c = BatchNorm('bn1', l)
                c = tf.nn.relu(c)
                c = conv('conv1', c, self.growthRate, 1)
                l = tf.concat([c, l], 3)
            return l

        def add_transition(name, l):
            shape = l.get_shape().as_list()
            in_channel = shape[3]
            with tf.variable_scope(name) as scope:
                l = BatchNorm('bn1', l)
                l = tf.nn.relu(l)
                l = Conv2D('conv1', l, in_channel, 1, stride=1, use_bias=False, nl=tf.nn.relu)
                l = AvgPooling('pool', l, 2)
            return l


        def dense_net(name):
            l = conv('conv0', image, 16, 1)
            with tf.variable_scope('block1') as scope:

                for i in range(self.N):
                    l = add_layer('dense_layer.{}'.format(i), l)
                l = add_transition('transition1', l)

            with tf.variable_scope('block2') as scope:

                for i in range(self.N):
                    l = add_layer('dense_layer.{}'.format(i), l)
                l = add_transition('transition2', l)

            with tf.variable_scope('block3') as scope:

                for i in range(self.N):
                    l = add_layer('dense_layer.{}'.format(i), l)
            l = BatchNorm('bnlast', l)
            l = tf.nn.relu(l)
            l = GlobalAvgPooling('gap', l)
            logits = FullyConnected('linear', l, out_dim=10, nl=tf.identity)

            return logits

        logits = dense_net(""dense_net"")

        prob = tf.nn.softmax(logits, name='output')

        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
        cost = tf.reduce_mean(cost, name='cross_entropy_loss')

        wrong = prediction_incorrect(logits, label)
        # monitor training error
        add_moving_summary(tf.reduce_mean(wrong, name='train_error'))

        # weight decay on all W
        wd_cost = tf.multiply(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')
        add_moving_summary(cost, wd_cost)

        add_param_summary(('.*/W', ['histogram']))   # monitor W
        self.cost = tf.add_n([cost, wd_cost], name='cost')

    def _get_optimizer(self):
        lr = tf.get_variable('learning_rate', initializer=0.1, trainable=False)
        tf.summary.scalar('learning_rate', lr)
        return tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)


def get_data(train_or_test):
    isTrain = train_or_test == 'train'
    ds = dataset.Cifar10(train_or_test)
    pp_mean = ds.get_per_pixel_mean()
    if isTrain:
        augmentors = [
            imgaug.CenterPaste((40, 40)),
            imgaug.RandomCrop((32, 32)),
            imgaug.Flip(horiz=True),
            #imgaug.Brightness(20),
            #imgaug.Contrast((0.6,1.4)),
            #imgaug.MapImage(lambda x: x - pp_mean),
        ]
    else:
        augmentors = [
            #imgaug.MapImage(lambda x: x - pp_mean)
        ]
    ds = AugmentImageComponent(ds, augmentors)
    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)
    #if isTrain:
        #ds = PrefetchData(ds, 3, 2)
    return ds

def get_config():
    log_dir = 'train_log/cifar10-single-fisrt%s-second%s-max%s' % (str(args.drop_1), str(args.drop_2), str(args.max_epoch))
    logger.set_logger_dir(log_dir, action='n')

    # prepare dataset
    dataset_train = get_data('train')
    steps_per_epoch = dataset_train.size()
    dataset_test = get_data('test')

    return TrainConfig(
        dataflow=dataset_train,
        callbacks=[
            ModelSaver(),
            InferenceRunner(dataset_test,
                [ScalarStats('cost'), ClassificationError()]),
            ScheduledHyperParamSetter('learning_rate',
                                      [(1, 0.1), (args.drop_1, 0.01), (args.drop_2, 0.001)])
        ],
        model=Model(depth=args.depth),
        steps_per_epoch=steps_per_epoch,
        max_epoch=args.max_epoch,
    )

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.') # nargs='*' in multi mode
    parser.add_argument('--load', help='load model')
    parser.add_argument('--drop_1',default=150, help='Epoch to drop learning rate to 0.01.') # nargs='*' in multi mode
    parser.add_argument('--drop_2',default=225,help='Epoch to drop learning rate to 0.001')
    parser.add_argument('--depth',default=40, help='The depth of densenet')
    parser.add_argument('--max_epoch',default=300,help='max epoch')
    args = parser.parse_args()

    if args.gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu

    config = get_config()
    if args.load:
        config.session_init = SaverRestore(args.load)
    
    nr_tower = 0
    if args.gpu:
        nr_tower = len(args.gpu.split(','))
    
    # SyncMultiGPUTrainer(config).train()
    launch_train_with_config(config, SimpleTrainer())
```",hey thanks lot answer pasted code answer think well code running point output error message python stopped working problem program stop working correctly ran worked correctly import import import import o import import import class model self depth super model self depth self return none none self image label image image name channel stride return name channel name shape shape name scope return name shape shape name scope return name image scope range scope range scope range return prob cost cost cost wrong label monitor training error wrong weight decay cost monitor cost self return lambda else lambda return prepare return parser list use mode load model drop learning rate mode drop learning rate depth depth epoch,issue,negative,negative,negative,negative,negative,negative
403057993,"The logs you posted does not contain any error. So the question is what makes you think it crashed?
The code runs well on linux. I recommend you to run some basic mnist examples  (in `examples/basics`) to rule out problems in your environment first.

Also, as mentioned in the issue template:
> 	+ Better to paste what you did instead of describing them.
",posted contain error question think code well recommend run basic rule environment first also issue template better paste instead,issue,positive,positive,positive,positive,positive,positive
403055894,"1. This open source code does not contain the experiment you mentioned. We will not provide the full precision model.
2. `PeriodicTrigger(ModelSaver(), every_k_epochs=5)`.",open source code contain experiment provide full precision model,issue,negative,positive,positive,positive,positive,positive
403013949,"after investigation, the code seems to crash when calling ""run_step"" of base.py of the trainer

when executing the line  self.hooked_sess.run(self.train_op)
```

    def run_step(self):
        """"""
        Defines what to do in one iteration. The default is:
        ``self.hooked_sess.run(self.train_op)``.

        The behavior of each iteration can be changed by either setting ``trainer.train_op``,
        or overriding this method.
        """"""
        if not hasattr(self, 'train_op'):
            raise NotImplementedError(
                ""Please either set `Trainer.train_op` or provide an implementation ""
                ""of Trainer.run_step()!"")
        self.hooked_sess.run(self.train_op)
```",investigation code crash calling trainer line self one iteration default behavior iteration either setting self raise please either set provide implementation,issue,negative,neutral,neutral,neutral,neutral,neutral
403012840,"@ppwwyyxx 
1. in the paper the author mentioned the training-from-scratch and fineuning-from-full-precision experiments. If I set --dorefa 32,32,32, then the model is full-precision , right? It can be used to be finetuning?
2. If I simply want to do evaluation every 1 epoch, and save model every 5 epoches, how to write the .PeriodicTrigger?
3. Given that there is clip operation to [0,1] on the activations, we need to zero the gradients corresponding to these 0s and 1s. **But I can not find this zero-gradient operation in the code**. Is this due to the auto-grad mechanism of tensorflow?",paper author set model right used simply want evaluation every epoch save model every write given clip operation need zero corresponding find operation code due mechanism,issue,positive,positive,neutral,neutral,positive,positive
402689551,"@ppwwyyxx 
1. In dorefa-net, most of the convoluion and fully connected layers have **no bias term**, is this true? If it is, why?
https://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/alexnet-dorefa.py#L93
",fully connected bias term true,issue,negative,positive,positive,positive,positive,positive
402471191,"@ppwwyyxx 
given that there is clip operation to [0,1] for the activations in dorefa-net, what is the pixel value range of the  network input?
[-1,1] or [-255,255]?? (since you have substracted mean). Will this influence the result?",given clip operation value range network input since mean influence result,issue,negative,negative,negative,negative,negative,negative
402388191,The code now produces the data with a fixed order. You probably want each GPU to take different data so you'll also need to modify the data loading code.,code data fixed order probably want take different data also need modify data loading code,issue,negative,positive,neutral,neutral,positive,positive
402387672,"oh  yes, 
I change the trainer to HorovodTrainer ,it works .",oh yes change trainer work,issue,negative,neutral,neutral,neutral,neutral,neutral
402376175,"so there is an option --gpu ,I think the example support GPU training.
I want to run an lstm example training on multi gpu , how would I change it? or there is another example I can use",option think example support training want run example training would change another example use,issue,negative,neutral,neutral,neutral,neutral,neutral
402354671,"Of course when `get_data` returns, the generator ends.
You can use `RepeatedData(dataflow, -1)` which will call `get_data` again after it returns.",course generator use call,issue,negative,neutral,neutral,neutral,neutral,neutral
402354431," @ppwwyyxx  Sorry  ,maybe I did't explain it clearly,I trained my models with keras and wrote my data generator with tensorpack ,I wrote my own Dataflow inherited from the RNGDataFlow, overrode the get_data() method:
```
def get_data(self):
    idxs = np.arange(self.size())
    self.rng.shuffle(idxs)
    for idx in idxs:
        yield [self.all_meta[idx]]
```
but at the end of the first eopch,it stopped iteration,the error logs: 

> Traceback (most recent call last):
>   File ""train_pose.py"", line 225, in <module>
>     initial_epoch=last_epoch)
>   File ""/home/lipengkun/anaconda2/envs/tf-pose-paf/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper
>     return func(*args, **kwargs)
>   File ""/home/lipengkun/anaconda2/envs/tf-pose-paf/lib/python3.5/site-packages/keras/engine/training.py"", line 2011, in fit_generator
>     generator_output = next(output_generator)
> StopIteration

so ,I think the iterator is empty ,how do i fix my code so that at the end of the first epoch,the generator would  re-generate an iterator so that it could continued training
",sorry maybe explain clearly trained wrote data generator wrote method self yield end first stopped iteration error recent call last file line module file line wrapper return file line next think empty fix code end first epoch generator would could continued training,issue,negative,positive,neutral,neutral,positive,positive
402352385,I cannot understand what your issue is. Could you post with the issue template? In particular please include all the logs.,understand issue could post issue template particular please include,issue,negative,positive,positive,positive,positive,positive
402342999,"@ppwwyyxx 
but still error after I downloaded the cifar-100 data manully and run the cifar-convnet in the /examples/basics foler.
![image](https://user-images.githubusercontent.com/13804492/42253196-338e7118-7f73-11e8-94da-444278fefd9e.png)
",still error data run image,issue,negative,neutral,neutral,neutral,neutral,neutral
402245271,"btw, even with 4 TitanX and TF 1.3, and standard alexnet (in https://github.com/tensorpack/tensorpack/tree/master/examples/ImageNetModels), I can see 4.8 it/s if each iteration is a total batch of 256.

DoReFa-Net is not a standard alexnet. On 4 TitanX and TF 1.3 I saw around 1.5it/s -- just a reference. You still need to figure out why your environment is slow. It's probably caused by slow data loader (i.e. slow cpu or disk). Since you're using anaconda, it's worth noticing that opencv in anaconda is usually very very slow: https://github.com/tensorpack/benchmarks/blob/master/ImageNet/benchmark-opencv-resize.py .
You might want to check that and use a different opencv (e.g. `pip install opencv-python`)",even standard see iteration total batch standard saw around reference still need figure environment slow probably slow data loader slow disk since anaconda worth anaconda usually slow might want check use different pip install,issue,negative,negative,negative,negative,negative,negative
402224478,"1. http://tensorpack.readthedocs.io/tutorial/performance-tuning.html
2. Don't know where the npz is from. But you'll need to modify the code to load from npz instead of checkpoint. http://tensorpack.readthedocs.io/tutorial/save-load.html#load-a-model
3. The same as above. Extract with `tar xzf xx.tar.gz`. You'll get a directory called `cifar-100-python`.
4. http://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.PeriodicTrigger",know need modify code load instead extract tar get directory python,issue,negative,neutral,neutral,neutral,neutral,neutral
402051756,"@ppwwyyxx 
no training error now . thanks a lot
1. why is the training so slow?( in caffe, alexnet for imagenet should be around  4 iter/s in speed) I am using 4 titan xp.
![image](https://user-images.githubusercontent.com/13804492/42216867-e02bc8a4-7ef5-11e8-86d5-249005ec883f.png)

2. besides,  if I want to train the quantized model based on the 32-bit full precision model, will simply ""--load alexnet.npz"" work? I tried, but error occurs.
3. how to train dorefa on cifar100? I tried the cifar-convnet.py, and download the dataset manullay and then unzip into the right folder, like this: (just like what I did in the alexnet-imagenet.py)
![image](https://user-images.githubusercontent.com/13804492/42217844-52c0d884-7ef9-11e8-9f82-abdbef39fe5a.png)
but why the code still download the cifar-100-python.tar.gz automatically, which leads to error?
4. can not find where to define ""how many epoches do an inference during training"". where is these parameters? the code is a little confusing to me",training error thanks lot training slow around speed image besides want train model based full precision model simply load work tried error train tried right folder like like image code still automatically error find define many inference training code little,issue,negative,positive,positive,positive,positive,positive
402049011,You can `print(tensorpack.__file__)` to see where to modify. You can also run `pip install -U git+https://github.com/tensorpack/tensorpack.git` again to upgrade.,print see modify also run pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
402048515,"@ppwwyyxx 
I will give a try instantly. Before this, I want to ask that, I have installed the tensorpack using ""pip install -U git+https://github.com/tensorpack/tensorpack.git"" , in the anaconda2 environment. So if I  want to manually make modifications based on your new commits, I need to modified the code files in the anaconda/lib/python2.7/tensorpack/xxxxxx ?",give try instantly want ask pip install anaconda environment want manually make based new need code,issue,negative,positive,neutral,neutral,positive,positive
402047433,"The above commit should fix this one (and the other ones in this issue).

The dorefa-net example actually explicitly said in the read me that it requires TF>=1.7: https://github.com/tensorpack/tensorpack/tree/master/examples/DoReFa-Net#use.
To really use TF1.3 you'll need to copy-paste an older version of `dorefa.py` as well (I assume you've done that already?)",commit fix one issue example actually explicitly said read really use need older version well assume done already,issue,positive,positive,positive,positive,positive,positive
402046505,"@ppwwyyxx 
sorry, I forgort to modify the batch_norm.py in the anaconda/lib/python2.7/xxxxx ..
after modifying that file, the error changes to:
```
root@997991b14e71:/data/home/users/wangdu/projects/tensorpack/examples/DoReFa-Net# ./alexnet-dorefa.py --dorefa 1,2,6 --data /data/data/ImageNetOrigin --gpu 4,5,6,7
/root/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[0703 07:51:08 @logger.py:109] WRN Log directory train_log/alexnet-dorefa-1,2,6 exists! Use 'd' to delete it. 
[0703 07:51:08 @logger.py:112] WRN If you're resuming from a previous run, you can choose to keep it.
Press any other key to exit. 
Select Action: k (keep) / d (delete) / q (quit):d
[0703 07:51:10 @logger.py:74] Argv: ./alexnet-dorefa.py --dorefa 1,2,6 --data /data/data/ImageNetOrigin --gpu 4,5,6,7
[0703 07:51:10 @alexnet-dorefa.py:222] Batch per tower: 64
[0703 07:51:10 @fs.py:88] WRN Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.
[0703 07:51:12 @parallel.py:290] [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[0703 07:51:12 @ilsvrc.py:128] [ILSVRC12] Assuming directory /data/data/ImageNetOrigin/val has 'original' structure.
[0703 07:51:12 @training.py:51] [DataParallel] Training a model of 4 towers.
[0703 07:51:12 @interface.py:31] Automatically applying QueueInput on the DataFlow.
[0703 07:51:12 @interface.py:42] Automatically applying StagingInput on the DataFlow.
[0703 07:51:12 @input_source.py:195] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0703 07:51:12 @training.py:111] Building graph for training tower 0 on device /gpu:0 ...
[0703 07:51:12 @registry.py:121] conv0 input: [None, 3, 224, 224]
[0703 07:51:12 @registry.py:129] conv0 output: [None, 96, 54, 54]
[0703 07:51:12 @registry.py:121] conv1 input: [None, 96, 54, 54]
[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight conv1/W
[0703 07:51:12 @registry.py:129] conv1 output: [None, 256, 54, 54]
[0703 07:51:12 @registry.py:121] pool1 input: [None, 256, 54, 54]
[0703 07:51:12 @registry.py:129] pool1 output: [None, 256, 27, 27]
[0703 07:51:12 @registry.py:121] conv2 input: [None, 256, 27, 27]
[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight conv2/W
[0703 07:51:12 @registry.py:129] conv2 output: [None, 384, 27, 27]
[0703 07:51:12 @registry.py:121] pool2 input: [None, 384, 27, 27]
[0703 07:51:12 @registry.py:129] pool2 output: [None, 384, 14, 14]
[0703 07:51:12 @registry.py:121] conv3 input: [None, 384, 14, 14]
[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight conv3/W
[0703 07:51:12 @registry.py:129] conv3 output: [None, 384, 14, 14]
[0703 07:51:12 @registry.py:121] conv4 input: [None, 384, 14, 14]
[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight conv4/W
[0703 07:51:12 @registry.py:129] conv4 output: [None, 256, 14, 14]
[0703 07:51:12 @registry.py:121] pool4 input: [None, 256, 14, 14]
[0703 07:51:12 @registry.py:129] pool4 output: [None, 256, 6, 6]
[0703 07:51:12 @registry.py:121] fc0 input: [None, 256, 6, 6]
[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight fc0/W
[0703 07:51:12 @registry.py:129] fc0 output: [None, 4096]
[0703 07:51:12 @registry.py:121] fc1 input: [None, 4096]
[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight fc1/W
[0703 07:51:12 @registry.py:129] fc1 output: [None, 4096]
[0703 07:51:12 @registry.py:121] fct input: [None, 4096]
[0703 07:51:12 @registry.py:129] fct output: [None, 1000]
[0703 07:51:12 @regularize.py:88] regularize_cost() found 3 variables to regularize.
[0703 07:51:12 @regularize.py:19] The following tensors will be regularized: fc0/W:0, fc1/W:0, fct/W:0
[0703 07:51:13 @training.py:111] Building graph for training tower 1 on device /gpu:1 ...
[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/conv1/W
[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/conv2/W
[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/conv3/W
[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/conv4/W
[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/fc0/W
[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/fc1/W
[0703 07:51:13 @regularize.py:88] regularize_cost() found 3 variables to regularize.
[0703 07:51:14 @training.py:111] Building graph for training tower 2 on device /gpu:2 ...
[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/conv1/W
[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/conv2/W
[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/conv3/W
[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/conv4/W
[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/fc0/W
[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/fc1/W
[0703 07:51:15 @regularize.py:88] regularize_cost() found 3 variables to regularize.
[0703 07:51:15 @training.py:111] Building graph for training tower 3 on device /gpu:3 ...
[0703 07:51:15 @alexnet-dorefa.py:79] Quantizing weight tower3/conv1/W
[0703 07:51:15 @alexnet-dorefa.py:79] Quantizing weight tower3/conv2/W
[0703 07:51:16 @alexnet-dorefa.py:79] Quantizing weight tower3/conv3/W
[0703 07:51:16 @alexnet-dorefa.py:79] Quantizing weight tower3/conv4/W
[0703 07:51:16 @alexnet-dorefa.py:79] Quantizing weight tower3/fc0/W
[0703 07:51:16 @alexnet-dorefa.py:79] Quantizing weight tower3/fc1/W
[0703 07:51:16 @regularize.py:88] regularize_cost() found 3 variables to regularize.
[0703 07:51:18 @training.py:318] 'sync_variables_from_main_tower' includes 243 operations.
[0703 07:51:18 @model_utils.py:63] Trainable Variables: 
name           shape                  dim
-------------  ----------------  --------
conv0/W:0      [12, 12, 3, 96]      41472
conv0/b:0      [96]                    96
conv1/W:0      [5, 5, 48, 256]     307200
bn1/beta:0     [256]                  256
bn1/gamma:0    [256]                  256
conv2/W:0      [3, 3, 256, 384]    884736
bn2/beta:0     [384]                  384
bn2/gamma:0    [384]                  384
conv3/W:0      [3, 3, 192, 384]    663552
bn3/beta:0     [384]                  384
bn3/gamma:0    [384]                  384
conv4/W:0      [3, 3, 192, 256]    442368
bn4/beta:0     [256]                  256
bn4/gamma:0    [256]                  256
fc0/W:0        [9216, 4096]      37748736
fc0/b:0        [4096]                4096
bnfc0/beta:0   [4096]                4096
bnfc0/gamma:0  [4096]                4096
fc1/W:0        [4096, 4096]      16777216
bnfc1/beta:0   [4096]                4096
bnfc1/gamma:0  [4096]                4096
fct/W:0        [4096, 1000]       4096000
fct/b:0        [1000]                1000
Total #vars=23, #params=60985416, size=232.64MB
[0703 07:51:18 @base.py:174] Setup callbacks graph ...
[0703 07:51:18 @parallel.py:52] WRN TENSORPACK_PIPEDIR is not used on Linux any more! Abstract sockets will be used.
Traceback (most recent call last):
  File ""./alexnet-dorefa.py"", line 227, in <module>
    launch_train_with_config(config, SyncMultiGPUTrainerReplicated(nr_tower))
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/interface.py"", line 90, in launch_train_with_config
    extra_callbacks=config.extra_callbacks)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/base.py"", line 306, in train_with_defaults
    steps_per_epoch, starting_epoch, max_epoch)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/base.py"", line 276, in train
    self.setup_callbacks(callbacks, monitors)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 181, in wrapper
    return func(*args, **kwargs)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/base.py"", line 176, in setup_callbacks
    self._callbacks.setup_graph(weakref.proxy(self))
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/callbacks/base.py"", line 52, in setup_graph
    self._setup_graph()
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/callbacks/group.py"", line 66, in _setup_graph
    cb.setup_graph(self.trainer)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/callbacks/base.py"", line 52, in setup_graph
    self._setup_graph()
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/input_source/input_source.py"", line 507, in _setup_graph
    unstage_op = tf.group(unstage_ops, name='unstage_all')
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2895, in group
    dev = inp.device
AttributeError: 'list' object has no attribute 'device'
PrefetchDataZMQ successfully cleaned-up.
```
",sorry modify file error root data conversion second argument float future float import log directory use delete previous run choose keep press key exit select action keep delete quit data batch per tower set fork one time assuming directory structure training model automatically automatically setting queue building graph training tower device input none output none input none weight output none pool input none pool output none input none weight output none pool input none pool output none input none weight output none input none weight output none pool input none pool output none input none weight output none input none weight output none input none output none found regularize following building graph training tower device weight weight weight weight weight weight found regularize building graph training tower device weight weight weight weight weight weight found regularize building graph training tower device weight weight weight weight weight weight found regularize trainable name shape dim total setup graph used abstract used recent call last file line module file line file line file line train file line wrapper return file line self file line file line file line file line file line group dev object attribute successfully,issue,negative,positive,neutral,neutral,positive,positive
402039242,"```diff
--- i/tensorpack/models/batch_norm.py
+++ w/tensorpack/models/batch_norm.py
@@ -166,7 +166,7 @@ def BatchNorm(inputs, axis=None, training=None, momentum=0.9, epsilon=1e-5,
                 center=center, scale=scale,
                 beta_initializer=beta_initializer,
                 gamma_initializer=gamma_initializer,
-                fused=True,
+                fused=(ndims == 4 and axis in [1, 3]),
                 _reuse=tf.get_variable_scope().reuse)
             if TF_version >= 1.5:
                 tf_args['virtual_batch_size'] = virtual_batch_size
```
Could you check whether this works for your version?",axis could check whether work version,issue,negative,neutral,neutral,neutral,neutral,neutral
402038151,It works fine under TF 1.8. Maybe an early version of TF cause this failure. Which version are you using? I can try to add some code to work around early versions.,work fine maybe early version cause failure version try add code work around early,issue,negative,positive,neutral,neutral,positive,positive
402037269,"@ppwwyyxx 
new error:
```
root@997991b14e71:/data/home/users/ccc/projects/tensorpack/examples/DoReFa-Net# ./alexnet-dorefa.py --dorefa 1,2,6 --data /data/data/ImageNetOrigin --gpu 4,5,6,7
/root/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[0703 07:15:20 @logger.py:109] WRN Log directory train_log/alexnet-dorefa-1,2,6 exists! Use 'd' to delete it. 
[0703 07:15:20 @logger.py:112] WRN If you're resuming from a previous run, you can choose to keep it.
Press any other key to exit. 
Select Action: k (keep) / d (delete) / q (quit):d
[0703 07:15:22 @logger.py:74] Argv: ./alexnet-dorefa.py --dorefa 1,2,6 --data /data/data/ImageNetOrigin --gpu 4,5,6,7
[0703 07:15:22 @alexnet-dorefa.py:222] Batch per tower: 64
[0703 07:15:22 @fs.py:88] WRN Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.
[0703 07:15:23 @parallel.py:290] [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[0703 07:15:23 @ilsvrc.py:128] [ILSVRC12] Assuming directory /data/data/ImageNetOrigin/val has 'original' structure.
[0703 07:15:23 @training.py:51] [DataParallel] Training a model of 4 towers.
[0703 07:15:23 @interface.py:31] Automatically applying QueueInput on the DataFlow.
[0703 07:15:23 @interface.py:42] Automatically applying StagingInput on the DataFlow.
[0703 07:15:23 @input_source.py:195] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0703 07:15:23 @training.py:111] Building graph for training tower 0 on device /gpu:0 ...
[0703 07:15:23 @registry.py:121] conv0 input: [None, 3, 224, 224]
[0703 07:15:23 @registry.py:129] conv0 output: [None, 96, 54, 54]
[0703 07:15:23 @registry.py:121] conv1 input: [None, 96, 54, 54]
[0703 07:15:23 @alexnet-dorefa.py:79] Quantizing weight conv1/W
[0703 07:15:23 @registry.py:129] conv1 output: [None, 256, 54, 54]
[0703 07:15:23 @registry.py:121] pool1 input: [None, 256, 54, 54]
[0703 07:15:23 @registry.py:129] pool1 output: [None, 256, 27, 27]
[0703 07:15:23 @registry.py:121] conv2 input: [None, 256, 27, 27]
[0703 07:15:23 @alexnet-dorefa.py:79] Quantizing weight conv2/W
[0703 07:15:23 @registry.py:129] conv2 output: [None, 384, 27, 27]
[0703 07:15:23 @registry.py:121] pool2 input: [None, 384, 27, 27]
[0703 07:15:23 @registry.py:129] pool2 output: [None, 384, 14, 14]
[0703 07:15:23 @registry.py:121] conv3 input: [None, 384, 14, 14]
[0703 07:15:23 @alexnet-dorefa.py:79] Quantizing weight conv3/W
[0703 07:15:24 @registry.py:129] conv3 output: [None, 384, 14, 14]
[0703 07:15:24 @registry.py:121] conv4 input: [None, 384, 14, 14]
[0703 07:15:24 @alexnet-dorefa.py:79] Quantizing weight conv4/W
[0703 07:15:24 @registry.py:129] conv4 output: [None, 256, 14, 14]
[0703 07:15:24 @registry.py:121] pool4 input: [None, 256, 14, 14]
[0703 07:15:24 @registry.py:129] pool4 output: [None, 256, 6, 6]
[0703 07:15:24 @registry.py:121] fc0 input: [None, 256, 6, 6]
[0703 07:15:24 @alexnet-dorefa.py:79] Quantizing weight fc0/W
[0703 07:15:24 @registry.py:129] fc0 output: [None, 4096]
Traceback (most recent call last):
  File ""./alexnet-dorefa.py"", line 227, in <module>
    launch_train_with_config(config, SyncMultiGPUTrainerReplicated(nr_tower))
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/interface.py"", line 81, in launch_train_with_config
    model._build_graph_get_cost, model.get_optimizer)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 181, in wrapper
    return func(*args, **kwargs)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/tower.py"", line 172, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/trainers.py"", line 166, in _setup_graph
    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/graph_builder/training.py"", line 222, in build
    use_vs=[False] + [True] * (len(self.towers) - 1))
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/graph_builder/training.py"", line 118, in build_on_towers
    ret.append(func())
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/tower.py"", line 199, in get_grad_fn
    cost = get_cost_fn(*input.get_input_tensors())
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/tfutils/tower.py"", line 255, in __call__
    output = self._tower_fn(*args)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py"", line 235, in _build_graph_get_cost
    ret = self.build_graph(*inputs)
  File ""/data/home/users/ccc/projects/tensorpack/examples/DoReFa-Net/imagenet_utils.py"", line 181, in build_graph
    logits = self.get_logits(image)
  File ""./alexnet-dorefa.py"", line 122, in get_logits
    .BatchNorm('bnfc0')
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/models/linearwrap.py"", line 47, in layer_func
    ret = layer(name, self._t, *args, **kwargs)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/models/registry.py"", line 124, in wrapped_func
    outputs = func(*args, **actual_args)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/models/tflayer.py"", line 64, in decorated_func
    return func(inputs, **ret)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorpack/models/batch_norm.py"", line 176, in BatchNorm
    xn = layer.apply(inputs, training=training, scope=tf.get_variable_scope())
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 503, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 443, in __call__
    self.build(input_shapes[0])
  File ""/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py"", line 150, in build
    'to its original shape. Got input rank: ', ndim)
ValueError: ('Only 4D inputs are currently supported with fused batch norm. Consider reshaping the input to 4D and reshape the output back to its original shape. Got input rank: ', 2)
```
",new error root data conversion second argument float future float import log directory use delete previous run choose keep press key exit select action keep delete quit data batch per tower set fork one time assuming directory structure training model automatically automatically setting queue building graph training tower device input none output none input none weight output none pool input none pool output none input none weight output none pool input none pool output none input none weight output none input none weight output none pool input none pool output none input none weight output none recent call last file line module file line file line wrapper return file line input file line input file line build false true file line file line cost file line output file line ret file line image file line file line ret layer name file line file line return ret file line file line apply return file line file line build original shape got input rank currently fused batch norm consider input reshape output back original shape got input rank,issue,negative,negative,neutral,neutral,negative,negative
402036391,"@ppwwyyxx 
still this error
![image](https://user-images.githubusercontent.com/13804492/42204185-88560f44-7ed3-11e8-80fe-b3d424a7c4d2.png)
between two ""ls"" , I run the alex_dorefa.py the folder caffe_ilsvrc12 exists.",still error image two run folder,issue,negative,neutral,neutral,neutral,neutral,neutral
402035301,"@ppwwyyxx 
I have downloaded this and put it into the  ~/tensorpack_data/ilsvrc_metadata, but it seems that the code will delete what I have manually downloaded.",put code delete manually,issue,negative,neutral,neutral,neutral,neutral,neutral
402035096,You have a network failure or something that made the download fail. Please download it by yourself at http://dl.caffe.berkeleyvision.org/caffe_ilsvrc12.tar.gz,network failure something made fail please,issue,negative,negative,negative,negative,negative,negative
401883898,"https://github.com/tensorpack/tensorpack/commit/2a712afea3e4eaefbaa1b85be59e9c8eebf53455 should fix the issue.
The reason `MultiProcessMapData` is not cleaned when you call `del` is because there is a line of useless code holding reference to it. ",fix issue reason call line useless code holding reference,issue,negative,negative,negative,negative,negative,negative
401863807,Please upgrade tensorpack to github master. The change to use `tb.summary` was reverted last week,please upgrade master change use last week,issue,negative,neutral,neutral,neutral,neutral,neutral
401863496,"import_meta_graph should not be used to run a model. That's a very common tensorflow mistake: https://github.com/tensorpack/tensorpack/issues/790#issuecomment-396726606

http://tensorpack.readthedocs.io/tutorial/inference.html#inference-after-training",used run model common mistake,issue,negative,negative,negative,negative,negative,negative
401695018,"@ppwwyyxx 
if I want to train the model from float-32 full-precision model, what param should the ""args.load"" be? 
alexnet.npz?",want train model model param,issue,negative,neutral,neutral,neutral,neutral,neutral
401681888,It's not expected to be run like that. And there is a small error in the `__main__` part of the file  (http://tensorpack.readthedocs.io/tutorial/dataflow.html#use-dataflow-outside-tensorpack).,run like small error part file,issue,negative,negative,negative,negative,negative,negative
401219163,"You're not answering my question.. 

The output of each block is not quantized. The first layer in each block is batchnorm followed by quantization, not a conv layer.",question output block first layer block quantization layer,issue,negative,positive,positive,positive,positive,positive
401218237,"@ppwwyyxx 
here : https://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/resnet-dorefa.py#L60
output of  get_stem_full function is not quanitzed, so here the return of function resblock: https://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/resnet-dorefa.py#L80
 and the return of function group:
https://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/resnet-dorefa.py#L88
 are not quantized. So the group output is the input of the next group--- But the first layer in a group is a conv layer.
",output function return function return function group group output input next group first layer group layer,issue,negative,positive,positive,positive,positive,positive
401216189,"> this can be the input of the first conv layer of the next resblock, right?

If so, could you point out in code __which__ conv layer?",input first layer next right could point code layer,issue,negative,positive,positive,positive,positive,positive
401215723,"@ppwwyyxx 
in [here](https://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/resnet-dorefa.py#L87) it seems that there is no quantization on the activations(or called ""outputs"") of the element-wise-adding operation in resnet,  but this can be the input of the first conv layer of the next resblock, right?",quantization operation input first layer next right,issue,negative,positive,positive,positive,positive,positive
401102161,"Thank you for the quick reply! What should I use instead? I could make the change in the repo too. 

Edit: Nevermind, thanks!",thank quick reply use instead could make change edit thanks,issue,positive,positive,positive,positive,positive,positive
401092817,The goal of the paper is to make the input of every conv layer quantized.,goal paper make input every layer,issue,negative,neutral,neutral,neutral,neutral,neutral
400988963,"@ppwwyyxx 
two questions:
1. in [here](https://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/alexnet-dorefa.py#L101) why did you quantize the activations after batchnorm,  but not before batchnorm?
2.  in [here](https://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/resnet-dorefa.py#L87), it seems that there is no quantization on the activations(or called ""outputs"") of the element-wise-adding operation in resnet, why?

thanks a lot!",two quantize quantization operation thanks lot,issue,negative,positive,positive,positive,positive,positive
400915651,"I changed the preprocessing of the model in 18b19d6dec901415ab8877a1405145cb5dda919b, but forgot to update the `run_image()` function accordingly.
I'll push a fix. You can also evaluate the model with this diff:
```diff
diff --git i/examples/DoReFa-Net/alexnet-dorefa.py w/examples/DoReFa-Net/alexnet-dorefa.py        
index 95a1e84..1b30f1b 100755                    
--- i/examples/DoReFa-Net/alexnet-dorefa.py      
+++ w/examples/DoReFa-Net/alexnet-dorefa.py      
@@ -220,6 +220,13 @@ if __name__ == '__main__':  
     else:                                       
         BITW, BITA, BITG = map(int, dorefa)     
                                                 
+    from imagenet_utils import eval_on_ILSVRC12 
+    from tensorpack.tfutils.sessinit import get_model_loader                                     
+    BATCH_SIZE = 128                            
+    eval_on_ILSVRC12(Model(), get_model_loader(args.load),                                       
+            get_data('val'))                    
+    import sys; sys.exit()                      
+                                                
     if args.gpu:                                
         os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu                                            
                                      
```",model forgot update function accordingly push fix also evaluate model git index ae else map import import model import,issue,negative,neutral,neutral,neutral,neutral,neutral
400384985,"```
╰─$python3 -c 'import tensorflow as tf; tf.placeholder(tf.float32, (None, [32], 32, 1), ""abc"")'
Traceback (most recent call last):
  File ""/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 141, in make_shape                                                                                  
    shape = tensor_shape.as_shape(v)
  File ""/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 946, in as_shape                                                                           
    return TensorShape(shape)
  File ""/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 541, in __init__                                                                           
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 541, in <listcomp>                                                                         
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 482, in as_dimension                                                                       
    return Dimension(value)
  File ""/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 37, in __init__                                                                            
    self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'list'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1734, in placeholder                                                                                
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 4922, in placeholder                                                                            
    shape = _execute.make_shape(shape, ""shape"")
  File ""/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 143, in make_shape                                                                                  
    raise TypeError(""Error converting %s to a TensorShape: %s."" % (arg_name, e))
TypeError: Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'list'. 
```
I seriously doubt that..",python none recent call last file line shape file line return shape file line file line file line return dimension value file line value argument must string object number handling exception another exception recent call last file string line module file line return file line shape shape shape file line raise error converting error converting shape argument must string object number seriously doubt,issue,negative,negative,neutral,neutral,negative,negative
400384305,"This is by design of tensorflow. `sess.run([a, b])` is not equivalent to `sess.run(a); sess.run(b)`. You can run the entire list instead of running them separately.",design equivalent run entire list instead running separately,issue,negative,neutral,neutral,neutral,neutral,neutral
400336703,"Yes, I just tried the example and it runs fine, and `cfg.crop_size` is an int, I'll check my other codes.
Thanks a lot.",yes tried example fine check thanks lot,issue,positive,positive,positive,positive,positive,positive
400287828,"I've found out that it runs get_data() for each self.hooked_sess.run(op).
How can I made Dataflow to be able to control trigger of new epoch? or can you show example of Dataflow like Callback.",found made able control trigger new epoch show example like,issue,negative,positive,positive,positive,positive,positive
400280394,"I've got same bad results - 

I didn't use simple trainer. 
I use trainer with many optimizer ops:

```
class AnalyzeTrainer(TowerTrainer):
    def __init__(self, input, model, period_class=1, period_pdf=1, period_ohe_l1=1):
        """"""
        Args:
            input (InputSource):
            model (AnalyzeModelDesc):
        """"""
        super(AnalyzeTrainer, self).__init__()
        assert isinstance(model, AnalyzeModelDesc), model
        assert isinstance(model, AnalyzeModelDesc), model
        assert min(period_class, period_pdf, period_ohe_l1) == 1
        self.period_class = period_class
        self.period_pdf = period_pdf
        self.period_ohe_l1 = period_ohe_l1
        self.period_ohe_l2 = period_ohe_l2

        inputs_desc = model.get_inputs_desc()
        # Setup input
        cbs = input.setup(inputs_desc)
        self.register_callback(cbs)

        self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc)
        with TowerContext('', is_training=True):
            self.tower_func(*input.get_input_tensors())
        opt = model.get_optimizer()

        # Define the training iteration
        with tf.name_scope('optimize'):
                self.train_minimize_class = opt.minimize(
                    model.analyze_class_cost,
                    var_list=model.varlist_analyze,
                    name='minimize_analyze_class_cost'
                )

                self.train_minimize_pdf = opt.minimize(
                    model.analyze_pdf_cost,
                    var_list=model.varlist_analyze,
                    name='minimize_analyze_pdf_cost'
                )

                self.train_minimize_ohe_l1 = list()
                for tensor in model.ohe_cost_l1:
                    self.train_minimize_ohe_l1.append(
                        opt.minimize(
                            tensor,
                            var_list=model.varlist_analyze,
                            name='minimize_ohe_l1/' + get_feature_name(tensor.name)
                        )
                    )

    def run_step(self):
            run_op = list()
            if self.global_step % self.period_class == 0:
                run_op.append(self.train_minimize_class)
            if self.global_step % self.period_pdf == 0:
                run_op.append(self.train_minimize_pdf)
            if self.global_step % self.period_ohe_l1 == 0:
                run_op.extend(self.train_minimize_ohe_l1)

            for op in run_op:
                self.hooked_sess.run(op)

```",got bad use simple trainer use trainer many class self input model input model super self assert model model assert model model assert min setup input opt define training iteration list tensor tensor self list,issue,negative,positive,neutral,neutral,positive,positive
400239684,"Closing now as this is unrelated to the original issue. If you got accuracy that's very different from the readme, you can open an issue following the issue template so we can investigate.",unrelated original issue got accuracy different open issue following issue template investigate,issue,negative,positive,neutral,neutral,positive,positive
400236665,"The callbacks look fine.
You can try this code below, modified from the mnist example. It is based on my understanding of what you're doing, and this code works as I expected:
```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import argparse
import numpy as np



# Just import everything into current namespace
from tensorpack import *
from tensorpack.tfutils import summary
from tensorpack.dataflow import dataset

IMAGE_SIZE = 28


class Model(ModelDesc):
    def inputs(self):
        """"""
        Define all the inputs (with type, shape, name) that the graph will need.
        """"""
        return [tf.placeholder(tf.float32, (None, IMAGE_SIZE, IMAGE_SIZE), 'input'),
                tf.placeholder(tf.int32, (None,), 'label')]

    def build_graph(self, image, label):
        """"""This function should build the model which takes the input variables
        and return cost at the end""""""

        # In tensorflow, inputs to convolution function are assumed to be
        # NHWC. Add a single channel here.
        image = tf.expand_dims(image, 3)

        image = image * 2 - 1   # center the pixels values at zero
        # The context manager `argscope` sets the default option for all the layers under
        # this context. Here we use 32 channel convolution with shape 3x3
        with argscope(Conv2D, kernel_size=3, activation=tf.nn.relu, filters=32):
            logits = (LinearWrap(image)
                      .Conv2D('conv0')
                      .MaxPooling('pool0', 2)
                      .Conv2D('conv1')
                      .Conv2D('conv2')
                      .MaxPooling('pool1', 2)
                      .Conv2D('conv3')
                      .FullyConnected('fc0', 512, activation=tf.nn.relu)
                      .Dropout('dropout', rate=0.5)
                      .FullyConnected('fc1', 10, activation=tf.identity)())

        tf.nn.softmax(logits, name='prob')   # a Bx10 with probabilities

        # a vector of length B with loss of each sample
        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
        cost = tf.reduce_mean(cost, name='cross_entropy_loss')  # the average cross-entropy loss

        return cost

    def optimizer(self):
        return tf.train.AdamOptimizer(0.01)



def get_config():
    def gen():
        cnt = 0
        while True:
            cnt += 1
            print('------------', cnt)
            for k in range(1024):
                yield [np.random.rand(28, 28), 0]
    dataset_train = DataFromGenerator(gen)
    steps_per_epoch = 10
    return TrainConfig(
        model=Model(),
        data=BatchQueueInput(dataset_train, 1024),  # the DataFlow instance for training
        callbacks=[ ],
        steps_per_epoch=steps_per_epoch,
        max_epoch=100,
    )


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.')
    parser.add_argument('--load', help='load model')
    args = parser.parse_args()
    if args.gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu

    # automatically setup the directory train_log/mnist-convnet for logging
    logger.auto_set_dir()

    config = get_config()
    if args.load:
        config.session_init = SaverRestore(args.load)
    # SimpleTrainer is slow, this is just a demo.
    # You can use QueueInputTrainer instead
    launch_train_with_config(config, SimpleTrainer())
```",look fine try code example based understanding code work python python import o import import import everything current import import summary import class model self define type shape name graph return none none self image label function build model input return cost end convolution function assumed add single channel image image image image center zero context manager default option context use channel convolution shape image vector length loss sample cost cost cost average loss return cost self return gen true print range yield gen return instance training parser list use load model automatically setup directory logging slow use instead,issue,negative,positive,neutral,neutral,positive,positive
400231971,"```
                'callbacks': [
                    ConfusionMatrixAfterEpoch('prediction/predicted_class', 'InputBlock/label/label', label_list),
                    ModelSaver(max_to_keep=3, keep_checkpoint_every_n_hours=6),
                    AnalyzeTensors(
                        save_tensors=['^InputBlock/email_features$'],  # ['^InputBlock/analyze/mix_tensor'],
                        save_ids=['^InputBlock/label/customer$'],  # ['^InputBlock/analyze/mix_tensor'],
                        save_prob=['^prediction/predicted_prob$'],  # ['^InputBlock/analyze/mix_tensor'],
                        init_variables=['analyze'],
                        features_names=get_features(select='email_history')
                    ),
                    ScheduledHyperParamSetter(
                        'optimizer/learning_rate',
                        [
                            (a.analyze['drop0'], a.analyze['lr0'])
                        ]
                    )
                ]

```


```
class AfterTrainEpoch(Callback):

    def _setup_graph(self):
        self.train_tower = self.trainer.tower_func.towers[0]
        self.outputs = [self.train_tower.get_tensor(name) for name in self.get_fetches()]

    def _before_train(self):
        pass

    def _before_epoch(self):
        self._before_train_epoch()

    def _before_train_epoch(self):
        pass

    def _before_run(self, _):
        return tf.train.SessionRunArgs(fetches=self.outputs)

    def _after_run(self, _, run_values):
        self.on_fetches(run_values.results)

    def on_fetches(self, outputs):
        self._on_fetches(outputs)

    def _on_fetches(self, outputs):
        raise NotImplementedError()

    def _trigger_epoch(self):
        ret = self._after_train_epoch()
        if ret is None:
            return
        for k, v in six.iteritems(ret):
            try:
                v = float(v)
            except ValueError:
                logger.warn(""{} returns a non-scalar statistics!"".format(type(self).__name__))
                continue
            else:
                self.trainer.monitors.put_scalar(k, v)

    def _after_train_epoch(self):
        pass

    def get_fetches(self):
        try:
            ret = self._get_fetches()
        except NotImplementedError:
            logger.warn(""Inferencer._get_output_tensors was deprecated and renamed to _get_fetches"")
            ret = self._get_output_tensors()

        return [get_op_tensor_name(n)[1] for n in ret]

    def _get_fetches(self):
        raise NotImplementedError()

    def _get_output_tensors(self):
        pass
```


```
class AnalyzeTensors(AfterTrainEpoch):
    """"""
    Save calculated tensors and initialize feature variables
    """"""

    def __init__(self,
                 save_tensors: list,
                 save_ids: list,
                 save_prob: list,
                 init_variables: list,
                 features_names: list
                 ):
        """"""
        Args:
            save_tensors(str): pattern of name tensor to save into file.
            init_variables(str): pattern of tensors to init.
        """"""
        super().__init__()
        self.save_ids = save_ids
        self.save_prob = save_prob
        self.save_tensors = save_tensors
        self.init_variables = init_variables
        self.features_names = features_names
        self.tensor_values_output_list = list()
        self.prob_output_list = list()
        self.tensor_values_output = None
        self.save_tensors_list = None

    def _setup_graph(self):
        G = tf.get_default_graph()
        self.save_tensors_list = list()
        patterns_list = list()
        patterns_list.extend(self.save_tensors)
        patterns_list.extend(self.save_ids)
        patterns_list.extend(self.save_prob)

        for pattern in patterns_list:
            self.save_tensors_list.extend([
                x.name for x in G.get_operations() if re.search(pattern, x.name)
            ])

        self.ids_output_list = list()

        init_variables_list = list()
        for pattern in self.init_variables:
            init_variables_list.extend([
                x for x in tf.trainable_variables() if re.search(pattern, x.name)
            ])
        self.init_op = tf.variables_initializer(init_variables_list, name='init_analyze_vars')

        super()._setup_graph()

    def _get_fetches(self):
        return self.save_tensors_list

    def _on_fetches(self, outputs):
        self.tensor_values_output, self.ids_output, self.prob_output = outputs

    def _trigger_epoch(self):
        self.tensor_values_output_list.append(self.tensor_values_output)
        self.ids_output_list.append(self.ids_output)
        self.prob_output_list.append(self.prob_output)
        self.init_op.run()

    def _after_train(self):
        a = GetConfig()
        index_values = np.concatenate(self.ids_output_list, axis=0)
        prob_values = np.concatenate(self.prob_output_list, axis=0)
        prob_values = pd.DataFrame(
            prob_values,
            index=index_values.flatten(),
            columns=[""prob_{:0>4d}"".format(i) for i in range(a.hm_labels)],
            dtype=np.float32)
        all_values = np.concatenate(tuple(self.tensor_values_output_list), axis=0)
        all_values = pd.DataFrame(all_values, index=index_values.flatten(), columns=self.features_names)
        all_values = pd.concat([all_values, prob_values], axis=1)
        folder_to_save = a.folders['analyze']  #  + ""{}/"".format(self.analyze_id)
        create_folders([folder_to_save])
        table_file = folder_to_save + a.analyze['prediction_file']
        check_files([table_file])
        with pd.HDFStore(table_file, chunkshape='auto', complevel=9, complib='blosc', mode='w') as store:
            store.append(a.analyze['res_table_name'], all_values)
        print(""result is saved to \""{}\"""".format(table_file))
```",class self name name self pas self self pas self return self self self raise self ret ret none return ret try float except statistic type self continue else self pas self try ret except ret return ret self raise self pas class save calculated initialize feature self list list list list list pattern name tensor save file pattern super list list none none self list list pattern pattern list list pattern pattern super self return self self self range store print result saved,issue,positive,positive,positive,positive,positive,positive
400229923,"1. What callbacks do you use for training? 
2. Did you write any callbacks that contains things like `sess.run` or `tensor.eval()`? If yes, what do they look like?",use training write like yes look like,issue,positive,neutral,neutral,neutral,neutral,neutral
400228434,All this about to change datapoints values of different epochs. I feed different values in different epochs. ,change different feed different different,issue,negative,neutral,neutral,neutral,neutral,neutral
400223150,"my Dataflow generate 10240 datapoints in ""for"" cycle - it should be epoch.
it prints "">>>>>>>>>>>>>>>>>>>>>>>>><><><><>< -------------------- epoch_counter"" before this ""for"" cycle. So I see this print each time it should start _new_ epoch and generate 10240 dataponts.
But I see this print many (41) times _inside_ one epoch. 
410624 - this is from confusion matrix - it collects all instances of current epoch (each step generates batch_size of results - this callback collects them after each step and print confusion matrix after epoch for all results in current epoch) and shows them in a table - so sum of them equals number of all instances.
Where is place to stop batches and finish an epoch?
Why BatchQueueInput gathers much more datapoints?",generate cycle epoch cycle see print time start epoch generate see print many time one epoch confusion matrix current epoch step step print confusion matrix epoch current epoch table sum number place stop finish epoch much,issue,negative,positive,positive,positive,positive,positive
400216239,"I still don't fully understand what is the issue. Now my understanding is:
1. Sounds like it has nothing to do with model restore, is that correct? 
2. You're saying that `BatchQueueInput` should give you 1024 instances per step and 10 steps per epoch -- this I agree. But how do you know it did not do so? What code prints the number 410624 and how does this number relate to what `BatchQueueInput` has done?",still fully understand issue understanding like nothing model restore correct saying give per step per epoch agree know code number number relate done,issue,positive,neutral,neutral,neutral,neutral,neutral
400211855,"I don't mind moving the implementation of `TFRecordData` into `load()`.

The only benefit (compared to implmenting `TFRecordData` in `load()`) is that `TFRecordData` is for reading data with a user-defined decoder. i.e., we have one API that reads an arbitrary tfrecord, and one API that's a dataflow deserializer, and they share code. Here they are named `TFRecordData` and `TFRecordSerializer.load` respectively.",mind moving implementation load benefit load reading data one arbitrary one share code respectively,issue,positive,negative,neutral,neutral,negative,negative
400201181,BatchQueueInput should make batches by 1024 instances 10 times per epoch only - but result is 401 times per epoch.,make time per epoch result time per epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
400199306,"I print ```>>>>>>>>>>>>>>>>>>>>>>>>><><><><>< -------------------- epoch_N ```
in get_data() of Dataflow each time it should start new epoch.
Settings are:
epoch size is 10240
batch size is 1024
so epoch is 10 batches by 1024 instances.
but I have got epoch  = 410624 instances = 401 times by 1024 instances (1024 batch size)",print time start new epoch epoch size batch size epoch got epoch time batch size,issue,negative,positive,positive,positive,positive,positive
400196012,"I can't understand what is your question. Could you simplify and clarify?
It seems that the number ""410624"" in the log is not what you expected, but I have no idea what code prints that number or how is that related to tensorpack or dataflow.

It seems that the part of logs that contain "">>>>>>>>>>>>>>>>>>>>>>>>><><><><>< -------------------- ."" is also different from what you expected but I have no idea what you're expecting.",ca understand question could simplify clarify number log idea code number related part contain also different idea,issue,negative,neutral,neutral,neutral,neutral,neutral
400192030,"I do not see any gain in having a `load` function which simply wraps/nests
https://github.com/tensorpack/tensorpack/blob/d5297c04578133fdaf14fe51b36919fd7da1ea76/tensorpack/dataflow/serialize.py#L111-L112

over a separate `TFRecordWriter` and `TFRecordData` object.",see gain load function simply separate object,issue,positive,neutral,neutral,neutral,neutral,neutral
400149568,"After some investigation and communication on gitter, it turns out that it is an expected behavior that not all files are cached.
Since each worker created by `PrefetchDataZMQ` randomly produces images independently, when the number of workers is larger than one, there will be duplication and therefore not all files are read.
To ensure a whole dataset is used, use indices + `MultiProcessMapData` (https://github.com/tensorpack/tensorpack/pull/794#issuecomment-398429188)

Some more details:
Assuming there are K workers, each worker will create a local shuffle and produce roughly the first N/K images in the first epoch. The probability for a particular worker to produce a particular image is 1/K.
The probability an image will not appear in the first epoch will be `[(K-1)/K]^K`.
When K=40, this number is 0.36 so the cache will be 64% full. Considering the prefetching the cache should be a little bit more than 64% full.",investigation communication turn behavior since worker randomly independently number one duplication therefore read ensure whole used use index assuming worker create local shuffle produce roughly first first epoch probability particular worker produce particular image probability image appear first epoch number cache full considering cache little bit full,issue,positive,positive,positive,positive,positive,positive
400069545,"There is not much to be shared among them -- and that's what I expected. There is not much duplication among them. 
Interface-wise they are different: for save, they have different options. Even ""filename"" is not a universal argument because LMDB can take a directory. And `overwrite=True` should be the user's responsibility. Would you call it a good design for `np.save(filename, overwrite=True)` or `pickle.dump(obj, file, overwrite=True)`? For load, they have different options (e.g. shuffle).
Implementation-wise the only code duplication is setting up the dataflow and progress bar, which is about 6 lines.
See #802. It's WIP but I think this design is much simpler.",much among much duplication among different save different even universal argument take directory user responsibility would call good design file load different shuffle code duplication setting progress bar see think design much simpler,issue,positive,positive,positive,positive,positive,positive
400015957,"If the example runs fine it's probably unrelated to tensorpack.
Please check that `cfg.crop_size` is an integer.",example fine probably unrelated please check integer,issue,negative,positive,positive,positive,positive,positive
399948218,"As I am absolutly convinced that a function-based interface for serialization is not an improvement, I implemented your second version:

```python
ds = SomeData()
NumpyDataSerializer('mydataset.npz').serialize(ds)
ds2 = NumpyDataSerializer('mydataset.npz').load()
```

But the implementation is really awkward. There is not so much which can be shared between `<Some>DataSerializer.serialize()` and `<Some>DataSerializer.load()`. The only thing is the filename.
Currently, the `load` method simply returns the `<Some>DataReader()` object. This really hurts: just returning another object.

Writing a dataflow to disk and reading a dataflow from disk are two very different things and require different objects. In Python preferring classes over function keeps the implementation easy and avoids code duplication. 

The more time I spend thinking about a good design, the more I prefer and really start to like my previous version from a1892b5fe0606437bd0a5f9d83d7c7f2f6239628. I suggest doing a rollback to this commit.

In this commit, there was **no** code duplication and no nesting of different objects, just

```python
ds = SomeData()
NumpyDataWriter(ds, 'mydataset.npz').serialize()
# I can live with ...
NumpyDataWriter('mydataset.npz').serialize(ds)
# ... as well, which might support multiple incoming ds in the future
# loading some data
ds2 = NumpyDataReader('mydataset.npz')
```

This looks pretty straightforward and it is something I can easily remember without having a look at the tensorpack docs (a very rare thing compare to TestDataSpeed, dump_to_something).

> Going from the traditional way to creating one class for each serializer/deserializer is too far a step

I think it is exactly the opposite. Think of a base-class which provides logic to prevent overriding files, verifying dataflows.

Rollback to a1892b5fe0606437bd0a5f9d83d7c7f2f6239628?
",convinced interface serialization improvement second version python implementation really awkward much thing currently load method simply object really another object writing disk reading disk two different require different python class function implementation easy code duplication time spend thinking good design prefer really start like previous version suggest rollback commit commit code duplication different python live well might support multiple incoming future loading data pretty straightforward something easily remember without look rare thing compare going traditional way one class far step think exactly opposite think logic prevent rollback,issue,positive,positive,positive,positive,positive,positive
399656483,Different formats may support different options (like write_frequency in lmdb and the data path for hdf5) so it's probably impossible to share the same function and dispatch based on the filename.,different may support different like data path probably impossible share function dispatch based,issue,positive,negative,negative,negative,negative,negative
399655285,"Then
ˋˋˋpython
DataflowSerializer.save(ds, ""db.lmdb"") # same as np.save
DataflowSerializer.load(""db.lmdb"")
ˋˋˋ
would be most straightforward depending on the actual file extension (""lmdb,npz,tfrecord,h5"") or eventually use `.save(ds, filename, format=""lmdb"", allow_overwrite=False)`.
I do not want to rewrite it again and again.
Let me know this sounds ok for you. 

There is one issue though: hdf5 requires a datapath argument. Tightly couple all functionalities into a single class/function look a bit weird. But I guess for the first version it would be ok.

",would straightforward depending actual file extension eventually use want rewrite let know one issue though argument tightly couple single look bit weird guess first version would,issue,negative,negative,neutral,neutral,negative,negative
399557252,"About names:
`LMDBDataReader` is not a better name than `LMDBDataPoint` because this simpler name sounds too general and may become misleading given that the class does not read __any__ LMDB. `LMDBData`, `LMDBDataDecoder` and `LMDBDataPoint` are already a bit confusing but I'm afraid that `LMDBDataReader` makes it worse. Same for `NumpyDataReader`, and I should add a `decoder=` option in `TFRecordData`. 

I think the names should at least convey the message that it reads some tensorpack-specific thing. But I also like the correspondence between `XXReader` and `XXWriter` because the reader can ONLY reads something dumped by the writer.

Taking a step back and look at the whole problem, these functions are just doing serialization-deserialization of dataflow. The traditional way of doing serialization/deserialization in python is to have `.load` and `.dump` (or  `getstate/setstate`, `tostring/fromstring`) defined in the class and users are never expected to understand the format at all. __The only difference__ here is that DataFlow needs to be serialized to multiple formats.

Going from the traditional way to creating one class for each serializer/deserializer is too far a step. I've been talking without giving any actual suggestions. But now I would propose the following interface:
```python
DataFlowSerializer.to_numpy(ds, filename)
DataFlowSerializer.from_numpy(filename)
DataFlowSerializer.to_lmdb(ds, filename, options)
....
```

or 
```python
NumpySerializer.dump(ds, filename)
NumpySerializer.load(filename)
LMDBSerializer.dump(ds, filename, options)
...
```

The benefits are:
1. They are more tightly paired (inside one namespace now, just like the traditional dump/load).
2. It's clear that they are serializers, i.e. you cannot use a LMDB deserializer to load an arbitrary lmdb file.

This is only about designing the interface and names. Implementation-wise the progress bar may still be shared.",better name simpler name general may become misleading given class read already bit afraid worse add option think least convey message thing also like correspondence reader something writer taking step back look whole problem traditional way python defined class never understand format need multiple going traditional way one class far step talking without giving actual would propose following interface python python tightly paired inside one like traditional clear use load arbitrary file designing interface progress bar may still,issue,negative,negative,neutral,neutral,negative,negative
399535977,"About interface:
The concept of writer exists in many libraries because there you can create a writer and call `write(bytes)` multiple times with different data -- that actually corresponds to ""puts"" here. That's why I was uncomfortable with the interface.

To match common terminology of writer, the shared public interface would be `.write(datapoint)` and `.close()` and possibly also a `.dump(dataflow)` which closes itself in the end. 

About function vs class:
It looks nice when they are sharing interfaces but I think `LMDBWriter(filename).dump(dataflow)` may not look as simple as a single function call like `LMDBWriter.dump(filename, dataflow)` or even `LMDBReader.dump(filename, dataflow)`.
",interface concept writer many create writer call write multiple time different data actually uncomfortable interface match common terminology writer public interface would possibly also end function class nice think may look simple single function call like even,issue,positive,positive,neutral,neutral,positive,positive
399522918,The accuracy should be similar to those in the readme. The code you added did nothing.,accuracy similar code added nothing,issue,negative,neutral,neutral,neutral,neutral,neutral
399381981,"Thanks very much.

And, I find some questions:
old:
```python
class Model(ImageNetModel):
    weight_decay = 5e-6
    weight_decay_pattern = 'fc.*/W'

    def get_logits(self, image):
        if BITW == 't':
            fw, fa, fg = get_dorefa(32, 32, 32)
            fw = ternarize
        else:
            fw, fa, fg = get_dorefa(BITW, BITA, BITG)
    #....
```

I found the accuracy to be low. So I guess there may be some problems. I add some code:
```python
class Model(ImageNetModel):
    weight_decay = 5e-6
    weight_decay_pattern = 'fc.*/W'

    def inputs(self):
        return [tf.placeholder(tf.float32, [None, 224, 224, 3], 'input'),
                tf.placeholder(tf.int32, [None], 'label')]

    def get_logits(self, image):
        if BITW == 't':
            fw, fa, fg = get_dorefa(32, 32, 32)
            fw = ternarize
        else:
            fw, fa, fg = get_dorefa(BITW, BITA, BITG)
    #...
```

The accuracy rate has changed. How about it?
",thanks much find old python class model self image fa else fa found accuracy low guess may add code python class model self return none none self image fa else fa accuracy rate,issue,negative,positive,positive,positive,positive,positive
399358245,"> ProcessTensors() doesn't instance list names of tensor is exist or not?

I don't understand what you mean. You can find its documentation at http://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.ProcessTensors

> Do I need use callbacks() in OfflinePredictor?

As you can see from the documentation http://tensorpack.readthedocs.io/modules/predict.html, there is no such argument. Callbacks are only for training, and it is supposed to be set in `TrainConfig`.",instance list tensor exist understand mean find documentation need use see documentation argument training supposed set,issue,negative,negative,negative,negative,negative,negative
399356564,"`ProcessTensors()` doesn't instance list names of tensor is exist or not?
Do I need use `callbacks()` in `OfflinePredictor`?",instance list tensor exist need use,issue,negative,neutral,neutral,neutral,neutral,neutral
399340765,"Just in case you expected a response from my side:

> It returns a buffer that works like bytes most of the time.

Then I found exactly the situation where this does not hold.

I do not think this increases the complexity. All these writers are following the same interface. It is a matter of consistency: Adding `dump_npy` and `dump_hdf5` just feels wrong.

Regarding the naming conventions: I am not strong about the exact naming, I just followed `TestDataSpeed`, which could be `test_data_speed(ds)` as well for the same argument.

For sure, `dump_lmdb` and `LMDBDataPoint` is unintuitive when compared to `LMDBDataWriter` and `LMDBDataReader`. And I don't think `read_lmdb(path)` like `dump_lmdb` is a good choice either.",case response side buffer work like time found exactly situation hold think complexity following interface matter consistency wrong regarding naming strong exact naming could well argument sure unintuitive think path like good choice either,issue,positive,positive,positive,positive,positive,positive
399321794,"There are `input_names` and `output_names` given to the PredictConfig, where you can define what do you want to feed to and fetch from the graph.
See http://tensorpack.readthedocs.io/modules/predict.html#tensorpack.predict.PredictConfig",given define want feed fetch graph see,issue,negative,neutral,neutral,neutral,neutral,neutral
399302071,"I use 
```python
            logits = (LinearWrap(image)
                .Conv2D('conv0', 96, 12, stride=4, padding='VALID')
                .print_tensor()  # conv0/output
                .apply(activate)
                #.......
```
got name of tensor. But I don't know that where add to ProcessTensor() .
Is it add to `OfflinePredictor`?",use python image activate got name tensor know add add,issue,negative,neutral,neutral,neutral,neutral,neutral
399291067,"Thanks. 
How to  add the names of tensors to `OfflinePredictor` ? The intermediate-results is `conv0` , `conv1` ,`bn1`, `pool1`....? How should I establish contact with them?",thanks add pool establish contact,issue,negative,positive,positive,positive,positive,positive
399132119,"1. Fixed now.

2. If you want to do this during training, http://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training
If you want to do this after training, add the names of tensors you want to dump to `OfflinePredictor`. http://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training",fixed want training want training add want dump,issue,negative,positive,neutral,neutral,positive,positive
398930997,"Tensorpack simply reads them, and does nothing about caching. It's up to your operating system.",simply nothing operating system,issue,negative,neutral,neutral,neutral,neutral,neutral
398780490,"`ds = DataFlowProducesIndices()` will only exist in one process. It will not be forked.
`MultiProcessMapData` creates processes to only run the mapper function.",exist one process forked run mapper function,issue,negative,neutral,neutral,neutral,neutral,neutral
398681510,"Thank You!

We had a conversation with @bzamecnik and we would like to ask You how is it achieved without giving the generator the information about the processID (and total number of processes)...?

Because if, in the end, this information would be needed, Iam ready to code it for us all.",thank conversation would like ask without giving generator information total number end information would ready code u,issue,positive,positive,neutral,neutral,positive,positive
398644156,"For pyarrow dumps() does not return bytes. It returns a buffer that works like bytes most of the time.

There are many changes that I'll take time to digest but I like the direction to organize readers and writers. For now I don't think it's worth increasing the complexity of interfaces just to share a progress bar among writers. This for example makes the write_frequency option in lmdb become a bit unnatural to implement.
I still prefer using functions for writers (i.e. `dump_lmdb(ds, filename)` instead of `LMDBWriter(ds, filename).write()`) because I think you'll never need to use an instance of writer twice. So you'll always call it with one line of `LMDBWriter(ds, filename).write()` which can be achieved with a function call.",return buffer work like time many take time digest like direction organize think worth increasing complexity share progress bar among example option become bit unnatural implement still prefer instead think never need use instance writer twice always call one line function call,issue,positive,positive,positive,positive,positive,positive
398502535,"The fix is working, but benchmarking shows some bad results:

```python
import msgpack
import pyarrow as pa
import time

import numpy as np

buf = []
for _ in range(1000):
    buf.append(np.random.randn(256, 256, 3))


serialized = []
for dp in buf:
    serialized.append(pa.serialize(dp).to_buffer().to_pybytes())

s1 = 0
elapsed = time.time()
for enc in serialized:
    el = pa.deserialize(enc)
    s1 += el.sum()
elapsed = time.time() - elapsed
print('pyarrow.to_buffer().to_pybytes()', elapsed)

serialized = []
for dp in buf:
    serialized.append(pa.serialize(dp).to_buffer())

s2 = 0
elapsed = time.time()
for enc in serialized:
    el = pa.deserialize(enc)
    s2 += el.sum()
elapsed = time.time() - elapsed
print('pyarrow.to_buffer()', elapsed)

# elapsed = time.time()
# s3 = 0
# for dp in buf:
#     enc = msgpack.dumps(dp, use_bin_type=True)
#     el = msgpack.loads(enc, raw=False)
#     s3 += el.sum()
# elapsed = time.time() - elapsed
# print('msgpack.dumps()', elapsed)
```

gives
```
('pyarrow.to_buffer().to_pybytes()', 0.20410513877868652)
('pyarrow.to_buffer()', 0.12622380256652832)
```
And msgpack is for some reasons not working *in this benchmark*

But this slow-down might be ok, if it was just when writing. But the benchmark measures reading/decoding. I guess this can be hided as it depends on i/o speed.

## Reading from cached file

```python
from tensorpack.dataflow.base import DataFlow
from tensorpack.dataflow.dftools import LMDBDataWriter
from tensorpack.dataflow.format import LMDBDataReader
import os
import numpy as np
import time


def delete_file_if_exists(fn):
    try:
        os.remove(fn)
    except OSError:
        pass


class SeededFakeDataFlow(DataFlow):
    """"""docstring for SeededFakeDataFlow""""""
    def __init__(self, seed=42, size=32):
        super(SeededFakeDataFlow, self).__init__()
        self.seed = seed
        self._size = size
        self.cache = []

    def reset_state(self):
        np.random.seed(self.seed)
        for _ in range(self._size):
            label = np.random.randint(low=0, high=10)
            img = np.random.randn(256, 256, 3)
            self.cache.append([label, img])

    def size(self):
        return self._size

    def get_data(self):
        for dp in self.cache:
            yield dp


ds = SeededFakeDataFlow(size=1000)
LMDBDataWriter(ds, 'tmp.lmdb').serialize()

s = 0
ds = LMDBDataReader('tmp.lmdb', shuffle=False)
ds.reset_state()

elapsed = time.time()
for dp in ds.get_data():
    s += dp[1].sum()
elapsed = time.time() - elapsed
print('pyarrow.to_buffer().to_pybytes()', elapsed)
```

gives no significant difference when averaging multiple runs

```
# ('pyarrow.to_buffer().to_pybytes()', 0.3695650100708008)
# ('pyarrow.to_buffer()', 0.3536560535430908)
```",fix working bad python import import pa import time import range el print el print el print working might writing guess hided speed reading file python import import import import o import import time try except pas class self super self seed size self range label label size self return self yield print significant difference multiple,issue,negative,positive,neutral,neutral,positive,positive
398429188,"I was a bit misleading. The solution in https://github.com/tensorpack/tensorpack/issues/401#issuecomment-328963606 does not guarantee strictly non-duplication. It could have a small amount of duplication at the end of the epoch, because those data could possibly mix with the next epoch.

At that time there was no `MultiProcessMapData`. What I had in mind is to have a single dataflow that produces all indices (so it's fast enough and don't need parallelization), and then run the function that maps indices to data in parallel, such as:

```python
ds = DataFlowProducesIndices()

def mapper(i):
    return cv2.imread(filenames[i])

ds = MultiProcessMapData(ds, 10, mapper, strict=True)
```

I think this can achieve what you expect.",bit misleading solution guarantee strictly could small amount duplication end epoch data could possibly mix next epoch time mind single index fast enough need parallelization run function index data parallel python mapper return mapper think achieve expect,issue,negative,negative,neutral,neutral,negative,negative
398361724,"I agree with @ppwwyyxx that this is a big change to the existing object model.

So far a forking operation just clones the given data flow graph and runs multiple instances of each.

What we'd like to achieve is to fork a dataflow into multiple parallel map operations, each working on a different slice. I can imagine slicing on infinite generators (done by a cooperative way by predefined rule) - keep sample if `index % total_workers == worker_index`. For that to work efficiently the pipeline would need to be evaluated in a lazy way, so that we'd skip computation for skipped samples. This assumes that complexity of mapping function is uniform. Otherwise we'd need some queue to balance the computation.

Anyway I'm starting to think whether Dask with it's task graph being a full DAG would be more suitable for our input pipeline.",agree big change object model far operation given data flow graph multiple like achieve fork multiple parallel map working different slice imagine slicing infinite done way rule keep sample index work efficiently pipeline would need lazy way skip computation complexity function uniform otherwise need queue balance computation anyway starting think whether task graph full dag would suitable input pipeline,issue,positive,positive,neutral,neutral,positive,positive
398337276,"[Travis test fail](https://travis-ci.org/tensorpack/tensorpack/jobs/394018037) due to some pyarrow issues. I do not use pyarrow. A workaround is checking 
```python
if os.environ.get('CONTINUOUS_INTEGRATION', 'false') == 'false':
   do_test() # we are not in travis and can do the test
``` 
(but this smells very bad).

Here [`CONTINUOUS_INTEGRATION` is a default ENV-var](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables).

Any idea, why pyarrow creates such issues. MsgPack works fine here on my local machine and passes the unit test.",travis test fail due use python travis test bad default idea work fine local machine unit test,issue,negative,negative,negative,negative,negative,negative
398331508,"Thanks! 
I will just ask, for clarity - the base dataflow producing different set of indices needs to know to which process it belongs and the total number of processes.
Where, in the code, would You prefer to give it the information?

My take would be to use reset_data(adding the slicing params here) called from ZMQ that would propagate to the base dataflow.
(I did write the new classes to avoid cluttering reset_data with params, because I was afraid You would not like the solution, but maybe I was wrong :) )

(Edit: Iam reading the mentioned #401 again and again and Iam having the feeling, that maybe I just do not get it if the prefetch is enough or if the dataflows need to know which process out of what number of processes they are. ... If I shuffle the data and prefetch some number of them, is there any guarantee that on 1 epoch (defined by the dataset size), that it will see each datapoint only once?)",thanks ask clarity base different set index need know process total number code would prefer give information take would use slicing would propagate base write new class avoid afraid would like solution maybe wrong edit reading feeling maybe get enough need know process number shuffle data number guarantee epoch defined size see,issue,positive,negative,negative,negative,negative,negative
398320691,"The issue in the old `dump_dataflow_to_lmdb_old` has been

```python
txn.put(u'{}'.format(idx).encode('ascii'), dumps(dp))
```
instead of 
```python
txn.put(u'{:08}'.format(idx).encode('ascii'), dumps(dp))
```

see docs:

> LMDB is a tiny database with some excellent properties:
> - Ordered map interface (keys are always lexicographically sorted).",issue old python instead python see tiny excellent ordered map interface always lexicographically sorted,issue,positive,positive,positive,positive,positive,positive
398310054,"closed in favor of #797, which allows

```python
def get_val_data():
    if not exists(file):
        ds = ....
        NumpyDataWriter(ds, file).serialize()
    return NumpyDataReader(file)
```

or

```python
def get_val_data():
    if not exists(file):
        ds = ....
        HDF5DataWriter(ds, file).serialize()
    return HDF5DataReader(file)
```

or

```python
def get_val_data():
    if not exists(file):
        ds = ....
        LMDBDataWriter(ds, file).serialize()
    return LMDBDataReader(file)
```",closed favor python file file return file python file file return file python file file return file,issue,negative,negative,neutral,neutral,negative,negative
398187656,"> the validation data which can be generated on-the-fly is stored in some persistent way for a fair comparison of different models.

Since this is the main goal, I think it naturally expects a logic to tell ""do I need to generate data"" or ""do I need to load data"". Check the existence of file is one way to do that and I'd prefer the user to manage such high-level logic on ""what to do"".

To reduce code, what about the following?:
```python
def get_val_data():
    if not exists(file):
        ds = ....
        DataFlowFromNpz.dump(ds, file)
    return DataFlowFromNpz(file)
```",validation data persistent way fair comparison different since main goal think naturally logic tell need generate data need load data check existence file one way prefer user manage logic reduce code following python file file return file,issue,negative,positive,positive,positive,positive,positive
398126961,"I was hoping for such a discussion. It not clear to me, what the ideal solution would be. In fact, I don't even think npz is the best way?

I was thinking about something along the lines of:
```
ds = CacheData(ds, serialize_obj=NpzSerializer('cache.npz'))
```

I just put it in the current draft into ""CacheData"", since otherwise, I don't need CacheData at all. But using `DataFlowFromNpz`  would put the if-else logic into the user-code and needs to be repeated all the time. That's what I am currently doing all the time.
I just want to prevent me from doing the same stuff again and again.

The problem or discussion should be: How can we ensure, that the validation data which *can* be generated on-the-fly is stored in some persistent way for a fair comparison of different models.

My current way is:
- writing a `data_sampler.py` storing stuff into `train.lmdb` and `val.lmdb`.
- writing a `data_provider.py` doing stuff on-the-fly
- running `data_provider.py` for just the validation data and dump ot again into `val-augmentation-baked-in.lmdb`
- train network using  'train.lmdb`  and `val-augmentation-baked-in.lmdb`. Any suggestions how to reduce the code and not to just rewrite it? I am looking for a way to make my code easier to understand and I just want to hide the if-else logic somewhere deep in tensorpack.

I was just hoping, to reduce the constantly recurring python code. I cannot see it anymore.",discussion clear ideal solution would fact even think best way thinking something along put current draft since otherwise need would put logic need repeated time currently time want prevent stuff problem discussion ensure validation data persistent way fair comparison different current way writing stuff writing stuff running validation data dump train network reduce code rewrite looking way make code easier understand want hide logic somewhere deep reduce constantly recurring python code see,issue,positive,positive,positive,positive,positive,positive
398099007,Thanks for reporting! Forgot to update the initial learning rate when I refactor the code. Should be fixed now.,thanks forgot update initial learning rate code fixed,issue,negative,positive,positive,positive,positive,positive
398096705,"Why not a function that dumps a dataflow to npz, and a dataflow that reads from npz? Like `DataFlowFromNpz(filename), DataFlowFromNpz.dump(ds, filename)`. This way we add new stuff but it's easier to understand and composable. Also allows other formats this way. 

And you can then move the logic of `if file exists:   ... else: ...` to your code. Having to explain an if-else logic in docs may suggest that the function may be doing too many things.",function like way add new stuff easier understand also way move logic file else code explain logic may suggest function may many,issue,positive,positive,positive,positive,positive,positive
398087902,"These are too many changes and it effectively adds a new interface of DataFlow, and you are on the road towards reimplementing this new interface for __every__ existing DataFlow..

There are simpler solutions for your use case. See https://github.com/tensorpack/tensorpack/issues/401#issuecomment-328963606.
You just need to write one ""base"" dataflow that produces different set of indices in each process.",many effectively new interface road towards new interface simpler use case see need write one base different set index process,issue,negative,negative,neutral,neutral,negative,negative
398002757,"(And if You know how to make the checks pass, tell me, i would love to fix it of course! :) )",know make pas tell would love fix course,issue,positive,positive,positive,positive,positive,positive
397999446,"I know the `npz` can be quite large. A solution is to use two different `InferenceRunner` (small with CacheData and large with generated on-the-fly data).

I currently need to write an extra `ds.dump_to_lmdb` function to make fair comparisons between different architectures. The changes would make it much easier. I am open to change `npz` to `lmdb`. But lmdb is not a required dependency and lmdb is without compression (by design).",know quite large solution use two different small large data currently need write extra function make fair different would make much easier open change dependency without compression design,issue,positive,positive,positive,positive,positive,positive
397537442,"oh I'm confused .Never mind~
I get the events of a non-master process
I still find horovod_broadcast on tensorboard,and I choose one Session runs option ,the op disappear.
Now I can see the broadcast only run once at the variable init process
",oh confused get process still find choose one session option disappear see broadcast run variable process,issue,negative,negative,negative,negative,negative,negative
397532975,"it works~
I think it't right to have a default,but the code is useless to check the chief_only of callbacks on train/base.py. 

    is_chief = True
    """"""
    Whether this process is the chief worker in distributed training.
    Certain callbacks will only be run by chief worker.
    """"""
xxx

    if not self.is_chief and cb.chief_only:
        logger.warn(""Callback {} is chief-only, skipped."".format(str(cb)))
        return False
    else:
        self._callbacks.append(cb)
        return True",think right default code useless check true whether process chief worker distributed training certain run chief return false else return true,issue,positive,positive,neutral,neutral,positive,positive
397530897,"Oh right. I forgot that monitors are by default chief-only.
You can change that by provide `monitors=[TFEventWriter().set_chief_only(False), JSONWriter(), ScalarPrinter()]` in `TrainConfig`. Let me know if this is not the case.

I'll think about whether it's a good default or not.",oh right forgot default change provide false let know case think whether good default,issue,negative,positive,positive,positive,positive,positive
397530394,"^[[32m[0614 06:03:51 @logger.py:74]^[[0m Argv: distributed-horovod-mnist-convnet.py
^[[32m[0614 06:03:51 @fs.py:88]^[[0m ^[[5m^[[31mWRN^[[0m Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.
^[[32m[0614 06:03:52 @trainers.py:346]^[[0m Horovod local rank=3
^[[32m[0614 06:03:52 @interface.py:31]^[[0m Automatically applying QueueInput on the DataFlow.
^[[32m[0614 06:03:52 @input_source.py:193]^[[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
^[[32m[0614 06:03:52 @registry.py:121]^[[0m conv0 input: [None, 28, 28, 1]
^[[32m[0614 06:03:52 @registry.py:129]^[[0m conv0 output: [None, 28, 28, 32]
^[[32m[0614 06:03:52 @registry.py:121]^[[0m pool0 input: [None, 28, 28, 32]
^[[32m[0614 06:03:52 @registry.py:129]^[[0m pool0 output: [None, 14, 14, 32]
^[[32m[0614 06:03:52 @registry.py:121]^[[0m conv1 input: [None, 14, 14, 32]
^[[32m[0614 06:03:52 @registry.py:129]^[[0m conv1 output: [None, 14, 14, 32]
^[[32m[0614 06:03:52 @registry.py:121]^[[0m conv2 input: [None, 14, 14, 32]
^[[32m[0614 06:03:52 @registry.py:129]^[[0m conv2 output: [None, 14, 14, 32]
^[[32m[0614 06:03:52 @registry.py:121]^[[0m pool1 input: [None, 14, 14, 32]
^[[32m[0614 06:03:52 @registry.py:129]^[[0m pool1 output: [None, 7, 7, 32]
^[[32m[0614 06:03:52 @registry.py:121]^[[0m conv3 input: [None, 7, 7, 32]
^[[32m[0614 06:03:52 @registry.py:129]^[[0m conv3 output: [None, 7, 7, 32]
^[[32m[0614 06:03:52 @registry.py:121]^[[0m fc0 input: [None, 7, 7, 32]
^[[32m[0614 06:03:52 @registry.py:129]^[[0m fc0 output: [None, 512]
^[[32m[0614 06:03:52 @registry.py:121]^[[0m fc1 input: [None, 512]
^[[32m[0614 06:03:52 @registry.py:129]^[[0m fc1 output: [None, 10]
^[[32m[0614 06:03:52 @regularize.py:88]^[[0m regularize_cost() found 2 variables to regularize.
^[[32m[0614 06:03:52 @regularize.py:19]^[[0m The following tensors will be regularized: fc0/W:0, fc1/W:0
^[[32m[0614 06:03:52 @model_utils.py:63]^[[0m ^[[36mTrainable Variables:
^[[0mname       shape              dim
---------  --------------  ------

xxxx


Total #vars=12, #params=836522, size=3.19MB^[[0m
^[[32m[0614 06:03:52 @base.py:131]^[[0m ^[[5m^[[31mWRN^[[0m Callback MovingAverageSummary is chief-only, skipped.
^[[32m[0614 06:03:52 @base.py:131]^[[0m ^[[5m^[[31mWRN^[[0m Callback MergeAllSummaries_RunWithOp is chief-only, skipped.
**^[[32m[0614 06:03:52 @base.py:131]^[[0m ^[[5m^[[31mWRN^[[0m Callback TFEventWriter is chief-only, skipped.**
^[[32m[0614 06:03:52 @base.py:131]^[[0m ^[[5m^[[31mWRN^[[0m Callback JSONWriter is chief-only, skipped.
^[[32m[0614 06:03:52 @base.py:179]^[[0m Setup callbacks graph ...
^[[32m[0614 06:03:52 @base.py:197]^[[0m Creating the session ...
^[[32m[0614 06:03:58 @base.py:212]^[[0m Graph Finalized.
^[[32m[0614 06:03:58 @concurrency.py:37]^[[0m Starting EnqueueThread QueueInput/input_queue ...
^[[32m[0614 06:03:58 @graph.py:73]^[[0m Running Op horovod_broadcast/group_deps ...
^[[32m[0614 06:03:58 @common.py:774]^[[0m ^[[36mDataFlow Info:^[[0m
^[[32m[0614 06:04:16 @base.py:232]^[[0m Start Epoch 1 ...
^[[32m[0614 06:05:22 @base.py:242]^[[0m Epoch 1 (global_step 468) finished, time:1 minute 5 seconds.
^[[32m[0614 06:05:22 @base.py:232]^[[0m Start Epoch 2 ...
^[[32m[0614 06:08:16 @base.py:242]^[[0m Epoch 2 (global_step 936) finished, time:2 minutes 53 seconds.
^[[32m[0614 06:08:16 @base.py:246]^[[0m Training has finished!




I think the reason is in  **^[[32m[0614 06:03:52 @base.py:131]^[[0m ^[[5m^[[31mWRN^[[0m Callback TFEventWriter is chief-only, skipped.",set local automatically setting queue input none output none pool input none pool output none input none output none input none output none pool input none pool output none input none output none input none output none input none output none found regularize following shape dim total setup graph session graph starting running start epoch epoch finished time minute start epoch epoch finished time training finished think reason,issue,negative,positive,neutral,neutral,positive,positive
397528830,could you paste the log of a non-master process?,could paste log process,issue,negative,neutral,neutral,neutral,neutral,neutral
397528029,"You can look for ""WRN logger directory was not set. Ignore TFEventWriter."" in logs to see if the above is the reason.",look logger directory set ignore see reason,issue,negative,neutral,neutral,neutral,neutral,neutral
397527384,"> I need the graph to ensure horovod only broadcast the variable at the begin of train from root_rank.
Other ranks get the grad after ring allreduce and apply it on its own and begin next step

This is exactly what happened.

`dump_event` dumps stuff to tensorboard event files. tensorboard event files will only be activated in the processes where `logger.set_logger_dir` was set. Maybe that's the reason? My example only set the directory for the master process, but you can set a different logger_dir for each process.

If this does not solve the problem, could you paste the log of a non-master process?",need graph ensure broadcast variable begin train get grad ring apply begin next step exactly stuff event event set maybe reason example set directory master process set different process solve problem could paste log process,issue,negative,positive,neutral,neutral,positive,positive
397526765,"All rank would dump its graph .
I read the doc of horovod 
I got that only broadcast the variable at the begin of train from root_rank.
Other ranks get the grad after ring allreduce and apply it on its own and begin next step.

I need the graph to check if I'right",rank would dump graph read doc got broadcast variable begin train get grad ring apply begin next step need graph check,issue,negative,negative,negative,negative,negative,negative
396726606,"To load a model for training, conceptually what's needed to be loaded is some mathematical model plus the variables. MetaGraph (or graph) is not the right abstraction for it.

A metagraph contains the entire graph. For the one saved by tensorpack, it includes not only the mathematical model, but also the training settings: queues, data iterators, placeholders, evaluations, summaries, collections. Loading all of these settings from the last training means it will be very hard (if possible at all) to change them - essentially making the entire tensorpack not functional. In general as long as you want to make changes to the last training settings, it's going to be problematic to load a metagraph.

The tensorpack way to work around this problem is to encourage writing the mathematical model by python and load variables only.

Another way is the tensorflow-hub way: provide a ""cleaner"" version of metagraph (but not the actual graph used in training) that only contains the math, and use many hacks during `import_meta_graph` to avoid bad interaction between the imported graph and the existing graph. ",load model training conceptually loaded mathematical model plus graph right abstraction entire graph one saved mathematical model also training data loading last training hard possible change essentially making entire functional general long want make last training going problematic load way work around problem encourage writing mathematical model python load another way way provide cleaner version actual graph used training math use many avoid bad interaction graph graph,issue,negative,negative,neutral,neutral,negative,negative
396705826,Thanks for the prompt reply. Do you mind pointing me to references of why modifying metagraph for training is not a good idea? ,thanks prompt reply mind pointing training good idea,issue,positive,positive,positive,positive,positive,positive
396704831,"In tensorflow, loading a metagraph and modifying it for training is NEVER a good idea. It almost always causes undesired trouble. In general you should write python code to recreate your model, and only load variables from a checkpoint.",loading training never good idea almost always undesired trouble general write python code recreate model load,issue,negative,negative,negative,negative,negative,negative
396418054,"This is how the model is trained on ImageNet.
This does not imply what you're supposed to do on a different dataset. That's your decision to make. 

I would guess using the same constants I pasted above to preprocess the image is more reasonable. But such discussion is out of the scope of the project.",model trained imply supposed different decision make would guess pasted image reasonable discussion scope project,issue,negative,positive,neutral,neutral,positive,positive
396417116,Thanks for responding so quickly! Does this mean that the Tensorpack ResNet models expect inputs whose bands across the entire dataset have mean 0 and standard deviation 1?,thanks quickly mean expect whose across entire mean standard deviation,issue,negative,negative,neutral,neutral,negative,negative
396205220,"1. And what is the address you use? I hope it's not `tcp://xxx:2222`.

2. This has nothing to do with pyarrow. As said you need `ZMQInput` as the receiver.",address use hope nothing said need receiver,issue,negative,neutral,neutral,neutral,neutral,neutral
396152625,"1.I also it's a network issue,but I don't how to fix it.
While if I use the default bind value ,the test runs ok
bind=False to send_dataflow_zmq and bind=True to RemoteDataZMQ
and I run the test succ with ipc send and bind =True

2.I run the benchmark.py send and recv ok.
pyarrow 0.9.0 do not support for deserialize the metadata version of  reviced dataflow BatchData(dataset.Mnist('train'), 128) 
",also network issue fix use default bind value test run test send bind run send support version,issue,positive,neutral,neutral,neutral,neutral,neutral
396104191,"1. If the error is `zmq.error.ZMQError: Cannot assign requested address`, then it's your network issue. Check your ip address is correct. Check your port is not occupied, etc.

3. The `zmq_ops` repo has an example to use it as a separate package. https://github.com/tensorpack/zmq_ops/blob/master/benchmark.py
To use it inside tensorpack, you can use [ZMQInput](http://tensorpack.readthedocs.io/modules/input_source.html#tensorpack.input_source.ZMQInput) as the input source for the trainer.",error assign address network issue check address correct check port example use separate package use inside use input source trainer,issue,negative,neutral,neutral,neutral,neutral,neutral
396100645,"1.I did pass bind=False to RemoteDataZMQ and the revicer holding when get_data .while the sender still got the same error

2. it works~

3. I pass format='zmq_op' to send_dataflow_zmq. I also Installed  zmq-ops in sender and revicer /usr/local/lib/python2.7/dist-packages/zmq_ops-0.1.0-py2.7.egg
how do I use zmq_ops on the receiver side?

4. I can not test it becase error 1 above



",pas holding sender still got error pas also sender egg use receiver side test error,issue,negative,neutral,neutral,neutral,neutral,neutral
395980667,"Same as #721. Please use latest tensorpack with 
`pip install -U git+https://github.com/tensorpack/tensorpack.git`",please use latest pip install,issue,negative,positive,positive,positive,positive,positive
395980369,"1. 
> when I pass bind=True to send_dataflow_zmq ,I got the error:

Then you need bind=False for the receiver side.

2.
> I find the error because class RemoteDataZMQ don't init cnt1 in init function.You may init cnt1 in init or tell the user must reset_state before get_data clearly！

It's clearly told in http://tensorpack.readthedocs.io/tutorial/dataflow.html

3. Could you describe what you did to cause such error? I assume you pass something other than `format=None` on the sender side. As mentioned in docs http://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.send_dataflow_zmq, the other format is for use with https://github.com/tensorpack/zmq_ops. To be able to use it, you need to install zmq_ops and use those ops on the receiver side.

4. Use the sender with `bind=True`, use the receiver with a remote ip address and `bind=False`. When there are multiple receivers, zmq will send 1 message to only 1 receiver.
",pas got error need receiver side find error class may tell user must clearly told could describe cause error assume pas something sender side format use able use need install use receiver side use sender use receiver remote address multiple send message receiver,issue,negative,positive,positive,positive,positive,positive
395757153,"If you need, I will glad to share my TPU quota with you.",need glad share quota,issue,positive,positive,positive,positive,positive,positive
395745555,"Looks like a tensorflow bug: https://github.com/tensorflow/benchmarks/issues/165.
And Google's team also suggests you to use horovod for the moment.
Horovod trainer also has better support in tensorpack than the native trainers, e.g.: https://github.com/tensorpack/benchmarks/tree/master/ResNet-Horovod",like bug team also use moment trainer also better support native,issue,positive,positive,positive,positive,positive,positive
395744026,"I would love to but I don't have access to a TPU to debug.

Resources:
use TPU without estimator: https://github.com/mlperf/training_results_v0.6/blob/master/Google/benchmarks/resnet/implementations/tpu-v3-512-resnet/resnet/train_and_eval_runner.py

good thread:
https://twitter.com/theshawwn/status/1223395022814339073",would love access use without estimator good thread,issue,negative,positive,positive,positive,positive,positive
395656829,"it works when I pass config=get_default_sess_config() to tf.Server

Then I got error  Master init: Unavailabel: OS Error when I run worker before ps,while it's ok when I run ps firstly.

[0608 05:55:08 @base.py:197] Creating the session ...
2018-06-08 05:55:09.075000: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error
2018-06-08 05:55:09.147089: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error
Traceback (most recent call last):
  File ""distributed-ps-mnist-convnet.py"", line 145, in <module>
    launch_train_with_config(config, DistributedTrainerParameterServer([int(x) for x in args.gpu.split("","")], server))
  File ""/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/train/interface.py"", line 90, in launch_train_with_config
    extra_callbacks=config.extra_callbacks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/train/base.py"", line 302, in train_with_defaults
    steps_per_epoch, starting_epoch, max_epoch)
  File ""/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/train/base.py"", line 273, in train
    self.initialize(session_creator, session_init)
  File ""/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/train/trainers.py"", line 199, in initialize
    get_distributed_session_creator(self.server), session_init)
  File ""/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/utils/argtools.py"", line 181, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/train/base.py"", line 200, in initialize
    self.sess = session_creator.create_session()
  File ""/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/tfutils/distributed.py"", line 40, in create_session
    return sm.prepare_session(master=server.target, init_op=init_op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session
    sess.run(init_op, feed_dict=init_feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: OS Error
",work pas got error master o error run worker run firstly session master unavailable o error master unavailable o error recent call last file line module server file line file line file line train file line initialize file line wrapper return file line initialize file line return file line file line run file line file line file line raise type message o error,issue,negative,positive,neutral,neutral,positive,positive
395614767,"Confirmed that train with ""ImageNet-ResNet50.npz"" pretrained network achieved the baseline as well. Thanks for the help! Closing the issue.",confirmed train network well thanks help issue,issue,positive,positive,positive,positive,positive,positive
395604031,"A small note.
Things work if I use `self.trainer.sess.run(self.mask_update_op)`
If I use, `self.mask_update_op.run()`, I get the following error.

```
2018-06-07 17:04:47.019623: W tensorflow/core/kernels/queue_base.cc:277] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed
Traceback (most recent call last):
  File ""sparse-cifar-convnet.py"", line 176, in <module>
    launch_train_with_config(config, trainer)
  File ""/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/train/interface.py"", line 90, in launch_train_with_config
    extra_callbacks=config.extra_callbacks)
[0607 17:04:47 @input_source.py:148] EnqueueThread QueueInput/input_queue Exited.
  File ""/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/train/base.py"", line 301, in train_with_defaults
    steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/train/base.py"", line 273, in train
    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/utils/argtools.py"", line 181, in wrapper
    return func(*args, **kwargs)
  File ""/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/train/base.py"", line 239, in main_loop
    self._callbacks.trigger_step()
  File ""/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/callbacks/group.py"", line 85, in trigger_step
    cb.trigger_step()
  File ""/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/callbacks/base.py"", line 138, in trigger_step
    self._trigger_step()
  File ""sparse-cifar-convnet.py"", line 101, in _trigger_step
    self.mask_update_op.run()
AttributeError: 'Tensor' object has no attribute 'run'
```

Closing this thread for now.",small note work use use get following error skipping attempt queue closed recent call last file line module trainer file line file line file line train file line wrapper return file line file line file line file line object attribute thread,issue,negative,negative,neutral,neutral,negative,negative
395602874,"Thanks!
Things work for me with your edits. 

The baseline network was getting to 90% validation accuracy.
With 90% sparsity I am getting very close to it (89.4%).

Attached is the training script and log file with the results.

[log.log.txt](https://github.com/tensorpack/tensorpack/files/2082449/log.log.txt)
[sparse-cifar-convnet.py.txt](https://github.com/tensorpack/tensorpack/files/2082450/sparse-cifar-convnet.py.txt)
",thanks work network getting validation accuracy sparsity getting close attached training script log file,issue,negative,positive,positive,positive,positive,positive
395583715,"From the docs https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning it looks like you want to run extra ops after each iteration. 

`SessionRunArgs` is only for `before_run` and `after_run`, to provided an interface to tensorflow's `SessionRunHooks`. It is for running extra ops __along with__ each iteration.

`trigger_step` is for running anything after each iteration. It is just a plain python function so you need `self.trainer.sess.run(mask_update_op)`, or simply `mask_update_op.run()`.",like want run extra iteration provided interface running extra iteration running anything iteration plain python function need simply,issue,negative,negative,neutral,neutral,negative,negative
395126998,I'll close this since this appears to be a tensorflow bug https://github.com/tensorflow/tensorflow/issues/19657 that a graph cannot be too large.,close since bug graph large,issue,negative,positive,positive,positive,positive,positive
395126683,"Btw, as mentioned in the docs, you're not recommended to use TensorFlow's native distributed trainer because they are not as efficient as HorovodTrainer.",use native distributed trainer efficient,issue,negative,neutral,neutral,neutral,neutral,neutral
395126347,"This is a TensorFlow error that ops cannot be placed on GPU.
You need to initialize the server with a tensorflow sessionconfig, which enables soft placement.
In tensorpack, you can just pass `config=get_default_sess_config()` to `tf.Server`.",error need initialize server soft placement pas,issue,negative,positive,neutral,neutral,positive,positive
395125772,"InputSource cannot be used alone.

 You're recommended to implement a custom trainer: http://tensorpack.readthedocs.io/tutorial/extend/trainer.html if you want to do anything substantially different from single-cost optimization.",used alone implement custom trainer want anything substantially different optimization,issue,negative,neutral,neutral,neutral,neutral,neutral
395121205,"See http://tensorpack.readthedocs.io/tutorial/trainer.html#tower-trainer 
Your model function will be called multiple times, and as a result will change your collection.",see model function multiple time result change collection,issue,negative,neutral,neutral,neutral,neutral,neutral
395096020,I reduced number of parameters and the model goes on. Maybe there is number of parameters limit.,reduced number model go maybe number limit,issue,negative,neutral,neutral,neutral,neutral,neutral
394977295,"I don't understand what does it mean exactly?
```
[0606 10:50:47 @collection.py:151] Size of these collections were changed in InferenceTower: (analyze_variables: 1->2), (pdf_variables: 72->144), (train_variables: 174->348)
[0606 10:50:47 @collection.py:164] These collections were modified but restored in InferenceTower: (tf.GraphKeys.UPDATE_OPS: 8->16)
```
why does it change collections size (*2) during InferenceTower?

I can collect variables without creating collection - maybe this will resolve the problem?",understand mean exactly size change size collect without collection maybe resolve problem,issue,negative,negative,neutral,neutral,negative,negative
394964605,"No I think it's caused by large constant tensor in the graph -- not necessarily in the data part, could be anywhere in the graph.
But that's just my guess and you'd better wait for responses from the TF team.",think large constant tensor graph necessarily data part could anywhere graph guess better wait team,issue,negative,positive,positive,positive,positive,positive
394962148,"Does it mean - I have value ~ 2**32 in my dataflow?
I made all my data normalized from 0 to 1 and saved it to TFrecords with compression.
Dataflow:
```
        train_size, test_size = get_train_test_size()

        train = TFRecordData(
            a.folders['inp'] + a.tf_records['train_file'],
            size=train_size,
            compress=True
        )
        train = BatchData(
            train,
            a.start_args['train_batch_size'],
            remainder=True)

        test = TFRecordData(
            a.folders['inp'] + a.tf_records['test_file'],
            size=test_size,
            compress=True
        )
        test = BatchData(
            test,
            a.start_args['test_batch_size'],
            remainder=True)
``` ",mean value made data saved compression train train train test test test,issue,positive,negative,negative,negative,negative,negative
394956353,OK. Looks like that's a tensorflow bug then. Seems to happen when you have a very large constant in the graph.,like bug happen large constant graph,issue,negative,positive,positive,positive,positive,positive
394955691,"Also I found this issue:  [https://github.com/tensorflow/tensorflow/issues/19657](url)
about the same problem.",also found issue problem,issue,negative,neutral,neutral,neutral,neutral,neutral
394954815,"1. here is log of error:

```
[0605 18:05:11 @graph.py:91] Applying collection UPDATE_OPS of 8 ops.
[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/message_lite.cc:68] CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)`
```

there is no ""creating session ....""

2. My program is crashed consistently. I have not any other version of TF.

3. config in this case is:

```
        dataset_train, dataset_test, _ = get_data(command=command)
        steps_per_epoch = dataset_train.size()
        dataset_train = StagingInput(QueueInput(dataset_train))
        dataset_test = QueueInput(dataset_test)
output_config = {
                'steps_per_epoch': steps_per_epoch,
                'callbacks': [
                    ModelSaver(max_to_keep=10, keep_checkpoint_every_n_hours=0.33),
                    MaxSaver('MAF_loss', filename='max_log_prob'),
                    LogProbAfterEpoch(""EstimatePDF/MAF_loss""),
                    InferenceRunner(
                        dataset_test,
                        [
                            LogProbSum(""EstimatePDF/MAF_loss"")
                        ]
                    ),
                    ScheduledHyperParamSetter('optimizer/learning_rate', [
                        (a.pdf_estimator['drop0'], a.pdf_estimator['lr0']),
                        (a.pdf_estimator['drop1'], a.pdf_estimator['lr1'])
                    ])
                ],
                'max_epoch': a.pdf_estimator['hm_epoch']
        }
```

4. here is code of collections:

```
        analyze_variables = tf.get_collection(
            tf.GraphKeys.TRAINABLE_VARIABLES,
            scope='.*analyze'
        )
        pdf_variables = tf.get_collection(
            tf.GraphKeys.TRAINABLE_VARIABLES,
            scope='.*EstimatePDF'
        )
        train_variables = tf.get_collection(
            tf.GraphKeys.TRAINABLE_VARIABLES,
            scope='^(?!(.*(EstimatePDF|analyze))).*$'
        )
        databus['analyze_variables'] = analyze_variables
        databus['pdf_variables'] = pdf_variables
        databus['train_variables'] = train_variables

        G = tf.get_default_graph()
        for tensor in databus['analyze_variables']:
            G.add_to_collection(name='analyze_variables', value=tensor)
        for tensor in databus['pdf_variables']:
            G.add_to_collection(name='pdf_variables', value=tensor)
        for tensor in databus['train_variables']:
            G.add_to_collection(name='train_variables', value=tensor)
```

I don't think I put any non variable into the collections.


here are callbacks I used:

```
class AfterTrainEpoch(Callback):
    def _setup_graph(self):
        self.train_tower = self.trainer.tower_func.towers[0]
        self.outputs = [self.train_tower.get_tensor(name) for name in self.get_fetches()]
    def _before_train(self):
        pass
    def _before_epoch(self):
        self._before_train_epoch()
    def _before_train_epoch(self):
        pass
    def _before_run(self, _):
        return tf.train.SessionRunArgs(fetches=self.outputs)
    def _after_run(self, _, run_values):
        self.on_fetches(run_values.results)
    def on_fetches(self, outputs):
        self._on_fetches(outputs)
    def _on_fetches(self, outputs):
        raise NotImplementedError()
    def _trigger_epoch(self):
        ret = self._after_train_epoch()
        if ret is None:
            return
        for k, v in six.iteritems(ret):
            try:
                v = float(v)
            except ValueError:
                logger.warn(""{} returns a non-scalar statistics!"".format(type(self).__name__))
                continue
            else:
                self.trainer.monitors.put_scalar(k, v)
    def _after_train_epoch(self):
        pass
    def get_fetches(self):
        try:
            ret = self._get_fetches()
        except NotImplementedError:
            logger.warn(""Inferencer._get_output_tensors was deprecated and renamed to _get_fetches"")
            ret = self._get_output_tensors()
        return [get_op_tensor_name(n)[1] for n in ret]
    def _get_fetches(self):
        raise NotImplementedError()
    def _get_output_tensors(self):
        pass
```

```
class LogProbAfterEpoch(AfterTrainEpoch):
    def __init__(self, log_prob_tensor_name):
        """"""
        Args:
            log_prob_tensor_name(str): name of the LogProb tensor.
        """"""
        self.log_prob_tensor_name = log_prob_tensor_name
    def _before_train_epoch(self):
        self.collector = list()
    def _get_fetches(self):
        return [self.log_prob_tensor_name]
    def _on_fetches(self, outputs):
        log_prob = outputs[0]
        self.collector.append(log_prob)
    def _after_train_epoch(self):
        log_prob = np.concatenate(self.collector, axis=0)
        return {
            'train_MAF_loss': np.sum(log_prob)
        }
```

```
class LogProbSum(Inferencer):
    def __init__(self, log_prob_tensor_name):
        """"""
        Args:
            log_prob_tensor_name(str): name of the LogProb tensor.
        """"""
        self.log_prob_tensor_name = log_prob_tensor_name
    def _before_inference(self):
        self.collector = list()
    def _get_fetches(self):
        return [self.log_prob_tensor_name]
    def _on_fetches(self, outputs):
        log_prob = outputs[0]
        self.collector.append(log_prob)
    def _after_inference(self):
        log_prob = np.concatenate(self.collector, axis=0)
        return {
            'MAF_loss': np.sum(log_prob)
        }
```
",log error collection fatal check concurrently serialization terminate throwing instance check concurrently serialization process finished exit code interrupted signal session program consistently version case code tensor tensor tensor think put non variable used class self name name self pas self self pas self return self self self raise self ret ret none return ret try float except statistic type self continue else self pas self try ret except ret return ret self raise self pas class self name self list self return self self return class self name self list self return self self return,issue,negative,positive,positive,positive,positive,positive
394843060,"I don't know how to reproduce this error and this is some error I've never seen before. Could be a TensorFlow bug as well. Having a piece of runable code that can reproduce the error would be awesome. Apart from that, some information that may make it easier to debug:
1. Have you seen the log ""creating the session...."" ? Usually this line of log comes after ""Applying collection UPDATE_OPS of 8 ops."". Knowing where it crashes can help a lot.
2. Does your code consistently crash or crash by chance? What about a different version of TensorFlow?
3. What does ""config"" look like?
4. If you add some weird stuff to TF collections (i.e., things that are not ops or tensors), try removing them",know reproduce error error never seen could bug well piece code reproduce error would awesome apart information may make easier seen log session usually line log come collection knowing help lot code consistently crash crash chance different version look like add weird stuff try removing,issue,negative,positive,neutral,neutral,positive,positive
393972098,"
Thanks for the prompt response! Confirmed that the pretrained COCO-ResNet50-MaskRCNN.npz network can achieve mAP of 0.377/0.330 (bbox/segm), which is same as the baseline. Trying ""train-from-scratch"" with the pretrained ResNet50 model (ImageNet-ResNet50.npz) downloaded from the website (http://models.tensorpack.com/ResNet/). Will update the status.
",thanks prompt response confirmed network achieve map trying model update status,issue,negative,positive,positive,positive,positive,positive
393952977,"You should load a ResNet50 (ImageNet) model to train, not a MaskRCNN model. The MaskRCNN model is fully trained already and evaluating that model without training should get you the right score. See the README.",load model train model model fully trained already model without training get right score see,issue,negative,positive,positive,positive,positive,positive
393251364,"If you're looking for kernel implementation, https://github.com/caffe2/caffe2/tree/master/caffe2/mobile/contrib/ulp2 contains some of the best open source ones I know.

gemmlowp only works with 8 bit and is quite slow. I don't know about BMXNet.",looking kernel implementation best open source know work bit quite slow know,issue,positive,positive,positive,positive,positive,positive
393089512,"Sorry for being confusing.

As i understand it, and correct me if i am wrong, the final weights are still stores in floats and have floating point gemm performed on them. I wanted to ask if you know of a way or a framewiork of compressing these weights to int8 or lower and have low precision gemm performed on them.",sorry understand correct wrong final still floating point ask know way lower low precision,issue,negative,negative,negative,negative,negative,negative
393044727,"I changed the mnist example with `tf.layers`. The function `enable_argscope_for_lib` is be a little bit more generic as now `enable_argscope_for_lib(tf.layers)` is sufficient. I do not recommend to hard-coded `tf.layers` in `argscope.py` to not introduce another dependency.

Well, `tf.keras.layers` :unamused: is a different beast. I have an aversion to a library trying to wrap plain tensors (same reason I dislike the upper-case `tf.layers` methods). Is there any advantage besides direct access to parameters?",example function little bit generic sufficient recommend introduce another dependency well unamused different beast aversion library trying wrap plain reason dislike advantage besides direct access,issue,negative,negative,neutral,neutral,negative,negative
392900056,"Q1,2: I guess it's better to let users do this explicitly since this is a monkey-patch hack. And if this is the case, hard-coding a list may not be necessary.

Q3: I don't know what the future of these libraries would be. I think for now supporting `tf.layers` alone is good enough. 
Btw I've heard from a couple of TF people that they are encouraging people to use `tf.keras.layers` instead of `tf.layers`..",guess better let explicitly since hack case list may necessary know future would think supporting alone good enough couple people encouraging people use instead,issue,positive,positive,positive,positive,positive,positive
392851574,All the math involving the weights are in the model file https://github.com/ppwwyyxx/tensorpack/blob/master/examples/DoReFa-Net/alexnet-dorefa.py so I'm not sure what you are asking.,math model file sure,issue,negative,positive,positive,positive,positive,positive
392777629,"The current state allows to write something along the lines

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import tensorflow as tf
from tensorpack import *


patch_tflayers(tf.layers)

net = tf.placeholder(tf.float32, (None, 28, 28, 1), 'image')

with argscope([tf.layers.conv2d], padding='same', activation=tf.identity):
    net = tf.layers.conv2d(net, 32, 3, name='conv0')

```

which will produce the output
```
('passed kwargs', {'padding': 'same', 'activation': <function identity at 0x7f6092031ed8>, 'name': 'conv0'})
```
as expected. This is just intended to discuss possible side-effects I am not aware of. Ideally, this `tf.layers` patch would further support the overview table of all used layers.

Possible points to discuss:
- Q1: should we hard-code the list of supported `tf.layers.methods`?
- Q2: any better way than using `patch_tflayers` (should this run automatically)?
- Q3: if decided to support `tf.layers` in argscope any other libraries which should be considered in the same edits

Q1: I am afraid, that no hard-coding these methods would make this hack unmaintainable
Q2: I feel there are low costs when asking the user to ad this line explicitly which further avoids unexpected behaviour.
Q3: As `tf.layers` is officially supported in TensorFlow I would say, it is reasonable to stick with `tf.layers` only",current state write something along python python import import net none net net produce output function identity intended discus possible aware ideally patch would support overview table used possible discus list better way run automatically decided support considered afraid would make hack unmaintainable feel low user ad line explicitly unexpected behaviour officially would say reasonable stick,issue,positive,positive,positive,positive,positive,positive
392714545,"I have the same problem, currently investigating it, want to make it work since tensorpack is such a great library and currently i have to use windows",problem currently investigating want make work since great library currently use,issue,negative,positive,positive,positive,positive,positive
392300322,That said it would surely be nice to have these features. Mark it as a possible future feature but it won't be my priority.,said would surely nice mark possible future feature wo priority,issue,positive,positive,positive,positive,positive,positive
392233651,"The top level `get_data()` function in the script defines and returns a dataflow. It does not take data from a dataflow -- that will happen when training starts.

The tutorial has a section about writing such dataflow: http://tensorpack.readthedocs.io/tutorial/extend/dataflow.html#more-data-processing .
Please refer to `JoinData` or `ConcatData` if it's not clear enough, e.g. http://tensorpack.readthedocs.io/_modules/tensorpack/dataflow/common.html#ConcatData",top level function script take data happen training tutorial section writing please refer clear enough,issue,positive,positive,positive,positive,positive,positive
392233398,"Can I define that ""itr"" within the get_data() function?
def get_data(train_or_test, isMixup, alpha):
    ds = dataset.Cifar10(train_or_test)
    batch = BATCH_SIZE
    ds = BatchData(ds, batch, remainder=not isTrain)


Specifically, I want the following codes in mixup to mix two batches of data:
    ds = BatchData(ds, batch, remainder=not isTrain)
    def f(dp):
        images, labels = dp
        one_hot_labels = np.eye(CLASS_NUM)[labels]  # one hot coding
        if not isTrain or not isMixup:
            return [images, one_hot_labels]

        # mixup implementation:
        # Note that for larger images, it's more efficient to do mixup on GPUs (i.e. in the graph)
        weight = np.random.beta(alpha, alpha, BATCH_SIZE)
        x_weight = weight.reshape(BATCH_SIZE, 1, 1, 1)
        y_weight = weight.reshape(BATCH_SIZE, 1)
        index = np.random.permutation(BATCH_SIZE)

        x1, x2 = images, images[index]
        x = x1 * x_weight + x2 * (1 - x_weight)
        y1, y2 = one_hot_labels, one_hot_labels[index]
        y = y1 * y_weight + y2 * (1 - y_weight)
        return [x, y]

    ds = MapData(ds, f)
",define within function alpha batch batch specifically want following mix two data batch one hot return implementation note efficient graph weight alpha alpha index index index return,issue,negative,positive,positive,positive,positive,positive
392232909,"```python
itr = ds.get_data()
data1 = next(itr)
data2 = next(itr)
```
will retrieve two datapoints from ds.",python data next data next retrieve two,issue,negative,neutral,neutral,neutral,neutral,neutral
392232812,"Can I retrieve two batches of data points from BatchData(ds, batch)?
",retrieve two data batch,issue,negative,neutral,neutral,neutral,neutral,neutral
392232382,"You can write a dataflow that takes datapoints from `ds = BatchData(ds, batch)`, then produce two datapoints together.
`JoinData([ds, ds])`  does something similar to this.",write batch produce two together something similar,issue,negative,neutral,neutral,neutral,neutral,neutral
392231717,"Thanks!  Any chance I can have two batches of data points when calling BatchData? 
That is, when I call  ds = BatchData(ds, batch, remainder=not isTrain), I would like the ds returns two random batches of data points.
",thanks chance two data calling call batch would like two random data,issue,positive,negative,negative,negative,negative,negative
392231261,"Can I code something like this?

def get_data(train_or_test, isMixup, alpha):
    ds_test = dataset.Cifar10(test)
    ds_train = dataset.Cifar10(train)
   ds_test = BatchData(ds_test, batch, remainder=not isTrain)
   ds_train = BatchData(ds_train, batch, remainder=not isTrain)
   mix(ds_test,ds_train)",code something like alpha test train batch batch mix,issue,negative,neutral,neutral,neutral,neutral,neutral
392230912,"During the test phase, when I have a batch of data points for testing, I would like to randomly add some data points from the training set (ideally, a small batch from the training data). ",test phase batch data testing would like randomly add data training set ideally small batch training data,issue,positive,negative,negative,negative,negative,negative
392230680,"> Any build-in function for the split? 

No. It should be like 10 lines of code.

> Also, is it possible to have two batches during test phase

I don't understand what this means. Could you describe in details what you want to do?",function split like code also possible two test phase understand could describe want,issue,negative,neutral,neutral,neutral,neutral,neutral
392230581,"Any build-in function for the split? Also, is it possible to have two batches during test phase: one from BatchData(testing data) and the other from BatchData(training data)?",function split also possible two test phase one testing data training data,issue,negative,neutral,neutral,neutral,neutral,neutral
392228188,"I don't know what do you mean by ""batch size"" because the term is ambiguous under the context of multi gpu training. But the tutorial should have an answer to your question: http://tensorpack.readthedocs.io/tutorial/trainer.html#multigpu-trainers 

To write a dataflow, see http://tensorpack.readthedocs.io/tutorial/extend/dataflow.html . You can first use `BatchData` with a larger batch and then a custom dataflow to split them and yield.",know mean batch size term ambiguous context training tutorial answer question write see first use batch custom split yield,issue,negative,negative,neutral,neutral,negative,negative
392224880,"If the batch size is 128 and 2 GPUs are used, then each GPU will has a batchsize of 64. That is, the permutation of mixup will use a batch of 64 images. Is that correct? Any example of ""write a dataflow that mixes more images than it yields every time""? Thanks a lot!",batch size used permutation use batch correct example write every time thanks lot,issue,negative,positive,positive,positive,positive,positive
392221840,"Two best C4 models are uploaded already. 
I may upload a FPN one later, but I won't have time to manage every one of them..",two best already may one later wo time manage every one,issue,positive,positive,positive,positive,positive,positive
392200632,"The permutation will be per-datapoint, i.e. a datapoint is a unit that's sent to one GPU.
If you want to permute over larger batches you can write a dataflow that mixes more images than it yields every time.",permutation unit sent one want permute write every time,issue,negative,neutral,neutral,neutral,neutral,neutral
392187084,"doing so, the permutation for the mixup could be incorrect? That is, you will have smaller BATCH_SIZE for the permutation?",permutation could incorrect smaller permutation,issue,negative,neutral,neutral,neutral,neutral,neutral
391945410,"If the loss is accumulated over all the valid anchors, that sounds equivalent to what is in my code.",loss valid equivalent code,issue,negative,neutral,neutral,neutral,neutral,neutral
391944507,"I asked because I just concat all the boxes and anchors across every level and compute the loss for rpn without placeholder and (1./ RPN_BATCH_PER_IM) term. 

I am wondering whether this will be the potential bugs in my previous implementation.

Thanks for clarification.",across every level compute loss without term wondering whether potential previous implementation thanks clarification,issue,negative,positive,neutral,neutral,positive,positive
391943374,"> why rpn loss is computed level-wise

I'm not sure I understand what you're asking. What is the alternative if not level-wise? Each level has different image sizes, so I compute loss on each level separately.

When the loss does not exist (no boxes at all) I don't want it to be shown as NaN. A placeholder can be set to any number and it will not affect training. It just makes the logs and tensorboard look nicer.",loss sure understand alternative level different image size compute loss level separately loss exist want shown nan set number affect training look,issue,negative,positive,positive,positive,positive,positive
391942414,"@ppwwyyxx  The solution to tf.image.imresize is awesome. Thanks! great job.
I still have an question about fpn's  rpn loss, if you can kindly explain to me will be very appreciated.
May I ask you why rpn loss is computed level-wise and what is the meaning of placeholder in rpn loss?",solution awesome thanks great job still question loss kindly explain may ask loss meaning loss,issue,positive,positive,positive,positive,positive,positive
391649885,"As mentioned in the issue template:
```
An issue has to be one of the following:
1. Unexpected Problems / Potential Bugs
2. Feature Requests
3. Usage Questions
```
Closing for the same reason as #771.",issue template issue one following unexpected potential feature usage reason,issue,negative,positive,neutral,neutral,positive,positive
391376259,"Because tensorpack is a training interface and does not care about inference. Tensorflow has many inference techniques (serving, quantization, const folding, tensorrt, tflite,...) for different scenarios. These are probably more important than queues.",training interface care inference many inference serving quantization folding different probably important,issue,positive,positive,positive,positive,positive,positive
390908616,"So far I think it can be done in the code using:

```
    def _aggregate_batch(data_holder, use_list=False):
        if isinstance(data_holder[0], map):
            iterat_indices = data_holder[0].keys()
        else:
            size = len(data_holder[0])
            iterat_indices = range(size)
        result = []
        for k in iterat_indices:
```

...or even better having a separate function, that would give us iterator of indices to iterate over (indices for list, keys for map, anything for anything else ...)",far think done code map else size range size result even better separate function would give u index iterate index list map anything anything else,issue,negative,positive,positive,positive,positive,positive
390852132,"Just need to replace some layers - so it should be easy.
Closing since this is not a bug report, feature request or usage question.",need replace easy since bug report feature request usage question,issue,negative,positive,positive,positive,positive,positive
390469536,"i run these code

import tensorflow as tf
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

Then I  got 

2018-05-20 17:45:16.481366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-05-20 17:45:16.481375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)
2018-05-20 17:45:16.482564: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 133.81M (140312576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2018-05-20 17:45:16.484438: I tensorflow/core/common_runtime/direct_session.cc:265] Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0
Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0
PyDev console: starting.
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux

Isn't  tensorflow-gpu",run code import sess got device device name bus id allocate device device device name bus id device device name bus id console starting python default,issue,negative,neutral,neutral,neutral,neutral,neutral
390108295,"I got it!
What indeed help to print tensor values easily in your framework is _ProcessTensors()_. 

And for _sys.exit()_ problem, probably I made some mistakes.",got indeed help print tensor easily framework problem probably made,issue,negative,positive,positive,positive,positive,positive
390078067,"`print_tensor()` is not for printing the value of the tensor. According to #12 it is for printing the name of the tensor. To print values during training, see [FAQ](http://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training).

It's impossible that adding `sys.exit()` has no effect -- I think you're not running the code you're modifying.",printing value tensor according printing name tensor print training see impossible effect think running code,issue,negative,negative,negative,negative,negative,negative
390072352,"![image](https://user-images.githubusercontent.com/38173114/40212185-49694524-5a81-11e8-8e6d-29635b66d7cd.png)
The initialization logs of gpu were ignored. As you can see, the project would begin to process the epochs after I run _python svhn-digit-dorefa.py_, but no tensor values were printed. What's more, though I added _sys.exit()_ sentence in your definition of _print_tensor()_ under tensorpack/models/linearwrap.py, it also seemed to make no sense.
![image](https://user-images.githubusercontent.com/38173114/40212389-36f4ae00-5a82-11e8-9945-7249e35dd530.png)
I am not familiar with your framework, so thanks for your help.",image see project would begin process run tensor printed though added sentence definition also make sense image familiar framework thanks help,issue,positive,positive,positive,positive,positive,positive
389959190,"And I prepared a DataFlow that compares two others like zip, checks
recursively equality of their nested components and reports differences.
Useful regression testing to quickly see we broke anything. I'll try to
clean it up and make a PR. I wish I had it earlier today. :)

Dne čt, 17. 5. 2018 19:00 uživatel Yuxin Wu <notifications@github.com>
napsal:

> Now most DataFlow assumes list. I feel some inconvenience with it as well.
> Allowing dict will be great!
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/768#issuecomment-389937808>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAbOrIjRJQuURF3kNNILiWqPqgCo3eWWks5tzazFgaJpZM4UC5k7>
> .
>
",prepared two like zip equality useful regression testing quickly see broke anything try clean make wish today list feel inconvenience well great thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
389937808,"Now most DataFlow assumes list. I feel some inconvenience with it as well. Allowing dict will be great!
Probably not nested list now -- it's harder to implement and unclear how to map it to inputs in the graph.",list feel inconvenience well great probably list harder implement unclear map graph,issue,negative,positive,positive,positive,positive,positive
389299169,"Well, thanks for the awesome work. At Rossum we'll certainly need to
improve our research infrastructure to be both easy to use and high
performance. So far Tensorpack DataFlow seems to be a really good step -
well composable and faster than our home grown multi-processing generators.
On the other hand leaving all the Keras tools like model saving/loading,
callbacks and other stuff may be just a too big step for less marginal
benefit. Let's see if we'll be able to improve Tensorpack to support Keras
better, or improve Keras for TF Queue/Staging/Multi GPU or just switch to
Tensorflow completely. Anyway if there's any improvement we'll be happy to
provide PR's.

Dne út, 15. 5. 2018 22:08 uživatel Yuxin Wu <notifications@github.com>
napsal:

> Yes. Keras support is just a proof-of-concept. And obviously it is
> unlikely to be a focus of this project.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/764#issuecomment-389296734>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAbOrLx861Fdwh9x9WYR6vgQu7-Z6ozzks5tyzXGgaJpZM4T_WkG>
> .
>
",well thanks awesome work certainly need improve research infrastructure easy use high performance far really good step well faster home grown hand leaving like model stuff may big step le marginal benefit let see able improve support better improve switch completely anyway improvement happy provide yes support obviously unlikely focus project thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
389296734,Yes. Keras support is just a proof-of-concept. And obviously it is unlikely to be a focus of this project.,yes support obviously unlikely focus project,issue,positive,negative,negative,negative,negative,negative
389266550,"I don't say you have to do it. It's just a reminder for me to write it in
future if needed. We're still considering whether to use just DataFlow or
also trainers in our project since Keras support is still quite
experimental.

Dne út, 15. 5. 2018 18:53 uživatel Yuxin Wu <notifications@github.com>
napsal:

> I don't think I'll be able to spend any more time on supporting Keras.
> Plus I think having sample_weight is a very ugly design..
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/764#issuecomment-389237679>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAbOrDf0WgiGYCkz1_ZfIK-P0vz7vksJks5tywgSgaJpZM4T_WkG>
> .
>
",say reminder write future still considering whether use also project since support still quite experimental think able spend time supporting plus think ugly design thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
389237679,"I don't think I'll be able to spend any more time on supporting Keras.
Plus I think having `sample_weight` is  a very ugly design..",think able spend time supporting plus think ugly design,issue,negative,positive,neutral,neutral,positive,positive
389236210,"If you can do something once, then you can do something multiple times. What is the issue?",something something multiple time issue,issue,negative,neutral,neutral,neutral,neutral,neutral
389235278,"Hi,
I'd like to initialize some variables after each epoch,
I can set up op while _setup_graph() with init_op=tf.variables_initializer(varlist)  but how do I run it in _trigger_epoch()?",hi like initialize epoch set run,issue,negative,neutral,neutral,neutral,neutral,neutral
389107168,"I meet the same thing too.

python 2.7 
tensorflow 1.7.0 by `pip install tensorflow==1.7.0`
install tensorpack using `pip install tensorpack`",meet thing python pip install install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
388906686,"I found the reason is that on windows, expressions like:
```python
            imgaug.MapImage(lambda x: x - pp_mean),
```
cannot be used with multiprocessing (the lambda is not pickleable).

An alternative fix is to define a function at global scope that does the subtraction. Then use 
```python
imgaug.MapImage(func)
```
instead.",found reason like python lambda used lambda alternative fix define function global scope subtraction use python instead,issue,negative,neutral,neutral,neutral,neutral,neutral
388881030,"It's already printed out:
```
[0514 20:52:53 @parallel.py:175] WRN MultiProcessPrefetchData does support windows. However, windows requires more strict picklability on processes, which may lead of failure on some of the code.
```",already printed support however strict may lead failure code,issue,negative,negative,negative,negative,negative,negative
388602428,"When I just used the TF's train function to train my model, the errors are as follows, it seems not related to tensorpack, but to some related tf errors as in https://github.com/CharlesShang/FastMaskRCNN/issues/159
https://github.com/tensorflow/serving/issues/627

So I will close this issue, if someone knows how to solve it, I will appreciate.
```
2018-05-13 00:55:05.452042: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument
2018-05-13 00:55:05.452913: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument
	 [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where[_device=""/job:localhost/replica:0/task:0/device:GPU:0""](fpn_maskrcnn_head/PyramidROIAlign/Equal/_777)]]
2018-05-13 00:55:05.453128: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument
	 [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where[_device=""/job:localhost/replica:0/task:0/device:GPU:0""](fpn_maskrcnn_head/PyramidROIAlign/Equal/_777)]]
2018-05-13 00:55:05.453190: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument
	 [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where[_device=""/job:localhost/replica:0/task:0/device:GPU:0""](fpn_maskrcnn_head/PyramidROIAlign/Equal/_777)]]
2018-05-13 00:55:05.453238: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument
	 [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where[_device=""/job:localhost/replica:0/task:0/device:GPU:0""](fpn_maskrcnn_head/PyramidROIAlign/Equal/_777)]]
Traceback (most recent call last):
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
    return fn(*args)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument
	 [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where[_device=""/job:localhost/replica:0/task:0/device:GPU:0""](fpn_maskrcnn_head/PyramidROIAlign/Equal/_777)]]
	 [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where/_779 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2167_fpn_maskrcnn_head/PyramidROIAlign/Where"", tensor_type=DT_INT64, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/chaoli/PycharmProjects/SuperCode/tensorpack-master/Tensorpack_Examples/Humanpose/test.py"", line 153, in <module>
    print(sess.run(mrcnn_loss, feed_dict=feed_datas))
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument
	 [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where[_device=""/job:localhost/replica:0/task:0/device:GPU:0""](fpn_maskrcnn_head/PyramidROIAlign/Equal/_777)]]
	 [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where/_779 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2167_fpn_maskrcnn_head/PyramidROIAlign/Where"", tensor_type=DT_INT64, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'fpn_maskrcnn_head/PyramidROIAlign/Where', defined at:
  File ""/home/chaoli/PycharmProjects/SuperCode/tensorpack-master/Tensorpack_Examples/Humanpose/test.py"", line 113, in <module>
    config.MASK_POOL_SIZE, config.NUM_CLASS, config.ANCHOR_STRIDES)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorpack/tfutils/scope_utils.py"", line 113, in wrapper
    return func(*args, **kwargs)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorpack/tfutils/scope_utils.py"", line 52, in wrapper
    return func(*args, **kwargs)
  File ""/home/chaoli/PycharmProjects/SuperCode/tensorpack-master/Tensorpack_Examples/Humanpose/model.py"", line 617, in fpn_maskrcnn_head
    roi_features = PyramidROIAlign(rois, fpn_features, pool_size, features_strides)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorpack/tfutils/scope_utils.py"", line 84, in wrapper
    return func(*args, **kwargs)
  File ""/home/chaoli/PycharmProjects/SuperCode/tensorpack-master/Tensorpack_Examples/Humanpose/model.py"", line 572, in PyramidROIAlign
    index = tf.where(tf.equal(leves,level))[:,0]
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 2439, in where
    return gen_array_ops.where(input=condition, name=name)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 5930, in where
    ""Where"", input=input, name=name)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InternalError (see above for traceback): WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument
	 [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where[_device=""/job:localhost/replica:0/task:0/device:GPU:0""](fpn_maskrcnn_head/PyramidROIAlign/Equal/_777)]]
	 [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where/_779 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2167_fpn_maskrcnn_head/PyramidROIAlign/Where"", tensor_type=DT_INT64, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]


Process finished with exit code 1
```",used train function train model related related close issue someone solve appreciate internal could launch cub count number true index status invalid configuration argument internal could launch cub count number true index status invalid configuration argument node internal could launch cub count number true index status invalid configuration argument node internal could launch cub count number true index status invalid configuration argument node internal could launch cub count number true index status invalid configuration argument node recent call last file line return file line status file line could launch cub count number true index status invalid configuration argument node node handling exception another exception recent call last file line module print file line run file line file line file line raise type message could launch cub count number true index status invalid configuration argument node node defined file line module file line wrapper return file line wrapper return file line file line wrapper return file line index level file line return file line file line file line file line see could launch cub count number true index status invalid configuration argument node node process finished exit code,issue,positive,positive,positive,positive,positive,positive
388583385,"It's a tensorflow issue that `tf.layers.batch_normalization` cannot be used inside conditionals. See https://github.com/tensorflow/tensorflow/issues/14809 and https://github.com/tensorflow/tensorflow/issues/14699.

Using tensorpack BatchNorm layer's `ema_update` option can probably solve this.",issue used inside see layer option probably solve,issue,negative,neutral,neutral,neutral,neutral,neutral
388535595,"The error message doesn't tell a lot. And since the original example is working (I assume) and I have no idea what you wrote in the code, I can't tell what went wrong from the error message. It feels like some error with the device placement.

Perhaps try removing `AccumGradOptimizer` and try again. From the error log it seems related. Or you can try a newer version of tensorflow.

Anyway if you have the unmodified version working, you should be able to find out a minimal set of changes needed to produce this error so people will be able to help.",error message tell lot since original example working assume idea wrote code ca tell went wrong error message like error device placement perhaps try removing try error log related try version anyway unmodified version working able find minimal set produce error people able help,issue,negative,positive,positive,positive,positive,positive
388521498,"Another thing.
I have tried to train imagenet on caffe2/caffe/pytorch. And never get close to the reported accuracy. The learning curve is different.
I can only get 72% top 1 accuracy. The `inception-style` data augmentation is used. I only have 4 titanX, so I adjusted the learning rate according to the `linear scaling rule`. No help either.

The tensorpack seems to be the chance. Hope it works great.",another thing tried train never get close accuracy learning curve different get top accuracy data augmentation used learning rate according linear scaling rule help either chance hope work great,issue,positive,positive,positive,positive,positive,positive
388519429,"It's weird. Pytorch and Caffe2 works well.
The TF was installed through pip. I will try to compile from source.",weird work well pip try compile source,issue,negative,negative,negative,negative,negative,negative
388412243,"Some possibilities: your TF does not support GPU. Your GPU was occupied or has other errors.
Posting __full__ logs would be more helpful.",support posting would helpful,issue,positive,neutral,neutral,neutral,neutral,neutral
388404671,I have some code to this lying around somewhere. I can add it in at some point.,code lying around somewhere add point,issue,negative,neutral,neutral,neutral,neutral,neutral
387890192,"Thanks. Unfortunately, `keras.py` is not covered by tests and CI is not set up in the GitHub repo. Existing tests are green. I only tried that in my Jupyter notebook and went fine - it just uncovered an unrelated exception that caused the variable being unassigned.",thanks unfortunately covered set green tried notebook went fine uncovered unrelated exception variable unassigned,issue,negative,negative,neutral,neutral,negative,negative
387814980,"DataFlow is meant to be stateless when they could. i.e., calling `get_data` twice is allowed and is designed to return two iterators that have no direct relationship with each other. Therefore the behavior you're seeing is expected. Certain DataFlow cannot be made stateless and therefore they are marked not reentrant.

Since DataFlow is stateless, then the one and the only consumer of a DataFlow can keep ownership and reuse a data point. Therefore to do this you need to use something like `MapData(indices, lambda x: make_features(x) + make_targets(x))`.

You can also write a custom version of something like `DataFlowFromFixedList` that does not shuffle inside `get_data` at the beginning, but in a `shuffle()` method which you can call when you need. 

I think considering it as a graph doesn't help and its behavior is indeed different from tensorflow iterators. They are just iterators.",meant stateless could calling twice designed return two direct relationship therefore behavior seeing certain made stateless therefore marked since stateless one consumer keep ownership reuse data point therefore need use something like index lambda also write custom version something like shuffle inside beginning shuffle method call need think considering graph help behavior indeed different,issue,positive,positive,positive,positive,positive,positive
387576179,"You didn't paste the full log, I think.

The sess.run actually succeeded, otherwise it's a tensorflow bug. You can ignore the error and still get the output. And in an actual training you'll not see such an error.
The error comes because you use a constant as input so the graph optimizer tries to first run everything on CPU as a constant folding step.",paste full log think actually otherwise bug ignore error still get output actual training see error error come use constant input graph first run everything constant folding step,issue,negative,positive,neutral,neutral,positive,positive
387566843,"No worry. I was studying the lib and IntelliJ Idea spelchecker was nagging from time to time. Anyway, awesome work!",worry idea nagging time time anyway awesome work,issue,negative,positive,positive,positive,positive,positive
387566408,Thanks! OMG I didn't know I had made so many typos.,thanks know made many,issue,negative,positive,positive,positive,positive,positive
387560881,@Superlee506 When reporting problems please following the issue template. Thanks!,please following issue template thanks,issue,positive,positive,neutral,neutral,positive,positive
387548445,"@ppwwyyxx 
When I tested the ""roi_align"" function, I also meet this problem and I used GPU.
`2018-05-08 17:24:44.425156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:85:00.0, compute capability: 3.7)
2018-05-08 17:24:44.624317: E tensorflow/core/common_runtime/executor.cc:643] Executor failed to create kernel. Invalid argument: Default AvgPoolingOp only supports NHWC.
	 [[Node: roi_align/AvgPool = AvgPool[T=DT_FLOAT, data_format=""NCHW"", ksize=[1, 1, 2, 2], padding=""SAME"", strides=[1, 1, 2, 2], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](roi_align/crop_and_resize/transpose_1)]]`",tested function also meet problem used device device name bus id compute capability executor create kernel invalid argument default node,issue,negative,neutral,neutral,neutral,neutral,neutral
387472460,If I use four GPUs each with batch size of 16. Will that be similar to run the network on one GPU with batch size of 64?,use four batch size similar run network one batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
387448947,You can use small batch size per GPU.,use small batch size per,issue,negative,negative,negative,negative,negative,negative
387128602,"Which links are you talking about?
http://tensorpack.readthedocs.io/ works fine.",link talking work fine,issue,negative,positive,positive,positive,positive,positive
386894781,"You can either try to formulate the problem to a single-cost optimization problem with `tf.cond` or `tf.control_dependencies`. If this is not possible, you can define the training step yourself, like the GAN trainer examples or the code above.",either try formulate problem optimization problem possible define training step like gan trainer code,issue,negative,neutral,neutral,neutral,neutral,neutral
386875109,"Hi,
I'd like to implement NN model with two cost functions.
First part - network will classify input data - tune all weights (W) by minimizing weighted logloss.
Second part will take some of these weights (W), apply linear algebra functions and minimize result (L2 or L1 loss). Second part should work on the (W) also.
What kind of approach (your examples) will you recommend?",hi like implement model two cost first part network input data tune weighted second part take apply linear algebra minimize result loss second part work also kind approach recommend,issue,positive,positive,positive,positive,positive,positive
386849007,Some checks were added to fix the collection after calling Keras models. Variables are now marked trainable iff it was marked trainable by Keras AND it appears in Keras's `model.weights`. This seems to make ConvLSTM work and produce a more reasonable warning.,added fix collection calling marked trainable marked trainable make work produce reasonable warning,issue,negative,positive,positive,positive,positive,positive
386768372,I'll take a look later. Maybe there are ways to handle this better.,take look later maybe way handle better,issue,negative,positive,positive,positive,positive,positive
386767649,In the Conv2DLSTM case I think it happens to do the correct thing. Because tensorpack trainer just ignores this variable. But I'm not totally sure.,case think correct thing trainer variable totally sure,issue,negative,positive,positive,positive,positive,positive
386765154,Thanks for the clarifying. then what does it actually do when i use multigpu to run the script? is it only tower0 works properly and tower1 and tower2 actually do nothing useful?,thanks actually use run script tower work properly tower tower actually nothing useful,issue,negative,positive,neutral,neutral,positive,positive
386759335,"The problem comes from the following code in ConvRNN2D:
```
  def get_initial_state(self, inputs):
    # (samples, timesteps, rows, cols, filters)
    initial_state = K.zeros_like(inputs)
    # (samples, rows, cols, filters)
    initial_state = K.sum(initial_state, axis=1)
    shape = list(self.cell.kernel_shape)
    shape[-1] = self.cell.filters
    initial_state = self.cell.input_conv(initial_state,
                                         K.zeros(tuple(shape)),
                                         padding=self.cell.padding)

    if hasattr(self.cell.state_size, '__len__'):
      return [initial_state for _ in self.cell.state_size]
    else:
      return [initial_state]
```
It calls `K.zeros`, which in my test case creates a all-zero VARIABLE
instead of a all-zero constant TENSOR.

This abuse of variables by itself looks like a bad choice because it hurt
performance and memory.

It does not create this variable with the variable scope name, but still
put the the variable into TRAINABLE_VARIABLES collection, which breaks a
lot of contract of a ""tower function"". Therefore such symbolic functions
cannot be used with tensorpack.
",problem come following code self shape list shape shape return else return test case variable instead constant tensor abuse like bad choice hurt performance memory create variable variable scope name still put variable collection lot contract tower function therefore symbolic used,issue,negative,negative,negative,negative,negative,negative
386747445,"I checked a bit, it looks like it is not the bidirectional wrapper that has problem, it is the convlstm2d layer in keras, i am not sure if you can reproduce the error using this one https://github.com/keras-team/keras/blob/master/examples/conv_lstm.py

If the variables are incompatible, then why can I still get results? is there any problem with the results I got? ",checked bit like bidirectional wrapper problem layer sure reproduce error one incompatible still get problem got,issue,negative,positive,positive,positive,positive,positive
386744870,"It's best if you could __paste__ some relevant code. They're always more helpful than words. Now it looks like some Keras layers create variables in a way incompatible with tensorpack's multigpu trainers. There may be solutions but I don't know how to reproduce such error now.

Also, full logs are more helpful than xxxxxx. If you prefer you can use 

```
<details>
<summary>Click to expand</summary>
logs
</details>
```
so that they don't take too much space.",best could relevant code always helpful like create way incompatible may know reproduce error also full helpful prefer use summary click expand take much space,issue,positive,positive,positive,positive,positive,positive
386742405,"Are you using some functions from tensorflow to create these bidirectional rnn variables or are you creating them with your own symbolic code? If it's from tensorflow, what is the function?",create bidirectional symbolic code function,issue,negative,neutral,neutral,neutral,neutral,neutral
386740195,"xxxxxxxxxxxxxx
[0504 13:37:58 @training.py:100] Building graph for training tower 1 on device /gpu:1 ...
xxxxxxxxxxxxxxx
[0504 13:39:42 @training.py:100] Building graph for training tower 2 on device /gpu:2 ...
xxxxxxxxxxxxxxx
[0504 13:42:36 @training.py:284] WRN [SyncMultiGPUReplicatedBuilder] variable tower0/bidirectional/Variable:0 has prefix 'tower0', this is unexpected.
xxxxxxxxxxxxxxxxxxxxxxxxxxxx
[0504 13:42:36 @training.py:284] WRN [SyncMultiGPUReplicatedBuilder] variable tower0/bidirectional/Variable/Adam:0 has prefix 'tower0', this is unexpected.
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
[0504 13:42:36 @training.py:297] WRN [ReplicatedTrainer] Cannot find bidirectional/conv_lst_m2d_16/kernel:0 in the graph!
[0504 13:42:36 @training.py:297] WRN [ReplicatedTrainer] Cannot find bidirectional/conv_lst_m2d_16/recurrent_kernel:0 in the graph!
xxxxxxxxxxxxxxxxxxxxxxxx
[0504 13:42:36 @training.py:299] 'sync_variables_from_main_tower' includes 606 operations.

i use xxxxxxxxxxx to replace similar messages",building graph training tower device building graph training tower device variable prefix unexpected variable prefix unexpected find graph find graph use replace similar,issue,negative,positive,neutral,neutral,positive,positive
386560065,"> iff `bytes` was necessary for msgpack

It is really about ""if and only if"".

I am not using the git-HEAD of tensorpack anymore since pyarrow was used as the default and installed by the requirements. So I did not test it and just saw this change.

Basically all LMDBs which took quite some time to build here are serialized using msgpack. I went through some trouble when pyarrow was set to the default.

I highly vote along these changes for introducing an ENV-var -- call it `TENSORPACK_SERIALIZE=pyarrow/msgpack`  or so. De-installing the pyarrow package is not an acceptable solution here when relying on `loads` with `msgpack`.
I reverted back to an old version which did not break data loading.

edit: I now fear when seeing some changes applied to the returned value from `loads`.",necessary really since used default test saw change basically took quite time build went trouble set default highly vote along call package acceptable solution back old version break data loading edit fear seeing applied returned value,issue,negative,positive,neutral,neutral,positive,positive
386556137,Do you have a failure case? I don't remember what the issue was.,failure case remember issue,issue,negative,negative,negative,negative,negative,negative
386552922,":scream: : Could this please handled by the specific `load` function implementation itself rather than directly removing `bytes`, iff `bytes` was necessary for msgpack. Currently, new tensorpack versions would break all existing code relying on msgpack.

",scream could please handled specific load function implementation rather directly removing necessary currently new would break code,issue,negative,positive,neutral,neutral,positive,positive
386181262,"OK now I understand. This is related to issue #668 and can probably be solved in the master branch if you've installed the ""python-prctl"" package.",understand related issue probably master branch package,issue,negative,neutral,neutral,neutral,neutral,neutral
386180720,"1. press ctrl-c can quit training process as normal. 
2. all error just as I posted before.  Using linux bash 'top', could see the training thread are not terminated, had to kill the thread by command
kill `ps -ef|grep python | grep -v grep|awk '{print $2}'`",press quit training process normal error posted bash could see training thread kill thread command kill python print,issue,negative,positive,positive,positive,positive,positive
386178332,"What will happen if you press ctrl-c a lot of times? --- Does this give you back to the command line or not?

And by saying ""Can not start the training precedure again"" what exactly did you observe? Does this give the error you've posted above or something else? Could you post the error?",happen press lot time give back command line saying start training exactly observe give error posted something else could post error,issue,negative,positive,positive,positive,positive,positive
386176854,"Thanks for your reply!

1 . What will happen if you don't press ctrl-c ?
Run normal, no error happends.
2. What will happen if you press ctrl-c a lot of times?
Change LearningRate or want to re-training another model, using ctrl-c to interupt previous trainning code. Can not start the training precedure again, must kill the tensorpack thread by command.",thanks reply happen press run normal error happen press lot time change want another model previous code start training must kill thread command,issue,negative,positive,neutral,neutral,positive,positive
386072980,"Please clarify:
What will happen if you don't press ctrl-c ?
What will happen if you press ctrl-c a lot of times?",please clarify happen press happen press lot time,issue,negative,neutral,neutral,neutral,neutral,neutral
385911652,"> The error occured when re-train the model or using ctrl-c to interupt the training precedure

I don't understand what do you mean by the ""or"". What exactly did you do to see this log? ",error model training understand mean exactly see log,issue,negative,negative,neutral,neutral,negative,negative
385537183,"There were some changes in the the arguments of ""tower function"" that was not updated in the Keras example.
Now with the latest code, you should be able to build a predictor in mnist-keras-v2.py like this:
```python
    pred = PredictConfig(
        tower_func=model_func, 
        inputs_desc=[
            InputDesc(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, 1], 'images')
        ],
        input_names=['images'],
        output_names=['activation/Softmax']
    )
    pred = OfflinePredictor(pred)
```
To get tensor names from Keras, use something like `print(M.layers[10].output`.",tower function example latest code able build predictor like python none get tensor use something like print,issue,positive,positive,positive,positive,positive,positive
385528767,I found some other issues using offlinepredictor for Keras models. I'll try to do a quick fix.,found try quick fix,issue,negative,positive,positive,positive,positive,positive
385526660,"By ""tensor"" I mean `tf.Tensor`. A layer is not a tensor. Therefore a layer's name is not a tensor's name.
I just went through Keras's container API, and it looks like you can access a tensor with something like `M.layers[10].output`. There you can see the name of the tensor.",tensor mean layer tensor therefore layer name tensor name went container like access tensor something like see name tensor,issue,positive,negative,negative,negative,negative,negative
385525402,"sorry I am a bit confused, so you mean print(M.summary()) does not necessarily print the corresponding tensor name? I can use print(M.layers[10].name) but it still outputs ""activation_1"", what do you mean by print the tensor name? can you please directly give an offlinepredictor example that can be used in the mnist-v2? ",sorry bit confused mean print necessarily print corresponding tensor name use print still mean print tensor name please directly give example used,issue,negative,negative,negative,negative,negative,negative
385525330,Oh if you're using the mnist-keras-v2.py example -- it's not written with Keras's functional API (unlike imagenet-resnet-keras.py example). It is not very obvious to me how to get tensor's name with Keras's container API.,oh example written functional unlike example obvious get tensor name container,issue,negative,neutral,neutral,neutral,neutral,neutral
385522743,"What Keras prints does not necessarily correspond to tensor names. It's up to Keras what it prints and I have no control over that.
You can use `print` (the python builtin function) to see the name of a tensor.",necessarily correspond tensor control use print python function see name tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
385522169,"so I used the mnist-keras-v2.py to do the test, i print the model summary:

conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 14, 14, 32)        9248      
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 14, 14, 32)        9248      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 7, 7, 32)          9248      
_________________________________________________________________
flatten_1 (Flatten)          (None, 1568)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               803328    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                5130      
_________________________________________________________________
activation_1 (Activation)    (None, 10)                0         


Total params: 836,522
Trainable params: 836,522
Non-trainable params: 0

-------------------------------------------------------------------------------------------------------------------
Then I directly use the ""activation_1"" layer name as the output name, but I still got the error:
KeyError: ""The name 'activation_1:0' refers to a Tensor which does not exist. The operation, 'activation_1', does not exist in the graph.""

But you can see activation_1 is in the model.

Any ideas why?
",used test print model summary none none none none none none flatten none dense none dropout none dense none activation none total trainable directly use layer name output name still got error name tensor exist operation exist graph see model,issue,negative,positive,neutral,neutral,positive,positive
385517968,"Names are defined by Keras so I have no control. You can use `print` to see the name of a tensor, or use `tf.identity` to give a tensor an alias name.",defined control use print see name tensor use give tensor alias name,issue,negative,neutral,neutral,neutral,neutral,neutral
385514924,"Yes, I tried, but I don't specifically know what to put in the ""output_names"", since the variables are in Keras mode. I tried directly use the Keras layer's name, but it said:
KeyError: ""The name 'label:0' refers to a Tensor which does not exist. The operation, 'label', does not exist in the graph.""
",yes tried specifically know put since mode tried directly use layer name said name tensor exist operation exist graph,issue,negative,positive,neutral,neutral,positive,positive
385197055,Yes. The anchor code is copied elsewhere which used a different definition of box.,yes anchor code copied elsewhere used different definition box,issue,negative,neutral,neutral,neutral,neutral,neutral
385196750,"@ppwwyyxx @sharpstill Hi, I know about the ""generate_anchors"" function, but I have a little confused about [Line 68](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/data.py#L68) in ""get_all_anchors"". Why do we need to add 1 as below:
![image](https://user-images.githubusercontent.com/13114675/39399701-90a345ac-4af0-11e8-9b23-41b24f1937d7.png)

Is it related with the float box you mentioned？ But I'm still confused.  Any suggestion would be appreciated.
",hi know function little confused line need add image related float box still confused suggestion would,issue,negative,negative,negative,negative,negative,negative
385180889,"Sorry, I used some old codes. It works perfectly now.",sorry used old work perfectly,issue,positive,positive,positive,positive,positive,positive
385137331,"You are right! I just find out the reason, the initializer is defined to initialize variables with scope 'metric_scope'. But the correct scope should be 'InferenceTower/metric_scope'",right find reason defined initialize scope correct scope,issue,negative,positive,positive,positive,positive,positive
385136695,"The ops definitely will run when you call sess.run
So the only explanation I see is that the op does not do what you expected it to do.",definitely run call explanation see,issue,negative,neutral,neutral,neutral,neutral,neutral
385134837,I also tried to initialize all local variables by `sess.run(tf.local_variables_initializer())`. But it did not work.,also tried initialize local work,issue,negative,neutral,neutral,neutral,neutral,neutral
385133989,PredictConfig can take a tower function and InputDesc. Both are already arguments of KerasModel in the example.,take tower function already example,issue,negative,neutral,neutral,neutral,neutral,neutral
385132522,"@qinhuan Yes, but performance dropped to 0.39 using 4 gpus. There maybe some bugs in my implementation :(",yes performance maybe implementation,issue,negative,neutral,neutral,neutral,neutral,neutral
384824875,I will try. Thanks very much for your help.,try thanks much help,issue,positive,positive,positive,positive,positive,positive
384761291,The long type problem should be fixed by the early commit. I was asking about other issues (if any).,long type problem fixed early commit,issue,negative,positive,neutral,neutral,positive,positive
384747018,"
ESC[32m[0426 10:25:23 @logger.py:74]ESC[0m Argv: imagenet-resnet.py --data ../../../imageNet2012/ --gpu 2,3 -d 18
ESC[32m[0426 10:25:23 @fs.py:89]ESC[0m ESC[5mESC[31mWRNESC[0m Env var $TENSORPACK_DATASET not set, using /home/testing/tensorpack_data for datasets.
ESC[32m[0426 10:25:24 @parallel.py:290]ESC[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
ESC[32m[0426 10:25:24 @param.py:195]ESC[0m Use train_log/imagenet-resnet/hyper.txt to set hyperparam: 'learning_rate'.
ESC[32m[0426 10:25:24 @config.py:166]ESC[0m ESC[5mESC[31mWRNESC[0m TrainConfig.nr_tower was deprecated! Set the number of GPUs on the trainer instead!
ESC[32m[0426 10:25:24 @config.py:167]ESC[0m ESC[5mESC[31mWRNESC[0m See https://github.com/ppwwyyxx/tensorpack/issues/458 for more information.
ESC[32m[0426 10:25:24 @base.py:345]ESC[0m ESC[5mESC[31mWRNESC[0m You're calling new trainers with old trainer API!
ESC[32m[0426 10:25:24 @base.py:346]ESC[0m ESC[5mESC[31mWRNESC[0m Now it returns the old trainer for you, please switch to use new trainers soon!
ESC[32m[0426 10:25:24 @base.py:347]ESC[0m ESC[5mESC[31mWRNESC[0m See https://github.com/ppwwyyxx/tensorpack/issues/458 for more information.
ESC[32m[0426 10:25:25 @input_source.py:194]ESC[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
ESC[32m[0426 10:25:25 @training.py:42]ESC[0m [DataParallel] Training a model of 2 towers.
ESC[32m[0426 10:25:25 @training.py:102]ESC[0m Building graph for training tower 0 ...
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m conv0 input: [None, 3, 448, 224]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m conv0 output: [None, 64, 224, 112]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m pool0 input: [None, 64, 224, 112]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m pool0 output: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block0/conv1 input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block0/conv1 output: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block0/conv2 input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block0/conv2 output: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block1/conv1 input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block1/conv1 output: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block1/conv2 input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block1/conv2 output: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block0/conv1 input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block0/conv1 output: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block0/conv2 input: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block0/conv2 output: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block0/convshortcut input: [None, 64, 112, 56]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block0/convshortcut output: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block1/conv1 input: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block1/conv1 output: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block1/conv2 input: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block1/conv2 output: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block0/conv1 input: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block0/conv1 output: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block0/conv2 input: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block0/conv2 output: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block0/convshortcut input: [None, 128, 56, 28]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block0/convshortcut output: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block1/conv1 input: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block1/conv1 output: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block1/conv2 input: [None, 256, 28, 14]
ESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block1/conv2 output: [None, 256, 28, 14]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block0/conv1 input: [None, 256, 28, 14]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block0/conv1 output: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block0/conv2 input: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block0/conv2 output: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block0/convshortcut input: [None, 256, 28, 14]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block0/convshortcut output: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block1/conv1 input: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block1/conv1 output: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block1/conv2 input: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block1/conv2 output: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m gap input: [None, 512, 14, 7]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m gap output: [None, 512]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m linear input: [None, 512]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m linear output: [None, 1000]
ESC[32m[0426 10:25:26 @registry.py:121]ESC[0m linear_2 input: [None, 512]
ESC[32m[0426 10:25:26 @registry.py:129]ESC[0m linear_2 output: [None, 1000]
ESC[32m[0426 10:25:26 @regularize.py:88]ESC[0m regularize_cost() found 22 variables to regularize.
ESC[32m[0426 10:25:26 @regularize.py:19]ESC[0m The following tensors will be regularized: conv0/W:0, group0/block0/conv1/W:0, group0/block0/conv2/W:0, group0/block1/conv1/W:0, group0/block1/conv2/W:0, group1/block0/conv1/W:0, group1/bloc
k0/conv2/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group3/bl
ock0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, linear/W:0, linear_2/W:0
ESC[32m[0426 10:25:26 @training.py:102]ESC[0m Building graph for training tower 1 ...

group3/block0/conv2/W:0          [3, 3, 512, 512]  2359296
group3/block0/convshortcut/W:0   [1, 1, 256, 512]   131072
group3/block1/preact/bn/gamma:0  [512]                 512
group3/block1/preact/bn/beta:0   [512]                 512
group3/block1/conv1/W:0          [3, 3, 512, 512]  2359296
group3/block1/conv1/bn/gamma:0   [512]                 512
group3/block1/conv1/bn/beta:0    [512]                 512
group3/block1/conv2/W:0          [3, 3, 512, 512]  2359296
bnlast/bn/gamma:0                [512]                 512
bnlast/bn/beta:0                 [512]                 512
linear/W:0                       [512, 1000]        512000
linear/b:0                       [1000]               1000
linear_2/W:0                     [512, 1000]        512000
linear_2/b:0                     [1000]               1000ESC[36m
Total #vars=58, #params=12200720, size=46.54MBESC[0m
ESC[32m[0426 10:25:27 @base.py:142]ESC[0m Setup callbacks graph ...
ESC[32m[0426 10:25:27 @predict.py:42]ESC[0m Building predictor tower 'InferenceTower' on device /gpu:0 ...
ESC[32m[0426 10:25:27 @collection.py:165]ESC[0m These collections were modified but restored in InferenceTower: (tf.GraphKeys.SUMMARIES: 7->8)
ESC[32m[0426 10:25:27 @summary.py:39]ESC[0m Maintain moving average summary of 4 tensors in collection MOVING_SUMMARY_OPS.
ESC[32m[0426 10:25:27 @summary.py:76]ESC[0m Summarizing collection 'summaries' of size 7.
ESC[32m[0426 10:25:27 @graph.py:92]ESC[0m Applying collection UPDATE_OPS of 34 ops.
ESC[32m[0426 10:25:32 @base.py:147]ESC[0m Creating the session ...
ESC[32m[0426 10:25:39 @base.py:151]ESC[0m Initializing the session ...
ESC[32m[0426 10:25:39 @base.py:158]ESC[0m Graph Finalized.
ESC[32m[0426 10:25:40 @inference_runner.py:100]ESC[0m InferenceRunner will eval 782 iterations
ESC[32m[0426 10:25:40 @concurrency.py:38]ESC[0m Starting EnqueueThread QueueInput/input_queue ...
ESC[32m[0426 10:25:40 @base.py:192]ESC[0m Start Epoch 1 ...
ESC[32m[0426 10:25:40 @input_source.py:502]ESC[0m Pre-filling StagingArea ...
ESC[32m[0426 10:25:40 @input_source.py:143]ESC[0m ESC[4mESC[5mESC[31mERRESC[0m Exception in EnqueueThread QueueInput/input_queue:
Traceback (most recent call last):
  File ""/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/input_source/input_source.py"", line 133, in run
    dp = next(self._itr)
  File ""/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py"", line 339, in get_data
    for dp in self.ds.get_data():
  File ""/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py"", line 274, in get_data
    for dp in self.ds.get_data():
  File ""/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py"", line 119, in get_data
    yield BatchData._aggregate_batch(holder, self.use_list)
  File ""/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py"", line 142, in _aggregate_batch
    raise TypeError(""Unsupported type to batch: {}"".format(type(dt)))
TypeError: Unsupported type to batch: <type 'long'>
ESC[32m[0426 10:25:40 @input_source.py:149]ESC[0m EnqueueThread QueueInput/input_queue Exited.
ESC[32m[0426 10:25:40 @base.py:208]ESC[0m Training was stopped.

",data set fork one time use set set number trainer instead see information calling new old trainer old trainer please switch use new soon see information setting queue training model building graph training tower input none output none pool input none pool output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none input none output none gap input none gap output none linear input none linear output none input none output none found regularize following building graph training tower total setup graph building predictor tower device maintain moving average summary collection collection size collection session session graph starting start epoch exception recent call last file line run next file line file line file line yield holder file line raise unsupported type batch type unsupported type batch type training stopped,issue,negative,positive,neutral,neutral,positive,positive
384730200,"Looks like there are a couple of problems with this kmeans.

First of all it creates trainable variables of unknown shape. Some of the utilities in tensorpack assume variables have fully defined shape. The above commit should help with that.
Secondly it does not respect variable reuse. It creates variables even under a ""reuse=True"" variable scope, which means you cannot use it with tensorpack [TowerTrainer](http://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html#tower-trainer) and all its subclasses.

Thirdly it is not trained by gradient descent, although it creates trainable variables. This means you cannot train it with tensorpack [ModelDesc](http://tensorpack.readthedocs.io/en/latest/tutorial/training-interface.html#with-modeldesc-and-trainconfig) which assumes single-cost gradient-based optimization.

To use kmeans you'll probably need to build your graph manually and [write your own trainer](http://tensorpack.readthedocs.io/en/latest/tutorial/extend/trainer.html). I don't know if there are other ways around because I'm not familiar with the kmeans module.",like couple first trainable unknown shape assume fully defined shape commit help secondly respect variable reuse even variable scope use thirdly trained gradient descent although trainable train optimization use probably need build graph manually write trainer know way around familiar module,issue,positive,positive,positive,positive,positive,positive
384712626,Could you please post full error?,could please post full error,issue,negative,positive,positive,positive,positive,positive
384710661,"The ""long"" issue is a python2 specific issue that should be fixed in the last commit.
Could you post your error log?",long issue python specific issue fixed last commit could post error log,issue,negative,positive,neutral,neutral,positive,positive
384659168,"Thanks! but it seems I still got the same error message. Also, on another machine I got a different message: /lib/python2.7/site-packages/tensorpack/dataflow/common.py"", line 142, in _aggregate_batch
    raise TypeError(""Unsupported type to batch: {}"".format(type(dt)))
TypeError: Unsupported type to batch: <type 'long'>

I use TF1.4+Python2.7",thanks still got error message also another machine got different message line raise unsupported type batch type unsupported type batch type use,issue,negative,positive,neutral,neutral,positive,positive
384443496,`virtual_batch_size` is only available after TF 1.5. Now the code won't touch this option if running under TF1.4.,available code wo touch option running,issue,negative,positive,positive,positive,positive,positive
384425709,Thanks for reporting. Looks like I was using a feature unavailable in TF 1.4. I'll update the code soon.,thanks like feature unavailable update code soon,issue,positive,positive,positive,positive,positive,positive
384404838,Thanks. I meant the implementation that achieves similar performance as that of the paper.,thanks meant implementation similar performance paper,issue,negative,positive,neutral,neutral,positive,positive
384395701,No. I believe it's trivial to do if one has already learned tensorflow.,believe trivial one already learned,issue,negative,neutral,neutral,neutral,neutral,neutral
384356808,"The model is pure tensorflow code and you change it in the same way you change a tensorflow model. For example you may want to change MODULE_SIZES in the for loop.

The two scripts are not comparable at all even with the same number of layers because the architecture have many other differences.",model pure code change way change model example may want change loop two comparable even number architecture many,issue,negative,positive,positive,positive,positive,positive
383758305,FYI I've included the implementation of `Trained Ternary Quantization` paper inside tensorpack examples now: https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net,included implementation trained ternary quantization paper inside,issue,negative,neutral,neutral,neutral,neutral,neutral
383367336,Do inferencer callback and the training graph use the same session?,training graph use session,issue,negative,neutral,neutral,neutral,neutral,neutral
383253289,"Regarding the original issue, turned out that PrefetchData does support windows, but windows has a more strict picklability requirement for processes, i.e. it requires to pickle `get_train_dataflow.preprocess`. Ref: https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods",regarding original issue turned support strict requirement pickle ref,issue,positive,positive,positive,positive,positive,positive
383164091,"Hi, I didn't properly reverted some code changes (the changes were about running inference on a CPU) on a remote computer (problems with `git --git-dir` command). Loading of the weights works fine on a GPU.",hi properly code running inference remote computer git command loading work fine,issue,negative,positive,positive,positive,positive,positive
383152515,`get_data` will be called in each process independently. In each process it is called the same way (called again after the iterator from the previous call reaches the end).,process independently process way previous call end,issue,negative,negative,neutral,neutral,negative,negative
383134934,"If PrefetchDataZMQ is used to start multiple processes to fetch data, will the `get_data` be called multiple times?",used start multiple fetch data multiple time,issue,negative,neutral,neutral,neutral,neutral,neutral
382631938,"Dataflow is just a python generator - it should have no built-in concept about ""training"" or ""machine learning"".

You can write a callback which, after every epoch, change some attributes of your dataflow.

Be careful with some of the parallelism, though. Some parallel dataflow (e.g. PrefetchDataZMQ) create forks of a dataflow, so setting attributes on the original dataflow will not be effective.",python generator concept training machine learning write every epoch change careful parallelism though parallel create setting original effective,issue,positive,positive,positive,positive,positive,positive
382539897,"Looks like MinSaver simply can not work with resume now.

The fundamental problem is that callbacks like MinSaver, JSONWriter are stateful. Therefore once the training was interrupted the state is lost. The way JSONWriter tries to recover its state when resuming is already kind of hacky. I will think if there is a better way to support stateful callbacks.",like simply work resume fundamental problem like stateful therefore training interrupted state lost way recover state already kind hacky think better way support stateful,issue,positive,positive,positive,positive,positive,positive
382530933,"I see. Make sense! 

I think ""rotate"" means ""deleting old ones while adding new ones"" which is not really what's going on here. Maybe rename to ""split_files"" or ""split_events"" ?",see make sense think rotate old new really going maybe rename,issue,negative,positive,positive,positive,positive,positive
382448703,"Either you don't have a nvidia GPU or your tensorflow is not built with GPU support.
Please run
```python
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
```
And make sure you can see a GPU device.",either built support please run python import print make sure see device,issue,positive,positive,positive,positive,positive,positive
382447326,"Thanks for your reply.
And I change command :
`python train.py --load '/path/to/ImageNet-ResNet50.npz' --gpu 0`
Error is same as above.

> Default MaxPoolingOp only supports NHWC.",thanks reply change command python load error default,issue,negative,positive,positive,positive,positive,positive
382303560,"It does not take space but it does take money for transfer :)
Effectively, if the size (N) of a file grows linearly, the total transfer cost grows with O(N^2) in the current version of tensorpack. 
The option above lowers the cost back to O(N).

Flushing the file every now and than if no changes actually occur to the content of the file should not be a problem. I can check it if you wish. 

I was thinking, maybe I should rename the argument to sth like: rotate_logs ?",take space take money transfer effectively size file linearly total transfer cost current version option cost back flushing file every actually occur content file problem check wish thinking maybe rename argument like,issue,negative,positive,positive,positive,positive,positive
382188083,"You're right that it does not fit into the scope of augmentor. That's why it's better to implement it into the data pipeline.
For instance you can use `MapDataComponent` to transform each 3D image to a 4D tensor with crops.",right fit scope better implement data pipeline instance use transform image tensor,issue,positive,positive,positive,positive,positive,positive
382185648,"Will it be possible to provide some suggestions on how you would implement that with minimal disruption to the original model in tensorpack. It seems to do the 10-crop test/validation, we need to support a 4D tensor where the tensor consists of the same validation image cropped at different location to the model, then average out the softmax output. 

However, it seems that imgaug.ImageAugmentor requiring the output to be a 3D array.",possible provide would implement minimal disruption original model need support tensor tensor validation image different location model average output however output array,issue,negative,positive,neutral,neutral,positive,positive
381911069,"@tkuanlun350  Hi, Have you finished the code of FPN? Could you share the code? Thanks.",hi finished code could share code thanks,issue,positive,positive,positive,positive,positive,positive
381668345,"Interesting, that's indeed a problem. It's just `DataParallelOfflinePredictor` was never useful (it's not a good option for deployment) so I wasn't thinking about it.",interesting indeed problem never useful good option deployment thinking,issue,negative,positive,positive,positive,positive,positive
381411383,"Hi, here is one TensorFlow SE-ResNet with pre-trained weights converted from Caffe. You can try [this](https://github.com/HiKapok/TF_Se_ResNe_t) if you are interested in.",hi one converted try interested,issue,negative,positive,positive,positive,positive,positive
381348622,"OK. When versioning is turned off, each flush recreates the entire file (i.e., takes unnecessary extra time but no extra space), is this correct?

Also, `tf.summary.FileWriter` itself will internally flush the file once a while. Will that also be a problem for you?",turned flush entire file unnecessary extra time extra space correct also internally flush file also problem,issue,negative,negative,neutral,neutral,negative,negative
381343961,"Yes, each flush creates a new copy. However, it does not matter whether versioning is on or not.
I turned on versioning only to highlight the problem. ",yes flush new copy however matter whether turned highlight problem,issue,negative,positive,positive,positive,positive,positive
381228622,"If I understand it correctly, the issue is that each flush creates a new copy of the entire file due to versioning, right?
It looks like it's just because versioning doesn't make sense if you're appending to files. What if you turn versioning off?",understand correctly issue flush new copy entire file due right like make sense turn,issue,negative,positive,neutral,neutral,positive,positive
381071566,"I am not sure if setting logdir to sth on gcs works right for tensorpack now although this could be a viable option. It does work with pure tensorflow.

Assuming it works (which is likely), if you set logdir to gs:// each time a file is flushed the whole file is transferred to gcs. 

You can test it for yourself if you create a bucket YOUR_VERSIONED_BUCKET,
enable object versioning on GCS: `gsutil versioning set on YOUR_VERSIONED_BUCKET`

and use the code below:
```
import tensorflow as tf
import time

fw = tf.summary.FileWriter(YOUR_VERSIONED_BUCKED)
for i in range(5):
  value = i
  s = tf.Summary(value=[tf.Summary.Value(tag=""summary_tag"", simple_value=value),
])
  fw.add_summary(s)
  fw.flush()
  print(""Sleeping"")
  time.sleep(2)```

and then when you verify: `gsutil ls -a YOUR_VERSIONED_BUCKET` you get sth like:

```YOUR_VERSIONED_BUCKET/test-event/events.out.tfevents.1523609555.Maciejs-MBP-2.waw.nomagic.io#1523609556432534
YOUR_VERSIONED_BUCKET/test-event/events.out.tfevents.1523609555.Maciejs-MBP-2.waw.nomagic.io#1523609557845069
...```",sure setting work right although could viable option work pure assuming work likely set time file whole file transferred test create bucket enable object set use code import import time range value print sleeping verify get like,issue,positive,positive,positive,positive,positive,positive
380992911,"Not a tensorpack question, so closing.
Stereo-Pose-Machines is not maintained any more as mentioned in its readme. The code is there for a reference if anyone is interested in studying such projects but this means people will need to figure out the code by themselves. It requires specific hardwares so it's not meant for anyone to actually run. 
It is developed around Dec 2016 with tensorpack version at that time. The error you saw is due to an API change half a year ago, which can probably be fixed in https://github.com/ppwwyyxx/Stereo-Pose-Machines/commit/cadb5d057f224e5d53d17c33e776b5296ca7eccf,  following [tensorpack changelog](https://github.com/ppwwyyxx/tensorpack/blob/master/CHANGES.md).
cpm.npy was the model we trained for that project that is not available now.",question code reference anyone interested people need figure code specific meant anyone actually run around version time error saw due change half year ago probably fixed following model trained project available,issue,negative,positive,neutral,neutral,positive,positive
380899165,"OK. Just to clarify what you're trying to achieve: how are people expected to use this feature? Setting `logdir` to somewhere on gcs?

If yes, what happened when you set `logdir` to gcs without this feature?",clarify trying achieve people use feature setting somewhere yes set without feature,issue,negative,neutral,neutral,neutral,neutral,neutral
380768986,"A non-appendable filesystem is what GCS, S3 etc. provide as object based filesystems scale much better without sacrificing resiliency ([more](https://datastudio.google.com/reporting/15RHc28q7bPfkLzZH9JjMuykqPPH7lho4/page/E2oC)).

After applying this patch, our average cost of uploading data to GCS dropped over 5 times.

As a demagogical argument: I heard from friends at Google, that's what people at Google do :-)

Indeed, I can't find specific example of such use in Supervisor. 

Still, this PR provides new feature.
",provide object based scale much better without sacrificing resiliency patch average cost data time demagogical argument people indeed ca find specific example use supervisor still new feature,issue,negative,positive,positive,positive,positive,positive
380573192,"There were no recent changes and all recent commits are supposed to work.
Can you post the entire log?",recent recent supposed work post entire log,issue,negative,neutral,neutral,neutral,neutral,neutral
380572138,"I forgot to update the error in the issue description when I returned to master from mask_rcnn branch. The error is the same on master (description updated).

Were there any recent changes to 'COCO-ResNet50-MaskRCNN.npz' on http://models.tensorpack.com, because I couldn't find a recent commit in the commit history that works?",forgot update error issue description returned master branch error master description recent could find recent commit commit history work,issue,negative,neutral,neutral,neutral,neutral,neutral
380513468,"You give:
```
File ""tensorpack/examples/FasterRCNN/train.py"", line 354, in 
output_names=get_model_output_names()))
```
But in either master or 0.8.5, line 354 in train.py is not this:
https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/train.py#L354
https://github.com/ppwwyyxx/tensorpack/blob/0.8.5/examples/FasterRCNN/train.py#L354

Could you make sure you're using either master or 0.8.5 and without code changes, and post your error again if any?",give file line either master line could make sure either master without code post error,issue,positive,positive,positive,positive,positive,positive
380382486,"I wrote a wrong version in the issue description, I actually used the latest version 0.8.5 when I was testing. Corrected.",wrote wrong version issue description actually used latest version testing corrected,issue,negative,neutral,neutral,neutral,neutral,neutral
380359561,You should use `val_fun/pred_value` because that's probably the name tensorflow decides to give to your tensor. You can `print(pred_val)` if you're unsure about what the name is.,use probably name give tensor print unsure name,issue,negative,neutral,neutral,neutral,neutral,neutral
380358584,"Sorry for not describing problem clearly.
I  defined a tensor with name **pred_value**
```
   def _build_value_loss(self, state, val):
    with tf.variable_scope('val_fun'):
      out = self._basic_model(state)
      out = FullyConnected('val', out, 1)
      pred_val = tf.identity(out, name='pred_value')
      loss = tf.nn.l2_loss(pred_val - val)
    return loss
```
and **KerError** was raised when I pass **pred_value** as a argument of ```self.trainer.get_predictor(*self.predictor_io_names) ```

```
  File ""/home/robot/Firework/tensorgo/trpo/simulator.py"", line 32, in _setup_graph
    self.value_pred = self.trainer.get_predictor(*self.value_predictor_io_names)
  File ""/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/train/tower.py"", line 106, in get_predictor
    output_tensors = tower.get_tensors(output_names)
  File ""/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/tfutils/tower.py"", line 320, in get_tensors
    return [self.get_tensor(name) for name in names]
  File ""/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/tfutils/tower.py"", line 320, in <listcomp>
    return [self.get_tensor(name) for name in names]
  File ""/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/tfutils/tower.py"", line 306, in get_tensor
    ret = get_op_or_tensor_by_name(name_with_ns)
  File ""/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/tfutils/common.py"", line 131, in get_op_or_tensor_by_name
    return f(name)
  File ""/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/tfutils/common.py"", line 126, in f
    return G.get_tensor_by_name(n)
  File ""/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3654, in get_tensor_by_name
    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)
  File ""/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3478, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3520, in _as_graph_element_locked
    ""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'tower-pred-0/pred_value:0' refers to a Tensor which does not exist. The operation, 'tower-pred-0/pred_value', does not exist in the graph.""
```

and if I removed the variable_scope, it worked correctly.
```
  def _build_value_loss(self, state, val):
    out = self._basic_model(state)
    out = FullyConnected('val', out, 1)
    pred_val = tf.identity(out, name='pred_value')
    loss = tf.nn.l2_loss(pred_val - val)
    return loss
```

TF_version: v1.7.0-3-g024aecf414 1.7.0
Tensorpack_version: master branch",sorry problem clearly defined tensor name self state state loss return loss raised pas argument file line file line file line return name name file line return name name file line ret file line return name file line return file line return name file line return file line graph name name tensor exist operation exist graph removed worked correctly self state state loss return loss master branch,issue,negative,negative,negative,negative,negative,negative
380357736,"Another method I used to test similar issues before was to use two DataParallelInferenceRunner callbacks -- they should not use the same dataflow, but use two dataflows created in the same way. And will produce the same error rate. This can rule out certain types of problems with data or the model.",another method used test similar use two use use two way produce error rate rule certain data model,issue,negative,positive,positive,positive,positive,positive
380355461,@Skylion007 I think that's a reasonable thing to do. I just hope to review those args and understand what they do before supporting them. I added this as a TODO in #627 ,think reasonable thing hope review understand supporting added,issue,positive,positive,positive,positive,positive,positive
380352013,Please follow the issue template and provide relevant information for any progress to be made.,please follow issue template provide relevant information progress made,issue,positive,positive,positive,positive,positive,positive
380198827,Can't we just adapt a way to pass all remaining keyword args straight to the tf.layers implementation? Would future proof it as well and we wouldn't have this issue arise again.,ca adapt way pas straight implementation would future proof well would issue arise,issue,negative,positive,neutral,neutral,positive,positive
380095928,"Sorry to bother, `data_format='channels_last'` will control the dimensions order. I will close.",sorry bother control order close,issue,negative,negative,negative,negative,negative,negative
379990982,"1. Tensorpack is a training interface agnostic of symbolic models. You can use whatever models - so different data types are already supported by design.

2. `tensorpack.models` is a small set of model implementation that exists mainly for historical reasons -- so don't expect any feature update there. You can use whatever other model implementation, such as `tf.layers`.

3. `tensorpack.models` uses `tf.layers` for many implementations already -- as a result it supports some other data types. I don't know what types `tf.layers` supports, but it supports fp16 at least. To use `tf.layers` with fp16 weights, just give it fp16 inputs. I've trained a lot of fp16 models with tensorpack.

4. Some utilities in tensorpack may have some false assumption on data types. I've fixed it for `regularize_cost` function last week. If you need to use it you might need to update.",training interface agnostic symbolic use whatever different data already design small set model implementation mainly historical expect feature update use whatever model implementation many already result data know least use give trained lot may false assumption data fixed function last week need use might need update,issue,negative,negative,neutral,neutral,negative,negative
379976198,"I do not see the issue here. Are you referring to the Conv2D layer in tensorpack.model? It was communicated that the preferred way of writing models is using the official TF layer implementations like tf.layers.Conv2D. Conv2D is more or less a wrapper for TF.layers.
So this seems to be an unsupported feature there.

And you can always remap the get-variable calls with the var-utils. Cross-reference #382 ",see issue layer preferred way writing official layer like le wrapper unsupported feature always remap,issue,negative,neutral,neutral,neutral,neutral,neutral
379572635,"What you want to do is up to your own choice. Tensorpack allows you to do either one.
Not a tensorpack question - closing -- unless you already know what you want to do but don't know how to do it in tensorpack.",want choice either one question unless already know want know,issue,negative,neutral,neutral,neutral,neutral,neutral
379124810,"4. @chunfuchen was not training a mask-rcnn, but a faster-rcnn. (because mask-rcnn implementation was added later)",training implementation added later,issue,negative,neutral,neutral,neutral,neutral,neutral
379124029,"There are many factors here:
1. 3k steps may be not enough warmup. I checked my recent logs and saw about 20% better speed at around 10k steps. I'll update the notes later.
2. I don't have numbers but I won't be surprised if K80 is 4~5 times slower than V100.
3. https://github.com/ppwwyyxx/tensorpack/blob/8b4d4f779dc48eea299a0b15fe7a0f714e5b8113/examples/FasterRCNN/model.py#L315-L319

These two lines are recently added. They may impact speed (probably not much, if any) but I haven't run a benchmark yet.",many may enough checked recent saw better speed around update later wo time two recently added may impact speed probably much run yet,issue,negative,positive,positive,positive,positive,positive
379120798,"@ppwwyyxx @chunfuchen How can you achieve 60~200s per epoch? I used 4 K80, and after 3K steps,  it still takes 1000s per epoch.  I fine-tuned the mask RCNN mode with the follow codes:
`python train.py --gpu 0,1,2,3 --load ImageNet-ResNet50.npz`
![image](https://user-images.githubusercontent.com/13114675/38399034-d3004aa4-3915-11e8-85c0-143e4b9a6dcc.png)
",achieve per epoch used still per epoch mask mode follow python load image,issue,negative,neutral,neutral,neutral,neutral,neutral
379081653,"@ppwwyyxx It works, thanks for your patience.  It has higher GPU utilization than Keras. I think keras can really play for fun and f I should abandon keras from now on and . Thanks again.",work thanks patience higher utilization think really play fun abandon thanks,issue,positive,positive,positive,positive,positive,positive
379076705,You need to remove `GPUUtilizationTracker` from the callbacks. Looks like it does not support windows either.,need remove like support either,issue,positive,neutral,neutral,neutral,neutral,neutral
379075041,"@ppwwyyxx Thanks for your timely reply. I remove the prefetch method as you say, but still have the following errors. Do I need to change the trainer method at the same time?


""Traceback (most recent call last):
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\train\base.py"", line 256, in main_loop
    self._callbacks.before_train()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\base.py"", line 63, in before_train
    self._before_train()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\group.py"", line 70, in _before_train
    cb.before_train()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\base.py"", line 63, in before_train
    self._before_train()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\prof.py"", line 60, in _before_train
    start_proc_mask_signal(self._proc)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\utils\concurrency.py"", line 212, in start_proc_mask_signal
    p.start()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\context.py"", line 212, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\context.py"", line 313, in _Popen
    return Popen(process_obj)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\popen_spawn_win32.py"", line 66, in __init__
    reduction.dump(process_obj, to_child)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\reduction.py"", line 59, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class 'module'>: attribute lookup module on builtins failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\group.py"", line 76, in _after_train
    cb.after_train()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\base.py"", line 171, in after_train
    self._after_train()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\prof.py"", line 80, in _after_train
    self._proc.join()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\process.py"", line 120, in join
    assert self._popen is not None, 'can only join a started process'
AssertionError: can only join a started process
Traceback (most recent call last):
  File ""D:/Github/Tensorpack_Examples/FasterRCNN/train.py"", line 401, in <module>
    launch_train_with_config(cfg, trainer)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\train\interface.py"", line 91, in launch_train_with_config
    extra_callbacks=config.extra_callbacks)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\train\base.py"", line 331, in train_with_defaults
    steps_per_epoch, starting_epoch, max_epoch)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\train\base.py"", line 303, in train
    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\utils\argtools.py"", line 182, in wrapper
    return func(*args, **kwargs)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\train\base.py"", line 256, in main_loop
    self._callbacks.before_train()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\base.py"", line 63, in before_train
    self._before_train()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\group.py"", line 70, in _before_train
    cb.before_train()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\base.py"", line 63, in before_train
    self._before_train()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\callbacks\prof.py"", line 60, in _before_train
    start_proc_mask_signal(self._proc)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\tensorpack\utils\concurrency.py"", line 212, in start_proc_mask_signal
    p.start()
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\context.py"", line 212, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\context.py"", line 313, in _Popen
    return Popen(process_obj)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\popen_spawn_win32.py"", line 66, in __init__
    reduction.dump(process_obj, to_child)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\reduction.py"", line 59, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class 'module'>: attribute lookup module on builtins failed
[0405 16:54:43 @input_source.py:149] EnqueueThread QueueInput/input_queue Exited.
D:\ProgramData\Anaconda3\envs\py35\lib\site-packages\h5py\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\spawn.py"", line 106, in spawn_main
    exitcode = _main(fd)
  File ""D:\ProgramData\Anaconda3\envs\py35\lib\multiprocessing\spawn.py"", line 116, in _main
    self = pickle.load(from_parent)
EOFError: Ran out of input
""",thanks timely reply remove method say still following need change trainer method time recent call last file line file line file line file line file line file line file line start self file line return file line return file line file line dump file protocol ca pickle class attribute module handling exception another exception recent call last file line file line file line file line join assert none join join process recent call last file line module trainer file line file line file line train file line wrapper return file line file line file line file line file line file line file line start self file line return file line return file line file line dump file protocol ca pickle class attribute module conversion second argument float future float import recent call last file string line module file line file line self ran input,issue,negative,positive,neutral,neutral,positive,positive
379071940,`PrefetchData` probably does not support windows either. You can just remove it and expect a minor slow down in speed.,probably support either remove expect minor slow speed,issue,negative,negative,negative,negative,negative,negative
379058971,"@vqdang I face the same question, do you find a solution?",face question find solution,issue,negative,neutral,neutral,neutral,neutral,neutral
378304587,"Adding the iteration number sounds OK. But this makes it harder to track (and overwrite) the last minsaver checkpoint, if you resume a training.",iteration number harder track overwrite last resume training,issue,negative,negative,neutral,neutral,negative,negative
378112785,"Actually, I think the number is correct here. Since you are resuming the interrupted training process, not retraining a pretrained model, the epoch number should follow previous process. As in your case, the model is stopped at No. 475000 iteration (475000/5000 = 95 epoch)， the resumed epoch number should be 96. @chunfuchen ",actually think number correct since interrupted training process model epoch number follow previous process case model stopped iteration epoch epoch number,issue,negative,negative,neutral,neutral,negative,negative
378080284,"We have horovod trainer already. Implementing a slower strategy does not make a lot of sense. 
Tensorflow will probably improve its native distributed all-reduce performance in the future and before that horovod is the best solution for distributed training.",trainer already strategy make lot sense probably improve native distributed performance future best solution distributed training,issue,positive,positive,positive,positive,positive,positive
378079708,"I'm happy with the current `ModelDesc` interface:

inputs() with placeholders
build_graph(self, a, b) which returns a cost (or nothing for generic `ModelDescBase`)
optimizer()

closing this finally.",happy current interface self cost nothing generic finally,issue,positive,positive,positive,positive,positive,positive
378071697,"Not sure what is going on in your case. 
After training the imagenet-resnet examples I can get exactly the same validation number using offline evaluation, if evaluated in the same environment (same software & hardware). The example is using image files instead of LMDB, but I don't think this will lead to issues.

You can also check whether there is any environment difference or preprocessing difference.",sure going case training get exactly validation number evaluation environment hardware example image instead think lead also check whether environment difference difference,issue,negative,positive,positive,positive,positive,positive
377925607,"@PatWie  My ""augmentors"" for the validation data is just the following

```
        augmentors = [
            imgaug.ResizeShortestEdge(size1, cv2.INTER_CUBIC),
            imgaug.CenterCrop((size2, size2)),
        ]
```
I don't think this causes any randomness.",validation data following size size size think randomness,issue,negative,neutral,neutral,neutral,neutral,neutral
377770095,UPDATE: now our implementation of roialign is correct on borders as well.,update implementation correct well,issue,negative,neutral,neutral,neutral,neutral,neutral
377722596,"How does ""augmentors"" looks like? If you introduce randomness there --like random crop etc-- the evaluation error will naturally varies.",like introduce randomness like random crop evaluation error naturally,issue,negative,negative,negative,negative,negative,negative
377670866,"I am using the following procedures to retrieve the validation data.
```
        ds = LMDBData(data_dir, shuffle=False)
        ds = PrefetchData(ds, 5000, 1)
        aug = imgaug.AugmentorList(augmentors)
        def mapf(dp):
            fname, cls = dp
            im = cv2.imdecode(fname, cv2.IMREAD_COLOR)
            im = aug.augment(im)
            return im, cls

        ds = LMDBDataPoint(ds)
        ds = ThreadedMapData(ds, cpu, mapf, buffer_size=2000, strict=True)
        ds = PrefetchDataZMQ(ds, 1)
        ds = BatchData(ds, batch_size, remainder=True)
```",following retrieve validation data return,issue,negative,neutral,neutral,neutral,neutral,neutral
377600396,It's an implementation detail that is not visible to users at all. An user can use either NCHW or channels_first.,implementation detail visible user use either,issue,negative,neutral,neutral,neutral,neutral,neutral
377599384,"I realized that in the group conv case, you are assuming the data_format is of the tf.nn 'NHWC' style consistently. However, the interface can still be more clear, since now you need to change the data_format input based on split=1 or split>1. ",group case assuming style consistently however interface still clear since need change input based split,issue,negative,positive,positive,positive,positive,positive
377459847,"I am using a fixed lmdb file. What do you mean by ""has the correct size""?",fixed file mean correct size,issue,negative,negative,negative,negative,negative,negative
377459479,"If your dataflow is not randomized (either explicitly or implicitly by paralleism), and has the correct size, then both methods should give you the same error rate.",either explicitly implicitly correct size give error rate,issue,negative,neutral,neutral,neutral,neutral,neutral
377046152,"Since the current version does not condition for the os, would it be reasonable to specify somewhere in the docs or readme that training isn't meant for osx? ",since current version condition o would reasonable specify somewhere training meant,issue,negative,positive,neutral,neutral,positive,positive
377043180,"The training wasn't meant to run on osx at the beginning.
An easy fix would be to use different types of sockets conditioned on the OS.",training meant run beginning easy fix would use different conditioned o,issue,negative,positive,positive,positive,positive,positive
376929761,Please report issues following the issue template.,please report following issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
376929477,You inputs have zero channel. How are you supposed to do batch norm.,zero channel supposed batch norm,issue,negative,neutral,neutral,neutral,neutral,neutral
376065189,"@John1231983 can I ask you some question? you said that you have successfully run the matterpot code on your own data set and mentioned the link to train_shapes.ipynb? Can you explain more detail about it? It's mean that base on the train_shapes.ipynb you edit the code to train for your own dataset (to recognize an object like a clock and do the segmentation, for example). Is it right? Or you just successfully run the code on train_shapes.ipynb on your computer? I also want to run the Mask RCNN on my own dataset but I don't know exactly what need I do with my trainning images and what format it is or just put the raw image inside. I'm still focusing on this point. Thanks",ask question said successfully run code data set link explain detail mean base edit code train recognize object like clock segmentation example right successfully run code computer also want run mask know exactly need format put raw image inside still point thanks,issue,positive,positive,positive,positive,positive,positive
376062374,Adding a new argument which most people won't care for ALL callbacks doesn't sound like a good idea to me -- it requires extra coding effort for every new callback to be written.,new argument people wo care sound like good idea extra effort every new written,issue,positive,positive,positive,positive,positive,positive
376061520,"It worked as expected.
Should it be a argument when constructing a new callback ?

    data_shuffle_cb = CallbackFactory(trigger=lambda self: [data_reader.shuffle_train_dataset()])
    data_shuffle_cb.set_chief_only(False)

    return TrainConfig(
            data=TFDatasetInput(training_dataset),
            model=Policy(),
            callbacks=[
                data_shuffle_cb,
                InferenceRunner(TFDatasetInput(test_dataset), ScalarStats('cost')),
                ],  
            steps_per_epoch=train_num // args.batch * 4,
            max_epoch=80)",worked argument new self false return,issue,negative,negative,negative,negative,negative,negative
375476650,The `ternarynet` project used very old tensorpack and tensorflow that are not compatible with what they are now. It has actually included a version of tensorpack in its repo. It's not expected to work with latest tensorpack.,project used old compatible actually included version work latest,issue,negative,positive,positive,positive,positive,positive
375431537,"Honestly, it never came to my mind to _artificially_ downscale the input image _before_ running the inference in practice. Still, it makes no sense for me in this particular work with GANs (things are different for ENet-E and PSNR).

But if this is an issue for you @KindleHe, feel free to process the images with **this** implementation _without_ down-scaling. And add a down-scaling _afterward_. Although it does solve the _same_ task, it might be not entirely fair.

To cite [Colorful Image Colorization](http://richzhang.github.io/colorization/)
> Welcome! Computer vision algorithms often work well on some images, but fail on others. Ours is like this too.

This is ~~probably~~ definitely the case here as well. You will ~~probably~~ find images, where the author's implementation beats this implementation. 

One last thought: I am frequently wondering how people publish methods: Do we benchmark these algorithms against specific down-scaling implementations and on synthetic cases? This adds another complexity by introducing aliasing artifacts besides other subtleties.
Or do we want to solve real-world problems?
With/without anti-aliasing? Bicubic, bi-linear, Lanczos? OpenCV, PIL, TensorFlow implementation?

It is really ludicrous when all this literature is about inverting some specific down-scaling operation implementation.
Feel free to disagree.
And yes you get the same behavior with the other linked implementation from the recipes repo.",honestly never came mind input image running inference practice still sense particular work different issue feel free process implementation add although solve task might entirely fair cite colorful image colorization welcome computer vision often work well fail like definitely case well find author implementation implementation one last thought frequently wondering people publish specific synthetic another complexity besides want solve implementation really ludicrous literature specific operation implementation feel free disagree yes get behavior linked implementation,issue,positive,positive,positive,positive,positive,positive
375419667,"Advice would be to install lmdb.
`pip install lmdb`.",advice would install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
375419465,"@ppwwyyxx Yes, I did not get the reason yet. Could you please give me some advice?",yes get reason yet could please give advice,issue,positive,neutral,neutral,neutral,neutral,neutral
375411554,"The error message is a bit confusing, but it's saying you didn't install lmdb.",error message bit saying install,issue,negative,neutral,neutral,neutral,neutral,neutral
375392238,"I think a non-appendable filesystem would be a potential problem for a lot of other things anyway. Could you just use `logdir` option so that event file won't be on GCS?

Splitting event files looks very hacky. I don't think Supervisor is doing anything like that. Could you point where?",think would potential problem lot anyway could use option event file wo splitting event hacky think supervisor anything like could point,issue,negative,neutral,neutral,neutral,neutral,neutral
375389437,"@ppwwyyxx yes, the latter one is also SR, and I found the re-implementation works not bad for the high-resolution input such as 512 but cannot achieve the paper's effect. I think it is fair that the SR algorithms all works on the same downsampling datasets such as  [Set5](https://webdav.tue.mpg.de/pixel/enhancenet/files/datasets/EnhanceNet_Set5.zip),[ Set14](https://webdav.tue.mpg.de/pixel/enhancenet/files/datasets/EnhanceNet_Set14.zip), [SunHays80](https://webdav.tue.mpg.de/pixel/enhancenet/files/datasets/EnhanceNet_SunHays80.zip), [Urban100](https://webdav.tue.mpg.de/pixel/enhancenet/files/datasets/EnhanceNet_Urban100.zip), [BSD100](https://webdav.tue.mpg.de/pixel/enhancenet/files/datasets/EnhanceNet_BSD100.zip).",yes latter one also found work bad input achieve paper effect think fair work set set urban,issue,negative,positive,neutral,neutral,positive,positive
375386369,"For 4x: I think that's intended. 128->512 is superresolution. 512->2048 is also superresolution. The latter one is used.

The original code is [here](https://github.com/cgtuebingen/tensorflow-recipes/tree/master/EnhanceNet) (but it cannot run anymore because it should have copied GAN.py). The model there is still downloadable. But it's not compatible with code in tensorpack examples.

I changed the code a bit to make the results slightly better in my experiments, but I never succeeded in getting something as good as the demo image. So bad results you're getting is not a surprise to me.",think intended also latter one used original code run copied model still compatible code code bit make slightly better never getting something good image bad getting surprise,issue,negative,positive,positive,positive,positive,positive
375384458,"According to your reply in #541 , the below [website] (https://github.com/ppwwyyxx/tensorpack/pull/541) cannot open with remind ""404 error"", I am curious how you always obtain a model better than baseline, and what the baseline here is ?(the paper's model?)
```
Did the following in https://gist.github.com/ppwwyyxx/6da15f2d9087373635254d4e6fbe4891:
Used the paper's normalization
Fixed VGG scale
Used same learning rate for G and D
Used SeparateGANTrainer

And now I can always obtain a model better than baseline (in all the 3 times I've tried).
```",according reply open remind error curious always obtain model better paper model following used paper normalization fixed scale used learning rate used always obtain model better time tried,issue,positive,positive,positive,positive,positive,positive
375381955,"Another problem is that the accuracy of the discriminator keeps about 0.6~0.7 after 217 epochs on train2017.zip. The loss don't decrease for many epochs. Here is my output parameters:
```
Epoch 217 (global_step 601524) finished, time:10 minutes 27 seconds.
[0322 16:36:23 @saver.py:84] Model saved to train_log/enet-pat/model-601524.
[0322 16:36:23 @monitor.py:428] GAN_loss/discrim/accuracy: 0.61338
[0322 16:36:23 @monitor.py:428] GAN_loss/discrim/loss: 0.64238
[0322 16:36:23 @monitor.py:428] GAN_loss/gen/accuracy: 0.17531
[0322 16:36:23 @monitor.py:428] GAN_loss/gen/loss: 0.91403
[0322 16:36:23 @monitor.py:428] QueueInput/queue_size: 50
[0322 16:36:23 @monitor.py:428] additional_losses/loss_LA: 1.8281
[0322 16:36:23 @monitor.py:428] additional_losses/loss_LP1: 0.3446
[0322 16:36:23 @monitor.py:428] additional_losses/loss_LP2: 0.26865
[0322 16:36:23 @monitor.py:428] additional_losses/loss_LT1: 0.026572
[0322 16:36:23 @monitor.py:428] additional_losses/loss_LT2: 0.026721
[0322 16:36:23 @monitor.py:428] additional_losses/loss_LT3: 0.05257
```
I find your experiment records in the #527 (a copy is below), I have two doubts: 

**1)why 40 epoches achieves 0.99414 discrim/accuracy while my 217get only 0.61338?**    
**2)why the loss_PA between your (8.9464) and my (1.8281) is much different?**
**3) why your LT1 LT2 LT3 is so small whiel my is not?** 
**4) Could you tell me your training config and machine config?**
```
100%|#######################|19714/19714[59:02<00:00, 5.56it/s]          
[1206 04:49:54 @base.py:255] Epoch 40 (global_step 788560) finished, time:3542.78 sec.                                                             
[1206 04:49:55 @saver.py:82] Model saved to train_log/enet-pat/model-788560.                                                                       
[1206 04:49:55 @monitor.py:363] GAN_loss/discrim/accuracy: 0.99414       
[1206 04:49:55 @monitor.py:363] GAN_loss/discrim/loss: 0.019299          
[1206 04:49:55 @monitor.py:363] GAN_loss/gen/accuracy: 0.0026996         
[1206 04:49:55 @monitor.py:363] GAN_loss/gen/loss: 8.9464
[1206 04:49:55 @monitor.py:363] QueueInput/queue_size: 49.585
[1206 04:49:55 @monitor.py:363] add: 19.204
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LA: 8.9464
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LP1: 1.3098
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LP2: 0.0015531
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT1: 2.3112e-37
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT2: 2.2718e-37
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT3: 2.3154e-37
[1206 04:49:55 @base.py:245] Start Epoch 41 ...
100%|#######################|19714/19714[59:03<00:00, 5.56it/s]
[1206 05:48:58 @base.py:255] Epoch 41 (global_step 808274) finished, time:3543.57 sec.
```",another problem accuracy discriminator loss decrease many output epoch finished time model saved find experiment copy two get much different small could tell training machine epoch finished time sec model saved add start epoch epoch finished time sec,issue,negative,positive,neutral,neutral,positive,positive
375310961,"1. I don't know. Maybe I'll be able to tell more if you give more information, such as those in the issue template.
2. [Save and Load models](http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html)",know maybe able tell give information issue template save load,issue,negative,positive,positive,positive,positive,positive
374898629,"Sorry,  I didn't understand your reply. What I want to say is that the final quantized output  `Wb` is not fixed-point integer according to your code. But the input value X,Y of formula 3 is the fixed point number.",sorry understand reply want say final output integer according code input value formula fixed point number,issue,negative,negative,negative,negative,negative,negative
374567637,You're using a (old) version of tensorpack that doesn't match the version of examples.,old version match version,issue,negative,positive,neutral,neutral,positive,positive
374519123,"Thank you very much for your patience! 
There is a problem that has troubled me for a long time. When `bitW=2`, the quantized output is 4 `（2^2）` different real numbers instead of `4bit `fixed-point integers, which cannot satisfy the condition of Formula 3 in the paper. Can you help me understand how to use Formula 3? Thank you！
![image](https://user-images.githubusercontent.com/24288811/37644156-d34f3890-2c5d-11e8-97f5-8dbaf539989c.png)
![image](https://user-images.githubusercontent.com/24288811/37644259-28d39086-2c5e-11e8-8655-d3f9243b7b2e.png)
![image](https://user-images.githubusercontent.com/24288811/37644228-056685ea-2c5e-11e8-96b0-ceb80e222fb4.png)

",thank much patience problem long time output different real instead bit satisfy condition formula paper help understand use formula thank image image image,issue,positive,positive,neutral,neutral,positive,positive
374437999,It will work only if you did not use tensorpack trainers to train the model.,work use train model,issue,negative,neutral,neutral,neutral,neutral,neutral
374437734,"so my use of 
```
with TowerContext('', is_training=True):
        Conv2D(...)
with TowerContext('', is_training=False):
        Conv2D(...)
```
in the old version will work? as 
```
        use_local_stat (bool): whether to use mean/var of the current batch or the moving average.
            Defaults to True in training and False in inference.
```",use old version work bool whether use current batch moving average true training false inference,issue,negative,negative,neutral,neutral,negative,negative
374437405,"To be honest, there is no benefit in using tensorpack layers if you don't use tensorpack trainers. It only causes more trouble with a set of strange interface.
The `training` option used to be named `use_local_stat`, but recently it was renamed to be consistent with official `tf.layers`.",honest benefit use trouble set strange interface training option used recently consistent official,issue,negative,positive,positive,positive,positive,positive
374437081,"but seems batchnorm also dont accept training argument:
```
def BatchNorm(x, use_local_stat=None, decay=0.9, epsilon=1e-5,
              use_scale=True, use_bias=True,
              gamma_init=tf.constant_initializer(1.0), data_format='NHWC',
              internal_update=False):
```",also dont accept training argument,issue,negative,neutral,neutral,neutral,neutral,neutral
374436206,could you pls point out a way to do this? Thanks ,could point way thanks,issue,negative,positive,positive,positive,positive,positive
374435133,"It would not work and you're supposed to see an error.

You can use a different argscope around each Conv2D. But it's better to not use BNReLU or reimplement BNReLU if you have complicated use cases.",would work supposed see error use different around better use complicated use,issue,negative,neutral,neutral,neutral,neutral,neutral
374432103,"but I only use the `Conv2D` which used` nl=BNRelu or nl= BN `
for the 
```
with TowerContext('', is_training=False):
        Conv2D(...)
```
part, I want to load pretrained batchnorm params(moving average)
for the 
```
with TowerContext('', is_training=True):
        Conv2D(...)
```
part, I want to train it as usual(use the batch statistics and keep moving average)
would the my way of 
```
with TowerContext('', is_training=True):
        Conv2D(...)
with TowerContext('', is_training=False):
        Conv2D(...)
```
work? Thank you ",use used part want load moving average part want train usual use batch statistic keep moving average would way work thank,issue,negative,negative,negative,negative,negative,negative
374248309,Feel free to reopen if you still have questions.,feel free reopen still,issue,positive,positive,positive,positive,positive,positive
374239161,"BatchNorm has the `training` option: http://tensorpack.readthedocs.io/en/latest/modules/models.html#tensorpack.models.BatchNorm

It is default to `ctx.is_training` but you can change it.",training option default change,issue,negative,neutral,neutral,neutral,neutral,neutral
374199166,"Thank you very much,  Yuxin Wu! I used your method, and it works well!
I have read your paper DoReFa-Net, and I run the python script  svhn-digit-dorefa.py --dorefa 1,2,4. It seems that  trained weights is not 1 bits which can represent 2 different real values but have many different real values in the first figure. Can you detail more about it?
Besides, what does the W-rms: 1.1019  mean in  param-summary/conv2/W-rms: 1.1019 ?
I am looking forward to your reply, thank your :) !
![image](https://user-images.githubusercontent.com/24288811/37596140-30c96098-2bb6-11e8-9c29-6a916d83b622.png)
![image](https://user-images.githubusercontent.com/24288811/37596193-5b001b4a-2bb6-11e8-8e66-46729559efd0.png)
",thank much used method work well read paper run python script trained represent different real many different real first figure detail besides mean looking forward reply thank image image,issue,positive,positive,positive,positive,positive,positive
374193397,"what if I want to let some layer's batchnorm act as testing state(use trained moving averages), while other layers as training state(use batch statistics)? ",want let layer act testing state use trained moving training state use batch statistic,issue,negative,neutral,neutral,neutral,neutral,neutral
374091121,"1. `import_meta_graph` and `saver.restore` is not the best tool for reading a TF checkpoint. You can use `tf.train.NewCheckpointReader` instead.
2. To use `import_meta_graph` you'll need the `clear_devices` option.

I'm closing it since this is a question unrelated to tensorpack. You can follow up if you still have questions.",best tool reading use instead use need option since question unrelated follow still,issue,positive,positive,positive,positive,positive,positive
373959462,The project uses flake8. see tox.ini for the configuration. ,project flake see configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
373959231,"few questions:
- does flake8 auto-format ?
- do you mind sharing the flake8 configuration in the project ? 
",flake mind flake configuration project,issue,negative,neutral,neutral,neutral,neutral,neutral
373958871,All sources were checked by flake8. I think that's enough.,checked flake think enough,issue,negative,neutral,neutral,neutral,neutral,neutral
373862212,"Considering `*args` as a default in
```python
class Parent(object):
    def __init__(self):
        super(Parent, self).__init__()

    def inputs(self):
        pass

    def graph(self, *inputs):
        pass


class Child(Parent):
    def __init__(self, num):
        super(Child, self).__init__()
        self.num = num

    def inputs(self):
        self.inputs = []
        for i in range(self.num):
            self.inputs.append(object)

    def graph(self, *inputs):
        image, label = inputs
        print image, label

class Child2(Parent):
    def __init__(self, num):
        super(Child2, self).__init__()
        self.num = num

    def inputs(self):
        self.inputs = []
        for i in range(self.num):
            self.inputs.append(object)

    def graph(self, image, label):
        print image, label

c = Child(3)
c.inputs()
c.graph(1, 5)

c = Child2(3)
c.inputs()
c.graph(1, 5)
```

would make the examples look much more intuitive. Looking forward to something like 

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# File: mnist-tflayers.py

import os
import argparse
import tensorflow as tf



# Just import everything into current namespace
from tensorpack import *
from tensorpack.tfutils import summary, get_current_tower_context
from tensorpack.dataflow import dataset

IMAGE_SIZE = 28


class Model(ModelDesc):
    def inputs(self):
        """"""
        Define all the inputs (with type, shape, name) that
        the graph will need.
        """"""
        return [tf.placeholder(tf.float32, (None, IMAGE_SIZE, IMAGE_SIZE), 'input'),
                tf.placeholder(tf.int32, (None,), 'label')]

    def graph(self, image, label):

        image = tf.expand_dims(image, 3) * 2 - 1

        l = tf.layers.conv2d(image, 32, 3, padding='same', activation=tf.nn.relu, name='conv0')
        l = tf.layers.max_pooling2d(l, 2, 2, padding='valid')
        l = tf.layers.conv2d(l, 32, 3, padding='same', activation=tf.nn.relu, name='conv1')
        l = tf.layers.conv2d(l, 32, 3, padding='same', activation=tf.nn.relu, name='conv2')
        l = tf.layers.max_pooling2d(l, 2, 2, padding='valid')
        l = tf.layers.conv2d(l, 32, 3, padding='same', activation=tf.nn.relu, name='conv3')
        l = tf.layers.flatten(l)
        l = tf.layers.dense(l, 512, activation=tf.nn.relu, name='fc0')
        l = tf.layers.dropout(l, rate=0.5,
                              training=get_current_tower_context().is_training)
        logits = tf.layers.dense(l, 10, activation=tf.identity, name='fc1')

        tf.nn.softmax(logits, name='prob')   # a Bx10 with probabilities

        # a vector of length B with loss of each sample
        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
        return tf.reduce_mean(cost, name='cross_entropy_loss')  # the average cross-entropy loss

    def optimizer(self):
        return tf.train.AdamOptimizer(0.001)


def get_data():
    train = BatchData(dataset.Mnist('train'), 128)
    test = BatchData(dataset.Mnist('test'), 256, remainder=True)
    return train, test


def get_config():
    dataset_train, dataset_test = get_data()
    # How many iterations you want in each epoch.
    # This is the default value, don't actually need to set it in the config
    steps_per_epoch = dataset_train.size()

    # get the config which contains everything necessary in a training
    return TrainConfig(
        model=Model(),
        dataflow=dataset_train,  # the DataFlow instance for training
        callbacks=[
            ModelSaver(),   # save the model after every epoch
            MaxSaver('validation_accuracy'),  # save the model with highest accuracy (prefix 'validation_')
            InferenceRunner(    # run inference(for validation) after every epoch
                dataset_test,   # the DataFlow instance used for validation
                ScalarStats(['cross_entropy_loss', 'accuracy'])),
        ],
        steps_per_epoch=steps_per_epoch,
        max_epoch=100,
    )


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.')
    parser.add_argument('--load', help='load model')
    args = parser.parse_args()
    if args.gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu

    # automatically setup the directory train_log/mnist-convnet for logging
    logger.auto_set_dir()

    config = get_config()
    if args.load:
        config.session_init = SaverRestore(args.load)
    # SimpleTrainer is slow, this is just a demo.
    # You can use QueueInputTrainer instead
    launch_train_with_config(config, SimpleTrainer())
```",considering default python class parent object self super parent self self pas graph self pas class child parent self super child self self range object graph self image label print image label class child parent self super child self self range object graph self image label print image label child child would make look much intuitive looking forward something like python python file import o import import import everything current import import summary import class model self define type shape name graph return none none graph self image label image image image vector length loss sample cost return cost average loss self return train test return train test many want epoch default value actually need set get everything necessary training return instance training save model every epoch save model highest accuracy prefix run inference validation every epoch instance used validation parser list use load model automatically setup directory logging slow use instead,issue,positive,positive,positive,positive,positive,positive
373860872,"This is awesome and exactly what I had in mind :+1: 
Let me now nag about `_build_graph` vs `graph` and `_get_optimizer` vs `optimizer` :-)",awesome exactly mind let nag graph,issue,negative,positive,positive,positive,positive,positive
373858868,"How about 39fa465655a2767b ? Both methods can be supported together, and I think `inputs()` + `tf.placeholder` look great.",together think look great,issue,positive,positive,positive,positive,positive,positive
373822304,Closing due to lack of activity. Feel free to reopen if you have follow-ups.,due lack activity feel free reopen,issue,negative,positive,positive,positive,positive,positive
373767909,"> But I believe it also works for 8-8-32 right? 

Yes

> By the way, dorefa will generate some checkpoints, however, how can I resume training from the checkpoints? Thanks,

https://github.com/ppwwyyxx/tensorpack/blob/550988138574d903798ab47ee82202fff8ac3c03/examples/DoReFa-Net/alexnet-dorefa.py#L265-L266",believe also work right yes way generate however resume training thanks,issue,positive,positive,positive,positive,positive,positive
373633042,"DoReFa was designed for ultra-low bits. But I believe it also works for 8-8-32 right? By the way, dorefa will generate some checkpoints, however, how can I resume training from the checkpoints? Thanks,",designed believe also work right way generate however resume training thanks,issue,negative,positive,positive,positive,positive,positive
373624902,We don't know. DoReFa-Net was designed mainly for ultra-low bit (<8). We haven't done many experiments on 8 bits or more.,know designed mainly bit done many,issue,negative,positive,positive,positive,positive,positive
373624528,"Yes I agree using a class is more powerful, e.g. able to change number of inputs, able to share attributes between these three functions, etc. These concepts are strongly connected and having them in a class makes sense. I thought about how I would write some of the complex models without the class and that doesn't look very good.

In the meantime, having a class makes it hard to write things. Somehow I recently felt tensorpack was too complicated for the easiest tasks. I'll need a ""boilerplate.py"" to start working -- that's a bit sad. I don't know whether `ModelFactory` can be a solution, but I think both renaming and `tf.placeholder` sound good!",yes agree class powerful able change number able share three strongly connected class sense thought would write complex without class look good class hard write somehow recently felt complicated easiest need start working bit sad know whether solution think sound good,issue,positive,positive,positive,positive,positive,positive
373619807,"For a second, it sounds like a nice idea, which might ease the way of using tensorpack for new users.
But again input-desc should be a function. Maybe, this would even allow to directly use tf.placeholders. I think for small examples (Mnist) the function style is much easier to understand. 

But thinking about a bit longer, I feel VERY uncomfortable.
Personally, I like the way of using a class rather than writing multiple lines of a config. Passing tons of lines as a config instead of objects usually smells like a bad design.  In addition, inputs, graph and optimizer are not a loose collection they are strongly connected. In C++ you usually want to pass struct  objects instead of its members to functions, right? Your proposal sounds like dropping C++ and going back to C.

When touching ModelDescr, maybe using the design of

```python
class Model
  def inputs
    return tf.placeholders
  def graph
    return cost
  def optimizer
    return Adam
```

 might be a proposal from my side: just rename these methods. Directly using `tf.placeholder` might be possible as well. You can always place them first in a second helper-graph to get their shapes and just delete this helper graph.

Hopefully, when changing anything you will still support the class-based way. Tensorpack is now in a state, where people like myself do not want to maintain all old examples -- besides maybe renaming ModelDescr to ModelDescrOld.

But when ModelFactory can be replaced by ModelDescr without further steps, thumbs up for this nice addition! I am just attached to the current design :-)
 ",second like nice idea might ease way new function maybe would even allow directly use think small function style much easier understand thinking bit longer feel uncomfortable personally like way class rather writing multiple passing instead usually like bad design addition graph loose collection strongly connected usually want pas instead right proposal like dropping going back touching maybe design python class model return graph return cost return might proposal side rename directly might possible well always place first second get delete helper graph hopefully anything still support way state people like want maintain old besides maybe without nice addition attached current design,issue,positive,positive,neutral,neutral,positive,positive
373533338,"Looking further, however, I think letting users to ""write a class"" is the one thing that makes tensorpack very hard to use at the first sight. Usually you don't use a library by inheritance..

For example, if we could transit to something like:
```python
TrainConfig(
model=ModelFactory(
  inputs_desc=[],
  tower_func=lambda image, label: return cost,
  optimizer=tf.train.GradientDescentOptimizer(0.1)
)
...
)
```
This would probably make everything looks much simpler. The Tensorpack+Keras example is written somewhat like this.",looking however think write class one thing hard use first sight usually use library inheritance example could transit something like python image label return cost would probably make everything much simpler example written somewhat like,issue,positive,negative,neutral,neutral,negative,negative
373491879,"You can use `*args`, but that's a good point. I'm not confident which one is better.

`return cost` will be used by `ModelDesc` which is intended for single-cost single-optimizer training only. Other models should use ` ModelDescBase` or (probably better) build the graph by their own.",use good point confident one better return cost used intended training use probably better build graph,issue,positive,positive,positive,positive,positive,positive
373348196,turns out updated op needs to be added with training ,turn need added training,issue,negative,neutral,neutral,neutral,neutral,neutral
373306937,"Not sure, if `build_graph(self, a, b)` is better than `_build_graph(self, inputs)`. In a recent project, we had something like
```python
def _get_inputs(self):
    InputDescs = []
    for i in range(ONLY_KNOWN_AT_RUNTIME):
        InputDescs.append(InputDesc(tf.float32, (B, H[i], W[i], 3), 'input_%i' % i))
    return InputDescs
```

This works quite nice in the current version, but might not work in the proposed changes. How does `_build_graph` looks like? How do you handle this case:


```python
class Parent(object):
    def __init__(self):
        super(Parent, self).__init__()

    def _get_inputs(self):
        pass

    def _build_graph(self, inputs):
        pass


class Child(Parent):
    def __init__(self, num):
        super(Child, self).__init__()
        self.num = num

    def _get_inputs(self):
        self.inputs = []
        for i in range(self.num):
            self.inputs.append(object)

    def _build_graph(self, a, b):
        print a, b

c = Child(3)
c._get_inputs()
c._build_graph([1, 5, 6])
```

How does `return cost` works with the `SeparateGANTrainer`?",sure self better self recent project something like python self range return work quite nice current version might work like handle case python class parent object self super parent self self pas self pas class child parent self super child self self range object self print child return cost work,issue,positive,positive,positive,positive,positive,positive
373038804,"I think the future (and final) changes to `ModelDesc` would be:
1. Prefer `build_graph(self, a, b)` instead of `_build_graph(self, inputs)`
2. Prefer `return cost` instead of `self.cost = cost`
3. Won't automatically apply REGULARIZATION_LOSSES collection if using the new `build_graph+return cost` interface. Keep (?) the old behavior if using the current interface. Reasons explained in [the old thread](https://github.com/ppwwyyxx/tensorpack/pull/81#issuecomment-318299215).

These can probably be smoothly executed __without breaking anything__.

I found a lot of changes I made recently were to remove some ""automatic"" parts from the framework (or at least make them more explicit): setting logger directory, automatically load epoch number, clear the inference queue before inference, etc. A lot of these ""automatic"" behaviors look good at the beginning but end up causing trouble in unexpected scenarios. ",think future final would prefer self instead self prefer return cost instead cost wo automatically apply collection new cost interface keep old behavior current interface old thread probably smoothly executed breaking found lot made recently remove automatic framework least make explicit setting logger directory automatically load epoch number clear inference queue inference lot automatic look good beginning end causing trouble unexpected,issue,negative,positive,neutral,neutral,positive,positive
373004442,Then you'll also need to change the learning rate schedule,also need change learning rate schedule,issue,negative,neutral,neutral,neutral,neutral,neutral
372991191,"But am I right that:
_steps_per_epoch = dataset_train.size() (default value)
and increase number of epoches in 40 times?_
is the same as code in current HED implementation?
Thank you.
",right default value increase number time code current implementation thank,issue,positive,positive,positive,positive,positive,positive
372571823,Thank you so much for your help! It is clear now.,thank much help clear,issue,positive,positive,positive,positive,positive,positive
372084808,"To load data in custom format, you need to write a dataflow for that format:
http://tensorpack.readthedocs.io/tutorial/extend/dataflow.html",load data custom format need write format,issue,negative,neutral,neutral,neutral,neutral,neutral
372084108,I don't know how to use other data format. Can you help me ?,know use data format help,issue,negative,neutral,neutral,neutral,neutral,neutral
371710104,"You can load a npz  by `np.load`. 
After that, how to create a tf checkpoint is not a tensorpack question, and I don't know the answer either.
At least you can manually create variables in the graph, load the values and use tf saver to save the checkpoint.",load create question know answer either least manually create graph load use saver save,issue,positive,negative,negative,negative,negative,negative
371543491,"After the above commit, protobuf will be required only if you call http://tensorpack.readthedocs.io/en/latest/modules/dataflow.dataset.html#tensorpack.dataflow.dataset.ILSVRCMeta.get_per_pixel_mean .
As a result running shufflenet will not require protobuf.",commit call result running require,issue,negative,neutral,neutral,neutral,neutral,neutral
371541783,"Similar to #681 you may have more than one versions installed in different places (by conda or pip or system).
msgpack master still has the raw option: https://github.com/msgpack/msgpack-python/blob/master/msgpack/_unpacker.pyx#L161-L169",similar may one different pip system master still raw option,issue,negative,negative,neutral,neutral,negative,negative
371535856,Please paste what you did and more logs following the issue template.,please paste following issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
371508939,"I add 'from tensorpack.dataflow import LMDBData' ,but it still have error:
ValueError:invalid literal for float():00000000_n02088632_1176.JPEG
Training was stopped.
EnqueueThread QueueInput/input_queue Exited.
PrefetDataZMQ successfully cleaned-up.

so ,what I should do?
",add import still error invalid literal float training stopped successfully,issue,negative,positive,positive,positive,positive,positive
371301267,But this is just for inference and I forced the batch size to be one. Will check that,inference forced batch size one check,issue,negative,negative,negative,negative,negative,negative
371293630,"Unfortunately 925MB is just not enough.
From the log, setting the environment variable `TF_CUDNN_USE_AUTOTUNE=0` may help.",unfortunately enough log setting environment variable may help,issue,negative,neutral,neutral,neutral,neutral,neutral
371024057,"The script was not expected to work for other checkpoints, but you're right this is the solution.",script work right solution,issue,negative,positive,positive,positive,positive,positive
371012463,"It is tensorflow official model file, so it is should not have problem file",official model file problem file,issue,negative,neutral,neutral,neutral,neutral,neutral
370945366,"This seems to be used by the authors: https://github.com/hongyi-zhang/mixup/blob/master/cifar/easy_mixup.py#L45-L50

Shouldn't be a big deal anyway, I assume. An easy change is to use `CenterPaste(background_filler=ConstantBackgroundFiller(128))`.",used big deal anyway assume easy change use,issue,negative,positive,positive,positive,positive,positive
370256249,"This usage was deprecated one year ago. Now gradient processors can be added onto optimizer with http://tensorpack.readthedocs.io/en/latest/modules/tfutils.html#tensorpack.tfutils.optimizer.apply_grad_processors .

For example: https://github.com/ppwwyyxx/tensorpack/blob/9972b15036c851ce82e63721cfe0905e68115475/examples/SpatialTransformer/mnist-addition.py#L90-L96",usage one year ago gradient added onto example,issue,negative,neutral,neutral,neutral,neutral,neutral
370246832,"I couldn't find any function as `get_gradient_processor()` in the base class `ModelDesc`. I tried adding this function into my Model, but it seemed that this function was never called.",could find function base class tried function model function never,issue,negative,negative,negative,negative,negative,negative
370210099,"@michaelklachko 

hi,  the params should include weight, bias(in conv), beta, gamma(in bn), those parameter number is unrelated to the dataset. 

please kindly correct me if my understanding is wrong.",hi include weight bias beta gamma parameter number unrelated please kindly correct understanding wrong,issue,negative,positive,neutral,neutral,positive,positive
370209846,A typical server machine in my experience has 2 CPUs each with 10 cores or more. ,typical server machine experience,issue,negative,negative,negative,negative,negative,negative
370209281,"I have done these. It seems that the cpu in my computer is the bottleneck.
What is suitable CPU configuration for this situation? Can you give me some advice?",done computer bottleneck suitable configuration situation give advice,issue,negative,positive,positive,positive,positive,positive
370209024,"Then do fewer preprocessing steps.

https://github.com/ppwwyyxx/tensorpack/blob/9972b15036c851ce82e63721cfe0905e68115475/examples/ResNet/imagenet_utils.py#L61-L74

May slightly affect accuracy.",may slightly affect accuracy,issue,negative,negative,negative,negative,negative,negative
370208929,"Ok, I get it. Thank you very much.
Are there some suggestion for accelerating the code in this situation? ",get thank much suggestion code situation,issue,negative,positive,positive,positive,positive,positive
370208787,Oh I found E5-1620 v4 only has 4 cores? This CPU definitely cannot do all the preprocessing faster than 2.97*256 im/s.,oh found definitely faster,issue,negative,neutral,neutral,neutral,neutral,neutral
370208653,"This is measuring disk speed and it means you can read 2.97*256 images per second from disk into memory. It's up to you whether you'll call it a bottleneck.

It's significantly faster than the whole pipeline so at least it's not ""blocking"" the rest.",measuring disk speed read per second disk memory whether call bottleneck significantly faster whole pipeline least blocking rest,issue,negative,positive,neutral,neutral,positive,positive
370208543,"Sorry for such issue. I have tested the code as you mentioned above.
`
ds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)

ds = BatchData(ds, 256, use_list=True)
`
The output is:
![image](https://user-images.githubusercontent.com/12790408/36943245-df1c0486-1fc0-11e8-9112-e638a3bed86b.png)
My cpu is: one CPU E5-1620 v4, 3.5Ghz, memory is 64G.

Does this result indicate that my hardware is the bottleneck?

",sorry issue tested code output image one memory result indicate hardware bottleneck,issue,negative,negative,negative,negative,negative,negative
370205517,"This function creates more problems than it solves, mainly because dataflow doesn't guarantee anything about being reset-able. Calling `reset_state` or `get_data` for more than one times is not guaranteed to do useful things, and for a lot of parallel dataflows it's usually hard to do anything. As a result this function actually will lose some data.

Before there's a clearer semantics on what it means to reset the state, this function was renamed to `refill_queue` to avoid being called by inferencerunnner.",function mainly guarantee anything calling one time useful lot parallel usually hard anything result function actually lose data clearer semantics reset state function avoid,issue,negative,positive,neutral,neutral,positive,positive
370158652,"Some examples to benchmark:
```python
ds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)
ds = BatchData(ds, 256, use_list=True)
```

```python
ds = LMDBData(db, shuffle=False)
ds = LocallyShuffleData(ds, 50000)
ds = PrefetchData(ds, 5000, 1)
ds = LMDBDataPoint(ds)
ds = PrefetchDataZMQ(ds, 25) # set to your cpu count
ds = BatchData(ds, 256, use_list=True)
```

```python
ds = LMDBData(db, shuffle=False)
ds = LocallyShuffleData(ds, 50000)
ds = PrefetchData(ds, 5000, 1)
ds = LMDBDataPoint(ds)
ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
ds = AugmentImageComponent(ds, [imgaug.Resize(224)])
ds = PrefetchDataZMQ(ds, 25)
ds = BatchData(ds, 256)
```",python python set count python lambda,issue,negative,neutral,neutral,neutral,neutral,neutral
370157539,"BatchData batch 256 data points together so each iteration takes 256x more time.

What you need to worry about is which step before it takes the most time. Follow the steps in the instructions: http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html#investigate-dataflow",batch data together iteration time need worry step time follow,issue,negative,neutral,neutral,neutral,neutral,neutral
370049708,"1.7M number refers to CIFAR10 architecture, which is a lot smaller than Imagenet version.",number architecture lot smaller version,issue,negative,neutral,neutral,neutral,neutral,neutral
370004324,"![0302-10 11 46](https://user-images.githubusercontent.com/1381301/36914437-27838b56-1e02-11e8-9277-246615781a2c.png)
If this is the number you're referring to, it's a completely unrelated thing.",number completely unrelated thing,issue,negative,positive,neutral,neutral,positive,positive
369852764,Please report issues following the template in your issue.,please report following template issue,issue,negative,neutral,neutral,neutral,neutral,neutral
369788976,"`ls-checkpoint` list the variables in the checkpoint. The output of the layer is a tensor, not a variable.
",list output layer tensor variable,issue,negative,neutral,neutral,neutral,neutral,neutral
369788312,"Sorry for the basic questions.
From tensorboard, I can clearly see there is a node named ""gap"" in the graph. But when I print the available nodes in the checkpoint. There is no ""gap"" node. The following is the available variable names in the graph.
/usr/bin/python2.7 /home/dk/james/tf-based/ls-checkpoint-tensorpack.py
Failed to load OpenCL runtime
[0301 20:09:14 @varmanip.py:165] WRN Checkpoint path /home/dk/james/tf-based/resnet50/20180216-182737/model-2321408.data-00000-of-00001 is auto-corrected to `/home/dk/james/tf-based/resnet50/20180216-182737/model-2321408.`
{'EMA/QueueInput/queue_size': [],
 'EMA/QueueInput/queue_size/biased': [],
 'EMA/QueueInput/queue_size/local_step': [],
 'EMA/cost': [],
 'EMA/cost/biased': [],
 'EMA/cost/local_step': [],
 'EMA/cross_entropy': [],
 'EMA/cross_entropy/biased': [],
 'EMA/cross_entropy/local_step': [],
 'EMA/train_error': [],
 'EMA/train_error/biased': [],
 'EMA/train_error/local_step': [],
 'Logits/biases': [10575],
 'Logits/biases/Momentum': [10575],
 'Logits/weights': [2048, 10575],
 'Logits/weights/Momentum': [2048, 10575],
 'conv1/BatchNorm/beta': [64],
 'conv1/BatchNorm/beta/Momentum': [64],
 'conv1/BatchNorm/moving_mean': [64],
 'conv1/BatchNorm/moving_variance': [64],
 'conv1/weights': [7, 7, 3, 64],
 'conv1/weights/Momentum': [7, 7, 3, 64],
 'conv2_1_branch1/BatchNorm/beta': [256],
 'conv2_1_branch1/BatchNorm/beta/Momentum': [256],
 'conv2_1_branch1/BatchNorm/moving_mean': [256],
 'conv2_1_branch1/BatchNorm/moving_variance': [256],
 'conv2_1_branch1/weights': [1, 1, 64, 256],
 'conv2_1_branch1/weights/Momentum': [1, 1, 64, 256],
 'conv2_1_branch2_a/BatchNorm/beta': [64],
 'conv2_1_branch2_a/BatchNorm/beta/Momentum': [64],
 'conv2_1_branch2_a/BatchNorm/moving_mean': [64],
 'conv2_1_branch2_a/BatchNorm/moving_variance': [64],
 'conv2_1_branch2_a/weights': [1, 1, 64, 64],
 'conv2_1_branch2_a/weights/Momentum': [1, 1, 64, 64],
 'conv2_1_branch2_b/BatchNorm/beta': [64],
 'conv2_1_branch2_b/BatchNorm/beta/Momentum': [64],
 'conv2_1_branch2_b/BatchNorm/moving_mean': [64],
 'conv2_1_branch2_b/BatchNorm/moving_variance': [64],
 'conv2_1_branch2_b/weights': [3, 3, 64, 64],
 'conv2_1_branch2_b/weights/Momentum': [3, 3, 64, 64],
 'conv2_1_branch2_c/BatchNorm/beta': [256],
 'conv2_1_branch2_c/BatchNorm/beta/Momentum': [256],
 'conv2_1_branch2_c/BatchNorm/moving_mean': [256],
 'conv2_1_branch2_c/BatchNorm/moving_variance': [256],
 'conv2_1_branch2_c/weights': [1, 1, 64, 256],
 'conv2_1_branch2_c/weights/Momentum': [1, 1, 64, 256],
 'conv2_2/conv2_2_1_identity_a/BatchNorm/beta': [64],
 'conv2_2/conv2_2_1_identity_a/BatchNorm/beta/Momentum': [64],
 'conv2_2/conv2_2_1_identity_a/BatchNorm/moving_mean': [64],
 'conv2_2/conv2_2_1_identity_a/BatchNorm/moving_variance': [64],
 'conv2_2/conv2_2_1_identity_a/weights': [1, 1, 256, 64],
 'conv2_2/conv2_2_1_identity_a/weights/Momentum': [1, 1, 256, 64],
 'conv2_2/conv2_2_1_identity_b/BatchNorm/beta': [64],
 'conv2_2/conv2_2_1_identity_b/BatchNorm/beta/Momentum': [64],
 'conv2_2/conv2_2_1_identity_b/BatchNorm/moving_mean': [64],
 'conv2_2/conv2_2_1_identity_b/BatchNorm/moving_variance': [64],
 'conv2_2/conv2_2_1_identity_b/weights': [3, 3, 64, 64],
 'conv2_2/conv2_2_1_identity_b/weights/Momentum': [3, 3, 64, 64],
 'conv2_2/conv2_2_1_identity_c/BatchNorm/beta': [256],
 'conv2_2/conv2_2_1_identity_c/BatchNorm/beta/Momentum': [256],
 'conv2_2/conv2_2_1_identity_c/BatchNorm/moving_mean': [256],
 'conv2_2/conv2_2_1_identity_c/BatchNorm/moving_variance': [256],
 'conv2_2/conv2_2_1_identity_c/weights': [1, 1, 64, 256],
 'conv2_2/conv2_2_1_identity_c/weights/Momentum': [1, 1, 64, 256],
 'conv2_2/conv2_2_2_identity_a/BatchNorm/beta': [64],
 'conv2_2/conv2_2_2_identity_a/BatchNorm/beta/Momentum': [64],
 'conv2_2/conv2_2_2_identity_a/BatchNorm/moving_mean': [64],
 'conv2_2/conv2_2_2_identity_a/BatchNorm/moving_variance': [64],
 'conv2_2/conv2_2_2_identity_a/weights': [1, 1, 256, 64],
 'conv2_2/conv2_2_2_identity_a/weights/Momentum': [1, 1, 256, 64],
 'conv2_2/conv2_2_2_identity_b/BatchNorm/beta': [64],
 'conv2_2/conv2_2_2_identity_b/BatchNorm/beta/Momentum': [64],
 'conv2_2/conv2_2_2_identity_b/BatchNorm/moving_mean': [64],
 'conv2_2/conv2_2_2_identity_b/BatchNorm/moving_variance': [64],
 'conv2_2/conv2_2_2_identity_b/weights': [3, 3, 64, 64],
 'conv2_2/conv2_2_2_identity_b/weights/Momentum': [3, 3, 64, 64],
 'conv2_2/conv2_2_2_identity_c/BatchNorm/beta': [256],
 'conv2_2/conv2_2_2_identity_c/BatchNorm/beta/Momentum': [256],
 'conv2_2/conv2_2_2_identity_c/BatchNorm/moving_mean': [256],
 'conv2_2/conv2_2_2_identity_c/BatchNorm/moving_variance': [256],
 'conv2_2/conv2_2_2_identity_c/weights': [1, 1, 64, 256],
 'conv2_2/conv2_2_2_identity_c/weights/Momentum': [1, 1, 64, 256],
 'conv3_1_branch1/BatchNorm/beta': [512],
 'conv3_1_branch1/BatchNorm/beta/Momentum': [512],
 'conv3_1_branch1/BatchNorm/moving_mean': [512],
 'conv3_1_branch1/BatchNorm/moving_variance': [512],
 'conv3_1_branch1/weights': [1, 1, 256, 512],
 'conv3_1_branch1/weights/Momentum': [1, 1, 256, 512],
 'conv3_1_branch2_a/BatchNorm/beta': [128],
 'conv3_1_branch2_a/BatchNorm/beta/Momentum': [128],
 'conv3_1_branch2_a/BatchNorm/moving_mean': [128],
 'conv3_1_branch2_a/BatchNorm/moving_variance': [128],
 'conv3_1_branch2_a/weights': [1, 1, 256, 128],
 'conv3_1_branch2_a/weights/Momentum': [1, 1, 256, 128],
 'conv3_1_branch2_b/BatchNorm/beta': [128],
 'conv3_1_branch2_b/BatchNorm/beta/Momentum': [128],
 'conv3_1_branch2_b/BatchNorm/moving_mean': [128],
 'conv3_1_branch2_b/BatchNorm/moving_variance': [128],
 'conv3_1_branch2_b/weights': [3, 3, 128, 128],
 'conv3_1_branch2_b/weights/Momentum': [3, 3, 128, 128],
 'conv3_1_branch2_c/BatchNorm/beta': [512],
 'conv3_1_branch2_c/BatchNorm/beta/Momentum': [512],
 'conv3_1_branch2_c/BatchNorm/moving_mean': [512],
 'conv3_1_branch2_c/BatchNorm/moving_variance': [512],
 'conv3_1_branch2_c/weights': [1, 1, 128, 512],
 'conv3_1_branch2_c/weights/Momentum': [1, 1, 128, 512],
 'conv3_2/conv3_2_1_identity_a/BatchNorm/beta': [128],
 'conv3_2/conv3_2_1_identity_a/BatchNorm/beta/Momentum': [128],
 'conv3_2/conv3_2_1_identity_a/BatchNorm/moving_mean': [128],
 'conv3_2/conv3_2_1_identity_a/BatchNorm/moving_variance': [128],
 'conv3_2/conv3_2_1_identity_a/weights': [1, 1, 512, 128],
 'conv3_2/conv3_2_1_identity_a/weights/Momentum': [1, 1, 512, 128],
 'conv3_2/conv3_2_1_identity_b/BatchNorm/beta': [128],
 'conv3_2/conv3_2_1_identity_b/BatchNorm/beta/Momentum': [128],
 'conv3_2/conv3_2_1_identity_b/BatchNorm/moving_mean': [128],
 'conv3_2/conv3_2_1_identity_b/BatchNorm/moving_variance': [128],
 'conv3_2/conv3_2_1_identity_b/weights': [3, 3, 128, 128],
 'conv3_2/conv3_2_1_identity_b/weights/Momentum': [3, 3, 128, 128],
 'conv3_2/conv3_2_1_identity_c/BatchNorm/beta': [512],
 'conv3_2/conv3_2_1_identity_c/BatchNorm/beta/Momentum': [512],
 'conv3_2/conv3_2_1_identity_c/BatchNorm/moving_mean': [512],
 'conv3_2/conv3_2_1_identity_c/BatchNorm/moving_variance': [512],
 'conv3_2/conv3_2_1_identity_c/weights': [1, 1, 128, 512],
 'conv3_2/conv3_2_1_identity_c/weights/Momentum': [1, 1, 128, 512],
 'conv3_2/conv3_2_2_identity_a/BatchNorm/beta': [128],
 'conv3_2/conv3_2_2_identity_a/BatchNorm/beta/Momentum': [128],
 'conv3_2/conv3_2_2_identity_a/BatchNorm/moving_mean': [128],
 'conv3_2/conv3_2_2_identity_a/BatchNorm/moving_variance': [128],
 'conv3_2/conv3_2_2_identity_a/weights': [1, 1, 512, 128],
 'conv3_2/conv3_2_2_identity_a/weights/Momentum': [1, 1, 512, 128],
 'conv3_2/conv3_2_2_identity_b/BatchNorm/beta': [128],
 'conv3_2/conv3_2_2_identity_b/BatchNorm/beta/Momentum': [128],
 'conv3_2/conv3_2_2_identity_b/BatchNorm/moving_mean': [128],
 'conv3_2/conv3_2_2_identity_b/BatchNorm/moving_variance': [128],
 'conv3_2/conv3_2_2_identity_b/weights': [3, 3, 128, 128],
 'conv3_2/conv3_2_2_identity_b/weights/Momentum': [3, 3, 128, 128],
 'conv3_2/conv3_2_2_identity_c/BatchNorm/beta': [512],
 'conv3_2/conv3_2_2_identity_c/BatchNorm/beta/Momentum': [512],
 'conv3_2/conv3_2_2_identity_c/BatchNorm/moving_mean': [512],
 'conv3_2/conv3_2_2_identity_c/BatchNorm/moving_variance': [512],
 'conv3_2/conv3_2_2_identity_c/weights': [1, 1, 128, 512],
 'conv3_2/conv3_2_2_identity_c/weights/Momentum': [1, 1, 128, 512],
 'conv3_2/conv3_2_3_identity_a/BatchNorm/beta': [128],
 'conv3_2/conv3_2_3_identity_a/BatchNorm/beta/Momentum': [128],
 'conv3_2/conv3_2_3_identity_a/BatchNorm/moving_mean': [128],
 'conv3_2/conv3_2_3_identity_a/BatchNorm/moving_variance': [128],
 'conv3_2/conv3_2_3_identity_a/weights': [1, 1, 512, 128],
 'conv3_2/conv3_2_3_identity_a/weights/Momentum': [1, 1, 512, 128],
 'conv3_2/conv3_2_3_identity_b/BatchNorm/beta': [128],
 'conv3_2/conv3_2_3_identity_b/BatchNorm/beta/Momentum': [128],
 'conv3_2/conv3_2_3_identity_b/BatchNorm/moving_mean': [128],
 'conv3_2/conv3_2_3_identity_b/BatchNorm/moving_variance': [128],
 'conv3_2/conv3_2_3_identity_b/weights': [3, 3, 128, 128],
 'conv3_2/conv3_2_3_identity_b/weights/Momentum': [3, 3, 128, 128],
 'conv3_2/conv3_2_3_identity_c/BatchNorm/beta': [512],
 'conv3_2/conv3_2_3_identity_c/BatchNorm/beta/Momentum': [512],
 'conv3_2/conv3_2_3_identity_c/BatchNorm/moving_mean': [512],
 'conv3_2/conv3_2_3_identity_c/BatchNorm/moving_variance': [512],
 'conv3_2/conv3_2_3_identity_c/weights': [1, 1, 128, 512],
 'conv3_2/conv3_2_3_identity_c/weights/Momentum': [1, 1, 128, 512],
 'conv4_1_branch1/BatchNorm/beta': [1024],
 'conv4_1_branch1/BatchNorm/beta/Momentum': [1024],
 'conv4_1_branch1/BatchNorm/moving_mean': [1024],
 'conv4_1_branch1/BatchNorm/moving_variance': [1024],
 'conv4_1_branch1/weights': [1, 1, 512, 1024],
 'conv4_1_branch1/weights/Momentum': [1, 1, 512, 1024],
 'conv4_1_branch2_a/BatchNorm/beta': [256],
 'conv4_1_branch2_a/BatchNorm/beta/Momentum': [256],
 'conv4_1_branch2_a/BatchNorm/moving_mean': [256],
 'conv4_1_branch2_a/BatchNorm/moving_variance': [256],
 'conv4_1_branch2_a/weights': [1, 1, 512, 256],
 'conv4_1_branch2_a/weights/Momentum': [1, 1, 512, 256],
 'conv4_1_branch2_b/BatchNorm/beta': [256],
 'conv4_1_branch2_b/BatchNorm/beta/Momentum': [256],
 'conv4_1_branch2_b/BatchNorm/moving_mean': [256],
 'conv4_1_branch2_b/BatchNorm/moving_variance': [256],
 'conv4_1_branch2_b/weights': [3, 3, 256, 256],
 'conv4_1_branch2_b/weights/Momentum': [3, 3, 256, 256],
 'conv4_1_branch2_c/BatchNorm/beta': [1024],
 'conv4_1_branch2_c/BatchNorm/beta/Momentum': [1024],
 'conv4_1_branch2_c/BatchNorm/moving_mean': [1024],
 'conv4_1_branch2_c/BatchNorm/moving_variance': [1024],
 'conv4_1_branch2_c/weights': [1, 1, 256, 1024],
 'conv4_1_branch2_c/weights/Momentum': [1, 1, 256, 1024],
 'conv4_2/conv4_2_1_identity_a/BatchNorm/beta': [256],
 'conv4_2/conv4_2_1_identity_a/BatchNorm/beta/Momentum': [256],
 'conv4_2/conv4_2_1_identity_a/BatchNorm/moving_mean': [256],
 'conv4_2/conv4_2_1_identity_a/BatchNorm/moving_variance': [256],
 'conv4_2/conv4_2_1_identity_a/weights': [1, 1, 1024, 256],
 'conv4_2/conv4_2_1_identity_a/weights/Momentum': [1, 1, 1024, 256],
 'conv4_2/conv4_2_1_identity_b/BatchNorm/beta': [256],
 'conv4_2/conv4_2_1_identity_b/BatchNorm/beta/Momentum': [256],
 'conv4_2/conv4_2_1_identity_b/BatchNorm/moving_mean': [256],
 'conv4_2/conv4_2_1_identity_b/BatchNorm/moving_variance': [256],
 'conv4_2/conv4_2_1_identity_b/weights': [3, 3, 256, 256],
 'conv4_2/conv4_2_1_identity_b/weights/Momentum': [3, 3, 256, 256],
 'conv4_2/conv4_2_1_identity_c/BatchNorm/beta': [1024],
 'conv4_2/conv4_2_1_identity_c/BatchNorm/beta/Momentum': [1024],
 'conv4_2/conv4_2_1_identity_c/BatchNorm/moving_mean': [1024],
 'conv4_2/conv4_2_1_identity_c/BatchNorm/moving_variance': [1024],
 'conv4_2/conv4_2_1_identity_c/weights': [1, 1, 256, 1024],
 'conv4_2/conv4_2_1_identity_c/weights/Momentum': [1, 1, 256, 1024],
 'conv4_2/conv4_2_2_identity_a/BatchNorm/beta': [256],
 'conv4_2/conv4_2_2_identity_a/BatchNorm/beta/Momentum': [256],
 'conv4_2/conv4_2_2_identity_a/BatchNorm/moving_mean': [256],
 'conv4_2/conv4_2_2_identity_a/BatchNorm/moving_variance': [256],
 'conv4_2/conv4_2_2_identity_a/weights': [1, 1, 1024, 256],
 'conv4_2/conv4_2_2_identity_a/weights/Momentum': [1, 1, 1024, 256],
 'conv4_2/conv4_2_2_identity_b/BatchNorm/beta': [256],
 'conv4_2/conv4_2_2_identity_b/BatchNorm/beta/Momentum': [256],
 'conv4_2/conv4_2_2_identity_b/BatchNorm/moving_mean': [256],
 'conv4_2/conv4_2_2_identity_b/BatchNorm/moving_variance': [256],
 'conv4_2/conv4_2_2_identity_b/weights': [3, 3, 256, 256],
 'conv4_2/conv4_2_2_identity_b/weights/Momentum': [3, 3, 256, 256],
 'conv4_2/conv4_2_2_identity_c/BatchNorm/beta': [1024],
 'conv4_2/conv4_2_2_identity_c/BatchNorm/beta/Momentum': [1024],
 'conv4_2/conv4_2_2_identity_c/BatchNorm/moving_mean': [1024],
 'conv4_2/conv4_2_2_identity_c/BatchNorm/moving_variance': [1024],
 'conv4_2/conv4_2_2_identity_c/weights': [1, 1, 256, 1024],
 'conv4_2/conv4_2_2_identity_c/weights/Momentum': [1, 1, 256, 1024],
 'conv4_2/conv4_2_3_identity_a/BatchNorm/beta': [256],
 'conv4_2/conv4_2_3_identity_a/BatchNorm/beta/Momentum': [256],
 'conv4_2/conv4_2_3_identity_a/BatchNorm/moving_mean': [256],
 'conv4_2/conv4_2_3_identity_a/BatchNorm/moving_variance': [256],
 'conv4_2/conv4_2_3_identity_a/weights': [1, 1, 1024, 256],
 'conv4_2/conv4_2_3_identity_a/weights/Momentum': [1, 1, 1024, 256],
 'conv4_2/conv4_2_3_identity_b/BatchNorm/beta': [256],
 'conv4_2/conv4_2_3_identity_b/BatchNorm/beta/Momentum': [256],
 'conv4_2/conv4_2_3_identity_b/BatchNorm/moving_mean': [256],
 'conv4_2/conv4_2_3_identity_b/BatchNorm/moving_variance': [256],
 'conv4_2/conv4_2_3_identity_b/weights': [3, 3, 256, 256],
 'conv4_2/conv4_2_3_identity_b/weights/Momentum': [3, 3, 256, 256],
 'conv4_2/conv4_2_3_identity_c/BatchNorm/beta': [1024],
 'conv4_2/conv4_2_3_identity_c/BatchNorm/beta/Momentum': [1024],
 'conv4_2/conv4_2_3_identity_c/BatchNorm/moving_mean': [1024],
 'conv4_2/conv4_2_3_identity_c/BatchNorm/moving_variance': [1024],
 'conv4_2/conv4_2_3_identity_c/weights': [1, 1, 256, 1024],
 'conv4_2/conv4_2_3_identity_c/weights/Momentum': [1, 1, 256, 1024],
 'conv4_2/conv4_2_4_identity_a/BatchNorm/beta': [256],
 'conv4_2/conv4_2_4_identity_a/BatchNorm/beta/Momentum': [256],
 'conv4_2/conv4_2_4_identity_a/BatchNorm/moving_mean': [256],
 'conv4_2/conv4_2_4_identity_a/BatchNorm/moving_variance': [256],
 'conv4_2/conv4_2_4_identity_a/weights': [1, 1, 1024, 256],
 'conv4_2/conv4_2_4_identity_a/weights/Momentum': [1, 1, 1024, 256],
 'conv4_2/conv4_2_4_identity_b/BatchNorm/beta': [256],
 'conv4_2/conv4_2_4_identity_b/BatchNorm/beta/Momentum': [256],
 'conv4_2/conv4_2_4_identity_b/BatchNorm/moving_mean': [256],
 'conv4_2/conv4_2_4_identity_b/BatchNorm/moving_variance': [256],
 'conv4_2/conv4_2_4_identity_b/weights': [3, 3, 256, 256],
 'conv4_2/conv4_2_4_identity_b/weights/Momentum': [3, 3, 256, 256],
 'conv4_2/conv4_2_4_identity_c/BatchNorm/beta': [1024],
 'conv4_2/conv4_2_4_identity_c/BatchNorm/beta/Momentum': [1024],
 'conv4_2/conv4_2_4_identity_c/BatchNorm/moving_mean': [1024],
 'conv4_2/conv4_2_4_identity_c/BatchNorm/moving_variance': [1024],
 'conv4_2/conv4_2_4_identity_c/weights': [1, 1, 256, 1024],
 'conv4_2/conv4_2_4_identity_c/weights/Momentum': [1, 1, 256, 1024],
 'conv4_2/conv4_2_5_identity_a/BatchNorm/beta': [256],
 'conv4_2/conv4_2_5_identity_a/BatchNorm/beta/Momentum': [256],
 'conv4_2/conv4_2_5_identity_a/BatchNorm/moving_mean': [256],
 'conv4_2/conv4_2_5_identity_a/BatchNorm/moving_variance': [256],
 'conv4_2/conv4_2_5_identity_a/weights': [1, 1, 1024, 256],
 'conv4_2/conv4_2_5_identity_a/weights/Momentum': [1, 1, 1024, 256],
 'conv4_2/conv4_2_5_identity_b/BatchNorm/beta': [256],
 'conv4_2/conv4_2_5_identity_b/BatchNorm/beta/Momentum': [256],
 'conv4_2/conv4_2_5_identity_b/BatchNorm/moving_mean': [256],
 'conv4_2/conv4_2_5_identity_b/BatchNorm/moving_variance': [256],
 'conv4_2/conv4_2_5_identity_b/weights': [3, 3, 256, 256],
 'conv4_2/conv4_2_5_identity_b/weights/Momentum': [3, 3, 256, 256],
 'conv4_2/conv4_2_5_identity_c/BatchNorm/beta': [1024],
 'conv4_2/conv4_2_5_identity_c/BatchNorm/beta/Momentum': [1024],
 'conv4_2/conv4_2_5_identity_c/BatchNorm/moving_mean': [1024],
 'conv4_2/conv4_2_5_identity_c/BatchNorm/moving_variance': [1024],
 'conv4_2/conv4_2_5_identity_c/weights': [1, 1, 256, 1024],
 'conv4_2/conv4_2_5_identity_c/weights/Momentum': [1, 1, 256, 1024],
 'conv5_1_branch1/BatchNorm/beta': [2048],
 'conv5_1_branch1/BatchNorm/beta/Momentum': [2048],
 'conv5_1_branch1/BatchNorm/moving_mean': [2048],
 'conv5_1_branch1/BatchNorm/moving_variance': [2048],
 'conv5_1_branch1/weights': [1, 1, 1024, 2048],
 'conv5_1_branch1/weights/Momentum': [1, 1, 1024, 2048],
 'conv5_1_branch2_a/BatchNorm/beta': [512],
 'conv5_1_branch2_a/BatchNorm/beta/Momentum': [512],
 'conv5_1_branch2_a/BatchNorm/moving_mean': [512],
 'conv5_1_branch2_a/BatchNorm/moving_variance': [512],
 'conv5_1_branch2_a/weights': [1, 1, 1024, 512],
 'conv5_1_branch2_a/weights/Momentum': [1, 1, 1024, 512],
 'conv5_1_branch2_b/BatchNorm/beta': [512],
 'conv5_1_branch2_b/BatchNorm/beta/Momentum': [512],
 'conv5_1_branch2_b/BatchNorm/moving_mean': [512],
 'conv5_1_branch2_b/BatchNorm/moving_variance': [512],
 'conv5_1_branch2_b/weights': [3, 3, 512, 512],
 'conv5_1_branch2_b/weights/Momentum': [3, 3, 512, 512],
 'conv5_1_branch2_c/BatchNorm/beta': [2048],
 'conv5_1_branch2_c/BatchNorm/beta/Momentum': [2048],
 'conv5_1_branch2_c/BatchNorm/moving_mean': [2048],
 'conv5_1_branch2_c/BatchNorm/moving_variance': [2048],
 'conv5_1_branch2_c/weights': [1, 1, 512, 2048],
 'conv5_1_branch2_c/weights/Momentum': [1, 1, 512, 2048],
 'conv5_2/conv5_2_1_identity_a/BatchNorm/beta': [512],
 'conv5_2/conv5_2_1_identity_a/BatchNorm/beta/Momentum': [512],
 'conv5_2/conv5_2_1_identity_a/BatchNorm/moving_mean': [512],
 'conv5_2/conv5_2_1_identity_a/BatchNorm/moving_variance': [512],
 'conv5_2/conv5_2_1_identity_a/weights': [1, 1, 2048, 512],
 'conv5_2/conv5_2_1_identity_a/weights/Momentum': [1, 1, 2048, 512],
 'conv5_2/conv5_2_1_identity_b/BatchNorm/beta': [512],
 'conv5_2/conv5_2_1_identity_b/BatchNorm/beta/Momentum': [512],
 'conv5_2/conv5_2_1_identity_b/BatchNorm/moving_mean': [512],
 'conv5_2/conv5_2_1_identity_b/BatchNorm/moving_variance': [512],
 'conv5_2/conv5_2_1_identity_b/weights': [3, 3, 512, 512],
 'conv5_2/conv5_2_1_identity_b/weights/Momentum': [3, 3, 512, 512],
 'conv5_2/conv5_2_1_identity_c/BatchNorm/beta': [2048],
 'conv5_2/conv5_2_1_identity_c/BatchNorm/beta/Momentum': [2048],
 'conv5_2/conv5_2_1_identity_c/BatchNorm/moving_mean': [2048],
 'conv5_2/conv5_2_1_identity_c/BatchNorm/moving_variance': [2048],
 'conv5_2/conv5_2_1_identity_c/weights': [1, 1, 512, 2048],
 'conv5_2/conv5_2_1_identity_c/weights/Momentum': [1, 1, 512, 2048],
 'conv5_2/conv5_2_2_identity_a/BatchNorm/beta': [512],
 'conv5_2/conv5_2_2_identity_a/BatchNorm/beta/Momentum': [512],
 'conv5_2/conv5_2_2_identity_a/BatchNorm/moving_mean': [512],
 'conv5_2/conv5_2_2_identity_a/BatchNorm/moving_variance': [512],
 'conv5_2/conv5_2_2_identity_a/weights': [1, 1, 2048, 512],
 'conv5_2/conv5_2_2_identity_a/weights/Momentum': [1, 1, 2048, 512],
 'conv5_2/conv5_2_2_identity_b/BatchNorm/beta': [512],
 'conv5_2/conv5_2_2_identity_b/BatchNorm/beta/Momentum': [512],
 'conv5_2/conv5_2_2_identity_b/BatchNorm/moving_mean': [512],
 'conv5_2/conv5_2_2_identity_b/BatchNorm/moving_variance': [512],
 'conv5_2/conv5_2_2_identity_b/weights': [3, 3, 512, 512],
 'conv5_2/conv5_2_2_identity_b/weights/Momentum': [3, 3, 512, 512],
 'conv5_2/conv5_2_2_identity_c/BatchNorm/beta': [2048],
 'conv5_2/conv5_2_2_identity_c/BatchNorm/beta/Momentum': [2048],
 'conv5_2/conv5_2_2_identity_c/BatchNorm/moving_mean': [2048],
 'conv5_2/conv5_2_2_identity_c/BatchNorm/moving_variance': [2048],
 'conv5_2/conv5_2_2_identity_c/weights': [1, 1, 512, 2048],
 'conv5_2/conv5_2_2_identity_c/weights/Momentum': [1, 1, 512, 2048],
 'global_step': [],
 'learning_rate': []}

Process finished with exit code 0

 The network configuration can be seen previously. It should be noticed that I use  keras.layers.GlobalAveragePooling2D() to define global average pooling layer. So how should I extract the features in ""gap"" layer? ",sorry basic clearly see node gap graph print available gap node following available variable graph load path process finished exit code network configuration seen previously use define global average layer extract gap layer,issue,negative,positive,neutral,neutral,positive,positive
369475126,"The training speed.
'ds = dataset.ILSVRC12(datadir, name, shuffle=False)'.
When shuffle = False, the result is the following:
`[0301 12:56:30 @concurrency.py:38] Starting EnqueueThread QueueInput/input_queue ...
[0301 12:56:30 @graph.py:72] Running Op sync_variables/sync_variables_from_main_tower ...
[0301 12:56:31 @concurrency.py:38] Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...
[0301 12:56:31 @base.py:255] Start Epoch 1 ...
  0%|          |0/5000[00:00<?,?it/s][0301 12:56:31 @input_source.py:496] Pre-filling StagingArea ...
[0301 12:56:32 @input_source.py:500] Successfully put 1 element to StagingArea.
  1%|1         |66/5000[01:56<2:50:12, 0.48it/s]`

'ds = dataset.ILSVRC12(datadir, name, shuffle=True)'.
When shuffle = True, the result is the following:
`[0301 12:59:26 @concurrency.py:38] Starting EnqueueThread QueueInput/input_queue ...
[0301 12:59:26 @graph.py:72] Running Op sync_variables/sync_variables_from_main_tower ...
[0301 12:59:27 @concurrency.py:38] Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...
[0301 12:59:27 @base.py:255] Start Epoch 1 ...
  0%|          |0/5000[00:00<?,?it/s][0301 12:59:27 @input_source.py:496] Pre-filling StagingArea ...
[0301 12:59:29 @input_source.py:500] Successfully put 1 element to StagingArea.
  0%|          |10/5000[01:06<8:40:35, 0.16it/s]`

Shuffle will cost more time, I can understand this situation. But the speed is too slow. This is my computer information.
![image](https://user-images.githubusercontent.com/12790408/36827848-fd1185b4-1d50-11e8-91e0-946c49b7af6a.png)
The imagenet dataset is saved in HDD disk.",training speed name shuffle false result following starting running starting start epoch successfully put element name shuffle true result following starting running starting start epoch successfully put element shuffle cost time understand situation speed slow computer information image saved disk,issue,positive,positive,positive,positive,positive,positive
369474472,"Anyway, since fake data is faster than true data, your hardware is probably not fast enough to run the given dataflow and the tutorials have the information on how to benchmark and improve dataflow.",anyway since fake data faster true data hardware probably fast enough run given information improve,issue,negative,positive,neutral,neutral,positive,positive
369473624,"Please be more clear about what you've done. What is 5.99 and 0.53it/s? Are they the speed of data or the speed of training? By saying you ""set"" a line of code, what exactly is happening to your code?",please clear done speed data speed training saying set line code exactly happening code,issue,positive,positive,positive,positive,positive,positive
369473261,"I have test the code by the fake data, the speed can reach 5.99 it/s. However, it is only 0.53 it/s using the ILSVRC12. And in this case, i set 'ds = dataset.ILSVRC12(datadir, name, shuffle=False)'.
It makes full use of CPU, but the util of GPU is 0 in most of time.

I  use imagenet-resnet.py without changing.
",test code fake data speed reach however case set name full use time use without,issue,negative,negative,neutral,neutral,negative,negative
369450534,"For speed, read http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html.
Also note that the speed of first 50 iterations are not reliable.",speed read also note speed first reliable,issue,negative,positive,positive,positive,positive,positive
369450333,"Thanks a lot, I found a lower version of msgpack. I uninstall it, and it works. but the speed is very slow, my computer has two 1080s. The util of GPU is always 0.
`[0301 10:05:46 @graph.py:72] Running Op sync_variables/sync_variables_from_main_tower ...
[0301 10:05:46 @concurrency.py:38] Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...
[0301 10:05:47 @base.py:255] Start Epoch 1 ...
  0%|          |0/5000[00:00<?,?it/s][0301 10:05:47 @input_source.py:496] Pre-filling StagingArea ...
[0301 10:05:51 @input_source.py:500] Successfully put 1 element to StagingArea.
  0%|          |12/5000[02:03<13:40:50, 0.10it/s]`",thanks lot found lower version work speed slow computer two always running starting start epoch successfully put element,issue,positive,positive,positive,positive,positive,positive
369449751,Maybe you also need to uninstall msgpack-python first. See https://github.com/msgpack/msgpack-python#pypi-package-name,maybe also need first see,issue,negative,positive,positive,positive,positive,positive
369447911,"msgpack unpackb has the argument 'raw': https://github.com/msgpack/msgpack-python/commit/5569a4efcdc913d343eaff4e55c9b19fafde4268#diff-82bf3c7d2bf3fc852729c754ffa93870 

Maybe you have multiple msgpack installed in different places.",argument maybe multiple different,issue,negative,neutral,neutral,neutral,neutral,neutral
369027905,"Thank you very much.

On Tue, Feb 27, 2018 at 4:09 PM, Yuxin Wu <notifications@github.com> wrote:

> To do inference during training, https://github.com/ppwwyyxx/
> tensorpack/blob/26d792d490cea55855f10e9f02da738726eb664b/examples/GAN/
> CycleGAN.py#L185-L201.
>
> To do inference after training, https://github.com/ppwwyyxx/
> tensorpack/blob/26d792d490cea55855f10e9f02da738726eb664b/examples/HED/hed.
> py#L217-L228
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/677#issuecomment-369026786>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AfQ3TneipVgikcJnVjEhCo68wvEu46q4ks5tZG8GgaJpZM4SVCG0>
> .
>
",thank much tue wrote inference training inference training thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
369026786,"To do inference during training, https://github.com/ppwwyyxx/tensorpack/blob/26d792d490cea55855f10e9f02da738726eb664b/examples/GAN/CycleGAN.py#L185-L201.

To do inference after training, https://github.com/ppwwyyxx/tensorpack/blob/26d792d490cea55855f10e9f02da738726eb664b/examples/HED/hed.py#L217-L228",inference training inference training,issue,negative,neutral,neutral,neutral,neutral,neutral
369023307,"Thank you for your advice. Is there an example about this? This will be
more helpful.

On Tue, Feb 27, 2018 at 11:38 AM, Yuxin Wu <notifications@github.com> wrote:

> If you want to do something during training, see above comments.
> If you want to do something after training, see
> http://tensorpack.readthedocs.io/en/latest/tutorial/inference.html.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/677#issuecomment-368941586>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AfQ3Tq1y0Q-NwsSG8KyN_7mBKzz5GgZvks5tZC-ggaJpZM4SVCG0>
> .
>
",thank advice example helpful tue wrote want something training see want something training see thread reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
368941586,"If you want to do something during training, see above comments.
If you want to do something after training, see http://tensorpack.readthedocs.io/en/latest/tutorial/inference.html.",want something training see want something training see,issue,negative,neutral,neutral,neutral,neutral,neutral
368939930,That's why I would recommend you to build the inference graph by yourself. See http://tensorpack.readthedocs.io/en/latest/tutorial/inference.html#inference-after-training ,would recommend build inference graph see,issue,negative,neutral,neutral,neutral,neutral,neutral
368913693,Seems like you already extract features as 'prelogits' tensor. Now you just need to write a callback where you do something with that tensor after each k iterations or each k epochs.,like already extract tensor need write something tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
368785732,"Thanks for your advices. However, I found more than 50k node names in the graph and I have no idea which are the input and output node names. ",thanks however found node graph idea input output node,issue,negative,positive,positive,positive,positive,positive
368771835,The name would depend how keras implement the layer so I don't know -- and it may change across versions. Look at the graph or print the nodes to find it out.,name would depend implement layer know may change across look graph print find,issue,negative,neutral,neutral,neutral,neutral,neutral
368771496,"Ohhh, thanks! I just realise that 'fc1000/kernel' is a variable. So the output node should be 'fc1000'? But it says `AssertionError: fc1000 is not in graph` after trying. ",thanks variable output node graph trying,issue,negative,positive,positive,positive,positive,positive
368765132,"Because you set the output node name to the variable, only the variable will be left.",set output node name variable variable left,issue,negative,neutral,neutral,neutral,neutral,neutral
368762679,"I update my tensorflow, and it works well.
Thanks you!",update work well thanks,issue,positive,positive,positive,positive,positive,positive
368723740,"I have checked the generated LMDB. It is about 149.7GB.
I will change the environment and try it again.
Thank you for your share and kind reply!
Thanks a lot!",checked change environment try thank share kind reply thanks lot,issue,positive,positive,positive,positive,positive,positive
368722161,Closing due to inactivity. Feel free to reopen if you want to discuss more.,due inactivity feel free reopen want discus,issue,positive,positive,positive,positive,positive,positive
368715755,Would be nice to train on other datasets. But I don't have time to do this in the near future.,would nice train time near future,issue,negative,positive,positive,positive,positive,positive
368712877,"I read raw jpegs. But reading from LMDB will have similar performance.

The numbers in the first table does not use preact.",read raw reading similar performance first table use preact,issue,negative,positive,neutral,neutral,positive,positive
368712107,"What the manner do you choose for training your resnet models? With LMDB file or reading from list randomly? 
Besides, is the performance reported in your README of Resnet evaluated on validation dataset without pre-activation?
Thanks a lot!",manner choose training file reading list randomly besides performance validation without thanks lot,issue,negative,negative,negative,negative,negative,negative
368711296,"Yes, I am talking about matterport implementation. Actually, I am using your code and matterport to work with DSB2018 dataset challence. I was successful to run it with matterport but I was not successful with your project. My dataset is not similar coco format ( only contains raw image and its masks). In matterport, I based on example of training shape  https://github.com/matterport/Mask_RCNN/blob/master/train_shapes.ipynb
I am very happy if you can make an example like that, because it may be helpful to work in difference dataset. Thanks",yes talking implementation actually code work successful run successful project similar coco format raw image based example training shape happy make example like may helpful work difference thanks,issue,positive,positive,positive,positive,positive,positive
368709518,"I'm not familiar with other implementations except for Detectron so I don't know what to answer.
If you're talking about matterport implementation, it has a lot of differences from the original paper, some of which are mentioned in their readme. Given all those differences it's hard to say what contribute to the suboptimal performance.",familiar except know answer talking implementation lot original paper given hard say contribute suboptimal performance,issue,negative,positive,positive,positive,positive,positive
368706517,"Hi ppwwppxx, could you tell me any reason why your performance (32.3%) is better than keras based (29.5%) using same resnet-50?",hi could tell reason performance better based,issue,negative,positive,positive,positive,positive,positive
368704656,I can reproduce the issue in docker. Looks like I'm using zmq socket in a wrong way where messages can get lost if the sockets aren't created with a desired order. ,reproduce issue docker like socket wrong way get lost desired order,issue,negative,negative,negative,negative,negative,negative
368703925,"Oh you're using improved-wgan, which requires 2nd order gradient. 
2nd order gradient for fused batch norm is only supported since TF 1.4: https://github.com/tensorflow/tensorflow/commit/4f3b13cc650ec08a246c16d31f9a10cf5bb96d65#diff-09d0717441656db7d7338b7febc2251a
I'll add a note in the code.
Closing as this is not a tensorpack issue.",oh order gradient order gradient fused batch norm since add note code issue,issue,negative,neutral,neutral,neutral,neutral,neutral
368654963,"This static hack really made my PyCharm happy. Single line ``from tensorpack import *`` is enough. 

Thank you!",static hack really made happy single line import enough thank,issue,positive,positive,positive,positive,positive,positive
368561676,"I don't know what happened. This is not an issue I can reproduce, but the code looks fine. Maybe check the lmdb file, something may went wrong when you generate it. For example if it's smaller than 140G then it's incomplete.",know issue reproduce code fine maybe check file something may went wrong generate example smaller incomplete,issue,negative,negative,neutral,neutral,negative,negative
368439688,"I could reproduce the issue using this docker file. Could you try it please?

[Dockerfile.txt](https://github.com/ppwwyyxx/tensorpack/files/1757877/Dockerfile.txt)
[test_tensorpack_inf_loop.txt](https://github.com/ppwwyyxx/tensorpack/files/1757878/test_tensorpack_inf_loop.txt)
Please remove .txt extensions from Dockerfile and replace it for `test_tensorpack_inf_loop.txt` by `py`. Github does not allow include py and dockerfile as is.

",could reproduce issue docker file could try please please remove replace allow include,issue,positive,neutral,neutral,neutral,neutral,neutral
368430062,"
1. ![image](https://user-images.githubusercontent.com/18508335/36660813-65448cce-1b14-11e8-82c9-5094e83df48e.png)

2. ![image](https://user-images.githubusercontent.com/18508335/36660741-31be2fb8-1b14-11e8-87a6-f7d70d3a594b.png)

I run the two on the same computer with same environment: TF1.40, Python2.7, and install the tensorpack use: pip install -U git+https://github.com/ppwwyyxx/tensorpack.git.
",image image run two computer environment install use pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
368430007,"Pushed the static hacks.
Basically all methods are hacks because no IDEs can be smart enough to understand python. I can push some of these hacks to perhaps make things better. But I don't know whether it would actually work for people with different IDEs. 
Now I assume a static analyzer can at least figure out all the dynamic imports. If people are interested, please try them out and make suggestions on improving it (because I don't really know how to hack these IDEs).",static basically ides smart enough understand python push perhaps make better know whether would actually work people different ides assume static analyzer least figure dynamic people interested please try make improving really know hack ides,issue,positive,positive,positive,positive,positive,positive
368420093,"Strange configuration I have. Here is a list of versions of some of packages I have:
```
msgpack (0.5.6)
msgpack-numpy (0.4.3)
msgpack-python (0.5.5)
numpy (1.14.1)
pickleshare (0.7.4)
pip (9.0.1)
pyzmq (17.0.0)
tensorpack (0.8.1)
```
on Ubuntu 16.04",strange configuration list pip,issue,negative,negative,neutral,neutral,negative,negative
368420020,"It's better to paste the changes (in the form of diff, for example) you made instead of describing them. As you can see from the comment above it's not easy to describe them accurately.

In addition to that there are a lot of other issues that may matter. What's the version of tensorflow? How do you install tensorpack? For the two runs you mentioned originally, are they on the same system with consistent software stack?",better paste form example made instead see comment easy describe accurately addition lot may matter version install two originally system consistent stack,issue,positive,positive,positive,positive,positive,positive
368419085,The code successfully exits. I use the latest version of msgpack & msgpack-numpy. Maybe also try upgrading pyzmq.,code successfully use latest version maybe also try,issue,negative,positive,positive,positive,positive,positive
368418698,"Could you try please this code too:
```python
from tensorpack.dataflow import MultiProcessMapDataZMQ, RNGDataFlow, MultiThreadMapData


dataset = [(""file_%i"" % i, i % 3) for i in range(500)]

class TestDataset(RNGDataFlow):
    
    def __init__(self, dataset):
        super(TestDataset, self).__init__()
        self.dataset = dataset
        
    def size(self):
        return len(self.dataset)
        
    def get_data(self):
        for k in range(len(self.dataset)):
            yield self.dataset[k]            

test_dataset = TestDataset(dataset)
test_dataset.size()

def data_transform(dp):
    return dp

mp_test_dataset = MultiProcessMapDataZMQ(test_dataset, nr_proc=10, map_func=data_transform, strict=True)

mp_test_dataset.reset_state()


for i, (fp, _) in enumerate(mp_test_dataset.get_data()):
    if i % 100 == 0:
        print(i, end="" . "", flush=True)
```

Okay, I'll try to upgrade msgpack, which version do you use ?
Version of msgpack I use is `msgpack (0.5.6)`  ",could try please code python import range class self super self size self return self range yield return enumerate print try upgrade version use version use,issue,positive,positive,positive,positive,positive,positive
368416466,"> The codes are copyed from your tutorials and shown as above.

Then you're using a different batch size compared to the original code.",shown different batch size original code,issue,negative,positive,positive,positive,positive,positive
368415482,"I didn't change other codes but only the codes for reading training data from LMDB file. The codes are copyed from your tutorials and shown as above. 
I use the commond:  ./imagenet-resnet.py --data /path/to/original/ILSVRC --gpu 0,1,2,3 -d 50 --mode resnet",change reading training data file shown use data mode,issue,negative,neutral,neutral,neutral,neutral,neutral
368387634,The above commit should fix the problem. Another way to fix the problem is to upgrade your tensorflow to 1.5,commit fix problem another way fix problem upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
368371309,"When i upgrade tensorpack to 0.8.1, there is another error has appeared. the error traces are as follows:

![tperror2](https://user-images.githubusercontent.com/4050099/36650334-d77a5c06-1add-11e8-9c74-6d3339ee5040.png)


Next i will try horovod trainer ,see if it can give me a reasonable speedup",upgrade another error error next try trainer see give reasonable,issue,negative,positive,neutral,neutral,positive,positive
368354242,"Actually, I have already updated to the master. Try a smaller size, for example (10, 10, 3) in FakeData",actually already master try smaller size example,issue,negative,neutral,neutral,neutral,neutral,neutral
368320971,"For speed, read tutorial: http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html
For accuracy: I don't know what happened -- any line of code that's written wrong could cause it. If you can post more information following the issue template maybe I'll be able to tell more. ",speed read tutorial accuracy know line code written wrong could cause post information following issue template maybe able tell,issue,negative,neutral,neutral,neutral,neutral,neutral
368319933,"1. You're using a very old tensorpack. Please upgrade and try again.
2. Tensorflow does not have a native fast distributed trainer. Only [horovod trainer](https://github.com/tensorpack/benchmarks/tree/master/ResNet-Horovod) can get a reasonable speedup.",old please upgrade try native fast distributed trainer trainer get reasonable,issue,negative,positive,positive,positive,positive,positive
368158365,"> everything will appear as if the training was never interrupted.

Not exactly.. because how hacky the epoch number is restored, the new epoch number is not visible to hyperparametersetter. Therefore if the learning rate is scheduled by epoch, it will be wrong after restore.

Just one more reason for me to remove the feature... or at least do it differently",everything appear training never interrupted exactly hacky epoch number new epoch number visible therefore learning rate epoch wrong restore one reason remove feature least differently,issue,negative,negative,neutral,neutral,negative,negative
368141734,"Oh but one thing you're correct: tensorflow pads zeros on borders, which makes my implementation of roi_align also wrong. But it's only wrong on the borders, so it's not a big deal. Usually boxes are not on the borders.
I put some proof-of-concept snippet in the code for people to verify the problem of `tf.image.crop_and_resize`.
```python
    import numpy as np
    import tensorflow.contrib.eager as tfe
    tfe.enable_eager_execution()

    # want to crop 2x2 out of a 5x5 image, and resize to 4x4
    image = np.arange(25).astype('float32').reshape(5, 5)
    boxes = np.asarray([[1, 1, 3, 3]], dtype='float32')
    target = 4

    print(crop_and_resize(
        image[None, None, :, :], boxes, [0], target)[0][0])
    """"""
    Expected values:
    4.5 5 5.5 6
    7 7.5 8 8.5
    9.5 10 10.5 11
    12 12.5 13 13.5
    Our implementation is not perfect either. When boxes are on the border of
    images, TF pads zeros instead of border values. But this rarely happens so it's fine.

    You cannot easily get the above results with tf.image.crop_and_resize.
    Try out yourself here:
    """"""
    print(tf.image.crop_and_resize(
        image[None, :, :, None],
        np.asarray([[1, 1, 2, 2]]) / 4.0, [0], [target, target])[0][:, :, 0])
```",oh one thing correct implementation also wrong wrong big deal usually put snippet code people verify problem python import import want crop image resize image target print image none none target implementation perfect either border instead border rarely fine easily get try print image none none target target,issue,negative,positive,neutral,neutral,positive,positive
368139608,"There was an issue about it #542. The short answer is ""this is how tensorflow works"".
I don't want to explain it more because it's complicated and you need to read tensorflow source code to figure out everything.",issue short answer work want explain complicated need read source code figure everything,issue,negative,negative,negative,negative,negative,negative
368104674,"I thought huber loss returns NaN on emtpy tensors. Turned out that it seems to return 0.
Another reason could be that you provide some boxes that have zero area. As a result `encode_bbox_target` will return a tensor with NaN values.",thought loss nan turned return another reason could provide zero area result return tensor nan,issue,negative,neutral,neutral,neutral,neutral,neutral
368094228,"It could be waiting data from a queue, from a named pipe, from shared memory, etc.",could waiting data queue pipe memory,issue,negative,neutral,neutral,neutral,neutral,neutral
368092492,Wouldn't the child receive an EOF? All file descriptors are closed when a process is terminated by any means right? Therefore the child would receive an EOF.,would child receive file closed process right therefore child would receive,issue,negative,positive,neutral,neutral,positive,positive
367975460,"box_loss is defined at: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/model.py#L429

since huber loss in tensorflow is defined as :
![image](https://user-images.githubusercontent.com/16185531/36589521-8d2d88b2-18c6-11e8-9786-9e16157ba05f.png)
it's robust and  will never cause NaN issues.
and tf.to_float(tf.shape(labels)[0]) in L431 is constant 256 since config.FASTRCNN_BATCH_PER_IM is set as 256.
so it will nerver be NaN whether with or without  valid foreground boxes, am I right ?  Is it possible the NaN issue is caused by moving average calculation?


",defined since loss defined image robust never cause nan constant since set nerver nan whether without valid foreground right possible nan issue moving average calculation,issue,negative,positive,neutral,neutral,positive,positive
367874637,For example when parent dies and a child is waiting data from the parent.,example parent child waiting data parent,issue,negative,neutral,neutral,neutral,neutral,neutral
367868342,"Are there any scenarios where it could get stuck in IO? I thought IO waits are by definition uninterruptible right? Process state 'D', if I am not mistaken?",could get stuck io thought io definition uninterruptible right process state mistaken,issue,negative,positive,positive,positive,positive,positive
367844525,"I didn't think about it thoroughly, but very likely you provide some training data without valid foreground boxes, causing NaN. However this was assured by the current data processing code. I think I'll just add an assertion somewhere.",think thoroughly likely provide training data without valid foreground causing nan however assured current data code think add assertion somewhere,issue,negative,neutral,neutral,neutral,neutral,neutral
367820144,"If prctl solves the problem for Linux I'll use it. You're welcomed to add better support for windows if you like.

> Another really clever method is the following:

This assumes the child is still able to run code when parent dies, which is not necessarily true. The child can be stuck in IO.",problem use add better support like another really clever method following child still able run code parent necessarily true child stuck io,issue,positive,positive,positive,positive,positive,positive
367814298,"NVM, I understand it's a hook on the child. Still this would only solve the issue on Linux. There is a way of dealing with this on Windows, but it's far more complicated. The only reliable way would be to pole the parent from the [subprocesses](https://stackoverflow.com/questions/23434842/python-how-to-kill-child-processes-when-parent-dies). The signal solution would solve pretty much ever other case though with the exception of SIGKILL and without the need for a Linux specific non-UNIX call for an edge case. Thoughts?

Another really clever method is the [following](https://stackoverflow.com/a/42924532/2444240): 
-    pass stdin pipe to child - you don't have to write any data into the stream.
-    Child reads indefinitely from stdin until EOF. An EOF signals that the parent has gone.
-    This is foolproof and portable way to detect when the parent has gone. 
",understand hook child still would solve issue way dealing far complicated reliable way would pole parent signal solution would solve pretty much ever case though exception without need specific call edge case another really clever method following pas pipe child write data stream child indefinitely parent gone foolproof portable way detect parent gone,issue,positive,positive,neutral,neutral,positive,positive
367810994,Can that library catch it though? I was under the impression that NOTHING can catch SIGKILL. That's the point of SIGKILL isn't it? ,library catch though impression nothing catch point,issue,negative,neutral,neutral,neutral,neutral,neutral
367792376,"Using `self.trainer.get_predictor()` in a callback is the easiest way to do it. See http://tensorpack.readthedocs.io/en/latest/tutorial/inference.html.

You can also write:
```python
class A(Inferencer):
    def _get_fetches(self):
        return ['conv0/output:0']

    def _on_fetches(self, outputs):
        print(outputs)
```",easiest way see also write python class self return self print,issue,negative,neutral,neutral,neutral,neutral,neutral
367781290,Wouldn't [signals](https://docs.python.org/2/library/signal.html?#signal.signal) be a more precise cross-platform way of handling it though?,would precise way handling though,issue,negative,positive,positive,positive,positive,positive
367761717,"A trivial fix would be to check for empty boxes, e.g. https://github.com/ppwwyyxx/tensorpack/commit/7f55d502aa181073f6ca0b9f2d4131c2c87369c7#diff-56a46708ffaa79088f015119c942d470 ",trivial fix would check empty,issue,negative,negative,neutral,neutral,negative,negative
367761329,Oh there was a similar issue before. This doesn't affect training. What happens is at some iteration there could be no valid boxes so box loss became NaN. Then the moving average stays NaN after that.,oh similar issue affect training iteration could valid box loss nan moving average stay nan,issue,negative,negative,neutral,neutral,negative,negative
367759669,atexit only get executed when the process exits in a clean way.,get executed process clean way,issue,negative,positive,positive,positive,positive,positive
367758828,"Hmm, I think you might need to use the more specific trick of deleting programmatically setting a boolean to false to fool some of the other IDEs. They may ignore constant false blocks. If it was that easy, I would expect more projects to use that instead of the statica hack. ",think might need use specific trick programmatically setting false fool ides may ignore constant false easy would expect use instead hack,issue,negative,negative,neutral,neutral,negative,negative
367757781,Can't we just use an atexit hook to do this? Don't see why another linux specific dependency would be necessary.,ca use hook see another specific dependency would necessary,issue,negative,neutral,neutral,neutral,neutral,neutral
367534148,"For 1, I found that `__all__` is necessary for sphinx to work. As a result, the current code (dynamically populate `__all__`) has to be kept. 
However, the static hack can probably tell parser how to find each name. 
I found that adding
```python
if False:
    from .model_desc import *
    from .training import *
    from .distributed import *
    from .predict import *
    from .utils import *
```
is enough for jedi to autocomplete everything. So I'll push this first.",found necessary sphinx work result current code dynamically populate kept however static hack probably tell parser find name found python false import import import import import enough everything push first,issue,negative,positive,neutral,neutral,positive,positive
367531252,"You could use pdb to debug it, but I suggest first trying to specfy checkpoints it in the same was as done in the test -- https://github.com/openai/gradient-checkpointing/blob/65bf6a97dd5244b5e0b5130cd2cc9d10575c224c/test/resnet_correctness_test.py#L247",could use suggest first trying done test,issue,negative,positive,positive,positive,positive,positive
367509930,@yaroslavvb I tried it and got exactly the same error as above. Why the above method does not work (supplying tensors list to checkpoints arg)? ,tried got exactly error method work list,issue,negative,positive,positive,positive,positive,positive
367506441,Closing due to inactivity. The conclusion seems to be slow hardware.,due inactivity conclusion slow hardware,issue,negative,negative,negative,negative,negative,negative
367440792,"These are two separate issues here:
1. The dynamic import in `__init__.py`  under each subfolder. Now it simply does `import *` of each file under the folder. I think it's OK to replace it with a bunch of explicit imports:
```python
from .model_desc import *
from .training import *
from .distributed import *
from .predict import *
from .utils import *
```

2. The top-level `__init__.py` conditions on `HAS_TF`. The [static hack](https://github.com/celery/kombu/blob/master/kombu/__init__.py#L24-L35) seems fine. Does this hack work with pycharm & sublime?",two separate dynamic import simply import file folder think replace bunch explicit python import import import import import static hack fine hack work sublime,issue,positive,positive,positive,positive,positive,positive
367432236,"So I tried [jedi](https://github.com/davidhalter/jedi). In my tests, putting names of objects into `__all__` explicitly doesn't help it to find out where the objects are defined, and as a result it cannot autocomplete the docs & methods of the objects. Seems like it has to import everything line by line in all `__init__` file to make autocompleter happy.",tried explicitly help find defined result like import everything line line file make happy,issue,positive,positive,positive,positive,positive,positive
367425645,I see. I should deal with torch outside of the try block.,see deal torch outside try block,issue,negative,neutral,neutral,neutral,neutral,neutral
367411379,It was correct. The goal is to hide torch so that pyarrow cannot see it.,correct goal hide torch see,issue,negative,neutral,neutral,neutral,neutral,neutral
367409535,"For unexpected problems, please post issues following the issue template. In particular, include what you did (changes you made, command you run) and what you observed (logs), and how you installed tensorpack.",unexpected please post following issue template particular include made command run,issue,negative,positive,neutral,neutral,positive,positive
367405524,One solution to this would be to use the [statica hack](https://github.com/celery/kombu/blob/master/kombu/__init__.py#L24-L35) along with [Werkzerg's lazy loading](https://github.com/pallets/werkzeug/blob/master/werkzeug/__init__.py) to have the IDE both be satisfied and also allow for the benefits of dynamic importing: inspired by this [Reddit thread](https://www.reddit.com/r/Python/comments/1bbbwk/whats_your_opinion_on_what_to_include_in_init_py/).,one solution would use hack along lazy loading ide satisfied also allow dynamic inspired thread,issue,positive,positive,neutral,neutral,positive,positive
367402916,"Ah, so is the issue with the HAS_TF variable in the main __init__ file? Seems like it is doing more harm than good. Surely there is a better way to separate out dataflow if that's the case or are there are constructs like try catch that editors can accurately interpret? ",ah issue variable main file like harm good surely better way separate case like try catch accurately interpret,issue,positive,positive,positive,positive,positive,positive
367390652,"I have the same issues in sublime text. Further, it is a waste of computational resources to compute the imports over and over again for autocompletion (even when there is a cache). (I see it as a waste of used brain cells to remember the concrete function names.
I even saw the mess of writing all imports to make autocompletion work in the atom editor by a colleague. 

These unsupported wrong imports would not happen if the __init__ files would be human-readable and ""IDE readable"". Overall the framework has a very clean design. However, this is a recurrent issue in the entire framework.",sublime text waste computational compute even cache see waste used brain remember concrete function even saw mess writing make work atom editor colleague unsupported wrong would happen would ide readable overall framework clean design however recurrent issue entire framework,issue,negative,negative,neutral,neutral,negative,negative
367386923,"Alternatively, are there anyway to get PyCharm to deal with Tensorpack in a friendly manner? From my preliminary research it looks PyCharm can correctly deal with __all__ correctly. Does PyCharm really need static imports? The other option is to provide some kind of short hand to access Tensorpack like Tensorflow uses. Import tp and use tp.graph_builder.ModelDesc and such. That doesn't seem clean either though. I don't think generating all those static imports are the answer even if it's done dynamically.",alternatively anyway get deal friendly manner preliminary research correctly deal correctly really need static option provide kind short hand access like import use seem clean either though think generating static answer even done dynamically,issue,positive,positive,positive,positive,positive,positive
367314893,"Should it be like this (`import pyarrow` before `sys.modules['torch'] = None`) ?
```
try:
    # https://github.com/apache/arrow/pull/1223#issuecomment-359895666
    import sys

    import pyarrow as pa

    old_mod = sys.modules.get('torch', None)
    sys.modules['torch'] = None
    # import pyarrow as pa
    if old_mod is not None:
        sys.modules['torch'] = old_mod
    else:
        del sys.modules['torch']
except ImportError:
pa = None
``` 
In case of no pyarrow package `sys.modules['torch']` is `None` ",like import none try import import pa none none import pa none else except pa none case package none,issue,negative,neutral,neutral,neutral,neutral,neutral
367310940,"Your trainer can ignore loaded variables, it could look something like:

`session_init=SaverRestore(args.load, ignore=['global_step'])`",trainer ignore loaded could look something like,issue,negative,neutral,neutral,neutral,neutral,neutral
367253152,"Above commit adds ""skip_collection"" support in `freeze_variables`. 
Though we provide such functions, in general how to build the layers is not the job of tensorpack. If you need more control over what and how variables are freezed, it's better to learn the usage of tensorflow custom_getter and implement something similar to `freeze_variables` by yourself.",commit support though provide general build job need control better learn usage implement something similar,issue,positive,positive,positive,positive,positive,positive
367249604,"> Tensorpack's builtin trainer minimizes variables in the collection ""TRAINABLE_VARIABLES"", doesn't it? 

It does.

> Can I modify the variable collection to minimize without re-writing the trainer?

You can. As I said you can edit the collection or use a custom getter for the variable scope so that the variables never gets into the collection.

Check tensorflow docs on `tf.get_collection_ref`, `tf.variable_scope` for the usage.

",trainer collection modify variable collection minimize without trainer said edit collection use custom getter variable scope never collection check usage,issue,negative,neutral,neutral,neutral,neutral,neutral
367249182,"Tensorpack's builtin trainer minimizes variables in the collection ""TRAINABLE_VARIABLES"", doesn't it? Can I modify the variable collection to minimize without re-writing the trainer?",trainer collection modify variable collection minimize without trainer,issue,negative,neutral,neutral,neutral,neutral,neutral
367247410,"It depends on what do you mean by ""set variables to be non-trainable"".
`tf.get_variable(trainable=True)` does nothing but adds the variable to a collection.  If what you want is to get the variable out of the collection, either modify the collection manually, or surround any layers with a variable scope with a custom getter. These have nothing to do with tensorpack.",mean set nothing variable collection want get variable collection either modify collection manually surround variable scope custom getter nothing,issue,negative,negative,negative,negative,negative,negative
367246633,"I mean how I can set some variables to be non-trainable when I build the graph with the functions like "".Conv2D"".",mean set build graph like,issue,negative,negative,negative,negative,negative,negative
367208129,"> DumpTensors callback runs in every training step 
> 
Yes. That’s my mistake. 

Any option to print tensor ops outputs during inference phase?

> On Feb 20, 2018, at 5:10 PM, Yuxin Wu <notifications@github.com> wrote:
> 
> DumpTensors callback runs in every training step so I don't see why it makes sense to evaluate inference tensors in it. That's up to you, though.
> 
> The tensor name is supposed to end with ""output:0"". Getting ""FusedBatchnorm:0"" is a bug. I'll fix it soon.
> 
> do you store the average of all the model tensors across the towers?
> 
> No. And as I said tensors on different towers, though have similar names, are not necessarily of the same shape at all.
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",every training step yes mistake option print tensor inference phase wrote every training step see sense evaluate inference though tensor name supposed end output getting bug fix soon store average model across said different though similar necessarily shape state reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
367186318,"Oh, I see. This should probably be mentioned somewhere in documentation.",oh see probably somewhere documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
367179024,"`DumpTensors` callback runs in every __training step__ so I don't see why it makes sense to evaluate inference tensors in it. That's up to you, though.

The tensor name is supposed to end with ""output:0"". Getting ""FusedBatchnorm:0"" is a bug. I'll fix it soon.

> do you store the average of all the model tensors across the towers?

No. And as I said tensors on different towers, though have similar names, are not necessarily of the same shape at all.",every see sense evaluate inference though tensor name supposed end output getting bug fix soon store average model across said different though similar necessarily shape,issue,negative,negative,neutral,neutral,negative,negative
367177373,"Ok, so its called InferenceTower instead of tower0.

One question to understand the framework: Each tower has a copy of the model and gets some number of inputs (=TotalBatchSize/NumGPUs). During inference or when checkpointing, do you store the average of all the model tensors across the towers?",instead tower one question understand framework tower copy model number inference store average model across,issue,negative,negative,negative,negative,negative,negative
367175203,"Is there no such thing as **tower0**/bnorm0/output:0, ie bnorm for tower0? 
This is with respect to resnet topology (very first layer there).

When I dump tower0/bnorm0/output:0 i get an error but not when I print tower1/bnorm0/output:0.",thing tower ie tower respect topology first layer dump get error print,issue,negative,positive,positive,positive,positive,positive
367171156,"To resume training, if you load the last checkpoint and select ""keep"", everything will appear as if the training was never interrupted.",resume training load last select keep everything appear training never interrupted,issue,negative,neutral,neutral,neutral,neutral,neutral
367170489,"Actually, the option ""keep"" does not make sense to me. Why is it needed? 

",actually option keep make sense,issue,negative,neutral,neutral,neutral,neutral,neutral
367159781,"Think of the 4 tensors as 4 independent tensors that happen to have similar names. 
They are not necessarily concat-able at all. 

You can implement this feature by writing a callback, either fetching all tensors to numpy and concat them there or create a concatenated tensor in the graph and fetch it.",think independent happen similar necessarily implement feature writing either fetching create tensor graph fetch,issue,negative,neutral,neutral,neutral,neutral,neutral
367147130,"Yes in this case it's already interacting with users, which I don't like. I actually have never used ""backup"" or ""new"" myself. They turn out to be bad ideas. I usually used `set_logger_dir` and mentally made sure the name doesn't conflict. If I were to rewrite it I would just throw an error when names conflict.

The message was changed to ""Rename the first epoch of this training to epoch #{}."".format(epoch)"". Hope this won't give users an impression that the checkpoint will be loaded.

The epoch number is by design not supposed to be changed dynamically. The current way of changing it is a bit hacky. And because of this change, callbacks that run before and after the `JSONWriter` will see different epoch numbers which is troublesome. Also this overwrites the `starting_epoch` even if users have manually set it, which is also undesirable.",yes case already like actually never used backup new turn bad usually used mentally made sure name conflict rewrite would throw error conflict message rename first epoch training epoch epoch hope wo give impression loaded epoch number design supposed dynamically current way bit hacky change run see different epoch troublesome also even manually set also undesirable,issue,negative,negative,neutral,neutral,negative,negative
367099614,"I agree in general, but in this case you already ask for user input. My suggestion is to ask for the dir name, if the action answer is ""backup"" or ""new"". If you don't think that's useful, fine, let's just change the ""starting from epoch number n"" message and close this ticket.

By the way, I'm curious, what issues did you mean when you said: ""I'm not a big fan of this epoch-loading feature since it has caused other troubles in the past.""",agree general case already ask user input suggestion ask name action answer backup new think useful fine let change starting epoch number message close ticket way curious mean said big fan feature since past,issue,positive,positive,neutral,neutral,positive,positive
367071989,"Before figuring out how to make IDE happy, please note that `from tensorpack.graph_builder.model_desc import ModelDesc` is not guaranteed to work in the future. The official name for this class, as written in the documentation, is [graph_builder.ModelDesc](http://tensorpack.readthedocs.io/en/latest/modules/graph_builder.html#tensorpack.graph_builder.ModelDesc), and it's better to import it with `from tensorpack.graph_builder import ModelDesc`.",make ide happy please note import work future official name class written documentation better import import,issue,positive,positive,positive,positive,positive,positive
367061214,Thanks. Right I can only set the interval of checkpoints in terms of numbers of epochs... right?,thanks right set interval right,issue,negative,positive,positive,positive,positive,positive
366993283,"@PatWie @ppwwyyxx , thank you both. but showing code is also more precise, and help us learning programming skills",thank showing code also precise help u learning,issue,positive,positive,positive,positive,positive,positive
366978476,"Please point people first to docs, not code.
There is a name argument in docs: http://tensorpack.readthedocs.io/en/latest/modules/models.html?highlight=conv2d#tensorpack.models.Conv2D

So there should be no confusion.",please point people first code name argument confusion,issue,negative,positive,positive,positive,positive,positive
366958698,"See https://github.com/ppwwyyxx/tensorpack/blob/0ffdcc44a9288057be69ef9328a16dcc9526fa69/tensorpack/models/registry.py#L77-L101

The function wrapper adds this argument. This has some other subtle side effects like error messages like:
> function expects 5 arguments, but 5 are given

which happens if you forget the name. This basically means:
> function expects **6** arguments, but 5 are given

Hope this changes, when transitioning to `tf.layers`",see function wrapper argument subtle side effect like error like function given forget name basically function given hope,issue,negative,negative,negative,negative,negative,negative
366817174,"Here's an example of using manual checkpoints:

https://github.com/openai/gradient-checkpointing/blob/65bf6a97dd5244b5e0b5130cd2cc9d10575c224c/test/resnet_correctness_test.py#L247

You use the collection to store checkpoint tensors, and then use `strategy=""collection""`",example manual use collection store use collection,issue,negative,neutral,neutral,neutral,neutral,neutral
366815655,"You can use `logger.set_logger_dir`, perhaps with a command line argument to do this.

Interacting with users is usually a bad idea. Despite all the other trouble it could cause, it assumes the existence of a user in front of the terminal, which isn't necessarily true at the first place.",use perhaps command line argument usually bad idea despite trouble could cause existence user front terminal necessarily true first place,issue,negative,negative,neutral,neutral,negative,negative
366814866,"Yes. For example, right now:
```
if act == 'b':
            backup_name = dirname + _get_time_str()
            shutil.move(dirname, backup_name)
```
After multiple runs with different hyperparams it's hard to determine which backup refers to which run. I have to look into log.log.
It would be better if instead I could simply enter by hand the name of the directory that reflects this particular choice of hyperparams, so that when I want to load a model I can immediately see which checkpoint has it.

Not a big deal though, just a thought.",yes example right act multiple different hard determine backup run look would better instead could simply enter hand name directory particular choice want load model immediately see big deal though thought,issue,positive,positive,neutral,neutral,positive,positive
366810917,"What do you mean by ""checkpoint names""? Do you mean the logging directory name?
You can use `logger.set_logger_dir` to set directory name.",mean mean logging directory name use set directory name,issue,negative,negative,negative,negative,negative,negative
366809807,"OK, let's change the message then. Also, the checkpoint names are not very descriptive. It would probably be better to let a user enter a name for a backup/new checkpoint.",let change message also descriptive would probably better let user enter name,issue,negative,positive,positive,positive,positive,positive
366796745,"That's impossible from a design perspective. All the modules are independent of each other. 

Json loader continues the old epoch number so that new data can be appended to the json file, and this has nothing to do with the rest of the world. Maybe the message can be changed a bit to avoid confusion. And frankly I'm not a big fan of this epoch-loading feature since it has caused other troubles in the past.",impossible design perspective independent loader old epoch number new data file nothing rest world maybe message bit avoid confusion frankly big fan feature since past,issue,negative,negative,negative,negative,negative,negative
366770526,"@yaroslavvb Ok, I just specified a list of tensors manually like this:

```
import memory_saving_gradients
from tensorflow.python.ops import gradients
def gradients_memory(ys, xs, grad_ys=None, **kwargs):
    tensors = ['tower0/conv0/Relu:0', 'tower0/group0/block0/conv1/Relu:0']
    return memory_saving_gradients.gradients(ys, xs, grad_ys, checkpoints=tensors, **kwargs)
gradients.__dict__[""gradients""] = gradients_memory
```

However, it ignores them:

```
Traceback (most recent call last):
  File ""/home/michael/tensorpack/examples/ResNet/imagenet-resnet.py"", line 150, in <module>
    launch_train_with_config(config, trainer)
  File ""/home/michael/tensorpack/tensorpack/train/interface.py"", line 87, in launch_train_with_config
    model._build_graph_get_cost, model.get_optimizer)
  File ""/home/michael/tensorpack/tensorpack/utils/argtools.py"", line 182, in wrapper
    return func(*args, **kwargs)
  File ""/home/michael/tensorpack/tensorpack/train/tower.py"", line 161, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/michael/tensorpack/tensorpack/train/trainers.py"", line 155, in _setup_graph
    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)
  File ""/home/michael/tensorpack/tensorpack/graph_builder/training.py"", line 217, in build
    use_vs=[False] + [True] * (len(self.towers) - 1))
  File ""/home/michael/tensorpack/tensorpack/graph_builder/training.py"", line 113, in build_on_towers
    ret.append(func())
  File ""/home/michael/tensorpack/tensorpack/train/tower.py"", line 199, in get_grad_fn
    aggregation_method=self.AGGREGATION_METHOD)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 460, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/michael/tensorpack/examples/ResNet/resnet_model.py"", line 27, in gradients_memory
    return memory_saving_gradients.gradients(ys, xs, grad_ys, checkpoints=tensors, **kwargs)
  File ""/usr/lib/python2.7/memory_saving_gradients.py"", line 184, in gradients
    raise Exception('no checkpoints nodes found or given as input! ')
Exception: no checkpoints nodes found or given as input! 
```
",list manually like import import return however recent call last file line module trainer file line file line wrapper return file line input file line input file line build false true file line file line file line file line return file line raise exception found given input exception found given input,issue,positive,negative,neutral,neutral,negative,negative
366761650,"@yaroslavvb but why did it fail? I'm trying to run default Resnet-18, similar to your deep_imagenet_benchmark.py setup. 

What would be a list of proper nodes to checkpoint for ResNet?",fail trying run default similar setup would list proper,issue,negative,negative,negative,negative,negative,negative
366757683,"The automatic mode failed, so you have to provide checkpoint nodes manually. For manual nodes to save memory, you have to pick sets nodes which separate your computation graph, like described [here](https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9)",automatic mode provide manually manual save memory pick separate computation graph like,issue,positive,neutral,neutral,neutral,neutral,neutral
366757009,"Thanks! Now I'm getting this error:

```
Traceback (most recent call last):
  File ""/home/michael/tensorpack/examples/ResNet/imagenet-resnet.py"", line 144, in <module>
    launch_train_with_config(config, trainer)
  File ""/home/michael/tensorpack/tensorpack/train/interface.py"", line 87, in launch_train_with_config
    model._build_graph_get_cost, model.get_optimizer)
  File ""/home/michael/tensorpack/tensorpack/utils/argtools.py"", line 182, in wrapper
    return func(*args, **kwargs)
  File ""/home/michael/tensorpack/tensorpack/train/tower.py"", line 161, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/michael/tensorpack/tensorpack/train/trainers.py"", line 155, in _setup_graph
    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)
  File ""/home/michael/tensorpack/tensorpack/graph_builder/training.py"", line 217, in build
    use_vs=[False] + [True] * (len(self.towers) - 1))
  File ""/home/michael/tensorpack/tensorpack/graph_builder/training.py"", line 113, in build_on_towers
    ret.append(func())
  File ""/home/michael/tensorpack/tensorpack/train/tower.py"", line 199, in get_grad_fn
    aggregation_method=self.AGGREGATION_METHOD)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 460, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/michael/tensorpack/examples/ResNet/resnet_model.py"", line 13, in gradients_memory
    return memory_saving_gradients.gradients(ys, xs, grad_ys, checkpoints='memory', **kwargs)
  File ""/usr/lib/python2.7/memory_saving_gradients.py"", line 141, in gradients
    raise Exception('unable to find bottleneck tensors! please provide checkpoint nodes manually, or use checkpoints=""speed"".')
Exception: unable to find bottleneck tensors! please provide checkpoint nodes manually, or use checkpoints=""speed"".
```

I tried changing to checkpoints='speed', but then it seems like no checkpoints are being made (the model runs out of memory). 

Not sure if this is a gradient-checkpointing issue, or a tensorpack issue. ",thanks getting error recent call last file line module trainer file line file line wrapper return file line input file line input file line build false true file line file line file line file line return file line raise exception find bottleneck please provide manually use speed exception unable find bottleneck please provide manually use speed tried like made model memory sure issue issue,issue,positive,positive,neutral,neutral,positive,positive
366621532,Can this be done by just using standard python library rather than a 3rd-party one?,done standard python library rather one,issue,negative,neutral,neutral,neutral,neutral,neutral
366617827,"In particular [these files](https://github.com/ppwwyyxx/tensorpack/blob/68edaa0c04a52edc6d8ae54e36b6e0b647e88eeb/tensorpack/models/__init__.py) just do

```python
__all__ = ['BatchNorm', 'BatchRenorm', 'layer_register', 'VariableHolder', 'Conv2D', 
'Deconv2D', 'FullyConnected', 'ImageSample', 'LayerNorm', 'InstanceNorm', 'LinearWrap', 
'Maxout', 'PReLU', 'LeakyReLU', 'BNReLU', 'MaxPooling', 'FixedUnPooling', 'AvgPooling', 
'GlobalAvgPooling', 'BilinearUpSample', 'regularize_cost', 
'l2_regularizer', 'l1_regularizer',
 'Dropout', 'ConcatWith', 'SoftMax']
```

A git-pre-commit-hook could produce these files directly. As far as the other pull-requests are discussed I could start working on this issue. I always need to look up the correct name for ""FixedSizeData, FixedDataSize, ..."". This would help!",particular python could produce directly far could start working issue always need look correct name would help,issue,negative,positive,positive,positive,positive,positive
366548828,"Yeah it's a typo. Should be like so:
```python
from tensorflow.python.ops import gradients
def gradients_memory(ys, xs, grad_ys=None, **kwargs):
    return memory_saving_gradients.gradients(ys, xs, grad_ys, checkpoints='memory', **kwargs)
gradients.__dict__[""gradients""] = gradients_memory 
```
code is from this [issue](https://github.com/openai/gradient-checkpointing/issues/4)",yeah typo like python import return code issue,issue,positive,neutral,neutral,neutral,neutral,neutral
366544259,"@yaroslavvb I'm getting import error:
>>> import tensorflow.python.ops as ops
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'python'
",getting import error import recent call last file line module object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
366485179,"By the way, have you reproduced the result of `140` Mflop, I only got `34.6`, around `2` points away from the paper's results, I used linear decay schedule with other settings as described in the paper.",way result got around away paper used linear decay schedule paper,issue,negative,neutral,neutral,neutral,neutral,neutral
366484796,The paper has many settings. This is just one of them which I got from the authors. You can set it to whatever you want.,paper many one got set whatever want,issue,negative,positive,positive,positive,positive,positive
366392271,"That's a unit test file, so presumably it gets excluded from the pip package to save space. You should be able to download / check it out and run it outside the repo if you want.",unit test file presumably pip package save space able check run outside want,issue,positive,positive,positive,positive,positive,positive
366391639,"@allenlavoie, I just checked and I don't see https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/grappler/memory_optimizer_test.py file on my machine:

> michael@Pascal:/usr/local/lib/python2.7/dist-packages/tensorflow/python/grappler$ ll
> total 40K
> -rw-r--r-- 1 staff 4.9K Feb 15 16:40 cluster.py
> -rw-r--r-- 1 staff 6.2K Feb 15 16:40 cluster.pyc
> -rw-r--r-- 1 staff    0 Feb 15 16:40 __init__.py
> -rw-r--r-- 1 staff  199 Feb 15 16:40 __init__.pyc
> -rw-r--r-- 1 staff 3.3K Feb 15 16:40 item.py
> -rw-r--r-- 1 staff 4.1K Feb 15 16:40 item.pyc
> -rw-r--r-- 1 staff 1.8K Feb 15 16:40 tf_optimizer.py
> -rw-r--r-- 1 staff 1.5K Feb 15 16:40 tf_optimizer.pyc
 
I installed TF 1.6rc1 with pip.",checked see file machine total staff staff staff staff staff staff staff staff pip,issue,negative,neutral,neutral,neutral,neutral,neutral
366380345,"@yaroslavvb, ok thanks, I'll try that as soon as my current test is finished.  

By the way, my modification of conv2d is the following chain of ops:

1. normal conv2d (but with 9x more output feature maps)  <<<that's what consumes all that RAM
2. multiplying the result by a constant mask
3. reshaping the result into groups of feature maps (9 maps per group)
4. doing tf.reduce_sum on each group to combine 9 maps into one.
5. the result is treated as the output of conv2d layer

I just noticed that my code uses different variables to hold the result from step 1 (`ret, conv_masked, and conv_grouped`):
https://github.com/michaelklachko/tensorpack/blob/learning/tensorpack/models/conv2d.py#L195-L197

Does it mean that I'm reserving 3x times as much memory as I really need?",thanks try soon current test finished way modification following chain normal output feature ram multiplying result constant mask result feature per group group combine one result output layer code different hold result step ret mean time much memory really need,issue,negative,positive,neutral,neutral,positive,positive
366379207,"@michaelklachko looks like that's just ""Sum"" and is not currently on the list. Feel free to add it and ping me on a PR.",like sum currently list feel free add ping,issue,positive,positive,positive,positive,positive,positive
366377286,"@michaelklachko most things are cheap except matmul/conv, because you have low ratio of compute to volume of data needed",cheap except low ratio compute volume data,issue,negative,positive,positive,positive,positive,positive
366376799,"@allenlavoie, I'm doing tf.reduce_sum op on the output from a normal conv2d layer:
https://github.com/michaelklachko/tensorpack/blob/learning/tensorpack/models/conv2d.py#L163

is that a cheap op to recompute? What would be its name string to add to your list (e.g. ""ReduceSum""?) ",output normal layer cheap recompute would name string add list,issue,negative,positive,positive,positive,positive,positive
366376331,"I think you need to do the other one, since `tf.gradients` is never called directly by TensorPack, it's probably called indirectly as ops.gradients.

```
import tensorflow.python.ops as ops
ops.__dict__['gradients'] = memory_saving_gradients.gradients_memory
```",think need one since never directly probably indirectly import,issue,negative,negative,neutral,neutral,negative,negative
366375160,"@yaroslavvb, so just to clarify, if I put
```
import memory_saving_gradients
tf.__dict__[""gradients""] = memory_saving_gradients.gradients_memory 
```
here: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/resnet_model.py#L6

It will enable checkpointing for the code launched in 
https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py ?

",clarify put import enable code,issue,negative,neutral,neutral,neutral,neutral,neutral
366368311,"FWIW Grappler's memory optimizer gets reasonable memory savings on many models already (20%ish?) from recomputing ""cheap"" ops like batch norm. May be worth [flipping the config option](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/grappler/memory_optimizer_test.py#L180) in the `RunConfig` passed to your `Session` to check.

If you're using ops which aren't in my [beautiful hard-coded list](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/memory_optimizer.cc#L54) you might need to either [add ""_recompute_hint""](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/grappler/memory_optimizer_test.py#L231) or append to the list.

Not full checkpointing, though, certainly.",grappler memory reasonable memory many already cheap like batch norm may worth option session check beautiful list might need either add append list full though certainly,issue,positive,positive,positive,positive,positive,positive
366366197,"For monkey-patching you don't need to find the place where it's called, just put it in the beginning of the file somewhere.

But if you wanted to make sure you are monkey patching the right thing, you may need to find how it gets called. The trick is to add `import pdb; pdb.set_trace()` into gradients.py file in the beginning of call, ie [here](https://github.com/tensorflow/tensorflow/blob/8210052620ac994cca507eb9e94ba69327189195/tensorflow/python/ops/gradients_impl.py#L478). Then type ""bt"" to see who called it.

You can to modify this file in local installation, ie, on my computer it's this file
`/Users/yaroslav/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py`
",need find place put beginning file somewhere make sure monkey right thing may need find trick add import file beginning call ie type see modify file local installation ie computer file,issue,negative,positive,positive,positive,positive,positive
366364544,"In my case, I designed a new type of a convolutional layer, however to test it, I have to compute 9x more feature maps in each layer. I want to test this on ResNet model, so basically I have to run 9x wider ResNet-18, and I only have 4 GPUs. I already reduced batch size from 256 to 32, and also reduced the default number of feature maps in half. Now it barely fits in memory. I'm afraid further reduction of batch size might lead to issues with batchnorm, and the hyperparams were probably not optimized for such small batches.

That's why I think gradient-checkpointing would be an ideal method for me. 

I'm trying to figure out the right place to override tf.gradients in tensorpack, so I followed the path of optimizers:

https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py#L143
https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/train/trainers.py#L132
https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/train/tower.py#L115

After that I'm lost. 
",case designed new type convolutional layer however test compute feature layer want test model basically run already reduced batch size also reduced default number feature half barely memory afraid reduction batch size might lead probably small think would ideal method trying figure right place override path lost,issue,negative,positive,neutral,neutral,positive,positive
366126326,"you can do something like this
```
import tensorflow as tf
tf.__dict__[""gradients""] = memory_saving_gradients.gradients_memory
```
However, that only overrides `tf.gradients` method called from tensorflow namespace. If you call something like `Optimizer.minimize`, internally, that's going to call `gradients` method from a different namespace, probably this one

```
from tensorflow.python.ops import gradients
```

So to cover those usages, you may have to do
```
import tensorflow.python.ops as ops
ops.__dict__['gradients'] = memory_saving_gradients.gradients_memory
```

The underlying lesson is that this library is not quite production quality. Before going down this route, are you sure your use case needs it? IE, you could use smaller batch size for your resnet",something like import however method call something like internally going call method different probably one import cover may import underlying lesson library quite production quality going route sure use case need ie could use smaller batch size,issue,positive,positive,positive,positive,positive,positive
366120376,"Where should I put the line `tf.__dict__[""gradients""] = memory_saving_gradients.gradients_memory` to enable checkpointing for the imagenet-resnet model? ",put line enable model,issue,negative,neutral,neutral,neutral,neutral,neutral
365906988,"This is non-trivial amount of imports just to make barebone example IDE-friendly:
```python
from tensorpack.graph_builder.model_desc import ModelDesc
from tensorpack.graph_builder.model_desc import InputDesc
from tensorpack.dataflow.raw import FakeData
from tensorpack.dataflow.common import BatchData
from tensorpack.dataflow.parallel import PrefetchDataZMQ
from tensorpack.train.config import TrainConfig
from tensorpack.input_source.input_source import QueueInput
from tensorpack.callbacks.saver import ModelSaver
from tensorpack.callbacks.inference_runner import InferenceRunner
from tensorpack.callbacks.inference import ScalarStats
from tensorpack.tfutils.sessinit import SaverRestore
from tensorpack.train.trainers import SimpleTrainer
from tensorpack.train.interface import launch_train_with_config
import tensorpack.utils.logger as logger
import tensorpack.tfutils.summary as summary
```",amount make barebone example python import import import import import import import import import import import import import import logger import summary,issue,negative,neutral,neutral,neutral,neutral,neutral
365723151,It's pretty easy to monkey-patch Gradient Checkpointing in theory. We don't need any additional support to add it in. Gradient checkpointing is also is not a good option to have on by default because it will not work on all graphs.,pretty easy gradient theory need additional support add gradient also good option default work,issue,positive,positive,positive,positive,positive,positive
365667666,"@ppwwyyxx yes. It works on graph_defs (proto in -> proto out) and also uses heuristics to determine which nodes to copy. It's not exactly checkpointing -- it'll find nodes with multiple consumers and duplicate them to allow to forget their value.  There's an update from @allenlavoie here, it sounds like it'll be a couple of quarters before it's ready https://github.com/openai/gradient-checkpointing/issues/13#issuecomment-360211530",yes work proto proto also determine copy exactly find multiple duplicate allow forget value update like couple quarter ready,issue,positive,positive,positive,positive,positive,positive
365505500,"I still don't understand what exactly is the feature you're requesting.

It seems to be something like ""Pad to a certain shape only if the image is smaller, do something else when image is larger"". I don't think this is a general enough operation to be added to a lilbrary, because ""something else"" can have many different choices, unless you believe one of them is more reasonable than the rest.

Nevertheless if you need any extra augmentations you can implement it by subclassing ImageAugmentor.",still understand exactly feature something like pad certain shape image smaller something else image think general enough operation added something else many different unless believe one reasonable rest nevertheless need extra implement,issue,positive,positive,positive,positive,positive,positive
365474730,"Tensorpack cannot be imported after pytorch, for now. It will be fixed after I upgrade pyarrow.
Relevant commit in pyarrow is: https://github.com/apache/arrow/commit/ff28c7647c1910f1a0d1d97b8ba68b2b554e5ce1",fixed upgrade relevant commit,issue,negative,positive,positive,positive,positive,positive
365450975,"OK, for reference for myself, I ended up doing below. Couldn't figure out the setting for adjusting flush frequency, will dig into that later if that becomes a problem

    return TrainConfig(
        model=Model(),
        ....
        extra_callbacks=train.DEFAULT_CALLBACKS()+[
          MergeAllSummaries(period=1),
        ],
",reference ended could figure setting flush frequency dig later becomes problem return,issue,negative,neutral,neutral,neutral,neutral,neutral
365448344,"ah, just found http://tensorpack.readthedocs.io/en/latest/tutorial/summary.html?highlight=add_moving_summary, looks like it's every epoch, I was getting ""every 6"" because I used larger batch size",ah found like every epoch getting every used batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
365353599,one problem is that centerpaste only works with smaller images. we could change it to just return the image if it is larger or alternatively pad the smaller axis to equal the larger one,one problem work smaller could change return image alternatively pad smaller axis equal one,issue,negative,neutral,neutral,neutral,neutral,neutral
365146342,"I see, it is intergreted into the graph, since it's in the graph, does it performed on gpu?",see graph since graph,issue,negative,neutral,neutral,neutral,neutral,neutral
365144946,"sorry, but where is mean substraction in preprocessing? 
```
    if isTrain:
        augmentors = [
            GoogleNetResize(),
            imgaug.RandomOrderAug(
                [imgaug.BrightnessScale((0.6, 1.4), clip=False),
                 imgaug.Contrast((0.6, 1.4), clip=False),
                 imgaug.Saturation(0.4, rgb=False),
                 # rgb-bgr conversion for the constants copied from fb.resnet.torch
                 imgaug.Lighting(0.1,
                                 eigval=np.asarray(
                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,
                                 eigvec=np.array(
                                     [[-0.5675, 0.7192, 0.4009],
                                      [-0.5808, -0.0045, -0.8140],
                                      [-0.5836, -0.6948, 0.4203]],
                                     dtype='float32')[::-1, ::-1]
                                 )]),
            imgaug.Flip(horiz=True),
        ]
    else:
        augmentors = [
            imgaug.ResizeShortestEdge(256, cv2.INTER_CUBIC),
            imgaug.CenterCrop((224, 224)),
        ]
```",sorry mean substraction conversion copied else,issue,negative,negative,negative,negative,negative,negative
365142746,"```
 imgaug.RandomOrderAug(
                [imgaug.BrightnessScale((0.6, 1.4), clip=False),
                 imgaug.Contrast((0.6, 1.4), clip=False),
                 imgaug.Saturation(0.4, rgb=False),
                 # rgb-bgr conversion for the constants copied from fb.resnet.torch
                 imgaug.Lighting(0.1,
                                 eigval=np.asarray(
                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,
                                 eigvec=np.array(
                                     [[-0.5675, 0.7192, 0.4009],
                                      [-0.5808, -0.0045, -0.8140],
                                      [-0.5836, -0.6948, 0.4203]],
                                     dtype='float32')[::-1, ::-1]
                                 )]),
```
removing some of them doesn't hurt much.",conversion copied removing hurt much,issue,negative,positive,positive,positive,positive,positive
365142536,"could you suggest for like shufflenet example, which preprocessing would more safe to remove? I mean just based on experience, thank you",could suggest like example would safe remove mean based experience thank,issue,positive,positive,neutral,neutral,positive,positive
365141092,"I see, thank you for pointing out, have you tried remove some of the preprocessing so that cpu burden could be reduced a bit?
",see thank pointing tried remove burden could reduced bit,issue,negative,neutral,neutral,neutral,neutral,neutral
365119437,"You can use RunOp as well. There is a difference though -- RunOp is an extra `session.run` call , that's why it uses `_trigger` instead of `_before_run`. If your op only operates on weights, an extra call doesn't hurt anyway.",use well difference though extra call instead extra call hurt anyway,issue,negative,neutral,neutral,neutral,neutral,neutral
365118716,"That said, it seems possible to integrate the two and have a single `PeriodicCallback` that works for every method. Marking this as a potential feature.",said possible integrate two single work every method marking potential feature,issue,negative,negative,neutral,neutral,negative,negative
365116921,`ProcessTensors` uses `_before/after_run` rather than `_trigger`. You need to use `PeriodicRunHooks` to schedule it.,rather need use schedule,issue,negative,neutral,neutral,neutral,neutral,neutral
365004386,"Do you understand the fact that:
with some augmentation, a dataflow could generate infinite number of different images",understand fact augmentation could generate infinite number different,issue,negative,neutral,neutral,neutral,neutral,neutral
364873428,"http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html has everything about this topic.
Three ways you can do this:
1. rename variables in your graph
2. SaverRestore supports the argument to ignore variables
3. convert the model to a dict and remove some variables from it.",everything topic three way rename graph argument ignore convert model remove,issue,negative,neutral,neutral,neutral,neutral,neutral
364675839,"Of course. In addition to training on your own dataset, If you also want to evaluate on your dataset, you need to have data loader for your evaluation set and implement the evaluation metrics.",course addition training also want evaluate need data loader evaluation set implement evaluation metric,issue,negative,neutral,neutral,neutral,neutral,neutral
364665837,"I am also trying to use own data for training. Although it's not difficult to provide masks etc to train.py (by modify data.py and coco.py),  I see many dependencies on pycocotools in eval.py, which makes further work required to make the code run on other dataset",also trying use data training although difficult provide modify see many work make code run,issue,negative,neutral,neutral,neutral,neutral,neutral
364627337,"The umount has the same error as rm, those recipes help track down who is
keeping files open

On Feb 9, 2018 21:18, ""Yuxin Wu"" <notifications@github.com> wrote:

> I'm not quite following. Why do you umount it? Do you need to mount it
> back so you can use it?
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/644#issuecomment-364627256>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AABaHLFqqWAQESD6G2sQ3vQV7-7AsjqDks5tTSasgaJpZM4R_VhP>
> .
>
",error help track keeping open wrote quite following need mount back use state reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
364627256,I'm not quite following. Why do you umount it? Do you need to mount it back so you can use it?,quite following need mount back use,issue,negative,neutral,neutral,neutral,neutral,neutral
364626542,I don't understand why you see 6. How did you log total_cost? If you did nothing special about it the default interval is every epoch.,understand see log nothing special default interval every epoch,issue,negative,positive,positive,positive,positive,positive
364620083,"I kept hitting this error, and it turned out to be due to TensorBoard keeping files open. So the trick is to always kill tensorboard before deleting files. Some tricks for future reference

```
The first command to try is a lazy unmount of the EFS volume. Below is the lazy unmount command.

$ umount -l ""EFS mount path"" 

The command below will find the process that is using the EFS share and output only the process id. Using ""ps aux"" you can find out more about the process. When ready to kill the process parse the below command to ""kill -9"".

$ lsof -t ""efs/share""

The command below is another way of finding and killing the process in one command. Remove ""k"" to view the process without killing it. 

$ fuser -km ""EFS mount path""
```
",kept error turned due keeping open trick always kill future reference first command try lazy unmount volume lazy unmount command mount path command find process share output process id find process ready kill process parse command kill command another way finding killing process one command remove view process without killing mount path,issue,negative,negative,neutral,neutral,negative,negative
364564776,"OK, makes sense. There's slight possibility that frequent event writing will have higher CPU utilization due to  protobuf encoding.  The actual flush interval is pretty long (30 seconds?), so probably it doesn't add much to slowness on the disk side.

I've had to increase the flush interval sometimes for interactive debugging

```
  from tensorflow.python.summary.writer.writer import FileWriter
  old_init = FileWriter.__init__
  def newinit(*args, **kwargs):
    print(""Overriding FileWriter flush_secs to 1"")
    kwargs['flush_secs']=1
    old_init(*args, **kwargs)
  FileWriter.__init__=newinit

```",sense slight possibility frequent event writing higher utilization due actual flush interval pretty long probably add much disk side increase flush interval sometimes interactive import print,issue,positive,positive,neutral,neutral,positive,positive
364437108,"I think

```python
    nvidia = NvidiaContext()
    nvidia.create_context()

    num_gpus = nvidia.NumCudaDevices()

    for device in nvidia.Devices():
        print(device.Name())
        print(device.UUID())

        print(device.Memory())
        print(device.Utilization())

    nvidia.destroy_context()
```

is pretty simple. For proper usage of ctypes you need all these exception handling to not left the user with unkown errors.",think python device print print print print pretty simple proper usage need exception handling left user,issue,negative,positive,neutral,neutral,positive,positive
364417771,A separate and __simpler__ file would be much better. ,separate file would much better,issue,negative,positive,positive,positive,positive,positive
364417274,"Depend on what trainer and training interface you use there are many different places.
Starting point: https://github.com/ppwwyyxx/tensorpack/blob/54c5a42db3c33df7310dc80106dd123e4e04d1b4/tensorpack/train/interface.py#L92",depend trainer training interface use many different starting point,issue,negative,positive,positive,positive,positive,positive
364411744,"But not found where inference called `_build_graph` or `build_graph`, could you pls point out? Thank you",found inference could point thank,issue,negative,neutral,neutral,neutral,neutral,neutral
364331903,"Yes they've been smoothed first inside the graph.
They idea was that summarizing all scalars in every iteration may have overhead (or maybe not), meanwhile summarizing once a while has variance. So they were smoothed before getting into tensorboard. ",yes first inside graph idea every iteration may overhead maybe meanwhile variance getting,issue,negative,positive,positive,positive,positive,positive
364331106,"I mean the values don't make sense to me, shouldn't they be integers? Are they being smoothed outside of tensorboard as well?
<img width=""381"" alt=""screenshot 2018-02-08 20 20 57"" src=""https://user-images.githubusercontent.com/23068/36011645-a376a96a-0d0d-11e8-8e04-3dcddbee1785.png"">
",mean make sense outside well,issue,negative,negative,negative,negative,negative,negative
364330930,"ok looks like shutil called rmtree which gives that error

the actual error was due to NFS/EFS misbehaving. It seems impossible to delete NFS directories sometimes, but it seems possible to move them to /efs/trash as a workaround
```
>>> shutil.rmtree('/efs/runs/yuxin_numpy/1gpu')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/shutil.py"", line 480, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File ""/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/shutil.py"", line 438, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File ""/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/shutil.py"", line 436, in _rmtree_safe_fd
    os.unlink(name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfsb3d9a8ed434c603a00000002'
```",like error actual error due impossible delete sometimes possible move recent call last file line module file line path file line file line name device resource busy,issue,negative,negative,neutral,neutral,negative,negative
364330077,"It's using shutil.rmtree already.
If files are being opened (e.g. by tensorboard) you'll also see this error. I saw this error quite often actually.",already also see error saw error quite often actually,issue,negative,neutral,neutral,neutral,neutral,neutral
364329891,"```python
class A(Callback):
    def _before_run(self, _):
        self._start = time.time()

    def _after_run(self, _, _):
        self.trainer.monitors.put_scalar('step_time', time.time() - self._start)
```
Maybe this?",python class self self maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
364329500,"I would like fractional values as well, but queue does not have an API to get its capacity.",would like fractional well queue get capacity,issue,positive,neutral,neutral,neutral,neutral,neutral
364317211,"I see, thanks

Somewhat related, is there an easy way to add to tensorboard duration of each train step? Perhaps it needs adding something like this inside tensorpack somewhere?

        start_time = time.perf_counter()
         ....run(train_op)
        duration = time.perf_counter() - start_time
        trainer.monitors.put_scalar('step_time', duration)
",see thanks somewhat related easy way add duration train step perhaps need something like inside somewhere run duration duration,issue,positive,positive,positive,positive,positive,positive
364314724,"I would use:
```
EnableCallbackIf(
GraphProfiler(dump_tracing=True, dump_event=True), 
lambda self: self.trainer.global_step > 10 and self.trainer.global_step < 20)
```
http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.GraphProfiler",would use lambda self,issue,negative,neutral,neutral,neutral,neutral,neutral
364289771,"Not sure, if you want to use a separate file `nvml.py`. Here is a stand-alone version:
https://gist.github.com/PatWie/68abc24de854ba87ff07c68dea61e778",sure want use separate file version,issue,negative,positive,positive,positive,positive,positive
364273427,"oh, oops, good point, that's in their code",oh good point code,issue,negative,positive,positive,positive,positive,positive
364177558,"In inference it calls `build_graph` with `is_training=False`. So you can use it to build a different model.
`build_graph` is defined in `imagenet_utils.py` which calls `get_logits`.",inference use build different model defined,issue,negative,neutral,neutral,neutral,neutral,neutral
364125281,The new changes with the decorator look good. Minor style point is that parse_args might not be the most descriptive name for what the decorator does. The decorator definitely helps make it more readable.,new decorator look good minor style point might descriptive name decorator decorator definitely make readable,issue,positive,positive,positive,positive,positive,positive
364035064,"Thank you. I will try to do it myself and ask again if another issue arises.
Best Regards.",thank try ask another issue best,issue,positive,positive,positive,positive,positive,positive
364029876,"Everything is based on tensorflow so you'd better forget tensorpack and come up with a tensorflow-way of doing it first. 
I don't know the paper and not sure what exactly you want to do, so I don't know whether `tfutils.optimizer.PostProcessOptimizer` will help or not.",everything based better forget come first know paper sure exactly want know whether help,issue,positive,positive,positive,positive,positive,positive
364029470,Thank you for your prompt reply. Is it possible to use util such as tfutils.optimizer.PostProcessOptimizer to solve this problem?,thank prompt reply possible use solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
364026000,"Please first consider how this can be done in pure tensorflow -- if it's possible then it's usually easy to make it work in tensorpack.

Slots depends on minimize call, minimize depends on knowing all the variables in the models, which requires that the model was built already. Therefore you cannot use slots when you're building the model. They way you're approaching seems impossible in tensorflow, so maybe you need to come up with something else first.",please first consider done pure possible usually easy make work minimize call minimize knowing model built already therefore use building model way approaching impossible maybe need come something else first,issue,positive,positive,neutral,neutral,positive,positive
363998735,"Yes. 
tf.train.piecewise_constant, tf.train.exponential_decay can be useful but you can just write any symbolic function to compute your lr.",yes useful write symbolic function compute,issue,positive,positive,positive,positive,positive,positive
363996077,"and for `tf.train.get_or_create_global_step`, do you mean write a `lr` decay function like `tf.train.exponential_decay` and pass it to optimizer?",mean write decay function like pas,issue,negative,negative,negative,negative,negative,negative
363994616,"```diff
diff --git i/examples/mnist-convnet.py w/examples/mnist-convnet.py
index 9459533..c94ed57 100755
--- i/examples/mnist-convnet.py
+++ w/examples/mnist-convnet.py
@@ -90,7 +90,7 @@ class Model(ModelDesc):
             decay_rate=0.3, staircase=True, name='learning_rate')
         # This will also put the summary in tensorboard, stat.json and print in terminal
         # but this time without moving average
-        tf.summary.scalar('lr', lr)
+        tf.summary.scalar('lr', lr, collections=['NEW_COLL'])
         return tf.train.AdamOptimizer(lr)
 
 
@@ -116,6 +116,10 @@ def get_config():
             InferenceRunner(    # run inference(for validation) after every epoch
                 dataset_test,   # the DataFlow instance used for validation
                 ScalarStats(['cross_entropy_loss', 'accuracy'])),
+            MergeAllSummaries(period=1, key='NEW_COLL')
+        ],
+        monitors=DEFAULT_MONITORS() + [
+            ScalarPrinter(enable_step=True, enable_epoch=False)
         ],
         steps_per_epoch=steps_per_epoch,
         max_epoch=100,
```
This makes mnist example print learning rate every step.
And you can also just simply use `tf.Print` in the graph.",git index class model also put summary print terminal time without moving average return run inference validation every epoch instance used validation example print learning rate every step also simply use graph,issue,negative,negative,neutral,neutral,negative,negative
363993892,"Could you please give an example?

I have used 
```
logger.set_logger_dir(exp_dir, 'k')
```

to setup the experiment directory. The experiment maybe cancelled, and I want to restart from the same directory.",could please give example used setup experiment directory experiment maybe want restart directory,issue,negative,neutral,neutral,neutral,neutral,neutral
363993588,"It depends on how you want to ""restart"".
If you want to load an old model you need to pass it to `session_init`. Otherwise the model will be initialized from scratch.",want restart want load old model need pas otherwise model scratch,issue,negative,positive,neutral,neutral,positive,positive
363992803,"What you asked for in the original issue is just one line of code in the graph.
https://github.com/ppwwyyxx/tensorpack/blob/9bbdf94da8be9976725fb131bfb5c9f5523b25ac/examples/mnist-convnet.py#L86-L90

get_global_step_var is just a wrapped call to `tf.train.get_or_create_global_step`.",original issue one line code graph wrapped call,issue,negative,positive,positive,positive,positive,positive
363992750,"as in the http://tensorpack.readthedocs.io/en/latest/tutorial/summary.html :
> All the ""what, when, where"" can be customized in either the graph or with the callbacks/monitors setting.

say,  I only want to change the lr to be step based, keep other summary epoch based, 
for
> with the callbacks/monitors setting 

seems tensorpack put all summaries writing together:
https://github.com/ppwwyyxx/tensorpack/blob/bf4d89389b508844280d2bc6826441fe5dcd8243/tensorpack/callbacks/monitor.py#L120-L141

is it difficult to modify the code since it is wrapped deeply?

for
> can be customized in either the graph

I dont understand what you mean by customize it graph, do you mean call `tf.summary.xxx` when we construct the graph? 

can you share a simple example of using `tf.train.get_or_create_global_step()`? ",either graph setting say want change step based keep summary epoch based setting put writing together difficult modify code since wrapped deeply either graph dont understand mean graph mean call construct graph share simple example,issue,negative,negative,negative,negative,negative,negative
363989386,"I change the line:
https://github.com/ppwwyyxx/tensorpack/blob/bf4d89389b508844280d2bc6826441fe5dcd8243/tensorpack/callbacks/param.py#L161
to  `def _trigger_step(self):` and it works properly!",change line self work properly,issue,negative,neutral,neutral,neutral,neutral,neutral
363928314,Thank you for your reply. Seems the issue has been fixed in recently. ,thank reply issue fixed recently,issue,negative,positive,neutral,neutral,positive,positive
363888856,"Decorator looks good. I'll give it a try.

Tensorflow promises to not do breaking change so using it is fine. It's their job to not break things and I'd rather not care about it.",decorator good give try breaking change fine job break rather care,issue,positive,positive,positive,positive,positive,positive
363875986,"Sorry, that Stackoverflow comment has nothing to do with the click library, but rather adding functionality that doesn't exist within it. I'll post the relevant portions.

```python
import functools

def rename_kwargs(**replacements):
    def actual_decorator(func):
        @functools.wraps(func)
        def decorated_func(*args, **kwargs):
            for internal_arg, external_arg in replacements.iteritems():
                if external_arg in kwargs:
                    kwargs[internal_arg] = kwargs.pop(external_arg)
            return func(*args, **kwargs)
        return decorated_func
    return actual_decorator


if __name__ == '__main__':

    @rename_kwargs(different_arg='format')
    def tester(different_arg):
        print different_arg

    tester(format='test value')
```
In the above snippet @rename_kwargs renames the keyword argument ""format"" to ""different_arg"" automatically. The decorator supports multiple key values passed in via a dict. You can augment the decorator to further modify the arguments as needed. That way when you decide to fully deprecate the old arguments you can just remove the decorators and the code will be readable and updated. Additionally, the decorator can be reused for all tf.layers argument translations and remove unnecessary code duplication. Isn't this how Tensorflow handles API changes anyhow?

> How does this astonish a user who doesn't go and read source code?
This is more about astonishing contributors like myself and others. It just adds pretty nasty complexity and adds to the technical debt. I definitely agree we should change the arguments, but there are better ways to do that.

Furthermore, supporting two separate versions of command arguments could get confusing. 

At the very least, do we want our examples to break whenever Tensorflow changes a default option if we rely on them for the defaults? One of the best parts of Tensorpack are the clean and well maintained examples that have excellent default values for performance.",sorry comment nothing click library rather functionality exist within post relevant python import return return return tester print tester value snippet argument format automatically decorator multiple key via augment decorator modify way decide fully deprecate old remove code readable additionally decorator argument remove unnecessary code duplication anyhow astonish user go read source code astonishing like pretty nasty complexity technical debt definitely agree change better way furthermore supporting two separate command could get least want break whenever default option rely one best clean well excellent default performance,issue,positive,positive,positive,positive,positive,positive
363863707,"Sorrry, I forget the lines that you have mentioned
>We only support single image per GPU.
Because of (3), BatchNorm statistics are not supposed to be updated during fine-tuning. This specific kind of BatchNorm will need my kernel which is included since TF 1.4. If using an earlier version of TF, it will be either slow or wrong

It means that the BN statistic will not learn during training. On other hands, it is frozen. Am I right?

",forget support single image per statistic supposed specific kind need kernel included since version either slow wrong statistic learn training frozen right,issue,negative,positive,neutral,neutral,positive,positive
363843467,"> readability and documentation.

I expect people to read tf.layers documentation instead of mine. I could copy docs from there. 

I could also add a new argument list plus **kwargs, i.e.:
`Conv2D(inputs, filters, kernel_size, strides=None, ........., bias_constraint=None, **kwargs)`.

>  the principle of least astonishment.

How does this astonish a user who doesn't go and read source code?

The click library seems to only work for command line args if I'm not mistaken. Also what I need to do is not only rename.",readability documentation expect people read documentation instead mine could copy could also add new argument list plus principle least astonishment astonish user go read source code click library work command line mistaken also need rename,issue,negative,negative,neutral,neutral,negative,negative
363839735,"My implementation follows the paper. I think you must have misread it somehow.
There are at least three different batch size in a fasterrcnn.",implementation paper think must misread somehow least three different batch size,issue,negative,negative,negative,negative,negative,negative
363839202,"As I said, the easiest way to do it now is to do it in the graph, with `tf.train.get_or_create_global_step()`. You can also write a new callback to do this. Hacking with the existing one will not work.

>  the summary is also updated epoch based, can we modify it to global_step based?

It can and is mentioned in docs. http://tensorpack.readthedocs.io/en/latest/tutorial/summary.html",said easiest way graph also write new hacking one work summary also epoch based modify based,issue,negative,positive,positive,positive,positive,positive
363831493,So I have a really hacky way I like to do this using tf.cond() and variable.assign. Basically you can either use the default value or assign the value based on whether the condition is true. This is what I have done whenever i need to decay the learning rate.,really hacky way like basically either use default value assign value based whether condition true done whenever need decay learning rate,issue,positive,positive,positive,positive,positive,positive
363829449,"Ouch, on one hand great job for maintaining backwards compatibility. On the other hand, it's big blow to readability and documentation. ;-; Maintaining two separate keyword lists seems pretty awful and some of these hacks definitely don't follow [the principle of least astonishment](https://en.wikipedia.org/wiki/Principle_of_least_astonishment).

Maybe a function decorator that would add an deprecation warning would be more readable? Perhaps something like the decorator in the top voted answer to [this](https://stackoverflow.com/questions/35567753/python-click-library-rename-argument) StackOverflow question?

You lose a lot of information and docs about what the default values are and such. It might be better just to explicitly rewrite all the args with a function decorator. You could have it take in a dict of value to remap and use a global dictionary to rewrite the args. That seems much more readable and transparent.",ouch one hand great job backwards compatibility hand big blow readability documentation two separate pretty awful definitely follow principle least astonishment maybe function decorator would add deprecation warning would readable perhaps something like decorator top answer question lose lot information default might better explicitly rewrite function decorator could take value remap use global dictionary rewrite much readable transparent,issue,positive,positive,neutral,neutral,positive,positive
363756593,"Thanks so much. I am reading your guide and let you know when it completed. I have one more question about your implementation. As you know, the Batch Norm training is important for segmentation and detection. The paper used the batch size of 16. I found that you used the batch size of 256. During the training, do you consider to train the Batch Norm statistic? In my opinion, it looks that you did not train BN parameters
",thanks much reading guide let know one question implementation know batch norm training important segmentation detection paper used batch size found used batch size training consider train batch norm statistic opinion train,issue,positive,positive,positive,positive,positive,positive
363750296,"I changed this two lines in param.py: https://github.com/ppwwyyxx/tensorpack/blob/bf4d89389b508844280d2bc6826441fe5dcd8243/tensorpack/callbacks/param.py#L281-L282

to    
```
def _get_value_to_set(self):
        return self.f(self.global_step, self.get_current_value())
```
then replace the original schedualer :
```
        ScheduledHyperParamSetter('learning_rate',
                                   [(0, 3e-1), (30, 3e-2), (60, 3e-3), (90, 3e-4)]),
```
with 
```
        HyperParamSetterWithFunc('learning_rate',
                                 lambda e, x: 0.5*(1-e/3e5)),
```
Am I right? but seems not right, after `10` epochs(global_step `12500`, total step `3e5`), lr still `0.4999`, not decrease to around `0.48`, by the way, seems the summary is also updated epoch based, can we modify it to `global_step` based?",two self return replace original lambda right right total step still decrease around way summary also epoch based modify based,issue,negative,positive,positive,positive,positive,positive
363686009,"Total loss didn't show up because I forgot to add its summary when no regularizer is given. Will add it.

Loss not decreasing is not a tensorpack issue unless you believe there is a tensorpack bug causing this.",total loss show forgot add summary regularizer given add loss decreasing issue unless believe bug causing,issue,negative,neutral,neutral,neutral,neutral,neutral
363685668,"the mobilenet code is from official keras implementation, the only change i made is the following function:
`def mobilenet(inputs):
    input = tf.layers.Input(tensor=inputs[0])

    def image_preprocess(image):
        image = ImageNetModel.image_preprocess(image)
        image = tf.transpose(image, [0, 3, 1, 2])
        return image

    x = Lambda(image_preprocess)(input)
    
    x = _conv_block(x, 32, ALPHA, strides=(2, 2))
    x = _depthwise_conv_block(x, 64, ALPHA, DEPTH_MULTIPLIER, block_id=1)

    x = _depthwise_conv_block(x, 128, ALPHA, DEPTH_MULTIPLIER,
                              strides=(2, 2), block_id=2)
    x = _depthwise_conv_block(x, 128, ALPHA, DEPTH_MULTIPLIER, block_id=3)

    x = _depthwise_conv_block(x, 256, ALPHA, DEPTH_MULTIPLIER,
                              strides=(2, 2), block_id=4)
    x = _depthwise_conv_block(x, 256, ALPHA, DEPTH_MULTIPLIER, block_id=5)

    x = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER,
                              strides=(2, 2), block_id=6)
    x = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER, block_id=7)
    x = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER, block_id=8)
    x = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER, block_id=9)
    x = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER, block_id=10)
    x = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER, block_id=11)

    x = _depthwise_conv_block(x, 1024, ALPHA, DEPTH_MULTIPLIER,
                              strides=(2, 2), block_id=12)
    x = _depthwise_conv_block(x, 1024, ALPHA, DEPTH_MULTIPLIER, block_id=13)

    shape = (int(1024 * ALPHA), 1, 1)

    x = GlobalAveragePooling2D()(x)
    x = Reshape(shape, name='reshape_1')(x)
    x = Dropout(0.001, name='dropout')(x)
    x = Conv2D(NUM_CLASSES, (1, 1),
               padding='same', name='conv_preds')(x)
    x = Activation('softmax', name='act_softmax')(x)
    x = Reshape((NUM_CLASSES,), name='reshape_2')(x)  
    
    M = tf.keras.models.Model(input, x, name='mobilenet_0.75_160')
    return M`




",code official implementation change made following function input image image image image image return image lambda input alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha shape alpha reshape shape dropout activation reshape input return,issue,negative,neutral,neutral,neutral,neutral,neutral
363660783,"I plan to add such things to callbacks, but right now you can just write the above expression in the graph. You can get the global step with `tf.train.get_or_create_global_step()`.",plan add right write expression graph get global step,issue,negative,positive,positive,positive,positive,positive
363635866,"Yes, when I tried on terminal, it works fine, but seems when I get a `tfdbg -> run`, it dumps so many things out, then root directory is filled, before run, there is 11G left, with a run, prompts no space left:
```
2018-02-07 10:23:07.433343: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/add_grad/Shape:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/add_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3;
2018-02-07 10:23:07.443635: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;
2018-02-07 10:23:07.443804: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad/ShapeN:1:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;
2018-02-07 10:23:07.445778: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;
2018-02-07 10:23:07.445894: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad/ShapeN:1:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;
2018-02-07 10:23:07.446192: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;
```
Have you  tried this?",yes tried terminal work fine get run many root directory filled run left run space left node watch key publish tensor data file due target due following create directory due create directory node watch key publish tensor data file due target due following create directory due create directory node watch key publish tensor data file due target due following create directory due create directory node watch key publish tensor data file due target due following create directory due create directory node watch key publish tensor data file due target due following create directory due create directory node watch key publish tensor data file due target due following create directory due create directory tried,issue,positive,negative,neutral,neutral,negative,negative
363580028,"> replace COCODetection.load_many in data.py by your own loader.

It can be any python code that loads data from any format. As long as in the end it returns the same data structure as documented:
https://github.com/tensorpack/tensorpack/blob/d2c5cc162aaf15b4f9077fb8370ed4b98ee8b848/examples/FasterRCNN/data.py#L261-L279",replace loader python code data format long end data structure,issue,negative,negative,neutral,neutral,negative,negative
363573040,Do I need convert my dataset to coco annotation format to use your code?,need convert coco annotation format use code,issue,negative,neutral,neutral,neutral,neutral,neutral
363569182,You can compute a bounding box from segmentation mask.,compute bounding box segmentation mask,issue,negative,neutral,neutral,neutral,neutral,neutral
363566332,"Thank you. I have read it. But the importance is that my dataset does not follow the coco style, it means no bounding box available, only masks for each object. It likes shape example in matterport's implementation. Could you tell me what should I do to make my data loader?",thank read importance follow coco style bounding box available object shape example implementation could tell make data loader,issue,positive,positive,positive,positive,positive,positive
363475367,Tensorflow requires you to run tfdebug inside a valid normal terminal. Pycharm is not.,run inside valid normal terminal,issue,negative,positive,positive,positive,positive,positive
363154713,"Any of those values < 0 seem to be caused by floating point rounding.
And since boxes are used for regression, I don't think it will be an issue. ",seem floating point rounding since used regression think issue,issue,negative,neutral,neutral,neutral,neutral,neutral
363152131,"What's the problem just using RandomResize + CenterPaste + Crop ?

Adding giant features is not a good idea because it's less likely to be useful to others.  ",problem crop giant good idea le likely useful,issue,negative,positive,positive,positive,positive,positive
362918492,@JesseYang I just officially added it to the Tensorpack Conv2D class with #625 so now we officially support it. ,officially added class officially support,issue,negative,neutral,neutral,neutral,neutral,neutral
362898126,"Thank you for point out the bug of code.
But I have no idea to fix my bugs.
Sorry. ",thank point bug code idea fix sorry,issue,negative,negative,negative,negative,negative,negative
362873107,"Yes, I think it is the best that the CPU and disk can do, as even all `6` cores with `12` threads are busy, they don't wait for disk reading, and reduce CPU preprocessing I still cannot get full disk reading upper bound. Have estimated or tested that in total like `100` epochs, that all images are visited?",yes think best disk even busy wait disk reading reduce still get full disk reading upper bound tested total like,issue,positive,positive,positive,positive,positive,positive
362872734,"Because no one can guarantee that 782 % #GPU == 0. 
For training this is not a problem. For validation you want to evaluate on precisely the whole dataset.",one guarantee training problem validation want evaluate precisely whole,issue,negative,positive,positive,positive,positive,positive
362872426,"I see, `782*(256/4) ~= 589*(256/3) ~=50000`, but why not make it the same like training bar, as `(782/4)*256 ~= (589/3)*256 ~= (196)*256`, so we always see `xxx/196` in validation bar",see make like training bar always see validation bar,issue,negative,neutral,neutral,neutral,neutral,neutral
362870946,"Thanks for your quick response, my confusion is that for training with `3` gpu and `4` gpu, training progress bar is `xxx/5000`, but validation bar for `3` gpu is `xxx/589`, while for `4` gpu is `xxx/782`, not the same, for training, `256` batch times `5000` is just `128` million, but what about validation? ",thanks quick response confusion training training progress bar validation bar training batch time million validation,issue,positive,positive,positive,positive,positive,positive
362870761,"Depend on how you write the code.
If you're talking about shufflenet example, it's in the code: 

https://github.com/ppwwyyxx/tensorpack/blob/c14d14961fee17116f062680624a04946fca7890/examples/ShuffleNet/shufflenet.py#L146-L147",depend write code talking example code,issue,negative,neutral,neutral,neutral,neutral,neutral
362870658,"Am I right that each data point contain `TOTAL_BATCH_SIZE / nr_tower` images? so in total, all gpus together compute `TOTAL_BATCH_SIZE` of images?",right data point contain total together compute,issue,negative,positive,positive,positive,positive,positive
362870481,"after regenerating the training `lmdb`, the problem solved, but still not clean why it happened",training problem still clean,issue,negative,positive,positive,positive,positive,positive
362833620,"It does not split a batch. Nor does training. Validation feeds 1 data points to each GPU, and update the progress bar by #GPUS. Training feeds 1 data point to each GPU and update the progress bar by 1.",split batch training validation data update progress bar training data point update progress bar,issue,positive,neutral,neutral,neutral,neutral,neutral
362833241,"For any unexpected problems, please follow the issue template.

TENSORPACK_DATASET is the default path to store downloaded data.

`shuffle=False` is correct to generate LMDB.",unexpected please follow issue template default path store data correct generate,issue,negative,positive,neutral,neutral,positive,positive
362794595,"I suspect may be the training `lmdb` file is not correct, would it be wrong if I generate the `lmdb` file with 
```
class BinaryILSVRC12(dataset.ILSVRC12Files):
    def get_data(self):
        for fname, label in super(BinaryILSVRC12, self).get_data():
            with open(fname, 'rb') as f:
                jpeg = f.read()
            jpeg = np.asarray(bytearray(jpeg), dtype='uint8')
            yield [jpeg, label]

ds0 = BinaryILSVRC12('/temp/wangtao/ILSVRC/Data/CLS-LOC/', 'train', shuffle = False)
ds1 = PrefetchDataZMQ(ds0, nr_proc=1)

dftools.dump_dataflow_to_lmdb(ds1, '/temp/wangtao/ILSVRC-train.lmdb')
```
I dont remember if my correct generate was 
`ds0 = BinaryILSVRC12('/temp/wangtao/ILSVRC/Data/CLS-LOC/', 'train', shuffle = True)`",suspect may training file correct would wrong generate file class self label super self open yield label shuffle false dont remember correct generate shuffle true,issue,negative,negative,neutral,neutral,negative,negative
362741792,"As a closing remark, even if this issue still exists, it's probably a very platform-specific problem because many people have already successfully used this piece of code. My last suggestions to anyone who may encounter OOM in the future: 

Make sure no other processes (e.g. Xorg) are running on GPUs.
For training, try smaller batch sizes (`FASTRCNN_BATCH_PER_IM`).
For evaluation, try smaller `TEST_POST_NMS_TOPK`.",remark even issue still probably problem many people already successfully used piece code last anyone may encounter future make sure running training try smaller batch size evaluation try smaller,issue,negative,positive,positive,positive,positive,positive
362646012,"150M/s upper bound your overall speed to 4-6it/s. And you seem to be also limited by CPU speed.

It does not assure all images are visited. The chunk is updated like a queue.",upper bound overall speed seem also limited speed assure chunk like queue,issue,negative,negative,neutral,neutral,negative,negative
362645450,"Tensorpack is compatible with tf1.5. Upgrading tensorpack may help.

To report an unexpected problem, please follow the issue template.",compatible may help report unexpected problem please follow issue template,issue,negative,positive,neutral,neutral,positive,positive
362588591,"But could you please explain a little about my confusion, thank you",could please explain little confusion thank,issue,negative,negative,negative,negative,negative,negative
362588473,"with `vmstat 1 4` on the machine for 3 GPU training, I get 
```
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
11  0  45548 684776 770364 32992844    0    0   286    21    1    1 34  4 61  0  0
19  0  45548 697964 770364 32980384    0    0 66176     0 38472 55734 69  6 24  1  0
11  0  45548 760188 770372 32910624    0    0 65984    12 36339 50278 70  5 24  1  0
12  0  45548 703368 770372 32965052    0    0 61628     0 22334 60357 72  3 24  0  0

```
the `wa` is almost 0 all the time, means cpu are not waiting for disk reading and `bi` is around `60000-65000`, which is reading around `60-65 M/s`. I think the bottleneck lies both in `cpu` performance and `disk` reading speed.
",machine training get free buff cache si bo u id wa st wa almost time waiting disk reading around reading around think bottleneck performance disk reading speed,issue,positive,positive,positive,positive,positive,positive
362573622,"I tested with `sudo hdparm -Tt /dev/sdX`, the cached reading speed is around `1200 M/s`, uncached disk reading is `150M/s`, with `iotop`, I observed the disk reading speed is around `75M/s` when training on `3 GPU`, a half of the tested reading speed of `150M/s`. 

I'm a bit confused here about the locally shuffle data, does it mean maintaining a `50000` image sized memory chunk on the RAM, and only fetch batches from it, then after `50000/3`(default shuffle interval) continuous images are fetched(batch by batch), we shuffle the whole `50000` image chunk, then do next `50000/3` fetch, but how does this machinism assure that all images are visited, and how do we update the `50000` image chunk, is it updated like a queue that newly readed image are put onto the first and olded one kick off the queue(`LMDBData(db, shuffle=False)` returns one image a time)",tested reading speed around disk reading disk reading speed around training half tested reading speed bit confused locally shuffle data mean image sized memory chunk ram fetch default shuffle interval continuous fetched batch batch shuffle whole image chunk next fetch machinism assure update image chunk like queue newly image put onto first one kick queue one image time,issue,negative,negative,neutral,neutral,negative,negative
362478310,"> which is strange that, at first the speed is around100 it/s, then to 20% of reading, it quickly drop down to 3 it/s, and maintain 3-8 it/s to the end.

This suggests that beginning of the file is cached in memory. But memory may not be big enough to cache the whole file.

3 it/s is still very slow unless that's the disk speed you've got.  8 it/s is more reasonable on an HDD. I recommend you do more benchmark here. See this paragraph in efficient dataflow tutorial: 
> Depending on whether the OS has cached the file for you (and how large the RAM is), the above script can run at a speed of 10\~130 it/s, roughly corresponding to 250MB\~3.5GB/s bandwidth. You can test your cached and uncached disk read bandwidth with sudo hdparm -Tt /dev/sdX. As a reference, on Samsung SSD 850, the uncached speed is about 16it/s.

Also your CPU is probably too weak, you need some Xeon E5 series CPUs for complicated preprocessing pipeline. But you can always remove some augmentors to speed it up.",strange first speed around reading quickly drop maintain end beginning file memory memory may big enough cache whole file still slow unless disk speed got reasonable recommend see paragraph efficient tutorial depending whether o file large ram script run speed roughly corresponding test disk read reference speed also probably weak need series complicated pipeline always remove speed,issue,negative,negative,neutral,neutral,negative,negative
362478121,"```
[0201 11:11:54 @monitor.py:363] DataParallelInferenceRunner/QueueInput/queue_size: 2.5153
[0201 11:11:54 @monitor.py:363] QueueInput/queue_size: 2.35e-37
```
for the `DataParallelInferenceRunner/QueueInput/queue_size` I used lmdb, but for  `QueueInput/queue_size` I didn't change it and just used the original random read, so it is much more slow.

could it be possible that my queue ratio `(2.5)` is too low that layer `PrefetchDataZMQ` processes cannot keep a busy state, and is it safely to conclude that it is caused by low disk reading speed as the raw reading speed with 
```
ds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)
ds = BatchData(ds, 256, use_list=True)
TestDataSpeed(ds).start()
```
only got stable reading speed of around `3 it/s`, hope you can give me some advice, thank you!",used change used original random read much slow could possible queue ratio low layer keep busy state safely conclude low disk reading speed raw reading speed got stable reading speed around hope give advice thank,issue,positive,negative,neutral,neutral,negative,negative
362475725,"I see. I tested the raw reading speed with 
```
ds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)
ds = BatchData(ds, 256, use_list=True)
TestDataSpeed(ds).start()
```
which is strange that, at first the speed is around` 100 it/s`, then to `20%` of reading, it quickly drop down to `3 it/s`, and maintain `3-8 it/s` to the end.

For locally shuffle data with 
```
ds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)
ds = LocallyShuffleData(ds, 50000)
ds = BatchData(ds, 256, use_list=True)
TestDataSpeed(ds).start()
```
which I got speed of 2-3 it/s when reading is stable.

```
For the parallelized version adding all augmentors for Shufflenet :
ds = LMDBData(db, shuffle=False)
ds = LocallyShuffleData(ds, 50000)
ds = PrefetchData(ds, 5000, 1)
ds = LMDBDataPoint(ds)
ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
ds = AugmentImageComponent(ds, lots_of_augmentors)
ds = PrefetchDataZMQ(ds, 25)
ds = BatchData(ds, 256)
```
which I got speed of `2.7 it/s` when reading is stable, and `12cores` of CPU are all around `70-80%` usage, but it is still not fast enough, I'm wondering if you can suggest how to further boost the dataflow, I tried change the local shuffle buffer size to `100000`(I have `64G RAM`), or change the `PrefetchDataZMQ` from `25` to `40`, but seems not much change in reading speed, all around `2-3 it/s`.",see tested raw reading speed strange first speed around reading quickly drop maintain end locally shuffle data got speed reading stable version lambda got speed reading stable around usage still fast enough wondering suggest boost tried change local shuffle buffer size ram change much change reading speed around,issue,negative,positive,neutral,neutral,positive,positive
362475314,"This is a Python issue. Python multiprocessing depends on pickle, and pickle has limitations on what type of objects it supports. You need to design your `GameDecoder` so that it doesn't include such objects.",python issue python pickle pickle type need design include,issue,negative,neutral,neutral,neutral,neutral,neutral
362361378,"It's a typo. Should be ILSVRC12Files.

To know why your data is slow, do the benchmarks:
http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html#investigate-dataflow",typo know data slow,issue,negative,negative,negative,negative,negative,negative
362232598,"the lines for creating the lmdb file is: 
```
from tensorpack.dataflow import *
class BinaryILSVRC12(dataset.ILSVRCFiles):
    def get_data(self):
        for fname, label in super(BinaryILSVRC12, self).get_data():
            with open(fname, 'rb') as f:
                jpeg = f.read()
            jpeg = np.asarray(bytearray(jpeg), dtype='uint8')
            yield [jpeg, label]
ds0 = BinaryILSVRC12()
ds1 = PrefetchDataZMQ(ds0, nr_proc=1)
dftools.dump_dataflow_to_lmdb(ds1, '/path/to/ILSVRC-train.lmdb')
```
but `dataset.ILSVRCFiles` is not found, I used `class dataflow.dataset.ILSVRC12Files`, and initialize the class as:
`ds0 = BinaryILSVRC12('/temp/wangtao/ILSVRC/Data/CLS-LOC/', 'train', shuffle=False)`
 is it the same ? Am I right?
Thanks",file import class self label super self open yield label found used class initialize class right thanks,issue,positive,positive,positive,positive,positive,positive
362148475,"See tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html.

> We do not know why your training is slow

> If you're going to open an issue about slow training, PLEASE do them and include your findings.",see know training slow going open issue slow training please include,issue,negative,negative,negative,negative,negative,negative
362148220,"could you suggest some possible modification on my case? e.g. since memory is `64G`, can I set some larger memory usage to get higher data speed?",could suggest possible modification case since memory set memory usage get higher data speed,issue,negative,positive,positive,positive,positive,positive
362146238,5000 is the buffer size of the queue used by `PrefetchData`.,buffer size queue used,issue,negative,neutral,neutral,neutral,neutral,neutral
362144794,"I changed it to read from single lmdb file, with the lines from your tutorial:
```
ds = LMDBData(db, shuffle=False)
ds = LocallyShuffleData(ds, 50000)
ds = PrefetchData(ds, 5000, 1)
ds = LMDBDataPoint(ds)
ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
ds = AugmentImageComponent(ds, lots_of_augmentors)
ds = PrefetchDataZMQ(ds, 25)
ds = BatchData(ds, 256)
```
now it is around `2.4 it/s`, and cpu usage is around `80%`for each core, `RAM` usage is around `18G/64G`, but GPU usage is not stable, sometimes all `70%-90%`, sometimes drop down to half of that, although on average keep in high usage.
for the first epoch, I got 
```
[0201 09:48:18 @monitor.py:363] DataParallelInferenceRunner/QueueInput/queue_size: 50
[0201 09:48:18 @monitor.py:363] QueueInput/queue_size: 0.85
```
for the second epoch, I got
```
[0201 10:30:16 @monitor.py:363] DataParallelInferenceRunner/QueueInput/queue_size: 2.5103
[0201 10:30:16 @monitor.py:363] QueueInput/queue_size: 2.2759e-37
```
for the third epoch, I got
```
[0201 11:11:54 @monitor.py:363] DataParallelInferenceRunner/QueueInput/queue_size: 2.5153
[0201 11:11:54 @monitor.py:363] QueueInput/queue_size: 2.35e-37
```
but sorry, I'm not much understand those numbers and features, about the efficient dataflow, there is explanations:
```
One process reads LMDB file, shuffle them in a buffer and put them into a multiprocessing.Queue (used by PrefetchData).

25 processes take items from the queue, decode and process them into [image, label] pairs, and send them through ZMQ IPC pipe.

The main process takes data from the pipe, makes batches.
```
does it mean read `50000` images from the `1.28` million and cached them in memory in a buffer(which corresponds to the `18G/64G` memory usage), then one process(corresponding to the `PrefetchData(ds, 5000, 1)`) read `5000` of them, next `25` processes do parallel decoding and preprocessing for the `5000` images, last, batch them into `256` chanks.
I may lack some basic knowledge to understand dataflow processes, and by look into source of functions  like `LMDBData`, `LocallyShuffleData`, I cannot understand them to know the details of the process, I'm wondering if you can point me to some references, Thanks for your attention!",read single file tutorial lambda around usage around core ram usage around usage stable sometimes sometimes drop half although average keep high usage first epoch got second epoch got third epoch got sorry much understand efficient one process file shuffle buffer put used take queue decode process image label send pipe main process data pipe mean read million memory buffer memory usage one process corresponding read next parallel last batch may lack basic knowledge understand look source like understand know process wondering point thanks attention,issue,positive,negative,neutral,neutral,negative,negative
361846002,"I fixed the problem because i noticed that the default buffer_size was set to be 2000. Howerver, my validation set has only 1500 images. That's why it failed.",fixed problem default set validation set,issue,negative,positive,neutral,neutral,positive,positive
361813132,"It is the reason. And it has already told you why it failed.
You can print the size of your dataflow by `print(dataflow.size())`.",reason already told print size print,issue,negative,neutral,neutral,neutral,neutral,neutral
361125624,"Amazing, it can work by using abstract socket. Maybe it is the filesystem problem. I am so happy to solve this problem!",amazing work abstract socket maybe problem happy solve problem,issue,positive,positive,positive,positive,positive,positive
361121630,"I found that ZMQ supports abstract socket which is filesystem-independent on linux: http://api.zeromq.org/4-1:zmq-ipc 
Maybe you can try the above commit, though I'm not sure now if filesystem is the root cause for the issue.",found abstract socket maybe try commit though sure root cause issue,issue,positive,positive,positive,positive,positive,positive
361118723,Thanks for your patient! These directories do not help. I have to use PrefetchData.,thanks patient help use,issue,positive,positive,positive,positive,positive,positive
361116694,"Not sure what's going on with your `/tmp`. You can definitely try `/run` or `/dev/shm`, or some of your own directories under `/home` or `/home1`. Make sure to check the environment variables before running.
And sometimes running inside a container can also make unix pipes behave weird. 
I won't be able to help more if you cannot figure it out what's special about your system. You can just use `PrefetchData`. It's not very efficient for large data but can still bring you somewhere.",sure going definitely try make sure check environment running sometimes running inside container also make behave weird wo able help figure special system use efficient large data still bring somewhere,issue,positive,positive,positive,positive,positive,positive
361115619,"Filesystem            Size  Used Avail Use% Mounted on
udev                   63G     0   63G   0% /dev
tmpfs                  13G   12M   13G   1% /run
/dev/sda3             880G   23G  813G   3% /
tmpfs                  63G  380K   63G   1% /dev/shm
tmpfs                 5.0M  4.0K  5.0M   1% /run/lock
tmpfs                  63G     0   63G   0% /sys/fs/cgroup
/dev/sda1             453M   62M  364M  15% /boot
/dev/sdb1             3.6T  2.5G  3.4T   1% /home
/dev/sdc1             3.6T   68M  3.4T   1% /home1
192.168.28.10:/DB     6.0T  5.5T  141G  98% /DB
DB7:/DATA3_DB7        9.1T  9.0T   96G  99% /DATA3_DB7
192.168.28.10:/DATA   9.9T  9.1T  255G  98% /DATA
192.168.28.10:/DATA2  8.2T  8.1T  121G  99% /DATA2
tmpfs                  13G   16K   13G   1% /run/user/557
tmpfs                  13G     0   13G   0% /run/user/546
tmpfs                  13G     0   13G   0% /run/user/593
tmpfs                  13G     0   13G   0% /run/user/563
tmpfs                  13G     0   13G   0% /run/user/572
tmpfs                  13G     0   13G   0% /run/user/565
tmpfs                  13G     0   13G   0% /run/user/566
tmpfs                  13G     0   13G   0% /run/user/544
tmpfs                  13G     0   13G   0% /run/user/552",size used avail use mounted,issue,negative,neutral,neutral,neutral,neutral,neutral
361114755,Then `TENSORPACK_PIPEDIR` may still be the reason behind it. `/tmp` may not always be a local directory. Could you post your output of `df` command?,may still reason behind may always local directory could post output command,issue,negative,negative,negative,negative,negative,negative
361113842,"Thank you very much! Finally, I run it with PrefetchData successfully.",thank much finally run successfully,issue,positive,positive,positive,positive,positive,positive
361008840,Awesome! Thanks for the quick fix!,awesome thanks quick fix,issue,positive,positive,positive,positive,positive,positive
361008102,Pushed a fix. Clipping was used in most other augmentors but somehow saturation was missing.,fix clipping used somehow saturation missing,issue,negative,negative,negative,negative,negative,negative
361007582,"Indeed! I think it's a uint8 overflow. After the saturation changes, values could go out of 255.",indeed think overflow saturation could go,issue,negative,neutral,neutral,neutral,neutral,neutral
360967348,This is very strange. Could you try run it without PrefetchDataZMQ to see if the bug still exists?,strange could try run without see bug still,issue,negative,negative,neutral,neutral,negative,negative
360964754,"Oh, I see!
Thanks for your information.
But the red wavy lines are really annoying.... ^_^
I'm explicitly import every class just for the time being, which works ok.
Maybe I shouldn't waste too much time on this little inconvinent ,It's time to move on.
",oh see thanks information red wavy really annoying explicitly import every class time work maybe waste much time little time move,issue,negative,negative,negative,negative,negative,negative
360964552,"@JasonHanG hi, I current use pycharm, this phenomenon is normal, just dont need to care about it. it really import it, just pycharm doesn't recognise it.

this is mine:
![tt](https://user-images.githubusercontent.com/13537104/35469455-fad622b4-036f-11e8-9f39-2c14ae6ebb44.jpg)
",hi current use phenomenon normal dont need care really import mine,issue,negative,positive,positive,positive,positive,positive
360963459,"Here are the implemented. Change it back, it can run correctly. The ""data list"" is a txt file of lines of ""image_path + label"".(""self.full_dir"" in the code) I think it can remember the first data list's information. When I change another data list, it goes wrong.
May I ask another question? In this implementation, how can I keep the images' order in one batch if I use more cpus in this code ""ds = PrefetchDataZMQ(ds, cpu)"". Maybe cpu=4 or more.

```python
class VideoDatasetFiles(RNGDataFlow):

    def init(self, dir, name, shuffle=None):
        assert name in ['train', 'test', 'val', 'simple_test'], name
        assert os.path.isdir(dir), dir
        self.full_dir = os.path.join(dir, '{}_videofolder.txt'.format(name))
        self.name = name
        if shuffle is None:
            shuffle = name == 'train'
        self.shuffle = shuffle
        imglist = []
        with open(self.full_dir) as f:
            lines = f.readlines()
        for line in lines:
            video, frames, label = line.strip().split()
            frames_list = ['{:05}.jpg'.format(i+1) for i in [int(j*(float(frames)/24)) for j in xrange(24)]]
            label = int(label)
            for img in frames_list:
                imglist.append((os.path.join(dir, '20bn-something-something-v1', video, img), label))
        self.imglist = imglist

    def size(self):
        return len(self.imglist)

    def get_data(self):
        idx = np.arange(self.size()/24)
        if self.shuffle:
            self.rng.shuffle(idx)
        idxs = [tt*24+jj for tt in idx for jj in xrange(24)]
        for k in idxs:
            fname, label = self.imglist[k]
            yield [fname, label]

class VideoDataset(VideoDatasetFiles):

    def init(self, dir, name, shuffle=None):
        super(VideoDataset, self).init(dir, name, shuffle)

    def get_data(self):
        for fname, label in super(VideoDataset, self).get_data():
            im = cv2.imread(fname, cv2.IMREAD_COLOR)
            assert im is not None, fname
            yield [im, label]
```",change back run correctly data list file label code think remember first data list information change another data list go wrong may ask another question implementation keep order one batch use code maybe python class self name assert name name assert name name shuffle none shuffle name shuffle open line video label float label label video label size self return self label yield label class self name super self name shuffle self label super self assert none yield label,issue,positive,positive,neutral,neutral,positive,positive
360962648,"Yes this looks like an IDE usage question. I have no knowledge of pycharm so I'm unable to help.
I guess there is some place in pycharm you can set paths to some external libraries.

As a hack, you can always do `sys.path.insert(0, '/path/to/cloned/tensorpack')` to make import succeed.",yes like ide usage question knowledge unable help guess place set external hack always make import succeed,issue,positive,negative,negative,negative,negative,negative
360962317,"Also, could you specify what do you mean by ""change the data list""? What happen if you change it back?",also could specify mean change data list happen change back,issue,negative,negative,negative,negative,negative,negative
360962247,"This is probably not the same as the original issue.

Your dataset is not producing data with homogeneous shape. I would recommend you to check how VideoDataset is implemented and what it is producing. Take some data out of the dataflow and check their shapes. See http://tensorpack.readthedocs.io/en/latest/tutorial/dataflow.html#use-dataflow-outside-tensorpack.",probably original issue data homogeneous shape would recommend check take data check see,issue,positive,positive,positive,positive,positive,positive
360959771,"Here are the dataset I used. If I changed the data list, the error appeared.

ds = VideoDataset(datadir, name, shuffle=True)
ds = AugmentImageComponent(ds, augmentors, copy=False)
ds = PrefetchDataZMQ(ds, cpu)
ds = BatchData(ds, batch_size, remainder=False)",used data list error name,issue,negative,neutral,neutral,neutral,neutral,neutral
360959377,"Here are the log, the error is same as HongyangGao provided. The part of the model loading log is omitted.


[0127 12:27:00 @sessinit.py:219] Restoring from dict ...
[0127 12:27:09 @base.py:157] Graph Finalized.
[0127 12:27:13 @concurrency.py:36] Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...
[0127 12:27:13 @concurrency.py:36] Starting EnqueueThread QueueInput/input_queue ...
[0127 12:27:13 @base.py:191] Start Epoch 1 ...
  0%|                                                                                                                                  |0/1788[00:00<?,?it/s][0127 12:27:13 @input_source.py:493] Pre-filling staging area ...
[0127 12:27:14 @common.py:138] ERR Cannot batch data. Perhaps they are of inconsistent shape?
Traceback (most recent call last):
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/dataflow/common.py"", line 136, in _aggregate_batch
    np.asarray([x[k] for x in data_holder], dtype=tp))
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/numpy/core/numeric.py"", line 492, in asarray
    return array(a, dtype, copy=False, order=order)
ValueError: setting an array element with a sequence.
[0127 12:27:14 @common.py:141] ERR Shape of all arrays to be batched: [(224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3),
 (224, 224, 3),
 (128, 224, 224, 3)]
[0127 12:27:14 @common.py:138] ERR Cannot batch data. Perhaps they are of inconsistent shape?
Traceback (most recent call last):
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/dataflow/common.py"", line 136, in _aggregate_batch
    np.asarray([x[k] for x in data_holder], dtype=tp))
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/numpy/core/numeric.py"", line 492, in asarray
    return array(a, dtype, copy=False, order=order)
ValueError: setting an array element with a sequence.
[0127 12:27:14 @input_source.py:142] ERR Exception in EnqueueThread QueueInput/input_queue:
Traceback (most recent call last):
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/input_source/input_source.py"", line 135, in run
    self.op.run(feed_dict=feed)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1744, in run
    _run_using_default_session(self, feed_dict, self.graph, session)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 4120, in _run_using_default_session
    session.run(operation, feed_dict)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
InvalidArgumentError: You must feed a value for placeholder tensor 'label' with dtype int32 and shape [?]
	 [[Node: label = Placeholder[dtype=DT_INT32, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'label', defined at:
  File ""video-pretrain-resnet-ft.py"", line 269, in <module>
    SyncMultiGPUTrainerParameterServer(config).train()
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/train/base.py"", line 327, in __new__
    return old_trainer(*args, **kwargs)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/trainv1/multigpu.py"", line 66, in __init__
    super(SyncMultiGPUTrainerParameterServer, self).__init__(config)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/trainv1/base.py"", line 73, in __init__
    self._setup()   # subclass will setup the graph and InputSource
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/trainv1/multigpu.py"", line 69, in _setup
    callbacks = self._input_source.setup(self.model.get_inputs_desc())
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 171, in wrapper
    return func(*args, **kwargs)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/input_source/input_source_base.py"", line 97, in setup
    self._setup(inputs_desc)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/input_source/input_source.py"", line 524, in _setup
    self._input.setup(inputs)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 171, in wrapper
    return func(*args, **kwargs)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/input_source/input_source_base.py"", line 97, in setup
    self._setup(inputs_desc)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/input_source/input_source.py"", line 185, in _setup
    self._input_placehdrs = [v.build_placeholder_reuse() for v in inputs]
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py"", line 68, in build_placeholder_reuse
    return self.build_placeholder()
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py"", line 53, in build_placeholder
    self.type, shape=self.shape, name=self.name)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1548, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2094, in _placeholder
    name=name)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'label' with dtype int32 and shape [?]
	 [[Node: label = Placeholder[dtype=DT_INT32, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

[0127 12:27:14 @input_source.py:148] EnqueueThread QueueInput/input_queue Exited.
[0127 12:27:14 @base.py:207] Training was stopped.

PrefetchDataZMQ successfully cleaned-up.
[0127 12:27:14 @parallel.py:62] [PrefetchDataZMQ] Context terminated.
[0127 12:27:14 @input_source.py:148] EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue Exited.
PrefetchDataZMQ successfully cleaned-up.",log error provided part model loading log graph starting starting start epoch staging area err batch data perhaps inconsistent shape recent call last file line file line return array setting array element sequence err shape err batch data perhaps inconsistent shape recent call last file line file line return array setting array element sequence err exception recent call last file line run file line run self session file line operation file line run file line file line file line raise type message must feed value tensor shape node label defined file line module file line return file line super self file line subclass setup graph file line file line wrapper return file line setup file line file line wrapper return file line setup file line file line return file line file line return file line file line file line file line see must feed value tensor shape node label training stopped successfully context successfully,issue,positive,positive,positive,positive,positive,positive
360955617,@PeisenZhao Please open a new issue following the [issue template](https://github.com/ppwwyyxx/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md) or add those information here. I don't know if it's related to this issue or not without you providing those information.,please open new issue following issue template add information know related issue without providing information,issue,negative,positive,neutral,neutral,positive,positive
360952487,"Hello, @ppwwyyxx . I have met the same problem as HongyangGao mentioned. I tried:
1 Add   export TENSORPACK_PIPEDIR=/tmp   to bashrc
2 Changed the tensorflow version to 1.3
but they did not help.

I found an another interesting phenomenon. If I kept the input data list as the first time I run the code(after reboot), the code could run. But if I changed the input data list, the error appeared. 

I think there are something fixed when I run the code first time. 
",hello met problem tried add export version help found another interesting phenomenon kept input data list first time run code code could run input data list error think something fixed run code first time,issue,negative,positive,positive,positive,positive,positive
360237506,"Keras has btw a very nice start-page:
https://keras.io/

compared to

http://tensorpack.com

Is it possible to just use the readme.md as the landing page in the docs?",nice possible use landing page,issue,negative,positive,positive,positive,positive,positive
360233393,The readme now looks better! I'll add that somewhere in the documentation as well. ,better add somewhere documentation well,issue,positive,positive,positive,positive,positive,positive
360227288,"There should not be ""ale"" inside env.

A3C is based on gym and to get lives in gym you need to `print(info)`.",ale inside based gym get gym need print,issue,negative,neutral,neutral,neutral,neutral,neutral
360008426,"1. `SyncMultiGPUTrainerParameterServer` works fine.
1. Agree. Will look into this later.

Close for now. Will reopen if new clues found.
Thanks for the reply! :p @ppwwyyxx ",work fine agree look later close reopen new found thanks reply,issue,positive,positive,positive,positive,positive,positive
359851401,"Does `SyncMultiGPUTrainerParameterServer` work this way?
I think this is a NCCL or tensorflow issue.",work way think issue,issue,negative,neutral,neutral,neutral,neutral,neutral
359592788,"Ok, cool. Thank you very much for your help.",cool thank much help,issue,positive,positive,positive,positive,positive,positive
359590959,"Most parallel dataflows do not support this, and even if some do, it may not work the same way you may expect. For example, MultiThreadPrefetchData can be used twice, but each data point from it will only go to one of the two but not both of them. In general it's best to not use it twice to avoid confusion.
",parallel support even may work way may expect example used twice data point go one two general best use twice avoid confusion,issue,positive,positive,positive,positive,positive,positive
359587951,"Thank you for quick response. When you say not use PrefetchDataZMQ does that mean any of the parallel dataflows ? 
I will try to redesign the whole flow to avoid using train_ds = BatchData(train_ds, batch, use_list=True) twice. ",thank quick response say use mean parallel try redesign whole flow avoid batch twice,issue,negative,positive,neutral,neutral,positive,positive
359582647,"You used this dataflow instance: `train_ds = BatchData(train_ds, batch, use_list=True)` twice and that's not allowed for `PrefetchDataZMQ`.
You should either create two dataflow instances or not use `PrefetchDataZMQ`.",used instance batch twice either create two use,issue,negative,neutral,neutral,neutral,neutral,neutral
359083635,"I believe I found the problem: as I mentioned earlier, my database is stored in lower case file names. When I ran sox to convert from NIST to RIFF waveform, this ment that the input and output file names where identical. I was aware of this, but assumed that the conversion would work nevertheless. However, now that I look at the wav files, they all have the same size, and probably contained no audio data at all. I will correct that. Thank you for your help!",believe found problem lower case file ran convert riff input output file identical aware assumed conversion would work nevertheless however look size probably audio data correct thank help,issue,negative,positive,positive,positive,positive,positive
359078827,"Training works fine for me. Looks like there is something wrong with your data processing: you got a extremely small feature and the feature has the same length for all audios. However audios are of different length so you should get different length of features.

I got something like this:
```python
(798, 39) (59,)
(680, 39) (35,)
(720, 39) (35,)
(901, 39) (59,)
(494, 39) (35,)
(401, 39) (33,)
(443, 39) (36,)
(425, 39) (28,)
(484, 39) (28,)
(647, 39) (33,)
(750, 39) (46,)
(666, 39) (47,)
(529, 39) (31,)
(780, 39) (44,)
(468, 39) (32,)
(644, 39) (44,)
(502, 39) (34,)
(693, 39) (32,)
(557, 39) (35,)
(332, 39) (29,)
(628, 39) (42,)
(811, 39) (50,)
(667, 39) (41,)
(871, 39) (70,)
(552, 39) (38,)
(509, 39) (32,)
(553, 39) (28,)
```
Please first check your *.wav files and make sure they are valid.",training work fine like something wrong data got extremely small feature feature length however different length get different length got something like python please first check make sure valid,issue,positive,positive,neutral,neutral,positive,positive
359034468,"I went a bit further: it seems each data point returned by get_data() has two arrays, the first always has the same size and the second varies in size. So, perhaps I have misinterpreted the data points returned by get_data(), thinking they would be utterances whereas they are fixed windows of features vectors, instead. I still can't explain the variable size of the second element of dp.

```
# get all the utterances
dps = [dp for dp in ds.get_data()]
# print the shapes for the first 10
for i in range(10):
   print(dps[i][0].shape, dps[i][1].shape)
```
((16, 39), (36,))
((16, 39), (37,))
((16, 39), (32,))
((16, 39), (35,))
((16, 39), (44,))
((16, 39), (19,))
((16, 39), (35,))
((16, 39), (30,))
((16, 39), (36,))
((16, 39), (37,))
",went bit data point returned two first always size second size perhaps data returned thinking would whereas fixed instead still ca explain variable size second element get print first range print,issue,negative,positive,positive,positive,positive,positive
359029903,"If it can help, I ran the following with my `train.mdb` file:
```
from tensorpack import *
ds = LMDBDataPoint('train.mdb', shuffle=False)
ds.reset_state()
# get the first utterance
for dp in ds.get_data():
   break
```
The I looked into `dp`: `len(dp)` returns 2, `dp[0].shape` returns `(16, 39)` and dp[1].shape returns `(36,)`.

It does indeed look strange if dp[1] is supposed to hold the labels for the feature vectors in dp[0]. But I am not sure how to fix it.",help ran following file import get first utterance break indeed look strange supposed hold feature sure fix,issue,negative,positive,positive,positive,positive,positive
359025194,"That looks like a data-processing error (Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]). I'll find the dataset and check it.",like error invalid argument label valid index need index find check,issue,negative,neutral,neutral,neutral,neutral,neutral
359024220,"Hi,
thank you for the fast answer and commit. The script now goes past the previous error, but I get the following problem (very long log, I am afraid). I know it's hard to debug without the data, but if you see anything that can be easily fixed I would appreciate any hints.

Thank you!

[0119 17:45:21 @logger.py:74] Argv: ./train-timit.py --train train.mdb --test test.mdb --stat stats.data
[0119 17:45:21 @format.py:86] Found 4620 entries in train.mdb
[0119 17:45:21 @format.py:86] Found 1680 entries in test.mdb
[0119 17:45:21 @param.py:189] Use train_log/train-timit/hyper.txt to set hyperparam: 'learning_rate'.
[0119 17:45:21 @inference_runner.py:80] InferenceRunner will eval 26 iterations
[0119 17:45:21 @input_source.py:193] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[0119 17:45:22 @registry.py:122] fc input: [None, 128]
[0119 17:45:22 @registry.py:130] fc output: [None, 62]
[0119 17:45:22 @model_utils.py:49] Model Parameters: 
name                                          shape          dim
--------------------------------------------  ----------  ------
rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0  [167, 512]   85504
rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0    [512]          512
rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0  [256, 512]  131072
rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0    [512]          512
fc/W:0                                        [128, 62]     7936
fc/b:0                                        [62]            62
Total #vars=6, #params=225598, size=0.86MB
[0119 17:45:22 @base.py:196] Setup callbacks graph ...
[0119 17:45:22 @predict.py:42] Building predictor tower 'InferenceTower' on device /gpu:0 ...
[0119 17:45:22 @collection.py:140] Size of these collections were changed in InferenceTower: (tf.GraphKeys.WHILE_CONTEXT: 1->2)
[0119 17:45:22 @summary.py:34] Maintain moving average summary of 8 tensors.
[0119 17:45:22 @base.py:212] Creating the session ...
2018-01-19 17:45:22.933029: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 17:45:22.933055: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 17:45:22.933062: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 17:45:23.141723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-19 17:45:23.142214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: TITAN Xp
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 11.74GiB
2018-01-19 17:45:23.142233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-01-19 17:45:23.142240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-01-19 17:45:23.142249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0)
[0119 17:45:23 @base.py:220] Initializing the session ...
[0119 17:45:23 @base.py:227] Graph Finalized.
[0119 17:45:23 @concurrency.py:36] Starting EnqueueThread QueueInput/input_queue ...
[0119 17:45:23 @base.py:247] Start Epoch 1 ...
  0%|                                                                                          |0/72[00:00<?,?it/s]2018-01-19 17:45:24.240321: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240328: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240344: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240358: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240364: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240542: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240563: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240553: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240578: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240596: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240601: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240780: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240786: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240803: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240815: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240822: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240967: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240973: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240981: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240985: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.240996: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241156: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241162: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241175: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241192: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241200: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241395: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241414: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241423: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241441: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241446: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241600: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241610: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241621: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241623: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241633: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241779: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241791: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241801: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241817: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241824: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241983: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.241992: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242000: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242009: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242010: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242148: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242156: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242166: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242179: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242186: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242329: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242341: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242350: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242364: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242370: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242477: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242506: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242529: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242552: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]
2018-01-19 17:45:24.242573: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]

2018-01-19 17:45:24.243335: W tensorflow/core/kernels/queue_base.cc:295] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed
Traceback (most recent call last):
  File ""./train-timit.py"", line 126, in <module>
[0119 17:45:24 @input_source.py:148] EnqueueThread QueueInput/input_queue Exited.
    launch_train_with_config(config, SimpleTrainer())
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/interface.py"", line 96, in launch_train_with_config
    config.steps_per_epoch, config.starting_epoch, config.max_epoch)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/base.py"", line 288, in train
    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 171, in wrapper
    return func(*args, **kwargs)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/base.py"", line 253, in main_loop
    self.run_step()  # implemented by subclass
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/base.py"", line 172, in run_step
    self.hooked_sess.run(self.train_op)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 518, in run
    run_metadata=run_metadata)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 862, in run
    run_metadata=run_metadata)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 818, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 972, in run
    run_metadata=run_metadata)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 818, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]

Caused by op u'CTCLoss', defined at:
  File ""./train-timit.py"", line 126, in <module>
    launch_train_with_config(config, SimpleTrainer())
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/interface.py"", line 92, in launch_train_with_config
    model._build_graph_get_cost, model.get_optimizer)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 171, in wrapper
    return func(*args, **kwargs)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/tower.py"", line 161, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/trainers.py"", line 52, in _setup_graph
    grads = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)()
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/tower.py"", line 188, in get_grad_fn
    cost = get_cost_fn(*input.get_input_tensors())
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/tfutils/tower.py"", line 206, in __call__
    output = self._tower_fn(*args)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py"", line 168, in _build_graph_get_cost
    self.build_graph(*inputs)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py"", line 116, in build_graph
    self._build_graph(inputs)
  File ""./train-timit.py"", line 54, in _build_graph
    loss = tf.nn.ctc_loss(label, logits, seqlen, time_major=False)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/ops/ctc_ops.py"", line 152, in ctc_loss
    ignore_longer_outputs_than_inputs=ignore_longer_outputs_than_inputs)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/ops/gen_ctc_ops.py"", line 168, in _ctc_loss
    name=name)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](transpose_1/_221, QueueInput/input_deque:1, QueueInput/input_deque:2, QueueInput/input_deque:4)]]

PrefetchDataZMQ successfully cleaned-up.",hi thank fast answer commit script go past previous error get following problem long log afraid know hard without data see anything easily fixed would appreciate thank train test found found use set setting queue input none output none model name shape dim total setup graph building predictor tower device size maintain moving average summary session library use available machine could speed library use available machine could speed library use available machine could speed successful node read negative value must least one node node zero found device name major minor total memory free memory device device name bus id session graph starting start epoch invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node invalid argument label valid index need index node skipping attempt queue closed recent call last file line module file line file line train file line wrapper return file line subclass file line file line run file line run file line run return file line run file line run return file line run file line file line file line raise type message label valid index need index node defined file line module file line file line wrapper return file line input file line input file line cost file line output file line file line file line loss label file line file line file line file line file line see label valid index need index node successfully,issue,negative,positive,neutral,neutral,positive,positive
359021047,Looks like the shape should at least have known dimension. Could you check again with the above commit? I don't have TIMIT data with me now.,like shape least known dimension could check commit data,issue,positive,negative,negative,negative,negative,negative
359009535,Try using `import tensorflow` as the first line in your program. Most of the times it solves the problem.,try import first line program time problem,issue,negative,positive,positive,positive,positive,positive
359009144,This seems to be a opencv-tensorflow incompatibility. Certain prebuilt opencv & tensorflow don't work well together.,incompatibility certain work well together,issue,negative,positive,positive,positive,positive,positive
358994794,"update: running
`conda install tensorflow-gpu`
instead of the above pip install, seems to work.",update running install instead pip install work,issue,negative,neutral,neutral,neutral,neutral,neutral
358740736,"https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/OgGd7sfu1bE has more discussion about this topic. Github issue of a deep learning library is not a proper place for discussing a paper, even though I happen to be one of the authors.",discussion topic issue deep learning library proper place paper even though happen one,issue,negative,neutral,neutral,neutral,neutral,neutral
358728004,"sorry, typo.. 

I really want is ResNet101-SE.. 😞 ",sorry typo really want,issue,negative,negative,negative,negative,negative,negative
358513485,"It depends on what do you mean by an ""epoch"", a ""step"", and how the input data is defined, etc, so there is not an answer to that.
But usually the default value will make one epoch larger than what most people would expect.",mean epoch step input data defined answer usually default value make one epoch people would expect,issue,negative,negative,negative,negative,negative,negative
358512342,"Thanks. So when using multiple GPUs, the steps_per_epoch should always be set manually?",thanks multiple always set manually,issue,negative,positive,neutral,neutral,positive,positive
358509903,"The default value for `steps_per_epoch` is `dataset_train.size()`. It does not know how you train the model (data parallel on multi GPU or not).
That's why you should not remove the line.

Also, http://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html#multigpu-trainers",default value know train model data parallel remove line also,issue,negative,neutral,neutral,neutral,neutral,neutral
358494177,http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html the tutorial has everything needed to do this. `DictRestore` allows you to load an arbitrary dict of values .,tutorial everything load arbitrary,issue,negative,negative,neutral,neutral,negative,negative
358319666,Just wanted to let you know that it does in fact work! Thanks,let know fact work thanks,issue,negative,positive,positive,positive,positive,positive
358308318,"Again, look at `tf.gather`, `tf.boolean_mask`.

The documentation of tensorflow is very clear that `tf.nn.sparse_softmax_cross_entropy_with_logits` does not support -1.

Not a tensorpack question. Closing.",look documentation clear support question,issue,positive,positive,positive,positive,positive,positive
358307892,if I set the label = -1 of a picture， that mean this picture does not calculate the loss,set label mean picture calculate loss,issue,negative,negative,negative,negative,negative,negative
358305738,"I don't know what do you mean by ""ignore"". Write your math down and convert it to tensorflow.

You might be interested in `tf.gather`,  `tf.boolean_mask`.",know mean ignore write math convert might interested,issue,negative,negative,neutral,neutral,negative,negative
358279624,"One question, note that TF r1.5 support cudnn 7.0 which highlighted with a faster group convolution implementation, will tensorpack add group conv based on cudnn 7.0 instead?",one question note support faster group convolution implementation add group based instead,issue,negative,neutral,neutral,neutral,neutral,neutral
358136396,Okay thanks! I’ll give that a shot and let you know what happens. Have a great day!,thanks give shot let know great day,issue,positive,positive,positive,positive,positive,positive
358135813,It was changed to `parallel` but I don't think you should use it either. You can `from tensorpack.dataflow import PrefetchData`,parallel think use either import,issue,negative,neutral,neutral,neutral,neutral,neutral
358134695,Thanks for getting back so quickly! What was it recently changed to?,thanks getting back quickly recently,issue,negative,positive,neutral,neutral,positive,positive
358132978,"I expect people to use `from tensorpack.dataflow import PrefetchData`. The name `prefetch` is actually quite arbitrary, and recently was changed.",expect people use import name actually quite arbitrary recently,issue,negative,negative,neutral,neutral,negative,negative
357825837,Turns out it is not related to the cudnn bugs. Thanks for your help. The rest of the problem should better ask on stackoverflow.,turn related thanks help rest problem better ask,issue,positive,positive,positive,positive,positive,positive
357760915,Closing since there is no response from @tonyw on details of the original issue.,since response original issue,issue,negative,positive,positive,positive,positive,positive
357750547,"Several combinations of opencv builds and tensorflow builds can often cause segfault and there has been several issues in TF about this.

In general I do not recommend using any of the pre-built opencv unless those that come with the OS distribution.

Not a tensorpack issue. closing.",several often cause several general recommend unless come o distribution issue,issue,negative,positive,neutral,neutral,positive,positive
357748935,"1. It'll be in 1.5, or maybe 1.6, I'm not sure. https://github.com/tensorflow/tensorflow/commit/3a3b7530ebc9a6bfbfff8ed50bc7622a78b5b36b
2. I don't understand what is the ""shape difference"" you're talking about. I don't have the context about what's going on in your code. But this sounds like a general tensorflow question to me. Are you talking about a potential bug in your code that you couldn't find or what?",maybe sure understand shape difference talking context going code like general question talking potential bug code could find,issue,positive,positive,positive,positive,positive,positive
357729499,"The workaround is to run this before running tensorpack:
```
pip install 'opencv-python==3.3.0.9'
```
Based on this [https://github.com/skvark/opencv-python/issues/44](https://github.com/skvark/opencv-python/issues/44)",run running pip install based,issue,negative,neutral,neutral,neutral,neutral,neutral
357566727,"hi, you can check any example in tensorpack, such as [resnet](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py#L81).

just follow them as convention.",hi check example follow convention,issue,negative,neutral,neutral,neutral,neutral,neutral
357565630,"Could yo elaborate a little bit more?
Can I add the code somewhere under _build_graph() as follows?

class Model(ModelDesc):
    def _build_graph(self, inputs):
       ...
      callbacks=[
RunOp(lambda: tf.add_check_numerics_ops(), run_before=False, run_as_trigger=False, run_step=True),
 ...]`.
       ...",could yo elaborate little bit add code somewhere class model self lambda,issue,negative,positive,positive,positive,positive,positive
357309346,Essentially this is to transform two components with some parameters shared (shape) and some parameters not shared (interpolation). This is way beyond what's included and I recommend you process it by yourself.,essentially transform two shape interpolation way beyond included recommend process,issue,negative,neutral,neutral,neutral,neutral,neutral
357225794,"my writing about multi-gpu training is wrong, now fixed, thank you",writing training wrong fixed thank,issue,negative,negative,negative,negative,negative,negative
357185764,"That's just my choice in the model and you can certainly use different strategies in your code.

>  If I want to train it on 4 GPUs, the max Batch size theoretically should be 4(1 per GPU), but the fact is even batch_size=4 or 3 or 2 is not ok

If what you're talking about is total batch size, how can you possibly train a total batch of 3 on 4 GPUs? I can't imagine how this can be implemented in tensorpack.",choice model certainly use different code want train batch size theoretically per fact even talking total batch size possibly train total batch ca imagine,issue,negative,positive,neutral,neutral,positive,positive
357184972,"maybe my implementation is wrong, I contrast with your imagenet-resnet.py.

TOTAL_BATCH_SIZE=256,

in your implementations, it seems that no matter how many GPUS, the total batch size is always 256.

I doubt:  since 1-gpu can hold 256, if you have 4-GPU, why not set batch size as 1024. in your implementation, if 4-gpu, single-gpu batch size is 256/4=64.
",maybe implementation wrong contrast matter many total batch size always doubt since hold set batch size implementation batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
357182233,Are you sure were you setting per-gpu batch size or total batch size?,sure setting batch size total batch size,issue,negative,positive,positive,positive,positive,positive
357128996,"Thank you for your patience. I get it. 
I use this code to get the weights:
`a = np.load('1.npy')
print a.item()['which weights you want']
`",thank patience get use code get print want,issue,negative,neutral,neutral,neutral,neutral,neutral
357126245,"Thanks. I know how to use fw(x). What confuse me is how to get the weights from the npy format ? (Because the weights are saved in a .npy file) I try to use np.load('xx.npy'), and just get :
![image](https://user-images.githubusercontent.com/20589365/34857092-f53ea396-f783-11e7-81a1-243123fc3608.png)
![image](https://user-images.githubusercontent.com/20589365/34857147-2b43ffa4-f784-11e7-964a-3da3024b6025.png)

",thanks know use confuse get format saved file try use get image image,issue,positive,positive,positive,positive,positive,positive
357122940,"```python
import numpy as np
import tensorflow as tf
import tensorflow.contrib.eager as eager
eager.enable_eager_execution()

bitW = 1

def fw(x):
    if bitW == 32:
        return x
    if bitW == 1:   # BWN
        E = tf.stop_gradient(tf.reduce_mean(tf.abs(x)))
        return tf.sign(x / E) * E
    x = tf.tanh(x)
    x = x / tf.reduce_max(tf.abs(x)) * 0.5 + 0.5
    return 2 * quantize(x, bitW) - 1


arr = np.random.rand(10, 10) - 0.5
print(arr, fw(arr))
```",python import import import eager return return return quantize print,issue,negative,neutral,neutral,neutral,neutral,neutral
357121979,"Sir, I failed to do the post-processing after training. I can't make weights in .npy going through Binary Function. Could you help me?",sir training ca make going binary function could help,issue,negative,neutral,neutral,neutral,neutral,neutral
356886344,"I've got it works 

here is callback:


`# -*- coding: UTF-8 -*-

import numpy as np
import six

from tensorpack.callbacks.base import Callback
from tensorpack.utils import logger
from tensorpack.utils.stats import ConfusionMatrix
from tensorpack.tfutils.common import get_op_tensor_name
import tensorflow as tf

__all__ = ['AfterTrainEpoch','ConfusionMatrixAfterEpoch']


class AfterTrainEpoch(Callback):

    def _setup_graph(self):
        train_tower = self.trainer.tower_func.towers[0]
        self.outputs = [train_tower.get_tensor(name) for name in self.get_fetches()]

    def _before_train(self):
        pass

    def _before_epoch(self):
        self._before_train_epoch()

    def _before_train_epoch(self):
        pass

    def _before_run(self, _):
        return tf.train.SessionRunArgs(fetches=self.outputs)

    def _after_run(self, _, run_values):
        self.on_fetches(run_values.results)

    def on_fetches(self, outputs):
        self._on_fetches(outputs)

    def _on_fetches(self, outputs):
        raise NotImplementedError()

    def _trigger_epoch(self):
        ret = self._after_train_epoch()
        if ret is None:
            return
        for k, v in six.iteritems(ret):
            try:
                v = float(v)
            except ValueError:
                logger.warn(""{} returns a non-scalar statistics!"".format(type(self).__name__))
                continue
            else:
                self.trainer.monitors.put_scalar(k, v)

    def _after_train_epoch(self):
        pass

    def get_fetches(self):
        try:
            ret = self._get_fetches()
        except NotImplementedError:
            logger.warn(""Inferencer._get_output_tensors was deprecated and renamed to _get_fetches"")
            ret = self._get_output_tensors()

        return [get_op_tensor_name(n)[1] for n in ret]

    def _get_fetches(self):
        raise NotImplementedError()

    def _get_output_tensors(self):
        pass


class ConfusionMatrixAfterEpoch(AfterTrainEpoch):
    """"""
    Compute precision / recall of classification, given the
    prediction vector and the label vector.
    """"""

    def __init__(self, pred_tensor_name, label_tensor_name, labels_list):
        """"""
        Args:
            pred_tensor_name(str): name of the 0/1 prediction tensor.
            label_tensor_name(str): name of the 0/1 label tensor.
        """"""
        self.label_list = labels_list
        self.pred_tensor_name = pred_tensor_name
        self.label_tensor_name = label_tensor_name
        self.stat = ConfusionMatrix()
        self.labels = {
                'labels': [],
                'predic': []
                }

    def _before_train_epoch(self):
        self.labels['labels']=[]
        self.labels['predic']=[]

    def _get_fetches(self):
        return [self.pred_tensor_name, self.label_tensor_name]

    def _on_fetches(self, outputs):
        pred, label = outputs
        self.labels['labels'].append(label)
        self.labels['predic'].append(pred)

    def _after_train_epoch(self):
        self.stat.feed(np.concatenate(self.labels['labels'], axis=0),
                       np.concatenate(self.labels['predic'], axis=0), self.label_list)
        out_dict = {}
        for i in range(self.stat.get_conf_matr.shape[0]):
            sum_recall =  self.stat.get_conf_matr[i,:].sum()
            if not bool(sum_recall):
                recall = -1
            else:
                recall = round(self.stat.get_conf_matr[i,i] / sum_recall * 100, 2)
            sum_precision = self.stat.get_conf_matr[:,i].sum()
            if not bool(sum_precision):
                precision = -1
            else:
                precision = round(self.stat.get_conf_matr[i,i] / sum_precision * 100, 2)
            tab='|'
            for j in range(self.stat.get_conf_matr.shape[1]):
                tab += '{: =6.0f}|'.format(self.stat.get_conf_matr[i,j])
            name = 'train conf matrix of class {}: {}, Recall:{: =5.2f}, Precision'.format(i,tab, recall)
            out_dict[name] = precision
        return out_dict
`

and I added function to utils.stats

`
from sklearn.metrics import confusion_matrix
class ConfusionMatrix(object):
    """"""
    Statistics for one_hot mutliclass binary decision,
    including precision, recall, false positive, false negative
    """"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.conf_matr = None  # positive label

    def feed(self, pred, label, labels):
        """"""
        Args:
            pred (np.ndarray): binary array.
            label (np.ndarray): binary array of the same size.
        """"""
        assert pred.shape == label.shape, ""{} != {}"".format(pred.shape, label.shape)
        self.conf_matr = confusion_matrix(label, pred, labels)

    @property
    def get_conf_matr(self):
        return self.conf_matr
`",got work import import six import import logger import import import class self name name self pas self self pas self return self self self raise self ret ret none return ret try float except statistic type self continue else self pas self try ret except ret return ret self raise self pas class compute precision recall classification given prediction vector label self name prediction tensor name label self self return self label label self range bool recall else recall round bool precision else precision round range tab name matrix class recall tab recall name precision return added function import class object statistic binary decision precision recall false positive false negative self reset self none positive label feed self label binary array label binary array assert label property self return,issue,positive,negative,negative,negative,negative,negative
356861800,"Now I use     
def _trigger_step(self):
        outputs = [tensor.eval() for tensor in self.outputs]
        self.on_fetches(outputs)


where is self.outputs - list of tensors. How can I get values of this tensors in after_run method?",use self tensor list get method,issue,negative,neutral,neutral,neutral,neutral,neutral
356858707,"I see. It uses a new batch of data because it's a new sess.run call. 
What you want is to evaluate the tensor **together with** the training and you'll need to use the `before/after_run` method in callbacks.",see new batch data new call want evaluate tensor together training need use method,issue,negative,positive,positive,positive,positive,positive
356857856,"I meant tensor.eval() method - just call to get an value of tensor.

program logic is -
- it runs train step of current batch. 
- collect step: I'd like to collect labels and predictions of current batch to save them into corresponded dictionary with lists inside. So I use tensor_of_prediction.eval() and tensor_of_labels.eval() method to get values of labels and predictions. This .eval() method should use current data batch to do such step properly.
the question is - does .eval() method use current input data batch to calculate forward run or it reads next batch of data? ",meant method call get value tensor program logic train step current batch collect step like collect current batch save dictionary inside use method get method use current data batch step properly question method use current input data batch calculate forward run next batch data,issue,positive,neutral,neutral,neutral,neutral,neutral
356851152,"I don't know what is the ""eval"" method and what's in there.

And what do you mean by ""new batch of data""? What data would be an old batch of data?",know method mean new batch data data would old batch data,issue,negative,negative,neutral,neutral,negative,negative
356850119,"I use BatchData(remainder=True). 
This error was eliminated when I setup BatchData(remainder=False).

one another question:
- if I use eval() method - does it force new batch of data from input queue?

",use error setup one another question use method force new batch data input queue,issue,negative,positive,positive,positive,positive,positive
356723137,"I checked my code and found the problem is caused by deleting 'steps_per_epoch=5000' in TrainConfig in imagenet-resnet.py. When this config is added, it shows the correct running progress now. Thanks a lot! ",checked code found problem added correct running progress thanks lot,issue,negative,positive,positive,positive,positive,positive
356528403,"really strange, I cannot even reproduce it today on my machine.",really strange even reproduce today machine,issue,negative,negative,neutral,neutral,negative,negative
356435986,"Thanks very much for your help. I will paste my final runnable code here in case anyone else want to do similar thing such as multitask training.
```python
import tensorflow as tf
from tensorpack import ModelDescBase, InputDesc, FullyConnected, \
    BatchData, DataFromQueue, StartProcOrThread, QueueInput, ModelSaver, PeriodicTrigger
from tensorpack.callbacks import Callback
from tensorpack.train.tower import TowerTrainer
from tensorpack.tfutils import optimizer, summary
from tensorpack.tfutils.tower import TowerContext, TowerFuncWrapper, get_current_tower_context
from tensorpack.utils.argtools import memoized
from tensorpack.utils import logger
import threading
from six.moves import queue
import numpy as np


class MyModel(ModelDescBase):
    """""" A custom model which consists of two sub-networks""""""

    def _get_inputs(self):
        l = self.get_inputs_desc_list()
        res = []
        for x in l:
            res += x
        return res

    def get_inputs_desc_list(self):
        ds1 = [InputDesc(tf.float32, (None, 2), 'input1'),
               InputDesc(tf.float32, (None,), 'label1')]
        ds2 = [InputDesc(tf.float32, (None, 2), 'input2'),
               InputDesc(tf.float32, (None,), 'label2')]
        return [ds1, ds2]

    def _build_graph(self, inputs):
        data1, label1, data2, label2 = inputs
        with tf.variable_scope(""share"") as share_scope:
            y1 = FullyConnected('fc0', data1, 10, nl=tf.nn.relu)

        with tf.variable_scope('net1'):
            y1 = FullyConnected('fc_out', y1, 1, nl=tf.identity)

        with tf.variable_scope('net2'):
            with tf.variable_scope(share_scope, reuse=True):
                y2 = FullyConnected('fc0', data2, 10, nl=tf.nn.relu)
            y2 = FullyConnected('fc_out', y2, 1, nl=tf.identity)

        self.y1_loss = tf.nn.l2_loss(y1 - label1)
        self.y1_loss = tf.truediv(self.y1_loss, tf.cast(
            tf.shape(label1)[0], tf.float32), name='net1_loss')
        self.y2_loss = tf.nn.l2_loss(y2 - label2)
        self.y2_loss = tf.truediv(self.y2_loss, tf.cast(
            tf.shape(label2)[0], tf.float32), name='net2_loss')

        self.vars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \
            tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net1')
        self.vars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \
            tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net2')

        summary.add_moving_summary(self.y1_loss, collection='loss1')
        summary.add_moving_summary(self.y2_loss, collection='loss2')

    @memoized
    def get_optimizer(self):
        lr = tf.get_variable(
            'learning_rate', initializer=1e-4, trainable=False)
        opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)
        return opt


class MyTrainer(TowerTrainer):
    """"""
    A custom trainer which optimize several costs from different dataflows simutaneously
    """"""

    def __init__(self, inputs, model):
        """"""
        inputs ([InputSource]): a list of several InputSources
        model (ModelDescBase): a model
        """"""
        super(MyTrainer, self).__init__()
        inputs_desc = model.get_inputs_desc()
        inputs_desc_list = model.get_inputs_desc_list()

        # setup input callbacks for each InputSource
        for dataflow, desc in zip(inputs, inputs_desc_list):
            cbs = dataflow.setup(desc)
            self.register_callback(cbs)

        # build the graph
        self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc)
        tensors = []
        for dataflow in inputs:
            tensors += dataflow.get_input_tensors()
        with TowerContext('', is_training=True):
            self.tower_func(*tensors)

        opt = model.get_optimizer()
        with tf.name_scope('optimize'):
            self.opt_y1 = opt.minimize(
                model.y1_loss, var_list=model.vars1, name='op1')
            self.opt_y2 = opt.minimize(
                model.y2_loss, var_list=model.vars2, name='op2')

    def run_step(self):
        self.hooked_sess.run([self.opt_y1, tf.get_collection('loss1')])
        self.hooked_sess.run([self.opt_y2, tf.get_collection('loss2')])

    def get_predictor1(self):
        return self.get_predictor(['input1'], ['net1/fc_out/output'])

    def get_predictor2(self):
        return self.get_predictor(['input2'], ['net2/fc_out/output'])


class DataThread1(threading.Thread):
    def __init__(self, batch_size):
        super(DataThread1, self).__init__()
        self.queue = queue.Queue(maxsize=batch_size * 8 * 2)
        self.daemon = True
        self.name = 'DataThread1'

    def run(self):
        while True:
            x = np.random.rand(2)
            y = np.sum(x)
            self.queue.put([x, y])


class DataThread2(threading.Thread):
    def __init__(self, batch_size):
        super(DataThread2, self).__init__()
        self.queue = queue.Queue(maxsize=batch_size * 8 * 2)
        self.daemon = True
        self.name = 'DataThread2'

    def run(self):
        while True:
            x = np.random.rand(2)
            y = np.sum(x) * 0.01
            self.queue.put([x, y])


class Evaluator(Callback):
    def _setup_graph(self):
        self.pred_func1 = self.trainer.get_predictor1()
        self.pred_func2 = self.trainer.get_predictor2()

    def _trigger(self):
        x = np.array([1, 2])
        pred1 = self.pred_func1(x[None])[0][0]
        pred2 = self.pred_func2(x[None])[0][0]
        self.trainer.monitors.put_scalar('pred1', np.asscalar(pred1))
        self.trainer.monitors.put_scalar('pred2', np.asscalar(pred2))

if __name__ == '__main__':
    logger.set_logger_dir('/home/sliay/models/trainer')
    BATCH_SIZE = 128
    data_gen1 = DataThread1(BATCH_SIZE)
    data_gen2 = DataThread2(BATCH_SIZE)
    data1 = BatchData(DataFromQueue(data_gen1.queue), BATCH_SIZE)
    data2 = BatchData(DataFromQueue(data_gen2.queue), BATCH_SIZE)
    M = MyModel()
    MyTrainer([QueueInput(data1), QueueInput(data2)], M).train_with_defaults(
        callbacks=[
            StartProcOrThread(data_gen1),
            StartProcOrThread(data_gen2),
            PeriodicTrigger(Evaluator(),
                every_k_epochs=1),
        ],
        steps_per_epoch=100,
        max_epoch=100,
    )
```",thanks much help paste final runnable code case anyone else want similar thing training python import import import import import summary import import import logger import import queue import class custom model two self return self none none none none return self data label data label share data data label label label label self opt return opt class custom trainer optimize several different self model list several model model super self setup input zip build graph opt self self return self return class self super self true run self true class self super self true run self true class self self none none data data data data,issue,positive,positive,positive,positive,positive,positive
356384434,"Thanks for your patch! I can confirm, that the error does not occur anymore!
  ",thanks patch confirm error occur,issue,negative,positive,positive,positive,positive,positive
356383964,"The code changes should really not be related, it is mainly just logic on which images to load and where to store the results.
What I did change, though is the config, I set
```
TEST_PRE_NMS_TOPK = 15000
TEST_POST_NMS_TOPK = 1000
FASTRCNN_NMS_THRESH = 0.8
RESULT_SCORE_THRESH = 0.0
RESULTS_PER_IM = 1000
```

I'm currently testing on images from KITTI",code really related mainly logic load store change though set currently testing,issue,negative,positive,neutral,neutral,positive,positive
356383911,"```diff
--- i/examples/FasterRCNN/eval.py
+++ w/examples/FasterRCNN/eval.py
@@ -15,7 +15,7 @@ from pycocotools.cocoeval import COCOeval
 import pycocotools.mask as cocomask
 
 from coco import COCOMeta
-from common import CustomResize
+from common import CustomResize, clip_boxes
 import config
 
 DetectionResult = namedtuple(
@@ -75,6 +75,7 @@ def detect_one_image(img, model_func):
     scale = (resized_img.shape[0] * 1.0 / img.shape[0] + resized_img.shape[1] * 1.0 / img.shape[1]) / 2
     boxes, probs, labels, *masks = model_func(resized_img)
     boxes = boxes / scale
+    boxes = clip_boxes(boxes, orig_shape)
 
     if masks:
         # has mask
```
Maybe this will fix it.",import import coco import common import common import import scale scale mask maybe fix,issue,negative,negative,negative,negative,negative,negative
356382825,Could you post your changes? I never saw this though I've trained it dozens of times.,could post never saw though trained time,issue,negative,neutral,neutral,neutral,neutral,neutral
356382353,"It indeed only rarely happened, I was processing a lot of images and it only crashed after around 200.
Please let me know, if you get further insights on this.
 
For now I implemented the following workaround (I hope it won't mess up the masks too much)
```
    # rounding errors could happen here, because masks were not originally computed for this shape.
    # but it's hard to do better, because the network does not know the ""original"" scale
    mask = (cv2.resize(mask, (w, h)) > 0.5).astype('uint8')
    ret = np.zeros(shape, dtype='uint8')
    if x1 >= ret.shape[1]:
        x1_old = x1
        x1 = ret.shape[1] - 1
        print(""warning, size mismatch (x), changing x1 from {} to {}"".format(x1_old, x1))
        x0 = x1 + 1 - w
    if y1 >= ret.shape[0]:
        y1_old = y1
        y1 = ret.shape[0] - 1
        print(""warning, size mismatch (y), chaing y1 from {} to {}"".format(y1_old, y1))
        y0 = y1 + 1 - h
    ret[y0:y1 + 1, x0:x1 + 1] = mask
```",indeed rarely lot around please let know get following hope wo mess much rounding could happen originally shape hard better network know original scale mask mask ret shape print warning size mismatch print warning size mismatch ret mask,issue,negative,positive,positive,positive,positive,positive
356381745,Oh this may be a bug (which rarely happens). Let me think about it.,oh may bug rarely let think,issue,negative,positive,positive,positive,positive,positive
356378214,The comment is unrelated. Your changes probably cause the ereor.,comment unrelated probably cause,issue,negative,neutral,neutral,neutral,neutral,neutral
356242616,"`net1/fc1_out` is the name scope of the fc layer. The tensor name is actually `net1/fc_out/output` if you print it. The documentation only vaguely mentions this. I'll add some more.

Also you have conflicting names in inputsdesc, so it's ambiguous when you do `get_predictor`. I should make it raise an error.",name scope layer tensor name actually print documentation vaguely add also conflicting ambiguous make raise error,issue,negative,negative,negative,negative,negative,negative
356214680,"I have updated the code to address the above three issues. It seems that they are all solved. However, when I tried to create separate online predictors in the trainer, some error happens. The following is my code. It seems that the tower function is having issues finding the tensor in the graph.
```python
import tensorflow as tf
from tensorpack import ModelDescBase, InputDesc, FullyConnected, \
    BatchData, DataFromQueue, StartProcOrThread, QueueInput, ModelSaver, PeriodicTrigger
from tensorpack.callbacks import Callback
# from tensorpack.train.base import Trainer
from tensorpack.train.tower import TowerTrainer
from tensorpack.tfutils import optimizer, summary
from tensorpack.tfutils.tower import TowerContext, TowerFuncWrapper, get_current_tower_context
from tensorpack.utils.argtools import memoized
from tensorpack.utils import logger
import threading
from six.moves import queue
import numpy as np


class MyModel(ModelDescBase):
    """""" A custom model which consists of two sub-networks""""""

    def _get_inputs(self):
        l = self.get_inputs_desc_list()
        res = []
        for x in l:
            res += x
        return res

    def get_inputs_desc_list(self):
        ds1 = [InputDesc(tf.float32, (None, 2), 'input'),
               InputDesc(tf.float32, (None,), 'label')]
        ds2 = [InputDesc(tf.float32, (None, 2), 'input'),
               InputDesc(tf.float32, (None,), 'label')]
        return [ds1, ds2]

    def _build_graph(self, inputs):
        data1, label1, data2, label2 = inputs
        with tf.variable_scope(""share"") as share_scope:
            y1 = FullyConnected('fc0', data1, 10, nl=tf.nn.relu)

        with tf.variable_scope('net1'):
            y1 = FullyConnected('fc_out', y1, 1, nl=tf.identity)

        with tf.variable_scope('net2'):
            with tf.variable_scope(share_scope, reuse=True):
                y2 = FullyConnected('fc0', data2, 10, nl=tf.nn.relu)
            y2 = FullyConnected('fc_out', y2, 1, nl=tf.identity)

        self.y1_loss = tf.nn.l2_loss(y1 - label1)
        self.y1_loss = tf.truediv(self.y1_loss, tf.cast(
            tf.shape(label1)[0], tf.float32), name='net1_loss')
        self.y2_loss = tf.nn.l2_loss(y2 - label2)
        self.y2_loss = tf.truediv(self.y2_loss, tf.cast(
            tf.shape(label2)[0], tf.float32), name='net2_loss')

        self.vars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \
            tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net1')
        self.vars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \
            tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net2')

        summary.add_moving_summary(self.y1_loss, collection='loss1')
        summary.add_moving_summary(self.y2_loss, collection='loss2')

    @memoized
    def get_optimizer(self):
        lr = tf.get_variable(
            'learning_rate', initializer=1e-4, trainable=False)
        opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)
        return opt


class MyTrainer(TowerTrainer):
    """"""
    A custom trainer which optimize several costs from different dataflows simutaneously
    """"""

    def __init__(self, inputs, model):
        """"""
        inputs ([InputSource]): a list of several InputSources
        model (ModelDescBase): a model
        """"""
        super(MyTrainer, self).__init__()
        inputs_desc = model.get_inputs_desc()
        inputs_desc_list = model.get_inputs_desc_list()

        # setup input callbacks for each InputSource
        for dataflow, desc in zip(inputs, inputs_desc_list):
            cbs = dataflow.setup(desc)
            self.register_callback(cbs)

        # build the graph
        self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc)
        tensors = []
        for dataflow in inputs:
            tensors += dataflow.get_input_tensors()
        with TowerContext('', is_training=True):
            self.tower_func(*tensors)

        opt = model.get_optimizer()
        with tf.name_scope('optimize'):
            self.opt_y1 = opt.minimize(
                model.y1_loss, var_list=model.vars1, name='op1')
            self.opt_y2 = opt.minimize(
                model.y2_loss, var_list=model.vars2, name='op2')

    def run_step(self):
        self.hooked_sess.run([self.opt_y1, tf.get_collection('loss1')])
        self.hooked_sess.run([self.opt_y2, tf.get_collection('loss2')])

    def get_predictor1(self):
        return self.get_predictor(['input'], ['net1/fc_out'])

    def get_predictor2(self):
        return self.get_predictor(['input'], ['net2/fc_out'])


class DataThread1(threading.Thread):
    def __init__(self, batch_size):
        super(DataThread1, self).__init__()
        self.queue = queue.Queue(maxsize=batch_size * 8 * 2)
        self.daemon = True
        self.name = 'DataThread1'

    def run(self):
        while True:
            x = np.random.rand(2)
            y = np.sum(x * x)
            self.queue.put([x, y])


class DataThread2(threading.Thread):
    def __init__(self, batch_size):
        super(DataThread2, self).__init__()
        self.queue = queue.Queue(maxsize=batch_size * 8 * 2)
        self.daemon = True
        self.name = 'DataThread2'

    def run(self):
        while True:
            x = np.random.rand(2)
            y = np.sum(x * x) * 0.001
            self.queue.put([x, y])


class Evaluator(Callback):
    def _setup_graph(self):
        self.pred_func = self.trainer.get_predictor1()

    def _trigger(self):
        x = np.array([1, 2])
        pred = self.pred_func(x[None])[0][0]
        self.trainer.monitors.put_scalar('pred', pred)

if __name__ == '__main__':
    logger.set_logger_dir('/home/sliay/models/trainer')
    BATCH_SIZE = 128
    data_gen1 = DataThread1(BATCH_SIZE)
    data_gen2 = DataThread2(BATCH_SIZE)
    data1 = BatchData(DataFromQueue(data_gen1.queue), BATCH_SIZE)
    data2 = BatchData(DataFromQueue(data_gen2.queue), BATCH_SIZE)
    M = MyModel()
    MyTrainer([QueueInput(data1), QueueInput(data2)], M).train_with_defaults(
        callbacks=[
            StartProcOrThread(data_gen1),
            StartProcOrThread(data_gen2),
            PeriodicTrigger(Evaluator(),
                every_k_epochs=1),
        ],
        steps_per_epoch=100,
        max_epoch=100,
    )

```",code address three however tried create separate trainer error following code tower function finding tensor graph python import import import import trainer import import summary import import import logger import import queue import class custom model two self return self none none none none return self data label data label share data data label label label label self opt return opt class custom trainer optimize several different self model list several model model super self setup input zip build graph opt self self return self return class self super self true run self true class self super self true run self true class self self none data data data data,issue,positive,positive,positive,positive,positive,positive
356075359,Will happen in the future. Along with that I'm thinking whether it's better to do `return cost` instead of `self.cost = cost`.,happen future along thinking whether better return cost instead cost,issue,negative,positive,positive,positive,positive,positive
356053388,"Tensorpack is a pure-python library, so any segfault is unlikely to be a tensorpack problem.
It's probably an environment problem. If you could follow the [issue template](https://github.com/ppwwyyxx/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md), and figure out which line in python does it segfault, maybe there are more clues I can tell. A stack trace doesn't provide much information.",library unlikely problem probably environment problem could follow issue template figure line python maybe tell stack trace provide much information,issue,negative,negative,negative,negative,negative,negative
355962087,"Update examples to use `build_graph` instead of `_build_graph`?
```python
def build_graph(self, image, label):
```
and 
```python
def _build_graph(self, inputs):
   image, label = inputs
```
are identical.

Currently, the internal API seems to use the nice version of `build_graph(self, image, label)` but the intended way (see examples) still seems to be `build_graph(self, inputs)`. Shouldn't it be the other way around? The `log_deprecated` is never called in `build_graph` in the examples.
  ",update use instead python self image label python self image label identical currently internal use nice version self image label intended way see still self way around never,issue,negative,positive,positive,positive,positive,positive
355925390,"Really sorry to bother you , I am not for fun to do it, My job is modify your faster r-cnn code.
 I have deep learning experience and have a vision paper in CVPR, but I used chainer framework before.
I am new to tensorpack . So sometimes I am stuck for your code and really need your help.",really sorry bother fun job modify faster code deep learning experience vision paper used chainer framework new sometimes stuck code really need help,issue,positive,positive,neutral,neutral,positive,positive
355924972,"Maybe I put a tag = ""tensorpack"" on stackoverflow . You may watch it, You can answer it or not based on your mood.",maybe put tag may watch answer based mood,issue,negative,neutral,neutral,neutral,neutral,neutral
355924810,Maybe stackoverflow is better place for these questions? Chainer group move these type of question to stackoverflow  which can answer by other developer who are willing to do as well.,maybe better place chainer group move type question answer developer willing well,issue,positive,positive,positive,positive,positive,positive
355922794,"1. you can have two threads for both data so they will not be blocked
2. correct. QueueInput will avoid this.
3. yes it will use a different collection name. then you'll need to get the ops from the collection and eval them together with the corresponding `hooked_sess.run`.

`SimplePredictBuilder` simply calls your `build_graph`. So it should be fine. In fact the existing `trainer.get_predictor` should work for your case.",two data blocked correct avoid yes use different collection name need get collection together corresponding simply fine fact work case,issue,negative,positive,positive,positive,positive,positive
355919901,"https://github.com/rbgirshick/py-faster-rcnn/blob/96dc9f1dea3087474d6da5a98879072901ee9bf9/lib/fast_rcnn/config.py#L89

Normalize the boxes.

Given the questions that're asked recently you don't seem to have enough knowledge on either tensorflow or faster rcnn to understand many of the code. But please I do not teach tensorflow or deep learning for unknown guys on github for fun. One or two questions are fine but there have been too many of these. I develop and maintain this library and hope github will be used for issues related to this library only.",normalize given recently seem enough knowledge either faster understand many code please teach deep learning unknown fun one two fine many develop maintain library hope used related library,issue,positive,positive,positive,positive,positive,positive
355915140,I also have another question in case of creating predictors in `TowerTrainer`. In my case I want to create different predictors for evaluating `y1` and `y2` separately. Do I need to create several different `tower_func` or I just need to call `SimplePredictBuilder` with different names?,also another question case case want create different separately need create several different need call different,issue,positive,neutral,neutral,neutral,neutral,neutral
355913041,"1. Thanks for your explanation. So if I have a master thread which maintains several queues, I can increase the maxsize to reduce the probability it is blocked.
2. As for the data feeding process, my current implementation will waste some data which will not be used for training. Is my understanding correct? That's not what I want. Can I avoid this by using QueueInput instead of FeedInput?
3. I did not get how to use different collection name for this. Will the following do or I also need to modify the other part?
```python
summary.add_moving_summary(self.y1_loss, collection='loss1')
summary.add_moving_summary(self.y2_loss, collection='loss2')
```",thanks explanation master thread several increase reduce probability blocked data feeding process current implementation waste data used training understanding correct want avoid instead get use different collection name following also need modify part python,issue,negative,neutral,neutral,neutral,neutral,neutral
355859366,"One big distinction between eager mode and graph mode: 

In eager mode, variable creation and the model math must be separate. Variables are created once, and the model is then executed many times. However, in graph mode they are both run only once and therefore they are usually together. A `tf.layers.dense(x)` call will create both the variables and the math operations.

As a result, in eager mode you'll almost always have to rewrite your model code anyway, into pytorch-style or Keras-style. That's not a tensorpack limitation. Eager documentation also mentions:

> Some API calls (such as the functional-style tf.layers.dense, tf.layers.conv2d) are not compatible with eager execution. Use of such methods should raise an error indicating the alternative (e.g., the tf.layers.Dense and tf.layers.Conv2D classes).

So probably this feature won't happen soon.",one big distinction eager mode graph mode eager mode variable creation model math must separate model executed many time however graph mode run therefore usually together call create math result eager mode almost always rewrite model code anyway limitation eager documentation also compatible eager execution use raise error alternative class probably feature wo happen soon,issue,positive,positive,neutral,neutral,positive,positive
355850192,"The document says: 
> When nr_proc=1, the dataflow produces the same data as ds in the same order.
When nr_proc>1, the dataflow produces the same distribution of data as ds if each sample from ds is i.i.d. (e.g. fully shuffled). You probably only want to use it for training.


""Having the same distribution"" implies that there may be duplications.

I don't know what behavior you want, but you may be interested in `MultiProcessMapData` or `MultiThreadMapData`. 
  ",document data order distribution data sample fully probably want use training distribution may know behavior want may interested,issue,positive,positive,positive,positive,positive,positive
355850004,It's the expected behavior that all process will give you randomly-sampled files from your list. Therefore they could be the same.,behavior process give list therefore could,issue,negative,neutral,neutral,neutral,neutral,neutral
355820329,"I define my get data as below.
 
```
   def get_data(self):
        self.rng.shuffle(self.filenames)
        for idx, fname in enumerate(self.filenames):
            with open(fname) as f:
                for l_idx, l in enumerate(f):
                    yield [os.getpid(), fname.split('.')[-1], l_idx] + self.extract_features_from_a_line(l)
```

And use prefetch as below 

```
    df = MDPRankDataFlow()
    #df = BatchData(df, 3)
    #cpu_cnt = cpu_count()
    cpu_cnt = 4
    df = PrefetchDataZMQ(df, cpu_cnt, 50)
    df.reset_state()
```

But I still get the same example from different process. I just want to get a example one time. ",define get data self enumerate open enumerate yield use still get example different process want get example one time,issue,negative,neutral,neutral,neutral,neutral,neutral
355810761,"Some other notes:
```
        for dataflow, desc in zip(inputs, inputs_desc_list):
            cbs = dataflow.setup(desc)
            self.register_callback(cbs)
```
Because of this, every `hooked_sess.run` will feed both data to the graph, though only part of them will not be used for training. This is because data feeding is performed through callbacks. You can use `QueueInput` instead which doesn't feed anything.

`summary.add_moving_summary(self.y1_loss, self.y2_loss)`
I mentioned earlier about summaries. This will let `hooked_sess.run` to evaluated both loss every time, though only one of them will be used for training. This function allows you to use a different collection name which can avoid this issue.

These are only performance-related, though. Shouldn't affect the correctness of your code.",zip every feed data graph though part used training data feeding use instead feed anything let loss every time though one used training function use different collection name avoid issue though affect correctness code,issue,negative,neutral,neutral,neutral,neutral,neutral
355810502,"Thanks for the code. It's very helpful to have runnable code for debugging.

`queue.put` will block when a queue reaches its max size. When you are unlucky: one queue reaches max size and the other queue is empty, the whole thread blocks. 
You can proof that the expected time it happens is (maxsize)^2. 
  ",thanks code helpful runnable code block queue size unlucky one queue size queue empty whole thread proof time,issue,negative,positive,positive,positive,positive,positive
355810209,"@sharpstill  
> `-24` only happens in the last conv of each unit.

The channels of previous units are 384

See here: (Ine 73-79): https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ShuffleNet/shufflenet.py#L73
  ",last unit previous see,issue,negative,negative,neutral,neutral,negative,negative
355809526,"You can print the static shape of tensors by something like `print(l)`.
 Inside a `sess.run` you can still print by `tf.Print`.",print static shape something like print inside still print,issue,negative,positive,positive,positive,positive,positive
355809008,"@ppwwyyxx oh really sorry, I think the tensorflow cannot print anything in the running process because the `sess.run` magic call hides everything. Maybe I am wrong, I am not familar with tf, I use chainer before.
  ",oh really sorry think print anything running process magic call everything maybe wrong use chainer,issue,negative,negative,negative,negative,negative,negative
355808508,"I wrote the following simple example where the the trainer is optimizing two sub-networks using two different dataflows. The two dataflows are generated by a separate thread. However, I found that in the training process, the program sometimes get stuck. It seems that the dataflows are blocked. Any idea on why that happens?
```python
import tensorflow as tf
from tensorpack.graph_builder.model_desc import ModelDescBase, InputDesc
from tensorpack import FullyConnected, BatchData, DataFromQueue, StartProcOrThread, FeedInput, ModelSaver
# from tensorpack.train.base import Trainer
from tensorpack.train.tower import TowerTrainer
from tensorpack.tfutils import optimizer, summary
from tensorpack.tfutils.tower import TowerContext, TowerFuncWrapper, get_current_tower_context
from tensorpack.utils.argtools import memoized
from tensorpack.utils import logger
import threading
from six.moves import queue
import numpy as np


class MyModel(ModelDescBase):
    """""" A custom model which consists of two sub-networks""""""

    def _get_inputs(self):
        l = self.get_inputs_desc_list()
        res = []
        for x in l:
            res += x
        return res

    def get_inputs_desc_list(self):
        ds1 = [InputDesc(tf.float32, (None, 2), 'input'),
               InputDesc(tf.float32, (None,), 'label')]
        ds2 = [InputDesc(tf.float32, (None, 2), 'input'),
               InputDesc(tf.float32, (None,), 'label')]
        return [ds1, ds2]

    def _build_graph(self, inputs):
        data1, label1, data2, label2 = inputs
        with tf.variable_scope(""share"") as share_scope:
            y1 = FullyConnected('fc0', data1, 10, nl=tf.nn.relu)

        with tf.variable_scope('net1'):
            y1 = FullyConnected('fc_out', y1, 1, nl=tf.identity)

        with tf.variable_scope('net2'):
            with tf.variable_scope(share_scope, reuse=True):
                y2 = FullyConnected('fc0', data2, 10, nl=tf.nn.relu)
            y2 = FullyConnected('fc_out', y2, 1, nl=tf.identity)

        self.y1_loss = tf.nn.l2_loss(y1 - label1)
        self.y1_loss = tf.truediv(self.y1_loss, tf.cast(
            tf.shape(label1)[0], tf.float32), name='net1_loss')
        self.y2_loss = tf.nn.l2_loss(y2 - label2)
        self.y2_loss = tf.truediv(self.y2_loss, tf.cast(
            tf.shape(label2)[0], tf.float32), name='net2_loss')

        self.vars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \
            tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net1')
        self.vars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \
            tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net2')

        summary.add_moving_summary(self.y1_loss, self.y2_loss)

    @memoized
    def get_optimizer(self):
        lr = tf.get_variable(
            'learning_rate', initializer=1e-4, trainable=False)
        opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)
        return opt


class MyTrainer(TowerTrainer):
    """"""
    A custom trainer which optimize several costs from different dataflows simutaneously
    """"""

    def __init__(self, inputs, model):
        """"""
        inputs ([InputSource]): a list of several InputSources
        model (ModelDescBase): a model
        """"""
        super(MyTrainer, self).__init__()
        inputs_desc = model.get_inputs_desc()
        inputs_desc_list = model.get_inputs_desc_list()

        # setup input callbacks for each InputSource
        for dataflow, desc in zip(inputs, inputs_desc_list):
            cbs = dataflow.setup(desc)
            self.register_callback(cbs)

        # build the graph
        self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc)
        tensors = []
        for dataflow in inputs:
            tensors += dataflow.get_input_tensors()
        with TowerContext('', is_training=True):
            self.tower_func(*tensors)

        opt = model.get_optimizer()
        with tf.name_scope('optimize'):
            self.opt_y1 = opt.minimize(
                model.y1_loss, var_list=model.vars1, name='op1')
            self.opt_y2 = opt.minimize(
                model.y2_loss, var_list=model.vars2, name='op2')

    def run_step(self):
        self.hooked_sess.run(self.opt_y1)
        self.hooked_sess.run(self.opt_y2)


class DataThread(threading.Thread):
    """"""
    A thread to generate data for different dataflows
    """"""

    def __init__(self, batch_size):
        super(DataThread, self).__init__()
        self.queue1 = queue.Queue(maxsize=batch_size * 8 * 2)
        self.queue2 = queue.Queue(maxsize=batch_size * 8 * 2)

        self.daemon = True
        self.name = 'DataThread'

    def run(self):
        while True:
            if np.random.rand() < 0.5:
                x = np.random.rand(2)
                y = np.sum(x * x)
                self.queue1.put([x, y])
            else:
                x = np.random.rand(2)
                y = np.sum(x * x) * 0.001
                self.queue2.put([x, y])


if __name__ == '__main__':
    # logger.set_logger_dir('/home/sliay/models/trainer')
    BATCH_SIZE = 128
    data_gen = DataThread(BATCH_SIZE)
    data1 = BatchData(DataFromQueue(data_gen.queue1), BATCH_SIZE)
    data2 = BatchData(DataFromQueue(data_gen.queue2), BATCH_SIZE)
    M = MyModel()
    MyTrainer([FeedInput(data1), FeedInput(data2)], M).train_with_defaults(
        callbacks=[StartProcOrThread(data_gen),
                   ],
        steps_per_epoch=100,
        max_epoch=100,
    )

```",wrote following simple example trainer two two different two separate thread however found training process program sometimes get stuck blocked idea python import import import import trainer import import summary import import import logger import import queue import class custom model two self return self none none none none return self data label data label share data data label label label label self opt return opt class custom trainer optimize several different self model list several model model super self setup input zip build graph opt self class thread generate data different self super self true run self true else data data data data,issue,positive,positive,positive,positive,positive,positive
355806823,I don't understand what you're saying but since the code can run and you can print the channels of every tensor I assume you can figure these out by your own.,understand saying since code run print every tensor assume figure,issue,negative,neutral,neutral,neutral,neutral,neutral
355805467,"@ppwwyyxx Really Sorry to bother you again,Why you say `384 % 16 == 0` is enough.  I notice that `
l = Conv2D('conv2', l,
               out_channel if stride == 1 else out_channel - in_channel,
               1, split=group, nl=BN)` , but in the caller `l = shufflenet_unit(l, channels[0], group, 2 if i == 0 else 1)`, thus the `channel=24 `must satisfy formular: `(out_channel - in_channel)%group == 0`? `(384-24)%8 == 0`

Am I understand correct?
  
  ",really sorry bother say enough notice stride else caller group else thus must satisfy formular group understand correct,issue,negative,negative,negative,negative,negative,negative
355804409,"Btw, after the recent updates, I found that zmq reader is slightly faster than using queues or `dataset.from_generator`.",recent found reader slightly faster,issue,negative,negative,neutral,neutral,negative,negative
355800783,"The document says "" a tower function is a callable that takes input tensors and adds one replicate of the model to the graph"". This is the definition.",document tower function callable input one replicate model graph definition,issue,negative,neutral,neutral,neutral,neutral,neutral
355798789,"@ppwwyyxx In your tutorial document , You write the conception of **tower**, What is **tower** concept, you didn't give any information in earlier section of your document? Can you give some information about it?",tutorial document write conception tower tower concept give information section document give information,issue,negative,neutral,neutral,neutral,neutral,neutral
355798720,@ppwwyyxx Thank you anyway. Your tensorpack give me a lot of help already,thank anyway give lot help already,issue,positive,neutral,neutral,neutral,neutral,neutral
355737887,It's your choice to make your models and I do not give guidance on that.,choice make give guidance,issue,negative,neutral,neutral,neutral,neutral,neutral
355733913,"Another question, If I want to use **ShuffleNet** as **Faster R-CNN** backbone base net, How to use it? Which layers can I used as RPN and RoI Pooling? Which layers can I use after RoI pooling?
  ",another question want use faster backbone base net use used roi use roi,issue,negative,negative,negative,negative,negative,negative
355723363,"Thanks a lot, I read source code, split does actually a lot of individual conv layer here.",thanks lot read source code split actually lot individual layer,issue,negative,positive,neutral,neutral,positive,positive
355721644,"In your  models.Conv2D: there is split parameter:
split (int): Split channels as used in Alexnet. Defaults to 1 (no split).
What does this `split  `mean, does this actually do **1x1 group conv** in ShuffleNet?
  ",split parameter split split used split split mean actually group,issue,negative,negative,negative,negative,negative,negative
355721021,">  the doment says output has in_channels * channel_multiplier channels

So if you set channel_multiplier to 1, output channel == input channel.",doment output set output channel input channel,issue,negative,neutral,neutral,neutral,neutral,neutral
355719837,"I just cannot get the ShuffleNet paper's  ` group convolution ` output channel == input channel? in your example, it seems not like this?  I read the paper, and did not found the useful information about how to does `group convolve` exactly?
Can you write just some information to help me please?",get paper group convolution output channel input channel example like read paper found useful information group convolve exactly write information help please,issue,positive,positive,positive,positive,positive,positive
355719135,"one question please, I see the tf document about` tf.nn.depthwise_conv2d`,
which is located in https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d
 What is filter's last  dimension `channel_multiplier`, I cannot understand why the doment says output has **in_channels * channel_multiplier** channels.
If I am not understand wrongly, the ShuffleNet paper's `group convolution ` is output channel is equals with the input channel????
Does the ShuffleNet paper's `group convolution  ` mean each input feature map's channel **will be convolved by multiple kernel channel**? and this will cause the output channel is **in_channels * channel_multiplier** ?



  ",one question please see document filter last dimension understand doment output understand wrongly paper group convolution output channel input channel paper group convolution mean input feature map channel multiple kernel channel cause output channel,issue,negative,negative,negative,negative,negative,negative
355718818,"I don't know what you mean by ""the example tutorial"". All examples are in https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ as you can see.",know mean example tutorial see,issue,negative,negative,negative,negative,negative,negative
355716607,"Thanks a lot , your example code sames also support **group conv** original from paper **ShuffleNet**(Face++),  You just show `tf.nn `support it, the trick is you just pass the filter_shape to W.
Without you , I cannot get the point and image how to implement it yet, can you put the example tutorial in Your Official Document?
  ",thanks lot example code also support group original paper show support trick pas without get point image implement yet put example tutorial official document,issue,positive,positive,positive,positive,positive,positive
355708447,"Also easy to implement by yourself (about 10 lines of code), e.g. in shufflenet:
https://github.com/ppwwyyxx/tensorpack/blob/c266152797221365d5f9b7377c99423b8c1a1aca/examples/ShuffleNet/shufflenet.py#L29-L44",also easy implement code,issue,negative,positive,positive,positive,positive,positive
355618526,"> Any chance to update the readme?

The readme is correct. Tensorpack depends on TF>=1.2; FasterRCNN depends on TF>=1.4. Both are mentioned in the corresponding readme.

@sharpstill I'll look at that in the future at #463.",chance update correct corresponding look future,issue,negative,neutral,neutral,neutral,neutral,neutral
355618016,"You can call slim or `tf.layers` in tensorpack.
As mentioned in the README, tensorpack is NOT a model wrapper.

Duplicate of #236. Closing.",call slim model wrapper duplicate,issue,negative,neutral,neutral,neutral,neutral,neutral
355585812,"tf 1.5 has already released , which fully support dynamic graph( like in PyTorch), which will ease the burden of debug breakpoint tf. Will tensorpack supply a switch argument to open dynamic graph let us to easy debug?
  ",already fully support dynamic graph like ease burden supply switch argument open dynamic graph let u easy,issue,positive,positive,positive,positive,positive,positive
355519351,"Thanks for the quick and clear answer, I am already downloading the 1.4 :)
Any chance to update the readme? 
I have just checked it and no hints, just something tf>=1.2 which is true but not very tight :)",thanks quick clear answer already chance update checked something true tight,issue,positive,positive,positive,positive,positive,positive
355512950,"After a thought I think it's due to how msgpack serializes/deserializes the data. What's you msgpack version and could you perhaps upgrade it?
  ",thought think due data version could perhaps upgrade,issue,negative,negative,negative,negative,negative,negative
355511334,There is no `b` if I print it. It may be due to some difference in python version or its json implementation.,print may due difference python version implementation,issue,negative,negative,negative,negative,negative,negative
355509087,"@ppwwyyxx 

COCO annotations are download from http://cocodataset.org/#home

print the img,
{..., b'license': 4, b'file_name': b'../data/COCO/train2014/COCO_train2014_000000158420.jpg'} 

when I modified the img['file_name'] to img[b'file_name'], the error disappear.  ",coco print error disappear,issue,negative,neutral,neutral,neutral,neutral,neutral
355505707,"Please follow the [issue template](https://github.com/ppwwyyxx/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md) to report problems.

I think you didn't use a valid COCO annotation file.",please follow issue template report think use valid coco annotation file,issue,negative,neutral,neutral,neutral,neutral,neutral
355501364,@tranorrepository I don't understand what your issue is. Could you open an issue following the [issue template](https://github.com/ppwwyyxx/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md).,understand issue could open issue following issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
355500956,"I got OOM error too. More information is below: 
fname, boxes, klass, is_crowd = img['file_name'], img['boxes'], img['class'], img['is_crowd']
KeyError: 'file_name'

P.S, python 3.4, tf 1.4 ",got error information python,issue,negative,neutral,neutral,neutral,neutral,neutral
355201440,"I can get same performance as the reference code, by using a different initializer for FC layer. Using the pytorch initializer somehow gave me worse performance starting from the first epoch and sometimes even NaN. I don't have time for now to investigate what's going on -- probably a very subtle issue. So I'll merge this one now. Thanks to @yselivonchyk and @dongzhuoyao for pointing out the bug!

Also I've removed the model code for other depth for simplicity. The same logic already exists in the resnet example so in this example it's better to just focus on mixup itself.",get performance reference code different layer somehow gave worse performance starting first epoch sometimes even nan time investigate going probably subtle issue merge one thanks pointing bug also removed model code depth simplicity logic already example example better focus,issue,negative,positive,neutral,neutral,positive,positive
355195894,"I use `IPython as IP; IP.embed()` to set break point.
Sometimes I use `tf.Print` to log.",use set break point sometimes use log,issue,negative,neutral,neutral,neutral,neutral,neutral
355193995,"for  complex code such as Faster R-CNN, You use IPython as IP; IP.embed(). to debug? or log? I am using pycharm before, and using chainer which can debug like usual python code. But this time I use your tensorpack , how to best practice of debug ?",complex code faster use log chainer like usual python code time use best practice,issue,positive,positive,positive,positive,positive,positive
355192568,"but in my knowledge, tensorflow cannot add break point and op cannot break point to debug, because sess.run magic hide the background actual flow",knowledge add break point break point magic hide background actual flow,issue,negative,positive,positive,positive,positive,positive
355189842,"tensorpack is written in python and you debug in the same way you debug python. I personally use `import IPython as IP; IP.embed() `.

Not a tensorpack issue. Closing.",written python way python personally use import issue,issue,negative,neutral,neutral,neutral,neutral,neutral
355180869,"> Slicing changes the shape. boxes = boxes / 16.0 changes the value.

The number of boxes on the image is (height/16 * width/16 * 15). This is SHAPE.
It has nothing to do with whether the coordinates (i.e. VALUES) are divided by 16 or not.",slicing shape value number image shape nothing whether divided,issue,negative,neutral,neutral,neutral,neutral,neutral
355180109,"@ppwwyyxx 
one point: 
in `fm_anchors = self._get_anchors(image)` the returned anchor box coordinates is height/16.0 and width/16.0 of original image height & width? Am I right? so decoded_boxes box coordinates is height/16.0 and width/16.0 of original image height & width??? why you say it is defined on original image?",one point image returned anchor box original image height width right box original image height width say defined original image,issue,positive,positive,positive,positive,positive,positive
355087022,"The input image can be small and may not use all the anchors. So I slice the anchors.

Shape is the number of elements in a tensor. Scale is about the value of elements in a tensor. Changing the shape doesn't change the values. Slicing changes the shape. `boxes = boxes / 16.0` changes the value.",input image small may use slice shape number tensor scale value tensor shape change slicing shape value,issue,positive,negative,negative,negative,negative,negative
355020166,"I have wechat, and email:sharpstill@163.com, If you can speak chinese , we can contact with each other more convenient , I am using your tensorpack to finish my PhD thesis, So it is very important for me",speak contact convenient finish thesis important,issue,negative,positive,positive,positive,positive,positive
355019925,"@ppwwyyxx  I am confused, in `get_all_anchors`, yes, I found it return anchor defined on **the original size** of image, but in https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/train.py#L86 , You only use `tf.shape(image)[0] // config.ANCHOR_STRIDE ` to slice the returned anchor ,why ?
and what is difference between shape and scale, for example, shape=[H/16, W/16] means the image size is 1/16 of original image(H,W)??
  ",confused yes found return anchor defined original size image use image slice returned anchor difference shape scale example image size original image,issue,positive,positive,positive,positive,positive,positive
354976953,You can print the return value of `get_all_anchors` to confirm what it is.,print return value confirm,issue,negative,neutral,neutral,neutral,neutral,neutral
354976274,"@ppwwyyxx 
about 1: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/train.py#L86  and https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/data.py#L50 prove that the fm_anchor is defined on 1/16.0 smaller scale feature map.
Am I correct or I am missing something?",prove defined smaller scale feature map correct missing something,issue,negative,negative,neutral,neutral,negative,negative
354975322,"1. `fm_anchors` is defined on original image
2. The comment is talking about the shape of `box_logits`, and has nothing to do with its scale.",defined original image comment talking shape nothing scale,issue,negative,positive,positive,positive,positive,positive
354974495,"@ppwwyyxx two proof:
1.  because fm_anchors is define on 1/16 feature map,
2. and in
https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/model.py#L43
 ,your comment also says the box_logits is defined on 1/16 feature map.
I think the `decoded_boxes `  is defined on 1/16 feature map ",two proof define feature map comment also defined feature map think defined feature map,issue,negative,neutral,neutral,neutral,neutral,neutral
354973614,"> the decoded_boxes is defined on the feature_map which is 1/16.0 scale smaller than original image, 

No. It's defined on the original image.",defined scale smaller original image defined original image,issue,positive,positive,positive,positive,positive,positive
354942077,"Tensorflow can only save variables. It cannot save a tensor which is computed from another variable. 

To save binary weights you'll need to create a variable to each of the weights, and assign them the value of `fw(actual weights)` every epoch before saving the actual weights. Since it is not useful at all to save binary weights during training, I recommend you just do the post-processing after training.",save save tensor another variable save binary need create variable assign value actual every epoch saving actual since useful save binary training recommend training,issue,positive,positive,neutral,neutral,positive,positive
354941279,haven't any ways or idea to directly save binary weights? Thanks,way idea directly save binary thanks,issue,positive,positive,positive,positive,positive,positive
354929900,Run `fw` manually on the saved actual weights.,run manually saved actual,issue,negative,neutral,neutral,neutral,neutral,neutral
354903467,"They were terminated but not joined, so they became zombie process. The above commit should fix it.",zombie process commit fix,issue,negative,neutral,neutral,neutral,neutral,neutral
354761823,"Thanks for your anwser. The `ds.__del__()` method doesn't close processes when run from command line. I used this code:

```
import tensorpack as tp
ds=tp.DataFromList(range(10))
ds=tp.PrefetchDataZMQ(ds, 6)
tp.TestDataSpeed(ds).start_test()
raw_input(""Enter to clean"")
ds.__del__()
raw_input(""Continue"")  # processes are still there 
exit()
```

Although, if run in debug mode with a breakpoint on the 'Continue' line, the processes are gone (and marked as TERMINATED in ds._procs).

For the naming conflict, I meant my file for the first code I submitted in this issue was named zmq.py. Maybe I'm wrong, but I don't think it should mix with another import inside a module.",thanks method close run command line used code import range enter clean continue still exit although run mode line gone marked naming conflict meant file first code issue maybe wrong think mix another import inside module,issue,negative,positive,neutral,neutral,positive,positive
354730512,Played with this a little bit yesterday. This experimental interface is too Keras-like and therefore cannot be made efficient to the best of my knowledge. Both of the two best-known scaling techniques (replicated and parameter_server mode in https://github.com/tensorflow/benchmarks) on single-machine require the use of variable scope. Therefore I cannot let users create their Keras models directly under a root variable scope -- it can never reach the best performance. Models have to be created by tensorpack like the way the old [mnist-keras example](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/mnist-keras.py) is doing. I'll make the change on this but the rest of the interface can probably still be Keras-like.,little bit yesterday experimental interface therefore made efficient best knowledge two scaling replicated mode require use variable scope therefore let create directly root variable scope never reach best performance like way old example make change rest interface probably still,issue,positive,positive,positive,positive,positive,positive
354628285,Tried the above code but found it cannot reach the paper's number either.. So maybe we can live with a slightly lower performance.,tried code found reach paper number either maybe live slightly lower performance,issue,negative,negative,neutral,neutral,negative,negative
354623482,"To manually close processes you can call `ds.__del__()` when ds is `PrefetchData` or `PrefetchDataZMQ`.

Name conflicting is a general Python issue and you should just avoid putting your own code into PYTHONPATH.",manually close call name conflicting general python issue avoid code,issue,negative,positive,neutral,neutral,positive,positive
354614024,"After further testing, I agree it works as it should: processes are closed after correctly exiting the main process with exit(). It doesn't work with the way pycharm closes the session though, so that is not a tensorpack issue.

So I have a feature request regarding this behaviour: _add a method to manually close processes created by PrefetchData and PrefetchDataZMQ_. It would be really useful during development for those who don't restart the interpreter all the time.

PS: I had an unexpected behaviour: having a file called zmq.py in my path messed up loading tensorpack module. I can live with that, if you want more info, I'll open another issue.
",testing agree work closed correctly main process exit work way session though issue feature request regarding behaviour method manually close would really useful development restart interpreter time unexpected behaviour file path loading module live want open another issue,issue,positive,positive,positive,positive,positive,positive
354596663,"Differences include:
1. how mixup is implemented (mix within 1 batch or between 2 batches)
2. initialization
3. input normalization",include mix within batch input normalization,issue,negative,neutral,neutral,neutral,neutral,neutral
354546092,"the code is run in an IPython console (in PyCharm), I'm left with 7 python2.7 processes : the original one and the 6 forks. And 6 forks remains after exiting the main process. I tried in a virtualenv and without.",code run console left python original one remains main process tried without,issue,negative,positive,positive,positive,positive,positive
354527007,Interesting. The code can exit correctly on my machine.,interesting code exit correctly machine,issue,negative,positive,positive,positive,positive,positive
354524552,"Same here. The following code reproduces the error under Ubuntu 16.04, python 2.7, with tensorpack updated from pip today :
```
import tensorpack as tp
ds=tp.DataFromList(range(10))
ds=tp.PrefetchDataZMQ(ds, 6)
tp.TestDataSpeed(ds).start_test()
```
",following code error python pip today import range,issue,negative,neutral,neutral,neutral,neutral,neutral
354425455,"`FullyConnected` is merely a symbolic function with `get_variable` inside. To share variables, call `FullyConnected` the second time in a previously-used variable scope with `reuse=True`.",merely symbolic function inside share call second time variable scope,issue,negative,negative,negative,negative,negative,negative
354424490,"I have some questions for sharing layers with different data batches. Let's say my model looks like follows:
```python
class MyModel(ModelDescBase):
    """""" A custom model which consists of two sub-networks on two different tasks with sharing layer""""""

    def _get_inputs(self):
    	return [InputDesc(tf.float32, (None, 2), 'input'),
                InputDesc(tf.float32, (None,), 'label'),
                InputDesc(tf.float32, (None, 2), 'input'),
                InputDesc(tf.float32, (None,), 'label'),] 
    
    def _build_graph(self, inputs):
        data1, label1, data2, label2 = inputs
        # sharing layer definition
        
    def _get_optimizer(self):
        lr = tf.get_variable(
            'learning_rate', initializer=1e-4, trainable=False)
        opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)
        return opt
```
It takes two different dataflows. So in the build_graph function, the received inputs will be two different data batch. I want to define the graph so that the two networks share common conv or fc layers. It seems that the provide FullyConnected or Conv2D does not support this. How should I achieve this in an elegant way? Does the following code with mixed primitive tf variables work?
```python
def _build_graph(self, inputs):
    data1, label1, data2, label2 = inputs
    # sharing layer definition
    sharing_w = tf.get_variable('w', shape=[2, 10], dtype=tf.float32)

    y1 = tf.nn.relu(tf.matmul(data1, sharing_w))
    y1 = FullyConnected('net1', y1, 1, nl=tf.identity)

    y2 = tf.nn.relu(tf.matmul(data2, sharing_w))
    y2 = FullyConnected('net2', y2, 1, nl=tf.identity)

    y1_loss = tf.nn.l2_loss(y1 - label1, name='net1_loss')
    y2_loss = tf.nn.l2_loss(y2 - label2, name='net2_loss')
```",different data let say model like python class custom model two two different layer self return none none none none self data label data label layer definition self opt return opt two different function received two different data batch want define graph two share common provide support achieve elegant way following code mixed primitive work python self data label data label layer definition data data label label,issue,positive,positive,neutral,neutral,positive,positive
354227608,"It's best to first figure out what makes your script crash. Virtual memory, as the name suggests, is virtual. There is no reason for slurm to limit your virtual memory usage. And using TB of virtual memory is exactly how lmdb works.

Slurm probably limits your resident memory use and it may overestimate it by adding resident memory of each process although they are actually shared. Using a smaller number of processes can probably make it happy. ",best first figure script crash virtual memory name virtual reason limit virtual memory usage virtual memory exactly work probably resident memory use may overestimate resident memory process although actually smaller number probably make happy,issue,positive,positive,positive,positive,positive,positive
354185446,"I'm using the `alexnet-dorefa.py` training script with data in LMDB format and seeing up to 4.099TB of virtual memory usage causing things to crash approx 30% of the way into Epoch 1 on a 4 GPU cluster with 24 cores and 125GB CPU memory. I adapted `get_imagenet_dataflow.py` from `imagenet_utils.py` according to http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html#sequential-read like so:
```
def get_imagenet_dataflow(
        datadir, name, batch_size,
        augmentors):
    """"""
    See explanations in the tutorial:
    http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html
    """"""
    assert name in ['train', 'val', 'test']
    assert datadir is not None
    assert isinstance(augmentors, list)
    isTrain = name == 'train'
    cpu = min(30, multiprocessing.cpu_count())
    if isTrain:
        ds = LMDBData(os.path.join(datadir, 'ILSVRC12-train.lmdb'), shuffle=False)
        ds = LocallyShuffleData(ds, 50000)
        ds = PrefetchData(ds, 5000, 1)
        ds = LMDBDataPoint(ds)
        ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
        ds = AugmentImageComponent(ds, augmentors, copy=False)
        ds = PrefetchDataZMQ(ds, cpu)
        ds = BatchData(ds, batch_size, remainder=False)
    else:
        ds = LMDBData(os.path.join(datadir, 'ILSVRC12-val.lmdb'), shuffle=False)
        aug = imgaug.AugmentorList(augmentors)
        ds = LMDBDataPoint(ds)
        ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
        ds = AugmentImageComponent(ds, augmentors, copy=False)
        ds = BatchData(ds, batch_size, remainder=True)
        ds = PrefetchDataZMQ(ds, 1)
    return ds
```
I have tried reducing `buffer_size` in LocallyShuffleData() but that doesn't seem to have an effect on the virtual memory allocation. Full output is attached if you have any suggestions as to how I can address this issue.
[cdr344-3799739.txt](https://github.com/ppwwyyxx/tensorpack/files/1589965/cdr344-3799739.txt)
",training script data format seeing virtual memory usage causing crash way epoch cluster memory according like name see tutorial assert name assert none assert list name min lambda else lambda return tried reducing seem effect virtual memory allocation full output attached address issue,issue,negative,positive,positive,positive,positive,positive
354147514,"Did the following in https://gist.github.com/ppwwyyxx/6da15f2d9087373635254d4e6fbe4891:
Used the paper's normalization
Fixed VGG scale
Used same learning rate for G and D
Used SeparateGANTrainer

And now I can always obtain a model better than baseline (in all the 3 times I've tried).",following used paper normalization fixed scale used learning rate used always obtain model better time tried,issue,negative,positive,positive,positive,positive,positive
354066106,"A little clarification:

- The PreResnet have two different implementations.   the official one is [https://github.com/facebook/fb.resnet.torch/blob/master/models/preresnet.lua](https://github.com/facebook/fb.resnet.torch/blob/master/models/preresnet.lua), and the unofficial one: [https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py](https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py).  what the tensorpack implemented is the official PreResnet. however, the mixup paper just choosed the unofficial one.
- the unofficial one is ""bn-relu first and then  diverge"".
- our mixup is just an example for tensorpack, and followed the mixup authors' implementation(an unofficial preresnet).  **so I think other examples in tensorpack based on official preresnet had better keep unchanged.**
- There is one mistake that @yselivonchyk corrected for me:

> ResNet-18 with preactivation as by https://github.com/kuangliu/pytorch-cifar is using ResNet with preactivation block with 2 consecutive convolution layers in the block. Existing implementation was using 3 convolutional blocks.

- more depths choice for mixup implementations are welcome.  but just keep it only suitable for this mixup example(should not influence other examples. because I think mixup example is a special case).",little clarification two different official one unofficial one official however paper unofficial one unofficial one first diverge example implementation unofficial think based official better keep unchanged one mistake corrected block consecutive convolution block implementation convolutional choice welcome keep suitable example influence think example special case,issue,negative,positive,positive,positive,positive,positive
354000379,"@ppwwyyxx @dongzhuoyao 
Regarding your comments about design of the residual block. I ran the model by @kuangliy and figured out next correct way of applying BnRelu on residual branch: apply BnRelu only before convolution in identity branch. Differences: my existing implementation always applies BnRelu.

The reference code expressing the ides from https://github.com/kuangliu/pytorch-cifar:
```
    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        ...
```
For example, for PreActResNet18 BnRelu before residual branch would be applied exactly 3 times: for the first block of the second, third and forth module (whenever filter depth changes).

PreActResNet18_Cifar correct schema:
input---c------id--------id----br-----c-------id-----br-----c-------id-----br-----c-------id-----mp-fc10-sm
................\brcbrc/ \brcbrc/....\cbrc/..\brcbrc/.....\cbrc/..\brcbrc/.....\cbrc/  \brcbrc/ 

I will add this changes to the pull request later.

It might also beg some changes in other Cirar+ResNet implementations as mentioned by @dongzhuoyao  but I am not that familiar with the code there to justify the changes.
",regarding design residual block ran model figured next correct way residual branch apply convolution identity branch implementation always reference code ides forward self self else example residual branch would applied exactly time first block second third forth module whenever filter depth correct schema input id id add pull request later might also beg familiar code justify,issue,negative,positive,positive,positive,positive,positive
353989086,Moving BnRelu into the residual branch (the correct way) leads to diverging training. figuring out why,moving residual branch correct way diverging training,issue,negative,neutral,neutral,neutral,neutral,neutral
353973227,"<img width=""423"" alt=""wx20171226-221107 2x"" src=""https://user-images.githubusercontent.com/13537104/34358332-c99b8986-ea89-11e7-907e-09cfe7297ccb.png"">

according to [https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py](https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py), the above implementation in tensorpack: line 26 and line 27 should be exchanged.  @ppwwyyxx 
",according implementation line line,issue,negative,neutral,neutral,neutral,neutral,neutral
353953474,"If I'm not mistaken, the two block functions were equivalent to the ones in `resnet_model.py`  (preresnet_basicblock, preresnet_bottleneck). Better to import them than rewrite them if that is the case.",mistaken two block equivalent better import rewrite case,issue,negative,positive,positive,positive,positive,positive
353861451,"thanks! problem solved!

But i still think that since reset_state can be called only once, it's better if it's called automatically when constructed ^_^",thanks problem still think since better automatically,issue,negative,positive,positive,positive,positive,positive
353835104,"I set 'TEST_POST_NMS_TOPK' option in config.py to 600, it worked in this way, thx",set option worked way,issue,negative,neutral,neutral,neutral,neutral,neutral
353693570,"The option was kept unchanged because I found it still helps my vision models. But now you should be able to set it by `trainer.COLOCATE_GRADIENTS_WITH_OPS=False`, before building the graph.",option kept unchanged found still vision able set building graph,issue,negative,positive,positive,positive,positive,positive
353683823,"Maybe `colocate_gradients_with_ops` is not a good option. I saw that `tensorflow/benchmarks` didn't set this option either, so I was considering removing it. ",maybe good option saw set option either considering removing,issue,negative,positive,positive,positive,positive,positive
353575468,"https://github.com/ppwwyyxx/tensorpack/blob/a3cc3a18c492ac16e6d678e1f26c076c3ae5f70f/tensorpack/train/tower.py#L185
After setting colocate_gradients_with_ops = False, It ran faster as it was.
It's this configuration which make embedding layer poor performance.
Any advice on this problem?
I think whether to open this configuration should depend on whether  an embedding layer was inside graph or not?",setting false ran faster configuration make layer poor performance advice problem think whether open configuration depend whether layer inside graph,issue,negative,negative,negative,negative,negative,negative
353564301,"That was fast.
Thank you!

By the way, any recommendation or guide on how to use tfdbg or other techniques to 'trace' value of gradients flow? I use gradient_map_override, modified gradients, and want to make sure things go correctly.

Currently I am porting code from[ bit-rnn ](https://github.com/hqythu/bit-rnn)(quantized RNN, He et al)to tensorpack",fast thank way recommendation guide use value flow use want make sure go correctly currently code al,issue,positive,positive,positive,positive,positive,positive
353554253,The code was only written for two layers. Now it should work for one layer as well.,code written two work one layer well,issue,negative,neutral,neutral,neutral,neutral,neutral
353527698,"Make sure no other processes (e.g. Xorg) are running on GPUs.
Try different configuration, e.g. one mentioned in another issue: https://github.com/ppwwyyxx/tensorpack/issues/467#issuecomment-341592143
As a last resort, change `config.py` to produce fewer predictions. There is a comment about it. ",make sure running try different configuration one another issue last resort change produce comment,issue,negative,positive,positive,positive,positive,positive
353504656,Thanks for your quick reply. I will try to implement this and update this issue for reference when it works.,thanks quick reply try implement update issue reference work,issue,negative,positive,positive,positive,positive,positive
353502860,"Also be careful with certain callbacks which evaluate tensors every step. 
For instance, all moving averages summary ops are by default in the same collection (added by the `add_moving_summary` function) and will be run every step (by the `MovingAverageSummary` callback), regardless of which cost you're optimizing. These ops probably depend on your input (e.g., when you're summarizing the output of some layer),  so you may want to disable this callback so that you won't evaluate `inputs2` when optimizing `cost1`.",also careful certain evaluate every step instance moving summary default collection added function run every step regardless cost probably depend input output layer may want disable wo evaluate cost,issue,negative,positive,neutral,neutral,positive,positive
353501957,You can have `InputSource` i1 and i2. Call  `tensors1 = i1.get_input_tensors()` and `tensors2 = i2.get_input_tensors()` respectively to get tensors from the two input source. Then build your graph out of those tensors. ,call respectively get two input source build graph,issue,negative,neutral,neutral,neutral,neutral,neutral
353500720,"I see. I now understand I have to write my own trainer by inheriting the Trainer class or TowerTrainer class. The [GAN tutorial](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/GAN/GAN.py) is a good example to start with.

There is one thing more. For my case, there are several dataflows instead of one dataflow in the GAN example. Let's say we have two different dataflows d1 and d2. When d1 is fed, it is used to train the sub-network. When d2 is fed, it is used to train the master network. The current example trainer does not show how to get the different dataflows and the corresponding optimizing loss in a train loop.

Could you please provide me some hints on this?",see understand write trainer trainer class class gan tutorial good example start one thing case several instead one gan example let say two different fed used train fed used train master network current example trainer show get different corresponding loss train loop could please provide,issue,negative,positive,positive,positive,positive,positive
353443045,Thanks for pointing out! Could you try again?,thanks pointing could try,issue,negative,positive,positive,positive,positive,positive
353355145,"@ppwwyyxx 
![image](https://user-images.githubusercontent.com/25046619/34258412-d202a252-e698-11e7-86f2-f8bd31ba0b84.png)
It doesn't work,  error information like this:
![Uploading image.png…]()

",image work error information like,issue,negative,neutral,neutral,neutral,neutral,neutral
353261547,"All built-in trainers are for single-cost optimization task. You need to write your own trainer if you need to alternate between different costs. The tutorial has more details: http://tensorpack.readthedocs.io/en/latest/tutorial/extend/trainer.html ; http://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html

You can also re-formulate your problem to a single-cost optimization one, e.g. by using `tf.cond`. But this may create some overhead.",optimization task need write trainer need alternate different tutorial also problem optimization one may create overhead,issue,positive,neutral,neutral,neutral,neutral,neutral
353255002,"When you ""specify output node name"" that is not for only one node. You model will be used more than one time and multiple nodes will be created. One may be named ""tower-pred-0/output"", one may be named ""tower1/output"", etc, depending on the name scope under which the tensor is created. This is how tensorflow works.",specify output node name one node model used one time multiple one may one may depending name scope tensor work,issue,negative,neutral,neutral,neutral,neutral,neutral
352989680,Is there a reproducible code? Could you provide your environment info?,reproducible code could provide environment,issue,negative,neutral,neutral,neutral,neutral,neutral
352948072,"Ah.. now I understand what the table means, The cases ""initialized"" are start from initialized weight with pretrained 32bit model and the others are initialized with randomized values!!! ",ah understand table start weight bit model,issue,negative,neutral,neutral,neutral,neutral,neutral
352947186,Initialized means initializing with weights from pretrained 32bit model. Otherwise they're randomly initialized.,bit model otherwise randomly,issue,negative,negative,negative,negative,negative,negative
352941456,"Ah.. you mean that the case ""initialized"" is that they initialized weights with a 32bit and the other case is just randomly chosen by W bits. Is it right? ",ah mean case bit case randomly chosen right,issue,negative,negative,negative,negative,negative,negative
352940774,You can only initialize the weights when training a neural network.,initialize training neural network,issue,negative,neutral,neutral,neutral,neutral,neutral
352939865,"Oh Thanks for reference. 

I had some minor questions in the experiment.

In the ImageNet experiment, there are case which is initialized. In the paper, it said that ""training has been initiailized with a 32-bit model.""

As I understand, the case ""initialized"" means that even though they are using 1-bit weight, activation and 32-bit gradients, they initialized all the 3 values(weight, activation, gradients) in 32-bit, different from other case which is initialized in lower than 32 bit. Am I in right track? 

If not, can I get some advice?",oh thanks reference minor experiment experiment case paper said training model understand case even though weight activation weight activation different case lower bit right track get advice,issue,negative,positive,positive,positive,positive,positive
352937081,"They are implemented and not open source.
btw recently caffe2 open source some relevant low-precision kernels: https://github.com/caffe2/caffe2/tree/master/caffe2/mobile/contrib/ulp2.",open source recently open source relevant,issue,negative,positive,positive,positive,positive,positive
352928568,That's better. You are right. I just was looking for red warnings.,better right looking red,issue,negative,positive,positive,positive,positive,positive
352924829,"While reading the paper, I have something curious. 

In the contribution, paper said 
""DoReFa-Net can use bit convolution kernels to accelerate both the forward pass and the backward pass of the training process.""

In this sentence, what is bit convolution kernel? Is it implemented by authors or just using exist thing?
I thought that it is implemented by authors, but I want to make sure and if it was implemented then is it open source? ",reading paper something curious contribution paper said use bit convolution accelerate forward pas backward pas training process sentence bit convolution kernel exist thing thought want make sure open source,issue,positive,positive,positive,positive,positive,positive
352832606,"Switched to another syntax following https://github.com/pypa/setuptools/issues/1087, so it works with older version of setuptools.",switched another syntax following work older version,issue,negative,positive,neutral,neutral,positive,positive
352812948,You probably need to upgrade setuptools and pip.,probably need upgrade pip,issue,negative,neutral,neutral,neutral,neutral,neutral
352667719,"Oh I didn't see the code yet. Thanks a lot. 

For 2., I should see the paper more in detail.",oh see code yet thanks lot see paper detail,issue,negative,positive,positive,positive,positive,positive
352665508,"Thanks a lot...!!! I got the all equation.

Sorry but I had little questions about N(k), 

Noise function N(k).. In the paper, 
 1. Is there any guarantee that dr/(2 * max_0(|dr|)) + 1/2 + N(k) < 1? (Becuase quantization should be in [0,1])

2. Can you give me some explanation about specific affect of noise function(N(k)) in gradient quantization? (For example, the performance becomes better because of some aspects... of noise function..)",thanks lot got equation sorry little noise function paper guarantee quantization give explanation specific affect noise function gradient quantization example performance becomes better noise function,issue,positive,positive,neutral,neutral,positive,positive
352663397,Closing because information about the problem was not provided. Reopen if you still have questions. ,information problem provided reopen still,issue,negative,neutral,neutral,neutral,neutral,neutral
352662473,">  Is there any special reason? 

To keep things in the same range.

> Also I can't get the intention of multiplying max_0(|dr|) in the front. Can you explain little bit in detail?

That's part of the inverse function.",special reason keep range also ca get intention multiplying front explain little bit detail part inverse function,issue,negative,positive,neutral,neutral,positive,positive
352662254,"To see the problem with crop_and_resize, try cropping the top-left 4x4 square from a 5x5 square. You may have to try a couple of time to succeed, because it just doesn't behave as what you may expect.

Closing as this is a tensorflow issue.",see problem try square square may try couple time succeed behave may expect issue,issue,negative,neutral,neutral,neutral,neutral,neutral
352662159,"Oh, I get it.. However why do we have to invert the transform? Is there any special reason? Also I can't get the intention of multiplying max_0(|dr|) in the front. Can you explain little bit in detail?",oh get however invert transform special reason also ca get intention multiplying front explain little bit detail,issue,negative,positive,neutral,neutral,positive,positive
352659249,"I have some questions in getting gradient quantization. In the paper, it said that 
""The above function first applies an affine transform on the gradient, to map it into [0, 1], and then inverts the transform after quantization.""

In this sentence, what is ""inverts"" mean?

I can't get the points.. Can I get some advice?",getting gradient quantization paper said function first affine transform gradient map transform quantization sentence mean ca get get advice,issue,negative,negative,neutral,neutral,negative,negative
352303609,That's not how I define an epoch but different people have different definitions. Please note that 1281167 is a prime. ,define epoch different people different please note prime,issue,negative,neutral,neutral,neutral,neutral,neutral
352303385,"Resnet needs to run 1281167 picture ( picture numbers of imagenet train set ) in one epoch ,right? If  TOTAL_BATCH_SIZE *steps_per_epoch != 1281167 , this will be contradiction with definition of epoch? 
 
> epoch: one forward pass and one backward pass of all the training examples",need run picture picture train set one epoch right contradiction definition epoch epoch one forward pas one backward pas training,issue,negative,positive,positive,positive,positive,positive
352303029,It doesn't support this feature. The code is only 5 lines so you can easily extend it or write a new one: http://tensorpack.readthedocs.io/en/latest/_modules/tensorpack/tfutils/gradproc.html#GlobalNormClip,support feature code easily extend write new one,issue,positive,positive,positive,positive,positive,positive
352289265,"> I am using the Ubuntu Windows store app and have installed opencv and pyglet, yet I get the error message.

I assume it means this? https://www.microsoft.com/en-us/store/p/ubuntu/9nblggh4msv6",store yet get error message assume,issue,negative,neutral,neutral,neutral,neutral,neutral
352281461,"If you keep `TOTAL_BATCH_SIZE *steps_per_epoch` a constant, this means one epoch go through the same amount of data. ",keep constant one epoch go amount data,issue,negative,neutral,neutral,neutral,neutral,neutral
352231809,"That script is perhaps not very reliable. IIRC it assumes input tensors for inference are placeholders, but that's not always true in a graph saved by tensorpack because placeholders are slow. It works for `InferenceRunner(some_dataflow)`, though.",script perhaps reliable input inference always true graph saved slow work though,issue,positive,positive,neutral,neutral,positive,positive
352230565,"I found a solution by using
 [tensorflow/tensorflow/python/tools/freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) to get a pure frozen inference graph after training a model from tenorpack (or any other trainining method)

Here is my usage 

```python
with tf.Session() as sess:
    model_path = tf.train.latest_checkpoint(CHECKPOINT_DIR)
    # get metagraphDef which contains graphDef, clear devices placement in training stage
    tf.train.import_meta_graph(os.path.join(CHECKPOINT_DIR,'graph-1214-001754.meta'),clear_devices=True)
    # write GraphDef to a GraphDef protocol buffer file which suitable for freeze_graph.py input 
    tf.train.write_graph(graph_or_graph_def=tf.get_default_graph().as_graph_def(),logdir='train_log',name='original.pbtxt',as_text=True)
```

Then use freeze.graph tool in shell command

```
python tensorflow/tensorflow/python/tools/freeze_graph.py \ 
--input_graph /home/yaren/densenet-tensorflow/train_log/original.pbtxt \ 
--input_checkpoint /home/yaren/densenet-tensorflow/train_log/cifar10-single-fisrt150-second225-max3001214-001743/model-234300 \
--output_node_names ""InferenceTower/in_top_k/InTopKV2"" \
--output_graph /home/yaren/densenet-tensorflow/train_log/frozen.pb 
```

In the above shell command, the `output_node_names` is very critical, the script based on it to decide which part of Graph is retained. 

There is a related issue #386

I haven't tested the frozen graph for inference  If I found some new information I would continue to reply in this issue",found solution get pure frozen inference graph training model method usage python sess get clear placement training stage write protocol buffer file suitable input use tool shell command python shell command critical script based decide part graph related issue tested frozen graph inference found new information would continue reply issue,issue,negative,positive,positive,positive,positive,positive
352109078,The graph has ops that are unsupported on GPUs. In training `allow_soft_placement=True` was enabled already so nothing happened.,graph unsupported training already nothing,issue,negative,neutral,neutral,neutral,neutral,neutral
352008745,"I have solved this issue if I use `allow_soft_placement=True` , 

```python
with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:

restorer.restore(sess=sess,save_path='/home/yaren/densenet-tensorflow/train_log/cifar10-single-fisrt150-second225-max3001214-001743/model-234300')
```

And I just find a solution and still can't figure out why ......",issue use python sess find solution still ca figure,issue,negative,neutral,neutral,neutral,neutral,neutral
351907052,"Thank you very much, I will just try to use `model._build_graph` ",thank much try use,issue,negative,positive,positive,positive,positive,positive
351863122,"It works.Thank you so much for correct my mistakes, and apologize for such a issue",much correct apologize issue,issue,negative,positive,positive,positive,positive,positive
351861737,"You were not writing a correct dataflow. A datapoint has to be a __list__ of python objects. It can be a list of 1 element.
See http://tensorpack.readthedocs.io/en/latest/tutorial/dataflow.html#what-is-dataflow
and http://tensorpack.readthedocs.io/en/latest/tutorial/extend/dataflow.html",writing correct python list element see,issue,negative,neutral,neutral,neutral,neutral,neutral
351860998,"I have changed `get_data` to yield one by one, it seems `BatchData()` still reproduce above 2nd mistake

```python
from tensorpack import *
import numpy as np

shape = (5,8,8,1)

class Data(RNGDataFlow):
    def size(self):
        return 10
    def get_data(self):
        for i in range(10):
            yield np.ones(shape)
```
```python
ds = Data()
ds.reset_state()
for i in ds.get_data():
    print(i.shape)
```
it will output `(5,8,8,1)` 10 times.

```python
ds = Data()
ds = BatchData(ds, 8, use_list = False)
ds.reset_state()
for i in ds.get_data():
    print(type(i))
    print(len(i))
    print(*[j.shape for j in i])
```

output
```python
<class 'list'>
5
(8, 8, 8, 1) (8, 8, 8, 1) (8, 8, 8, 1) (8, 8, 8, 1) (8, 8, 8, 1)
```",yield one one still reproduce mistake python import import shape class data size self return self range yield shape python data print output time python data false print type print print output python class,issue,negative,negative,negative,negative,negative,negative
351857351,"You were not writing a dataflow. You need to yield your datapoints one by one.
See docs: http://tensorpack.readthedocs.io/en/latest/tutorial/extend/dataflow.html",writing need yield one one see,issue,negative,neutral,neutral,neutral,neutral,neutral
351783213,"There is no way to automatically ""guess"" which part of graph is useful for inference.
You can just write (copy) pure symbolic code to construct the graph again.
Also see https://github.com/ppwwyyxx/tensorpack/issues/513#issuecomment-348093710
and tutorial http://tensorpack.readthedocs.io/en/latest/tutorial/inference.html#inference-after-training",way automatically guess part graph useful inference write copy pure symbolic code construct graph also see tutorial,issue,negative,positive,positive,positive,positive,positive
351782507,Unrelated to `tf.summary.image`. But you do need to read the [doc](http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.Monitors.put_image) before calling an API.,unrelated need read doc calling,issue,negative,neutral,neutral,neutral,neutral,neutral
351715652,"Resolved due to scale issue in tf.summary.image 
https://www.tensorflow.org/api_docs/python/tf/summary/image ",resolved due scale issue,issue,negative,negative,negative,negative,negative,negative
351517044,"I don't see much value in bringing in TFGAN. From what I can see the training utilities will only make the current code more complicated. The symbolic functions aren't very interesting either, and if you need them for a new type of GAN you can just import and call them.",see much value see training make current code complicated symbolic interesting either need new type gan import call,issue,positive,positive,neutral,neutral,positive,positive
351116271,"https://github.com/ppwwyyxx/tensorpack/blob/3b994877d4502539d83f101597de2277a1b00919/examples/FasterRCNN/config.py#L18-L19

Also, to report a bug, please follow the issue template.",also report bug please follow issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
351115912,It tries to clear the queue. It does not hurt but I'll see if I can get rid of this annoying message.,clear queue hurt see get rid annoying message,issue,negative,negative,negative,negative,negative,negative
350934578,"Should be fixed now. It's always very hard to run tensorflow on multiple processes. The code worked before but is broken now probably because I'm using a newer tensorflow. 

I'll just disable multiprocessing for now, since it shouldn't affect speed for fasterrcnn.",fixed always hard run multiple code worked broken probably disable since affect speed,issue,negative,negative,negative,negative,negative,negative
350932556,Can I check out an old tensorpack to get the training started?,check old get training,issue,negative,positive,neutral,neutral,positive,positive
350931052,Oh looks like latest master is actually broken. It's probably related to a recent commit a couple days ago.. I'll check that.,oh like latest master actually broken probably related recent commit couple day ago check,issue,negative,positive,neutral,neutral,positive,positive
350929729,"How did you install tensorpack? There was an old version which may have this problem.
If it's installed with pip, try: `pip install -U git+https://github.com/ppwwyyxx/tensorpack.git`.",install old version may problem pip try pip install,issue,negative,positive,neutral,neutral,positive,positive
350502010,Closing because keeping the order of data often isn't useful.,keeping order data often useful,issue,negative,positive,positive,positive,positive,positive
350501643,"Keep steps_per_epoch constant solved my problem. 

change the code
from
`steps_per_epoch = dataset_train.size()`
to
`steps_per_epoch = dataset_train.size()//nr_tower`
",keep constant problem change code,issue,negative,neutral,neutral,neutral,neutral,neutral
350498819,"Closing because necessary information about the problem wasn't provided. 
It's very likely that upgrading TF will just solve this issue.",necessary information problem provided likely solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
350496482,"Maybe you can give me your e-mail, and i can send you my code . the code is less than 300 lines",maybe give send code code le,issue,negative,neutral,neutral,neutral,neutral,neutral
350496164,"Again, if these are two independent runs, you cannot expect them to have the same results anyway. There could be many other reasons in your code that cause differences. 

At least I wanted to make sure you're using batch size & steps correctly. On existing examples such as cifar10, when batch size & steps are changed accordingly with different #GPUs the performance will be roughly the same.

If you want this to be a bug report you'd better provide more information such as code and instructions and logs. ""the problem is still there"" isn't helpful at all in solving any problems.
",two independent expect anyway could many code cause least make sure batch size correctly batch size accordingly different performance roughly want bug report better provide information code problem still helpful,issue,positive,positive,positive,positive,positive,positive
350495711,"I change the code as following:
from 
`steps_per_epoch = dataset_train.size()`
to
`steps_per_epoch = dataset_train.size()//nr_tower`

and pass it to funciton:

```
return TrainConfig(
        dataflow=dataset_train,
        callbacks=callbacks,
        model=Model(args.k, args.num_block, args.layers_per_block, args.path, args.increasing_rate, args.layer_increasing_rate),
        steps_per_epoch=steps_per_epoch,
        max_epoch=args.max_epoch,
        nr_tower=nr_tower,
    )
```
the step is kept the same, but the problem is still there :(",change code following pas return step kept problem still,issue,negative,neutral,neutral,neutral,neutral,neutral
350494764,"Keep batch_size (i.e. batch_size_per_gpu) * #GPU * steps_per_epoch unchanged.
Simplest way is just to keep steps_per_epoch a constant.",keep unchanged way keep constant,issue,negative,neutral,neutral,neutral,neutral,neutral
350494657,So How should I fix the #steps_per_epoch? should I pass 64(as before) as the second param for the BatchData function?,fix pas second param function,issue,negative,neutral,neutral,neutral,neutral,neutral
350494318,"Since batch size is halfed and #GPU is doubled already, steps_per_epoch should be fixed, rather than doubled. Maybe you've missed the doc [here](http://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html#multigpu-trainers).

When you don't have enough devices it will still run. TensorFlow will try to put the graph on existing devices.",since batch size doubled already fixed rather doubled maybe doc enough still run try put graph,issue,negative,positive,neutral,neutral,positive,positive
350493946,"I do use it in the get_data() function, codes are as followings:
```
    ds = AugmentImageComponent(ds, augmentors)
    nr_tower = args.gpu.split(',')
    BATCH_SIZE = 64
    BATCH_SIZE  /= len(nr_tower)
    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)
    if isTrain:
        ds = PrefetchData(ds, 3, 2)
    return ds

```
The iterations (For one epoch) change from 781 to 1562 for training, change from 157 to 313 for testing(two gpus versus one gpu).

And I find one strange thing, 
```
callbacks.append(DataParallelInferenceRunner(
                dataset_test, [ScalarStats('cost'), ClassificationError()], [0,1,2]))
```
stills work, but I only have two gpus on my computer",use function return one epoch change training change testing two versus one find one strange thing work two computer,issue,negative,negative,neutral,neutral,negative,negative
350492656,"My best guess is you did something wrong with batch size. But again, there is not enough code to tell. You set `batch_size  /= len(nr_tower)` but I don't see where it is used.",best guess something wrong batch size enough code tell set see used,issue,negative,positive,positive,positive,positive,positive
350492241,The validation(test) error are exactly the same !   I do not think this is right!,validation test error exactly think right,issue,negative,positive,positive,positive,positive,positive
350491372,"Because by default they use the same name for logging.

Both `ScalarStats` and `ClassificationError` has options to use a different name. See docs: http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.ScalarStats",default use name logging use different name see,issue,negative,neutral,neutral,neutral,neutral,neutral
350491086,"When i enable both InferenceRunner and DataParallelInferenceRunner(by pushing both to callbacks), there is only one output. Maybe I do it wrong?  ",enable pushing one output maybe wrong,issue,negative,negative,negative,negative,negative,negative
350490188,"Is it possible ""Pre-filling staging area"" affect the result?

 In the line 205 of Inference_runnner.py, it says that ""hooks from StagingInput will force the consumption of nr_tower datapoints in every run.""",possible staging area affect result line force consumption every run,issue,negative,neutral,neutral,neutral,neutral,neutral
350479809,"I prefer to believe you did something wrong but there is not enough code to tell. And to be honest, there could be many other reasons specific to your task that can cause performance differences.

If you think there is something wrong with `DataParallelInferenceRunner`, the way to test it is to enable both InferenceRunner together and see if they gives consistent results. That's how I tested its correctness.",prefer believe something wrong enough code tell honest could many specific task cause performance think something wrong way test enable together see consistent tested correctness,issue,negative,positive,neutral,neutral,positive,positive
350479406,"firstly, i do half the batch size from 64 to 32 correctly when using two gpus.  

The results are of  the FIRST epoch under two conditions(one gpu & two gpus). When I ran the code twice on one gpu, then the results of The FIRST EPOCH are almost the same. 

All the other codes are the same under one gpu versus two gpus",firstly half batch size correctly two first epoch two one two ran code twice one first epoch almost one versus two,issue,negative,positive,positive,positive,positive,positive
350478161,"If this is two independent runs, you cannot expect them to have the same results in any case.

Another thing to note is, if you didn't half the batch size correctly you may have trained the dataset twice so you got better results with two gpus.

But there isn't enough information given. You're not using any examples and there is not enough code to tell what happened to you.",two independent expect case another thing note half batch size correctly may trained twice got better two enough information given enough code tell,issue,negative,positive,neutral,neutral,positive,positive
350429916,"You will NOT see anything like this if you're using the ResNet example in this repo and only changed the batch size. Please confirm what you did, including the changes you made and command you run.",see anything like example batch size please confirm made command run,issue,positive,neutral,neutral,neutral,neutral,neutral
350429744,"The log is like this:
[1208 22:46:33 @base.py:164] Start Epoch 45 …
24%|##3       |6316/26690[1:30:25<4:54:41, 1.15it/s]

> On Dec 9, 2017, at 00:18, Yuxin Wu <notifications@github.com> wrote:
> 
> Please always paste your observation instead of describing it. I do not understand what you observed.
> 
> As far as I know you will not see anything like 4908/26690 if changing TOTAL_BATCH_SIZE is the only thing you did.
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub <https://github.com/ppwwyyxx/tensorpack/issues/544#issuecomment-350429653>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFNYNK9kD-e2V9HzjN_81-9xH4yC1npSks5s-jQtgaJpZM4Q76Pw>.
> 

",log like start epoch wrote please always paste observation instead understand far know see anything like thing thread reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
350429653,"Please always __paste__ your observation instead of describing it. I do not understand what you observed.

As far as I know you will not see anything like `4908/26690` if changing TOTAL_BATCH_SIZE is the only thing you did.",please always observation instead understand far know see anything like thing,issue,positive,positive,neutral,neutral,positive,positive
350429202,"When I set the TOTAL_BATCH_SIZE = 192, the progress bar will show something like 4908/26690. What does that mean? Is this the progress of iteration?

It seems to make sense that 26690 = 1281167/48 (the size of training is 1281167). 

> On Dec 8, 2017, at 23:54, Yuxin Wu <notifications@github.com> wrote:
> 
> Yes.
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub <https://github.com/ppwwyyxx/tensorpack/issues/544#issuecomment-350428750>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFNYNNq9bEYK4kN-oCP9Xw-4RaQZxwwdks5s-i6NgaJpZM4Q76Pw>.
> 

",set progress bar show something like mean progress iteration make sense size training wrote yes thread reply directly view mute thread,issue,positive,negative,negative,negative,negative,negative
350428733,"Do you mean in one epoch, 4 GPUs will go through the dataset one time instead of 4 times? 

> On Dec 8, 2017, at 23:19, Yuxin Wu <notifications@github.com> wrote:
> 
> TOTAL_BATCH_SIZE is always 256 no matter how many GPUs you use. So one epoch is always one epoch. The number of GPUs only affects how many samples per GPU.
> 
> The original resnet paper means the same thing. It may differ a little bit (i.e. 5004 vs 5000) but that's negligible.
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub <https://github.com/ppwwyyxx/tensorpack/issues/544#issuecomment-350427331>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFNYNMBeBa6zSg7k2S7IlTdl3yw6bIyBks5s-iZlgaJpZM4Q76Pw>.
> 

",mean one epoch go one time instead time wrote always matter many use one epoch always one epoch number many per original paper thing may differ little bit negligible thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
350427331,"TOTAL_BATCH_SIZE is always 256 no matter how many GPUs you use. So one epoch is always one epoch. The number of GPUs only affects the number of samples per GPU.

The original resnet paper are the same in terms of the definition of iterations and epochs. It may differ a little bit (i.e. 5004 vs 5000) but that's negligible.",always matter many use one epoch always one epoch number number per original paper definition may differ little bit negligible,issue,negative,positive,positive,positive,positive,positive
350177038,"If you could just make a mathematical (rather than conceptual) definition of ROIAlign, it'll be easy to do it correctly then. And it'll be easy to see why some coordinate transform is necessary. It's hard to explain it by words. The simplest answer is, their formula is different, and you can only see that if you derive the formula.

The definition would be something like: given an NxN matrix and a bbox (x0,y0,x1,y1) following my definition of bounding box in the notes, get an roi of size kxk, write down the formula for pixel (i, j) in the roi. ",could make mathematical rather conceptual definition easy correctly easy see transform necessary hard explain answer formula different see derive formula definition would something like given matrix following definition bounding box get roi size write formula roi,issue,positive,positive,neutral,neutral,positive,positive
349904711,"vgg19 should be applied to images of range [0,255]-mean. But just found that here it is on images of [0,1]-mean/255. (check the [initial commit](https://github.com/ppwwyyxx/tensorpack/pull/527/commits/b10effc0bff7882d8ec18af231f7974247e6d70b) )So the entire additional_losses would probably do nothing good.",applied range found check initial commit entire would probably nothing good,issue,negative,positive,positive,positive,positive,positive
349872059,"The results are clearly better, though the training is not that stable.",clearly better though training stable,issue,positive,positive,positive,positive,positive,positive
349789706,This is a an issue from a [cub primitive](https://nvlabs.github.io/cub/) usually used in CUDA kernels and it is very likely to be caused by a TF issue itself.,issue cub primitive usually used likely issue,issue,negative,negative,negative,negative,negative,negative
349704684,"There is no recent activity so I'm closing it. In general for any performance problems please do some investigation as in http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html because I'm unable to run your code with your hardware.
Feel free to reopen if you still have questions.",recent activity general performance please investigation unable run code hardware feel free reopen still,issue,positive,negative,neutral,neutral,negative,negative
349702937,"There is no recent activity so I'm closing it. In general for any performance problems please do some investigation as in http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html because I'm unable to run your code.
Feel free to reopen if you still have questions.",recent activity general performance please investigation unable run code feel free reopen still,issue,positive,negative,neutral,neutral,negative,negative
349702286,No indication that this issue is a tensorpack problem. Closing now. Reopen if you have more information to provide.,indication issue problem reopen information provide,issue,negative,neutral,neutral,neutral,neutral,neutral
349701602,"And FYI for most tensorflow models I've seen, the result can be the same if you __correctly__ rewrite the model with a different layout.",seen result rewrite model different layout,issue,negative,neutral,neutral,neutral,neutral,neutral
349689601,"comparison

![colorize](https://user-images.githubusercontent.com/6756603/33671737-bd836790-daa8-11e7-81f7-c7db7109ef32.jpg)
![color2](https://user-images.githubusercontent.com/6756603/33671740-bfb14884-daa8-11e7-9564-f3acd6a9b8d8.png)

I think, that this model is equally good at the Place Dataset, but does not generalize as good as the authors model. Seems to be reasonable, it is their teaser image which usually shows the best results. Further, there is some resizing going on in their inference code and they filtered out some images from the Place-Dataset, without describing details. And of course, I did not train the model for 3 weeks.

Any suggestions?
",comparison colorize color think model equally good place generalize good model reasonable teaser image usually best going inference code without course train model,issue,positive,positive,positive,positive,positive,positive
349683644,Pull the latest version of TensorPack and the error seems to be gone.,pull latest version error gone,issue,negative,positive,positive,positive,positive,positive
349654965,"Why would you have `checkpoint model-806817`? Are we using the same dataset?:
```
100%|#######################|19714/19714[59:02<00:00, 5.56it/s]          
[1206 04:49:54 @base.py:255] Epoch 40 (global_step 788560) finished, time:3542.78 sec.                                                             
[1206 04:49:55 @saver.py:82] Model saved to train_log/enet-pat/model-788560.                                                                       
[1206 04:49:55 @monitor.py:363] GAN_loss/discrim/accuracy: 0.99414       
[1206 04:49:55 @monitor.py:363] GAN_loss/discrim/loss: 0.019299          
[1206 04:49:55 @monitor.py:363] GAN_loss/gen/accuracy: 0.0026996         
[1206 04:49:55 @monitor.py:363] GAN_loss/gen/loss: 8.9464
[1206 04:49:55 @monitor.py:363] QueueInput/queue_size: 49.585
[1206 04:49:55 @monitor.py:363] add: 19.204
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LA: 8.9464
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LP1: 1.3098
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LP2: 0.0015531
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT1: 2.3112e-37
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT2: 2.2718e-37
[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT3: 2.3154e-37
[1206 04:49:55 @base.py:245] Start Epoch 41 ...
100%|#######################|19714/19714[59:03<00:00, 5.56it/s]
[1206 05:48:58 @base.py:255] Epoch 41 (global_step 808274) finished, time:3543.57 sec.
```",would epoch finished time sec model saved add start epoch epoch finished time sec,issue,negative,negative,neutral,neutral,negative,negative
349548956,"hi,@ppwwyyxx  I have the same question,could you give me any advice? 
I read the docs http://tensorpack.readthedocs.io/en/latest/modules/train.html?highlight=DistributedTrainerReplicated#tensorpack.train.DistributedTrainerReplicated
and  I modified last lines of tensorpack/examples/ResNet/cifar10-resnet.py

```
166    config = TrainConfig(
167         model=Model(n=NUM_UNITS),
168         dataflow=dataset_train,
169         callbacks=  [
170             ModelSaver(),
171             InferenceRunner(dataset_test,
172                             [ScalarStats('cost'), ClassificationError('wrong_vector')]),
173              ScheduledHyperParamSetter('learning_rate',
174                                       [(1, 0.1), (82, 0.01), (123, 0.001), (300, 0.0002)]),
175        ],
176         max_epoch=164,
177         session_init=SaverRestore(args.load) if args.load else None,
178 
179     )
180     config.session_config=None
181     print(config.data,config.model)
182     nr_gpu = max(get_nr_gpu(), 1)
183 
184     hosts = ['gpu2', 'gpu3']
185     cluster_spec = tf.train.ClusterSpec({
186             'ps': [h + ':2222' for h in hosts],
187             'worker': [h + ':2223' for h in hosts]
188          })
189 
190     server = tf.train.Server(
191                 cluster_spec, job_name=args.job, task_index=args.task,
192                     config = get_default_sess_config()  )
193 
194     #DistributedTrainerReplicated(config, server).train()
195 
196     #launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_gpu))
197     #launch_train_with_config(config, SyncMultiGPUTrainer(nr_gpu))
198     launch_train_with_config(config, DistributedTrainerReplicated(nr_gpu, server))
```

and I launch it like this:
`(tf-py27) [chongyang@gpu2 ResNet]$ python cifar10-resnet.py --job worker --task 0`

and part of the error message
```
[1206 14:16:28 @training.py:90] Building graph for training tower 1 on device /job:worker/task:0/gpu:1...
2017-12-06 14:16:28.115375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:06:00.0, compute capability: 6.0)
2017-12-06 14:16:28.115407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)
[1206 14:16:47 @trainers.py:193] WRN For efficiency, local MODEL_VARIABLES are only synced to PS once every epoch. Be careful if you save the model more frequently than this.
Traceback (most recent call last):
  File ""cifar10-resnet.py"", line 198, in <module>
    launch_train_with_config(config, DistributedTrainerReplicated(nr_gpu, server))
  File ""/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/interface.py"", line 88, in launch_train_with_config
    model._build_graph_get_cost, model.get_optimizer)
  File ""/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 171, in wrapper
    return func(*args, **kwargs)
  File ""/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/tower.py"", line 148, in setup_graph
    self.register_callback(input_callbacks + train_callbacks)
  File ""/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/base.py"", line 146, in _register_callback
    self._register_callback(x)
  File ""/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/base.py"", line 149, in _register_callback
    assert not isinstance(self._callbacks, Callbacks), \
AttributeError: 'DistributedTrainerReplicated' object has no attribute '_callbacks'
```


",hi question could give advice read last else none print server server server launch like python job worker task part error message building graph training tower device device device name bus id compute capability device device name bus id compute capability efficiency local every epoch careful save model frequently recent call last file line module server file line file line wrapper return file line file line file line assert object attribute,issue,positive,negative,neutral,neutral,negative,negative
349482383,"If I write a graph with data_format "" NHWC"", but freeze with the checkpoint trained by ""NCHW"". Is it still right? or I have to retrain the checkpoint with data_format ""NHWC""?",write graph freeze trained still right retrain,issue,negative,positive,positive,positive,positive,positive
349005089,"""enet-pat"" (patch-, adversarial, & texture-loss) is from the paper, see Figure 3 in the paper. I trained them with two GPUs (hence batch size 12) until global step 806817. I use the checkpoint model-806817, because I felt the results were really good at this point already. Discriminator had accuracy around 95%, though.",paper see figure paper trained two hence batch size global step use felt really good point already discriminator accuracy around though,issue,negative,positive,positive,positive,positive,positive
348850799,"confirmed it works fine by simply removing it, by default tensorpack has the flag as false, thx for the info.",confirmed work fine simply removing default flag false,issue,negative,positive,positive,positive,positive,positive
348693117,The visualization is pure LaTeX + Tikz. Saccade is only an image viewer to flip between images easily or the other stuff from the readme.,visualization pure latex saccade image viewer flip easily stuff,issue,negative,positive,positive,positive,positive,positive
348690590,"TensorFlow does very strange things when resizing images  with/without `align_corners`. For the sake of consistency I would like to keep the current version. 
Feel free to retrain the model :wink:",strange sake consistency would like keep current version feel free retrain model wink,issue,positive,positive,positive,positive,positive,positive
348688939,"Do you think you need `align_corners=True` for `tf.image.resize_*`? 
To be honest I don't like `tf.image.resize_` because they never align pixel to pixel regardless of `align_corners`. But maybe True makes more sense here?

You can check this out:
```python
import numpy as np
from skimage.transform import rescale

np.set_printoptions(linewidth=10000)

arr = np.array(
    [[1,2,3,4],
    [5,6,7,8],
    [9,10,11,12]], dtype='float32')

input = tf.constant(arr)
input4D = tf.reshape(input, [1, 3, 4, 1])
resize = tf.image.resize_bilinear(input4D, [6, 8], align_corners=True)[0,:,:,0]
sess = tf.Session()
r1 = sess.run(resize)
r2 = rescale(arr/100.0, 2, mode='edge') * 100

print(r1)
print(r2)
```",think need honest like never align regardless maybe true sense check python import import input input resize sess resize print print,issue,positive,positive,positive,positive,positive,positive
348666075,"After remove that line, training aborted with an error complaining either vertical or horiz flip needs to be on.",remove line training aborted error either vertical flip need,issue,negative,neutral,neutral,neutral,neutral,neutral
348665715,"> Any plan to add NHWC support in tensorpack? Most of frameworks support test/inference on CPU.

tensorpack supports NHWC because it doesn't care what your model is at all. But you'll always need to implement NHWC version of fasterrcnn which I don't plan to do.

You can just remove the line `imgaug.Flip(horiz=True)`",plan add support support care model always need implement version plan remove line,issue,positive,neutral,neutral,neutral,neutral,neutral
348664038,"After reverted NHWC changes, it now starting normal training. So the error was from NHWC changes. thx.

Any plan to add NHWC support in tensorpack? Most of frameworks support test/inference on CPU.

Another question is on data augmentation. Some dataset needs to turn off flip (in data.py: ""imgaug.Flip(horiz=True)""). However, after turn-off flip, there's an error from tensorpack that one of vertical or horiz needs to be on). It will be good for tensorpack to remove this restriction.

Will close the issue shortly. ",starting normal training error plan add support support another question data augmentation need turn flip however flip error one vertical need good remove restriction close issue shortly,issue,negative,positive,positive,positive,positive,positive
348663643,Presumably it should be able to train any Keras model. Although there might be unknown issues because I haven't tried anything other than mnist yet.,presumably able train model although might unknown tried anything yet,issue,negative,positive,positive,positive,positive,positive
348662561,"Yeah, I simply did global replacement and commented out transpose in train.py. Let me revert back to clean repository to see if the issue disappears. Thx for quick comment.",yeah simply global replacement transpose let revert back clean repository see issue quick comment,issue,positive,positive,positive,positive,positive,positive
348662476,"Oh.. if you made changes you should tell.
The code is written for NCHW. To work for NHWC you need to do more than global string replace.
I'm sure the change is the cause of your issue. Please debug that.",oh made tell code written work need global string replace sure change cause issue please,issue,positive,positive,positive,positive,positive,positive
348662372,"Forgot to updated tensorpack, just updated (install update) to latest tensorpack 0.8 (from 0.7.1), still got the same error. Also I made change to NHWC (so it could run inference on CPU-only). Will do a fresh repository pull to see whether it's fine.",forgot install update latest still got error also made change could run inference fresh repository pull see whether fine,issue,negative,positive,positive,positive,positive,positive
348661840,"Things work fine on my side. By ""pulled latest code"" do you mean you update __both__ tensorpack and fasterrcnn? From the log they are located in different directories.

Older examples will run with new tensorpack - that's back-compatibility. But newer examples don't necessarily run with old tensorpack.",work fine side latest code mean update log different older run new necessarily run old,issue,negative,positive,positive,positive,positive,positive
348655884,Interesting! Can networks like densenet and resnet can be constructed? These rely on the functional API.,interesting like rely functional,issue,positive,positive,positive,positive,positive,positive
348623253,"okay, my bad. but the start epoch is read from `stat.json` rather than deriving from `global_step` and `step_num`
",bad start epoch read rather,issue,negative,negative,negative,negative,negative,negative
348617381,Please ignore it now... something wrong from my side....,please ignore something wrong side,issue,negative,negative,negative,negative,negative,negative
348600079,"Is there any tensorpack code involved in your code?..
I don't know what you mean by ""result not right"". I don't even know what your code is trying to do. From the information you provided it seems like model restoring itself is working fine.",code involved code know mean result right even know code trying information provided like model working fine,issue,positive,positive,positive,positive,positive,positive
348580374,"I just change the batch_size form 1 to 128, everything is ok. but I don‘t know why...",change form everything know,issue,negative,neutral,neutral,neutral,neutral,neutral
348577221,Closing due to lack of activity. Feel free to reopen and include details if you still encounter error.,due lack activity feel free reopen include still encounter error,issue,negative,positive,positive,positive,positive,positive
348576217,"Not a tensorpack issue. As the model is just in standard checkpoint format and you can definitely load it.

There could be millions of reasons why the ""result is not right"". You have to confirm it's because of model restoring. I would `sess.run()` a weight tensor after restoring, and see if it equals the one saved in the checkpoint.",issue model standard format definitely load could million result right confirm model would weight tensor see one saved,issue,positive,positive,neutral,neutral,positive,positive
348561656,"I think this pull request is ready to be reviewed. My trained model is here:
http://files.patwie.com/models/enet-pat.npy
Feel free to add suggestions or edit when required.

Please consider, I am not that patient to train it more epochs and cherry pick the best model.

Comparison can be done by
```bash
CHPT=/external/patwie/CHECKPOINTS/enet-pat.npy
for i in $( ls enhancenet_pretrained/input); do
    python enet-pat.py --apply --load ${CHPT} --lowres ""enhancenet_pretrained/input/$i"" --output ""enhancenet_pretrained/output/$i"" --gpu 1
done
```",think pull request ready trained model feel free add edit please consider patient train cherry pick best model comparison done bash python apply load output done,issue,positive,positive,positive,positive,positive,positive
348542638,"Results are available and comparable with the authors implementations (there is no clear winner)
[results.tar.gz](https://github.com/ppwwyyxx/tensorpack/files/1522218/results.tar.gz)

- *.pngprediction.jpg: this implementation
- *-EnhanceNet.jpg: official weights

I compressed them to JPGs for GitHub",available comparable clear winner implementation official compressed,issue,positive,positive,positive,positive,positive,positive
348486967,"Interestingly enough `from Image import PIL` gives significantly better performance

```
python 25.3957339631 db
matlab 25.0888434195 db
pil    26.8381044874 db
```


Arrrgh, starting training from scratch again both pull-requests. But I still just hope, this makes no difference ...",interestingly enough image import significantly better performance python starting training scratch still hope difference,issue,positive,positive,positive,positive,positive,positive
348430992,"Please tell me __what you did__ as mentioned in the [issue template](https://github.com/ppwwyyxx/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md).
Also for such problems your environment info is relevant.",please tell issue template also environment relevant,issue,negative,positive,positive,positive,positive,positive
348429102,Look at your log directory. That's where your models are. Breakout-v0.npy is my released pretrained model.,look log directory model,issue,negative,neutral,neutral,neutral,neutral,neutral
348393699,"Yes, ZMQ can works well with windows and mac.

but the latency between machine is the biggest issues as far as I can tell
",yes work well mac latency machine biggest far tell,issue,positive,positive,neutral,neutral,positive,positive
348392760,No. It's expected to be used to send data for training.,used send data training,issue,negative,neutral,neutral,neutral,neutral,neutral
348392387,"ZMQ is good for  message, Do you want to do reactive version of tensorpack?",good message want reactive version,issue,negative,positive,positive,positive,positive,positive
348278422,"Still, it has nothing to do with tensorpack.
From tensorflow source code contrib/lite/toco/import_tensorflow.cc it seems like it has something to with convolution data_format.",still nothing source code like something convolution,issue,negative,neutral,neutral,neutral,neutral,neutral
348272392,"```
2017-11-30 00:22:17.240715: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: ReorderAxes
Aborted (core dumped)
```

I used the tensorpack's LinearWrap to generate the graph and combined the checkpoint, but I got this error by  toco tool. I did not find this operator ""ReorderAxes"" in the graphdef ops. What is this  ""ReorderAxes"" operator? Is it related with LinearWrap?

Thanks!",unsupported operator aborted core used generate graph combined got error toco tool find operator operator related thanks,issue,negative,positive,neutral,neutral,positive,positive
348271818, Just to inform you that there are still many unfixed errors in the function `visualize` in `train.py`.,inform still many unfixed function visualize,issue,negative,positive,positive,positive,positive,positive
348240573,Ah. I'll correct the name. These visualization code wasn't maintained for a while.,ah correct name visualization code,issue,negative,neutral,neutral,neutral,neutral,neutral
348196009,"3 days ago, opencv supports disable this warning. https://github.com/opencv/opencv/pull/10155",day ago disable warning,issue,negative,neutral,neutral,neutral,neutral,neutral
348182796,There is no way I would know why your model does not work for your data. There is no indication that this is a tensorpack-related problem. Closing now.,way would know model work data indication problem,issue,negative,neutral,neutral,neutral,neutral,neutral
348172517,"I might be easy to further convert the released model by the authors into a python dictionary, which can be restored by `DictRestore`. I haven't tried this yet and just used the author's code to compare to this implementation.",might easy convert model python dictionary tried yet used author code compare implementation,issue,negative,positive,positive,positive,positive,positive
348162781,"I want to change the label shape to (128,), when i load data by tensorpack, how can I change the data shape manually？",want change label shape load data change data shape,issue,negative,neutral,neutral,neutral,neutral,neutral
348125781,"I do not have the time to test the entire dataset (it took enough time to install Matlab R2016a), but for the first example I took:
[comparison.tar.gz](https://github.com/ppwwyyxx/tensorpack/files/1517114/comparison.tar.gz)
python is slightly better:

```
python 25.3957339631 db
matlab 25.0888434195 db
```
I would guess there is no clear winner.

But I agree
![compare](https://user-images.githubusercontent.com/6756603/33422470-16ca89ac-d5b6-11e7-9628-71bf28453fc7.jpg)

the Matlab results (left) looks much better (although it has a lower PSNR) than the Python+OpenCV2 result (right).

may the PSNR be with you ... :-)",time test entire took enough time install ra first example took python slightly better python would guess clear winner agree compare left much better although lower result right may,issue,positive,positive,positive,positive,positive,positive
348119968,"```
opencv:
import numpy as np
import cv2
import sys
im = cv2.imread(sys.argv[1])
im = im.astype('float32')
imy = cv2.cvtColor(im, cv2.COLOR_BGR2YCR_CB)
print imy.shape
imy = imy[:,:,0]
down = cv2.resize(imy, (0,0), 0, 0.5, 0.5, cv2.INTER_CUBIC)
print down.shape
up = cv2.resize(down, (0,0), 0, 2, 2, cv2.INTER_CUBIC)
v = np.square(up - imy).mean()
print v
print 20 * np.log10(255 / np.sqrt(v))


matlab:
function bicubic
im = imread('barbara.bmp');
im = rgb2ycbcr(im);
imy = im(:,:,1);
up_scale=2;
down = imresize(imy, 0.5, 'bicubic');
up = imresize(down, 2, 'bicubic');
diff = up - imy;
l2 = mean(mean(diff.*diff))
20 * log10(255/sqrt(l2))
end
```
This was the test code I was using (two years ago when I tried to reproduce SR-CNN). It only compares the y-channel. I remember on Set14 opencv is much worse than matlab.
Maybe on 3-channel images it's different.

After fixing the bicubic issue we were able to reproduce SRCNN, and that's also when I stop working on it cause I felt the dark side of SR.",import import import print print print print function mean mean log end test code two ago tried reproduce remember set much worse maybe different fixing issue able reproduce also stop working cause felt dark side,issue,negative,negative,negative,negative,negative,negative
348118306,"Sure you can. As long as you can build the graph and define what to do in an iteration, you can train it with tensorpack.

There are no documents about GAN training. GAN is just an example of how to define a trainer for non-standard tasks. GAN is not part of the library. The documents about defining a trainer are http://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html and http://tensorpack.readthedocs.io/en/latest/tutorial/extend/trainer.html.",sure long build graph define iteration train gan training gan example define trainer gan part library trainer,issue,negative,positive,positive,positive,positive,positive
348111829,"So the solution is: 
1. generate simple inference graph by pure tensorflow ( or tensorpack symbolic)
2. attach the checkpoint file to freeze the graph
3. convert the freeze graph to tflite.

I will try it.
Thanks for your help! 

 ",solution generate simple inference graph pure symbolic attach file freeze graph convert freeze graph try thanks help,issue,positive,positive,positive,positive,positive,positive
348111443,"The error is telling you already. Your label shape is (128, 1), but (128,) is expected.
Because before batching, your label is a vector of length 1, not a scalar.
` np.array([int(temp_str[1])])` apparently this line of code creates a vector of length 1.",error telling already label shape label vector length scalar apparently line code vector length,issue,negative,positive,neutral,neutral,positive,positive
348111012,"Sure you can always convert between the two.
Just feel writing `tensorA, tensorB, tensorC = inputs` all the time is redundant.",sure always convert two feel writing time redundant,issue,negative,positive,positive,positive,positive,positive
348109558,"Given a List, we can anyway use `func(*my_list)`. So there is no huge difference.",given list anyway use huge difference,issue,negative,positive,positive,positive,positive,positive
348093710,"To build the graph for inference you do not need to use tensorpack. Just build like normal tensorflow. All you need to do is create placeholders and call your symbolic functions on them.

To avoid writing all the symbolic functions again you can use `model.build_graph(placeholder1, placeholder2)` but honestly it's equivalent to writing them with pure tensorflow.
You can leave `_get_inputs` the way it was. It has nothing to do with whether the graph uses placeholders or not.",build graph inference need use build like normal need create call symbolic avoid writing symbolic use honestly equivalent writing pure leave way nothing whether graph,issue,positive,positive,positive,positive,positive,positive
348090518,"I think current calling convention for predictors are positional-args, which means you don't need to use list. You should be able to call with `pred(tensorA)` or `pred(tensorA, tensorB)`. Please tell me if this fails. The idea for `build_graph` is similar because I feel this is more intuitive, then `pred([tensorA])` and `pred([tensorA, tensorB])`. But maybe I'm wrong.",think current calling convention need use list able call please tell idea similar feel intuitive maybe wrong,issue,negative,neutral,neutral,neutral,neutral,neutral
348089686,"Be careful for these changes, e.g. the Offline-predictor now requires a single tensor as an input or a list. I found this a little bit unintuitive as I now have some cases, where I want to evaluate the perceptual loss only sometimes. So sometimes I feed in two images (and need a list) and sometimes only one image (without a list). This makes writing the code a little bit cumbersome. 

Are there any similar side effects here.",careful single tensor input list found little bit unintuitive want evaluate perceptual loss sometimes sometimes feed two need list sometimes one image without list writing code little bit cumbersome similar side effect,issue,negative,negative,negative,negative,negative,negative
348089683,"```python
 def _get_inputs(self):
        return [InputDesc(self.image_dtype, [None, self.image_shape, self.image_shape, 3], 'input'),
                InputDesc(tf.int32, [None], 'label')]
```

The ""input"" in the resnet model is very similar as other example models,  How to modify it with ""placeholders""?  

",python self return none none input model similar example modify,issue,negative,neutral,neutral,neutral,neutral,neutral
348088522,"Most of these papers kind of cheat by blurring the highres images before bi-cubic downscaling the image for a low-res input. If so, the gap between bi-cubic and network visually _seems_ to be larger than here. This Gaussian blur usually helps the network as the information from the highres is partly available in the lowres.
 Further, the public benchmarks I found, use gray-scale images. 

I observed the opposite (matlab vs. opencv2 which agrees to https://stackoverflow.com/a/22093004). But I guess, this again depends on the specific input :-) I was a little disappointed seeing these public benchmarks only contain very few images....

I don’t have Matlab here anymore.",kind cheat image input gap network visually blur usually network information partly available public found use opposite guess specific input little disappointed seeing public contain,issue,negative,negative,neutral,neutral,negative,negative
348083707,"Sorry my bad. They are identical. 
I was doing
`wget https://github.com/openai/atari-py/blob/master/atari_py/atari_roms/ms_pacman.bin` instead of `wget https://github.com/openai/atari-py/raw/master/atari_py/atari_roms/ms_pacman.bin`

Thanks for the quick response.
",sorry bad identical instead thanks quick response,issue,negative,negative,negative,negative,negative,negative
348081885,"Where do the roms in ""site-packages/atari_py/atari_roms"" come from? I think they came from exactly the same place (i.e. atari_py).",come think came exactly place,issue,negative,positive,positive,positive,positive,positive
348077023,"For `summarize_graph`: by ""inputs"" it means ""placeholders"". But indeed there are no placeholders in the graph, because placeholders are slow. So it works as expected.

For `toco`: tensorflow seems to only support NHWC for tflite, but the graph you used is for GPU and therefore is NCHW. So the graph cannot be converted to tflite. 

To work around both of the above issues you'll need to build the graph again, with placeholders & NHWC, rather than import the metagraph.",indeed graph slow work toco support graph used therefore graph converted work around need build graph rather import,issue,negative,negative,negative,negative,negative,negative
348066791,"I am expecting the summarize_graph can find both ""input"" and ""output"". 

If I convert this graphdef file to tflite file
```

bazel-bin/tensorflow/contrib/lite/toco/toco -- \
  --input_file=resnet.pb \
  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
  --output_file=resnet.lite --inference_type=FLOAT \
  --input_type=FLOAT --input_arrays=input \
  --output_arrays=InferenceTower/linear/output  --input_shapes=1,224,224,3

2017-11-29 18:45:39.848975: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: FIFOQueueV2
2017-11-29 18:45:39.849072: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: QueueDequeueV2
2017-11-29 18:45:39.849104: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Transpose
2017-11-29 18:45:39.849122: F tensorflow/contrib/lite/toco/import_tensorflow.cc:286] Check failed: GetStringAttr(node, ""data_format"") == ""NHWC"" (NCHW vs. NHWC)
Aborted (core dumped)
```


",find input output convert file file converting unsupported operation converting unsupported operation converting unsupported operation transpose check node aborted core,issue,negative,neutral,neutral,neutral,neutral,neutral
348063808,"1. covert the  imagenet resnet18 checkpoints file to graphdef file resnet.pb using 
      output_node = ['InferenceTower/linear/output']

 ```python
   with tf.Session() as sess:
        new_saver = tf.train.import_meta_graph(
            metafile, clear_devices=True)
        new_saver.restore(sess, ckpt.model_checkpoint_path)

        output_graph_def = tf.graph_util.convert_variables_to_constants(
            sess,  # The session is used to retrieve the weights
            tf.get_default_graph().as_graph_def(),  # The graph_def is used to retrieve the nodes
            output_node  # The output node names are used to select the usefull nodes
        )

        # Finally we serialize and dump the output graph to the filesystem
        with tf.gfile.GFile(output_graph, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())
        print(""%d ops in the final graph."" % len(output_graph_def.node))
```

2  try to use summarize_graph 
 ```
    bazel-bin/tensorflow/tools/graph_transforms/summarize_graph \
    >   --in_graph= resnet.pb
    No inputs spotted.
    No variables spotted.
    Found 1 possible outputs: (name=InferenceTower/linear/output, op=Identity)
    Found 11699467 (11.70M) const parameters, 0 (0) variable parameters, and 0 control_edges
    Op types used: 208 Const, 124 Identity, 81 Reshape, 61 Mul, 48 Add, 21 Sub, 20 Conv2D, 20 Rsqrt, 17 
    Relu, 1 Mean, 1 Transpose, 1 RealDiv, 1 QueueDequeueV2, 1 MaxPool, 1 MatMul, 1 FIFOQueueV2, 1 Cast, 1 BiasAdd
```
I also tried output_node = ['tower0/linear/output'] and had the same result as ""No inputs spotted."". 

Thanks!



",covert file file python sess sess sess session used retrieve used retrieve output node used select finally serialize dump output graph print final graph try use spotted spotted found possible found variable used identity reshape add sub mean transpose cast also tried result thanks,issue,negative,negative,neutral,neutral,negative,negative
348060759,"https://github.com/tensorpack/tensorpack/blob/130f60ac4491d55a13632eabbad839c2e23138ba/examples/mnist-keras-v2.py#L35-L59

A more keras-like API which could run faster than keras especially when data is large.
It supports multi-gpu, but there are potential issues for now, e.g. weight-decay is probably not performed correctly.",could run faster especially data large potential probably correctly,issue,negative,positive,neutral,neutral,positive,positive
348060283,"Btw. `build_graph(tensorA, tensorB)` looks more natural than `_build_graph(inputs): tensorA, tensorB = inputs`. The latter exists for historical reasons. But maybe it's better to use the first one?",natural latter historical maybe better use first one,issue,positive,positive,positive,positive,positive,positive
348057746,"It's been a while now and I'm not sure if the problem still exists. I've never seen OOM but I don't have TitanX or TitanXp or K80 either. The training runs fine on M40 (also 12G memory) and P100. Others have run it successfully on V100 as well. 

With more results been published recently the code is more stable now, although I still have no clues why you guys saw OOM. I did find that using PrefetchDataZMQ here increase about 100MB GPU memory in training but this is probably irrelevant. 

After all even you two saw different types of OOM (one in training but one in evaluation). So I still believe this is an environment-specific problem.",sure problem still never seen either training fine also memory run successfully well recently code stable although still saw find increase memory training probably irrelevant even two saw different one training one evaluation still believe problem,issue,positive,positive,positive,positive,positive,positive
348007347,"When I played with SR before I found opencv bicubic baseline has very bad PSNR compared to matlab bicubic, which makes some results very hard to reproduce. Maybe you want to check that.",found bad hard reproduce maybe want check,issue,negative,negative,negative,negative,negative,negative
347947240,"The code looks reasonable. If I'm to debug this problem I'll need something I can run. If you're sure dataflow is the cause, post a snippet that only runs `TestDataSpeed` (i.e. no model and no training), and don't include file dependencies (e.g. use a constant numpy array) so I can run it.
See more at http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html",code reasonable problem need something run sure cause post snippet model training include file use constant array run see,issue,negative,positive,positive,positive,positive,positive
347855171,@ppwwyyxx Thanks a ton! You're a savior. Works well now.,thanks ton savior work well,issue,positive,positive,positive,positive,positive,positive
347848338,"```diff
diff --git i/examples/ResNet/cifar10-resnet.py w/examples/ResNet/cifar10-resnet.py
index d2cbc6c..764ca75 100755
--- i/examples/ResNet/cifar10-resnet.py
+++ w/examples/ResNet/cifar10-resnet.py
@@ -48,12 +48,10 @@ class Model(ModelDesc):
     def _build_graph(self, inputs):
         image, label = inputs
         image = image / 128.0
-        assert tf.test.is_gpu_available()
-        image = tf.transpose(image, [0, 3, 1, 2])
 
         def residual(name, l, increase_dim=False, first=False):
             shape = l.get_shape().as_list()
-            in_channel = shape[1]
+            in_channel = shape[3]
 
             if increase_dim:
                 out_channel = in_channel * 2
@@ -68,12 +66,12 @@ class Model(ModelDesc):
                 c2 = Conv2D('conv2', c1, out_channel)
                 if increase_dim:
                     l = AvgPooling('pool', l, 2)
-                    l = tf.pad(l, [[0, 0], [in_channel // 2, in_channel // 2], [0, 0], [0, 0]])
+                    l = tf.pad(l, [[0, 0], [0, 0], [0, 0], [in_channel // 2, in_channel // 2]])
 
                 l = c2 + l
                 return l
 
-        with argscope([Conv2D, AvgPooling, BatchNorm, GlobalAvgPooling], data_format='NCHW'), \
+        with argscope([Conv2D, AvgPooling, BatchNorm, GlobalAvgPooling], data_format='NHWC'), \
                 argscope(Conv2D, nl=tf.identity, use_bias=False, kernel_shape=3,
                          W_init=variance_scaling_initializer(mode='FAN_OUT')):
             l = Conv2D('conv0', image, 16, nl=BNReLU)
```",git index ca class model self image label image image assert image image residual name shape shape shape class model return image,issue,negative,neutral,neutral,neutral,neutral,neutral
347846476,"@ppwwyyxx I already made the necessary changes in `build_graph` to bypass GPU checks. I still cannot get why the

`ValueError: Dimensions must be equal, but are 3 and 16 for 'tower0/res1.0/add' (op: 'Add') with input shapes: [?,3,32,3], [?,3,32,16].`

error occurs.

Shapes should have nothing to do with GPU/CPU I guess.
",already made necessary bypass still get must equal input error nothing guess,issue,negative,neutral,neutral,neutral,neutral,neutral
347829499,"If you can understand tensorflow symbolic code, the changes are easy to identify in `build_graph`. If not I suggest you learn basic tensorflow symbolic programming before using tensorpack.",understand symbolic code easy identify suggest learn basic symbolic,issue,negative,positive,positive,positive,positive,positive
347829049,Can you please help me with changes @ppwwyyxx ? I'm stuck on this for long now. Don't know where to make the changes for this to run on CPU. All the help is appreciated!,please help stuck long know make run help,issue,positive,negative,neutral,neutral,negative,negative
347828205,"`cifar10-resnet.py` is written for GPU.
You need to change the relevant symbolic code after changing data_format.",written need change relevant symbolic code,issue,negative,positive,positive,positive,positive,positive
347827896,"Hello,
Even I faced the same error, and on solving with the method as suggested by @ppwwyyxx I solved that error. However, i guess changing the `data_format` also changed some shapes.

Now I'm getting error:

`ValueError: Dimensions must be equal, but are 3 and 16 for 'tower0/res1.0/add' (op: 'Add') with input shapes: [?,3,32,3], [?,3,32,16].`

Please help me for the same. I need to train cifar10-resnet.py on CPU. I don't have GPU with me.",hello even faced error method error however guess also getting error must equal input please help need train,issue,negative,neutral,neutral,neutral,neutral,neutral
347825419,"The latest commit may support InferenceRunner without size (but DataParallelInferenceRunner still requires size), as long as the tf dataset will stop (e.g. no `repeat()`). I don't have a use case to test whether it works.",latest commit may support without size still size long stop repeat use case test whether work,issue,negative,positive,positive,positive,positive,positive
347822252,"Now `InferenceRunner` only takes something that has a size, so it can know when to stop inference.

The whole `predictors` are designed to use Python data for quick demo.
Tensorpack doesn't support predictors with tf dataset. You'll need to do them by yourself, i.e. call `build_graph` on tf.dataset tensors and evaluate the output in your own loop.",something size know stop inference whole designed use python data quick support need call evaluate output loop,issue,negative,positive,positive,positive,positive,positive
347819036,"how to use tf dataset in InferenceRunner and Predictor? 
TFDatasetInput didn't implement size(), it cannot be used in InferenceRunner.
Predictors like SimpleDatasetPredictor only accept DataFlow, is there any way to use tf dataset?",use predictor implement size used like accept way use,issue,positive,neutral,neutral,neutral,neutral,neutral
347778259,"I've found it. I am using the function`dump_chkpt_vars()` to retrieve the values .

Thanks a lot!",found function retrieve thanks lot,issue,negative,positive,positive,positive,positive,positive
347760476,"I know the names of the variables I need, but my difficulty is how to retrieve the variable values using these names. `tf.train.NewCheckpointReader(path).get_variable_to_shape_map()` seems to only provide a dictionary of the variable name and shape.  Is there a dictionary that can tell me the value corresponding to each name?",know need difficulty retrieve variable path provide dictionary variable name shape dictionary tell value corresponding name,issue,negative,neutral,neutral,neutral,neutral,neutral
347606137,"http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html#work-with-tf-checkpoint
If you could print the names it should be easy to know which name you need.",could print easy know name need,issue,negative,positive,positive,positive,positive,positive
347109853,This seems to be a side effect of the GPU utilization tracker callback. Should be fixed in the latest commit.,side effect utilization tracker fixed latest commit,issue,negative,positive,positive,positive,positive,positive
347068252,Distributed training was supported. Didn't test speed but according to horovod benchmarks I assume it will be fast..,distributed training test speed according assume fast,issue,negative,positive,positive,positive,positive,positive
346925890,"Thanks man
git clone again , everything seems working but I dont even have GPU in my Mac need a new machine for training, from your benchmark, you done really well",thanks man git clone everything working dont even mac need new machine training done really well,issue,positive,positive,positive,positive,positive,positive
346902838,"tensorpack saves models in standard format so theoretically there is nothing you cannot do as long as tensorflow supports it, regardless of what symbolic libraries you use.

I don't know what you mean by ""doesn't work"". Segfault always sounds like a tensorflow bug. Without you providing information about what you did I cannot help more.",standard format theoretically nothing long regardless symbolic use know mean work always like bug without providing information help,issue,negative,negative,neutral,neutral,negative,negative
346902196,A3C-Gym/common.py is a symbolic link. You did something during clone that broke symbolic links.,symbolic link something clone broke symbolic link,issue,negative,neutral,neutral,neutral,neutral,neutral
346859593,"About forks, you can also just write a collection of functions for your convenience, e.g. a wrapper around DictRestore or logger.set_logger.dir. This might be easier than maintaining a personal fork.

About light-weight. Yes I hope it's lightweight -- though there is a lot of complexity already. So in general I don't like one-line features. 

About consistency: if there are 5 *Restore and one is different from others, I'll call it a consistency issue. These two in my mind were just two things with different functionalities and therefore different APIs. I don't even know if there will be a third *Restore, because I can't imagine what else one can need.",also write collection convenience wrapper around might easier personal fork yes hope lightweight though lot complexity already general like consistency restore one different call consistency issue two mind two different therefore different even know third restore ca imagine else one need,issue,positive,positive,neutral,neutral,positive,positive
346807389,You can always write print in your code if you like. I don't see why this would become a problem,always write print code like see would become problem,issue,negative,neutral,neutral,neutral,neutral,neutral
346774957,"I just code them, when **I need** them without asking anybody.
Honestly, I don't care if @ppwwyyxx wants them to be included a public version of tensorpack or not. If not, I close the pull-request, like I did in the past. That's the nice thing about GIT and forks or monkey-patches.

I understand the point, that this library wants to be light-weight. But I this case I am still convinced, it might be good to keep the interfaces at least consistent. So I use it in my local version. 

I respect @ppwwyyxx decision. So there should be no discussion.",code need without anybody honestly care included public version close like past nice thing git understand point library case still convinced might good keep least consistent use local version respect decision discussion,issue,positive,positive,positive,positive,positive,positive
346768246,"it may be better discuss the function first,  then start coding.",may better discus function first start,issue,negative,positive,positive,positive,positive,positive
346767884,"I see your point. Just wondering, if `class *Restore` shouldn't share the same interface. It is not convenient if they differ.

",see point wondering class restore share interface convenient differ,issue,positive,neutral,neutral,neutral,neutral,neutral
346754976,In fact using the dict and npy format is specifically for users to load and modify the model manually. Being easy to load and modify is the only benefit it has compared to tf format.,fact format specifically load modify model manually easy load modify benefit format,issue,positive,positive,positive,positive,positive,positive
346754029,Users should care about the format if they use DictRestore. That's why it takes a dict but not a file name.,care format use file name,issue,negative,neutral,neutral,neutral,neutral,neutral
346751432,Users dont care about the internal format of npy files. They want to restore weights from npy files like restoring from checkpoint. It is a matter of consistency.,dont care internal format want restore like matter consistency,issue,negative,neutral,neutral,neutral,neutral,neutral
346746141,"For `DictRestore` this is not needed. Users can just use
`dict = {prefix + name: value for name, value in six.iteritems(dict)}` to do the same thing.",use prefix name value name value thing,issue,positive,neutral,neutral,neutral,neutral,neutral
346663739,"The tutorial http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html contains some explanation of ""what's going on"" that can help you understand. For the code you posted, there are a total of 3 processes because you use two prefetch with nr_proc=1.
Process A reads LMDB and put data in a queue.
Process B decodes data from the queue, run mapping with 30 threads, put results to ZMQ.
Main process takes data from zmq and batch it. ",tutorial explanation going help understand code posted total use two process put data queue process data queue run put main process data batch,issue,negative,positive,neutral,neutral,positive,positive
346661773,"Every time `PrefetchDataZMQ` is used, it creates one or more extra processes for the function before it. And the function following `PrefetchDataZMQ` is the main process. Is it correct?
In the following example, there is one process reading data from the disk and a second one doing data mapping. If I don't use `PrefetchDataZMQ`, there would be only one process sequentially handling all the work. Is it what's happening?
```python
        ds = LMDBData(datadir, shuffle=False)
        ds = PrefetchData(ds, 5000, 1)
        ds = LMDBDataPoint(ds, aug)
        ds = ThreadedMapData(ds, 30, mapf, buffer_size=2000, strict=True)
        ds = PrefetchDataZMQ(ds, 1)
        ds = BatchData(ds, batch_size, remainder=True)
```",every time used one extra function function following main process correct following example one process reading data disk second one data use would one process sequentially handling work happening python,issue,negative,positive,neutral,neutral,positive,positive
346637479,"multiprocessing is just a way a program runs but how it can be used is up to the user. What you described sounds like `MultiProcessMapData` but that's very different from `PrefetchData` as you can see from their APIs.

> Assume that I have A, B, C, D functions to process the data.

I assume you mean ""map"" the data, i.e. you have x and you need D(C(B(A(x)))). Then yes, you can use multiple processes to run the mapping on a lot of `x`s simultaneously, to perhaps improve speed, if you can beat the overhead of processes. The overhead is mainly on copying the data to processes and copying the result back.

> That means A doesn't have to wait for B,C,D to finish before starting a new round of work. 

Perhaps you mean BCD don't have to wait for A? Not true because logically B always depends on the output of A. But A may finish sooner because you run it multiple processes.

> the best way to use multi-process should be adding a multi-process function to any computationally intensive step.

Not true. Using processes has significant communication overhead so you can't use it everywhere. If you have four mapper ABCD to apply, it's better to merge into one function f(x) = D(C(B(A(x)))) and run f(x) in multiple processes so you don't end up doing too much communication.

`PrefetchDataZMQ` is a completely different mechanism. You can think that it makes several copies of a dataflow and run them in multiple processes. When they are running the results are copied back to main process for use.",way program used user like different see assume process data assume mean map data need yes use multiple run lot simultaneously perhaps improve speed beat overhead overhead mainly data result back wait finish starting new round work perhaps mean wait true logically always output may finish sooner run multiple best way use function intensive step true significant communication overhead ca use everywhere four mapper apply better merge one function run multiple end much communication completely different mechanism think several run multiple running copied back main process use,issue,positive,positive,positive,positive,positive,positive
346629994,"I'd like to make sure if I correctly understand how multi-process functions like `PrefetchDataZMQ` or `MultiProcessMapData` work. 

Assume that I have A, B, C, D functions to process the data. If I don't use these multi-process functions, then all  A, B, C, D would run sequentially in one process, which is obviously slow. If I add a multi-process function to one of the four, let's say A, then A would run in one or more separate process in parallel with the other 3. That means A doesn't have to wait for B,C,D to finish before starting a new round of work. If I add  a multi-process function to each of them, then all the four can run in parallel. If this is the case, the best way to use multi-process should be adding a multi-process function to any computationally intensive step.

I would be sorry that if this question sounds too basic, but I would like to hear about your explanation before I can step forward with my project on tensorpack. Appreciate your help a lot! 




",like make sure correctly understand like work assume process data use would run sequentially one process obviously slow add function one four let say would run one separate process parallel wait finish starting new round work add function four run parallel case best way use function intensive step would sorry question basic would like hear explanation step forward project appreciate help lot,issue,positive,positive,neutral,neutral,positive,positive
346595381,"The purpose of benchmark is to figure out __what is the bottleneck__, by changing the pipeline. Only knowing how slow the whole pipeline is doesn't help you improve it.

The [documentation](http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.PrefetchDataZMQ) of `PrefetchDataZMQ` is clear that it requires the underlying dataflow to be fully shuffled.
There are other multi-process dataflow such as `MultiProcessMapData`.",purpose figure pipeline knowing slow whole pipeline help improve documentation clear underlying fully,issue,positive,negative,neutral,neutral,negative,negative
346593346,"Thanks very much for your suggestion. It's very helpful! I compared the performance of the two cases. The multi-process one has a speed of 87.37it/s and the multi-thread one has only 23it/s.

Now the question is if I can use multi-process to inference on the validation set. In your tutorial, you mention that the multi-process data reading is not suitable for the validation set. Is my multi-process code reading a unique and complete copy of the validation set? If not, is there any way to modify the code to make it usable for the validation set?

",thanks much suggestion helpful performance two one speed one question use inference validation set tutorial mention data reading suitable validation set code reading unique complete copy validation set way modify code make usable validation set,issue,positive,positive,positive,positive,positive,positive
346551119,"There are other ways to print tensors other than summaries: https://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training

Summaries are added to tensorboard in the callbacks:
https://tensorpack.readthedocs.io/tutorial/summary.html#tensorflow-summariesAnd you can use a different `period` option: https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.MergeAllSummaries",way print added use different period option,issue,negative,neutral,neutral,neutral,neutral,neutral
346505118,"Thanks. 
We switched to v1 and that works. Will test v2 trainer later on.",thanks switched work test trainer later,issue,negative,positive,neutral,neutral,positive,positive
346354229,"it's caused by `prefetch` and should be nothing about tensorpack. I changed the code as below and the issue was gone.
```
tfrecord_dataset = (
    tf.data.Dataset.from_tensor_slices(tfrecord_list)
      .shuffle(buffer_size=1024)
      .flat_map(tf.data.TFRecordDataset)
      .map(record_parser, num_parallel_calls=10)
      .shuffle(buffer_size=5000)
      .repeat()
      .batch(batch_size)
      .prefetch(5)
)
```",nothing code issue gone,issue,negative,neutral,neutral,neutral,neutral,neutral
346352265,"Ideally similar code should not co-exist, like I'm saying in another PR: https://github.com/ppwwyyxx/tensorpack/pull/481#issuecomment-345672569
And having two models (or just two tower_func) sounds good.
But you're right, for something like VGG, people usually just want something simple to copy and paste.. ",ideally similar code like saying another two two good right something like people usually want something simple copy paste,issue,positive,positive,positive,positive,positive,positive
346351157,Ok too late :grin: (already merged). Pretrained model is available as well.,late grin already model available well,issue,positive,positive,neutral,neutral,positive,positive
346351087,"As this is commonly used, I am not sure if we should merge them into a single file hide the model behind flags and if-branches. But a good alternative would be having `VGG16Model` and `VGG19Model`.",commonly used sure merge single file hide model behind good alternative would,issue,positive,positive,neutral,neutral,positive,positive
346350206,"Having both load-vgg16 and load-vgg19 still feels weird. Maybe later will move things (with load-alexnet, or maybe load-cpm as well) to a separate directory.",still weird maybe later move maybe well separate directory,issue,negative,negative,negative,negative,negative,negative
346340449,"There isn't a good setting that works for everyone's machine and you'll need to figure it out by testing.  Things can be very different especially with slow hardware. For example, you use 30 threads but I don't even know whether your machine has 30 cores or not, how can I comment anything on it? 

You can run some benchmarks easily to figure out what is causing the slow down.
See [tutorial on performance tuning](http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html#improve-dataflow).
",good setting work everyone machine need figure testing different especially slow hardware example use even know whether machine comment anything run easily figure causing slow see tutorial performance tuning,issue,positive,positive,neutral,neutral,positive,positive
346338157,"It's just a simple `iterator.get_next()` as you can see. No magic is happening: 
https://github.com/ppwwyyxx/tensorpack/blob/ef9fb4b8b7b1c2c4b1b8b19bd92650665a48f822/tensorpack/input_source/input_source.py#L433-L445

I don't have a good explanation for that tfrecord become slower than image_dataset during training. Some points you might want to investigate:
1. How do you know IO is the bottleneck? Is that a correct conclusion?
2. Use only one GPU to make the problem simple
3. Have you tested more than one times to make sure the observation is stable?
4. During training a different session config is used (`tensorpack.tfutils.get_default_sess_config`). Maybe it affects the speed and you can try to benchmark with this config instead.",simple see magic happening good explanation become training might want investigate know io bottleneck correct conclusion use one make problem simple tested one time make sure observation stable training different session used maybe speed try instead,issue,positive,positive,positive,positive,positive,positive
346330475,"The returned value of `test_perf(tfrecord_dataset)` is smaller than `test_perf(image_dataset)`, but using `tfrecord_dataset`, IO is the bottleneck, `image_dataset` not. How is `TFDatasetInput._iterator` used in training progress?",returned value smaller io bottleneck used training progress,issue,positive,neutral,neutral,neutral,neutral,neutral
346309680,"You bring two issues if I understand correctly:
1. Reading tfrecord is slower than reading images directly, both with tf.data APIs.

This is only about ""how to write more efficient tf.data pipeline"" and is nothing about tensorpack. We won't  discuss it here.

2. You used a snippet to test raw speed of tfrecord and found it faster than reading images.

-- How do you know the speed of `image_dataset`? Have you written a similar script to test it? If not, you're probably not comparing things apple-to-apple. ",bring two understand correctly reading reading directly write efficient pipeline nothing wo discus used snippet test raw speed found faster reading know speed written similar script test probably,issue,negative,negative,neutral,neutral,negative,negative
346299738,"1. The first argument (gpus) needs to be a int or a list of int. Somehow it's missing from the documentation.
2. Please post full error when you report bugs.",first argument need list somehow missing documentation please post full error report,issue,negative,positive,positive,positive,positive,positive
346297970,"You'll create a predict function inside the callback, and call it however you want when the callback is triggered. You'll need to compute those statistics on your own.
See http://tensorpack.readthedocs.io/en/latest/tutorial/extend/callback.html#explain-the-callback-methods
> self.trainer.get_predictor() is a helper function to create a callable under inference mode.

Examples:
https://github.com/ppwwyyxx/tensorpack/blob/aaf62f2db9ae4fcf40772a35df8b41d4f5c5ddf1/examples/GAN/CycleGAN.py#L188-L199",create predict function inside call however want triggered need compute statistic see helper function create callable inference mode,issue,positive,neutral,neutral,neutral,neutral,neutral
346233575,"Thanks! I know the model probably gets slightly better than before, but haven't got time to train it.",thanks know model probably slightly better got time train,issue,positive,positive,positive,positive,positive,positive
346230066,"Just want to share the trained results, basemodel is ResNet-50:
Evaluation on **minival** set, FASTRCNN_BATCH=256, ~33h on 8 v100.
Average speed: after epoch 300, it costs ~115 seconds per epcoh.
```
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.344
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.555
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.365
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.391
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.498
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.462
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.534
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.66
```
",want share trained evaluation set average speed epoch per average precision average precision average precision average precision small average precision average precision large average recall ar average recall ar average recall ar average recall ar small average recall ar average recall ar large,issue,negative,negative,negative,negative,negative,negative
346010774,You're the one who wrote things with PReLU and you'll make this decision by yourself.,one wrote make decision,issue,negative,neutral,neutral,neutral,neutral,neutral
345932255,Thanks.I can implement it when BITA ==32 .  And PReLU in Activaton which is not 32 bit will make no difference. Am I right？,implement bit make difference,issue,negative,neutral,neutral,neutral,neutral,neutral
345927954,"the line `PReLU('prelu', x)` got executed multiple times. You have to use different names each time.",line got executed multiple time use different time,issue,negative,neutral,neutral,neutral,neutral,neutral
345927501,"Sir, I use `return PReLU('prelu',x)`
and it also doesnt run.

(tensorflow) Precision-Tower-7910:~/tensorflow/BNN/DoReFa-Net$ python alexnet-dorefa.py --dorefa 1,32,32 --data /resources/data/ILSVRC2012/images/ --gpu 0
Failed to load OpenCL runtime (expected version 1.1+)
[1121 14:09:59 @alexnet-dorefa.py:305] Batch per tower: 128
[1121 14:09:59 @logger.py:94] WRN Log directory train_log/alexnet-dorefa exists! Please either backup/delete it, or use a new directory.
[1121 14:09:59 @logger.py:96] WRN If you're resuming from a previous run you can choose to keep it.
[1121 14:09:59 @logger.py:97] Select Action: k (keep) / b (backup) / d (delete) / n (new) / q (quit):
d
[1121 14:10:00 @logger.py:74] Argv: alexnet-dorefa.py --dorefa 1,32,32 --data /resources/data/ILSVRC2012/images/ --gpu 0
[1121 14:10:00 @fs.py:89] WRN Env var $TENSORPACK_DATASET not set, using /home/xxxxx/tensorpack_data for datasets.
[1121 14:10:02 @prefetch.py:263] [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[1121 14:10:02 @ilsvrc.py:118] Assuming directory /resources/data/ILSVRC2012/images/val has original structure.
[1121 14:10:02 @inference_runner.py:82] InferenceRunner will eval on an InputSource of size 391
[1121 14:10:02 @input_source.py:180] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[1121 14:10:02 @training.py:90] Building graph for training tower 0 on device LeastLoadedDeviceSetter-/gpu:0...
[1121 14:10:02 @registry.py:121] conv0 input: [None, 224, 224, 3]
[1121 14:10:02 @registry.py:129] conv0 output: [None, 54, 54, 96]
[1121 14:10:02 @registry.py:121] conv1 input: [None, 54, 54, 96]
[1121 14:10:02 @alexnet-dorefa.py:94] Binarizing weight conv1/W
mul:0
[1121 14:10:02 @registry.py:129] conv1 output: [None, 54, 54, 256]
[1121 14:10:02 @registry.py:121] pool1 input: [None, 54, 54, 256]
[1121 14:10:02 @registry.py:129] pool1 output: [None, 27, 27, 256]
Traceback (most recent call last):
  File ""alexnet-dorefa.py"", line 310, in <module>
    launch_train_with_config(config, SyncMultiGPUTrainer(nr_tower))
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/train/interface.py"", line 88, in launch_train_with_config
    model._build_graph_get_cost, model.get_optimizer)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/utils/argtools.py"", line 165, in wrapper
    return func(*args, **kwargs)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/train/tower.py"", line 137, in setup_graph
    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/train/trainers.py"", line 79, in _setup_graph
    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/graph_builder/training.py"", line 137, in build
    grad_list = DataParallelBuilder.build_on_towers(self.towers, get_grad_fn, devices)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/graph_builder/training.py"", line 95, in build_on_towers
    ret.append(func())
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/train/tower.py"", line 166, in get_grad_fn
    cost = get_cost_fn(*input.get_input_tensors())
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/tower.py"", line 198, in __call__
    output = self._tower_fn(*args)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py"", line 169, in _build_graph_get_cost
    self.build_graph(*inputs)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py"", line 119, in build_graph
    self._build_graph(inputs)
  File ""alexnet-dorefa.py"", line 128, in _build_graph
    .apply(activate) 
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/models/linearwrap.py"", line 75, in apply
    ret = func(self._t, *args, **kwargs)
  File ""alexnet-dorefa.py"", line 113, in activate
    return fa(nonlin(x))
  File ""alexnet-dorefa.py"", line 105, in nonlin
    return PReLU('prelu',x)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/models/registry.py"", line 124, in wrapped_func
    outputs = func(*args, **actual_args)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/models/nonlin.py"", line 55, in PReLU
    alpha = tf.get_variable('alpha', [], initializer=init)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1065, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 962, in get_variable
    use_resource=use_resource, custom_getter=custom_getter)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 360, in get_variable
    validate_shape=validate_shape, use_resource=use_resource)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/varreplace.py"", line 53, in custom_getter
    v = getter(*args, **kwargs)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 352, in _true_getter
    use_resource=use_resource)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 664, in _get_single_variable
    name, """".join(traceback.format_list(tb))))
ValueError: Variable prelu/alpha already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:

  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/varreplace.py"", line 53, in custom_getter
    v = getter(*args, **kwargs)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/models/nonlin.py"", line 55, in PReLU
    alpha = tf.get_variable('alpha', [], initializer=init)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/models/registry.py"", line 124, in wrapped_func
    outputs = func(*args, **actual_args)

",sir use return also doesnt run python data load version batch per tower log directory please either use new directory previous run choose keep select action keep backup delete new quit data set fork one time assuming directory original structure size setting queue building graph training tower device input none output none input none weight output none pool input none pool output none recent call last file line module file line file line wrapper return file line input file line input file line build file line file line cost file line output file line file line file line activate file line apply ret file line activate return fa file line return file line file line alpha file line file line file line file line getter file line file line name variable already mean set originally defined file line getter file line alpha file line,issue,positive,positive,neutral,neutral,positive,positive
345897491,"But now the model converted from caffe can be downloaded at http://models.tensorpack.com/caffe/.

Thanks @PatWie for hosting the models! When I have good network I'll move more models there.",model converted thanks hosting good network move,issue,positive,positive,positive,positive,positive,positive
345708255,Thanks! The code is OK. If you don't have time I can train it as well.,thanks code time train well,issue,positive,positive,positive,positive,positive,positive
345706037,"sorry for my carelessness, even though got a 3.7 result.  is this code ok? I will retrain it later.",sorry carelessness even though got result code retrain later,issue,negative,negative,negative,negative,negative,negative
345688513,"Each resnet block in the reference code has conv{1,2,3} and another conv in shortcut. https://github.com/kuangliu/pytorch-cifar/blob/886af4caa4ee224c3fd4b42e997e43b1b4f234b9/models/preact_resnet.py#L57-L64 
One seems to be missing here?",block reference code another one missing,issue,negative,negative,negative,negative,negative,negative
345672569,Could you merge two scripts into one with a flag? For example purposes it doesn't make sense to have many files with almost identical contents.,could merge two one flag example make sense many almost identical content,issue,negative,positive,positive,positive,positive,positive
345632403,"A subtle bug that makes the result 2 points worse: https://github.com/ppwwyyxx/tensorpack/commit/6fc4378cc65c3896e802f037bd8d289412d6b2c1  .
Now the training curve looks the same as what I had before -- it hasn't finished, but probably is correct now.

This again shows how [important it is to match the paper's performance](https://medium.com/@ppwwyyxx/unawareness-of-deep-learning-mistakes-d5b5774da0ba) -- if I didn't try to compare with some reference number, I'll never find hidden mistakes like this.",subtle bug result worse training curve finished probably correct important match paper performance try compare reference number never find hidden like,issue,negative,negative,negative,negative,negative,negative
345587941,"Tensorflow convolution needs warm up. For variable-size inputs it needs more.
The overall speed will always first increase (until about 10 epochs) and then decrease",convolution need warm need overall speed always first increase decrease,issue,negative,positive,positive,positive,positive,positive
345587713,"A bug of resuming of faster RCNN:

After I resume a trained model, the learning rate of the first epoch will be 0.003.
```
[1119 22:52:36 @param.py:144] After epoch 0, learning_rate will change to 0.00300000
[1119 22:52:36 @monitor.py:262] Found training history from JSON, now starting from epoch number 117.
[1119 22:52:36 @base.py:209] Start Epoch 117 ...
[1119 22:58:19 @argtools.py:142] WRN Input /home/chenrich/dataset/COCO14/train2014/COCO_train2014_000000273046.jpg is filtered for training: No valid foreground/background for RPN!
[1119 23:06:41 @base.py:219] Epoch 117 (global_step 34800) finished, time:844.78 sec.
[1119 23:06:41 @graph.py:70] Running Op sync_variables_from_main_tower ...
[1119 23:06:41 @monitor.py:363] learning_rate: 0.003
```

On the other hand, I can understand the speed per epoch might be varied for each epoch since the number of positive proposals might be increased after more epochs; however, when I resume model, the speed will become as slow as training from scratch and then it gradually increases its speed.   However, the number of positive proposals after resume should be identical or similar to previous one, why the speed is slow?  (As you can see that above log shows finishing time are 844 sec, I can get about 200 seconds on average before resuming.)
",bug faster resume trained model learning rate first epoch epoch change found training history starting epoch number start epoch input training valid epoch finished time sec running hand understand speed per epoch might varied epoch since number positive might however resume model speed become slow training scratch gradually speed however number positive resume identical similar previous one speed slow see log finishing time sec get average,issue,positive,negative,neutral,neutral,negative,negative
345535538,"FYI I have a bug introduced in Nov 13 and fixed just now. It will affect the precision.
Lots of changes are being pushed recently. I tested the model periodically which means bugs will be found with a delay.",bug fixed affect precision lot recently tested model periodically found delay,issue,negative,positive,neutral,neutral,positive,positive
345522754,"wuuwu,I realized where I was wrong. I realize that is the absolute path and the relative path difference.Thanks a lot.",wrong realize absolute path relative path lot,issue,negative,negative,neutral,neutral,negative,negative
345521718,"Thanks . It's does't work .
information is as following  @dongzhuoyao

Traceback (most recent call last):
  File ""alexnet-dorefa.py"", line 306, in <module>
    config.session_init = SaverRestore(args.load)
  File ""/home/wa/.local/lib/python2.7/site-packages/tensorpack/tfutils/sessinit.py"", line 106, in __init__
    model_path = get_checkpoint_path(model_path)
  File ""/home/wa/.local/lib/python2.7/site-packages/tensorpack/tfutils/varmanip.py"", line 167, in get_checkpoint_path
    assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path
AssertionError: /train_log/alexnet-dorefa/model-660000
(tensorflow) w@hu-Precision-Tower-7910:~/tensorflow/BNN/DoReFa-Net$ python alexnet-dorefa.py --dorefa 1,32,32 --load /train_log/alexnet-dorefa/model-660000 --data /resources/data/ILSVRC2012/images/ --gpu 1
",thanks work information following recent call last file line module file line file line assert python load data,issue,negative,positive,neutral,neutral,positive,positive
345521398,"I am sorry. It turned out to be my own carelessness. These parameters are saved fine.  I should remove this issue.  
Thanks!",sorry turned carelessness saved fine remove issue thanks,issue,positive,positive,neutral,neutral,positive,positive
345520209,"In any case, for others to understand what the problem is, please post ""what you've observed"", as suggested in the issue template.",case understand problem please post issue template,issue,negative,neutral,neutral,neutral,neutral,neutral
345519924,"Those variables will be saved.

By ""when testing"", did you mean ""when you define the graph for testing""?",saved testing mean define graph testing,issue,negative,negative,negative,negative,negative,negative
345519647,You start with the epoch number but didn't load the previous model. The example has a `--load` option.,start epoch number load previous model example load option,issue,negative,negative,negative,negative,negative,negative
345516769,"`get_data` will only be called when `Dataset.Iterator.initializer` is called, which is when `InputSource.reset_state()` is called, which happens once before training.

About speed, when python-side iterator is fast enough, dataset is as fast as queues.",training speed fast enough fast,issue,negative,positive,positive,positive,positive,positive
345490337,"Sorry, my bad, never mind. Thanks for reminding to use ""tensor""... I used to use ""list"" to specify the targeted tensor size.. ",sorry bad never mind thanks use tensor used use list specify targeted tensor size,issue,negative,negative,negative,negative,negative,negative
345489961,"Thanks for your reply.  Yes, `tf.image.resize_images` (or more specific `tf.image.resize_bilinear`) support target size as a ""tensor"", but it is a 1-D tensor with `[new_height, new_width]`; however, when the size of the input of graph is `[None,None,3]`, I do not find out a way to derive the `[new_height, new_width]` for `tf.image.resize_images` during building the graph.

E.g. in my graph, there are two tensors, A and B, and the size of A is larger than B; during the running time, I would like to resize B to the size of A (the size of A is varied); however, I can not infer the size of A since I create a placeholder `[None,None,3]` for A for dynamic input size. Hence, during building the graph, when I try to use `get_shape().as_list()` to get A's shape, I would only get `[None, None, 3]` and then I can not derive the size of B for `tf.image._resize_images` function.

Thanks.",thanks reply yes specific support target size tensor tensor however size input graph none none find way derive building graph graph two size running time would like resize size size varied however infer size since create none none dynamic input size hence building graph try use get shape would get none none derive size function thanks,issue,positive,positive,neutral,neutral,positive,positive
345487571,"I can't understand exactly what is ""dynamically change the image resolution"". But from all I can see `tf.image.resize` support target size as a ""tensor"" so it's dynamic.",ca understand exactly dynamically change image resolution see support target size tensor dynamic,issue,positive,positive,positive,positive,positive,positive
345457715,"Hi, this might be related to the general tensorflow question, if you think stackoverflow is a better place to ask, please just ignore it.

Now, the input image size is `[None, None, 3]` for dynamic image size in faster rcnn; however, my basemodel needs to do upsampling to dynamically change the image resolution on-the-fly (like encoder-decoder architecture); however, with `None` type in image size, I can not build the graph since the upsampling needs deterministic shape to perform upsample (I have tried `BilinearUpsampling` and `FixedUnpooling` in tensorpack (specify the ratio to 2), and 'tf.image.resize' in tensorflow (it requires final size of an image).)

Do you have any suggestion? Thanks.",hi might related general question think better place ask please ignore input image size none none dynamic image size faster however need dynamically change image resolution like architecture however none type image size build graph since need deterministic shape perform tried specify ratio final size image suggestion thanks,issue,positive,positive,positive,positive,positive,positive
345451113,"hmm, weird, after I comment out the line 78 in `utils/box_ops.py`
```
os.environ['CUDA_VISIBLE_DEVICES'] = ''  # we don't want the dataflow process to touch CUDA
```

It works again. maybe I system need to keep this variable existed even when we would set Ops in CPU.
",weird comment line want process touch work maybe system need keep variable even would set,issue,negative,negative,negative,negative,negative,negative
345450581,"Another error :( 

I just pull the newest changes.

```
2017-11-18 10:44:54.781565: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE
2017-11-18 10:44:54.781715: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: xxxx
2017-11-18 10:44:54.781731: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: xxx
2017-11-18 10:44:54.781960: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 384.81.0
2017-11-18 10:44:54.782014: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  384.81  Sat Sep  2 02:43:11 PDT 2017
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)
""""""
2017-11-18 10:44:54.782060: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.81.0
2017-11-18 10:44:54.782072: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 384.81.0
^```",another error pull call diagnostic information host version driver version file content version kernel module sat version version kernel version kernel version match,issue,negative,neutral,neutral,neutral,neutral,neutral
345448649,"Thanks for your help. After about 400 epochs, the speed is more stable (~105 seconds per epoch). 

After pull the newest changes, got an error about mismtached data type.

At `model.py`, line 88:

```
precision = tf.truediv(pos_prediction_corr, nr_pos_prediction)
```
Error message:
```
TypeError: x and y must have the same dtype, got tf.int64 != tf.int32
```

since you cast the `valid_prediction` to tf.int32 at line 80:
```
valid_prediction = tf.cast(valid_label_prob > th, tf.int32)
```

However, the `tf.count_nonzero` will return the `tf.int64` by default, I should set the dtype for `tf.count_nonzero` to `tf.int32`, right?

Note: I am using python3.6.
",thanks help speed stable per epoch pull got error data type line precision error message must got since cast line th however return default set right note python,issue,negative,positive,positive,positive,positive,positive
345447829,"Hi,can you help me with my problem during using dump-model-params?
(tensorflow) xxxxx-Precision-Tower-7910:~/tensorflow/DoReFa-Net$ ./dump-model-params.py --meta train_log/alexnet-dorefa1118-000755/graph-1118-000802.meta train_log/alexnet-dorefa1118-000755/model out.npy
Failed to load OpenCL runtime (expected version 1.1+)
Traceback (most recent call last):
  File ""./dump-model-params.py"", line 35, in <module>
    init = get_model_loader(args.model)
  File ""/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/sessinit.py"", line 261, in get_model_loader
    return SaverRestore(filename)
  File ""/home/xxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/sessinit.py"", line 106, in __init__
    model_path = get_checkpoint_path(model_path)
  File ""/home/xxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/varmanip.py"", line 167, in get_checkpoint_path
    assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path
AssertionError: train_log/alexnet-dorefa1118-000755/model

",hi help problem meta load version recent call last file line module file line return file line file line assert,issue,negative,neutral,neutral,neutral,neutral,neutral
345437172,"thanks ppwwyyxx! the reference is very useful.
I found a workaround method by add two line below `__name__ == '__main__'`:
`    gpu_options = tf.GPUOptions(allow_growth=True)`
`    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))`
just create a useless session with gpu_options in the begging of sample.
It seems like gpu_options is a global setting and not a session setting.",thanks reference useful found method add two line sess create useless session begging sample like global setting session setting,issue,positive,neutral,neutral,neutral,neutral,neutral
345434472,"Possible workaround:
1. Remove all use of `get_nr_gpu` in imagenet, so that cuda context is not initialized before session creation. This may let TensorFlow repsect your session config.
2. Setting `per_process_memory_fraction` in session config will probably limit the maximum memory to use (at least on my laptop). `allow_growth` seems broken and I don't know the workaround.
",possible remove use context session creation may let session setting session probably limit maximum memory use least broken know,issue,positive,negative,negative,negative,negative,negative
345429267,"https://help.github.com/articles/distributing-large-binaries/
Maybe we can try this, probably on a separate repo, (e.g. tensorpack/models).
Not sure if github is OK with us putting GBs of models in an empty project.",maybe try probably separate sure u empty project,issue,negative,positive,positive,positive,positive,positive
345415151,"The document says: ""To reduce the effect of GIL to your main training thread, you want to uncomment the line so that everything above it (including all the threads) happen in an independent process.""
Maybe you can try this. The gap indeed looks too large.",document reduce effect main training thread want line everything happen independent process maybe try gap indeed large,issue,negative,positive,positive,positive,positive,positive
345285942,"Here the speed roughly decreased from 70 sec / epoch @epoch10 to 120 sec / epoch  @epoch700. It decreases because of more and more positive predictions. I haven't seen 200.
1. Those images are filtered out.
2. Precision was only added yesterday so I don't know. It's probably OK for training -- if rpn predicts everything as negative (even just for once) it will be nan forever. But I'll change it because this metric becomes useless.",speed roughly sec epoch epoch sec epoch epoch positive seen precision added yesterday know probably training everything negative even nan forever change metric becomes useless,issue,negative,negative,negative,negative,negative,negative
345283678,"Thanks. It seems that the speed per epoch is varied on my machine even after 30 epoch (9k steps)
It can be from 90 seconds to 200 seconds per epoch (with 8 gpus, no other users use the gpus), I guess that because the number of proposals on each image might be varied which affects the speed of each image.

Furthermore, may I have few questions about training log?
1. The Warning message of training image:
  `COCO_val2014_000000251330.jpg is invalid for training: No valid foreground/background for RPN!`
I think it is okay since the image did not provide FG/BG for RPN

2. Performance metric:
I observed that I will get `nan` on certain metrics, is it normal? Here is log

```
[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/precision_th0.1: 0.39033
[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/precision_th0.2: nan
[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/precision_th0.5: nan
[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/recall_th0.1: 0.98191
[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/recall_th0.2: 0.95139
[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/recall_th0.5: 0.67927
```

Thanks.




",thanks speed per epoch varied machine even epoch per epoch use guess number image might varied speed image furthermore may training log warning message training image invalid training valid think since image provide performance metric get nan certain metric normal log nan nan thanks,issue,positive,positive,positive,positive,positive,positive
345221637,@ppwwyyxx  Could you provide an example on how to use this feature? Thanks.,could provide example use feature thanks,issue,negative,positive,positive,positive,positive,positive
345172494,"You use `tf.summary.histogram(ret.name, ret) `. So you should look for ""ret.name"" in tensorboard, whatever it is. The name is definitely not ""param-summary/conv2/W-histogram"".",use ret look whatever name definitely,issue,negative,neutral,neutral,neutral,neutral,neutral
345172040,"sir,i do this many times, but i got the weights histogram like normal distribution. Which steps did I make wrong?
![image](https://user-images.githubusercontent.com/20589365/32936839-dc505874-cbb0-11e7-84ce-93d0e6006e28.png)
",sir many time got histogram like normal distribution make wrong image,issue,negative,positive,neutral,neutral,positive,positive
345140703,The performance will get stable only after about 3k steps. The default settings will take 70~80 seconds per epoch. GPU utilization is 70%~80% with the current default setting.,performance get stable default take per epoch utilization current default setting,issue,negative,neutral,neutral,neutral,neutral,neutral
345140386,"Do you mind sharing the performance speed on 8 P100 when training Faster RCNN?

When using 8 gpus, I can only get ~200-300 seconds per epoch, and utilization of each one is about 50%-60%.
The `QueueInput/queue_size` is 46.96 in the log. Do you think it is related to prefetch? 
Thanks.
",mind performance speed training faster get per epoch utilization one log think related thanks,issue,negative,positive,neutral,neutral,positive,positive
345131846,"Interesting. Mine is P100. It might have something to do on how the new GPUs handle the fork. It works on old GPUs, though.
I'll take a deeper look when I got time. Meanwhile you can just disable the prefetch because data is not a bottleneck for detection.",interesting mine might something new handle fork work old though take look got time meanwhile disable data bottleneck detection,issue,negative,positive,positive,positive,positive,positive
345131520,"My machine has 8 v100 GPU and it is a bare metal machine.
All are `Compute Mode: Default`.",machine bare metal machine compute mode default,issue,negative,positive,neutral,neutral,positive,positive
345129393,Yes. I've seen the same error on one machine (but not the others) and use the same solution. I think that's because the GPU on that machine is in exclusive mode -- so using multiprocess may cause problems like this.,yes seen error one machine use solution think machine exclusive mode may cause like,issue,positive,neutral,neutral,neutral,neutral,neutral
345110652,"Hi，I meet the same problem in runing alxnet-dorefa.py ,and weights are 1 bit. In tensorboard I see a normal distribution weights too. When I follow your advice above, I also get  a normal distribution weights. 
![image](https://user-images.githubusercontent.com/20589365/32923680-cf0564f8-cb73-11e7-9ac8-48e889338359.png)

Should not the weights be just -1 or 1? Thanks a lot.
",meet problem bit see normal distribution follow advice also get normal distribution image thanks lot,issue,negative,positive,positive,positive,positive,positive
344858652,The [resnet example](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet) supports both data format. But you do need to learn some tensorflow before you can change the quantization function.,example data format need learn change quantization function,issue,negative,neutral,neutral,neutral,neutral,neutral
344856966,"thanks a lot.  I am a newer ,so I may be not change it by myself. Are you have changed any example such as restnet ,VGG ,and I will follow example to change alexnet? ",thanks lot may change example follow example change,issue,negative,positive,positive,positive,positive,positive
344853831,"You change your layers to NCHW. But your input is not in NCHW.
Also you may need to modify the quantization function since it may assume NHWC.",change input also may need modify quantization function since may assume,issue,negative,neutral,neutral,neutral,neutral,neutral
344839427,All models are hosted on google drive with limited space. VGG is much larger than any other models and I'm afraid I can't share that before we use a better place to host models. ,drive limited space much afraid ca share use better place host,issue,negative,negative,neutral,neutral,negative,negative
344264211,"Sure. Please also change the docstrings at top of the file, to include notes about the settings and the performance.",sure please also change top file include performance,issue,positive,positive,positive,positive,positive,positive
344263666,"yes, the code now  is a little messy,  I will clean it up",yes code little messy clean,issue,negative,negative,neutral,neutral,negative,negative
344262562,Good result! Is this still following the kuangliu/pytorch-cifar setting?,good result still following setting,issue,negative,positive,positive,positive,positive,positive
344130997,"the new preact18 architecture achieves 4.08, still exists a 1.08 difference.   =。=",new preact architecture still difference,issue,negative,positive,positive,positive,positive,positive
343980524,"they changed 0.9 to 1 in the original code, so for educational purposes would be a good idea to change as well ",original code educational would good idea change well,issue,positive,positive,positive,positive,positive,positive
343975994,"The current exception handling code piece is repeated for 3 times -- which means it needs to be extracted out to a common function, with an option that disables exception handling.",current exception handling code piece repeated time need extracted common function option exception handling,issue,negative,negative,negative,negative,negative,negative
343946728,"The original idea was to allow some errors to happen occasionally as long as the training is still going, because failing the whole training due to some dirty data can be frustrating.
But I think this is not a good default, due to reasons you mentioned. You can add an option and change the default behavior. Do you want to work on it?",original idea allow happen occasionally long training still going failing whole training due dirty data think good default due add option change default behavior want work,issue,negative,positive,neutral,neutral,positive,positive
343921877,"```diff
diff --git i/examples/ShuffleNet/shufflenet.py w/examples/ShuffleNet/shufflenet.py
index fa27ad1..42a890b 100755
--- i/examples/ShuffleNet/shufflenet.py
+++ w/examples/ShuffleNet/shufflenet.py
@@ -88,9 +88,9 @@ class Model(ImageNetModel):
         with argscope([Conv2D, MaxPooling, AvgPooling, GlobalAvgPooling, BatchNorm], data_format=self.data_format), \
                 argscope(Conv2D, use_bias=False):
             group = 8
-            channels = [224, 416, 832]
+            channels = [384, 768, 1536]
 
-            l = Conv2D('conv1', image, 16, 3, stride=2, nl=BNReLU)
+            l = Conv2D('conv1', image, 24, 3, stride=2, nl=BNReLU)
             l = MaxPooling('pool1', l, 3, 2, padding='SAME')
 
             with tf.variable_scope('group1'):
```
The above code runs without problems.

`(384-24) / 4 = 90 % 8 != 0.` this formula comes from nowhere. 
`384 % 16 == 0` is enough. `-24` only happens in the last conv of each unit.",git index class model group image image code without formula come nowhere enough last unit,issue,negative,neutral,neutral,neutral,neutral,neutral
343718786,"btw, by default training data won't be reset between epochs.",default training data wo reset,issue,negative,neutral,neutral,neutral,neutral,neutral
343466365,The dataflow will be stopped. The queue will be closed but not removed until you reset the graph.,stopped queue closed removed reset graph,issue,negative,negative,neutral,neutral,negative,negative
343465948,"@ppwwyyxx I forgot to ask, after finishing SyncMultiGPUTrainer(config).train(), the existing dataflow will stop and existing queue will be removed right? Sorry to bring this up after closing the issue.",forgot ask finishing stop queue removed right sorry bring issue,issue,negative,negative,negative,negative,negative,negative
343462302,"Ah that's a problem. If you use queues, there will definitely be several datapoints from the old distribution after you made any changes to the dataflow. 
To really avoid this you will need to 1. pause your dataflow (make it block) and 2. Eval the inputs with `sess.run` + timeout option, until it timeout so the queue is empty. Perhaps this can be made a builtin method of `QueueInput`.",ah problem use definitely several old distribution made really avoid need pause make block option queue empty perhaps made method,issue,negative,positive,neutral,neutral,positive,positive
343460903,"I solved the problem. Using '//' in the path broke the tensorflow loader, I don't know why and it seems silly. Here an example:

Above: /media/vqdang/Data_2/dang/output/miccai/v1.3.0_b//model-8.index
Fixed : /media/vqdang/Data_2/dang/output/miccai/v1.3.0_b/model-8.index

About the dataflow, is the queue reset every epoch? I haven't thought too much about that but the dataflow-callback way as in https://github.com/ppwwyyxx/tensorpack/blob/2c129dede46b8eb76e3f8a137d80d6accd4b48ff/examples/DeepQNetwork/expreplay.py#L109
looks more elegant and I want to try that. However, I need to make sure that from the next period, every samples used for training must be generated from the new distribution.",problem path broke loader know silly example fixed queue reset every epoch thought much way elegant want try however need make sure next period every used training must new distribution,issue,negative,positive,positive,positive,positive,positive
343430554,"The way you're using should work. The error is saying the model path is wrong. Any chance you may have removed the checkpoint by mistake?

Everything other than the training can be done in callbacks. You can write a callback which, when triggered, run the evaluation (or get previous evaluation results) and change `train_dataflow.dist`. You can have the DataFlow be a subclass of Callback as well. In reinforcement learning the data always depend on the training, so this has been used in both RL examples, e.g. in DQN [dataflow is a callback](https://github.com/ppwwyyxx/tensorpack/blob/2c129dede46b8eb76e3f8a137d80d6accd4b48ff/examples/DeepQNetwork/expreplay.py#L109).",way work error saying model path wrong chance may removed mistake everything training done write triggered run evaluation get previous evaluation change subclass well reinforcement learning data always depend training used,issue,negative,negative,negative,negative,negative,negative
343393101,"This is not a tensorpack-related issue.

ShuffleNet is designed to have low-flops with good accuracy. Low-flops doesn't mean faster. To be fast you'll need good kernels (mainly for group conv) which tensorflow doesn't have.",issue designed good accuracy mean faster fast need good mainly group,issue,positive,positive,positive,positive,positive,positive
343376837,"Briefly talked to the author. Their ""CIFAR-10 PreAct ResNet-18"" is the architecture [here](https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py), not exactly what we have in the examples.",briefly author preact architecture exactly,issue,negative,positive,positive,positive,positive,positive
343368377,"Sorry but we hope to only include examples with [reproducible performance](https://github.com/ppwwyyxx/tensorpack/tree/master/examples#speech--nlp). It's hard and that's why it's important. Otherwise it becomes a big collection of code that appears useful but frustrates the users, like most of the deep learning code I've seen on github. Pretty much everyone knows how to write a new model, but the hard part is in achieving the performance.
In anycase a README is a must. Without a result that people can reproduce, open source deep learning code is of no meaning. ",sorry hope include reproducible performance hard important otherwise becomes big collection code useful like deep learning code seen pretty much everyone write new model hard part performance must without result people reproduce open source deep learning code meaning,issue,positive,positive,neutral,neutral,positive,positive
343363725,"if I set alpha=1, I can obtain a 4.74 result. could you merge the code first?  

I will setup a README file to clarify the difference between the paper. hope me or someone else interested can fix it later.
",set obtain result could merge code first setup file clarify difference paper hope someone else interested fix later,issue,positive,positive,positive,positive,positive,positive
343123209,"Any valid tensorflow symbolic code is valid in tensorpack. You can just use it the same way. Just remember that your model code might get called multiple times if trained with multiple GPUs.

tensorpack trainers don't use the loss collection for training. You can use it for your own purpose, of course.",valid symbolic code valid use way remember model code might get multiple time trained multiple use loss collection training use purpose course,issue,negative,neutral,neutral,neutral,neutral,neutral
343111636,"btw, `MultiProcessDatasetPredictor` doesn't scale very well. Doing inference is not a focus of the library and everything that predicts offline in tensorpack is `feed_dict` based and not well optimized.",scale well inference focus library everything based well,issue,negative,neutral,neutral,neutral,neutral,neutral
343068758,"In the paper, ""CIFAR-10 PreAct ResNet-18"" error goes from 5.6 to 3.9",paper preact error go,issue,negative,neutral,neutral,neutral,neutral,neutral
343062951,"I guess the real question is how do you clip the gradients.
If you could just remove the None values before clipping you won't have the problem.",guess real question clip could remove none clipping wo problem,issue,negative,positive,positive,positive,positive,positive
342811051,"Done. See updated readme. https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ShuffleNet

Only after uploading the model I've realized how small it is...",done see model small,issue,negative,negative,negative,negative,negative,negative
342809954,"The [tutorial](http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html) has more information about how to find the bottleneck.

`PrefetchData` is not efficient especially for large data and you'd better use `PrefetchDataZMQ` whenever possible. [docs](http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.PrefetchData)

I don't know why it uses large VM but in my experience tensorflow always uses large VM. I don't think that's a big problem. Just using 4 TitanX alone would've accounted for 48G VM space already.",tutorial information find bottleneck efficient especially large data better use whenever possible know large experience always large think big problem alone would space already,issue,negative,positive,positive,positive,positive,positive
342742713,"pip uninstall tensorpack
python setup.py install

problem solved!!",pip python install problem,issue,negative,neutral,neutral,neutral,neutral,neutral
342531679,"Write your own dataflow for it. In that dataflow you can call `_get_augment_params(image)` and then `_augment(image, params)`. 
Or you don't have to write your augmentation under the `imgaug` API, it can be written more casually since you need your own dataflow anyway.",write call image image write augmentation written casually since need anyway,issue,negative,negative,negative,negative,negative,negative
342512941,"In tensorfllow, '/gpu:0' and '/gpu:1' means the first and second GPU in `CUDA_VISIBLE_DEVICES`, as long as you set the environment variable.",first second long set environment variable,issue,negative,positive,neutral,neutral,positive,positive
342512255,"Setting GPU 2,3 on a cluster with 4 GPUs starts training and building /gpu:0 and /gpu:1. I'm not admin for these machines. Would be also nice to have a choice on which device to train when using only one GPU on a multi-GPU cluster.",setting cluster training building would also nice choice device train one cluster,issue,negative,positive,positive,positive,positive,positive
342382582,"'DumpTensors' and 'ProcessTensors' cannot be recognized by Tensorpack somewhat. I have the following error when trying using them.
```
ImportError: cannot import name DumpTensors
```",somewhat following error trying import name,issue,negative,neutral,neutral,neutral,neutral,neutral
342260192,"Good! How do I check input images in each iteration while training so that I can make sure it is going correctly? I know that in evaluation I can do so by passing the variable name to ""PredictConfig"", but I couldn't find such a interface in training.",good check input iteration training make sure going correctly know evaluation passing variable name could find interface training,issue,positive,positive,positive,positive,positive,positive
342143316,Ah my bad. `dataflow.get_data()` actually won't be called at epoch beginning -- input source already takes care of the iterator state when sending them to the graph. So the iterator state will be preserved and dataflow won't start over at each training epoch.,ah bad actually wo epoch beginning input source already care state sending graph state wo start training epoch,issue,negative,negative,negative,negative,negative,negative
342141517,"[A section](http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html#cannot-scale-to-multi-gpu) about debugging scalability was added to the documentation recently. Without more details no more suggestions can be given. Closing this thread now, but if you have more questions feel free to reopen.",section added documentation recently without given thread feel free reopen,issue,positive,positive,positive,positive,positive,positive
342027419,"Oh yes, it's my mistake. It is in traing.

<img width=""1440"" alt=""screen shot 2017-11-06 at 10 02 02 am"" src=""https://user-images.githubusercontent.com/3326649/32422185-b8ce4b84-c2d9-11e7-8f84-bcfc5fe8c586.png"">
",oh yes mistake screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
342026819,"@lbin You mean in training?
And 127 epochs not steps? This sounds like a different problem.",mean training like different problem,issue,negative,negative,negative,negative,negative,negative
341887187,I guess this is a bit counter-intuitive and may cause subtle error. I'll see if I can change this somehow.,guess bit may cause subtle error see change somehow,issue,negative,negative,negative,negative,negative,negative
341887057,"It will call `dataflow.get_data()` again, and it depends on the `get_data` implementation whether it will start over. For most dataflow it will start over.
You can use [FixedSizeData](http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.FixedSizeData) to keep the iteration state.",call implementation whether start start use keep iteration state,issue,negative,neutral,neutral,neutral,neutral,neutral
341886862,"One more question is if I set 'steps_per_epoch' smaller than the actual epoch size of the training dataset (e.g. actual epoch size for imagenet with batch size 100 is about 13000), then at the second epoch, will 'Dataflow' continue fetching the data from where it stops at the first epoch or start all over again? I'm afraid that if it is the latter case, some data will never be trained if I use a pre-shuffling strategy.",one question set smaller actual epoch size training actual epoch size batch size second epoch continue fetching data first epoch start afraid latter case data never trained use strategy,issue,negative,negative,neutral,neutral,negative,negative
341886458,"Thanks for your suggestion. I have resolved the problem. I didn't shuffle the training data when generating the LMDB file and only used the local shuffling strategy when reading the LDMB file. Consequently, only a couple of categories are trained in each epoch, which results in bad generalization results. ",thanks suggestion resolved problem shuffle training data generating file used local shuffling strategy reading file consequently couple trained epoch bad generalization,issue,negative,negative,negative,negative,negative,negative
341699510,Thanks very much for finding this subtle bug. I think your solution makes sense.,thanks much finding subtle bug think solution sense,issue,positive,positive,neutral,neutral,positive,positive
341670950,My best guess is that you use some inappropriate data preprocessing in training. But I can't help more since there is no code I can run and it's unlikely a tensorpack issue.,best guess use inappropriate data training ca help since code run unlikely issue,issue,positive,positive,positive,positive,positive,positive
341643381,Looks like a python2 only issue. I'll fix it soon. Next time please include relevant version information for such bugs.,like python issue fix soon next time please include relevant version information,issue,positive,positive,positive,positive,positive,positive
341622633,Sorry I mean a quantized version (binary) for DoReFa-Net.,sorry mean version binary,issue,negative,negative,negative,negative,negative,negative
341615874,"Thanks Yuxin, this is very helpful. I'll look into it and let you know. ",thanks helpful look let know,issue,positive,positive,positive,positive,positive,positive
341615217,"It's probably an issue in lower level. Maybe specific to driver or GPU and their interaction with softwares like cudnn or tensorflow. I know some version of driver can cause CPU memory leak with cudnn.

Another related environment variable is `TF_CUDNN_WORKSPACE_LIMIT_IN_MB` which is default to 4G.  Although ideally it shouldn't be a problem.
Also the related code for ""no-autotune"" (i.e. default algorithm) is https://github.com/tensorflow/tensorflow/blob/6a8322f6dc007573e97a452e056b76f2be4794a7/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2200-L2262 (assuming you don't have XLA or JIT enabled, otherwise the code path might be different), which indeed has something to do with memory, and you might find some clues from the error log if you wish. But unfortunately I cannot reproduce it, so I can't help much.",probably issue lower level maybe specific driver interaction like know version driver cause memory leak another related environment variable default although ideally problem also related code default algorithm assuming otherwise code path might different indeed something memory might find error log wish unfortunately reproduce ca help much,issue,negative,positive,neutral,neutral,positive,positive
341592143,"Seems that it has something to do with auto tuning. I **commented** this line 

`os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'`

to enable tensorflow auto tuning and the issue is gone. Otherwise, the GPU memory keeps growing with number of evaluation steps until OOM. 

My current tensorflow version is 1.4.0 and cudnn version is 6.0.21  ",something auto tuning line enable auto tuning issue gone otherwise memory growing number evaluation current version version,issue,negative,neutral,neutral,neutral,neutral,neutral
341441507,"@ppwwyyxx   That's exactly what i want, I need  to know whether I am in training mode or testing mode ,  and when I am in testing mode, I need change the network a little bit, Just like what dropout does! ",exactly want need know whether training mode testing mode testing mode need change network little bit like dropout,issue,negative,positive,neutral,neutral,positive,positive
341438987,"I'm not sure I understand what do you mean by ""testing"".
I assume you want to use the model in testing mode __during training__, with the `get_predictor` or `InferenceRunner` API. If that is the case you can just write the model differently for testing, like this:
 https://github.com/ppwwyyxx/tensorpack/blob/9f6b58d547c74e61422bf0a3f99565009b2dc500/examples/FasterRCNN/train.py#L93-L94",sure understand mean testing assume want use model testing mode case write model differently testing like,issue,positive,positive,neutral,neutral,positive,positive
341370805,"Tensorflow saver by default will delete some old models: https://www.tensorflow.org/api_docs/python/tf/train/Saver#__init__

> max_to_keep: Maximum number of recent checkpoints to keep. Defaults to 5.
keep_checkpoint_every_n_hours: How often to keep checkpoints. Defaults to 10,000 hours

You can change the behavior by the same options in `ModelSaver`: http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.ModelSaver",saver default delete old maximum number recent keep often keep change behavior,issue,negative,positive,neutral,neutral,positive,positive
341370049,"I can't reproduce this problem either.  😢 
Evaluating the model shouldn't use much memory, and since you can eval it during training I have no idea why this happens. 
If you're sure no one else is using the GPU at the same time, maybe you can try a different version of tensorflow/cudnn.",ca reproduce problem either model use much memory since training idea sure one else time maybe try different version,issue,negative,positive,positive,positive,positive,positive
341319813,"Hi Yuxin,

Yes, upgrading to the latest master solved this problem. Thanks, and great work! ",hi yes latest master problem thanks great work,issue,positive,positive,positive,positive,positive,positive
341303504,"Hi Bichen,
Seems like I can't reproduce this problem. Could you update tensorpack to latest  master and try again? (with `pip install -U git+https://github.com/ppwwyyxx/tensorpack.git`). tensorpack is going through some major changes recently, so there are some unstable versions.",hi like ca reproduce problem could update latest master try pip install going major recently unstable,issue,negative,positive,positive,positive,positive,positive
340968776,"Btw, as mentioned in the README, running with many GPUs doesn't give you much benefits.",running many give much,issue,negative,positive,positive,positive,positive,positive
340967425,"It's a long-standing issue when data depends on the model and may get dead-locked during initialization. Now you can run it with (<=2) GPU (`--gpu 0,1`) but not more. I'll try to fix it.",issue data model may get run try fix,issue,negative,neutral,neutral,neutral,neutral,neutral
340965924,"But wait, what is the fix? I tried to import other callbacks from base.py and trigger.py instead of Triggerable, but so far the learning seems to be stuck at  `Pre-filling staging area ...`",wait fix tried import instead far learning stuck staging area,issue,negative,positive,neutral,neutral,positive,positive
340964233,"95%~97% speed of existing trainers on single machine.
Don't have the resource to try distributed now. Not sure it works..",speed single machine resource try distributed sure work,issue,negative,positive,positive,positive,positive,positive
340940767,"74ca05d, 1a262e8 Awesome! Any benchmarks :wink: ?",cad ae awesome wink,issue,positive,positive,positive,positive,positive,positive
340334426,Fused batch norm do have 2nd order gradient IIRC. If not you should report this to tensorflow.,fused batch norm order gradient report,issue,negative,neutral,neutral,neutral,neutral,neutral
340211404,"About the original question, after latest changes 897d29e to the trainer, using ""cost"" would be sufficient. ""tower0/cost"" also works.",original question latest de trainer cost would sufficient also work,issue,negative,positive,positive,positive,positive,positive
340211318,"About the original question, after latest changes 897d29e3f1dbc5 to the trainer, using ""cost"" would be sufficient. ""tower0/cost"" also works.",original question latest trainer cost would sufficient also work,issue,negative,positive,positive,positive,positive,positive
340188168,"Sorry! I mistake the data layout with other implementation.
So there is no issue of ""tensorpack vs the other implementation"", and thanks again for your kindness!",sorry mistake data layout implementation issue implementation thanks kindness,issue,negative,negative,negative,negative,negative,negative
340187657,"You said that the ""other implementation"" use the same data layout, right?
The issue is ""tensorpack vs the other implementation"", not ""fused vs non-fused"", because non-fused is slow as expected.

The resnet example in tensorpack support both data layout.",said implementation use data layout right issue implementation fused slow example support data layout,issue,negative,negative,neutral,neutral,negative,negative
340185396,"As you said, the reason is the data format with NCHW. https://github.com/tensorflow/tensorflow/issues/9141
Is there any problem occurred if we transform the data format at data layer to NHWC in tensorpack?",said reason data format problem transform data format data layer,issue,negative,neutral,neutral,neutral,neutral,neutral
340185005,Actually we try to do something like renormalization in tensorpack.,actually try something like,issue,negative,neutral,neutral,neutral,neutral,neutral
340178305,"With the new trainer API, we can answer the problems at the beginning:
> Is it OK to create placeholders inside build_graph?
What symbolic functions are allowed to use and what not? (e.g. tf.layers.batch_norm? tf.train.input_producer?)..

You can use anything as long as your training iteration can still run. e.g., if you use a placeholder, you need a callback to feed it when the placeholder is a dependency. If you use tensorflow input ops, you may need a callback to start the queues.

> What to put in get_inputs and what not? Is this interface even necessary?

They are used only to setup `InputSource` in `SingleCostTrainer` as well as `InferenceRunner`. You don't have to use `InputDesc` if not working with `InputSource`. But you'd better use `InputSource` for performance reasons.

> How to access a tensor a bit later? Because setting self.xxx sadly doesn't work (#287), and using the tensor names is not easy.

A new abstraction called `TowerTrainer` will cover models whose graph can be built by calling a function (tower function) with input tensors one or more times. `SingleCostTrainer` belongs to `TowerTrainer`. 
The tower function will keep track of the scopes every time it get called, and then the tensors can be accessed. The API is still not ready yet, but should be something like `trainer.tower_func.towers[0]['cost']`, or `trainer.tower_func.towers['tower0']['cost']`.
Of course, you can only access the tenors after the graph has been built. So if using high-level one-shot training wrappers, you will only have the chance to use this interface inside `callback.setup_graph`.

> On the contrary, self.cost needs to be set. This seems very hard-coded, and the reason behind it is that self.cost is only set because some (but not all) trainers need it. This contract between Model and Trainer needs to be addressed in a clearer way.

It is hard-coded. We'll keep it this way. But now `SingleCostTrainer` takes a function which explicitly returns one cost, i.e., the hidden contract become an API requirement. Other trainer doesn't have any notion about ""cost"".",new trainer answer beginning create inside symbolic use use anything long training iteration still run use need feed dependency use input may need start put interface even necessary used setup well use working better use performance access tensor bit later setting sadly work tensor easy new abstraction cover whose graph built calling function tower function input one time tower function keep track every time get still ready yet something like course access graph built training chance use interface inside contrary need set reason behind set need contract model trainer need clearer way keep way function explicitly one cost hidden contract become requirement trainer notion cost,issue,positive,positive,neutral,neutral,positive,positive
340168839,"> With nn.moments they calculate bm and bv, and update bm and bv with moving_mean and moving_var, after which they are fed to non-fused batch norm to output xn in training.

Thanks. This looks like standard (if not the only) way to use non-fused BN. How do you do it differently in tensorpack?",calculate update fed batch norm output training thanks like standard way use differently,issue,positive,positive,neutral,neutral,positive,positive
340167153,"With nn.moments they calculate bm and bv, and update bm and bv with moving_mean and moving_var, after which they are fed to non-fused batch norm to output xn in training.
Ok, we try to collect details later and thanks a lot to your patience.",calculate update fed batch norm output training try collect later thanks lot patience,issue,negative,positive,neutral,neutral,positive,positive
340166498,"> The only difference is they use moving_mean and moving_var in training for which they need to use moment&batch_normalization
I don't understand from this sentence what the difference is. AFAIK nn.moments doesn't give you moving_mean and moving_var.

As I said, if you want this to be an issue about ""tensorpack slower than others"", I would need much more details.",difference use training need use moment understand sentence difference give said want issue would need much,issue,negative,positive,positive,positive,positive,positive
340166310,"Ah, actually it is multi-GPU training with 8 TITAN X. And we've checked that the batch size and data layout are as same as yours. The only difference is they use moving_mean and moving_var in training for which they need to use moment&batch_normalization. I'm really confused that fused_batch_norm is significantly 6 more times faster than moment&batch_normalization. What's more, we observed a poor GPU utilization with moment&batch_normalization which is around 100% when using fused_batch_norm.",ah actually training checked batch size data layout difference use training need use moment really confused significantly time faster moment poor utilization moment around,issue,negative,negative,negative,negative,negative,negative
340165816,"One possible explanation, for example, is that non-fused batch norm is faster with NHWC (according to https://github.com/tensorflow/tensorflow/issues/12419) than NCHW. Our resnet uses fused batch norm and therefore NCHW. If the other code uses NHWC, then you're not comparing apple-to-apple.",one possible explanation example batch norm faster according fused batch norm therefore code,issue,negative,neutral,neutral,neutral,neutral,neutral
340165693,"""tensorpack slower than others"" is a potential performance issue then. To get more serious you'd better also include what you did (command and settings), what you observed (logs and speed numbers), if possible.",potential performance issue get serious better also include command speed possible,issue,negative,positive,neutral,neutral,positive,positive
340165203,"What's the ""other resnet implementation"" you're talking about? I would love to know.
Btw, tensorpack reaches about the [best possible](https://github.com/tensorpack/benchmarks/tree/master/ResNet-MultiGPU) resnet performance with tensorflow (with fused batch norm, or course). Comparing the wrong setting (non-fused batch norm) isn't of much interest to me. 
But I still hope to take a look in your case to see if there's an simple answer. My doubt would be there are other factors that affect your conclusion, e.g. not comparing the same model with the same setting, bottlenecked by data, etc.",implementation talking would love know best possible performance fused batch norm course wrong setting batch norm much interest still hope take look case see simple answer doubt would affect conclusion model setting data,issue,positive,positive,positive,positive,positive,positive
340164983,"Ok, I got it that fused_batch_norm is faster than batch_normalization, but my point is that in other resnet implemented framework, which use moment + batch_normalization, their speed is 2 more times faster than your code with moment + batch_normalization.",got faster point framework use moment speed time faster code moment,issue,negative,neutral,neutral,neutral,neutral,neutral
340161768,"But many other platforms implementing ResNets use tf.moments + batch_normalization instead of fused_batch_norm, such as the code training from scratch in Tensorflow official webpage, their speed is not so slow. Any communication delay problem in your code when using tf.moments + batch_normalization?",many use instead code training scratch official speed slow communication delay problem code,issue,negative,positive,neutral,neutral,positive,positive
340159706,"> Could you post what speed have you seen on what GPUs training what models?

Could you post for 1, 2 and more GPUs to show the problems you've met?",could post speed seen training could post show met,issue,negative,neutral,neutral,neutral,neutral,neutral
340134627,"![image](https://user-images.githubusercontent.com/6397103/32130621-aa41b172-bb50-11e7-9c18-a1ca2329af98.png)
This is training progress. Currently I am using 3 gpus and training a MobileNet like model on k80.",image training progress currently training like model,issue,positive,neutral,neutral,neutral,neutral,neutral
340126174,"Could you post what speed have you seen on what GPUs training what models?
Also see http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html",could post speed seen training also see,issue,negative,neutral,neutral,neutral,neutral,neutral
340081420,"I think it is the problem of tensorflow versions. I tried tf1.3. It is working now. But one more question, two GPU training is the fastest. When I try more gpus, the speed decreases significantly. Is there anything I can do to promote more gpus?",think problem tried working one question two training try speed significantly anything promote,issue,negative,positive,positive,positive,positive,positive
340018669,"but I have use this code os.environ['TENSORPACK_PIPEDIR'] = '/tmp'
and also export TENSORPACK_PIPEDIR=/tmp in bash

Should I do something else?",use code also export bash something else,issue,negative,neutral,neutral,neutral,neutral,neutral
340006678,"`--load` a model file, not the directory.",load model file directory,issue,negative,neutral,neutral,neutral,neutral,neutral
339907417,"Your log does not contain the error you mentioned.

From your log it looks like the same issue about zmq pipe dir.",log contain error log like issue pipe,issue,negative,neutral,neutral,neutral,neutral,neutral
339906344,We don't discuss general (non-tensorpack) machine learning questions like this in the issue. ,discus general machine learning like issue,issue,negative,positive,neutral,neutral,positive,positive
339905182,"Sure. I didn't use sqlite reader. I just follow the imagenet-resnet and give the dataflow the address of my data.

[32m[1027 00:20:33 @logger.py:74][0m Argv: main.py
[32m[1027 00:20:33 @tensor_net.py:46][0m Running on 2 towers. Batch size per tower: 64
[32m[1027 00:20:33 @fs.py:89][0m [5m[31mWRN[0m Env var $TENSORPACK_DATASET not set, using /home/hgao/tensorpack_data for datasets.
[32m[1027 00:20:34 @prefetch.py:263][0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[1027 00:20:34 @ilsvrc.py:118][0m Assuming directory /tempspace2/hgao/data/imagenet/val has original structure.
[32m[1027 00:20:34 @param.py:189][0m Use ./logdir1/hyper.txt to set hyperparam: 'learning_rate'.
[32m[1027 00:20:34 @inference_runner.py:83][0m InferenceRunner will eval on an InputSource of size 782
[32m[1027 00:21:08 @input_source.py:178][0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...
[32m[1027 00:21:08 @input_source.py:459][0m Setting up StagingArea for GPU prefetching ...
[32m[1027 00:21:08 @training.py:41][0m Training a model of 2 towers
[32m[1027 00:21:08 @training.py:92][0m Building graph for training tower 0 on device LeastLoadedDeviceSetter-/gpu:0...
[32m[1027 00:21:09 @regularize.py:108][0m Add REGULARIZATION_LOSSES of 58 tensors on the total cost.
[32m[1027 00:21:11 @training.py:92][0m Building graph for training tower 1 on device LeastLoadedDeviceSetter-/gpu:1...
[32m[1027 00:21:11 @regularize.py:108][0m Add REGULARIZATION_LOSSES of 58 tensors on the total cost.
[32m[1027 00:21:13 @model_utils.py:47][0m [36mModel Parameters: 
[0mname                                            shape                   dim  device
----------------------------------------------  ------------------  -------  -------------
conv_s/weights:0                                [3, 3, 3, 32]           864  /device:GPU:0
conv_s/batch_norm/gamma:0                       [32]                     32  /device:GPU:1
conv_s/batch_norm/beta:0                        [32]                     32  /device:GPU:1
conv_1_0/conv1/conv/weights:0                   [3, 3, 32, 1]           288  /device:GPU:1
conv_1_0/conv1/batch_norm/gamma:0               [32]                     32  /device:GPU:1
conv_1_0/conv1/batch_norm/beta:0                [32]                     32  /device:GPU:1
conv_1_0/conv2/weights:0                        [1, 1, 32, 64]         2048  /device:GPU:1
conv_1_0/conv2/batch_norm/gamma:0               [64]                     64  /device:GPU:0
conv_1_0/conv2/batch_norm/beta:0                [64]                     64  /device:GPU:0
conv_1_1/conv1/conv/weights:0                   [3, 3, 64, 1]           576  /device:GPU:0
conv_1_1/conv1/batch_norm/gamma:0               [64]                     64  /device:GPU:0
conv_1_1/conv1/batch_norm/beta:0                [64]                     64  /device:GPU:0
conv_1_1/conv2/weights:0                        [1, 1, 64, 128]        8192  /device:GPU:0
conv_1_1/conv2/batch_norm/gamma:0               [128]                   128  /device:GPU:1
conv_1_1/conv2/batch_norm/beta:0                [128]                   128  /device:GPU:1
conv_1_2/conv1/conv/weights:0                   [3, 3, 128, 1]         1152  /device:GPU:1
conv_1_2/conv1/batch_norm/gamma:0               [128]                   128  /device:GPU:1
conv_1_2/conv1/batch_norm/beta:0                [128]                   128  /device:GPU:1
conv_1_2/conv2/weights:0                        [1, 1, 128, 128]      16384  /device:GPU:1
conv_1_2/conv2/batch_norm/gamma:0               [128]                   128  /device:GPU:0
conv_1_2/conv2/batch_norm/beta:0                [128]                   128  /device:GPU:0
conv_1_3/conv1/conv/weights:0                   [3, 3, 128, 1]         1152  /device:GPU:0
conv_1_3/conv1/batch_norm/gamma:0               [128]                   128  /device:GPU:0
conv_1_3/conv1/batch_norm/beta:0                [128]                   128  /device:GPU:0
conv_1_3/conv2/weights:0                        [1, 1, 128, 256]      32768  /device:GPU:0
conv_1_3/conv2/batch_norm/gamma:0               [256]                   256  /device:GPU:1
conv_1_3/conv2/batch_norm/beta:0                [256]                   256  /device:GPU:1
conv_1_4/conv1/conv/weights:0                   [3, 3, 256, 1]         2304  /device:GPU:1
conv_1_4/conv1/batch_norm/gamma:0               [256]                   256  /device:GPU:1
conv_1_4/conv1/batch_norm/beta:0                [256]                   256  /device:GPU:1
conv_1_4/conv2/weights:0                        [1, 1, 256, 256]      65536  /device:GPU:1
conv_1_4/conv2/batch_norm/gamma:0               [256]                   256  /device:GPU:0
conv_1_4/conv2/batch_norm/beta:0                [256]                   256  /device:GPU:0
conv_1_5/conv1/conv/weights:0                   [3, 3, 256, 1]         2304  /device:GPU:0
conv_1_5/conv1/batch_norm/gamma:0               [256]                   256  /device:GPU:0
conv_1_5/conv1/batch_norm/beta:0                [256]                   256  /device:GPU:0
conv_1_5/conv2/weights:0                        [1, 1, 256, 512]     131072  /device:GPU:0
conv_1_5/conv2/batch_norm/gamma:0               [512]                   512  /device:GPU:1
conv_1_5/conv2/batch_norm/beta:0                [512]                   512  /device:GPU:1
conv_2/group_0_conv0/conv/weights:0             [1, 1, 4, 1, 1]           4  /device:GPU:1
conv_2/group_0/conv_0/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_0/conv_0/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_0/conv_0/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_0/conv_0/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_0/conv_0/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_0/conv_0/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_0/conv_1/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_0/conv_1/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_0/conv_1/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_0/conv_1/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_0/conv_1/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_0/conv_1/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_0/conv_2/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_0/conv_2/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_0/conv_2/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_0/conv_2/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_0/conv_2/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_0/conv_2/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_0/conv_3/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_0/conv_3/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_0/conv_3/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_0/conv_3/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_0/conv_3/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_0/conv_3/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_0/conv_4/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_0/conv_4/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_0/conv_4/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_0/conv_4/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_0/conv_4/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_0/conv_4/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_1_conv0/conv/weights:0             [1, 1, 4, 1, 1]           4  /device:GPU:0
conv_2/group_1/conv_0/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0
conv_2/group_1/conv_0/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_1/conv_0/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_1/conv_0/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0
conv_2/group_1/conv_0/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_1/conv_0/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_1/conv_1/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_1/conv_1/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_1/conv_1/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_1/conv_1/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_1/conv_1/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_1/conv_1/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_1/conv_2/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0
conv_2/group_1/conv_2/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_1/conv_2/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_1/conv_2/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0
conv_2/group_1/conv_2/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_1/conv_2/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_1/conv_3/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_1/conv_3/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_1/conv_3/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_1/conv_3/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_1/conv_3/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_1/conv_3/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_1/conv_4/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0
conv_2/group_1/conv_4/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_1/conv_4/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_1/conv_4/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0
conv_2/group_1/conv_4/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_1/conv_4/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_2_conv0/conv/weights:0             [1, 1, 4, 1, 1]           4  /device:GPU:1
conv_2/group_2/conv_0/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_2/conv_0/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_2/conv_0/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_2/conv_0/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_2/conv_0/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_2/conv_0/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_2/conv_1/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0
conv_2/group_2/conv_1/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_2/conv_1/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_2/conv_1/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0
conv_2/group_2/conv_1/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_2/conv_1/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_2/conv_2/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_2/conv_2/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_2/conv_2/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_2/conv_2/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_2/conv_2/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_2/conv_2/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_2/conv_3/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0
conv_2/group_2/conv_3/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_2/conv_3/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_2/conv_3/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0
conv_2/group_2/conv_3/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_2/conv_3/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_2/conv_4/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_2/conv_4/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_2/conv_4/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_2/conv_4/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_2/conv_4/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_2/conv_4/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_3_conv0/conv/weights:0             [1, 1, 4, 1, 1]           4  /device:GPU:0
conv_2/group_3/conv_0/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0
conv_2/group_3/conv_0/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_3/conv_0/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_3/conv_0/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0
conv_2/group_3/conv_0/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_3/conv_0/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_3/conv_1/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_3/conv_1/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_3/conv_1/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_3/conv_1/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_3/conv_1/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_3/conv_1/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_3/conv_2/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0
conv_2/group_3/conv_2/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_3/conv_2/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_3/conv_2/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0
conv_2/group_3/conv_2/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_3/conv_2/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_3/conv_3/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1
conv_2/group_3/conv_3/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_3/conv_3/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_2/group_3/conv_3/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1
conv_2/group_3/conv_3/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_3/conv_3/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_3/conv_4/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0
conv_2/group_3/conv_4/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0
conv_2/group_3/conv_4/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0
conv_2/group_3/conv_4/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0
conv_2/group_3/conv_4/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1
conv_2/group_3/conv_4/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1
conv_3_0/conv1/conv/weights:0                   [3, 3, 512, 1]         4608  /device:GPU:1
conv_3_0/conv1/batch_norm/gamma:0               [512]                   512  /device:GPU:1
conv_3_0/conv1/batch_norm/beta:0                [512]                   512  /device:GPU:1
conv_3_0/conv2/weights:0                        [1, 1, 512, 1024]    524288  /device:GPU:1
conv_3_0/conv2/batch_norm/gamma:0               [1024]                 1024  /device:GPU:0
conv_3_0/conv2/batch_norm/beta:0                [1024]                 1024  /device:GPU:0
conv_3_1/conv1/conv/weights:0                   [3, 3, 1024, 1]        9216  /device:GPU:0
conv_3_1/conv1/batch_norm/gamma:0               [1024]                 1024  /device:GPU:0
conv_3_1/conv1/batch_norm/beta:0                [1024]                 1024  /device:GPU:0
conv_3_1/conv2/weights:0                        [1, 1, 1024, 1024]  1048576  /device:GPU:0
conv_3_1/conv2/batch_norm/gamma:0               [1024]                 1024  /device:GPU:1
conv_3_1/conv2/batch_norm/beta:0                [1024]                 1024  /device:GPU:1
out/pool/batch_norm/gamma:0                     [1024]                 1024  /device:GPU:1
out/pool/batch_norm/beta:0                      [1024]                 1024  /device:GPU:1
out/dense/weights:0                             [1024, 1000]        1024000  /device:GPU:1
out/dense/biases:0                              [1000]                 1000  /device:GPU:0[36m
Total #vars=179, #param=3251000 (12.40 MB assuming all float32)[0m
[32m[1027 00:21:13 @base.py:207][0m Setup callbacks graph ...
[32m[1027 00:21:15 @input_source.py:178][0m Setting up the queue 'DataParallelInferenceRunner/QueueInput/input_queue' for CPU prefetching ...
[32m[1027 00:21:15 @predictor_factory.py:54][0m Building predictor tower 'InferenceTower0' on device /gpu:0 ...
[32m[1027 00:21:15 @predictor_factory.py:54][0m Building predictor tower 'InferenceTower1' on device /gpu:1 ...
[32m[1027 00:21:16 @summary.py:34][0m Maintain moving average summary of 4 tensors.
[32m[1027 00:21:16 @graph.py:91][0m Applying collection UPDATE_OPS of 232 ops.
[32m[1027 00:21:19 @base.py:212][0m Creating the session ...
[32m[1027 00:21:22 @base.py:216][0m Initializing the session ...
[32m[1027 00:21:22 @base.py:223][0m Graph Finalized.
[32m[1027 00:21:24 @concurrency.py:36][0m Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...
[32m[1027 00:21:24 @concurrency.py:36][0m Starting EnqueueThread QueueInput/input_queue ...
[32m[1027 00:21:24 @input_source.py:418][0m Pre-filling staging area ...
[32m[1027 00:21:25 @common.py:140][0m [4m[5m[31mERR[0m Cannot batch data. Perhaps they are of inconsistent shape?
Traceback (most recent call last):
  File ""/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py"", line 136, in _aggregate_batch
    np.asarray([x[k] for x in data_holder], dtype=tp))
  File ""/tempspace/hgao/py3.6/lib/python3.6/site-packages/numpy/core/numeric.py"", line 531, in asarray
    return array(a, dtype, copy=False, order=order)
ValueError: setting an array element with a sequence.
[32m[1027 00:21:25 @common.py:143][0m [4m[5m[31mERR[0m Shape of all arrays to be batched: [(64, 224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3),
 (224, 224, 3)]
[32m[1027 00:22:20 @common.py:140][0m [4m[5m[31mERR[0m Cannot batch data. Perhaps they are of inconsistent shape?
Traceback (most recent call last):
  File ""/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py"", line 136, in _aggregate_batch
    np.asarray([x[k] for x in data_holder], dtype=tp))
  File ""/tempspace/hgao/py3.6/lib/python3.6/site-packages/numpy/core/numeric.py"", line 531, in asarray
    return array(a, dtype, copy=False, order=order)
ValueError: setting an array element with a sequence.
[32m[1027 00:22:20 @input_source.py:140][0m [4m[5m[31mERR[0m Exception in EnqueueThread QueueInput/input_queue:
Traceback (most recent call last):
  File ""/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py"", line 136, in _aggregate_batch
    np.asarray([x[k] for x in data_holder], dtype=tp))
  File ""/tempspace/hgao/py3.6/lib/python3.6/site-packages/numpy/core/numeric.py"", line 531, in asarray
    return array(a, dtype, copy=False, order=order)
ValueError: setting an array element with a sequence.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/input_source/input_source.py"", line 130, in run
    for dp in self.dataflow.get_data():
  File ""/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py"", line 110, in get_data
    yield BatchData._aggregate_batch(holder, self.use_list)
  File ""/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py"", line 142, in _aggregate_batch
    s = pprint.pformat([x[k].shape for x in data_holder])
  File ""/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py"", line 142, in <listcomp>
    s = pprint.pformat([x[k].shape for x in data_holder])
AttributeError: 'int' object has no attribute 'shape'
[32m[1027 00:22:20 @base.py:273][0m Training was stopped.
[32m[1027 00:22:20 @input_source.py:146][0m EnqueueThread QueueInput/input_queue Exited.
[32m[1027 00:22:20 @prefetch.py:56][0m [PrefetchDataZMQ] Context terminated.
[32m[1027 00:22:20 @input_source.py:146][0m EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue Exited.
",sure use reader follow give address data running batch size per tower set fork one time assuming directory original structure use set size setting queue setting training model building graph training tower device add total cost building graph training tower device add total cost shape dim device total assuming float setup graph setting queue building predictor tower device building predictor tower device maintain moving average summary collection session session graph starting starting staging area batch data perhaps inconsistent shape recent call last file line file line return array setting array element sequence shape batch data perhaps inconsistent shape recent call last file line file line return array setting array element sequence exception recent call last file line file line return array setting array element sequence handling exception another exception recent call last file line run file line yield holder file line file line object attribute training stopped context,issue,negative,positive,neutral,neutral,positive,positive
339903800,"I am using the specified parameters in the code except for the batchsize ( batchsize for training set is 48 and for validation set is1000 ) to train resnet50 on imagenet. What is confusing me is that up to the 59-th epoch, the train-error-top1 and train-error-top5 are 0.3 and 0.1, but the val-error-top1 and val-error-top5 are still 0.8 and 0.7. Does batchsize influence generalization? What can I do to make validation work better?",code except training set validation set train epoch still influence generalization make validation work better,issue,negative,positive,positive,positive,positive,positive
339894000,"The error becomes sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. Can you help with it? Thank you.
",error becomes thread used thread help thank,issue,negative,neutral,neutral,neutral,neutral,neutral
339879354,"Yes, I not running this code in local. I can have a try. Thank you very much.",yes running code local try thank much,issue,positive,positive,neutral,neutral,positive,positive
339878944,"Actually, sometimes it will show this error. Sometimes, it will enter ipython.",actually sometimes show error sometimes enter,issue,negative,neutral,neutral,neutral,neutral,neutral
339878861,"Thank you. But I am running the same code. When the server is just rebooted, this can work.  But if I use CTRL-C to stop the program, it will show this problem. Actually I am using the way in imagenet-resnet example to prepare the data. I am wondering if the queue in the last run is not exiting. Can you give me some advice? or do you need my code?",thank running code server work use stop program show problem actually way example prepare data wondering queue last run give advice need code,issue,negative,neutral,neutral,neutral,neutral,neutral
339878398,"ValueError: Cannot feed value of shape (224, 224, 3) for Tensor 'input:0', which has shape '(?, 224, 224, 3)'

This is saying that you didn't batch your data.",feed value shape tensor shape saying batch data,issue,negative,neutral,neutral,neutral,neutral,neutral
339250362,"Actually nvidia 387. But I think this doesn't matter too much.
TF1.4 has to be compiled from source to work with cuda9/cudnn7.",actually think matter much source work,issue,negative,positive,neutral,neutral,positive,positive
338938720,"You should read the tutorial on performance tuning first.
http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html",read tutorial performance tuning first,issue,negative,positive,positive,positive,positive,positive
338611704,Thank you very much for your kind help!,thank much kind help,issue,positive,positive,positive,positive,positive,positive
338584327,"I'll figure out whether I can work the GPU up somehow. Your suggestion really helps!

I have one more question regarding building my own network. Can I put the functions for network layers outside `class Model(ModelDesc)` and use a function in `_build_graph` to call them? ",figure whether work somehow suggestion really one question regarding building network put network outside class model use function call,issue,negative,positive,neutral,neutral,positive,positive
338577711,"As I said when you have about 95% utilization, that's the best you can get.
Two GPU performance depend on a lot more other things (e.g. PCIe speed), so it may not be able to scale linearly on ""any"" two GPUs.

I don't know about why your 1080Ti seems slower. I would doubt it's a version issue. I'm using the latest of TF/cuda/cudnn/nvidida driver, if that matters. Now there seems to be no problems with the use of tensorpack.",said utilization best get two performance depend lot speed may able scale linearly two know ti would doubt version issue latest driver use,issue,negative,positive,positive,positive,positive,positive
338571092,"Sorry I forgot to mention that the results above are for resnet 50 with batch size 32. 

What I Modify is to replace 
```python
        ds = dataset.ILSVRC12(datadir, name, shuffle=True)
        ds = AugmentImageComponent(ds, augmentors, copy=False)
        ds = PrefetchDataZMQ(ds, cpu)
        ds = BatchData(ds, batch_size, remainder=False)
```
in the function `get_imagenet_dataflow` with 
```python 
        ds = LMDBData(datadir, shuffle=False)
        ds = LocallyShuffleData(ds, 50000)
        ds = PrefetchData(ds, 5000, 1)
        ds = LMDBDataPoint(ds)
        ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
        ds = AugmentImageComponent(ds, augmentors)
        ds = PrefetchDataZMQ(ds, 25)
        ds = BatchData(ds, batch_size)\
```
And switch off validation in training process.

The following are for resnet 18 with batchsize 256. This time I use two same 1080 ti GPUs instead of two different ones, and they seem to be collaborating well as you said. 

1. Use 1 1080 ti GPU;
```python
[1023 10:11:52 @base.py:232] Start Epoch 1 ...
100%|##########|50/50[00:37<00:00, 1.32it/s]
[1023 10:12:30 @base.py:242] Epoch 1 (global_step 50) finished, time:38.01 sec.
[1023 10:12:30 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-50.
[1023 10:12:30 @monitor.py:359] GPUUtil/0: 83.667
[1023 10:12:30 @monitor.py:359] QueueInput/queue_size: 47.456
[1023 10:12:30 @monitor.py:359] l2_regularize_loss: 0.48821
[1023 10:12:30 @monitor.py:359] learning_rate: 0.1
[1023 10:12:30 @monitor.py:359] train-error-top1: 0.89975
[1023 10:12:30 @monitor.py:359] train-error-top5: 0.68685
[1023 10:12:30 @monitor.py:359] xentropy-loss: 3.554
[1023 10:12:30 @base.py:232] Start Epoch 2 ...
100%|##########|50/50[00:31<00:00, 1.58it/s]
[1023 10:13:02 @base.py:242] Epoch 2 (global_step 100) finished, time:31.59 sec.
[1023 10:13:02 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-100.
[1023 10:13:02 @monitor.py:359] GPUUtil/0: 95.267
[1023 10:13:02 @monitor.py:359] QueueInput/queue_size: 49.818
[1023 10:13:02 @monitor.py:359] l2_regularize_loss: 0.48518
[1023 10:13:02 @monitor.py:359] learning_rate: 0.1
[1023 10:13:02 @monitor.py:359] train-error-top1: 0.89762
[1023 10:13:02 @monitor.py:359] train-error-top5: 0.67561
[1023 10:13:02 @monitor.py:359] xentropy-loss: 3.7917
[1023 10:13:02 @base.py:232] Start Epoch 3 ...
100%|##########|50/50[00:31<00:00, 1.58it/s]
[1023 10:13:34 @base.py:242] Epoch 3 (global_step 150) finished, time:31.68 sec.
[1023 10:13:34 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-150.
[1023 10:13:34 @monitor.py:359] GPUUtil/0: 94.733
[1023 10:13:34 @monitor.py:359] QueueInput/queue_size: 49.986
[1023 10:13:34 @monitor.py:359] l2_regularize_loss: 0.48217
[1023 10:13:34 @monitor.py:359] learning_rate: 0.1
[1023 10:13:34 @monitor.py:359] train-error-top1: 0.89215
[1023 10:13:34 @monitor.py:359] train-error-top5: 0.68439
[1023 10:13:34 @monitor.py:359] xentropy-loss: 4.0553
[1023 10:13:34 @base.py:232] Start Epoch 4 ...
100%|##########|50/50[00:30<00:00, 1.63it/s]
[1023 10:14:05 @base.py:242] Epoch 4 (global_step 200) finished, time:30.64 sec.
  0%|          |0/50[00:00<?,?it/s][1023 10:14:05 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-200.
[1023 10:14:05 @monitor.py:359] GPUUtil/0: 95.069
[1023 10:14:05 @monitor.py:359] QueueInput/queue_size: 49.999
[1023 10:14:05 @monitor.py:359] l2_regularize_loss: 0.47961
[1023 10:14:05 @monitor.py:359] learning_rate: 0.1
[1023 10:14:05 @monitor.py:359] train-error-top1: 0.8763
[1023 10:14:05 @monitor.py:359] train-error-top5: 0.64162
[1023 10:14:05 @monitor.py:359] xentropy-loss: 4.0434
[1023 10:14:05 @base.py:232] Start Epoch 5 ...
[1023 10:14:34 @base.py:242] Epoch 5 (global_step 250) finished, time:29.51 sec.
100%|##########|50/50[00:29<00:00, 1.69it/s]
[1023 10:14:35 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-250.
[1023 10:14:35 @monitor.py:359] GPUUtil/0: 94.5
[1023 10:14:35 @monitor.py:359] QueueInput/queue_size: 50
[1023 10:14:35 @monitor.py:359] l2_regularize_loss: 0.47672
[1023 10:14:35 @monitor.py:359] learning_rate: 0.1
[1023 10:14:35 @monitor.py:359] train-error-top1: 0.86738
[1023 10:14:35 @monitor.py:359] train-error-top5: 0.64623
[1023 10:14:35 @monitor.py:359] xentropy-loss: 3.7505
```

2. Use two 1080ti GPUs:
```python
100%|##########|50/50[00:26<00:00, 1.86it/s]
[1023 10:07:56 @base.py:242] Epoch 1 (global_step 50) finished, time:26.88 sec.
[1023 10:07:56 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-50.
[1023 10:07:57 @monitor.py:359] GPUUtil/0: 62.16
[1023 10:07:57 @monitor.py:359] GPUUtil/1: 54.36
[1023 10:07:57 @monitor.py:359] QueueInput/queue_size: 49.353
[1023 10:07:57 @monitor.py:359] l2_regularize_loss: 0.48833
[1023 10:07:57 @monitor.py:359] learning_rate: 0.1
[1023 10:07:57 @monitor.py:359] train-error-top1: 0.91258
[1023 10:07:57 @monitor.py:359] train-error-top5: 0.70073
[1023 10:07:57 @monitor.py:359] xentropy-loss: 3.5975
[1023 10:07:57 @base.py:232] Start Epoch 2 ...
100%|##########|50/50[00:16<00:00, 3.09it/s]
[1023 10:08:13 @base.py:242] Epoch 2 (global_step 100) finished, time:16.19 sec.
[1023 10:08:13 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-100.
[1023 10:08:13 @monitor.py:359] GPUUtil/0: 88.867
[1023 10:08:13 @monitor.py:359] GPUUtil/1: 79.067
[1023 10:08:13 @monitor.py:359] QueueInput/queue_size: 49.5
[1023 10:08:13 @monitor.py:359] l2_regularize_loss: 0.48509
[1023 10:08:13 @monitor.py:359] learning_rate: 0.1
[1023 10:08:13 @monitor.py:359] train-error-top1: 0.9074
[1023 10:08:13 @monitor.py:359] train-error-top5: 0.70409
[1023 10:08:13 @monitor.py:359] xentropy-loss: 3.8236
[1023 10:08:13 @base.py:232] Start Epoch 3 ...
  0%|          |0/50[00:00<?,?it/s][1023 10:08:28 @base.py:242] Epoch 3 (global_step 150) finished, time:15.10 sec.
100%|##########|50/50[00:15<00:00, 3.31it/s]
[1023 10:08:28 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-150.
[1023 10:08:28 @monitor.py:359] GPUUtil/0: 93.214
[1023 10:08:28 @monitor.py:359] GPUUtil/1: 89
[1023 10:08:28 @monitor.py:359] QueueInput/queue_size: 49.648
[1023 10:08:28 @monitor.py:359] l2_regularize_loss: 0.4818
[1023 10:08:28 @monitor.py:359] learning_rate: 0.1
[1023 10:08:28 @monitor.py:359] train-error-top1: 0.89979
[1023 10:08:28 @monitor.py:359] train-error-top5: 0.70344
[1023 10:08:28 @monitor.py:359] xentropy-loss: 4.0616
[1023 10:08:28 @base.py:232] Start Epoch 4 ...
100%|##########|50/50[00:14<00:00, 3.34it/s]
[1023 10:08:43 @base.py:242] Epoch 4 (global_step 200) finished, time:14.98 sec.
[1023 10:08:43 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-200.
[1023 10:08:43 @monitor.py:359] GPUUtil/0: 93.071
[1023 10:08:43 @monitor.py:359] GPUUtil/1: 94.214
[1023 10:08:43 @monitor.py:359] QueueInput/queue_size: 49.676
[1023 10:08:43 @monitor.py:359] l2_regularize_loss: 0.47896
[1023 10:08:43 @monitor.py:359] learning_rate: 0.1
[1023 10:08:43 @monitor.py:359] train-error-top1: 0.89005
[1023 10:08:43 @monitor.py:359] train-error-top5: 0.67852
[1023 10:08:43 @monitor.py:359] xentropy-loss: 4.1054
[1023 10:08:43 @base.py:232] Start Epoch 5 ...
  0%|          |0/50[00:00<?,?it/s][1023 10:09:01 @base.py:242] Epoch 5 (global_step 250) finished, time:17.26 sec.
100%|##########|50/50[00:17<00:00, 2.90it/s]
[1023 10:09:01 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-250.
[1023 10:09:01 @monitor.py:359] GPUUtil/0: 96.25
[1023 10:09:01 @monitor.py:359] GPUUtil/1: 76.938
[1023 10:09:01 @monitor.py:359] QueueInput/queue_size: 49.638
[1023 10:09:01 @monitor.py:359] l2_regularize_loss: 0.47608
[1023 10:09:01 @monitor.py:359] learning_rate: 0.1
[1023 10:09:01 @monitor.py:359] train-error-top1: 0.87799
[1023 10:09:01 @monitor.py:359] train-error-top5: 0.66664
[1023 10:09:01 @monitor.py:359] xentropy-loss: 3.796
```

The data reading and processing speed  is 4.27it/s. This time, the speed performances all seem reasonable, aren't they?

But the GPU utility is still not full. Is there a good way to increase GPU utility? And I cannot make it run as fast as 2.7it/s (your mentioned speed) with only one 1080 ti.
",sorry forgot mention batch size modify replace python name function python lambda switch validation training process following time use two ti instead two different seem well said use ti python start epoch epoch finished time sec model saved start epoch epoch finished time sec model saved start epoch epoch finished time sec model saved start epoch epoch finished time sec model saved start epoch epoch finished time sec model saved use two ti python epoch finished time sec model saved start epoch epoch finished time sec model saved start epoch epoch finished time sec model saved start epoch epoch finished time sec model saved start epoch epoch finished time sec model saved data reading speed time speed seem reasonable utility still full good way increase utility make run fast speed one ti,issue,positive,negative,neutral,neutral,negative,negative
338512830,"It drops from 28.3 to 3.9 because that's about best your GPU can do. Yes it drops a lot but what else could you expect?
With a 1080Ti I can only get 2.7it/s for resnet18 batch=256. Don't even know how you could get 3.x.

If data is fast enough and you didn't modify the code about batch size, you should see about linear speed up. In no way it should drop and I still doubt whether you have measured things correctly.
If you still can't figure out, please attach the modification you've done and training log for at least two epochs with a small epoch size (like steps_per_epoch=50).",best yes lot else could expect ti get even know could get data fast enough modify code batch size see linear speed way drop still doubt whether measured correctly still ca figure please attach modification done training log least two small epoch size like,issue,positive,positive,positive,positive,positive,positive
338493443,"For 'sequential read':

Queue size is 50 and GPU utilization is around 96% in 1-GPU case. For the 2-GPU case, queue size is around 49 and GPU Utility is 35% and 92% with respect to each GPU. It seems that one GPU is always around 90%, and the other is intermittently high and low.",read queue size utilization around case case queue size around utility respect one always around intermittently high low,issue,negative,positive,neutral,neutral,positive,positive
338492932,"Dropping from 4.3 to 3.6 sounds reasonable. But for the 'sequential read' case, it drops from 28.31 to 3.97, which is too big.  The data should be enough in this case, but when I use 2 GPUs, the performance drop from 3.97 (1 gpu) to 1.42. I don't expect it to be slower with 2 GPUs.

",dropping reasonable read case big data enough case use performance drop expect,issue,negative,positive,neutral,neutral,positive,positive
338489944,"You mean drop from 4.3 to 3.6? I would expect the speed number for GTX 1080 is between 3~5 but I don't remember. 
You can check your queue size and GPU utilization. If you see above 95% utilization then that's the best you can do.",mean drop would expect speed number remember check queue size utilization see utilization best,issue,positive,positive,positive,positive,positive,positive
338487413,"I am sorry. I just found that I was using different batch sizes. After correcting the batch size, I have the performances of 4.35 it/s and 28.31 it/s for 'random read' and 'sequential read', respectively. Now it seems that when training using GPUs, the performance is dropping dramatically. I think my GPUs are fine to run fast enough. They shouldn't become such a big bottleneck.",sorry found different batch size correcting batch size read read respectively training performance dropping dramatically think fine run fast enough become big bottleneck,issue,negative,positive,neutral,neutral,positive,positive
338481974,"Become faster from 0.4it/s to 3.6it/s, with the same batch size 256? This is impossible and you probably read the number too early when it's unstable. ",become faster batch size impossible probably read number early unstable,issue,negative,negative,negative,negative,negative,negative
338480828,"By two versions, I mean 'random read' and 'sequential read' as shown in the two code blocks. I change nothing except for the `get_imagenet_dataflow` function in `imagenet_resnet_utils.py`. I try using these two versions to read the training data, and get the above-mentioned results. 'Random read' becomes faster and 'Sequential read' becomes slower.",two mean read read shown two code change nothing except function try two read training data get read becomes faster read becomes,issue,negative,negative,negative,negative,negative,negative
338480390,"The two queues are for training and validation respectively. Queues being empty basically says your data is slow.
When data is the bottleneck you should've seen the performance goes roughly from 3.6 -> 1.8 when going from 1 GPU to 2.

I don't understand what have you done. Which ""two versions"" have you used in the end exactly? How could you possibly get 3.6it/s when data is only 0.4it/s. ",two training validation respectively empty basically data slow data bottleneck seen performance go roughly going understand done two used end exactly could possibly get data,issue,negative,negative,neutral,neutral,negative,negative
338480382,"Information of my two GPUs:

```
2017-10-22 17:07:07.234903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:03:00.0
Total memory: 10.91GiB
Free memory: 10.13GiB
2017-10-22 17:07:07.765882: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x57d1530 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-10-22 17:07:07.767095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: Quadro K6000
major: 3 minor: 5 memoryClockRate (GHz) 0.9015
pciBusID 0000:04:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
```",information two found device name ti major minor total memory free memory context work found device name major minor total memory free memory,issue,positive,positive,positive,positive,positive,positive
338479899,"One more question: are `InferenceRunner/QueueInput/queue_size` and `QueueInput/queue_size` the same thing?  `InferenceRunner/QueueInput/queue_size` seems to be always 50, but `QueueInput/queue_size` sometimes is very low.",one question thing always sometimes low,issue,negative,neutral,neutral,neutral,neutral,neutral
338479295,"@ppwwyyxx 
I independently tested the codes you provide in '[efficient DataFlow](http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html)' as following

```python
ds = dataset.ILSVRC12(datadir, 'train', meta_dir=metadir, shuffle=True)
ds = AugmentImageComponent(ds, augmentors)
ds = PrefetchDataZMQ(ds, nr_proc=25)
ds = BatchData(ds, 256)
```


and 

```python
ds = LMDBData(datadir, shuffle=False)
ds = LocallyShuffleData(ds, 50000)
ds = PrefetchData(ds, 5000, 1)
ds = LMDBDataPoint(ds)
ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
ds = AugmentImageComponent(ds, augmentors)
ds = PrefetchDataZMQ(ds, 25)
ds = BatchData(ds, 256, use_list=True)
```



The former one gives the performance of 4.35 it/s and the latter one 28.31 it/s. Then I used these two versions of reading data in the ['ResNet example'](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet). I got the performances of  3.62it/s and 3.97it/s in 1-gpu case, and 1.43it/s and 1.42it/s in 2-gpu case. 

I don't understand why this happens. It seems that the use of GPU is influencing  data reading, and that for random read and sequential read, GPU is influencing in different ways.",independently tested provide efficient following python python lambda former one performance latter one used two reading data example got case case understand use data reading random read sequential read different way,issue,negative,negative,neutral,neutral,negative,negative
338419956,"Ah, I see. NVM then, I got my local example working by modifying the script.",ah see got local example working script,issue,negative,neutral,neutral,neutral,neutral,neutral
338411806,But doesn't tensorpack now call tf.layers and tesnorpack naming is no longer used? or is there a backwards preserving translation somewhere I missing?,call naming longer used backwards translation somewhere missing,issue,negative,negative,negative,negative,negative,negative
338411644,"It's not ""broken"" because it never worked. VGG-16.npy uses tensorpack naming, not tf.layers naming.",broken never worked naming naming,issue,negative,negative,negative,negative,negative,negative
338356538,"One general philosophy about design is that: when you pass something to someone that, in certain cases, logically doesn't need it, it's a design problem. Packing a bunch of (possibly None) objects together is a way to hide this design problem (because then you only pass one giant object), but the problem still exists.

`ModelDesc` packs three things: `InputDesc`, `build_graph`, `get_optimizer`.
`InputDesc` is only for working with different types of InputSource. Different InputSource creates very different tensors but they share this common metadata. InputDesc is not used for anything else in the whole project.
`get_optimizer` is only used for training, so ideally `ModelDesc` shouldn't be used to inference.

`TrainConfig` packs a number of things. A lot of them are not useful in certain cases, e.g. only multiGPU trainers need towers. A custom trainer may not need model or dataflow.
Packing all stuff together introduces another problem in `TrainConfig`: those stuff cannot depend on each other, although they may logically do. For example, a callback may depend on the outcome of building the model (need to use tensor names).

One story is that at the beginning of tensorpack, optimizer is actually part of `TrainConfig`. This again is showing these options are packed together a bit arbitrarily.  

The new trainer interface in development doesn't depend on either `ModelDesc` or `TrainConfig`. Instead of taking those packaged objects, it takes individual options. `ModelDesc` and `TrainConfig` can then be used through [high-level wrappers](https://github.com/ppwwyyxx/tensorpack/blob/4e6442903b511ae5dd398d8204cd9eebf0729c77/tensorpack/trainv2/interface.py#L44) around the trainers. Then users will get more control over the details when the high-level interface isn't enough, or when a different high-level interface is needed. For example, it will be much easier to train a Keras model with the new trainers.

(nothing is broken for the moment. just sharing some progress)",one general philosophy design pas something someone certain logically need design problem bunch possibly none together way hide design problem pas one giant object problem still three working different different different share common used anything else whole project used training ideally used inference number lot useful certain need custom trainer may need model stuff together another problem stuff depend although may logically example may depend outcome building model need use tensor one story beginning actually part showing together bit arbitrarily new trainer interface development depend either instead taking individual used around get control interface enough different interface example much easier train model new nothing broken moment progress,issue,positive,positive,neutral,neutral,positive,positive
338261623,"Make sure that `_build_graph` build the part of graph from input to cost.

For your case, it could be your model has very different communication-computation ratio so it spends more time in communication. Or you might mistakenly introduce some dependencies among the two GPUs so they don't run together. It's best if you could show some code or at least describe what you've done.",make sure build part graph input cost case could model different ratio time communication might mistakenly introduce among two run together best could show code least describe done,issue,positive,positive,positive,positive,positive,positive
338049120,"`ModelDesc` is for single-cost models only, so whatever changes should not affect GANs.
a GAN model should build the forward/backward graph by itself, not using anything in `ModelDesc` other than `build_graph`, `get_inputs`.

`GANModelDesc` mistakenly subclasses `ModelDesc` at the beginning but not anymore. So they are totally unrelated now.",whatever affect gan model build graph anything mistakenly beginning totally unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
338048061,"No but ModelDesc has and I really hacked it alot to get it do what I want.
I ended up not pursuing it because the performance was really bad over
multiple GPUs and I just stuck with default multiGPU GAN trainer since then.

On Thu, Oct 19, 2017 at 5:12 PM, Yuxin Wu <notifications@github.com> wrote:

> What do you mean by new interface? GAN.py hasn't been changed much since
> MultiGPUGANTrainer was added.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/427#issuecomment-338039656>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AB9WXzcD1Furjo8Tblt7jyZVWUOnV0a2ks5st7tWgaJpZM4PhlXt>
> .
>
",really hacked get want ended performance really bad multiple stuck default gan trainer since wrote mean new interface much since added thread reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
338039656,What do you mean by new interface? `GAN.py` hasn't been changed much since MultiGPUGANTrainer was added.,mean new interface much since added,issue,negative,positive,neutral,neutral,positive,positive
338038768,"Hmm, I haven't taken a look at the new interface. Last time I tried this was in July.",taken look new interface last time tried,issue,negative,positive,neutral,neutral,positive,positive
338038129,"Not, it doesn't have to do with building the graph, but  it does have two
separate costs that both have to be optimized with custom variable lists.
That can be hard to access through the current interface.



On Wed, Oct 18, 2017 at 11:56 PM, Yuxin Wu <notifications@github.com> wrote:

> I'll try the aggregation methods. Writing a GAN trainer should have
> nothing to do with this method because you're supposed to build the graph
> by yourself anyway.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/427#issuecomment-337792796>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AB9WX1rNTLGVnvtl7ItJpoNJvxHVnUtYks5stshrgaJpZM4PhlXt>
> .
>
",building graph two separate custom variable hard access current interface wed wrote try aggregation writing gan trainer nothing method supposed build graph anyway thread reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
337968748,"OK. I thought you want to run multiple predictors.
Setting the thread to 0 make single predictor multi-threaded. This setting is usually not good for training.

You don't need to modify any code. You can pass this to PredictConfig:
```python
config = tf.ConfigProto()
# change config
PredictConfig(session_creator=NewSessionCreator(config=config)) 
```",thought want run multiple setting thread make single predictor setting usually good training need modify code pas python change,issue,negative,positive,positive,positive,positive,positive
337792796,"I'll try the aggregation methods. 
Writing a GAN trainer should have nothing to do with this method because you're supposed to build the graph by yourself anyway. I can make some utilities (e.g. average the gradients) public, though.",try aggregation writing gan trainer nothing method supposed build graph anyway make average public though,issue,negative,negative,neutral,neutral,negative,negative
337788469,"The experimental aggregation methods can be significantly more memory
efficient. I also wrote a custom multiGPUGANTrainer that averages the
gradient instead of the loss and found that rather difficult to do even
with _get_cost_and_grad.

On Wed, Oct 18, 2017 at 5:32 PM, Yuxin Wu <notifications@github.com> wrote:

> What's the reason you want to change the aggregation method? I don't
> expect this to be changed as the default should be good.
> After some recent refactoring there is no _get_cost_and_grad any more (it
> wasn't a public method so I didn't take too much care on it) and the
> relevant code to compute gradients is buried deeper.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/427#issuecomment-337734224>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AB9WXwBF_91rLOnRqWn-CMjDT9b9fdJIks5stm5YgaJpZM4PhlXt>
> .
>
",experimental aggregation significantly memory efficient also wrote custom gradient instead loss found rather difficult even wed wrote reason want change aggregation method expect default good recent public method take much care relevant code compute buried thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
337734224,"What's the reason you want to change the aggregation method? I don't expect this to be changed as the default should be good.
After some recent refactoring there is no `_get_cost_and_grad` any more (it wasn't a public method so I didn't take too much care on it) and the relevant code to compute gradients is buried deeper.",reason want change aggregation method expect default good recent public method take much care relevant code compute buried,issue,positive,positive,positive,positive,positive,positive
337315597,"tensorpack doesn't care about inference or deployment so we won't add this feature. It gives you a tensorflow checkpoint and you can use whatever inference features tensorflow supports. The simplest thing is to just open many Python threads to run `sess.run`, but I don't know does tensorflow support anything more than that. You'd better ask this question in the tensorflow community.",care inference deployment wo add feature use whatever inference thing open many python run know support anything better ask question community,issue,positive,positive,positive,positive,positive,positive
336946783,"It uses similar time but does twice more work.
This means it has over 90%  efficiency. See #353, #359 and the [documentation](http://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html).
",similar time twice work efficiency see documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
336758347,"Could you explain what is ""Between-graph mode"". Are you still talking about multi-gpu training or distributed training?",could explain mode still talking training distributed training,issue,negative,neutral,neutral,neutral,neutral,neutral
336753623,"thanks,problem solved.
more question:

i want to train with 4 gpus use Between-graph mode,how to do?",thanks problem question want train use mode,issue,negative,positive,positive,positive,positive,positive
336743270,"Does it make sense if we start the thread in a function and pass the function to slim.learning.train as the init_fn argument?

```
def init_fn(sess):
  with sess.as_default():
    thread.start()

slim.learning.train(...., init_fn=init_fn)
```",make sense start thread function pas function argument sess,issue,negative,neutral,neutral,neutral,neutral,neutral
336742409,I am still not sure how to use the dataflow with 'slim.learning.train' where we don’t have direct access to the session object. Maybe we need to write an operation which reads from ds (using py_func?) and enqueues it and makes a QueueRunner for that?,still sure use direct access session object maybe need write operation,issue,negative,positive,positive,positive,positive,positive
336690620,"I have no interest in it .. 
and implementing papers for others is far beyond what a maintainer of a deep learning library can do.",interest far beyond maintainer deep learning library,issue,negative,positive,neutral,neutral,positive,positive
336652981,"With replicated training, the tensor name becomes `tower0/cost`. Could you try with that?",replicated training tensor name becomes could try,issue,negative,neutral,neutral,neutral,neutral,neutral
336652905,"During training there is no need for quantization because batchnorm is fast.
During inference, quantized batch norm becomes `quantize([quantized number] x [constant] + [constant])`, which can be implemented by a lookup table on FPGA.

This is not a tensorpack question. I suggest you email the authors about the paper details.",training need quantization fast inference batch norm becomes quantize number constant constant table question suggest paper,issue,negative,positive,neutral,neutral,positive,positive
336635864,Thanks for your reply. I think I really need further looking into the framework of tensorflow and make better use of it to meet my requests.,thanks reply think really need looking framework make better use meet,issue,positive,positive,positive,positive,positive,positive
336619039,"It won't necessarily run in serial. It's allowed to run in parallel and tensorflow runtime engine will decide what to do.
You can write whatever model implementation with tensorflow. `Conv2D` is just a simple wrapper over a common layer, you can and should write your own layer if you need more than what's commonly used.",wo necessarily run serial run parallel engine decide write whatever model implementation simple wrapper common layer write layer need commonly used,issue,negative,negative,negative,negative,negative,negative
336565558,"`tf.Print` may work for you. https://github.com/ppwwyyxx/tensorpack/issues/389 is also related. But I'm not sure are those exactly what you need.

To access the weight tensor of a layer,also see https://github.com/ppwwyyxx/tensorpack/issues/389#issuecomment-327971447 .",may work also related sure exactly need access weight tensor layer also see,issue,negative,positive,positive,positive,positive,positive
336357306,"You should only call `update_ema` when `ctx.is_main_training_tower` is True. Or at least when `ctx.is_training` is True.
Otherwise you're adding inference tensors to `UPDATE_OPS` as well. Evaluating inference tensors during training will give you the error.",call true least true otherwise inference well inference training give error,issue,positive,positive,positive,positive,positive,positive
336353832,"Hi there! I've created a self-contained working example of the issue:

https://gist.github.com/skoppula/93320039610b0d2d4332bb18ce70ff19

This example is almost entirely Tensorpack's regular CIFAR10 ResNet example, with about 30 extra lines of code adding a new layer (that includes moving averages). Adding this layer to the graph produces the error (line 118: `l = RescaleActivationLayer('rescale', l)`). Most of this new layer code is based off the Tensorpack's BatchNorm implementation.

Any ideas?

Thanks.",hi working example issue example almost entirely regular example extra code new layer moving layer graph error line new layer code based implementation thanks,issue,negative,positive,neutral,neutral,positive,positive
336086120,"Or could you provide what exactly did you add to make it fail?
Please note that `build_graph` will be called multiple times (#gpu times for training and 1 time for inference). Adding things that have side effect may cause problems like this.",could provide exactly add make fail please note multiple time time training time inference side effect may cause like,issue,negative,negative,neutral,neutral,negative,negative
336034575,"I don't know what's the reason. It's best if there is a minimum failure example.
Are you using anything like `self.xxx = xxx` inside `build_graph` (except for the cost)? This might cause problems like this.",know reason best minimum failure example anything like inside except cost might cause like,issue,positive,positive,positive,positive,positive,positive
335960211,"The fundamental reason is that calling `tf.Variable` will ignore variable scope reuse, which is the fundamental mechanism of multigpu training. I'm not sure is this necessary for Keras or is it possible to change. There is actually a question on Keras about this: https://github.com/fchollet/keras/issues/7992

The current way to build keras model inside tensorpack isn't too bad IMHO. Apart from that maybe more things can be done on tensorpack side, e.g. integrate with Keras loss/regularization automatically.",fundamental reason calling ignore variable scope reuse fundamental mechanism training sure necessary possible change actually question current way build model inside bad apart maybe done side integrate automatically,issue,negative,negative,neutral,neutral,negative,negative
335932756,@ppwwyyxx is there a change that would make it easier to integrate with fairly straightforward?,change would make easier integrate fairly straightforward,issue,negative,positive,positive,positive,positive,positive
335880165,"Technically, the log file stores any logs generated after you've set the log directory. If you set the directory earlier you can see logs about TrainConfig.",technically log file set log directory set directory see,issue,negative,neutral,neutral,neutral,neutral,neutral
335758184,"Ok, I might have misunderstood the concept of the log file that is automatically generated during training. I thought it would contain any logs generated by the TensorPack code. However, if understand correctly now, this log is meant to only store the logs generated during the training process (and not during setting up the TrainConfig etc). This makes sense to me and as far as I am concern this issue be closed. Thanks for addressing this concern and keep up the good work :)",might misunderstood concept log file automatically training thought would contain code however understand correctly log meant store training process setting sense far concern issue closed thanks concern keep good work,issue,positive,positive,positive,positive,positive,positive
335662634,Closing because the current status (a [mnist-keras example](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/mnist-keras.py)) is probably all we can support. The use of `tf.Variable` everywhere in Keras makes it very hard to work with any other frameworks.,current status example probably support use everywhere hard work,issue,negative,negative,negative,negative,negative,negative
335632098,It says `logger directory was not set`. So there is **no log file** at that time. You can't expect it to store things in a log file that is created in the future.,logger directory set log file time ca expect store log file future,issue,negative,neutral,neutral,neutral,neutral,neutral
335629747,"Thanks for your quick replay. Yes, you are right that this warning is printed to the console. However, it is not stored in the log file (e.g. log.log). I used the log file for debugging the problem with no tfevents file being generated and couldn't find the warnings there. I believe that having the warnings in the log would make debugging easier.

The first line of the log is stored only after the log file handler is set in [2]. The TFEventWriter warnings are generated before this file handler is set. Additionally, the InferenceRunner log in [3] is similarly printed only to the console and not to the log file.
 
Best,
Jakub

[2] https://github.com/ppwwyyxx/tensorpack/blob/a36ad1807632060824a0cb156e644315216085ad/tensorpack/utils/logger.py#L74

[3] https://github.com/ppwwyyxx/tensorpack/blob/d5f3350de60974424feb08057af84fbb82dff21c/tensorpack/callbacks/inference_runner.py#L81",thanks quick replay yes right warning printed console however log file used log file problem file could find believe log would make easier first line log log file handler set file handler set additionally log similarly printed console log file best,issue,positive,positive,positive,positive,positive,positive
335599516,"Closing. Only two layers were not switched to `tf.layers` implementation:
1. Conv2D. `tf.layers` doesn't support group conv.
2. BatchNorm. `tf.layers` doesn't support use_local_stat != is_training.",two switched implementation support group support,issue,positive,neutral,neutral,neutral,neutral,neutral
335527905,"```
╰─$python3 -c 'from tensorpack import *; x = TrainConfig(dataflow=FakeData([1]))'
Failed to load OpenCL runtime (expected version 1.1+)
[1010 09:18:31 @monitor.py:198] WRN logger directory was not set. Ignore TFEventWriter.
[1010 09:18:31 @monitor.py:238] WRN logger directory was not set. Ignore JSONWriter.
```
Could you double check?",python import load version logger directory set ignore logger directory set ignore could double check,issue,negative,neutral,neutral,neutral,neutral,neutral
335025118,"prefetch with nr_proc>1 will not give you the same data. See http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.PrefetchDataZMQ

When `73768 % BATCH_SIZE != 0`, use of batch can also be a problem (missing a few datapoints in the end). You can use `remainder=True` (http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.BatchData)",give data see use batch also problem missing end use,issue,negative,negative,negative,negative,negative,negative
335003998,"2. My DataFlow = DataPoints are dumped to tfrecords by your dump_dataflow_to_tfrecord() to make  data injection faster (I added compress option to make result files much smaller). So test set is the same all times:
'''
test = TFRecordData('./test_.tfrecords', size=73768)
test = PrefetchDataZMQ(test, nr_proc=4, hwm=100)
test = BatchData(test, BATCH_SIZE)
'''
maybe that is consequent of ""nr_proc=4""?",make data injection faster added compress option make result much smaller test set time test test test test test maybe consequent,issue,negative,neutral,neutral,neutral,neutral,neutral
334992080,"1. Just print the matrix in your code and don't use `monitors.put`. Monitors don't support ""matrix"" anyway.
2. Your inference dataflow might not produce the same set of data every time, e.g. due to batching, random shuffling. You may need to check the dataflow. ",print matrix code use support matrix anyway inference might produce set data every time due random shuffling may need check,issue,negative,negative,negative,negative,negative,negative
334991097,"Thank you. 
I see that it is just tensorflow graph, but your queue to plug new data into it - it is not just ""feed"" ).
I use your tensorpack and it works well to push my GPU to 100% - 
I added stat function like confusion matrix.
'''
[32m[1007 15:52:17 @monitor.py:355][0m confusion matrix of class 0: | 38643|  2818|   431|    86|, Recall:92.06, Precision: 84.61
[32m[1007 15:52:17 @monitor.py:355][0m confusion matrix of class 1: |  6174| 12156|  1510|    59|, Recall:61.09, Precision: 72.79
[32m[1007 15:52:17 @monitor.py:355][0m confusion matrix of class 2: |   795|  1662|  6974|   313|, Recall:71.57, Precision: 76.09
[32m[1007 15:52:17 @monitor.py:355][0m confusion matrix of class 3: |    58|    63|   250|  1736|, Recall:82.39, Precision: 79.12
'''
but it adds new line with new epoch in the tensorboard scalars. 
![confusion_matrix_tensorboard](https://user-images.githubusercontent.com/28757897/31314917-cdc1cd08-ac14-11e7-8950-1f6c0f2814a3.png)

Question:
1) how can I turn off output my confusion matrix to tensorboard
2) why my test step (inference with test data) is not the same (quantities of data-points in the confusion matrix inside one class are not the same)?
'''
[0m confusion matrix of class 0: | 39992|  2136|   128|   104|
[0m confusion matrix of class 1: |  6744| 11696|   704|    80|
[0m confusion matrix of class 2: |   896|  1568|  7160|   344|
[0m confusion matrix of class 3: |    48|    40|   192|  1896|
'''
compare with the same test set but another run:
'''
[0m confusion matrix of class 0: | 38643|  2818|   431|    86|
[0m confusion matrix of class 1: |  6174| 12156|  1510|    59|
[0m confusion matrix of class 2: |   795|  1662|  6974|   313|
[0m confusion matrix of class 3: |    58|    63|   250|  1736|
'''",thank see graph queue plug new data feed use work well push added function like confusion confusion matrix class recall precision confusion matrix class recall precision confusion matrix class recall precision confusion matrix class recall precision new line new epoch question turn output confusion matrix test step inference test data confusion matrix inside one class confusion matrix class confusion matrix class confusion matrix class confusion matrix class compare test set another run confusion matrix class confusion matrix class confusion matrix class confusion matrix class,issue,negative,positive,positive,positive,positive,positive
334900516,"```python
from tensorpack.graph_builder.input_source import EnqueueThread

placeholder = [tf.placeholder(...), tf.placeholder(...)]
queue = tf.FIFOQueue(50, [x.dtype for x in placeholder])
thread = EnqueueThread(queue, ds, placeholder)
tensors = queue.dequeue()

with sess.as_default():
    thread.start()
```
something like this may work.",python import queue thread queue something like may work,issue,negative,neutral,neutral,neutral,neutral,neutral
334886331,"I only use tensorpack's dataflow.

I don't use tensorpack's wrapper for tensorflow.

If I create a model by pure tensorflow, can dataflow like below be converted to tensorflow queue easily?

```
def get_mnist_data(is_train, image_size, batchsize):
    ds = MNISTCh('train' if is_train else 'test', shuffle=True)

    if is_train:
        augs = [
            imgaug.RandomApplyAug(imgaug.RandomResize((0.8, 1.2), (0.8, 1.2)), 0.3),
            imgaug.RandomApplyAug(imgaug.RotationAndCropValid(15), 0.5),
            imgaug.RandomApplyAug(imgaug.SaltPepperNoise(white_prob=0.01, black_prob=0.01), 0.25),
            imgaug.Resize((224, 224), cv2.INTER_AREA)
        ]
        ds = AugmentImageComponent(ds, augs)
        ds = PrefetchData(ds, 128*10, multiprocessing.cpu_count())
        ds = BatchData(ds, batchsize)
        ds = PrefetchData(ds, 256, 4)
    else:
        # no augmentation, only resizing
        augs = [
            imgaug.Resize((image_size, image_size), cv2.INTER_CUBIC),
        ]
        ds = AugmentImageComponent(ds, augs)
        ds = BatchData(ds, batchsize)
        ds = PrefetchData(ds, 20, 2)
    return ds
```

I have created a simple class to convert, and used it for me...",use use wrapper create model pure like converted queue easily else else augmentation return simple class convert used,issue,positive,positive,positive,positive,positive,positive
334761879,tensorpack is already using tensorflow queue. What features have you done/do you want in addition to that?,already queue want addition,issue,negative,neutral,neutral,neutral,neutral,neutral
334662243,"After training you can use `OfflinePredictor` with `PredictConfig` to get a callable. It is quite easy -- you usually only need to provide 4 arguments (model, where to load, input, output). Docs is [here](http://tensorpack.readthedocs.io/en/latest/modules/predict.html#tensorpack.predict.PredictConfig). Most examples contain the use of `OfflinePredictor` if you could grep.

We won't provide more sophisticated use -- because it's just a normal tensorflow model and you can use whatever features of tensorflow for inference.",training use get callable quite easy usually need provide model load input output contain use could wo provide sophisticated use normal model use whatever inference,issue,positive,positive,positive,positive,positive,positive
334250327,"You might be using some operations in the model e.g. slicing of variables, that this implementation of accumgrad doesn't support yet.",might model slicing implementation support yet,issue,negative,neutral,neutral,neutral,neutral,neutral
334017216,"Everything works well on a cluster, I have tested on the following configurations:
- 4 nodes, each with one gpus
- 2 nodes, each with two gpus

Thanks for your help.",everything work well cluster tested following one two thanks help,issue,positive,positive,neutral,neutral,positive,positive
332834715,"
@ppwwyyxx  Thank you very much! I change the learning rate to a very small one, and now everything becomes normal.",thank much change learning rate small one everything becomes normal,issue,negative,positive,neutral,neutral,positive,positive
332760429,"Assuming you are using the right model with the right command, you may need a much smaller learning rate to start with.",assuming right model right command may need much smaller learning rate start,issue,negative,positive,positive,positive,positive,positive
332698190,"@ppwwyyxx Thanks, I already did the same way you do for this problem, It works properly.

I will pull your code to use it. Thanks! ",thanks already way problem work properly pull code use thanks,issue,negative,positive,positive,positive,positive,positive
332643467,Thanks. Will close this issue after I validate on the cluster.,thanks close issue validate cluster,issue,positive,positive,positive,positive,positive,positive
332627334,"Thanks for your great help, it is really helpful for me. I can train it now.

Btw, I saw a warning message as below:
```
WARNING:tensorflow:From ~/tensorpack/callbacks/param.py:69: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.
```

Is it okay? I think you made this change for distributed trainer yesterday.(584e9cd4cb2812c722b65a79309cc78ee9d5b552)",thanks great help really helpful train saw warning message warning removed please use instead think made change distributed trainer yesterday,issue,positive,positive,positive,positive,positive,positive
332621107,Should've been fixed now. Sorry for these bugs -- I never really used distributed trainer for anything serious so a lot of functionalities are not well tested.,fixed sorry never really used distributed trainer anything serious lot well tested,issue,negative,negative,negative,negative,negative,negative
332591146,Seems like I can reproduce the problem. I'll look into it.,like reproduce problem look,issue,negative,neutral,neutral,neutral,neutral,neutral
332556028,"No luck, by using my laptop as a single server, I still keep getting the above message... (the non-chief worker can not start working.)

May I know that by using your example codes, `cifar10-resnet.py`, I think I only need to replace original `SyncMultiGPUTrainerParameterServer` to `DistributedTrainerReplicated` and setup `tf.train.ClusterSpec` and `tf.train.Server`, right? Do I need to change Optimizer or model definition?

Here is codes examples [cifar10-resnet-dist.py]. (I modify the codes to run on CPU, replace AvgPooling with Stride Conv.) (https://gist.github.com/chunfuchen/158434a1d86f44d77666ec685f57e501)

Any suggestion would be appreciated. ",luck single server still keep getting message worker start working may know example think need replace original setup right need change model definition modify run replace stride suggestion would,issue,positive,positive,positive,positive,positive,positive
332418372,Added an option which can control the queue size of zmq on the receiver side.,added option control queue size receiver side,issue,negative,neutral,neutral,neutral,neutral,neutral
332321723,"okay, I need to ask admin about the machine configuration, I will try to find another machine to test it. Thanks.",need ask machine configuration try find another machine test thanks,issue,negative,positive,positive,positive,positive,positive
332314124,I can use single machine to run 2ps 2 workers using your cluster_spec. Not sure what's going on at your side. I did notice that I cannot run distributed training inside a chroot-based container but not sure that's related.,use single machine run sure going side notice run distributed training inside container sure related,issue,positive,positive,positive,positive,positive,positive
332309555,":( still can not train the model, 

My clusterSpec is 
```python
    cluster_spec = tf.train.ClusterSpec({
            'ps': ['localhost:2222', 'localhost:2232'],
            'worker': ['localhost:2223', 'localhost:2233']
        })
```

I start 4 processes: 2 param servers, 2 workers (start 2 workers first, and then start 2 param sersers.)

After 4 processes are started, I do see 2 param servers show the message:

```
[0926 15:04:30 @distributed.py:72] My role in the cluster: job=ps, task=0
[0926 15:04:30 @distributed.py:196] Running ps 0
[0926 15:04:30 @distributed.py:197] Kill me with 'kill 29729'
```
and

```
[0926 15:03:41 @distributed.py:72] My role in the cluster: job=ps, task=1
[0926 15:03:41 @distributed.py:196] Running ps 1
[0926 15:03:41 @distributed.py:197] Kill me with 'kill 29473'
```

One of workers displayed already

```
0926 15:05:34 @concurrency.py:36] Starting EnqueueThread QueueInput/input_queue ...
[0926 15:05:35 @base.py:228] Start Epoch 1 ...
0%|                                                         |0/5000[00:00<?,?it/s]
```

however, the other worker keep displaying  (I assume that this worker should show the same message to the other worker but it did not, so I guess there is something wrong.)
```
Start master session 6c3a06a21ffc4486 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: ""BFC"" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true
2017-09-26 15:24:24.040698: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session a27bc07052114c4c with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: ""BFC"" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true
2017-09-26 15:24:54.368830: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session ede2e745ee062fe4 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: ""BFC"" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true
2017-09-26 15:25:24.735236: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session f5989ba1e0196365 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: ""BFC"" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true
```

May I use single machine to mimic distributed learning? I am testing my script and then I will deploy on real cluster.

Thanks.
",still train model python start param start first start param see param show message role cluster running kill role cluster running kill one displayed already starting start epoch however worker keep assume worker show message worker guess something wrong start master session true true true start master session true true true start master session true true true start master session true true true may use single machine mimic distributed learning testing script deploy real cluster thanks,issue,positive,positive,positive,positive,positive,positive
332289484,I remember TF will print some duplicated logs when starting up distributed training. Can't recall if this is the one.,remember print starting distributed training ca recall one,issue,negative,neutral,neutral,neutral,neutral,neutral
332288955,"okay, thanks. 

Keep getting:
```
2017-09-26 14:18:27.912036: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 1dc3a76f13e926d3 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: ""BFC"" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true
```
I think it should be my fault in setting up cluster spec. Let me take a look first.
Thanks for your help.",thanks keep getting start master session true true true think fault setting cluster spec let take look first thanks help,issue,positive,positive,positive,positive,positive,positive
332287213,Yeah. TF by default allocate all GPU memory although a PS may not need GPU. You can start the worker first as a workaround.,yeah default allocate memory although may need start worker first,issue,negative,positive,positive,positive,positive,positive
332286958,"Thanks. I do not encounter the error but I get out of memory issue if I start param server and worker at the same gpu. (launch order: param server --> worker)

The param server will occupy all memory as tensorflow did by default. 
So, in practice, I need to set up the upper bound memory utilzation ratio for parameter server, right?
Or for distributed learning, the parameter server is allocated at cpu?
",thanks encounter error get memory issue start param server worker launch order param server worker param server occupy memory default practice need set upper bound memory ratio parameter server right distributed learning parameter server,issue,negative,positive,positive,positive,positive,positive
332054920,"I used `print(grads)` in the above snippet to see the name. I agree this is annoying. 
For multigpu training you can still use this to access per-tower gradients, but it'll be hard to access the aggregate gradients, because gradients are internal to the trainers. One feasible way is to write a `GradientProcessor` that does nothing but print the tensor.",used print snippet see name agree annoying training still use access hard access aggregate internal one feasible way write nothing print tensor,issue,negative,negative,negative,negative,negative,negative
332049865,"One (stupid) question:
How did you figure out the string for the gradient tensor? For example this string -- **gradients**/conv1/**Conv2D_grad/Conv2DBackpropInput**

I see such string in the events file in the log directory but for multi-gpu training they have ""towerXX"" in front of them. Is there a way to print the aggregate gradients?

Here is what I have in the callbacks and I am getting errors:
```
DumpTensor(['gradients/group2/block2/conv2/Conv2D_grad/Conv2DBackpropInput:0', 'gradients/group2/block2/conv2/Conv2D_grad/Conv2DBackpropFilter:0']),
DumpTensor(['group2/block2/conv2/output:0']),
```
`KeyError: ""The name 'gradients/group2/block2/conv2/Conv2D_grad/Conv2DBackpropInput:0' refers to a Tensor which does not exist. The operation, 'gradients/group2/block2/conv2/Conv2D_grad/Conv2DBackpropInput', does not exist in the graph.""`

The error is produced even when I run with a very small batch size on 1 gpu.",one stupid question figure string gradient tensor example string see string file log directory training front way print aggregate getting name tensor exist operation exist graph error produced even run small batch size,issue,negative,negative,negative,negative,negative,negative
331964631,"Crop, resize, mean subtraction, or any other possible preprocessing. Basically you need to go through a working evaluation code (assuming they have one) and figure out everything that's happening. ",crop resize mean subtraction possible basically need go working evaluation code assuming one figure everything happening,issue,negative,negative,negative,negative,negative,negative
331958561,Not a tensorpack question. Closing. You can ask this in some more general machine learning communities.,question ask general machine learning,issue,negative,positive,neutral,neutral,positive,positive
331846474,"Thanks for the quick response. I was in fact missing to adjust the labels, results are much better now.
Yet still the performance is notably (-15%) worse than it should be. Are there more common pitfalls you know of? I paid attention to the value ranges and BGR/RGB space as previously mentioned by you.",thanks quick response fact missing adjust much better yet still performance notably worse common know attention value space previously,issue,negative,negative,neutral,neutral,negative,negative
331609501,"Hi, coming back with another question that just crossed my mind. The reported accuracy numbers (from the code) after each epoch -- generated via inference runner -- are based-on the quantized weights. Right? ",hi coming back another question crossed mind accuracy code epoch via inference runner right,issue,negative,positive,positive,positive,positive,positive
331551904,"If you need to change it frequently then it makes sense to make that tensor one of your inputs.
If not, you can also create an op which modifies that variable, and run that op once a while, maybe through the `RunOp` callback.",need change frequently sense make tensor one also create variable run maybe,issue,negative,positive,neutral,neutral,positive,positive
331509874,"I have a follow up question. I can pass in a ""criteria"" tensor (built as part of the tf.global_variables) consists of only 0 or 1 to achieve per weight gradient control of any layer. However it seems that the ScaleGradient function only loads the content of this tensor once initially when the graph is built (which makes sense) and assume the content remains static.

As an example, initially the ""criteria"" tensor is set to 1 everywhere, meaning that every weight can be updated. Then at certain point I want to modify this tensor in my callback function so that say the first two rows (assuming its a 2D tensor) become 0, meaning the corresponding two rows of weight will no longer be updated. 

How would you go about approaching this problem? 

Thanks,",follow question pas criterion tensor built part achieve per weight gradient control layer however function content tensor initially graph built sense assume content remains static example initially criterion tensor set everywhere meaning every weight certain point want modify tensor function say first two assuming tensor become meaning corresponding two weight longer would go approaching problem thanks,issue,negative,positive,positive,positive,positive,positive
331503673,"btw, our examples contain an inceptionv3 code which reaches 74% accuracy. This is what you should __expect__ to get if you're training from scratch. 78% will need super large batch size (e.g. 1600). See Google's comments (https://github.com/tensorflow/models/blob/master/research/inception/inception/inception_train.py#L66)",contain code accuracy get training scratch need super large batch size see,issue,positive,positive,positive,positive,positive,positive
331501817,"As I said in #421 already there are many places things can be different. Calling a model from slim doesn't make them equivalent.

Most likely your issue is because of this: https://github.com/ppwwyyxx/tensorpack/issues/148#issuecomment-285965752",said already many different calling model slim make equivalent likely issue,issue,negative,positive,positive,positive,positive,positive
331500904,You are right. You can pass them through quantization function __before__ inference as well.,right pas quantization function inference well,issue,negative,positive,positive,positive,positive,positive
331476387,"Thanks. The saved models reflect the actual weights, not the quantized one. As such, when using such pretrained models for inference, one should pass the weights through the quantization function and then use it. Please correct me if I am wrong.",thanks saved reflect actual one inference one pas quantization function use please correct wrong,issue,positive,negative,neutral,neutral,negative,negative
331279494,GradientProcessor is designed for manipulating the gradient. You can subclass `GradientProcessor` and implement something similar to `ScaleGradient` like you've already done.,designed gradient subclass implement something similar like already done,issue,negative,neutral,neutral,neutral,neutral,neutral
331278762,"Hi ppwwyyxx,

Agree with you on the example I gave above,  but it might still be useful in some cases where one may want to manipulate the gradient on-the-fly based on some user-defined criteria. I was able to modify your ScaleGradient code so that the grad tensor is multiplied with a ""criteria"" tensor to achieve per weight gradient control. This way, I only need to modify the ""criteria"" tensor to do what I want.

Thanks for all the help",hi agree example gave might still useful one may want manipulate gradient based criterion able modify code grad tensor criterion tensor achieve per weight gradient control way need modify criterion tensor want thanks help,issue,positive,positive,positive,positive,positive,positive
331261732,"It's not saving the quantized version.
You can call `tf.summary.histogram` on the quantized tensor.
```diff
--- i/examples/DoReFa-Net/svhn-digit-dorefa.py  
+++ w/examples/DoReFa-Net/svhn-digit-dorefa.py  
@@ -65,7 +65,9 @@ class Model(ModelDesc):       
                 return v                       
             else:      
                 logger.info(""Binarizing weight {}"".format(v.op.name))                           
-                return fw(v)                   
+                ret = fw(v)                    
+                tf.summary.histogram(ret.name, ret)                                             
+                return ret                     
                        
         def cabs(x):   
             return tf.minimum(1.0, tf.abs(x), name='cabs')  
```",saving version call tensor class model return else weight return ret ret return ret return,issue,negative,neutral,neutral,neutral,neutral,neutral
331224858,"For 8 bit there are better options than DoReFa-Net. Even quantize a float32 network after training might be better. DoReFa-Net is designed for <=4 bits.
You can call those quantization ops in tensorflow directly. Although I doubt you'll see any performance improvement.",bit better even quantize float network training might better designed call quantization directly although doubt see performance improvement,issue,positive,positive,positive,positive,positive,positive
331210707,"This feature you need already break the abstraction of a ""layer"". It's best to then write the layer yourself.",feature need already break abstraction layer best write layer,issue,positive,positive,positive,positive,positive,positive
331198408,"Probably should have given more thoughts before posting. A colleague just pointed out I should able to accomplish what I described using ScaleGradient, but instead of passing in a scalar, pass in a tensor instead. 

Will come back if I figure out how it works.",probably given posting colleague pointed able accomplish instead passing scalar pas tensor instead come back figure work,issue,negative,positive,positive,positive,positive,positive
331196482,"Hi could you resolve visualization on test set after every 3 epochs.  

Thanks,",hi could resolve visualization test set every thanks,issue,positive,positive,positive,positive,positive,positive
330615128,"Thank you for such a quick response!
I would check even further to try to solve it...
Thanks anyway!",thank quick response would check even try solve thanks anyway,issue,positive,positive,positive,positive,positive,positive
330614532,"This is not something I can help with because any tiny difference between your code and the original ones can cause this and I couldn't know what that difference is.
You can actually just call `slim.inception_v3` inside tensorpack, and this might help reduce the differences.",something help tiny difference code original cause could know difference actually call inside might help reduce,issue,positive,positive,positive,positive,positive,positive
330298170,Thanks. I see. Does it discuss in any paper? Do you mind pointing me?,thanks see discus paper mind pointing,issue,negative,positive,positive,positive,positive,positive
330296117,They will be trained. It's a common technique to improve resnet.,trained common technique improve,issue,negative,negative,negative,negative,negative,negative
330293936,I can not find out the issue... I just re-train it and now it looks well... I will close this issue. Thanks.,find issue well close issue thanks,issue,positive,positive,positive,positive,positive,positive
330146319,The code looks fine and I don't know why this happened. Random initialization should not make such a big difference.,code fine know random make big difference,issue,negative,negative,neutral,neutral,negative,negative
330099562,"From the logs it looks like it will fail if you `import tensorflow.contrib` (`from tensorflow.contrib.framework import add_model_variable`
). I don't think it's related to tensorpack.",like fail import import think related,issue,negative,negative,negative,negative,negative,negative
330064298,"Here is my code gist:
https://gist.github.com/chunfuchen/1df7224432cd53fbd88bdd9ce143f589

The modification I did are commented with ""modified"" keyword, only three lines.

Thanks.",code gist modification three thanks,issue,negative,positive,positive,positive,positive,positive
330059595,I imagine posting your modified code in a GIST would help debug. ,imagine posting code gist would help,issue,negative,neutral,neutral,neutral,neutral,neutral
330026696,"Awesome, thanks for tackling it so quickly! What is the best way to install tensorpack from a commit like that?",awesome thanks tackling quickly best way install commit like,issue,positive,positive,positive,positive,positive,positive
329994944,Ah you're right. A lot of code need to be moved to gfile.,ah right lot code need,issue,negative,positive,positive,positive,positive,positive
329993414,"`MaxSaver` and `MinSaver` are for now useless on GCS as well, as `shutil.copy` and `glob.glob` are used instead of respectively `tf.gfile.Copy` and `tf.gfile.Glob`. shutil and glob will fail on paths starting with ""gs://""",useless well used instead respectively fail starting,issue,negative,negative,negative,negative,negative,negative
329992570,"Thanks for the explanations. After reading more on the code, I think it'll work to call `CreateDir` when `IsDirectory` returns false -- it will create a so-called ""directory marker"" in gcs. I can make this the default behavior in ModelSaver (since this is how tensorflow decides to make gcs compatible with traditional fs).",thanks reading code think work call false create directory marker make default behavior since make compatible traditional,issue,positive,negative,neutral,neutral,negative,negative
329992155,"You can't create a directory in GCS. 

In GCS, just like in S3 there is no real tree structure, it's all flat. A file `gs://mybucket/a/b/c/d/myfile` is just a file and `gs://mybucket/a/b` is NOT a directory. You can do with this address many things you'd do with a directory, e.g. you can ""list a directory"". However, you can't create a directory unless you place a file in it The closest analogy that should be familiar to github user is... git :) You can't `git add` an (empty) directory, can you?

On the other hand, if you have an _empty_ bucket `gs://mybucket`, unlike on typical filesystems, nothing stops you from writing a file into `gs://mybucket/a/b/c/d/myfile`. It's 100% legal.",ca create directory like real tree structure flat file file directory address many directory list directory however ca create directory unless place file analogy familiar user git ca git add empty directory hand bucket unlike typical nothing writing file legal,issue,positive,positive,positive,positive,positive,positive
329991662,"I'm not familiar with gcs, but from the [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/cloud/gcs_file_system.cc#L1220), it looks like `IsDirectory()` can return true for gcs path.
Do you have a better idea why it returns false for your path?",familiar code like return true path better idea false path,issue,positive,positive,positive,positive,positive,positive
329514302,"It is not a tensor. Otherwise you wouldn't have all the features of it.
There is a pair of braces in the end, to make it a tensor: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/mnist-convnet.py#L58",tensor otherwise would pair brace end make tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
328985584,"@ppwwyyxx thank you !
Actually, following your comment and the documentation [here](http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html#sequential-read) :
```
ds = LMDBData(db, shuffle=False)
ds = LocallyShuffleData(ds, 50000)

ds = PrefetchData(ds, 5000, 1) # !!! ADDING THIS !!!

ds = LMDBDataPoint(ds)
ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)
ds = AugmentImageComponent(ds, lots_of_augmentors)
ds = PrefetchDataZMQ(ds, 25)
ds = BatchData(ds, 256)
```
it helps also to avoid duplicates in my case too without using indices ! 

In my case it looks like :
```
train_ds = Cifar10('train')
pref_train_ds = PrefetchData(train_ds, nr_prefetch=5*batch_size, nr_proc=1)
aug_train_ds = AugmentImageComponent(pref_train_ds, geom_augmenters, index=0, copy=False)
aug_train_ds = AugmentImageComponent(aug_train_ds, color_augmenters, index=0, copy=False)  
train_batches = BatchData(aug_train_ds, batch_size=batch_size, use_list=True, remainder=False)
train_batches = PrefetchDataZMQ(train_batches, nr_proc=15)
```
Why in your examples you have (main process makes batches) :
```
... -> PrefetchDataZMQ -> BatchData
```
and not 
```
... -> BatchData -> PrefetchDataZMQ
```
is there a particular reason ? 




",thank actually following comment documentation lambda also avoid case without index case like main process particular reason,issue,negative,positive,positive,positive,positive,positive
328963606,"The interface is different in that we don't have `__getitem__`. DataFlow is an iterator interface, while pytorch dataset is an array interface.
An array interface is less performant because it needs to keep track of the indices of every data point. The difference is probably tiny, but keeping track of indices is also harder to write.

DataFlow can emulate the array interface, by having a dataflow producing the indices, and a `MapData` to gather the actual data from the indices. This way, parallel dataflow can avoid producing duplicated data as long as they take indices from the same queue, and this is exactly how pytorch multiprocess prefetch is implemented.

To do this, you can:
```python
df = DataFlowProducingIndices()
df = PrefetchData(df, 100, 1)  # use 1 multiprocessing.Queue for indices
df = MapData(df, from_indice_to_data)
df = PrefetchDataZMQ(df, 10)   # fork the mapping part
```
There are plans to make it more intuitive (something like MultiProcessMapData, similar to MultiThreadMapData).",interface different interface array interface array interface le performant need keep track index every data point difference probably tiny keeping track index also harder write emulate array interface index gather actual data index way parallel avoid data long take index queue exactly python use index fork part make intuitive something like similar,issue,negative,positive,neutral,neutral,positive,positive
328951872,"Hi @ppwwyyxx 

Sorry to revive this closed issue, I would like to comeback to `PrefetchData` function and duplicated datapoints. As you said, duplicates are explained by forking the dataflow, i.e. dataflow iterator.
Comparing to other frameworks, for example, *PyTorch*, their [`Dataset`](http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset) implementation requires similar methods: `__len__` and `__getitem__`. Another class [`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) provides similarly a [prefetch with multiprocessing](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py). Their implementation does not have duplicates in the beginning.
I wonder why you chose to provide iterator with `get_data` and not a similar to *pytorch* API ?

Thanks


 


",hi sorry revive closed issue would like comeback function said example implementation similar another class similarly implementation beginning wonder chose provide similar thanks,issue,positive,negative,neutral,neutral,negative,negative
328661776,"Ah, yup, of course. Thanks for the quick response",ah course thanks quick response,issue,negative,positive,positive,positive,positive,positive
328581507,"Setting TENSORPACK_PIPEDIR to /tmp resolves the issue. Now, I can run the original code with PrefetchDataZMQ on AFS. Thanks.",setting issue run original code thanks,issue,positive,positive,positive,positive,positive,positive
328319881,"TENSORPACK_PIPEDIR should not point to anywhere on afs.
You can use, for instance, '/tmp', '/dev/shm', which are usually not on afs.

The command to set it is `export TENSORPACK_PIPEDIR=/tmp`.

There are no suggestions for the parameters, usually this is something for users to tune. Your parameters seems OK.",point anywhere use instance usually command set export usually something tune,issue,negative,negative,negative,negative,negative,negative
328318719,"Thanks for the suggestion. When I use PrefetchData instead, the code is running fine on AFS. For the nr_prefetch, nr_proc arguments I am using 128, 4 respectively. Is there any recommended values for these parameters?
However, PrefetchDataZMQ is still generating error on AFS after setting $TENSORPACK_PIPEDIR
I am using the following command to set it on shell:
export TENSORPACK_PIPEDIR=$TENSORPACK_PIPEDIR:THE_DIR_PATH

bash-4.1$ echo $TENSORPACK_PIPEDIR
/afs/nd.edu/user19/ipalit/Segmentation/dorefa_net/pipedir/

I use the following command for running the code:
bash-4.1$ python3 svhn-digit-dorefa.py --dorefa 1,2,4 --gpu 0

The log is attached. Thanks again.
[log.txt](https://github.com/ppwwyyxx/tensorpack/files/1290255/log.txt)

",thanks suggestion use instead code running fine respectively however still generating error setting following command set shell export echo use following command running code python log attached thanks,issue,positive,positive,positive,positive,positive,positive
328290839,"It can run on afs. Could you paste the command you run, the directory you set, and the logs.

Also, you don't have to use PrefetchDataZMQ. Use PrefetchData which is slower but doesn't have filesystem requirement.",run could paste command run directory set also use use requirement,issue,negative,neutral,neutral,neutral,neutral,neutral
328288767,"Thanks. I created a directory and set the $TENSORPACK_PIPEDIR to this location, however it made no difference in output. So, does it mean that Tensorpack can't be run on AFS?",thanks directory set location however made difference output mean ca run,issue,negative,negative,neutral,neutral,negative,negative
328252345,I can work on it. I wouldn’t be able to finish until CVPR submission deadline though.,work able finish submission deadline though,issue,negative,positive,positive,positive,positive,positive
328197015,Btw I have a faster-rcnn code in tensorpack which gets reasonable performance on coco. But I'm busy with other stuffs and don't have time to clean it up and open source it recently.,code reasonable performance coco busy time clean open source recently,issue,negative,positive,positive,positive,positive,positive
328170097,Thanks. I had somehow missed the mnist-vis example. Its a nice example to access the internal variables. I also saw you updated the docs to mention these. Thanks again.,thanks somehow example nice example access internal also saw mention thanks,issue,positive,positive,positive,positive,positive,positive
327971447,"This diff can make `mnist-visualizations.py` dump the gradients:
```diff
diff --git i/examples/mnist-visualizations.py w/examples/mnist-visualizations.py                 
index b9fc60c..1ff6803 100755                   
--- i/examples/mnist-visualizations.py          
+++ w/examples/mnist-visualizations.py          
@@ -112,6 +112,12 @@ class Model(ModelDesc):    
                               regularize_cost('fc.*/W', tf.nn.l2_loss),                         
                               name='regularize_loss')                                           
         self.cost = tf.add_n([wd_cost, cost], name='total_cost')                                
+                       
+        c1_input = p0  
+        c1_W = c1.variables.W                  
+        grads = tf.gradients(self.cost, [c1_input, c1_W])                                       
+        print(grads)   
+                       
         summary.add_moving_summary(cost, wd_cost, self.cost, accuracy)                          
                        
         summary.add_param_summary(('.*/W', ['histogram', 'rms']))                               
@@ -144,6 +150,8 @@ def get_config():           
             ModelSaver(),                      
             InferenceRunner(                   
                 dataset_test, [ScalarStats('cross_entropy_loss'), ClassificationError('incorrect')]),
+            DumpTensor(['gradients/conv1/Conv2D_grad/Conv2DBackpropInput:0',                    
+                'gradients/conv1/Conv2D_grad/Conv2DBackpropFilter:0'])                          
         ],             
         steps_per_epoch=dataset_train.size(),  
         max_epoch=100, 
```",make dump git index class model cost print cost accuracy,issue,negative,neutral,neutral,neutral,neutral,neutral
327966147,"Also, I was using DumpTensor in my callbacks. 
I put DumpTensor('conv0/output:0'), in callbacks but I get error:
`KeyError: ""The name 'c:0' refers to a Tensor which does not exist. The operation, 'c', does not exist in the graph.""`

What should I put for the name of a tensor? I was trying this on mnist-convnet.py example.
Thanks
",also put get error name tensor exist operation exist graph put name tensor trying example thanks,issue,negative,positive,positive,positive,positive,positive
327962604,"Could you give an example? That would be helpful.
Say, in the tensorpack imagenet-resnet.py example, what should I do to dump the two gradients (wrt input and wrt weights) for CONV2 layer?
",could give example would helpful say example dump two input layer,issue,negative,neutral,neutral,neutral,neutral,neutral
327954447,Create that tensor in the graph with `tf.gradients` and use the above callback.,create tensor graph use,issue,negative,neutral,neutral,neutral,neutral,neutral
327954067,Is there a way to dump the gradient tensors for a batch of inputs?,way dump gradient batch,issue,negative,neutral,neutral,neutral,neutral,neutral
327344725,Added a page about performance tuning: http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html,added page performance tuning,issue,negative,neutral,neutral,neutral,neutral,neutral
327196555,"Ok, I have switched to tensorflow 1.3 and cudnn/6.0. Now it seems to work perfectly. It is a bit strange that cudnn 5.1 does not work at all, whereas cudnn/6 works fine. ",switched work perfectly bit strange work whereas work fine,issue,positive,positive,positive,positive,positive,positive
327096804,You can check out the tutorial about efficient data loading: http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html,check tutorial efficient data loading,issue,negative,neutral,neutral,neutral,neutral,neutral
327080547,"The average queue size during the first training epoch appears to be very low indeed: 2.32e-37. Also, because I was able to draw ~7.8 it/s with the --fake option on, so tensorflow seems good.

In the experiment, there seems to be some difference in performance (it/s) between train and predict phases: 0.19 it/s during training vs. 1.68 it/s during prediction.

I first thought that this could be explained by the difference in the amount of computation needed for the two phases, but I'm a little confused now because the input queue seems to be full (average queue size is 50?) during prediction.

```
[0905 03:44:38 @monitor.py:355] DataParallelInferenceRunner/QueueInput/queue_size: 50
[0905 03:44:38 @monitor.py:355] QueueInput/queue_size: 2.3199e-37
```

I'll look into further what may cause slow data in my system. Do you have any suggestion on how to best profile the system with the imagenet example?",average queue size first training epoch low indeed also able draw fake option good experiment difference performance train predict phase training prediction first thought could difference amount computation two phase little confused input queue full average queue size prediction look may cause slow data system suggestion best profile system example,issue,negative,positive,positive,positive,positive,positive
326887633,"The document actually said you need to use tuple. 
Anyway, now list is allowed.",document actually said need use anyway list,issue,negative,neutral,neutral,neutral,neutral,neutral
326871124,What's the queue size shown in the training log? If the queue is near empty you're blocked by data. It's easier to understand why data is slow than to understand why tensorflow is slow.,queue size shown training log queue near empty blocked data easier understand data slow understand slow,issue,negative,negative,negative,negative,negative,negative
326821173,"I cannot reproduce your problem.
If you're using cudnn 5.1.5 you should upgrade it. I cannot think of any other reasons.",reproduce problem upgrade think,issue,negative,neutral,neutral,neutral,neutral,neutral
326630559,"Different GPUs always communicate directly when they can.

The strategy would depend on the actual model and will need a number of trials, provided that you understand what and when communication is happening. TF documentation has something about sharding and it may help.",different always communicate directly strategy would depend actual model need number provided understand communication happening documentation something may help,issue,negative,positive,neutral,neutral,positive,positive
326599403,"Alternatively, someone could create that project and have it be a dependency of Tensorpack. While Tensorpack may not be the proper place for many of these loss functions, it has by far some of the cleanest and best implementations of them. I'd be happy to try to start that project if there is interest. @PatWie care to share a gist of the loss.py file?

Regardless, I wouldn't mind seeing loss.py as part of say the example section.",alternatively someone could create project dependency may proper place many loss far best happy try start project interest care share gist file regardless would mind seeing part say example section,issue,positive,positive,positive,positive,positive,positive
326529320,"Thanks for your reply.I am trying to do something for current model. Could you please give me some advice?
Is there a faster way to communicate among different GPUs? I supposed the current communication like this:GPU0->cpu->GPU1(e.g. cifar10_multi_gpu_train.py)
Is there a TF API for me to communicate among different GPUs directly? ",thanks trying something current model could please give advice faster way communicate among different supposed current communication like communicate among different directly,issue,positive,positive,neutral,neutral,positive,positive
326526921,"There are currently two multigpu methods in tensorpack (parameter server and replicated), which are both data-parallel but different in the communication pattern. They may have different performance on your task.

To get better performance for scenarios like this, you might need to design specific multi-gpu strategy for your task (e.g. sharding of variables, mix of data parallel and model parallel, etc). Tensorpack doesn't implement these cases.",currently two parameter server replicated different communication pattern may different performance task get better performance like might need design specific strategy task mix data parallel model parallel implement,issue,positive,positive,neutral,neutral,positive,positive
326447056,But I guess that's what the `freeze_graph` scripts in tensorflow is for? (although it's unclear how to use it..,guess although unclear use,issue,negative,neutral,neutral,neutral,neutral,neutral
326446591,"TF has a few in `tf.losses.xxx` and a few in `tf.nn.xxx_loss`. But yes they are very limited.
However I do need to remove `huber_loss` and use `tf.losses.huber_loss`.

Having some symbolic function collections is never a bad idea but tensorpack isn't the right place for it. I'd like to see a project called ""a bunch of useful TF symbolic functions"" and then not only tensorpack but also anyone who uses tensorflow could benefit from it.",yes limited however need remove use symbolic function never bad idea right place like see project bunch useful symbolic also anyone could benefit,issue,positive,positive,positive,positive,positive,positive
326445453,"I started writing a `loss.py` file. But I do not want to maintain it and it seems to be not in line with the decision to use `tf.layers`. But it might be worth at thinking about such a `loss.py` file as TensorFlow only supports streaming-metrics (not loss functions).

This can also incorporate all millions of proposed GAN-losses :wink: 

",writing file want maintain line decision use might worth thinking file loss also incorporate million wink,issue,negative,positive,positive,positive,positive,positive
326444905,Why not using `examples/scripts` to put an example (maybe for the mnist-classification) for freezing a graph.,put example maybe freezing graph,issue,negative,neutral,neutral,neutral,neutral,neutral
326435086,"Apologies for the break. I should've marked them as deprecated first.
They're removed for the same reasons why GAN is not included or [freeze graph](https://github.com/ppwwyyxx/tensorpack/issues/386#issuecomment-324694820) is not included. Being useful is never the whole reason to add something. ",break marked first removed gan included freeze graph included useful never whole reason add something,issue,negative,positive,positive,positive,positive,positive
326415764,"This is the current implementation of `PrefetchData` -- it forks the dataflow. There is no way to __automatically__ make a single iterator run in parallel, unless you fork it to become many iterators.

`ThreadedMapData` also runs in parallel but doesn't do the fork. (Similar things can be done for multiprocess as well but that hasn't been implemented). 

But they essentially have different semantics. By forking a dataflow it accelerates that dataflow. If only running the mapping function in parallel, the underlying dataflow doesn't get accelerated. Then you can let the underlying dataflow produces simple things such as indices or filenames, and let the mapping function do all the heavy work.",current implementation way make single run parallel unless fork become many also parallel fork similar done well essentially different semantics running function parallel underlying get accelerated let underlying simple index let function heavy work,issue,negative,positive,neutral,neutral,positive,positive
326413259,"@ppwwyyxx actually my initial problem before facing `FixedSizeData` is related to `PrefetchData` and what you mention in [documentation](http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.PrefetchData) : 
> e.g. you are likely to see duplicated datapoints at the beginning

It is very strange to see several identical batchs. Do you think we can do something with that or we can keep it like this ? 
",actually initial problem facing related mention documentation likely see beginning strange see several identical think something keep like,issue,negative,negative,neutral,neutral,negative,negative
326388071,"Ah man, I was using those symbolic functions. They are actually incredibly useful for any type of Siamese networks, I would recommend commonly used metrics/model types should be better stored in Tensorpack. For instance, does GAN.py with it's useful and specific trainers really belong in examples? ",ah man symbolic actually incredibly useful type would recommend commonly used better instance useful specific really belong,issue,positive,positive,positive,positive,positive,positive
326380675,"Yes, I see what you mean. So every epoch I will get the same sequence of images. Sure that this is not good.",yes see mean every epoch get sequence sure good,issue,positive,positive,positive,positive,positive,positive
326379192,"That's OK. But note that although you can get a subset of the dataset, you still lose the opportunity to fully shuffle the data. So maybe this is not that useful.
As it is a streaming interface, a full shuffle can only happen at the very beginning of the dataflow.",note although get subset still lose opportunity fully shuffle data maybe useful streaming interface full shuffle happen beginning,issue,negative,positive,positive,positive,positive,positive
326377424,Maybe an option can be added to get a different behavior.,maybe option added get different behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
326375932,"The [document](http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.FixedSizeData) says, ""The iterator state of the underlying DataFlow will be kept if not exhausted"". So this is actually an intended behavior.",document state underlying kept exhausted actually intended behavior,issue,negative,negative,negative,negative,negative,negative
326185307,I used opencv to combine the old and new one. Too lazy to run all algorithms again.,used combine old new one lazy run,issue,negative,negative,neutral,neutral,negative,negative
326161861,"Don't know exactly why, but running `gm convert results.jpg new.jpg` makes the image half the size without visible quality change 
(See ""swipe"" 
 in https://github.com/ppwwyyxx/tensorpack/pull/400/commits/12648f22c69393447cb2f1aab4746ce8a76b65fe)",know exactly running convert image half size without visible quality change see swipe,issue,negative,positive,neutral,neutral,positive,positive
326132437,"Using `reset_default_graph` under `tf.Graph()` is already forbidden in latest TensorFlow (I'm using nightly).

Using this and it will be fine:
```python
for val_fold_index in range(n_folds):
    with tf.Graph().as_default():
      # no reset
```

Or this:
```python
for val_fold_index in range(n_folds):
    # no new graph
    tf.reset_default_graph()
```",already forbidden latest nightly fine python range reset python range new graph,issue,negative,positive,positive,positive,positive,positive
326130249,"I rerun my code with the latest `tensorpack` and the bug disappeared 👍  
However, at the end there is an `IndexError` that is thrown after the messge `Train is finished!`. Here is the traceback:
```
[0830 21:45:55 @base.py:242] Training has finished!
Traceback (most recent call last):
  File ""cifar-convnet-cross-validation.py"", line 190, in <module>
    trainer.train()
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3815, in get_controller
    if self.stack[-1] is not default:
IndexError: list index out of range
[0830 21:45:55 @prefetch.py:174] [Prefetch Master] Context terminated.
Prefetch process exited.
[0830 21:45:55 @input_source.py:203] EnqueueThread QueueInput/input_queue Exited.
Prefetch process exited.
Prefetch process exited.
Prefetch process exited.
Prefetch process exited.
```
@ppwwyyxx  could you hint how to fix this? Thanks
",rerun code latest bug however end thrown train finished training finished recent call last file line module file line next file line default list index range master context process process process process process could hint fix thanks,issue,negative,positive,positive,positive,positive,positive
326120538,"What I had in mind is a MapData as well. Or maybe a subclass of AugmentorBase which calls iaa.

Just found that the keypoints transformations in iaa seems to assume integer coordinates -- this is going to hurt training for coordinate sensitive tasks.",mind well maybe subclass found assume integer going hurt training sensitive,issue,negative,positive,neutral,neutral,positive,positive
326116193,"Yes, there are nice features as `iaa.Sometimes`, `iaa.SomeOf` etc (similar to `RandomApplyAug` and `RandomChooseAug`), keypoints transformations, `iaa.ElasticTransformation` and `iaa.PiecewiseAffine`, and other color augmentations, dropouts etc. Image augmentation randomness is implemented in a [developed way](https://github.com/aleju/imgaug/blob/master/imgaug/parameters.py), various sampling provided.  
However, it mostly uses `skimage` vs `opencv` (in tensorpack) and bounded by `uint8` and range `0-255`.  In my experience, transformations like `iaa.PiecewiseAffine` can slow down dataflow.

@ppwwyyxx I do not know what kind of wrapping you would like to provide. Probably, the simplest way to use `aleju/imgaug` within tensorpack is with `MapData`. And it would be up to user to handle type of transformations, sampling, determinism/repeatability of `aleju/imgaug` transformations. 

HTH


",yes nice similar color image augmentation randomness way various sampling provided however mostly bounded range experience like slow know kind wrapping would like provide probably way use within would user handle type sampling,issue,positive,positive,positive,positive,positive,positive
326094345,"Should be fixed now. BTW, because you created a new graph each time, you probably don't need a reset.",fixed new graph time probably need reset,issue,negative,positive,positive,positive,positive,positive
326090230,I think you need to clear the graph with `tf.reset_default_graph()` before starting a new training.,think need clear graph starting new training,issue,negative,positive,neutral,neutral,positive,positive
325714370,I think it would be a good idea to include it as it's useful documentation that would otherwise be hard to find.,think would good idea include useful documentation would otherwise hard find,issue,positive,positive,positive,positive,positive,positive
325595081,"The simplest way to try it now is to just replace `dataflow=mydataflow` in TrainConfig by:
```python
ds = TFDatasetInput.dataflow_to_dataset(dataflow, [tf.float32, tf.int64]).prefetch(50)
TrainConfig(
data=TFDatasetInput(ds)
)
```",way try replace python,issue,negative,neutral,neutral,neutral,neutral,neutral
325086380,The sort of lame way of doing validation tracking would be to scrape the log files which output the validation statistics at the end of every epoch -- is there a Tensorpack script to do this already?,sort lame way validation would scrape log output validation statistic end every epoch script already,issue,negative,negative,negative,negative,negative,negative
324703352,Thanks! The code looks reasonable. However it's not relying on anything in tensorpack and not used by anything in tensorpack -- it's only demonstrating a usage of tensorflow. So it may not be a good idea to include them here.,thanks code reasonable however anything used anything usage may good idea include,issue,positive,positive,positive,positive,positive,positive
324694820,"i was stuck on this for too many days so i want to share my solution.
running freeze_graph directly on the tensorpack save files yielded different results than before the freeze_graph. i had to first apply export and only then freeze. i ended up with the following solution which worked for me:

`
        from tensorpack.tfutils.export import ModelExport
        from tensorflow.python.saved_model import tag_constants
        from tensorflow.python.framework.graph_util import convert_variables_to_constants
        
        tf.reset_default_graph()
        e = ModelExport(Model(), input_node_list, output_node_list)
        e.export(checkpoint_file, export_folder)

        with tf.Session() as sess:
            tf.saved_model.loader.load(sess, [tag_constants.SERVING], export_folder)
            G = tf.get_default_graph().as_graph_def()
            for node in G.node:
                node.device = ''
            G = convert_variables_to_constants(sess, G, output_node_list)
            with tf.gfile.GFile(frozen_filename, 'wb') as f:
                f.write(G.SerializeToString())
`


to infer:
`
        with tf.gfile.GFile(os.path.join(frozen_filename), 'rb') as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())

        with tf.Graph().as_default() as graph:
            tf.import_graph_def(graph_def)
            input_tensor = graph.get_tensor_by_name('import/input_nodename:0')
            output_tensor = graph.get_tensor_by_name('import/output_node_name:0')
            with tf.Session(graph=graph) as sess:
                output = sess.run(output_tensor, {input_tensor: input_image[None]})[0]
`
hopes this helps. if @ppwwyyxx is interested i can wrap this in some helper functions and make a PR
",stuck many day want share solution running directly save different first apply export freeze ended following solution worked import import import model sess sess node sess infer graph sess output none interested wrap helper make,issue,positive,positive,positive,positive,positive,positive
324688317,"So you mean they are in different threads?
Hence your answer to the `threading.Barrier` question is yes?
I think if `run()` (by `threading.Thread`) and `_trigger_step()` (by callbacks) are called in different threads, the answer should be yes. So these two threads can be synchronized.
Thanks!
",mean different hence answer question yes think run different answer yes two synchronized thanks,issue,positive,negative,neutral,neutral,negative,negative
324642254,"When something inherits `threading.Thread`, it's `run()` method will be called under a different thread.

Callback methods are always called by the trainer, so they are called under main thread.",something run method different thread always trainer main thread,issue,negative,positive,neutral,neutral,positive,positive
324579233,"I found a new API of Python 3 `threading.Barrier`, with the parameter `parties` of 2.
So now my question is straightforward, if in `_trigger_step(self)` and `run()`, the `threading.Barrier` is called,  would these two functions wait for each other? Thanks. ",found new python parameter question straightforward self run would two wait thanks,issue,negative,positive,positive,positive,positive,positive
324484087,"This is probably a common need (I'm using it recently as well), so I just added a callback `DumpTensors` that basically implements the above snippet: http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.DumpTensors",probably common need recently well added basically snippet,issue,negative,negative,negative,negative,negative,negative
324357125,You can try to make the function `f` in `MapDataComponent` become its member function instead of a closure. Maybe that'll make it pickleable.,try make function become member function instead closure maybe make,issue,negative,neutral,neutral,neutral,neutral,neutral
324355657,Yes. Windows has some different fork behavior so PrefetchData may not be available either.,yes different fork behavior may available either,issue,negative,positive,positive,positive,positive,positive
324199547," Sorry, I didn't explain it clearly. 
My question is how to calculate the derivatives (∂ro/∂ri) in equation 10. In other words, how to calculate the derivatives (∂Wb/∂W), Wb is the weight after quantization and W is the origin float-point weight.
My result is tanh(W)'==(1-tanh(W) * tanh(W)), but i see the other result in other realization.",sorry explain clearly question calculate equation calculate weight quantization origin weight result tanh tanh see result realization,issue,negative,negative,negative,negative,negative,negative
324085992,"It should work. But I don't use it for my work so I can't be sure.
Gradient update happens every k steps.",work use work ca sure gradient update every,issue,negative,positive,positive,positive,positive,positive
324085150,"I don't understand your question. Could you explain ""According to the formula, r0's derivative on r1 is 1-tanh2(r0)""",understand question could explain according formula derivative,issue,negative,neutral,neutral,neutral,neutral,neutral
323849830,"```python
class MyCallback(Callback):
    def _setup_graph(self):
	t = self.graph.get_tensor_by_name('conv0/output:0')
        self._fetches = tf.train.SessionRunArgs(fetches=[t])

    def _before_run(self, _):
        return self._fetches

    def _after_run(self, _, rv):
        t = rv.results
        np.save('output-{}.txt'.format(self.global_step), t)
```",python class self self return self,issue,negative,neutral,neutral,neutral,neutral,neutral
323594569,"Having a human-readable serialization in general is not always a possible thing, because conceptually an augmentor is allowed to contain any object. To repeat an experiment it's better to still use binary format.
Having a serialization which can be transformed into human-readable format is easier, and that's what `__str__` is for. 

Also, for `__repr__`, this is from Python official document:

`object.__repr__(self)`
Called by the repr() built-in function and by string conversions (reverse quotes) to compute the “official” string representation of an object. If at all possible, **this should look like a valid Python expression that could be used to recreate an object with the same value** (given an appropriate environment).

So if `__repr__` is written well, you can get human-readable serialization for most augmentors (actually all existing augmentors) already. And in this case you don't need pickle anymore.
For RandomCrop, this would be:
```python
def __repr__(self):
    return ""imgaug.RandomCrop({})"".format(self.crop_shape)

__str__ = __repr__
```

Tests would be nice, of course.. But I myself don't have plans for it now.",serialization general always possible thing conceptually contain object repeat experiment better still use binary format serialization format easier also python official document self function string reverse compute official string representation object possible look like valid python expression could used recreate object value given appropriate environment written well get serialization actually already case need pickle would python self return would nice course,issue,positive,positive,positive,positive,positive,positive
323594009,"I didn't tried pickle mostly because, actually, yes it is better (at least for me) to have human-readable presentation too. My idea behind is to be able to log into a file the whole pipeline used for training/validation and repeat a certain experiment after.

So, yes, I reimplement `__str__` of [`Augmentor`](https://github.com/vfdev-5/tensorpack/blob/master/tensorpack/dataflow/imgaug/base.py#L95) and print parameters without `rng` and any callable. 

I do not quite understand how is done module importing in `dataflow.imgaug` and that's why [`serialize` method code](https://github.com/vfdev-5/tensorpack/blob/master/tensorpack/dataflow/imgaug/base.py#L161) I couldn't put near [`deserialize`](https://github.com/vfdev-5/tensorpack/blob/master/tensorpack/dataflow/imgaug/__init__.py#L36) in `__init__.py`.
And do you plan to cover `imgaug` with tests ? 
",tried pickle mostly actually yes better least presentation idea behind able log file whole pipeline used repeat certain experiment yes print without callable quite understand done module serialize method code could put near plan cover,issue,positive,positive,positive,positive,positive,positive
323592995,"Does pickle work? Or do you really want human-readable serialization?
For human-readability you can always just implement `__str__` for the augmentors and use pickle + print. You can let `__str__` print those parameters.",pickle work really want serialization always implement use pickle print let print,issue,negative,positive,positive,positive,positive,positive
323553194,"#106 had something similar. 
You can probably also just use `tf.get_default_graph().as_graph_def()`.

According to tensorflow documents you seem to need those variables restored in the session. If you are starting from scratch you might need to load the metagraph first, than use `SaverRestore(checkpoint).init(sess)` to initialize the session (http://tensorpack.readthedocs.io/en/latest/modules/tfutils.html#tensorpack.tfutils.SessionInit).

Of course you can just use `tf.train.Saver().restore(sess, ...)` as well. Things are saved in standard TF format already so you don't have to worry anything about tensorpack.

Btw I never use these TF features so I'm not sure what is correct. ",something similar probably also use according seem need session starting scratch might need load first use sess initialize session course use sess well saved standard format already worry anything never use sure correct,issue,negative,positive,positive,positive,positive,positive
323552621,i could not figure out how do i get the graphdef from the file? and how should i initialize the session?,could figure get file initialize session,issue,negative,neutral,neutral,neutral,neutral,neutral
323540573,That part of code in tensorflow is not documented well and is not even a public API. After a glance I think it's probably easier to just call `tf.graph_util.convert_variables_to_constants` directly. You can get the graphdef from the metagraphdef file saved by tensorpack.,part code well even public glance think probably easier call directly get file saved,issue,positive,positive,neutral,neutral,positive,positive
323538190,Thanks. Could you provide some clues on how to apply freeze graph after using tensorpack?,thanks could provide apply freeze graph,issue,negative,positive,positive,positive,positive,positive
323533980,"The protobuf saved by `tf.saved_model.builder.SavedModelBuilder` is not the format expected by freeze_graph.
This issue is not related to tensorpack, closing.",saved format issue related,issue,negative,neutral,neutral,neutral,neutral,neutral
323210589,What is your status on this project? I spent some time on this while ago. The accuracy I got was ~10% less than the original [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn) implementation I used. I never found some time to review the code and find the problem. I can share the code with you if you are willing to work on that.,status project spent time ago accuracy got le original implementation used never found time review code find problem share code willing work,issue,negative,positive,positive,positive,positive,positive
323210334,"Are you saying that you parameterize a bounding box as 8 floats (instead of 4)? i.e. (x1, y1, ...., x4, y4) . That's the only way transformed points can be converted to a box.",saying bounding box instead way converted box,issue,negative,neutral,neutral,neutral,neutral,neutral
323209292,"I used a simple layer to convert the point coordinates to the box coordinates. It also makes sure box coordinates are inside the image:
```
@layer_register()
def convert_bboxes(bottoms):
    box_coords, w, h = bottoms
    shape = tf.shape(box_coords)
    xs = tf.reshape(box_coords[:, :, 0], [shape[0], -1, 4], name = 'reshape-xs')
    ys = tf.reshape(box_coords[:, :, 1], [shape[0], -1, 4], name = 'reshape-ys')

    # Change coordinates to what rcnn uses
    # Equivalent to tf.round(xs - 0.5)
    xs = tf.round(xs - .5)
    ys = tf.round(ys - .5)

    x1 = tf.reduce_min(xs, axis=-1, keep_dims=True, name='minx')
    x2 = tf.reduce_max(xs, axis=-1, keep_dims=True, name='maxx')
    y1 = tf.reduce_min(ys, axis=-1, keep_dims=True, name='miny')
    y2 = tf.reduce_max(ys, axis=-1, keep_dims=True, name='maxy')

    x1 = tf.clip_by_value(x1, .0, tf.to_float(w))
    x2 = tf.clip_by_value(x2, .0, tf.to_float(w))
    y1 = tf.clip_by_value(y1, .0, tf.to_float(h))
    y2 = tf.clip_by_value(y2, .0, tf.to_float(h))
    bbox = tf.concat(axis = 2, values=[x1,y1,x2,y2])
    return bbox
```
Since the same function could be used for any type of augmentation (including rotation and more complicated ones), I think having a separate helper function for that would be more convenient. It could be also implemented as a helper augmenter that does the conversion at the end.",used simple layer convert point box also sure box inside image shape shape name shape name change equivalent axis return since function could used type augmentation rotation complicated think separate helper function would convenient could also helper augmenter conversion end,issue,positive,neutral,neutral,neutral,neutral,neutral
323204937,To simplify the code I removed the freeze_collection part in 9850edf5342a27f. It also fixed an import bug which caused crashes in evaluation.,simplify code removed part also fixed import bug evaluation,issue,negative,positive,neutral,neutral,positive,positive
323017297,"Maybe one way is to add the `_augment_boxes` method, but let it call `_augment_coords` by default. Then only the rare cases such as flip & transpose needs to be dealt with.
What do you think @haamoon ? since you seem to have worked on r-cnn.",maybe one way add method let call default rare flip transpose need dealt think since seem worked,issue,negative,positive,positive,positive,positive,positive
323015840,"One caveat of the current design: 
Assuming you have box parameterized as (x1, y1, x2, y2), after a horizontal flip, coordinates are mapped to (x1', y1', x2', y2'), but this is not a valid bounding box anymore. The actual bounding box should become (x2', y1, x1', y2).
I don't know if there is any better way to handle this other than letting the user do it. In general the problem is that the different coordinates in the Nx2 array might have some structure that can be broken by the augmentor -- so probably the user do have to take care of it manually. ",one caveat current design assuming box horizontal flip valid bounding box actual bounding box become know better way handle user general problem different array might structure broken probably user take care manually,issue,negative,positive,neutral,neutral,positive,positive
322999418,"That's how [PrefetchDataZMQ](http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.PrefetchDataZMQ) is implemented.

Some possible combinations of dataflow are shown in the [tutorial](http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html). As mentioned above, [`ThreadedMapData`](http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.ThreadedMapData) may help if fork is undesired, but it all depends on the actual problem.",possible shown tutorial may help fork undesired actual problem,issue,negative,neutral,neutral,neutral,neutral,neutral
322998353,"@ppwwyyxx Hi, yuxin. Would you explain why DataFlow with prefetchDataZMQ will forks the ImageNet reader? Can we avoid this problem while keep it fast? Thanks!",hi would explain reader avoid problem keep fast thanks,issue,negative,positive,positive,positive,positive,positive
322955053,"Good! Thanks a lot. Your words are very clear.
Thanks!",good thanks lot clear thanks,issue,positive,positive,positive,positive,positive,positive
322953129,"No. These variables will not be trained. In tensorflow, only variables added to this collection `tf.GraphKeys.TRAINABLE_VARIABLES` will be trained by default. These variables are added and them removed from the collection.

Your observation is correct. You can remove `freeze_collection` and it will work the same. But tensorpack will print warnings when some ""TRAINABLE_VARIABLES"" have no gradient, which is usually a bug. freeze_collection avoid this warning message. ",trained added collection trained default added removed collection observation correct remove work print gradient usually bug avoid warning message,issue,negative,negative,negative,negative,negative,negative
322952370,"Thanks.
So what is done here is that these variables might be trained, but within that context, these variables would finally be restored to its original value. So anyway, implicitly, they are not trained, right?
I still have a doubt. In the code, there are the following:
`best_v = tf.reduce_max(targetQ_predict_value, 1)    # N,`
or
`best_v = tf.reduce_sum(targetQ_predict_value * predict_onehot, 1)`, depending on whether the DQN variant is double or not. But I feel the point locates at the following code:
`target = reward + (1.0 - tf.cast(isOver, tf.float32)) * self.gamma * tf.stop_gradient(best_v)`

We can notice the `tf.stop_gradient` operation. So is it still necessary to have the `collection.freeze_collection` when creating the target network? I mean the variables within the target network are all `stop_gradient` to some extent.
So my new question is whether `collection.freeze_collection` is still necessary.
I am not questioning the correctness of the code. I know it is correct. I just want to make sure I understand the underlying logic. Thanks a lot!




",thanks done might trained within context would finally original value anyway implicitly trained right still doubt code following depending whether variant double feel point following code target reward notice operation still necessary target network mean within target network extent new question whether still necessary correctness code know correct want make sure understand underlying logic thanks lot,issue,positive,positive,positive,positive,positive,positive
322815432,Variables in this scope don't need to be trained. So they shouldn't be added to the TRAINABLE_VARIABLES collection.,scope need trained added collection,issue,negative,neutral,neutral,neutral,neutral,neutral
322609003,Those examples only support one data format.,support one data format,issue,negative,neutral,neutral,neutral,neutral,neutral
322608808,"I see. 
Why is this transformation not done for other networks in the examples - alexnet, VGG, inception?",see transformation done inception,issue,negative,neutral,neutral,neutral,neutral,neutral
322277027,"You use whichever you like, and implement new layers in whichever ways you like. They are just doing the same thing with different interface (different function name, argument name, etc) and you can always use one in place of anther. Why would there be confusion?",use whichever like implement new whichever way like thing different interface different function name argument name always use one place anther would confusion,issue,negative,positive,neutral,neutral,positive,positive
322275522,The question then begins if we create new layers should we support the Tensorflow arguments or the legacy Tensorpack ones? How can we easily mix the argscopes between Tensorpack and Tensorflow? Or is that what the custom getter is trying to achieve? And if Tensorpack layers are not being deprecated it becomes confusing when to use which one. Should the user register a new tf.layer or a Tensorpack layers? This seems like it is going to cause a lot of issues down the line if we don't eventually merge or deprecate the Tensorpack layers.,question create new support legacy easily mix custom getter trying achieve becomes use one user register new like going cause lot line eventually merge deprecate,issue,positive,positive,positive,positive,positive,positive
322260908,"great,

thanks for the quick response!",great thanks quick response,issue,positive,positive,positive,positive,positive,positive
322259581,"The reason is simply because that's what most people use. The same reason why other wrappers including `tf.layers.conv2d` assumes float32.

For special needs, you don't have to use tensorpack layers. You can write symbolic function yourself, or some other wrappers, e.g.:
```python
from tensorflow.python.layers.convolutional import Conv2D
Conv2D(..., dtype=tf.float16)
```

Layers are not being deprecated. #291 is only talking about using `tf.layers` for the implementation of layers.

UPDATE: float16 is fully supported in layers for a long time now.",reason simply people use reason float special need use write symbolic function python import talking implementation update float fully long time,issue,negative,positive,positive,positive,positive,positive
321983877,In any case you can use `BatchData(use_list=True)` which does no type check. Otherwise it only makes sense to batch ndarray from ndarray.,case use type check otherwise sense batch,issue,negative,neutral,neutral,neutral,neutral,neutral
321946790,"As said in the readme, this implementation follows the architecture in fb.resnet.torch.
And this is the file I referred to: https://github.com/facebook/fb.resnet.torch/blob/master/models/preresnet.lua

The arguments of BNReLU is `BNReLU(layer_name, input)`.

UPDATE: you can use BNReLU in two ways:
`BNReLU(layer_name, input, name=output_tensor_name)`
or 
`BNReLU(input, name=output_tensor_name)`.
The first way is undocumented.",said implementation architecture file input update use two way input input first way undocumented,issue,negative,positive,positive,positive,positive,positive
321911735,"The readme already made it clear:
```
Training code of ResNet on ImageNet, with pre-activation and squeeze-and-excitation. The pre-act ResNet follows the setup in fb.resnet.torch (except for the weight decay) and gets similar performance (with much fewer lines of code).
```
`load-resnet.py` has its own usage (and its only usage) and it's also mentioned in the readme.",already made clear training code setup except weight decay similar performance much code usage usage also,issue,negative,positive,positive,positive,positive,positive
321757924,"Thanks. So you wrote a inferencer, and the methods were recently renamed, so it prints a warning.",thanks wrote recently warning,issue,negative,positive,neutral,neutral,positive,positive
321757262,"I'm using the default `InferenceRunner`. Some more details:

```
def get_valid_data_generator(data, data_shape, label, label_shape, batch_size = 16):
    ds = Dataset(data, label)
    shape_aug = [
        imgaug.CenterCrop(data_shape),
    ]
    ds = AugmentImageComponents(ds, shape_aug, (0, 1), copy=True)
    augmentors = [
        imgaug.CenterCrop(label_shape),
    ]        
    ds = AugmentImageComponent(ds, augmentors, index = 1, copy=True)
    ds = BatchData(ds, batch_size)
    return ds
```
And the setup in `TrainConfig`:

```
InferenceRunner(
      get_valid_data_generator(x_valid, base_shape,
                                                y_valid, mask_shape,
                                                batch_size = 32),
      ClassificationStats('predmap', 'segmapx'))
```",default data label data label index return setup,issue,negative,neutral,neutral,neutral,neutral,neutral
321592481,"They use the ""slow"" K80 for benchmarking. I can confirm that this is an unrealistic setting (try that with the new TitanX). So far I can only use 2 GPU for real-world task and 4 GPUs for image classification on a single machine.",use slow confirm unrealistic setting try new far use task image classification single machine,issue,negative,negative,negative,negative,negative,negative
321563557,"Thanks.

The EC2 distributed benchmark at the bottom of https://www.tensorflow.org/performance/benchmarks indicates good scalability. Am I reading it wrong or is that an unrealistic setting? I would like to understand the current situation better before choosing a solution.",thanks distributed bottom good reading wrong unrealistic setting would like understand current situation better choosing solution,issue,positive,positive,neutral,neutral,positive,positive
321490865,Last time I tried tensorflow still didn't have good distributed performance. So I never wrote an example about it..,last time tried still good distributed performance never wrote example,issue,negative,positive,positive,positive,positive,positive
321396742,Oh it's actually still working. I thought I broke it.,oh actually still working thought broke,issue,negative,neutral,neutral,neutral,neutral,neutral
321356135,"It really does deserve a ton of stars because it makes Tensorflow actually useful by adding much better support for experimentation. The dataflow and imgaug operators alone make it extremely useful and all the excellently well maintained examples make learning the code base easy. The multiGPU features are absolute icing on the cake and really help on large datasets. Also the documentation, quick bug fixes, and excellent examples are impeccable. 👍 ",really deserve ton actually useful much better support experimentation alone make extremely useful excellently well make learning code base easy absolute icing cake really help large also documentation quick bug excellent impeccable,issue,positive,positive,positive,positive,positive,positive
321328807,Oh it's probably just the get_vatiable issue. It creates duplicate variables in each tower. So there is no gradient for cost and variables that are on different tower.,oh probably issue duplicate tower gradient cost different tower,issue,negative,neutral,neutral,neutral,neutral,neutral
321325459,"If you want data-parallel you need `tf.get_variable` instead of `tf.Variable` , this way you can reuse the variables.

For the rest I don't have time for now to look at it. Maybe it's just you use some ops that doesn't have gradients?",want need instead way reuse rest time look maybe use,issue,negative,neutral,neutral,neutral,neutral,neutral
321324034,"But I run it ok when I use QueueInputTrainer.
this is my code https://gist.github.com/tonyw/e9d1ec7f0debd58b7b1433207adc1894
I set the lost like this 
> self.cost = tf.reduce_mean(self.cost) + l2_penalty + l1_penalty
Is there something wrong with me?",run use code set lost like something wrong,issue,negative,negative,negative,negative,negative,negative
321314730,"opencv **compiled with cuda** may slow down tensorflow **initialization**. It seems perfectly fine to me.
Also opencv is an optional dependency of tensorpack. ",may slow perfectly fine also optional dependency,issue,positive,positive,neutral,neutral,positive,positive
321313968,"Your previous dataflow produces a list of list, which is not supported by BatchData. BatchData only works for a list of [ndarray or scalar].
I don't know any good way to support it, because a list can contain anything...",previous list list work list scalar know good way support list contain anything,issue,positive,positive,positive,positive,positive,positive
321313101,"Prefetch has different mechanism, please see the [tutorial](http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html) for more details. And in practice it depend on what interface you have to your data file, and what kind of dataflow you need, e.g. can you afford random access in the file? Do you want dataflow to be shuffled? If each datapoint is a random sample, do you want each pass of the dataflow to be exactly the whole data file? All these questions affect how you write your dataflow.
If you have any specific question after reading the tutorial, feel free to ask with your specific scenario.
If you just don't want parallel workers producing the same data, ThreadedMapData may help.",different mechanism please see tutorial practice depend interface data file kind need afford random access file want random sample want pas exactly whole data file affect write specific question reading tutorial feel free ask specific scenario want parallel data may help,issue,positive,positive,neutral,neutral,positive,positive
321312005,The trainer is saying it finds no variables to optimize. This agrees with the warning messages you saw ( No Gradient w.r.t tower0/emb_1/emb1) You might need to check your models. ,trainer saying optimize warning saw gradient might need check,issue,negative,neutral,neutral,neutral,neutral,neutral
321310907,"You can import your file directly... not import from tensorpack.
e.g. this line: https://github.com/ppwwyyxx/tensorpack/blob/5c241e091f0bfa3723bb031482e727e946ad0304/examples/CTC-TIMIT/train-timit.py#L21",import file directly import line,issue,negative,positive,neutral,neutral,positive,positive
320995926,Yes. #139 has a todo list about improving the performance.,yes list improving performance,issue,positive,neutral,neutral,neutral,neutral,neutral
320990404,"Thanks for rapid reply. Then, I think the above codes can be used for `DataParallelInfereneRunner `, right? 

With this improvement, I can ~2x by using `DataParallelInfereneRunner ` ( 4 gpus are used.)
",thanks rapid reply think used right improvement used,issue,positive,positive,positive,positive,positive,positive
320989125,"We had sparse inputdesc support for a while, but later it introduces some issues mainly because not all tensorflow features work for sparse placeholders. We might revisit this feature at some time. 

If my understanding is correct, in tensorflow a sparse tensor is just 3 dense tensors? You can check out [CTC-TIMIT](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/CTC-TIMIT/train-timit.py) example which uses 3 dense InputDesc to get a sparse tensor.",sparse support later mainly work sparse might revisit feature time understanding correct sparse tensor dense check example dense get sparse tensor,issue,negative,positive,neutral,neutral,positive,positive
320876406,"- [x] make a fast dataflow for inference (with epoch-preserving prefetch)
- [x] let InferenceRunner support QueueInput
- [x] let DataParallelInferenceRunner support QueueInput",make fast inference let support let support,issue,positive,positive,positive,positive,positive,positive
320873086,"```python
    ds = ILSVRCNames()

    aug = imgaug.AugmentorList(augs)
    def mapf(dp):
        fname, cls = dp
        im = cv2.imread(fname, cv2.IMREAD_COLOR)
        if im.ndim == 2:
            im = np.expand_dims(im, 2).repeat(3, 2)
        im = aug.augment(im)
        return im, cls
    ds = ThreadedMapData(ds, 30, mapf, buffer_size=1000, strict=True)
    ds = BatchData(ds, batch)
    ds = PrefetchDataZMQ(ds, 1)
    TestDataSpeed(ds).start_test()
```
With this [code](https://bitbucket.org/ppwwyyxx/tensorpack-benchmark/src/66999b67b20712575e2ed87f0f2a32e817897813/ImageNet/benchmark-ImageNet.py?at=master&fileviewer=file-view-default), and `ILSVRCNames` producing simply (filename, label) pairs, I am able to get about 20x speed up (about 1.8k images/s) over the non-prefetched dataflow, while still keeping it equal to the whole validation set. This speed is often enough to keep at least a couple of GPUs busy. 

GPUs are becoming so fast that it's already pretty hard to keep them busy at training -- for inference it's harder, especially with Python. I'll go with this solution for now and update examples/docs these days unless some day I found it's becoming a bottleneck again.

Closing this now and track #139 on inference performance. ",python return batch code simply label able get speed still keeping equal whole validation set speed often enough keep least couple busy becoming fast already pretty hard keep busy training inference harder especially python go solution update day unless day found becoming bottleneck track inference performance,issue,positive,positive,positive,positive,positive,positive
320857929,"I guess there are two types of solutions: either fork ImageNet reader or not.
My current plan was to use a single reader (to just produce file names, for example) but multiple workers to process it (`ThreadedMapData` is already doing something similar). This way the imagenet reader doesn't need to be aware of potential parallelism. 
It might be also a solution to fork but let each reader have its own range.",guess two either fork reader current plan use single reader produce file example multiple process already something similar way reader need aware potential parallelism might also solution fork let reader range,issue,negative,positive,neutral,neutral,positive,positive
320856697,"Thanks for your help as well! 

I think tflearn and keras are easier to learn and therefore welcomed by a larger community, although they maybe less powerful.
Anyway I'll continue improving it. If it could ever reach a reasonable size of community I will also move it to the organization account from you.",thanks help well think easier learn therefore community although maybe le powerful anyway continue improving could ever reach reasonable size community also move organization account,issue,positive,positive,positive,positive,positive,positive
320845446,Yes. For stochastic training it doesn't matter.,yes stochastic training matter,issue,negative,neutral,neutral,neutral,neutral,neutral
320845006,"Thanks. I see. Nonetheless, in this case, for training data, after completing one epoch, it also does not guarantee all images are processed once, right? (It might be okay for training.)",thanks see nonetheless case training data one epoch also guarantee right might training,issue,positive,positive,positive,positive,positive,positive
320841173,"You shouldn't enable multi-process prefetch for inference (and that's one of the reasons why inference is slow and that's what I'm fixing).
You can use at most one process for prefetching in inference, otherwise you have many processes reading validation data at the same time -- processes don't talk to each other, you may end up getting one data point multiple times. The second process might produce image 1,2,3 which the first process already read and produced.

As a result you get an estimate of the validation error, but not the true validation error. When you use shuffle=True for validation you get an unbiased estimate of the validation error.",enable inference one inference slow fixing use one process inference otherwise many reading validation data time talk may end getting one data point multiple time second process might produce image first process already read produced result get estimate validation error true validation error use validation get unbiased estimate validation error,issue,negative,positive,positive,positive,positive,positive
320838956,"Here are my logs on resnet-18 over the ImageNet dataset. (I tested on P100)
w/o `DataParallelInfereneRunner`: ~50 seconds
w/  `DataParallelInfereneRunner` (4 gpus): ~26 seconds

Even if it is not linear speedup, it still helps a lot.

Btw, to achieve the above results, I need to enable data prefetch for inference as well (in your example codes, the prefetch is disable.)

[examples/ResNet/imagenet-resnet.py line 79](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py#L79)
```
if isTrain:
    ds = PrefetchDataZMQ(ds, min(20, multiprocessing.cpu_count()))
```",tested even linear still lot achieve need enable data inference well example disable line min,issue,negative,neutral,neutral,neutral,neutral,neutral
320829763,"Probably a bug I introduced while working on #139. I will take a look.

btw are you able to see any speed improvements over the non-parallel InferenceRunner? It's quite slow in my cases so I reopen #139.",probably bug working take look able see speed quite slow reopen,issue,negative,positive,neutral,neutral,positive,positive
320811268,The change is supposed to be transparent to users. There won't be any trouble as said in the issue.,change supposed transparent wo trouble said issue,issue,negative,negative,negative,negative,negative,negative
320810124,"Any update on this front? The longer we wait to fix this, the more troublesome an issue it will become.",update front longer wait fix troublesome issue become,issue,negative,neutral,neutral,neutral,neutral,neutral
320586586,"Yes. We didn't use protobuf either.
Our zeromq receiver in Python uses msgpack to deserialize, and our zeromq receiver op just uses memcpy to deserialize.

And deserialization is also just a pre-processing stage -- it can always run asynchronously. As long as it's not too slow it doesn't affect training speed from my experience. But I think there will be scenarios (e.g. much faster GPUs as you said) where it matters. ",yes use either receiver python receiver also stage always run long slow affect training speed experience think much faster said,issue,negative,negative,neutral,neutral,negative,negative
320585458,"While copying itself might not be the bottleneck, serialisation might be :) See https://github.com/tensorflow/tensorflow/issues/10530 for the rationale. Removing protobuf serialisation of tensors improves the throughput from 0.5GB/s to 1.5GB/s, so it could be quite significant. In fact, even the gRPC runtime has specialised implementation to avoid serialisation; see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_tensor_coding.h#L51).",might bottleneck might see rationale removing throughput could quite significant fact even implementation avoid see,issue,negative,positive,positive,positive,positive,positive
320584286,"Thanks byron for your work in GDR!
We don't use IB in clusters and for data feeding the use of IB could be limited. It reduces the copy latency, but in practice I was never bounded by copy since the latency can usually be hidden.",thanks work use data feeding use could limited copy latency practice never bounded copy since latency usually hidden,issue,negative,negative,neutral,neutral,negative,negative
320580945,@PatWie Any comments? https://github.com/tensorflow/tensorflow/pull/11392 is our PR to improve model update efficiency during training. It improves `RecvTensor`'s throughput of large tensors (>100MB) from 0.8GB/s to 3.6GB/s on a single machine. I am wondering if the same technique could be applied in data feeding as well.,improve model update efficiency training throughput large single machine wondering technique could applied data feeding well,issue,positive,positive,neutral,neutral,positive,positive
320579612,"I am interested in providing peer direct DMA support to TF data feeding, following the same spirit of my GDR patch. It seems to be a bottleneck transferring tensors from either network or disk to GPU;
as better and faster GPUs become available in-stock, it would become an important feature for actual deployment, which is often ignored during benchmarking with synthetic data.

With proper hardware (rNIC or NVMe SSD) peer direct DMA could bypass CPU and host memory and feed tensors to GPU directly. 

The tentative design would be re-using the current ZMQ operator to setup control plane, and avoid serialisation and memory copies using direct DMA in the data plane. Batching and pipelining would be helpful to reduce the latency and increase the throughput.

If your cluster has IB/NVMe available, let me know! We are interested in deployments of peer direct DMA in realistic applications, and we'd love to help.",interested providing peer direct support data feeding following spirit patch bottleneck transferring either network disk better faster become available would become important feature actual deployment often synthetic data proper hardware peer direct could bypass host memory feed directly tentative design would current operator setup control plane avoid memory direct data plane would helpful reduce latency increase throughput cluster available let know interested peer direct realistic love help,issue,positive,positive,positive,positive,positive,positive
320464637,"Better to ask why `tensorflow.contrib` import all modules in [`__init__.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/__init__.py).

TF is improving it recently. See
https://github.com/tensorflow/tensorflow/pull/11919 https://github.com/tensorflow/tensorflow/pull/11830",better ask import improving recently see,issue,positive,positive,positive,positive,positive,positive
320464132,"The above commits should improve it a little bit, but the majority of time is spent in tensorflow.
```python
# a.py:
import tensorflow.contrib.framework             
import cv2              
import time             
s = time.time()         
import tensorpack       
print(time.time() - s) 
```
`time python a.py` takes 8s to finish, but prints 0.25s",improve little bit majority time spent python import import import time import print time python finish,issue,negative,negative,negative,negative,negative,negative
320115241,I just removed protobuf dependency. The op should be at least runnable now but there is still work to do.,removed dependency least runnable still work,issue,negative,negative,negative,negative,negative,negative
320098848,"None of the prefetching technique was applied, therefore the performance of `DataParallelInferenceRunner` is very pool. It's becoming a bottleneck for my training. Reopen.",none technique applied therefore performance pool becoming bottleneck training reopen,issue,negative,positive,positive,positive,positive,positive
319936250,EVALUATE_PROC in train-atari.py is of no use. Maybe it's a good idea to remove it.,use maybe good idea remove,issue,negative,positive,positive,positive,positive,positive
319891277,"Btw I'm going to fix it by just letting it call `tf.layers.batch_normalization` with renorm option, since wrapping symbolic functions to layers is not a focus of this project. You can also just call `tf.layers` instead of tensorpack layers.

The two implementation are checkpoint-incompatible because they have different number of variables. I would just trust the TF version since they wrote the paper.",going fix call option since wrapping symbolic focus project also call instead two implementation different number would trust version since wrote paper,issue,positive,neutral,neutral,neutral,neutral,neutral
319722337,"It's weird, but likely a TF or cuda issue. And since it works on newer version probably no one in TF team would look at it.",weird likely issue since work version probably one team would look,issue,negative,negative,negative,negative,negative,negative
319706704,"One iteration means one [update-step](https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/train/base.py#L102). It is then `batchsize*towers` unless you [change it](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py#L89) explictly.

The shuffle should be the same as in single-gpu.",one iteration one unless change shuffle,issue,negative,neutral,neutral,neutral,neutral,neutral
319693528,"Ok, let me then ask these questions:
What does an iteration mean in the case of multi-gpu training? Is it
batch_size*nr_tower or batch_size ?

Are there any requirements when it comes to shuffling data in dataflow when
using multigpu trainer?

Best,
Maciej


On 2 August 2017 at 08:29, Yuxin Wu <notifications@github.com> wrote:

> Closed #359 <https://github.com/ppwwyyxx/tensorpack/issues/359>.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/359#event-1188938584>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AAiVLZrFZc2BthsdzilWcgUKIgVi6P-qks5sUBdRgaJpZM4OqpwW>
> .
>
",let ask iteration mean case training come shuffling data trainer best august wrote closed thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
319681611,"Ok, I just tested adding GraphProfiler to imagenet-resnet.py from examples and figured out that

```
python imagenet-res.py --fake --gpu=0
```

Crashes on tensorflow-1.1.0 but seems to run ok on tensorflow-1.2.1-rc",tested figured python fake run,issue,negative,negative,negative,negative,negative,negative
319552391,"Ah I see what you are talking about. The two equation actually have different value, but you're right they have the same gradient <del>(actually differed by a factor of 2 I think)</del>. The value_loss I mentioned above has the same value, but the wrong gradient.

I think it's fine to keep it in the current equation to be consistent with the formulation used in the paper.",ah see talking two equation actually different value right gradient actually factor think value wrong gradient think fine keep current equation consistent formulation used paper,issue,negative,positive,neutral,neutral,positive,positive
319545857,"Thanks for the instant reply.

What I mean is what is the gradient of `tf.nn.l2_loss`?
The answer seems to be just the `advantage * \nabla self.value`.
So  one really needs the `stop_gradient` of `advantage` to make the gradient be correct.

I'm not sure of its correctness. So I'm just asking you for the advice.",thanks instant reply mean gradient answer advantage one really need advantage make gradient correct sure correctness advice,issue,positive,positive,positive,positive,positive,positive
319545023,"I think what you meant to say is `value_loss = tf.reduce_sum(advantage * advantage, name='value_loss'`.

And this is wrong, because advantage has `stop_gradient`.",think meant say advantage advantage wrong advantage,issue,negative,negative,negative,negative,negative,negative
319423113,"before_run only adds 
```python
opt = tf.RunOptions()
opt.trace_level = tf.RunOptions.FULL_TRACE
```
to sess.run. I have no clue why it crashes, but it's probably not a tensorpack issue.",python opt clue probably issue,issue,negative,neutral,neutral,neutral,neutral,neutral
319347254,"Sometimes the pre-processing data can benefit from running on a GPU, e.g. you can use GPU 1&2 for training and 3&4 for pre-processing data on-the-fly (e.g. OpenCV supports GPUs or custom code)
see: https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/dataflow/prefetch.py#L207",sometimes data benefit running use training data custom code see,issue,negative,neutral,neutral,neutral,neutral,neutral
319249816,"We don't take ""paper implementation request"". Closing now.",take paper implementation request,issue,negative,neutral,neutral,neutral,neutral,neutral
319249645,If you still have any specific questions feel free to reopen the issue.,still specific feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
319249396,"After dc448bdab391a6eac87de47efd5f8a64caebaf99, [API documentation](http://tensorpack.readthedocs.io/en/latest/modules/models.html) was now updated to include the name argument.",documentation include name argument,issue,negative,neutral,neutral,neutral,neutral,neutral
319193362,Thanks and I'll find a place in tutorial to mention what is an iteration.,thanks find place tutorial mention iteration,issue,negative,positive,positive,positive,positive,positive
319132829,I don't think there will be confusion if you know what happen in an iteration. Progress bar has nothing to do with this -- it doesn't know what is a batch anyway.,think confusion know happen iteration progress bar nothing know batch anyway,issue,negative,neutral,neutral,neutral,neutral,neutral
319054684,"So I think if tensorpack supply a specific progressbar for sync Multi-GPU Trainer, it's will be a better job.And will avoid make other users confused like me.",think supply specific sync trainer better avoid make confused like,issue,negative,positive,neutral,neutral,positive,positive
319053994,"I red the ProgressBar class. I think you're right.The throughput is not a common definition for sync and async training. But In sync Multi-GPU Trainer,the throughput is just mean in a sync iterator,all gpu process data counter.",red class think throughput common definition sync training sync trainer throughput mean sync process data counter,issue,negative,negative,negative,negative,negative,negative
319052165,what's the definition of total throughput around all Gpu cards?,definition total throughput around,issue,negative,neutral,neutral,neutral,neutral,neutral
319051880,"Thank you.
Is there anyway to evaluate the total throughput around all Gpu cards in real time?",thank anyway evaluate total throughput around real time,issue,negative,positive,neutral,neutral,positive,positive
319044540,"In each iteration each tower takes one datapoint from your dataflow and train on it.
Therefore if your total batch size changed, you might want to change your epoch size.",iteration tower one train therefore total batch size might want change epoch size,issue,negative,neutral,neutral,neutral,neutral,neutral
319042561,"Oh, you mean the tensorpack would fetch double data , If I use two tower? 
In that case , if I set the max_epoch = 1, each tower run half epoch or each tower will run all epoch?",oh mean would fetch double data use two tower case set tower run half epoch tower run epoch,issue,negative,negative,negative,negative,negative,negative
319040935,"Then you should set batch size to be `batch_size / num_gpu`, as is done in [resnet example](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py#L89)",set batch size done example,issue,negative,neutral,neutral,neutral,neutral,neutral
319040755,"[log.log.zip](https://github.com/ppwwyyxx/tensorpack/files/1187338/log.log.zip)
there is the raw log.
I want to see a clearly performance raise by the progress bar.for example: if I use 1 GPU,the progress bar print 1it/s,when I use 2 GPU, the pregress bar print 2it/s. like this.
In that case, I did not change the batch size.",raw log want see clearly performance raise progress example use progress bar print use bar print like case change batch size,issue,positive,negative,neutral,neutral,negative,negative
319039722,"This would depend on how you write `get_data()`.
If you use the same batch size in `get_data()`, the total batch size would be doubled when you use a multigpu trainer.",would depend write use batch size total batch size would doubled use trainer,issue,negative,neutral,neutral,neutral,neutral,neutral
319039380,"when I run with 1 GPU,the progress bar print the throughput like this ""99/590[00:50<03:23,2.41it/s"". when I use 2 GPU's by changing the 'CUDA_VISIBLE_DEVICES',the progress bar print the same value.
At the begin, there are some log like this:
[0731 19:01:07 @model_utils.py:47] Model Parameters: 
name         shape                   dim  device
-----------  ----------------  ---------  -------------
conv1_1/W:0  [3, 3, 3, 64]          1728  /device:GPU:0
conv1_1/b:0  [64]                     64  /device:GPU:1
conv1_2/W:0  [3, 3, 64, 64]        36864  /device:GPU:1
conv1_2/b:0  [64]                     64  /device:GPU:0
conv2_1/W:0  [3, 3, 64, 128]       73728  /device:GPU:0
conv2_1/b:0  [128]                   128  /device:GPU:1
conv2_2/W:0  [3, 3, 128, 128]     147456  /device:GPU:1
conv2_2/b:0  [128]                   128  /device:GPU:0
conv3_1/W:0  [3, 3, 128, 256]     294912  /device:GPU:0
conv3_1/b:0  [256]                   256  /device:GPU:1
conv3_2/W:0  [3, 3, 256, 256]     589824  /device:GPU:1
conv3_2/b:0  [256]                   256  /device:GPU:0
conv3_3/W:0  [3, 3, 256, 256]     589824  /device:GPU:0
conv3_3/b:0  [256]                   256  /device:GPU:1
conv4_1/W:0  [3, 3, 256, 512]    1179648  /device:GPU:1
conv4_1/b:0  [512]                   512  /device:GPU:0
conv4_2/W:0  [3, 3, 512, 512]    2359296  /device:GPU:0
conv4_2/b:0  [512]                   512  /device:GPU:1
conv4_3/W:0  [3, 3, 512, 512]    2359296  /device:GPU:1
conv4_3/b:0  [512]                   512  /device:GPU:0
conv5_1/W:0  [3, 3, 512, 512]    2359296  /device:GPU:0
conv5_1/b:0  [512]                   512  /device:GPU:1
conv5_2/W:0  [3, 3, 512, 512]    2359296  /device:GPU:1
conv5_2/b:0  [512]                   512  /device:GPU:0
conv5_3/W:0  [3, 3, 512, 512]    2359296  /device:GPU:0
conv5_3/b:0  [512]                   512  /device:GPU:1
fc6/W:0      [25088, 4096]     102760448  /device:GPU:1
fc6/b:0      [4096]                 4096  /device:GPU:0
fc7/W:0      [4096, 4096]       16777216  /device:GPU:0
fc7/b:0      [4096]                 4096  /device:GPU:0
fc8/W:0      [4096, 2]              8192  /device:GPU:0
fc8/b:0      [2]                       2  /device:GPU:0
Total #vars=32, #param=134268738 (512.19 MB assuming all float32)


I think there are must be someting wrong

",run progress bar print throughput like use progress bar print value begin log like model name shape dim device total assuming float think must wrong,issue,positive,negative,negative,negative,negative,negative
319038019,"What do you mean "" the performance is the same as 1 GPU""?
Please describe what you saw, not only the interpretation of what you saw.
Training logs will be helpful, in particular.",mean performance please describe saw interpretation saw training helpful particular,issue,positive,negative,neutral,neutral,negative,negative
318991558,"InferenceRunner will call _build_graph with ctx.is_training == False.

`_build_graph` was also called multiple times if you use multi-gpu training.",call false also multiple time use training,issue,negative,negative,negative,negative,negative,negative
318991039,Does InferenceRunner call _build_graph with ctx.is_training == False? I though _build_graph is called only once when the training starts and the inference is also run on the training graph...,call false though training inference also run training graph,issue,negative,negative,negative,negative,negative,negative
318990601,Also if the difference is small you can just check `if ctx.is_training` in `_build_graph`.,also difference small check,issue,negative,negative,negative,negative,negative,negative
318988733,"No. Everything happens in one graph, and it's not trivial to share variables between two graphs.
The existing tools are for the most common cases only, for your situation you can setup the validation ops in `_setup_graph` and write the inference yourself.
btw, training and validation uses different model doesn't mean they have to be in different graphs.",everything one graph trivial share two common situation setup validation write inference training validation different model mean different,issue,negative,negative,negative,negative,negative,negative
318849664,"Ok. Got it. 



> On Jul 29, 2017, at 11:28 AM, Yuxin Wu <notifications@github.com> wrote:
> 
> Then just summarize both...what's the question?
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",got wrote summarize question thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
318848870,"Actually I need gradient stats for grads wrt variables(weights) and the grads that get back-propagated to the previous layer. 

Goal is to look at both the grad stats - grads that are involved with weight update and the grad stats that get back-prop'ed to the previous layer (to update weights in that layer). 

Any suggestions?

> On Jul 29, 2017, at 11:00 AM, Yuxin Wu <notifications@github.com> wrote:
> 
> Your method summarizes gradients w.r.t layer outputs, not variables.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",actually need gradient get previous layer goal look grad involved weight update grad get previous layer update layer wrote method layer thread reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
318848613,"So, method 2 is correct then?


> On Jul 29, 2017, at 11:00 AM, Yuxin Wu <notifications@github.com> wrote:
> 
> Your method summarizes gradients w.r.t layer outputs, not variables.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",method correct wrote method layer thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
318814071,the WGAN example was just updated to include two ways to clip the weights.,example include two way clip,issue,negative,neutral,neutral,neutral,neutral,neutral
318766963,"If you have complicated control logic it's easier to do it in callbacks. As long as you don't call `var.eval()` and `var.load()` it's not going to hurt much in performance.
Just create the assignment ops you need to run in `_setup_graph()` and run them in `_trigger_step()`. You can access the epoch number in a callback by `self.epoch_num`.

You can do it in the graph as well: In the graph there is no notion of ""epoch"". You can use `global_step` (with `tf.train.get_or_create_global_step()`) and `tf.cond` for the condition.",complicated control logic easier long call going hurt much performance create assignment need run run access epoch number graph well graph notion epoch use condition,issue,negative,negative,negative,negative,negative,negative
318764602,"Sorry to bother you again. I was able to follow your Wasserstein GAN example and made the per-variable operation with VariableAssignmentOptimizer. However, is there a way to optionally perform this operation say after epoch 20? In the case of the Wasserstein GAN example, this means only perform clipping after epcoh 20.",sorry bother able follow gan example made operation however way optionally perform operation say epoch case gan example perform clipping,issue,negative,neutral,neutral,neutral,neutral,neutral
318732529,my bad. my usage is correct. just my file on server is not up-to-date..,bad usage correct file server,issue,negative,negative,negative,negative,negative,negative
318564830,Until now tensorflow still doesn't have a nice way to reuse a name_scope (https://github.com/tensorflow/tensorflow/issues/6007). Without this feature it's hard to group relevant ops together in tensorboard. ,still nice way reuse without feature hard group relevant together,issue,negative,positive,positive,positive,positive,positive
318559176,"The only problem with the current zmq op seems to be protobuf, and it's easy to solve: just need to serialize shape + dtype somehow. Maybe it is easier to do this way.",problem current easy solve need serialize shape somehow maybe easier way,issue,positive,positive,positive,positive,positive,positive
318370714,"There are examples using other layers. You need a scope name as the first argument. 

X1 = BatchRenorm('bn', X1, rmax, dmax, data_format='NHWC')
",need scope name first argument,issue,negative,positive,positive,positive,positive,positive
318314031,"after I use newest code,it works too.
Thank you 


",use code work thank,issue,negative,neutral,neutral,neutral,neutral,neutral
318299215,"I still thinks that users should handle the `REGULARIZATION_LOSSES` collection by themselves. Some reasons:
1. consistent with where regularization is done in tensorpack. This is causing confusion such as #611
2. It's not always easy to handle it automatically, especially when now we have many different multiGPU/distributed strategies I found it hard to make every scenario correct. Similar things has happened to UPDATE_OPS as well.
And even if it's correct, it's not the most efficient to use the collection on every GPU, as shown by https://github.com/tensorflow/benchmarks/commit/7fc628d041fd7b7fafceccf60ba1f52448c50330. If the user can do it explicitly, this can be easily done.
3. Also, imagine using `tf.layers` inside GAN -- `get_cost` is never used, so you have to manually process the collection anyway. This looks like a surprise to users so perhaps it's better to be explicit than to do too much under the hood.",still handle collection consistent regularization done causing confusion always easy handle automatically especially many different found hard make every scenario correct similar well even correct efficient use collection every shown user explicitly easily done also imagine inside gan never used manually process collection anyway like surprise perhaps better explicit much hood,issue,positive,positive,positive,positive,positive,positive
318286716,I tested with mnist-convnet.py. Just replace the trainer and it works.,tested replace trainer work,issue,negative,neutral,neutral,neutral,neutral,neutral
318265000,"@ppwwyyxx But it's alway failed,and throw exception like:
ValueError: Variable conv0/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
you can test with mnist-slim or anyone other's",alway throw exception like variable exist mean set test anyone,issue,negative,negative,negative,negative,negative,negative
318232713,"Possible solutions to the problems left:

1. (DONE) Allow users to build the whole graph by themselves. (i.e. no ModelDesc at all)
This seems to solve all the problems mentioned, and provide best flexibility for complex graphs. 

It's easy to do: currently the trainer does two jobs: setting up the whole graph from ModelDesc, and run the loop. We just need a trainer which does not set up the graph.

2. ModelDesc will be only for simple graphs, then. But it's still awkward to access tensors. The ideal way is to simply do `model.towers[0].cost`, or `model.towers['tower0'].cost` for a tensor set by `self.cost = ...`. This is possible but I still have to see how nicely it can be done.

3. Make it easier to build graph before initialize trainer. (by `SomeTrainer.setup_graph()`?). This would avoid the ""guess the name of that tensor"" game.",possible left done allow build whole graph solve provide best flexibility complex easy currently trainer two setting whole graph run loop need trainer set graph simple still awkward access ideal way simply tensor set possible still see nicely done make easier build graph initialize trainer would avoid guess name tensor game,issue,positive,positive,positive,positive,positive,positive
318231041,"In addition to the above comments to @Skylion007 
Partial inference: yes the graph might vary a lot in prediction and that's one of the use case I put in this issue.

Loading a network: I don't see any problems with that.

InputDesc: it exists mainly for working with `InputSource`, which is an abstraction of all possible input source, not only including placeholders, but could switch among tensors, queues, etc.
In tensorflow you cannot pump data into any part of the subgraph when data comes from an op.

Parameterize ModelDesc: as @PatWie said you can do it.

More complicated models: as mentioned there is no plan to add more models.

`collect_variables` is not a function of tensorpack. 
It's an example -- meaning that's something the user (in this case myself), should write.
The user might not write it clear enough but that's unrelated to the design of tensorpack.
",addition partial inference yes graph might vary lot prediction one use case put issue loading network see mainly working abstraction possible input source could switch among pump data part data come said complicated plan add function example meaning something user case write user might write clear enough unrelated design,issue,positive,negative,neutral,neutral,negative,negative
318129475,"With keras you simply cannot do replicated training. That's by design of keras.
Apart from that, you should be able to replace SyncMultiGPUTrainerParameterServer with SyncMultiGPUTrainerReplicated anywhere.",simply replicated training design apart able replace anywhere,issue,negative,positive,positive,positive,positive,positive
317953375,"The InputDescr is needed, otherwise the data flow does not know how the data goes into the graph. 
If it wouldn't be there models hard to read as you have to backtrack the cost tensor to the input which sometimes require a large mental stack.

And you can use [Models in Models](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/Saliency/saliency-maps.py#L28) as the constructor is empty. I did this multiple times. Models parameterization is also possible, just add a constructor with the parameter. I used it to build entire recurrent networks and specify the time steps which can varying during interference. 

So it is not as bad as it looks like 😄 

Also the partial inference: you just specify the output tensor and your done?! 
And in TensorFlow you can send data to placeholders only. This is not a limit of tensorpack which even provide a [workaround](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/GAN/InfoGAN-mnist.py#L90).

I just see the issue with the namespace, which depends on which by trainer is used. Maybe a default 'tower0' fixes this.

It might be that I inhaled already too much tensorpack code. Please do not try to make a second ideas or of it and oversimplify things.

**Edit:** one more point about 'self.cost': why not changing this into a class-method 'self.cost()'? This might be helpful also in GanModelsDescr to hide collect_variables.
",otherwise data flow know data go graph would hard read backtrack cost tensor input sometimes require large mental stack use constructor empty multiple time also possible add constructor parameter used build entire recurrent specify time interference bad like also partial inference specify output tensor done send data limit even provide see issue trainer used maybe default might already much code please try make second oversimplify edit one point might helpful also hide,issue,negative,negative,neutral,neutral,negative,negative
317949853,"So the usual business: make it compile, make it link, make it work, make it correct, make it fast, make it flexible. 
I will ping this thread, when I prepared something. But it will take sometime as it has low priority.",usual business make compile make link make work make correct make fast make flexible ping thread prepared something take sometime low priority,issue,negative,negative,neutral,neutral,negative,negative
317924699,I checked it. Its runs smoothly. Thank you respected job,checked smoothly thank job,issue,negative,positive,positive,positive,positive,positive
317923966,"yes，tensorflow can support hdfs,but tensorpack     not.by specific，there is an assert in the ""callback.saver"" class,in tensorpack souce  code. So I think the  source  keeper shuold remove the assert statement or use gfile interface",support assert class code think source keeper remove assert statement use interface,issue,negative,neutral,neutral,neutral,neutral,neutral
317908375,Switched to `tf.gfile.IsDirectory`. Please reopen if the problem still exists.,switched please reopen problem still,issue,negative,neutral,neutral,neutral,neutral,neutral
317849996,"It would be nice to have this op! But recently I don't have to time to look at your implementation in details. I guess there are at least these points to clarify:
1. Make it support more tensor types? (including string)
2. How does Python side and C++ side agree on the serialization format they use? Maybe that's some msgpack internals (`packb`) but I'm not very familiar.
3. The performance? If it's still slower than queues, it would be necessary to at least understand what amount of time copy and deserialization contribute.",would nice recently time look implementation guess least clarify make support tensor string python side side agree serialization format use maybe internals familiar performance still would necessary least understand amount time copy contribute,issue,positive,positive,neutral,neutral,positive,positive
317678181,"You can use either regularizers of tf.layers, or the existing `regularize_cost()` function -- it's just a symbolic function and you can call it any time anywhere when it suits.",use either function symbolic function call time anywhere,issue,negative,neutral,neutral,neutral,neutral,neutral
317556007,We can also try autorefactor tools to update all the examples and put deprecation warning under the old layers. Python [rope](https://github.com/python-rope/rope/blob/master/docs/rope.rst) seems like the best solution if we do that approach.,also try update put deprecation warning old python rope like best solution approach,issue,positive,positive,positive,positive,positive,positive
317538484,"Some other things to consider about how to potentially break and fix ModelDesc:

Partial inference: DiscoGAN is a good example. For prediction you might want to only go from A->B.
Additionally, you may want to the graph to vary significantly in prediction (like running A->B generator multiple times recursively, this is currently rather cumbersome with the predictor).

Loading in a predictor network: Networks that rely on things like Inception score for instance. PyTorch handles this really well by allowing you to just call the model inference of Inception from another model. 

Input Desc: Seems a little redundant to me, how is it different from place holders? Can I only send data to placeholder Tensors? It would be nice to be able to pump data into any part of the subgraph. In which case, what's the point of placeholders at all? Can we inject placeholders into the model on demand? Does this have negative performance implications? Does this violate the principle of least astonishment. 

Easy way to parameterize models. Additionally, well self.XXX is nonstandard, I think it can be extremely useful for building the model. CycleGAN example of using Resblocks come to mind. Defining the depth of the model should be easy to parameter meaning there should be some easy way to send model arguments. Maybe even trainers that can do parameter sweeps? Should trainers be part of the model.

We really should encourage people to use proper namespaces. Should we prefix entire ModelDesc with model names? What if a user want to do this.

I know it's slightly outside the scope of Tensorpack, but can we make ModelDesc composable? Can I have models in my models? Can I have ModelDesc from stuff as simple as Resblocks, Predictor Nets? Should we have generic ModelDesc for different frameworks? Maybe we should have composer functions for common network types that don't appear in tf.layers and likely won't. Or even stuff that is higher level / more cutting edge. 

When and where to call collect_variables can be a little confusing. Should it be baked into the model? Should it not?

Idea for accessing specific Tensors: Encourage Tensor named namespaces? Force users to use them in the model somehow. Maybe even draft a style guide. Will this hurt Tensorboard Visualization?

I will say, one of my favorite things about Tensorpack is the ability to learn by example thanks to the very flushed out examples and limited symbolic functions. While I understand maintaining the latter is outside the scope of this project, implementing models can really help find flaws in stuff like trainers and ModelDesc. The example of WGAN comes to mind. We need to allow the models to be hackable. Maybe using stuff like self.XXX is fine as long it's used responsibly.

We might also want to mark the hacky stuff as experimental (with a decorator maybe?) and give warnings when people try to use it.  ",consider potentially break fix partial inference good example prediction might want go additionally may want graph vary significantly prediction like running generator multiple time currently rather cumbersome predictor loading predictor network rely like inception score instance really well call model inference inception another model input little redundant different place send data would nice able pump data part case point inject model demand negative performance violate principle least astonishment easy way additionally well nonstandard think extremely useful building model example come mind depth model easy parameter meaning easy way send model maybe even parameter part model really encourage people use proper prefix entire model user want know slightly outside scope make stuff simple predictor generic different maybe composer common network appear likely wo even stuff higher level cutting edge call little baked model idea specific encourage tensor force use model somehow maybe even draft style guide hurt visualization say one favorite ability learn example thanks limited symbolic understand latter outside scope project really help find stuff like example come mind need allow maybe stuff like fine long used responsibly might also want mark hacky stuff experimental decorator maybe give people try use,issue,positive,positive,neutral,neutral,positive,positive
317530696,"I am really curious how the regularizer will change since it looks like most tf.layers have a regularizer  as an argument? That seems like it would be more difficult to replace with simple function decorators. I definitely encourage using tf.layers for consistency, but I just want to make sure any regularizer I write will be future proof.",really curious regularizer change since like regularizer argument like would difficult replace simple function definitely encourage consistency want make sure regularizer write future proof,issue,positive,negative,neutral,neutral,negative,negative
317526627,Should I create an example with that zmq and polish the implementation to add it to TensorPack?,create example polish implementation add,issue,negative,neutral,neutral,neutral,neutral,neutral
317514394,"We do not focus on models/symbolic functions and don't intend to add any more layers. We will shift the existing ones to `tf.layers` instead. You should be able to use any symbolic functions in tensorflow for your model.

In particular, `varreplace.remap_variables` will be a helpful function to write weight normalization.",focus intend add shift instead able use symbolic model particular helpful function write weight normalization,issue,negative,positive,positive,positive,positive,positive
317507730,"Thanks. I didn't realize that you can do var.load(value) to update the value. This works but now it is extremely slow (as expected). I really need to consider rewriting everything in ops in the graph now...

Thanks again!",thanks realize value update value work extremely slow really need consider everything graph thanks,issue,positive,positive,neutral,neutral,positive,positive
317507300,Closing as a duplicate of #221. If we are going to fix it in the future probably we'll not use protobuf at all.,duplicate going fix future probably use,issue,negative,neutral,neutral,neutral,neutral,neutral
317506825,Sounds like a reasonable feature to add.,like reasonable feature add,issue,negative,positive,positive,positive,positive,positive
317506451,"https://gist.github.com/ppwwyyxx/e1900b0e49bfb6771af22cc882b57bb5
Doesn't make much difference in my experience.",make much difference experience,issue,negative,positive,positive,positive,positive,positive
317498609,Is there any easy way to add a replay buffer in Tensorpack? That seems like a useful feature that could be a nice dataflow augmentation.,easy way add replay buffer like useful feature could nice augmentation,issue,positive,positive,positive,positive,positive,positive
317498318,"It would be really nice to have a version random resize that took in a range of pixels. Otherwise, you have to calculate the maximum size of the image, resize it to that and then calculate a downsize range to preserve as much information as possible and remove downsampling artifacts. ",would really nice version random resize took range otherwise calculate maximum size image resize calculate range preserve much information possible remove,issue,negative,positive,neutral,neutral,positive,positive
317495795,"It'll be slow if you do the operations in numpy. But there are ways to do it.
You should be able to just call `var.eval()` and `var.load(value)` to read and write to a variable in `trigger_step`, in this case you don't need to worry about `setup_graph` because you created no extra ops in the graph. If you meet errors please post them.",slow way able call value read write variable case need worry extra graph meet please post,issue,negative,positive,neutral,neutral,positive,positive
317475471,"Thanks ppwwyyxx for the prompt response. I have been looking at the 2 solutions you mentioned over the weekend, but still unable to develop a working solution.

I hope I can make this clear, just to briefly describe what I want to achieve

In every step, I want to look at the entire or subset (i.e., in the case of the weights for fully connected layer, say the first 10 rows) weights of one or more layers. I want to do some post-processing (i.e., by computing std, mean, median, etc). Depending on the post-processed info, I would then want to have the ability to change the weight value on the same/different layer at the specific location to specific values (i.e., last row of the fully-connected layer). 

First suggestion (create ops in _setup_graph):
I don't seen to be able to access the weight value in this case(i.e., by calling tf.trainable_variables()[index].eval()). Can you please elaborate more on what you meant by ""create ops in _setup_graph""? Does this mean that all the operations need to be in tf world?

Second suggestion:
I looked at the Wasserstein GAN example you provided. It uses the tf.clip_by_value function provided by tensorflow, and works on the entire weight only. I traced through the code to gen_math_ops.py and op_def_library.py but its not straight-forward at this point, and is limited to tf world only. It would be nice if I can work using numpy (which I don't seen to be able to do it in this case)

The post-processing in numpy is key in this case I guess, as there are some complicated algorithms that have been developed in numpy world.

Thanks so much for the help!


",thanks prompt response looking weekend still unable develop working solution hope make clear briefly describe want achieve every step want look entire subset case fully connected layer say first one want mean median depending would want ability change weight value layer specific location specific last row layer first suggestion create seen able access weight value case calling index please elaborate meant create mean need world second suggestion gan example provided function provided work entire weight code point limited world would nice work seen able case key case guess complicated world thanks much help,issue,positive,positive,neutral,neutral,positive,positive
317314746,Feel free to reopen if you have more to discuss.,feel free reopen discus,issue,positive,positive,positive,positive,positive,positive
317153061,"You may wish to make your assignment an explicit dependency of the training iteration, to save the extra ""sess.run"" overhead. To do this, `PostProcessOptimizer`, `VariableAssignmentOptimizer` may help do some easy per-variable assignment, for example, the [Wasserstein GAN](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/GAN/WGAN.py#L36) example clips the weights every iteration. If you need something more complicated, please give some more details -- there is always a way to do it.",may wish make assignment explicit dependency training iteration save extra overhead may help easy assignment example gan example clip every iteration need something complicated please give always way,issue,positive,negative,neutral,neutral,negative,negative
317152675,"You can do it through a callback. You have to create the ops in `_setup_graph`, and run them in `_trigger_step`. This design is exactly to prevent you from doing `self.sess.run(tf.trainable_variables()[counter].assign(new_weights)` which will add thousands of ops to the graph and blow everything.
 ",create run design exactly prevent counter add graph blow everything,issue,negative,positive,positive,positive,positive,positive
316768332,This [stack overflow post](https://stackoverflow.com/questions/39079065/tensorflow-check-which-protobuf-implementation-is-being-used) seems relevant. More black magic with undocumented APIs and environment variables. 👍 ,stack overflow post relevant black magic undocumented environment,issue,negative,positive,positive,positive,positive,positive
316767600,"Yeah, I wish there was a feature in Tensorflow for hidden namescopes or such. Regardless, I've figured out how to look at Generator and Discriminator based Tensorboards, but it can definitely be confusing at times. My biggest pete peave is the l2regularizers and conditionals which creates a long lines of disconnected ops and require zooming in a lot. Also a lot of the dataflow code could probably be a little better organized. All in all, the namescopes at last make it readable which is more than I can say for other libraries.",yeah wish feature hidden regardless figured look generator discriminator based definitely time biggest pete long disconnected require lot also lot code could probably little better organized last make readable say,issue,positive,positive,neutral,neutral,positive,positive
316759218,"Method 2 and 3 works for regularization, because regularization is a loss.
Also you can choose what variables to regularize.
",method work regularization regularization loss also choose regularize,issue,negative,neutral,neutral,neutral,neutral,neutral
316664506,I am reading the documents for freezing several layers in [FAQ](http://tensorpack.readthedocs.io/en/latest/tutorial/faq.html#how-to-freeze-some-variables-in-training). It seems that in cases we are also doing l2_regularization using stop_gradient may not be sufficient to freeze some weights. Since they get gradients from l2_regularizer too. Is that right?  ,reading freezing several also may sufficient freeze since get right,issue,negative,positive,positive,positive,positive,positive
316592411,"The graph is just so complex, and I don't know how tensorboard organizes a graph and how to make it looks better. Maybe I'll try adding some name scope here and there but I'm not sure.",graph complex know graph make better maybe try name scope sure,issue,positive,positive,positive,positive,positive,positive
316588206,"This exists in #221 that hasn't been fixed yet. The op isn't ready for use, now you'll have to build it against the exact same version of protobuf your tensorflow is using, which is hard to tell.",fixed yet ready use build exact version hard tell,issue,negative,positive,neutral,neutral,positive,positive
316585634,"stack of crash
(gdb) bt
#0  0x00007fff73b104ee in google::protobuf::Arena::AllocateAligned(std::type_info const*, unsigned long) ()
   from /home/zbhuang/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#1  0x00007fff739c2f13 in tensorflow::TensorProto::New(google::protobuf::Arena*) const ()
   from /home/zbhuang/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007fff39c4c038 in google::protobuf::MessageLite::ParseFromArray(void const*, int) () from /usr/lib/x86_64-linux-gnu/libprotobuf.so.9
#3  0x00007fff39f18c61 in ZMQConnection::recv_tensor_list (this=0x5e2fff0, tlist=0x7ffd397f8d30) at zmq_conn.h:57
#4  0x00007fff39f1948e in ZMQRecvOp::Compute (this=0x5e30ab0, ctx=0x7ffd397f98a0) at zmq_recv_op.cc:44
",stack crash unsigned long void,issue,negative,negative,neutral,neutral,negative,negative
316532638,"But it doesn't build smoothly..
I won't have time for this one in the near future.",build smoothly wo time one near future,issue,negative,positive,positive,positive,positive,positive
316279396,"OK, I got it. 
Thanks for your prompt reply!",got thanks prompt reply,issue,negative,positive,positive,positive,positive,positive
316278349,"In a serialized graph, there is nothing called ""inputs"", ""layers"". These are just human-made abstractions that will get lost once you dump the graph.
To do fine-tuning, the best way would be to copy the model code from the old script and modify upon it.",graph nothing get lost dump graph best way would copy model code old script modify upon,issue,positive,positive,positive,positive,positive,positive
316277840,"Specifically, I have trained the resnet50 by your example ""imagenet-resnet.py"", then I want to build a new model upon this pretrained resnet, say, add some layers after the output of some conv layer of resnet50. However, when I import the meta graph which saved by the exmaple ""imagenet-resnet.py"" and I print the imported model by ""decscribe_model"" function but did not find the input placeholder for feeding new data. 
Do I need to create new placeholders for new data? If so, how to connect (which I said redirect) these new placeholder to the pretrained resnet? Or is there some elegant api to do this without build the whole model from scratch(like input placeholders -> every blocks of resnet-> my new layers -> ...)?",specifically trained example want build new model upon say add output layer however import meta graph saved print model function find input feeding new data need create new new data connect said redirect new elegant without build whole model scratch like input every new,issue,positive,positive,positive,positive,positive,positive
316271431,"TF checkpoint only stores variables. 

What do you mean by ""redirect the input placeholders to the loaded pretrained model""?",mean redirect input loaded model,issue,negative,negative,negative,negative,negative,negative
316226821,"This is what ""split"" means. Each output only connects to half channels in the input.
https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf",split output half input,issue,negative,negative,negative,negative,negative,negative
316224437,Absolutely. Let me know if you need help on other stuff. I would like to collaborate with you.,absolutely let know need help stuff would like collaborate,issue,positive,positive,positive,positive,positive,positive
315972462,"Just tested AugmentImageCoordinates. Use attached code to test all.
[Test.zip](https://github.com/ppwwyyxx/tensorpack/files/1154887/Test.zip)
 ",tested use attached code test,issue,negative,neutral,neutral,neutral,neutral,neutral
315956982,Take a look at #335. Let me know what you think.,take look let know think,issue,negative,neutral,neutral,neutral,neutral,neutral
315900231,"To simplify things, I think for now you can leave it the way it is. And in the future we can either add support of coordinate mapping to it, or improve documents saying that `MapImage` doesn't support coordinate mapping.

To add support to it, I think it's OK to pass an optional function which by default throws exception.",simplify think leave way future either add support improve saying support add support think pas optional function default exception,issue,positive,neutral,neutral,neutral,neutral,neutral
315894229,"How do you want to change MapImage? Should it receive two functions? one for mapping image array and one (optional) for mapping coordinates? In case second function is None should we do identity mapping or throw not implemented exception? 

",want change receive two one image array one optional case second function none identity throw exception,issue,negative,neutral,neutral,neutral,neutral,neutral
315869813,"Yes. I think it's OK to change it in-place which is currently the way how images are augmented.
I think assuming numpy arrays is fine, similar to what have been done for images. If we ever need a type check it can be added in the dataflow.",yes think change currently way augmented think assuming fine similar done ever need type check added,issue,positive,positive,positive,positive,positive,positive
315867363,"So for crop it should looks like this:

```
def _augment_coords(self, coords, param):
   h0, w0 = param
   #coords = np.copy(coords)
   # change x
   coords[:, 0] = coords[:, 0] - w0
   # change y
   coords[:, 1] = coords[:, 1] - h0    
   return coords
```
I assume we can directly change coords and similar to AugmentImageComponents copying data will be handled in AugmentImageCoordinates. Can we assume coords are always numpy arrays? Any need to do a type check? And I agree with the notes.",crop like self param param change change return assume directly change similar data handled assume always need type check agree,issue,positive,positive,neutral,neutral,positive,positive
315664764,"Just did a rename because the original name (which I probably made a year ago) wasn't very good.

Thanks a lot for helping! The design is like the following (and you can certainly make suggestions):
1. If some augmentor changes geometry of an image (more precisely, change the coordinates of pixels), it should implement `_augment_coords(self, coords, param)`, where:
`coords` is an `nx2` (or 2xn, whatever) array of (x, y) __floating point__ coordinates.
`param` is the augmentation parameters returned by `_get_augment_params`.
And returns the new `coords`.

Don't have to do this to every augmentor. There might be too many. You can leave others as `raise NotImplementedError()`.

2. Implement this method for `AugmentorList` as well, which basically calls `_augment_coords` on all the augmentors one by one.

3. A DataFlow `AugmentImageCoordinates`, which maps every datapoint of an existing dataflow by the following:
```python
def func(dp):
    img, coords = dp[image_index], dp[coords_index]
    im, prms = self.augs._augment_return_params(im)
    dp[image_index] = im
    coords = self.augs._augment_coords(coords, prms)
    dp[coords_index] = coords
    return dp
```
The existing `AugmentImageComponents` should be a good reference.

Some notes:
1. bounding box is a collection of coordinates, and there are other coordinate-based tasks that don't use bounding boxes (e.g. keypoint localization, non-rectangular bounding box, etc). So it's better to use coordinates.
2. Coordinates rounding error and misalignment can lead to significant degradation in performance, so it's important to use floating point coordinates, and always consider a pixel as a square rather than a point. It's a bug that some open-source implementations of RCNN doesn't take this into account. For example, for a 4x4 image, the top-left corner point is (0,0), bottom-right corner point is (4,4). The center point of the first pixel is (0.5, 0.5), etc. 
Actually this probably makes implementation easier than integer coordinates. For example a resize will be just a simple scaling of coordinates.",rename original name probably made year ago good thanks lot helping design like following certainly make geometry image precisely change implement self param whatever array param augmentation returned new every might many leave raise implement method well basically one one every following python return good reference bounding box collection use bounding localization bounding box better use rounding error misalignment lead significant degradation performance important use floating point always consider square rather point bug take account example image corner point corner point center point first actually probably implementation easier integer example resize simple scaling,issue,positive,positive,positive,positive,positive,positive
315661402,I'd be happy to do this. Tell me what was the original design you had in mind. ,happy tell original design mind,issue,positive,positive,positive,positive,positive,positive
315649007,The `_fprop_coord` method was originally designed for this. But I never found time to implement it.,method originally designed never found time implement,issue,negative,positive,positive,positive,positive,positive
315394964,Maybe it's OK to move to RGB default to be consistent within dataflow/.,maybe move default consistent within,issue,negative,positive,positive,positive,positive,positive
315387078,"btw, when i played with saturation augmentation it seemed like it was changing the hue for some pixels (even when taking into account to BGR/RGB issue)",saturation augmentation like hue even taking account issue,issue,negative,neutral,neutral,neutral,neutral,neutral
315384043,"1. There is a more general DataFromGenerator
2. Why do you want this? I don't think this'll be useful for training. For testing you can call whatever webcam libraries anyway.

In general just write whatever code in get_data().http://tensorpack.readthedocs.io/en/latest/tutorial/extend/dataflow.html",general want think useful training testing call whatever anyway general write whatever code,issue,negative,positive,positive,positive,positive,positive
315247475,"Some parameters (learning rate, entropy factor) may need some tuning for difficult games. These models were trained a year ago and I don't remember much details. 2 days might be somewhere around 2million steps?",learning rate entropy factor may need tuning difficult trained year ago remember much day might somewhere around million,issue,negative,negative,negative,negative,negative,negative
315246348,I think you can use whatever names as long as the variables created inside the layer don't conflict with others.,think use whatever long inside layer conflict,issue,negative,negative,neutral,neutral,negative,negative
315244381,"My bad. I think we don't need to pass the parent scope down to other layers. We just need to pass down different names for each layer. Correct me if it is wrong.
 ",bad think need pas parent scope need pas different layer correct wrong,issue,negative,negative,negative,negative,negative,negative
315235574,"Thanks.

I find it can be diverge easily on some difficult games (such as WizardOfWor). Did you have similar situation? How did you deal with that? continue training until it converge at some point or restart the training process?

You mentioned that you train 2 days to get those models in your Model Zoo, how many epochs of those model?",thanks find diverge easily difficult similar situation deal continue training converge point restart training process train day get model zoo many model,issue,negative,positive,neutral,neutral,positive,positive
315154374,A (better) work around is easy so I also pushed one.,better work around easy also one,issue,positive,positive,positive,positive,positive,positive
314982542,Thanks! I am trying to port faster-rcnn implementation (https://github.com/endernewton/tf-faster-rcnn) to tensorpack. It seems they have fixed BatchNorm layers during training. Any suggestion for freezing their params? ,thanks trying port implementation fixed training suggestion freezing,issue,negative,positive,positive,positive,positive,positive
314807332,"thx, I also tried to use the basic tensorflow layer (because I need use conv3d) but it's training accuracy was stuck at 50%  for a binary classification problem...
but, after I used the keras to build the model everything works fine.(but followed by lots of other issues ",also tried use basic layer need use training accuracy stuck binary classification problem used build model everything work fine lot,issue,negative,positive,positive,positive,positive,positive
314801337,"I don't know what happened in your model.
And I don't know what name scope problems you saw. Keras models were only tested with the most basic functionalities in tensorpack.",know model know name scope saw tested basic,issue,negative,neutral,neutral,neutral,neutral,neutral
314796076,"A strange thing happens to me, the `OfflinePredictor` always produce the same inference results for every batch and even same result for every single data in the batch. 
I checked the batch data are different and my model value is correct. Do you have any idea for this kind of error? thx
By the way, there still some name scope related problems when I try to use the `DataParallelOfflinePredictor`.",strange thing always produce inference every batch even result every single data batch checked batch data different model value correct idea kind error way still name scope related try use,issue,negative,positive,neutral,neutral,positive,positive
314650135,"These parameters should only affect speed. I don't know why would you fine-tune to a worse score, but in my experience vanilla a3c models are not very stable anyway.",affect speed know would worse score experience vanilla stable anyway,issue,negative,negative,negative,negative,negative,negative
313745084,"Just looked at keras code a bit, looks like there is no way to know what the actual name of `learning_phase` will be before the graph is created.",code bit like way know actual name graph,issue,negative,neutral,neutral,neutral,neutral,neutral
313598034,"In my case the learning_phase's name actually is
`batch_normalization_1/keras_learning_phase`
which is different with `keras.backend.learning_phase().name`
hence after I add this name to `input_names` of `PredictConfig`
and use `result = pred([batch, 0])[0]` when predicting
It works well now!",case name actually different hence add name use result batch work well,issue,negative,neutral,neutral,neutral,neutral,neutral
313591602,"I called **K.set_learning_phase(1)** this before keras.backend.learning_phase().name. Now I comment this line, and get the correct name ''keras_learning_phase""
But it seems does not exist in the graph
```
Traceback (most recent call last):
  File ""src/evaluate.py"", line 486, in <module>
    main()
  File ""src/evaluate.py"", line 479, in main
    scores = evaluate(nodules, arguments['<data_path>'], arguments['<model>'], n_gpus=n_gpus, load=arguments['<load>'])
  File ""src/evaluate.py"", line 397, in evaluate
    pred = OfflinePredictor(pred)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/predict/base.py"", line 141, in __init__
    input_tensors = get_tensors_by_names(config.input_names)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/tfutils/common.py"", line 120, in get_tensors_by_names
    ret.append(G.get_tensor_by_name(varn))
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2563, in get_tensor_by_name
    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2414, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2456, in _as_graph_element_locked
    ""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'keras_learning_phase:0' refers to a Tensor which does not exist. The operation, 'keras_learning_phase', does not exist in the graph.""
```
",comment line get correct name exist graph recent call last file line module main file line main evaluate model load file line evaluate file line file line file line return name file line return file line graph name name tensor exist operation exist graph,issue,negative,positive,neutral,neutral,positive,positive
313457943,"You might be using old version of Keras.
```
$ keras.backend.learning_phase()
Out[4]: <tf.Tensor 'keras_learning_phase:0' shape=<unknown> dtype=bool>
```",might old version unknown,issue,negative,neutral,neutral,neutral,neutral,neutral
313446703,"`keras.backend.learning_phase()` seems to be an int, hence does not have a `name`. Is there a variable or    tensor things to get a correct name?",hence name variable tensor get correct name,issue,negative,neutral,neutral,neutral,neutral,neutral
313434051,"You can add `keras.backend.learning_phase().name` to `input_names` in `PredictConfig`, and call it with an extra zero: `result = pred([batch, 0])[0]`",add call extra zero result batch,issue,negative,neutral,neutral,neutral,neutral,neutral
313380043,"@ppwwyyxx Thx, I tried append a `savename_prefix=tower0` in `get_savename_from_varname`. It works for me. 
I'd like to try to add a `tf.name_scope('tower0')` It should work well.

And another question is that the model is restored but due to the Keras model callback
```
class KerasCallback(Callback):
    def __init__(self, isTrain):
        self._isTrain = isTrain
        self._learning_phase = KB.learning_phase()

    def _before_run(self, ctx):
        return tf.train.SessionRunArgs(
            fetches=[], feed_dict={self._learning_phase: int(self._isTrain)})
```
it give me an error when I try to  `pred = OfflinePredictor(pred)`
```
[0706 20:08:47 @sessinit.py:102] Restoring checkpoint from /mnt/ficusspain/yxwang/research_lung/08_all_e2e2/506_resume-95/cls_3d/src/train_log/model_kp:./train_keras/model-74478 ...
Traceback (most recent call last):
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1039, in _do_call
    return fn(*args)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1021, in _run_fn
    status, run_metadata)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool
         [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
         [[Node: output/_101 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_239_output"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""src/evaluate.py"", line 470, in <module>
    main()
  File ""src/evaluate.py"", line 463, in main
    scores = evaluate(nodules, arguments['<data_path>'], arguments['<model>'], n_gpus=n_gpus, load=arguments['<load>'])
  File ""src/evaluate.py"", line 386, in evaluate
    result = pred([batch])[0]
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/predict/base.py"", line 55, in __call__
    output = self._do_call(dp)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/predict/base.py"", line 122, in _do_call
    output = self.sess.run(self.output_tensors, feed_dict=feed)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 778, in run
    run_metadata_ptr)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1032, in _do_run
    target_list, options, run_metadata)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool
         [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
         [[Node: output/_101 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_239_output"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op 'batch_normalization_1/keras_learning_phase', defined at:
  File ""src/evaluate.py"", line 470, in <module>
    main()
  File ""src/evaluate.py"", line 463, in main
    scores = evaluate(nodules, arguments['<data_path>'], arguments['<model>'], n_gpus=n_gpus, load=arguments['<load>'])
  File ""src/evaluate.py"", line 384, in evaluate
    pred = OfflinePredictor(pred)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/predict/base.py"", line 139, in __init__
    config.model.build_graph(input_placehdrs)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/models/model_desc.py"", line 105, in build_graph
    self._build_graph(model_inputs)
  File ""/mnt/ficusspain/yxwang/research_lung/08_all_e2e2/000_nodule_workspace_template/cls_3d/src/model_kp.py"", line 238, in _build_graph
    M = self._build_keras_model()
  File ""/mnt/ficusspain/yxwang/research_lung/08_all_e2e2/000_nodule_workspace_template/cls_3d/src/model_kp.py"", line 74, in _build_keras_model
    model.add(normalization.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/keras/models.py"", line 476, in add
    output_tensor = layer(self.outputs[0])
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/keras/engine/topology.py"", line 596, in __call__
    output = self.call(inputs, **kwargs)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/keras/layers/normalization.py"", line 190, in call
    training=training)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2600, in in_train_phase
    training = learning_phase()
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 116, in learning_phase
    name='keras_learning_phase')
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1507, in placeholder
    name=name)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1997, in _placeholder
    name=name)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool
         [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
         [[Node: output/_101 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_239_output"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool
         [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
         [[Node: output/_101 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_239_output"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
```
Is there any good way to handle this placeholder problem, I am not very familiar with tensorflow. 
Thx!",tried append work like try add work well another question model due model class self self return give error try recent call last file line return file line status file line next file line status must feed value tensor bool node node handling exception another exception recent call last file line module main file line main evaluate model load file line evaluate result batch file line output file line output file line run file line file line file line raise type message must feed value tensor bool node node defined file line module main file line main evaluate model load file line evaluate file line file line file line file line file line add layer file line output file line call file line training file line file line file line file line file line file line see must feed value tensor bool node node file line file line see must feed value tensor bool node node good way handle problem familiar,issue,positive,positive,positive,positive,positive,positive
313373411,"Things would work just fine if you don't use Keras model. I believe that's because Keras use `tf.Variable` to create variables instead of `tf.get_variable`.

If you really want to use Keras model, either add a `tf.name_scope('tower0')` at predict time, or modify your checkpoint to remove the `tower0` prefix.

",would work fine use model believe use create instead really want use model either add predict time modify remove tower prefix,issue,positive,positive,positive,positive,positive,positive
313301405,"I think it might be a special problem on my machine or data, when I use the batch size = 1, it won't slow down anymore. Thx",think might special problem machine data use batch size wo slow,issue,negative,positive,neutral,neutral,positive,positive
313197601,I think I narrowed down the issue. Model saver + visualizing test set can easily take more than ten seconds. (Since they use different dataflows it thinks the training one has timed out since it hasn't received anything in 10+ seconds). Model saving alone can easily take 10 seconds if you are saving over the network. Maybe you need to add an exception to the timeout for callbacks?,think issue model saver test set easily take ten since use different training one timed since received anything model saving alone easily take saving network maybe need add exception,issue,negative,positive,positive,positive,positive,positive
312981455,It's too bad if it takes >10s for a datapoint. Anyway I'll increase it and provide an option.,bad anyway increase provide option,issue,negative,negative,negative,negative,negative,negative
312410957,"You need to check your dataflow again then. Make sure it could run that many of datapoints without slowing down or throwing exception. I don't think anything in tensorpack will slow it down over time.
Some users have reported that their dataflow code doesn't throw exception (but silently died) due to bad opencv installed (#68). This may also lead to your situation.",need check make sure could run many without throwing exception think anything slow time code throw exception silently due bad may also lead situation,issue,negative,negative,neutral,neutral,negative,negative
312407435,"Well, there are still some problems with the ds in multi-GPUs. It do work on the first several hours of training, but it will slow down after a few hours(GPU waiting for the data)
![screenshot from 2017-07-01 11 18 10](https://user-images.githubusercontent.com/3047375/27758728-ff7954ec-5e4e-11e7-9a72-5af9b712a6f4.png)
finally it will look like this
![screenshot from 2017-07-01 11 19 10](https://user-images.githubusercontent.com/3047375/27758739-296d913c-5e4f-11e7-8335-da3704c16b24.png)
and for the single gpu case it looks like this
![screenshot from 2017-07-01 11 20 26](https://user-images.githubusercontent.com/3047375/27758750-5b4141ae-5e4f-11e7-9ab2-45e9b3a52bc8.png)

And today morning I even found it got stuck in the get_data() here， when executing InferenceRunner. But it works well in the previous epoch.
```
 def run(self):
        with self.default_sess():
            try:
                self.dataflow.reset_state()
                while True:
                    for dp in self.dataflow.get_data():
                        feed = dict(zip(self.placehdrs, dp))
                        # print 'qsize:', self.sess.run([self.op, self.size_op], feed_dict=feed)[1]
                        self.op.run(feed_dict=feed)
            except (tf.errors.CancelledError, tf.errors.OutOfRangeError):
                pass
            except Exception:
                logger.exception(""Exception in EnqueueThread:"")
            finally:
                try:
                    self.close_op.run()
                except Exception:
                    pass
                logger.info(""EnqueueThread Exited."")
```
",well still work first several training slow waiting data finally look like single case like today morning even found got stuck work well previous epoch run self try true feed zip print except pas except exception exception finally try except exception pas,issue,positive,positive,neutral,neutral,positive,positive
312198270,GPU util seems good after we fixed a disk mounting problem (by moving data from network-mounted to local). Thanks :p,good fixed disk mounting problem moving data local thanks,issue,negative,positive,positive,positive,positive,positive
312191313,"Number of GPUs shouldn't affect the speed of your ds, because ds doesn't use GPU at all. You might have done something wrong somewhere, or test the wrong number.",number affect speed use might done something wrong somewhere test wrong number,issue,negative,negative,negative,negative,negative,negative
312174137,"You've given an implementation already, therefore no plan.

The implementation claims to be a ""drop-in"" replacement of other tf optimizers. If that's true you can just use it.",given implementation already therefore plan implementation replacement true use,issue,negative,positive,positive,positive,positive,positive
311982593,"@ppwwyyxx I still do not get it, why it should be a problem. After all it is an entire computation graph. And the issues comes with Tf itself.
TensorPack can provide a function `get_tensor` which tries to be smart enough to test whether there is a training tower or not. ",still get problem entire computation graph come provide function smart enough test whether training tower,issue,negative,positive,neutral,neutral,positive,positive
311961779,any example code for outputting the last layer's representation across the entire training and testing datasets of all batches?,example code last layer representation across entire training testing,issue,negative,neutral,neutral,neutral,neutral,neutral
311864323,"Thanks! 
Downgrade the gym to 0.8.2, it works now.  ",thanks downgrade gym work,issue,negative,positive,positive,positive,positive,positive
311862404,This is https://github.com/openai/atari-py/issues/16. Closing now. I might upload a new model if I have time.,might new model time,issue,negative,positive,positive,positive,positive,positive
311860465,"This is because gym change their environment in recent versions.

On Jun 28, 2017 8:21 PM, ""Qi Wu"" <notifications@github.com> wrote:

> UPDATE: It works on other model and env, such as WizardOfWor-v0
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/319#issuecomment-311851772>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABUTteDHs5t8qTacE4Biz7LMvUaIUJ3zks5sIxg8gaJpZM4OIxRo>
> .
>
",gym change environment recent wrote update work model thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
311746130,"@vqdang Are you looking for (?):

```python
m = Model()

with TowerContext('', is_training=False):
    m._build_graph(m.get_reused_placehdrs())

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

        # do you stuff here
```

Then the `ctx` is available.",looking python model sess stuff available,issue,negative,positive,positive,positive,positive,positive
311743177,"I'd like to point out that the new paper [TTUR](https://github.com/bioinf-jku/TTUR) also calculates stats across large portions of the dataset. It might be useful to integrate such a system as a feature in Tensorpack, especially if we can allow a user to define a temporary static graph to calculate it and a universal data format to store the preprocessed data.",like point new paper also across large might useful integrate system feature especially allow user define temporary static graph calculate universal data format store data,issue,positive,positive,positive,positive,positive,positive
311712082,"I still could not see what's the problem with the current graph in tensorboard. In your code you still dumped everything in `_build_graph` to tensorboard. I can see the only difference to what is now done in tensorpack is that 1. you won't dump the backward part 2. you won't dump the inference tower. I would expect these two parts won't make too much mess in tensorboard.

See [docs](http://tensorpack.readthedocs.io/en/latest/tutorial/model.html#use-models-outside-tensorpack) for your problem about the context.
Could you maybe open a new issue? I think this question is unrelated to this thread.",still could see problem current graph code still everything see difference done wo dump backward part wo dump inference tower would expect two wo make much mess see problem context could maybe open new issue think question unrelated thread,issue,negative,negative,neutral,neutral,negative,negative
311592357,"@ppwwyyxx Sorry for not being clear enough. It's not about all of the ops in the whole graph but the main portion itself. For example:

```
def _build_graph(self, inputs):
    image, label = inputs

    ### Main Network - simple neural network
    with argscope(Conv2D, out_channel=32, kernel_shape=3, nl=tf.nn.relu):
        l = Conv2D('conv0', image, 32, 3, nl=tf.nn.relu)
        l = MaxPooling('pool0', l, 2)
        l = FullyConnected('fc0', l, 512, nl=tf.nn.relu)
        l = Dropout('dropout', l, 0.5)
        l = FullyConnected('fc1', l, 10, nl=tf.identity)

    ### Main Network - complex network with lots of interconnections between layers
    with argscope(Conv2D, out_channel=32, kernel_shape=3, nl=tf.nn.relu):
        l = Conv2D('conv0', image, 32, 3, nl=tf.nn.relu)
        l1, l2, l3 = layer_a('complex_layer_a', l)
        l = layer_b('complex_layer_b', [l, l1, l2])
        l = tf.concat([l, l1, l2, l3], 3)
        l = FullyConnected('fc1', l, 10, nl=tf.identity)

    ### Others stuff in graph for whole training process
    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
    loss = tf.reduce_mean(loss, name='xentropy-loss')

    add_moving_summary(loss)
    self.cost = loss
```

And I want to debug the `l` tree, in a simple case, there is no need for visualization before actual training. But in a more complex case, visualization is needed if I want to make sure all the layers (or components) are linked as intended before actually run the trainer. For now, I do this via

```
def _visualize_graph(self, outdir):
    x = tf.placeholder(tf.uint8, shape=(None, None, None, 3))
    y = tf.placeholder(tf.uint8, shape=(None, None, None, None))
    self._build_graph([x, y])

    with tf.Session() as sess:
        writer = tf.summary.FileWriter(outdir, sess.graph)
        print(sess.run(self.graph_tree))
        writer.close() 
```
With `self.graph_tree = l` in `_build_graph`. However, for `_visualize_graph` to run just by
```
model = Model()
model._visual_graph('model_graph')
```
I need to modify whenever variable `ctx` with `ctx = get_current_tower_context()` is used in all the ops in `l` (in my case, it is the `BatchNorm`). Because in case above, there clearly no context of the tower and I'm not sure how to set the context manually. 

In short, some placeholder for similar situation or an official support to visualize part of the whole graph tree. Beside, to actually get to the graph tree in Tensorboard via running the trainer is also taking much longer time and not suit to debug just the main structure I think.
",sorry clear enough whole graph main portion example self image label main network simple neural network image dropout main network complex network lot image stuff graph whole training process loss loss loss loss loss want tree simple case need visualization actual training complex case visualization want make sure linked intended actually run trainer via self none none none none none none none sess writer print however run model model need modify whenever variable used case case clearly context tower sure set context manually short similar situation official support visualize part whole graph tree beside actually get graph tree via running trainer also taking much longer time suit main structure think,issue,negative,positive,neutral,neutral,positive,positive
311563530,"I really like the current design. Let me just make an objection:
It would be frustrating to refactor all previously written models. 

- You want to have placeholders in the graph for the OfflinePredictor and exporting models.
- all stateless TF functions?
- If you do not put it into the ModelDescr, do you want use to put it multiple times in the TrainConfig, PredictConfig, ... ?
- The `Trainer.tower[0].model.tensor_obj` would be helpful in some cases. When previously defining `self.tensor_obj = ...` in the ModelDescr. Or just make `tower0` the default.
- There always will be a cost tensor in a model and it will be always a scalar. So it needs to really be  hard-coded there.
- As far as I know you can always nest the `tf.device` context and I do not see any use cases. Most approaches are simply data parallel.

It is not really hard to get a tensor. You can always use the `get_tensor_by_name` method of TF. This is what makes TensorPack quite unique: it allows to use _plain_ TensorFlow methods almost everywhere. This follows the ""Principle of Least Astonishment"", which is good.
Maybe it is just a documentation issue.

",really like current design let make objection would previously written want graph stateless put want use put multiple time would helpful previously make tower default always cost tensor model always scalar need really far know always nest context see use simply data parallel really hard get tensor always use method quite unique use almost everywhere principle least astonishment good maybe documentation issue,issue,positive,positive,neutral,neutral,positive,positive
311557636,"Could you be more specific what's wrong with the current graph in tensorboard?
My understanding is that it includes something you don't want to see. But a graph is complicated because neural net training is just so complicated, no way I can automatically guess what you want to see in the graph and what you don't. If you're OK with what your code sample produces, then you can just use it.

What's wrong with batch norm? I think their ops would be grouped under the variable scope so it should look fine.",could specific wrong current graph understanding something want see graph complicated neural net training complicated way automatically guess want see graph code sample use wrong batch norm think would grouped variable scope look fine,issue,negative,negative,negative,negative,negative,negative
311550641,"Could you also add a method to just dump out the base graph to Tensorboard? (No training yet, just general structure debugging) I'm using something like below but it requires to comment/uncomment  some part in `batch_norm.py` every time between debugging/training (mostly the `ctx`).

```
def _visualize_graph(self, outdir):
    x = tf.placeholder(tf.uint8, shape=(None, None, None, 3))
    y = tf.placeholder(tf.uint8, shape=(None, None, None, None))
    self._build_graph([x, y])

    with tf.Session() as sess:
        writer = tf.summary.FileWriter(outdir, sess.graph)
        print(sess.run(self.graph_tree))
        writer.close()
```",could also add method dump base graph training yet general structure something like part every time mostly self none none none none none none none sess writer print,issue,negative,negative,neutral,neutral,negative,negative
311506962,"For that specific instance, the name seems to be input_deque, and a working command line is as follows:

checkpoint-prof.py --model train_log/main/checkpoint --meta train_log/main/graph-0625-115015.meta --input input_deque:0=1,28,28 input_deque:1=1 --output cross_entropy_loss:0 --print-flops",specific instance name working command line model meta input output,issue,negative,neutral,neutral,neutral,neutral,neutral
311410424,"When training with queues, the input tensor is called `input_dequeue:0`, `input_dequeued:1`, the output tensor is `loss:0`
It also builds the graph for validation, where input tensor is named e.g. `input:0`, `image:0`, and output tensor is `towerp0/loss:0`. You can use these names as well.
You can `print(tensor.name)` to see the name. This is currently very unintuitive, I'll fix it in the future.",training input tensor output tensor loss also graph validation input tensor input image output tensor use well print see name currently unintuitive fix future,issue,negative,neutral,neutral,neutral,neutral,neutral
311410004,"@kdplus Please try the above suggestion.
@PatWie This is a big problem with the current interface, also brought out by some others earlier. I would want to improve clarity of ModelDesc in different aspects, but haven't got a concrete plan yet.

Fixing it in the callback can only solve it momentarily. Similar problems happen in a lot of other places. In general, there should be an API to use the name 'cost' to get all costs from each tower. And users should be able to define more tensors like this to use in their callbacks.",please try suggestion big problem current interface also brought would want improve clarity different got concrete plan yet fixing solve momentarily similar happen lot general use name get tower able define like use,issue,positive,positive,neutral,neutral,positive,positive
311318574,"In the multi-Tower setting you need to use 
```python
ProgressBar(['tower0/cost:0']),
```

Each tower has its own scope. To get the name of the Tensor, I do something like

```python
self.cost = bla bla bla
print(self.cost.name)
```

This should give the name for the progressbar.

@ppwwyyxx Should I add some logic to look into Tower 0, if no tensor was found?

**edit** I think it is bad idea to edit the ProgressBar.",setting need use python tower scope get name tensor something like python print give name add logic look tower tensor found edit think bad idea edit,issue,negative,negative,negative,negative,negative,negative
311215880,Providing more and more predefined layers is not a focus of this project. You can use any symbolic function available in tensorflow and it's easier if you implement this by yourself in your training code.,providing focus project use symbolic function available easier implement training code,issue,negative,positive,positive,positive,positive,positive
310859785,"Maybe.
tensorpack did nothing special about saving the events. I think it's not a bug in this project. Closing for now.",maybe nothing special saving think bug project,issue,negative,positive,positive,positive,positive,positive
310787218,But as mentioned https://github.com/tensorflow/tensorflow/issues/9498#issuecomment-307884950 a non-streaming version is usually very simple to implemented from scratch. So in tensorpack using a non-streaming version + `ScalarStats` should be enough for most situations.,version usually simple scratch version enough,issue,negative,neutral,neutral,neutral,neutral,neutral
310785761,"`ClassificationError` is only (slightly) not equivalent to `ScalarStats('some_error_tensor_in_the_graph')`, when the size of validation dataset is not a multiple of validation batch size. Most of the time, as long as you can compute the per-batch error in the graph, `ScalarStats('tensor')` (which gives you mean error for all batches) should be enough and you don't need to add new metrics to callbacks.

In terms of the interface, I think it's a TensorFlow design flaw that they provide only streaming interface without a non-streaming interface. And do not have reset is an even bigger issue..
",slightly equivalent size validation multiple validation batch size time long compute error graph mean error enough need add new metric interface think design flaw provide streaming interface without interface reset even bigger issue,issue,negative,negative,neutral,neutral,negative,negative
310708172,"Issue related to my browser? Got ""Rats! WebGL hits a snag!"" error from Chrome.",issue related browser got snag error chrome,issue,negative,neutral,neutral,neutral,neutral,neutral
310706359,"yes, the code works perfectly.

100%|#####################################################################################|390/390[01:11<00:00, 4.76it/s]
[0623 12:07:18 @base.py:254] Epoch 11 (global_step 4290) finished, time:71.23 sec.
[0623 12:07:19 @saver.py:66] Model saved to _test_/model-4290.
100%|#######################################################################################|79/79[00:05<00:00,14.95it/s]
[0623 12:07:24 @monitor.py:294] cross_entropy_loss: 0.39552
[0623 12:07:24 @monitor.py:294] input_queue_size: 50
[0623 12:07:24 @monitor.py:294] learning_rate: 0.1
[0623 12:07:24 @monitor.py:294] train_error: 0.13705
[0623 12:07:24 @monitor.py:294] val_error: 0.1673
[0623 12:07:24 @monitor.py:294] validation_cost: 0.85421
[0623 12:07:24 @monitor.py:294] wd_cost: 0.33984
[0623 12:07:24 @group.py:43] Callbacks took 5.392 sec in total. InferenceRunner: 5.286sec
[0623 12:07:24 @base.py:167] Start Epoch 12 ...
100%|#####################################################################################|390/390[01:11<00:00, 4.82it/s]
[0623 12:08:36 @base.py:254] Epoch 12 (global_step 4680) finished, time:71.73 sec.
[0623 12:08:36 @saver.py:66] Model saved to _test_/model-4680.
100%|#######################################################################################|79/79[00:05<00:00,15.07it/s]
[0623 12:08:41 @monitor.py:294] cross_entropy_loss: 0.38523
[0623 12:08:41 @monitor.py:294] input_queue_size: 50
[0623 12:08:41 @monitor.py:294] learning_rate: 0.1
[0623 12:08:41 @monitor.py:294] train_error: 0.13363
[0623 12:08:41 @monitor.py:294] val_error: 0.1769
[0623 12:08:41 @monitor.py:294] validation_cost: 0.87989
[0623 12:08:41 @monitor.py:294] wd_cost: 0.3224
[0623 12:08:41 @group.py:43] Callbacks took 5.347 sec in total. InferenceRunner: 5.243sec
[0623 12:08:41 @base.py:167] Start Epoch 13 ...
100%|#####################################################################################|390/390[01:11<00:00, 4.95it/s]
[0623 12:09:53 @base.py:254] Epoch 13 (global_step 5070) finished, time:71.64 sec.
[0623 12:09:53 @saver.py:66] Model saved to _test_/model-5070.
100%|#######################################################################################|79/79[00:05<00:00,14.82it/s]
[0623 12:09:58 @monitor.py:294] cross_entropy_loss: 0.34247
[0623 12:09:58 @monitor.py:294] input_queue_size: 50
[0623 12:09:58 @monitor.py:294] learning_rate: 0.1
[0623 12:09:58 @monitor.py:294] train_error: 0.11957
[0623 12:09:58 @monitor.py:294] val_error: 0.1642
[0623 12:09:58 @monitor.py:294] validation_cost: 0.80837
[0623 12:09:58 @monitor.py:294] wd_cost: 0.30838
[0623 12:09:58 @group.py:43] Callbacks took 5.416 sec in total. InferenceRunner: 5.334sec
[0623 12:09:58 @base.py:167] Start Epoch 14 ...
",yes code work perfectly epoch finished time sec model saved took sec total start epoch epoch finished time sec model saved took sec total start epoch epoch finished time sec model saved took sec total start epoch,issue,positive,positive,neutral,neutral,positive,positive
310705170,"Cannot reproduce your problem. Have you seen logs like ""Model saved to train_log/cifar10-resnet/model-390""? It'll appear after the first epoch.",reproduce problem seen like model saved appear first epoch,issue,negative,positive,positive,positive,positive,positive
310554208,Oh I don't know such machine exists. I'll try to fix.,oh know machine try fix,issue,negative,neutral,neutral,neutral,neutral,neutral
310271445,"@ppwwyyxx 
It's abit troublesome because I need both upsampling and concatenation (via `tf.concat` and `tf.image.resize_nearest_neighbor` or conv2d transpose) but the existence of either resize or conv2d transpose still make iter/sec goes down the same way. I will look it through again today.

@PatWie  Thanks for your nice code! I haven't known such thing exist.

I will close this because this is now out of tensorpack scope. Thanks everyone.
",troublesome need concatenation via transpose existence either resize transpose still make go way look today thanks nice code known thing exist close scope thanks everyone,issue,positive,positive,positive,positive,positive,positive
310220208,"the above merge fixes add border_value for shift and rotate image augmentation. please note that for rgb border_value should be a tuple (r,g,b)",merge add shift rotate image augmentation please note,issue,negative,neutral,neutral,neutral,neutral,neutral
310114120,Usually a bisection can quickly point me to the slow area. You can try profiling as well.,usually bisection quickly point slow area try well,issue,negative,negative,neutral,neutral,negative,negative
310087072,"Do not forget that TensorPack is still TensorFlow. Just do profiling!

```python
m = Model()
hdrs = m.get_reused_placehdrs()


with TowerContext('', is_training=False):
    m._build_graph(hdrs)
    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        sess.run(tf.global_variables_initializer())

        prediction = tf.get_default_graph().get_tensor_by_name('........')
        sess.run(prediction, {hdrs[0]: .....}, options=options, run_metadata=run_metadata)

        fetched_timeline = timeline.Timeline(run_metadata.step_stats)
        chrome_trace = fetched_timeline.generate_chrome_trace_format()
        with open('profile.json', 'w') as f:
            f.write(chrome_trace)
```

This will tell you which ops are executed on CPU and which are on GPU (even on which cuda stream). In addition, you get all timings.

**edit**: It is an interplay with staring at `nvidia-smi -l` and `top` to see the performance.",forget still python model sess prediction prediction open tell executed even stream addition get edit interplay staring top see performance,issue,negative,positive,positive,positive,positive,positive
310017308,Do you know anyway too track which tensorflow operation running on GPU/CPU? I'm doing brute force by rerunning the graph to check it for now but this way is a bit too counter-intuitive.,know anyway track operation running brute force graph check way bit,issue,negative,neutral,neutral,neutral,neutral,neutral
309976455,"If your data is 90it/s and training is only 1.8it/s, bottleneck is not on the data but in the graph.
If GPU utilization is low, it's the CPU operations in the graph that's slow, or too many CPU/GPU transfer, or (unlikely) tensorflow doesn't implement some GPU kernels very well.

If the above statement is correct you may want to check the operations involved in your graph. For example if you used preprocessing ops you might want to explicitly put them on CPUs as mentioned in [tensorflow performance guide](https://www.tensorflow.org/performance/performance_guide)",data training bottleneck data graph utilization low graph slow many transfer unlikely implement well statement correct may want check involved graph example used might want explicitly put performance guide,issue,negative,negative,neutral,neutral,negative,negative
309975567,"Running `TestDataSpeed` got me at least 90 iter/sec both on server and local machine. I rechecked my operation in the graph and changed cropping using slicing to `tf.image.resize_image_with_crop_or_pad`,  (I need to crop output at some bottom layers and then concatenate them at some upper layers, like Unet)   

Testing on single gpu for both server and local machine (same augmentations)

- local machine (nr_proc = 16):  iter/sec stable around 1.84 iter/sec (it improved alot) (just 16 logical processor)
- the server: reach higher peak but dropping to low at some following epochs. `nr_proc > 16` just made it dropping faster (it has 32 logical processor). I then removed the augmentation but it changed nothing.

I also used resizing in the network, do all tensorflow operations operate on GPU? I honestly don't know how should I go with this. But I highly suspect that something CPU-related are my bottleneck, I just dont know how to pinpoint it.

I plan to try the following but I am not sure how I can apply to my case.
```
ds0 = dataset.ILSVRC12('/path/to/ILSVRC12', 'train', shuffle=True)
augmentor = AugmentorList(lots_of_augmentors)
ds1 = ThreadedMapData(
    ds0, nr_thread=25,
    map_func=lambda x: augmentor.augment(x), buffer_size=1000)
# ds1 = PrefetchDataZMQ(ds1, nr_proc=1)
ds = BatchData(ds1, 256)
```

This is my `get_data()`
```
"""""" data: numpy array for images
    label: numpy array for images' labels
""""""
def get_data_generator(data, data_shape, label, label_shape, 
                                batch_size = 16, is_train = False):
    ds = Dataset(data, label)
    ### augmentation for both the input and label
    if is_train:
        shape_aug = [
            ElasticDeform(size = (4, 8), ampl = (6, 10)),
            RandomRotation(45),
            imgaug.Flip(horiz=True),
            imgaug.Flip(vert=True),
            imgaug.ToUint8()
        ]
    else:
        shape_aug = [
            imgaug.CenterCrop(data_shape),
        ]
    ds = AugmentImageComponents(ds, shape_aug, (0, 1), copy=True)

    ### augmentation for just the output
    augmentors = [
        # finally crop mask size to output shape
        LabelProc(False, True), # to fix label value or shape
        imgaug.CenterCrop(label_shape),
    ]
    ds = AugmentImageComponent(ds, augmentors, index = 1, copy=True)

    ### augmentation for just the input
    if is_train:
        augmentors = [
            imgaug.CenterCrop(data_shape),
        ]
        ds = AugmentImageComponent(ds, augmentors, index = 0, copy=False)
        ds = BatchDataByShape(ds, batch_size, idx=0)
        ds = PrefetchDataZMQ(ds, 16)
    else:
        ds = BatchData(ds, batch_size)

    return ds
```
Thanks
",running got least server local machine operation graph slicing need crop output bottom concatenate upper like testing single server local machine local machine stable around logical processor server reach higher peak dropping low following made dropping faster logical processor removed augmentation nothing also used network operate honestly know go highly suspect something bottleneck dont know pinpoint plan try following sure apply case data array label array data label false data label augmentation input label size else augmentation output finally crop mask size output shape false true fix label value shape index augmentation input index else return thanks,issue,positive,positive,neutral,neutral,positive,positive
309685950,"Most likely your data is too slow. You can probably see that input queue size is nearly zero in the log.
hwm in `PrefetchDataZMQ` rarely affects speed and it's better to keep it as default. You can use `TestDataSpeed` to benchmark your dataflow pipeline and figure out which part is slow, or in particular which part may become slower overtime. See some info in [tutorial](http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html) as well.",likely data slow probably see input queue size nearly zero log rarely speed better keep default use pipeline figure part slow particular part may become overtime see tutorial well,issue,negative,positive,neutral,neutral,positive,positive
309598028,"There are too many things you can do in opencv and I don't expect to cover all of them. Users who need this can write their own augs, and I think people usually don't care much about stuff like border value.
You're welcome to send PRs. But this feature is probably not something I'm going to work on.",many expect cover need write think people usually care much stuff like border value welcome send feature probably something going work,issue,positive,positive,positive,positive,positive,positive
309597085,"More options can be added of course.. but the point is that essentially people want to customize __every__ part in the path. Using the more general interface is the clean solution for users to understand what to do, rather than adding more and more options and explaining in the doc how the actual path is made from those options.

Anyway I think the name options is OK. ",added course point essentially people want part path general interface clean solution understand rather explaining doc actual path made anyway think name,issue,positive,positive,positive,positive,positive,positive
309594680,"About the path issue: What about adding yet another argument ""root"" with Default ""./train_log/""?",path issue yet another argument root default,issue,negative,neutral,neutral,neutral,neutral,neutral
309587000,"In the other mode https://www.tensorflow.org/performance/performance_models#replicated_variables each tower has its own var scope.

Yes.",mode tower scope yes,issue,negative,neutral,neutral,neutral,neutral,neutral
309586812,thanks. I though each Tower had its own varscope. Is this the new LeastLoadedDeviceSetter ? ,thanks though tower new,issue,negative,positive,positive,positive,positive,positive
309585521,"It is data parallel but variables are spread among GPUs.
https://www.tensorflow.org/performance/performance_models#parameter_server_variables",data parallel spread among,issue,negative,neutral,neutral,neutral,neutral,neutral
309254042,"An environment variable for log directory may not be a good idea, because I don't always want logs in the same place -- for small experiments I prefer them in the current directory. Env vars should be used for something that's always the same.

Imaging I don't have this function at all and someone sends a PR to add the same `auto_set_dir`, I certainly wouldn't like it because it's personal preference: current dir + ""train_log"" + basename. I think it's already a mistake to bake in my own preference -- clearly now it doesn't work for everyone (including you, #266, and me as well).

I don't know now what's the best thing to do about it. I guess one thing is to reduce the use of it in examples so others know the existence of `set_logger_dir`. Adding an option to partially fix the awkwardness might be OK, but the fix is very partial -- I still have the directory problem.",environment variable log directory may good idea always want place small prefer current directory used something always function someone add certainly would like personal preference current think already mistake bake preference clearly work everyone well know best thing guess one thing reduce use know existence option partially fix awkwardness might fix partial still directory problem,issue,positive,positive,positive,positive,positive,positive
309209487,"You can provide different shapes for different phases, if the Model-InputDesc have a shape-format like `[None, None, None, 3]` for rgb images. This is more about how to use a TensorFlow graph and not TensorPack.

**edit:**
But using the test-split like you described is abusing it as a validation set. The common way (like @ppwwyyxx mentioned it) is to run the testing *after* the training+validation.
",provide different different phase like none none none use graph edit like validation set common way like run testing,issue,positive,negative,neutral,neutral,negative,negative
309208862,"I don't understand how the `run` part can help me achieve that. From what I have seen, the `run` part is not executed at the same session as training. What I need is preferable like this

```
config = TrainConfig(
            dataflow=train_generator,
            callbacks=[
                      InferenceRunner(valid_generator, BinaryClassificationStats('prediction', 'edgemap4d')),
                      InferenceRunner(test_generator, TestInferencer('prediction', 'edgemap4d'))
                      ],
            model=Model(),
            steps_per_epoch=steps_per_epoch,
            max_epoch=100)
```

`test_generator `will also provide different shape from from the `valid_generator`. So far, I have sub-classed `Inferencer` to create the `TestInferencer`. But it seems that I could not even provide input with different shapes to the `Inferencer`. This is the error when `valid_generator` output shape [102, 102, 3] and `test_generator` output shape [478, 478, 3]

`tensorflow/core/framework/tensor_shape.cc:240] Check failed: size >= 0 (-3800563712 vs. 0)`
",understand run part help achieve seen run part executed session training need preferable like also provide different shape far create could even provide input different error output shape output shape check size,issue,positive,positive,neutral,neutral,positive,positive
309206852,"That `git diff` is only working, when one is regularly doing commits and there is a git-repo :-) But I like it.

Ok, maybe the word `idea` is misleading.
Currently, I am directly facing a problem, where I can easily adjust a hyper-parameter about e.g. the depth of the network and I just find it convenient to have a function which adds a suffix to the path. Still, my current solution with pen and paper is not optimal.",git working one regularly like maybe word idea misleading currently directly facing problem easily adjust depth network find convenient function suffix path still current solution pen paper optimal,issue,positive,positive,positive,positive,positive,positive
309206691,"That said, I'm OK with making a function for ""getting the basename for the entry point"" into utils. It's just 3 lines but not trivial lines..

To try new ideas I just rename the main script. I used to modify the scripts in-place until I realize it's hard to stay on track. I ran idea1-run1, idea2-run2... in parallel, and the next morning I forgot what exactly is idea1 and run1, because the file is now on idea2.

I've seen some scripts from openai, where every time something is run, a `git diff` is created to the log directory. That could be also a solution to my problem..",said making function getting entry point trivial try new rename main script used modify realize hard stay track ran parallel next morning forgot exactly idea run file idea seen every time something run git log directory could also solution problem,issue,negative,positive,neutral,neutral,positive,positive
309206169,"Of course it's harder to write with the dumb `set_logger_dir` interface. But ""auto"" is not so smart either. 
I can totally see that adding a ""name"" option would make things easier, but what if I have to store to a different directory (this is happening to me now because of disk space)? Another ""dir"" option? What if someone doesn't like ""train_log"" (#266 seems to suggest so)? Another option? Maybe the only thing people wouldn't hate is the basename part..",course harder write dumb interface auto smart either totally see name option would make easier store different directory happening disk space another option someone like suggest another option maybe thing people would hate part,issue,negative,negative,negative,negative,negative,negative
309205587,"Do you really want us to write
```python
logger.set_logger_dir(os.path.join('train_log', basename[:basename.rfind('.')]) + ""idea"")
```

all the time instead of 

```python
logger.auto_set_dir(name='idea')
```
**edit**
How do you solve this issue? The ResNet example only has `logger.auto_set_dir()`",really want u write python idea time instead python edit solve issue example,issue,negative,positive,positive,positive,positive,positive
309205545,"If `logger.set_logger_dir` sees an existing directory, it will still give an interactive menu",directory still give interactive menu,issue,negative,neutral,neutral,neutral,neutral,neutral
309202539,"1. The ""high-water-mark"" is what people called buffer size. http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.PrefetchDataZMQ
If both cpu and gpu are starved you might want to reduce the communication by e.g. compression.

2. A lot of examples run testing after training, for instance [this one](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/HED/hed.py#L189).",people buffer size starved might want reduce communication compression lot run testing training instance one,issue,negative,neutral,neutral,neutral,neutral,neutral
309202352,"@ppwwyyxx  Thanks

I also have some more questions to ask.

- Is there anyway to set the buffer size for `PrefetchDataZMQ` ? Setting more nr_proc didn't seem to better utilize my CPUs while the GPUS still starved

- `get_current_tower_context().is_training`, allows `InferenceRunner` to perform validation. I also need to run testing after training like validation (inputs at `_build_graph` only has image, no label). Basically at each epoch it will be training -> validation -> testing. How can I achieve that?
",thanks also ask anyway set buffer size setting seem better utilize still starved perform validation also need run testing training like validation image label basically epoch training validation testing achieve,issue,positive,positive,positive,positive,positive,positive
307865627,"I think bn averages should be updated when you fine tune.

If you'd like to only update some of your bn update ops, you can write a new callback similar to `RunUpdateOps`, and filter the ops by your self. The code is very simple:
```python
class RunUpdateOps(RunOp):
    def __init__(self, collection=tf.GraphKeys.UPDATE_OPS):
        def f():
            ops = tf.get_collection(collection)
# do your filtering here
            if ops:
                logger.info(""Applying UPDATE_OPS collection of {} ops."".format(len(ops)))
                return tf.group(*ops, name='update_ops')
            else:
                return tf.no_op(name='empty_update_ops')

        super(RunUpdateOps, self).__init__(
            f, run_before=False, run_as_trigger=False, run_step=True)
```",think fine tune like update update write new similar filter self code simple python class self collection filtering collection return else return super self,issue,positive,positive,positive,positive,positive,positive
307860830,"I think batch normalization layer is updated through moving averages. So even if gradient is zero, batch normalization parameters will still change. Is there a way to only apply `RunUpdateOps()` to some of batch normalzation layers?",think batch normalization layer moving even gradient zero batch normalization still change way apply batch,issue,negative,neutral,neutral,neutral,neutral,neutral
307855943,Is there a way to freeze only part of batchnorm layers? My project requires freezing part of the model and fine-tuning the rest of it. Thanks.,way freeze part project freezing part model rest thanks,issue,negative,positive,positive,positive,positive,positive
307599205,">  However, if you wanted to experiment about say dividing by the difference between your predictions and the actual dataset, that would be interesting.

It's easy if this is what you want to do. Though I don't see it has anything to do with your original question.
You can just write a callback which runs some forward predictions and do whatever with its output. [CycleGAN example](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/GAN/CycleGAN.py#L182) has such a callback.",however experiment say dividing difference actual would interesting easy want though see anything original question write forward whatever output example,issue,positive,positive,positive,positive,positive,positive
307598599,"Should of been more clear, it would not change. However, if you wanted to experiment about say dividing by the difference between your predictions and the actual dataset, that would be interesting. I just wanted to demonstrate their should be a way to have a different training regime happen every x epochs, possibly one that doesn't involve backprop. I guess the example I used could be hacked using the inferencerunner? Just wanted to see what you would recommend in that case.",clear would change however experiment say dividing difference actual would interesting demonstrate way different training regime happen every possibly one involve guess example used could hacked see would recommend case,issue,positive,positive,positive,positive,positive,positive
307595110,"Also should mention that it needs to be updated every n epochs. Essentially, it would be like a scheduled hyperparam setter but the values used would be dynamically calculated based on the dataset. I could loop over it I suppose, but it's pretty inelegant and probably a good use case for a new type of callback. I was wondering if there were any callbacks that could be exploited to do this already.",also mention need every essentially would like setter used would dynamically calculated based could loop suppose pretty inelegant probably good use case new type wondering could already,issue,positive,positive,positive,positive,positive,positive
307431097,You can disable moving average update by overwrite the default callback list (remove `RunUpdateOps` from `extra_callbacks`). http://tensorpack.readthedocs.io/en/latest/tutorial/callback.html,disable moving average update overwrite default list remove,issue,negative,negative,negative,negative,negative,negative
307326261,batch norm uses moving averages which get changed whenever training data pass through.,batch norm moving get whenever training data pas,issue,negative,neutral,neutral,neutral,neutral,neutral
307075837,"Thanks. Maybe also mention somewhere in the ""efficient data loading"" documentation that this incompatibility exists and that old LMDB files don't work anymore when upgrading? The dependence might not be obvious to everyone and it took me a long time to figure out what the problem was (probably also as I am less experienced)",thanks maybe also mention somewhere efficient data loading documentation incompatibility old work dependence might obvious everyone took long time figure problem probably also le experienced,issue,negative,positive,positive,positive,positive,positive
307063907,Just updated the dependency to require msgpack-numpy>=0.3.9 1175aade9a482. Thanks for your help!,dependency require thanks help,issue,positive,positive,positive,positive,positive,positive
307063032,"Looks like this is mentioned in their [docs](https://github.com/lebedov/msgpack-numpy/blob/master/CHANGES.rst#release-039---february-7-2017), that data serialized with an older version (<0.3.9) cannot be deserialized with a newer version.",like data older version version,issue,negative,positive,positive,positive,positive,positive
307052360,"Ok, now I can reproduce the error. I uninstalled msgpack (in conda) and only installed msgpack-python/msgpack-numpy with pip.

For msgpack-numpy latest version (0.4.0) I get the following error
```
   File ""mini_imagenet.py"", line 340, in <module>
    df = get_data('test', 'predict', print_datapoints=1, classes=classes, n_split=len(classes), shuffle=False)
  File ""mini_imagenet.py"", line 267, in get_data
    ds = PrintData(ds, num=print_datapoints)
  File ""~/lib/tensorpack/tensorpack/dataflow/common.py"", line 626, in __init__
    self.print_info()
  File ""~/lib/tensorpack/tensorpack/dataflow/common.py"", line 696, in print_info
    for i, dummy in enumerate(cutoff(ds.get_data(), self.num)):
  File ""~/lib/tensorpack/tensorpack/dataflow/common.py"", line 686, in cutoff
    for el in gen:
  File ""~/lib/tensorpack/tensorpack/dataflow/common.py"", line 231, in get_data
    for dp in self.ds.get_data():
  File ""~/lib/tensorpack/tensorpack/dataflow/common.py"", line 231, in get_data
    for dp in self.ds.get_data():
  File ""~/lib/tensorpack/tensorpack/dataflow/common.py"", line 231, in get_data
    for dp in self.ds.get_data():
  File ""~/lib/tensorpack/tensorpack/dataflow/common.py"", line 232, in get_data
    ret = self.func(dp)
  File ""~/lib/tensorpack/tensorpack/dataflow/common.py"", line 254, in f
    r = func(dp[index])
  File ""mini_imagenet.py"", line 242, in <lambda>
    ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)   # [[w, h, c], [class]]; c is in BGR
TypeError: buf is not a numpy array, neither a scalar
```

If I use msgpack-numpy==0.3.8 everything works smoothly. Thus, v0.4.0 seems to de-serialise the lmdb-file differently to v0.3.8 (everything else stayed the same). ",reproduce error uninstalled pip latest version get following error file line module class file line file line file line dummy enumerate cutoff file line cutoff el gen file line file line file line file line ret file line index file line lambda lambda class array neither scalar use everything work smoothly thus differently everything else stayed,issue,negative,positive,positive,positive,positive,positive
306798774,Thanks! Works like a charm!,thanks work like charm,issue,positive,positive,positive,positive,positive,positive
306690014,"Wrapping the predictor in 'with tf.device(...)' doesn't work because the OfflinePredictor builds the graph within an TowerContex which uses an '\gpu:0' as default.
I build want to use a Resnet to preprocess the input for another  Resnet (OfflinePredictor as target function of an ThreadedMapData). Because of there size of these networks I need to put them in distinct GPUs. (Writing the activations to a file isn't feasible ether)

- Christian",wrapping predictor work graph within default build want use input another target function size need put distinct writing file feasible ether,issue,negative,positive,positive,positive,positive,positive
306515760,What is your use case? I think you can always just write `tf.device` around your predictor.,use case think always write around predictor,issue,negative,neutral,neutral,neutral,neutral,neutral
306428271,"The old API had a versioning. I am pretty confident, they will add this also to the new one.",old pretty confident add also new one,issue,positive,positive,positive,positive,positive,positive
306423584,"You are wrong. The correct slogan is ""One more thing (tm)"".",wrong correct slogan one thing,issue,negative,negative,negative,negative,negative,negative
306419177,"One last thing, could you put the file in `tfutils/` instead?",one last thing could put file instead,issue,negative,neutral,neutral,neutral,neutral,neutral
306349372,"Now `build(checkpoint)` can only be called once per instance. So this class is essentially one-time-use only.

I don't know if this use case really exists, but if you move the `checkpoint` argument into `export`, you can then do the following:
```python
e = Export(model, inputs, outputs)
e.build() # or do it in __init__
e.export(checkpoint1, export1)
# ...
e.export(checkpoint2, export2)
```",build per instance class essentially know use case really move argument export following python export model export export,issue,negative,positive,neutral,neutral,positive,positive
306278096,"Ah, I see. The NVIDIA® DGX-1 is the $129,000 US single machine monster :scream::laughing:",ah see registered u single machine monster scream,issue,negative,negative,neutral,neutral,negative,negative
306272513,"Will add some examples.
Note that the code actually doesn't scale very well, neither does the official benchmark in my tests. Their benchmark number appears good but that's on slow K80s so communication is less of a problem. I'll bring the issue to them.",add note code actually scale well neither official number good slow communication le problem bring issue,issue,negative,positive,positive,positive,positive,positive
306270668,"Yes I hope to keep the shape output. Just want to use tf.layers for the actual implementation.

I guess first step we can switch the implementation but keep all the APIs/names unchanged (by custom_getter). 
One bottleneck now is that nested custom_getter is unsupported for the moment (tensorflow#6634)",yes hope keep shape output want use actual implementation guess first step switch implementation keep unchanged one bottleneck unsupported moment,issue,negative,positive,positive,positive,positive,positive
306269617,"Does `tf.layers` follows a common parameter pattern (name, input, param1, param2)? Does their BatchNorm provides ""use_local_stats""?

I also thought about a ""@tflayer_register()"", which is probably the smoothest transition towards `tf.layers`. Hopefully, the output of ""incoming-shape, outgoing shape, the number of params, size"" will still exist in the future tensorpack versions.

",common parameter pattern name input param param also thought probably transition towards hopefully output outgoing shape number size still exist future,issue,positive,negative,negative,negative,negative,negative
306128434,"It would be nice of course.
But you can just call `tf.layers.conv2d` in your model, with the `dilation_rate` option.",would nice course call model option,issue,negative,positive,positive,positive,positive,positive
306052775,But an imagenet Resnet example to just get the usage would be nice.,example get usage would nice,issue,negative,positive,positive,positive,positive,positive
306027913,"If that's what's confusing you: `pos_weight` in my code is __not__ the positive ratio, 1-beta is.

Closing now. Please reopen if you still have questions.",code positive ratio please reopen still,issue,positive,positive,positive,positive,positive,positive
305914384,"yeah, a tutorial with some examples about callbacks will be greatly beneficial for sure! ",yeah tutorial greatly beneficial sure,issue,positive,positive,positive,positive,positive,positive
305905016,"before_run/after_run is called together with every `sess.run`. That is every step in an epoch.
Haven't got time to write any tutorials about writing callbacks.. I should definitely do that soon.",together every every step epoch got time write writing definitely soon,issue,negative,neutral,neutral,neutral,neutral,neutral
305904717,"before_run is just able to collect the current mini-batch data or can collect all the mini batches in a epoch? In order to have the projected representations of all the training data, I will need to collect the last layer's representations of all batches of an epoch in all the GPU towers, is it correct?",able collect current data collect epoch order training data need collect last layer epoch correct,issue,negative,positive,positive,positive,positive,positive
305895328,"No because different GPUs train on different data.
If you need them all just add them in before_run.",different train different data need add,issue,negative,neutral,neutral,neutral,neutral,neutral
305894521,"Would the tensors in different towers return the same results for the _after_run? 
Thanks!
",would different return thanks,issue,negative,positive,neutral,neutral,positive,positive
305864750,"Finalize the graph, create the session ...
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0a:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)
I tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:2 for node 'tower2/input_deque' because the input edge from 'input_queue' is a reference connection and already has a device field set to /GPU:3
I tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'tower1/input_deque' because the input edge from 'input_queue' is a reference connection and already has a device field set to /GPU:3
I tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'tower0/input_deque' because the input edge from 'input_queue' is a reference connection and already has a device field set to /GPU:3
[0602 13:49:13 @base.py:132] Graph variables initialized.
[0602 13:49:13 @concurrency.py:36] Starting EnqueueThread ...
[0602 13:49:13 @base.py:167] Start Epoch 1 ...
  0%|                                                                                                              |0/390[00:00<?,?it/s]
W tensorflow/core/kernels/queue_base.cc:294] _0_input_queue: Skipping cancelled enqueue attempt with queue not closed
Traceback (most recent call last):
[0602 13:49:16 @input_data.py:125] EnqueueThread Exited.
  File ""cifar10-resnet.py"", line 599, in <module>
    SyncMultiGPUTrainer(config).train()
  File ""/data/datapk/tensorpack/tensorpack/train/base.py"", line 96, in train
    self.main_loop()
  File ""/data/datapk/tensorpack/tensorpack/train/base.py"", line 240, in main_loop
##     last_layer_rep.append(self.model.lastLayerRepresentation.eval())
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 567, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3729, in _eval_using_default_session
    return session.run(tensors, feed_dict)
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input' with dtype float
         [[Node: input = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
         [[Node: towerp0/gap/output/_3953 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_683_towerp0/gap/output"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'input', defined at:
  File ""cifar10-resnet.py"", line 599, in <module>
    SyncMultiGPUTrainer(config).train()
  File ""/data/datapk/tensorpack/tensorpack/train/base.py"", line 95, in train
    self.setup()
  File ""/data/datapk/tensorpack/tensorpack/train/base.py"", line 110, in setup
    self._setup()   # subclass will setup the graph
  File ""/data/datapk/tensorpack/tensorpack/train/multigpu.py"", line 112, in _setup
    super(SyncMultiGPUTrainer, self)._setup()
  File ""/data/datapk/tensorpack/tensorpack/train/feedfree.py"", line 39, in _setup
    self._input_method.setup_training(self)
  File ""/data/datapk/tensorpack/tensorpack/train/input_data.py"", line 158, in setup_training
    super(QueueInput, self).setup_training(trainer)
  File ""/data/datapk/tensorpack/tensorpack/train/input_data.py"", line 39, in setup_training
    self.setup(trainer.model)
  File ""/data/datapk/tensorpack/tensorpack/train/input_data.py"", line 148, in setup
    self.input_placehdrs = model.get_reused_placehdrs()
  File ""/data/datapk/.local/lib/python2.7/site-packages/functools32/functools32.py"", line 378, in wrapper
    result = user_function(*args, **kwds)
  File ""/data/datapk/tensorpack/tensorpack/models/model_desc.py"", line 64, in get_reused_placehdrs
    return self.build_placeholders()
  File ""/data/datapk/tensorpack/tensorpack/models/model_desc.py"", line 88, in build_placeholders
    name=prefix + v.name))
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1502, in placeholder
    name=name)
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2149, in _placeholder
    name=name)
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input' with dtype float
         [[Node: input = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
         [[Node: towerp0/gap/output/_3953 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_683_towerp0/gap/output"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
",finalize graph create session device device name bus id device device name bus id device device name bus id device device name bus id device specification node input edge reference connection already device field set device specification node input edge reference connection already device field set device specification node input edge reference connection already device field set graph starting start epoch skipping attempt queue closed recent call last file line module file line train file line file line return self session file line return file line run file line file line file line raise type message must feed value tensor float node input node defined file line module file line train file line setup subclass setup graph file line super self file line self file line super self trainer file line file line setup file line wrapper result file line return file line file line file line file line file line file line see must feed value tensor float node input node,issue,positive,positive,neutral,neutral,positive,positive
305864545,"I see what's going on. You're not supposed to use `model.lastLayerRepresentation` directly -- there are many of them. When you use multiGPU there is multiple of this tensors on each GPU. When you enable inference there is one extra of this tensor in the inference graph. Using `model.xxx` cannot give you the tensor you need.

For `before_run/after_run`:
```python
def _before_run(self, _):
    return ['fc/output']  # the name of the tensor(s) to eval. (you can also return a SessionRunArgs to be compatible with SessionRunHook). 
# You can print the tensor in _build_graph to figure out the name of your tensor (remember there could be multiple of them)

def _after_run(self, _, values):
    # values.results contain the value of the tensors
```
See https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook.
",see going supposed use directly many use multiple enable inference one extra tensor inference graph give tensor need python self return name tensor also return compatible print tensor figure name tensor remember could multiple self contain value see,issue,negative,positive,positive,positive,positive,positive
305861670,"I meant the projected embeddings of the training data at the last layer (before the FC) of the ResNet. I need to collect all min-batches of one epoch before I can run my clustering.

It  was caused by the added eval() call in the middle of  of the main_loop(self) in the base.py. 
The complaint came from the InferenceRunner(dataset_test,[ScalarStats('cost'), ClassificationError()]) function. If I remove the InferenceRunner, everything works fine. Any example of using tf.train.SessionRunHook? 
",meant training data last layer need collect one epoch run clustering added call middle self complaint came function remove everything work fine example,issue,negative,positive,neutral,neutral,positive,positive
305858678,"What do you mean by real data?
Do you want to run the network in training mode or test mode?
Do you want to run it on the data you're currently trained on or some other data?
The easiest way if you just want to run the training network on the data you're currently trained, is to use `before_run` and `after_run`, which have identical API with `tf.train.SessionRunHook`.

And another problem is that why would your ""lastLayerRepresentation"" requires a feed. It shouldn't happen if inputs are properly defined by tensorpack. Could you post some logs on that?",mean real data want run network training mode test mode want run data currently trained data easiest way want run training network data currently trained use identical another problem would feed happen properly defined could post,issue,negative,negative,neutral,neutral,negative,negative
305856488,"Thanks. 
So how could I collect the real data from the lastLayerRepresentation of the ResNet? I need to collect several steps(epochs) of the lastLayerRepresentation  data and then run a clustering on the collection. 
I need to do that periodically during the training of the ResNet. ",thanks could collect real data need collect several data run clustering collection need periodically training,issue,negative,positive,positive,positive,positive,positive
305835898,"Sorry. I thought the k (keep) selection below would perform the ""--load"" action to restore the latest saved model.

[0602 12:16:41 @logger.py:86] WRN If you're resuming from a previous run you can choose to keep it.
[0602 12:16:41 @logger.py:87] Select Action: k (keep) / b (backup) / d (delete) / n (new):


It works perfectly now when I use this one:  python nips-cifar10-resnet.py --gpu 0,1,2,3 -n 5 --load train_log_pretrain_10epoch390_/model-1170

Thanks a lot for your prompt help!",sorry thought keep selection would perform load action restore latest saved model previous run choose keep select action keep backup delete new work perfectly use one python load thanks lot prompt help,issue,positive,positive,positive,positive,positive,positive
305727574,"see:
https://github.com/ppwwyyxx/tensorpack/blob/master/.travis.yml#L39-L40
I would suggest to build OpenCV from source and not let this library dictate how to build OpenCV.",see would suggest build source let library dictate build,issue,negative,neutral,neutral,neutral,neutral,neutral
305709064,closed in favor of the distributed branch. Nice work @ppwwyyxx !!,closed favor distributed branch nice work,issue,positive,positive,positive,positive,positive,positive
305696380,"If you have such code already, you can call your code or tflearn wrapper directly within a tensorpack model.

We might add some simple wrappers around `tf.layers` in the future (#236), but directly calling tf.layers works the same.",code already call code wrapper directly within model might add simple around future directly calling work,issue,negative,positive,neutral,neutral,positive,positive
305681386,"Could you provide the command you're using, and the full log of the second run?
It looks like you're not running the examples. Could you explain/post what changes you've made to the code?",could provide command full log second run like running could made code,issue,negative,positive,positive,positive,positive,positive
305622563,Thanks for trying to figure out the problem! For your information I've never installed msgpack anywhere. I always only install msgpack-python/msgpack-numpy with pip (and I always use latest version).,thanks trying figure problem information never anywhere always install pip always use latest version,issue,negative,positive,positive,positive,positive,positive
305620962,"Unfortunately, I cannot do any testing until Tuesday. I currently use version 0.3.8 which works. A colleague has been using 0.3.9 and had problems; however, I am not certain that was the cause as we haven't checked that 0.3.8 solved the problem for him

The problem was that:
a) no error message when msgpack was not installed (initially I had msgpack-python and msgpack-numpy installed and didn't get an error but it also didn't work). I would expect an error message ""module missing"" or something similar in that case.
b) The main problem for finding the error was that de-serialization did not return an error but a datastream that could not be interpreted by opencv. Thus, I was looking into issues with opencv for a while before realising that the de-serialization was actually the culprit.

Possible solutions:
a) I will do testing for version specificity next week
b) I think an exception should be raised if a module is missing instead of just returning something that cannot be interpreted (even if msgpack is in the requirement; I used the pip installation recommended in the readme and it did not install msgpack on its own).",unfortunately testing currently use version work colleague however certain cause checked problem problem error message initially get error also work would expect error message module missing something similar case main problem finding error return error could thus looking actually culprit possible testing version specificity next week think exception raised module missing instead something even requirement used pip installation install,issue,negative,negative,neutral,neutral,negative,negative
305618624,"Are there more details? 
Otherwise I can only do some guess on what version is needed, based on msgpack changelog.",otherwise guess version based,issue,negative,neutral,neutral,neutral,neutral,neutral
305236432,"msgpack is in the requirement, but no version is specified now.
What's the behavior when a wrong version is installed?",requirement version behavior wrong version,issue,negative,negative,negative,negative,negative,negative
303912808,"It will depend on what you mean by ""weights"".

As I said, no bias, no bn scaling factors.",depend mean said bias scaling,issue,negative,negative,negative,negative,negative,negative
303911519,"In my comprehension, it means all trainable weights in every layer. Is that right?",comprehension trainable every layer right,issue,negative,positive,positive,positive,positive,positive
303794546,I changed these lines and also partly tested python3 compatibility.,also partly tested python compatibility,issue,negative,neutral,neutral,neutral,neutral,neutral
303542982,"Oh I probably renamed the collection some time ago.
This shouldn't affect dump-model-params. I'll fix it.",oh probably collection time ago affect fix,issue,negative,neutral,neutral,neutral,neutral,neutral
303469232,"I have collected a large image data set, where I simply rejected images without any prominent edges. Using this custom dataset helps a lot to converge faster.
So, I would guess it depends heavily on the training data, e.g. try MNIST.",collected large image data set simply without prominent custom lot converge faster would guess heavily training data try,issue,negative,positive,positive,positive,positive,positive
303445441,"Do you know why they get to a very small loss with only 490 steps (and their loss is summed over the image and averaged over batch). Maybe because their dataset is easier?
Anyway the current result you showed is pretty good.",know get small loss loss summed image batch maybe easier anyway current result pretty good,issue,positive,positive,positive,positive,positive,positive
303302724,"Because that would be wrong I think.
The test of whether label is true or false is already done in weighted_cross_entropy. Pos/Neg cost are already weighted.

If you think the current code is not equivalent to the original formula please show why.",would wrong think test whether label true false already done cost already weighted think current code equivalent original formula please show,issue,positive,negative,neutral,neutral,negative,negative
303299867,"Sorry if I wasn't clear, I guess my wondering is whether that line should instead be something like:
`cost = tf.reduce_mean(tf.where(tf.equal(y_true, 1.), cost * (1 - beta), cost))`


",sorry clear guess wondering whether line instead something like cost cost beta cost,issue,positive,negative,negative,negative,negative,negative
303297025,"I don't fully understand your question. From what I can tell the two implementation handle the ratio in the same way.
Maybe you didn't see this line of code? ` cost = tf.reduce_mean(cost * (1 - beta))`",fully understand question tell two implementation handle ratio way maybe see line code cost cost beta,issue,negative,neutral,neutral,neutral,neutral,neutral
303294878,"I can reproduce the bug with 1.0.1. So probably a bug in TF.
I could fix it in tensorpack as well.",reproduce bug probably bug could fix well,issue,negative,neutral,neutral,neutral,neutral,neutral
303288243,Thanks for your prompt reply. I am using tensorflow 1.0.1 and the newest version of tensorpack. Should I upgrade tensorflow to 1.1.0 to fix this problem?,thanks prompt reply version upgrade fix problem,issue,negative,positive,positive,positive,positive,positive
303189699,"I don't think, that their ""dynamic conv implementation"" is more efficient (in terms of speed *and* memory). Rather than  using their explicit im2col, this implementation uses a highly optimized call of TF. 
Further, TensorFlow uses the cuda streams, so this would also hide the latency from data transfers between kernel calls from the `[... zip(...)]`.

",think dynamic implementation efficient speed memory rather explicit implementation highly call would also hide latency data kernel zip,issue,positive,positive,neutral,neutral,positive,positive
303140529,Cannot reproduced the problem. Could you tell how you installed tensorpack and tensorflow? Maybe one of them is outdated.,problem could tell maybe one outdated,issue,negative,negative,negative,negative,negative,negative
303027539,"Alright, it seems to be quite easy to [send](https://github.com/PatWie/tf_zmq/blob/master/write.py) and [receive](https://github.com/PatWie/tf_zmq/blob/master/read_py.py) the `tensor_msg` format with native python.

So **all** combinations are now working:
send (py, c++) and receive (py, c++, tf).

https://github.com/PatWie/tf_zmq",alright quite easy send receive format native python working send receive,issue,positive,positive,positive,positive,positive,positive
302996619,"Since they did open source their code https://github.com/dbbert/dfn/blob/master/experiment_steerableFilter_tensorflow.ipynb, I guess it would be better to follow the network & hyperparam rather than making things up.
Plus they seem to have a more efficient dynamic conv implementation.",since open source code guess would better follow network rather making plus seem efficient dynamic implementation,issue,positive,positive,positive,positive,positive,positive
302966089,You can use the `identity` workaround. I'll also prepare a fix.,use identity also prepare fix,issue,negative,neutral,neutral,neutral,neutral,neutral
302963370,"Ah, figured it out. It's the same bug the was encountered in the rate portion of DiscoGAN. get_global_step_var()'s return value needs to be wrapped in a tf.identity call.",ah figured bug rate portion return value need wrapped call,issue,negative,neutral,neutral,neutral,neutral,neutral
302917801,"if you run this network, you can use tensorboard and will see these filters after a while. Tensorboard will show (OnlineTensorboardCallback) both the expected and learned filters.",run network use see show learned,issue,negative,neutral,neutral,neutral,neutral,neutral
302907751,"Looks like this is section4.1 of the paper. 
Were you able to see the network learning expected filters? If it's successful it would be a good example to add.",like section paper able see network learning successful would good example add,issue,positive,positive,positive,positive,positive,positive
302770797,"On a HDD RAID with 1GB/s bandwidth, after clearing cache, running the equivalent of:
```python
ds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)
ds = BatchData(ds, 256, use_list=True)
```
LMDB sequential: 42it/s
TFRecord sequential 35it/s.",raid clearing cache running equivalent python sequential sequential,issue,negative,neutral,neutral,neutral,neutral,neutral
302574258,"i can confirm this fixed the logging issue.
",confirm fixed logging issue,issue,negative,positive,neutral,neutral,positive,positive
302565322,"Don't have a windows to test, but from strace log it looks like now all files are closed before performing the `delete` action.
```
[pid 14405] open(""train_log/mnist-convnet/stat.json.tmp"", O_WRONLY|O_CREAT|O_TRUNC|O_CLOEXEC, 0666) = 11</home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/stat.json.tmp>                                             
[pid 14405] close(11</home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/stat.json.tmp>) = 0                                                                                                                            
[0518 16:02:39 @monitor.py:294] accuracy: 0.29745                                                                                                                                                                                       
[0518 16:02:39 @monitor.py:294] cross_entropy_loss: 1.9657                                                                                                                                                                              
[0518 16:02:39 @monitor.py:294] lr: 0.001                                                                                                                                                                                               
[0518 16:02:39 @monitor.py:294] param-summary/conv0/W-rms: 0.46391                                                                                                                                                                      
[0518 16:02:39 @monitor.py:294] param-summary/conv1/W-rms: 0.083028                                                                                                                                                                     
[0518 16:02:39 @monitor.py:294] param-summary/conv2/W-rms: 0.084169                                                                                                                                                                     
[0518 16:02:39 @monitor.py:294] param-summary/conv3/W-rms: 0.083865                                                                                                                                                                     
[0518 16:02:39 @monitor.py:294] param-summary/fc0/W-rms: 0.034486                                                                                                                                                                       
[0518 16:02:39 @monitor.py:294] param-summary/fc1/W-rms: 0.062386                                                                                                                                                                       
[0518 16:02:39 @monitor.py:294] regularize_loss: 0.0049347                                                                                                                                                                              
[0518 16:02:39 @monitor.py:294] total_cost: 1.9706                                                                                                                                                                                      
[0518 16:02:39 @monitor.py:294] train_error: 0.70253                                                                                                                                                                                    
[pid 14405] close(10</home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/events.out.tfevents.1495148557.KeepLearning>) = 0                                                                                              
[pid 14478] +++ exited with 0 +++                                                                                                                                                                                                       
[pid 14405] close(9</home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/log.log>) = 0                                                                                                                                   
[0518 16:02:39 @logger.py:92] WRN Directory train_log/mnist-convnet exists! Please either backup/delete it, or use a new directory.                                                                                                     
[0518 16:02:39 @logger.py:94] WRN If you're resuming from a previous run you can choose to keep it.                                                                                                                                     
[0518 16:02:39 @logger.py:95] Select Action: k (keep) / b (backup) / d (delete) / n (new):   
```",test log like closed delete action open close accuracy close close directory please either use new directory previous run choose keep select action keep backup delete new,issue,positive,positive,neutral,neutral,positive,positive
302511312,"Oh I see. It attempts to delete the old log file while it is open. It is allowed in linux.
I'll see if I can fix it.",oh see delete old log file open see fix,issue,negative,positive,neutral,neutral,positive,positive
302510821,this is when trying to write to the same log file with set_logger_dir(action='d'),trying write log file,issue,negative,neutral,neutral,neutral,neutral,neutral
302502228,"Even when you've set `logger_dir` to a different place? Do you know which file it is referring to?

I did find that I should've unload the existing file handler when `logger_dir` was changed. But this seems unrelated.",even set different place know file find unload file handler unrelated,issue,negative,neutral,neutral,neutral,neutral,neutral
302500018,In windows i get error 32 file in use,get error file use,issue,negative,neutral,neutral,neutral,neutral,neutral
302483528,"What's the issue with logging?
I do find that you may need to `set_logger_dir` to a different place otherwise the second training would see the old logs and thinks the training has finished already.",issue logging find may need different place otherwise second training would see old training finished already,issue,negative,positive,neutral,neutral,positive,positive
302467187,"wow! how did you pinpoint the problem? I was debugging for hours...

for completeness, in my usecase i also have to do between runs:
import logging; logging.shutdown()",wow pinpoint problem completeness also import logging,issue,negative,positive,neutral,neutral,positive,positive
302462685,"Ah, I see your point.
I was experimenting with pybind (seems easier than swig) and msgpack. 
Then,one just needs the `sbuffer` from msgpack.",ah see point easier swig one need,issue,negative,neutral,neutral,neutral,neutral,neutral
302447730,"To underscore my point, here is the entirety of contrib staging module minus comments:
```python

""""""contrib module containing StagingArea.""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from tensorflow.python.ops.data_flow_ops import StagingArea
```",underscore point entirety staging module minus python module import import division import import,issue,negative,negative,neutral,neutral,negative,negative
302444470,"@ppwwyyxx This does the same thing as it being added to nightly. Check out the file they add to [contrib](https://github.com/tensorflow/tensorflow/blob/ffd1ed2df78bdc719abe6180db896b26d04d1b70/tensorflow/contrib/staging/__init__.py). Since they do not appear to be moving the file and just referencing, this method would be preferable wouldn't it?

",thing added nightly check file add since appear moving file method would preferable would,issue,negative,neutral,neutral,neutral,neutral,neutral
302434994,"The staging contrib module does not appear to be in the pip build on Google's website. 
from tensorflow.contrib import staging is failing even though I have the required version of 1.1.0.

Found the bug, StagingArea got moved and is in from tensorflow.python.ops.data_flow_ops import StagingArea instead of contrib.",staging module appear pip build import staging failing even though version found bug got import instead,issue,negative,neutral,neutral,neutral,neutral,neutral
302352441,"I have thought about that. But if you are working in python, then it is easier to directly generate `tf.Tensors ` and just send them. Anyway, my latest attempt to create a `write.py` is here:
https://github.com/PatWie/tf_zmq/blob/bin/python/example.py#L9",thought working python easier directly generate send anyway latest attempt create,issue,positive,positive,positive,positive,positive,positive
302312001,Nice work! Looks like it's using msgpack to replace protobuf. I guess the next question is how to do the equivalent encode method in Python (so that we can send a DataFlow to TF through zmq).,nice work like replace guess next question equivalent encode method python send,issue,positive,positive,positive,positive,positive,positive
302193562,"As TF does not export these symbols, I wrote a small `tensor_msg` class which can be serialized by msgpack: 
https://github.com/PatWie/tf_zmq",export wrote small class,issue,negative,negative,negative,negative,negative,negative
301930040,I didn't get why this is needed. What's the benefit of using a new API serialize/deserialize compared to the original APIs `dump_to_lmdb` and `LMDBDataPoint`? You need to write decode/encode somewhere anyway.,get benefit new original need write somewhere anyway,issue,positive,positive,positive,positive,positive,positive
301888494,"At least there should be a convenient usage:
- get data
- encode
- dump

and then
- load
- decode

What about a combined `Serializer` class which can return properties like a data stream for reading and writing, like

```
ds = ...
wrapper = MyFileTypeWrapper(file='/tmp/test.lmdb', format='lmdb')
wrapper.serialize(ds)
```

and then

```
wrapper = MyFileTypeWrapper(file='/tmp/test.lmdb', format='lmdb')
wrapper.deserialize(ds) # a combined version of image-encode, decode and loads and dump
```
",least convenient usage get data encode dump load decode combined class return like data stream reading writing like wrapper wrapper combined version decode dump,issue,negative,negative,negative,negative,negative,negative
301887473,"I agree they are common and I'm only concerned with the type confusion. ""Encode"" always means ""convert to bytes"" except in opencv. In fact, encoding to nparray is only useful when you decode with opencv. 
When we add distributed training, we probably have to use jpeg-encoded __bytes__ in a TFRecord and decode with TF, to reach maximum speed. At least that's what google is doing. In that case we'll need encode/decode to handle bytes.
I don't mind adding it if it could at least support bytes, but it will just look too complicated (both the docs and the code) given that it could be done in one line. Originally I used bytes in the tutorial and I only added nparray conversion for speed (and not even sure if it really helps).

I agree on the rename. But `MsgPackDecoder` is not very good because it decodes `dp[1]`, which is an unexpected behavior from the name. Decoding from `dp[1]` is LMDB-specific behavior (just like decoding nparray is cv-specific behavior) so we better keep LMDB in the name.",agree common concerned type confusion encode always convert except fact useful decode add distributed training probably use decode reach maximum speed least case need handle mind could least support look complicated code given could done one line originally used tutorial added conversion speed even sure really agree rename good unexpected behavior name behavior like behavior better keep name,issue,positive,positive,positive,positive,positive,positive
301878313,"I would still vote for these atomic on-liners `ImageEncode`, `ImageDecode`  as they are pretty common in any good LMDB based training procedure. And they do not force the user to remember the specific cv2-calls. In general, I agree on removing one-liner and promote to use `MapData` instead. However, the decoding and encoding are pretty much standard.

Please note, that the code from the current documentation does not work:

```python
ds = LMDBData(db, shuffle=False)
ds = LocallyShuffleData(ds, 50000)
ds = LMDBDataPoint(ds)
```
In `LMDBDataPoint` there is a `isinstance(args[0], LMDBData)` which is obviously `False` if there is a `LocallyShuffleData` between.

Maybe a renaming from
`LMDBDataPoint` to `MsgPackDecoder` or `LMDBdumpDeoder` is also more intuitive. ",would still vote atomic pretty common good based training procedure force user remember specific general agree removing promote use instead however pretty much standard please note code current documentation work python obviously false maybe also intuitive,issue,positive,positive,neutral,neutral,positive,positive
301527718,"It will prompt you for action if the logging directory exists.
You can set a different directory #266 , or pass an action so it won't ask you for it, e.g. `auto_set_dir('d')`.
http://tensorpack.readthedocs.io/en/latest/modules/utils.html#tensorpack.utils.logger.set_logger_dir

All the logs from tensorpack will be saved in `logging directorey/log.log` by the way.",prompt action logging directory set different directory pas action wo ask saved logging way,issue,negative,positive,neutral,neutral,positive,positive
300970262,"No. my number was after batching to 256, the exact code in the tutorial.
And it seems like you've done imdecode so the performance is lower.",number exact code tutorial like done performance lower,issue,negative,positive,positive,positive,positive,positive
300945707,"Ah, looking closer at how sample is called I see that's the case.

Looks like the real bug is in my prioritized experience replay implementation which handles this case incorrectly.",ah looking closer sample see case like real bug experience replay implementation case incorrectly,issue,negative,positive,positive,positive,positive,positive
300938584,Slice is only useful when the memory buffer is full,slice useful memory buffer full,issue,negative,positive,positive,positive,positive,positive
300881907,Probably RAM on my test machines wasn't very good. I just ran a hdparm benchmark on a new machine and see 11G/s cached read speed. This seems to be close to the speed you get so I think we can update the docs.,probably ram test good ran new machine see read speed close speed get think update,issue,negative,positive,positive,positive,positive,positive
300862086,"@ppwwyyxx Are you really sure, that the reading speed is just 130 it/s (cached)? I get 400 it/s on local SSD with reasonable RAM.
```
[0511 19:25:24 @format.py:84] Found 1281167 entries in /ssd/patwie/imagenet.lmdb
[0511 19:25:24 @common.py:702] DataFlow Info:
datapoint 0<1 with 2 components consists of
   dp 0: is ndarray of shape (375, 500, 3) with range [0.0000, 255.0000]
   dp 1: is int of shape () with range [49]
  3%|#3                                                |34741/1281167[01:27<49:52,416.50it/s]
```",really sure reading speed get local reasonable ram found shape range shape range,issue,negative,positive,positive,positive,positive,positive
300767063,thanks. would be nice to have an option in auto_set_dir to change 'train_log' but still use the base_name logic,thanks would nice option change still use logic,issue,positive,positive,positive,positive,positive,positive
300685235,"Just wrapping tf.layers will be a breaking change as you will never be able to load old models again without a lot of code in the ModelLoader. Further, some layers like BatchNorm in your lib have some extra versions (local stats) which are helpful.
So, we need 
- mnist-prettytensor.py
- mnist-sonnet.py
- mnist-tf.layers.py
- mnist-sugartensor.py

? :-)

Maybe, providing the common layers is ok. They won't be changed again (maybe just the fused conv2d+bias). So I closed this pull-request.",wrapping breaking change never able load old without lot code like extra local helpful need maybe providing common wo maybe fused closed,issue,negative,negative,neutral,neutral,negative,negative
300669057,"@Skylion007 just made a MultiGPU GAN trainer in 01486c39dfbe141adbff3. 
Tested on BEGAN, it scales 1.95x on 2 GPUs and 3.88x on 4 GPUs.
May not be the most efficient way but the performance is OK.",made gan trainer tested scale may efficient way performance,issue,negative,neutral,neutral,neutral,neutral,neutral
300615148,"In the future we can hopefully be free from maintaining all kinds of layers, and just copy or wrap `tf.layers` or other well-maintained wrappers. `tf.layers` have the attribute accessor as well (`.variables`, etc) so this could be possible. The only inconsistency now is the naming (W vs weights, out_channel vs filters, etc), so I hope to keep them in sync at least in terms of functionality.

If I restart the project I would just use `tf.layers`.. but at that time there was no such thing. 😞 ",future hopefully free copy wrap attribute well could possible inconsistency naming hope keep sync least functionality restart project would use time thing,issue,positive,positive,neutral,neutral,positive,positive
300615052,"I feel very confused about what it really does when these convolutions are mixed in one function, especially when the concept of ""separable"" and ""depthwise"" already kind of overlap.

I hope we can just provide individual functions so that they have their corresponding counterparts in `tf.nn.*` or `tf.layers.*`, with the same set of arguments. ",feel confused really mixed one function especially concept separable depthwise already kind overlap hope provide individual corresponding set,issue,positive,positive,neutral,neutral,positive,positive
300508746,Even though you have multiple GPUs you still need to split the task to run on multiple of them.,even though multiple still need split task run multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
300461435,"@PatWie Sorry to bother you again, I still want to know does the above mentioned error occurred due to the memory limitation of single TitanX (12GB)? so we can't solve it by multiple GPUs?",sorry bother still want know error due memory limitation single ca solve multiple,issue,negative,negative,negative,negative,negative,negative
300458229,"The line
```
outputs = predictor([[im.astype('float32')]])
```
runs the network on `im` and returns the result. So you have to do something like (pseudo-code)
```
im = some_image
for patch in im:
  rs = predictor([[patch.astype('float32')]])[5][0]
  put rs at correct position in im
```

",line predictor network result something like patch predictor put correct position,issue,negative,neutral,neutral,neutral,neutral,neutral
300455502,"@PatWie 
Change this line to what? I can't figure out what's the purpose of this line, it's just get the same size of image?",change line ca figure purpose line get size image,issue,negative,neutral,neutral,neutral,neutral,neutral
300450376,"You need to extract patches from the image and process each individually if the memory is not large enough. I barely see deep-learning algorithm which works on fullHD. Just change the line in:
https://github.com/ppwwyyxx/tensorpack/blob/master/examples/HED/hed.py#L198",need extract image process individually memory large enough barely see algorithm work change line,issue,negative,positive,neutral,neutral,positive,positive
300227284,"You are right! I changed the behavior of the `domain` arg. Maybe `range` is more consistent (not correct, but consistent). There is a collision with the `six.range`.",right behavior domain maybe range consistent correct consistent collision,issue,negative,positive,positive,positive,positive,positive
300227247,"TensorFlow only supports NHWC __on cpu__.
If you really want to use CPU, you can change `data_format` to 'NHWC' at [this line](https://github.com/ppwwyyxx/tensorpack/blob/da8899e7755bea5cb48c4c23974c493f8c151495/examples/ResNet/cifar10-resnet.py#L74), and remove the layout transform [at this line](https://github.com/ppwwyyxx/tensorpack/blob/da8899e7755bea5cb48c4c23974c493f8c151495/examples/ResNet/cifar10-resnet.py#L50).",really want use change line remove layout transform line,issue,negative,positive,positive,positive,positive,positive
300143656,"dtype argument is needed, but why will the domain be useful?
My thought was, that if you ever need ""RandomUniformData"", then it's better to write a ""RandomUniformData"" rather than using something called ""FakeData"".",argument domain useful thought ever need better write rather something,issue,positive,positive,positive,positive,positive,positive
299960213,It might be worth to refactor this into a class with `__call__` to reuse code.,might worth class reuse code,issue,negative,positive,positive,positive,positive,positive
299903665,"A comment:
I think it is hard to parse the current mnist-convnet example when starting working with tensorpack. Further, I always start by trimming the MNIST example, when I start a new project. I think a well-maintained boilerplate template without deprecation warnings is a nice start point.

I think I started to mess up the code in `mnist-convnet.py` by hacking in the tfSlim library (which I never used again from that day).",comment think hard parse current example starting working always start trimming example start new project think template without deprecation nice start point think mess code hacking library never used day,issue,negative,positive,neutral,neutral,positive,positive
299868239,"also seeing this.

in image2image.py after 
        GANTrainer(config).train()
i added:
        model_file = os.path.join(logger.LOG_DIR, 'checkpoint')
        sample(args.data, model_file)

this used to work. but after recent changes to master i am getting:
Traceback (most recent call last):
  File ""Image2Image.py"", line 230, in <module>
    sample(args.data, model_file)
  File ""Image2Image.py"", line 199, in sample
    pred = SimpleDatasetPredictor(pred, ds)
  File ""C:\Anaconda3\lib\site-packages\tensorpack\predict\dataset.py"", line 65, in __init__
    self.predictor = OfflinePredictor(config)
  File ""C:\Anaconda3\lib\site-packages\tensorpack\predict\base.py"", line 141, in __init__
    config.model.build_graph(input_placehdrs)
  File ""C:\Anaconda3\lib\site-packages\tensorpack\models\model_desc.py"", line 116, in build_graph
    self._build_graph(model_inputs)
  File ""Image2Image.py"", line 118, in _build_graph
    self.build_losses(real_pred, fake_pred)
  File ""C:\Users\eyaler\Dropbox\python\tensorpack\examples\GAN\GAN.py"", line 54, in build_losses
    add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)
  File ""C:\Anaconda3\lib\site-packages\tensorpack\tfutils\summary.py"", line 162, in add_moving_summary
    avg_maintain_op = averager.apply(v)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\training\moving_averages.py"", line 387, in apply
    (1.0 + num_updates) / (10.0 + num_updates))
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 1392, in minimum
    result = _op_def_lib.apply_op(""Minimum"", x=x, y=y, name=name)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 331, in apply_op
    g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3912, in _get_graph_from_inputs
    _assert_same_graph(original_graph_element, graph_element)
  File ""C:\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3851, in _assert_same_graph
    ""%s must be from the same graph as %s."" % (item, original_item))
ValueError: Tensor(""truediv_2:0"", shape=(), dtype=float32) must be from the same graph as Tensor(""EMA/decay:0"", shape=(), dtype=float32).",also seeing added sample used work recent master getting recent call last file line module sample file line sample file line file line file line file line file line file line file line apply file line minimum result minimum file line file line file line must graph item tensor must graph tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
299827620,"Silly me. 

edit: Sorry: It is always the case after opening an issue, the mistake becomes totally visible.",silly edit sorry always case opening issue mistake becomes totally visible,issue,negative,negative,negative,negative,negative,negative
299681482,"Turned out the only change needed is to assign variables to different GPUs, and make sure sum of gradients is colocated with the variable.
Also, because variables are scattered around, BN update needs to be delayed (otherwise will hurt performance a lot).

```
+-------------------------------+----------------------+----------------------+
|   2  Tesla M40           On   | 0000:09:00.0     Off |                    0 |
|  0%   60C    P0   170W / 250W |  10998MiB / 11443MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla M40           On   | 0000:0A:00.0     Off |                    0 |
|  0%   62C    P0   173W / 250W |  10996MiB / 11443MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla M40           On   | 0000:0B:00.0     Off |                    0 |
|  0%   52C    P0   176W / 250W |  10996MiB / 11443MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla M40           On   | 0000:0C:00.0     Off |                    0 |
|  0%   60C    P0   180W / 250W |  10996MiB / 11443MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla M40           On   | 0000:0D:00.0     Off |                    0 |
|  0%   56C    P0   159W / 250W |  10994MiB / 11443MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla M40           On   | 0000:0E:00.0     Off |                    0 |
|  0%   58C    P0   164W / 250W |  10994MiB / 11443MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
```",turned change assign different make sure sum variable also scattered around update need otherwise hurt performance lot mib mib default mib mib default mib mib default mib mib default mib mib default mib mib default,issue,negative,positive,positive,positive,positive,positive
299677811,"At some time I stop summarizing ""cost"" by default, and this creates a problem with slim.
Because gradients don't actually depend on cost, so the UPDATE_OPS were never called.
I'll fix it soon.",time stop cost default problem slim actually depend cost never fix soon,issue,negative,neutral,neutral,neutral,neutral,neutral
299605725,"Able to see some improvements on multigpu training.
M40, ResNet18, batch per GPU=128. Use float32 image. 
Compare `StageInputWrapper(QueueInput)` with `QueueInput`:
Each epoch is 50steps. Listing per-epoch time (not including warm up epoch):
1GPU: 17.92s. about the same
2GPU: 18.5s vs 19.6s. 6% faster
4GPU: 19.26s vs 21.6s. 12% faster
6GPU: 20.55s vs 22.9s. 11.5% faster

Ugly code: https://gist.github.com/ppwwyyxx/4e45ab862e0589e2941edc490187ba17

I'll add it as default for MultiGPUTrainer.
It probably will run faster by getting rid of python threads and use TF pipeline.

NOTE: 
tested on 08/06, 4GPU number can be (roughly) reproduced with TF1.2.0rc0, tensorpack 9d0b28a. (19.9s v 21.6s)
with TF1.3.0rc2, tensorpack 1dbf615430703, 19.22s vs 20.3s.
Also, it is observed that staging doesn't improve speed of resnet50.",able see training batch per use float image compare epoch listing time warm epoch faster faster faster ugly code add default probably run faster getting rid python use pipeline note tested number roughly also staging improve speed,issue,negative,positive,neutral,neutral,positive,positive
299493371,"I like this interface. Not too hacky and convenient.
",like interface hacky convenient,issue,negative,neutral,neutral,neutral,neutral,neutral
299206698,"To avoid OOM, we have to pre-fill the staging area with some data, and run stage_op together with train_op (instead of in a separate thread). This will avoid staging area growing too large. This is from tensorflow/benchmarks.
I'm trying this but haven't got any faster yet.. I may have used it in a wrong way.",avoid staging area data run together instead separate thread avoid staging area growing large trying got faster yet may used wrong way,issue,negative,negative,negative,negative,negative,negative
299174187,"TF just released their ""good"" distributed code yesterday. So it will take some time.",good distributed code yesterday take time,issue,negative,positive,positive,positive,positive,positive
299173088,"This has nothing to do with TrainConfig.
You can tell whether you're in training phase in the model by `get_current_tower_context().is_training`. See this line: [mnist example](https://github.com/tensorpack/tensorpack/blob/f417c49fe45759fb2c69cafcabe6613ea85ec469/examples/basics/mnist-tflayers.py#L54), and it is also explained in the [docs](https://tensorpack.readthedocs.io/tutorial/trainer.html#what-you-can-do-inside-tower-function).",nothing tell whether training phase model see line example also,issue,negative,neutral,neutral,neutral,neutral,neutral
299133356,"I found an interesting thing that if I changed the input data:

`input, output = input / 128.0 - 1, output / 128.0 - 1`

as:

`output = output / 128.0 - 1`.

Then, both `discrim/accuracy` and `gen/accuracy` tended to be `0.5` during the training.

Therefore, the operation `input = input / 128.0 - 1` is unnecessary.",found interesting thing input data input output input output output output training therefore operation input input unnecessary,issue,negative,positive,neutral,neutral,positive,positive
299063916,"Official code and benchmarks were released:
https://www.tensorflow.org/performance/performance_models
https://www.tensorflow.org/performance/benchmarks

I'll study it when I have time and see how much we can integrate them.",official code study time see much integrate,issue,negative,positive,positive,positive,positive,positive
299049653,"Now you can use `config.predict_tower = [-1]` to let it use CPU. Or `config.predict_tower = [1]` to let it use GPU1 instead of GPU0 (default).

But note that TF doesn't support convolution with NCHW on CPU. So it won't work out of the box for convnet anyway (if you use NCHW for speed).",use let use let use instead default note support convolution wo work box anyway use speed,issue,negative,neutral,neutral,neutral,neutral,neutral
299048127,"hmm I've hard-coded to use the first GPU. I can make some changes.
But using a smaller batch size in validation will be faster than CPU anyway.",use first make smaller batch size validation faster anyway,issue,negative,positive,positive,positive,positive,positive
298591549,"Why?
Python3 is supported. Those libraries (subprocess32, functools32) doesn't matter.
The warning you saw is intended.",python matter warning saw intended,issue,negative,neutral,neutral,neutral,neutral,neutral
298577810,"Thank you for your replies. I find a [result](https://github.com/tambetm/simple_dqn/blob/master/results/breakout.csv) generated by simple_dqn. I don't know if you have checked out this repository. Although the max score is >400, the average score is ~200. I am not quite understand why there is a different.",thank find result know checked repository although score average score quite understand different,issue,negative,negative,neutral,neutral,negative,negative
298570532,"Yes.
In the code there is some lines implementing the original arch. I just uncomment it and run.
1. I don't remember how long it takes. It iterates 2x faster but may take more iterations to train. The gpu is TitanX. Framerate has different definitions.
2. About 400 on breakout.",yes code original arch run remember long faster may take train different breakout,issue,positive,positive,positive,positive,positive,positive
298569457,"So your implement change:
1. using maxpool
2. using different relu
3. padding (I'm not sure. It is not mentioned in paper)
4. double q
5. initializer (Some said deepmind were using xavier)

Is this list correct?
And you said the original architecture worked well. Do you still remember the details? I have several questions:
1. In the README, you said training DQN only take about one day, so the original network only took half of the day? If so, what's the gpu? Do you still remember how much frame rate in training phase and evaluation phase?
2. Is the score still >300 pts in original setup?",implement change different padding sure paper double said list correct said original architecture worked well still remember several said training take one day original network took half day still remember much frame rate training phase evaluation phase score still original setup,issue,positive,positive,positive,positive,positive,positive
298561710,Yes. But I tried the original architecture in the paper and it works similarly well.,yes tried original architecture paper work similarly well,issue,positive,positive,positive,positive,positive,positive
298560745,"No, I mean I cannot reproduce using my own code. When I visit openai gym website, your report score of a3c is on the list. So I check out your code and want to learn how you implement DQN. 
So do using maxpool layer improve the performance?
BTW, I saw people were talking about reproducing score at [here](https://github.com/tambetm/simple_dqn/issues/32).",mean reproduce code visit gym report score list check code want learn implement layer improve performance saw people talking score,issue,negative,negative,negative,negative,negative,negative
298557053,"Yes.
Do you mean you cannot reproduce with my code or others?
I'm using double DQN and that's what I'm trying to reproduce. A cmdline flag can change it to DQN and I believe it can be reproduced as well.",yes mean reproduce code double trying reproduce flag change believe well,issue,positive,negative,negative,negative,negative,negative
298452015,"I see. 
But based on the above code, TOTAL_BATCH_SIZE is hard-coded and it is 512. BATCH_SIZE in turn is calculated based on total batch size and nr_gpus. 
",see based code turn calculated based total batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
298447220,You need NR_GPU=4 for 4 gpus. Otherwise it'll actually use a total batch of 256.,need otherwise actually use total batch,issue,negative,neutral,neutral,neutral,neutral,neutral
298430539,"The function was just a hack to go against TF scope rules. 
I'm not aware any current use of this function. If you don't have a good use of the function I'll just remove it.

If you want to open a non-reuse scope inside a reuse scope, it may be better to just open an issue in TF and question their design.",function hack go scope aware current use function good use function remove want open scope inside reuse scope may better open issue question design,issue,positive,positive,positive,positive,positive,positive
298427559,"This may be a silly question but did you modify ""NR_GPU"" in the code?",may silly question modify code,issue,negative,negative,negative,negative,negative,negative
298311645,"Another difference is that, with
```
sess.run(self.d_min)
sess.run(self.g_min)
```
G and D are trained on different batch of input images. When written together they are trained on same batch of images.",another difference trained different batch input written together trained batch,issue,negative,neutral,neutral,neutral,neutral,neutral
298271405,Thanks! That works fine! Is it possible to assign different GPU to different predictors?,thanks work fine possible assign different different,issue,positive,positive,positive,positive,positive,positive
298250997,"@PatWie Could you exploit this fact to make a Sync / Async MultiGPU GAN trainer? I've been trying to figure out how to add one in, but I have had little luck figuring out the best way to go about it.",could exploit fact make sync gan trainer trying figure add one little luck best way go,issue,positive,positive,positive,positive,positive,positive
298239870,"What do you mean by ""load""?
You can easily use multiple models. Just create multiple PredictConfig and then predictors. Is that what you want?",mean load easily use multiple create multiple want,issue,positive,positive,neutral,neutral,positive,positive
298211089,You can just use tf.nn.dropout in your model code with the argument you like.,use model code argument like,issue,negative,neutral,neutral,neutral,neutral,neutral
298155538,"So much luck dring your training! I also ran the models, but they also generated creepy faces. So BEGAN basically shows that that others just generate much more creepy faces. Their paper also states that they used their own dataset.",much luck training also ran also creepy basically generate much creepy paper also used,issue,negative,negative,negative,negative,negative,negative
298155420,"You can also write
```
sess.run(self.d_min)
sess.run(self.g_min)
```

later in the GANTrainer. But the 
http://devdocs.io/tensorflow~python/tf/control_dependencies
connect both operations within the graph, which reduces the sess.run overhead.",also write later connect within graph overhead,issue,negative,neutral,neutral,neutral,neutral,neutral
298151170,"Thank @ppwwyyxx ,

I have another question. Why did you need tf.control_dependencies before d_min ops?
",thank another question need,issue,negative,neutral,neutral,neutral,neutral,neutral
298150395,"In the GAN.py implementation, is it true that we need to minimize d_loss, as opposed to the **minimizing -d_loss** (maximizing d_loss) ops in GAN theory?

```python
# by default, run one d_min after one g_min
        self.g_min = opt.minimize(self.model.g_loss, var_list=self.model.g_vars, name='g_op')
        with tf.control_dependencies([self.g_min]):
            self.d_min = opt.minimize(self.model.d_loss, var_list=self.model.d_vars, name='d_op')
        self.train_op = self.d_min
```",implementation true need minimize opposed gan theory python default run one one,issue,negative,positive,positive,positive,positive,positive
297645686,"Yes. For classic GAN, the number is unrelated to image quality.",yes classic gan number unrelated image quality,issue,negative,positive,positive,positive,positive,positive
297644127,"I just reopened for more discussion. 
If this is the case, we can not trust the result from the generator, can we?
The generated images are just qualitatively fine, not quantitatively. ",discussion case trust result generator qualitatively fine quantitatively,issue,positive,positive,positive,positive,positive,positive
297613640,"You can now use `self.trainer.monitors.put_image(name, array)` inside a callback, to put a numpy array to tensorboard.",use name array inside put array,issue,negative,neutral,neutral,neutral,neutral,neutral
297609818,You need to write a DataFlow to handle your own data format. See [documentation](http://tensorpack.readthedocs.io/en/latest/tutorial/extend/dataflow.html),need write handle data format see documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
297609641,"Cheers! I figured it out, after manually moving some files from one to another. Appreciation. 👍 

BTW, if I want to use my own dataset, the path should look like:
    PATH/
      train/
        n02134418/
          n02134418_198.JPEG
          ...
        ...
      val/
        ILSVRC2012_val_00000001.JPEG
        ...
where is the label input to your model? If I have all pictures information contained in a file with format like:
    pic1.path label1
    pic2.path label2
    ....
any suggestions for adapting my dataset to your model?",figured manually moving one another appreciation want use path look like label input model information file format like label label model,issue,positive,neutral,neutral,neutral,neutral,neutral
297607736,"You have to have protoc to run anything related to imagenet.
I don't know how to install it on windows.",run anything related know install,issue,negative,neutral,neutral,neutral,neutral,neutral
297607554,"You got the point, I mistook the resnet file as alexnet file... but it came another bug below, it happend as well when I tried to train the model:

[0426 23:18:57 @sessinit.py:190] Restoring from dict ...
[0426 23:18:58 @fs.py:99] WRN $TENSORPACK_DATASET not set, using C:\Users\Pengyu Li/tensorpack_data for dataset.
caffe.proto: 65.5KB [00:01, 43.4KB/s]
Succesfully downloaded caffe.proto 58196 bytes.
caffe.proto: No such file or directory
Traceback (most recent call last):
  File ""alexnet-dorefa.py"", line 311, in <module>
    run_image(Model(), DictRestore(np.load(args.load, encoding='latin1').item()), args.run)
  File ""alexnet-dorefa.py"", line 262, in run_image
    meta = dataset.ILSVRCMeta()
  File ""D:\Software\Anaconda\envs\py35\lib\site-packages\tensorpack\dataflow\dataset\ilsvrc.py"", line 33, in __init__
    self.caffepb = get_caffe_pb()
  File ""D:\Software\Anaconda\envs\py35\lib\site-packages\tensorpack\utils\loadcaffe.py"", line 128, in get_caffe_pb
    ""Command `protoc caffe.proto --python_out .` failed!""
AssertionError: Command `protoc caffe.proto --python_out .` failed!

",got point mistook file file came another bug well tried train model set file directory recent call last file line module model file line meta file line file line command command,issue,negative,neutral,neutral,neutral,neutral,neutral
297605180,"The ""theory"" doesn't necessarily work in practice.",theory necessarily work practice,issue,negative,neutral,neutral,neutral,neutral,neutral
297597237,"Hi, when I run your code I got an error like this:

(py35) D:\workplace\comp 535\src>Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""D:\Software\Anaconda\envs\py35\lib\multiprocessing\spawn.py"", line 106, in spawn_main
    exitcode = _main(fd)
  File ""D:\Software\Anaconda\envs\py35\lib\multiprocessing\spawn.py"", line 116, in _main
    self = pickle.load(from_parent)
EOFError: Ran out of input

any suggestions?",hi run code got error like recent call last file string line module file line file line self ran input,issue,negative,neutral,neutral,neutral,neutral,neutral
297477446,"The new tensorboard has a slider below each image-summary. If `tf.summary.image` is called on the same input, one could inspect the result directly in the browser from different epochs. This is currently not possible in tensorpack (at least this seems to be requested).",new slider input one could inspect result directly browser different currently possible least,issue,negative,negative,neutral,neutral,negative,negative
297456925,"If it is just a fixed validation set, why do you need to do it during training instead of before training?

And you can do that by writing an Inferencer. No need to hack anything.",fixed validation set need training instead training writing need hack anything,issue,negative,positive,neutral,neutral,positive,positive
297455152,Also a request would be if you could allow the callback to say save images of the validation set in a folder. That would be extremely useful is one of the main issues I have with the current repository. Is there a way to hack inference runner to do this currently? ,also request would could allow say save validation set folder would extremely useful one main current repository way hack inference runner currently,issue,positive,positive,positive,positive,positive,positive
297232648,"There is a plan about it but not ongoing.
Basically the goal is to let the [monitor](https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/callbacks/monitor.py) support data types other than scalar.",plan ongoing basically goal let monitor support data scalar,issue,negative,neutral,neutral,neutral,neutral,neutral
296417641,"This is the failure procedures.
1. Install tensorflow 0.12.1 using docker and installed opencv(make install), tensorpack
  - There's no problem.
2. Upgrade tensorflow using ""pip install -upgrade"" to v1.0.1, and tensorpack 0.1.8
  - Segfault occurred. 
3. I removed opencv and make install again.
  - Segfault occurred.


After that, I re-installed tensorflow(using docker), opencv and tensorpack from scratch. 
And there's no problem. It might be caused by upgrading tensorflow.
I tried to identify the problem with existing repository, but it failed.

Anyway, it is solved.
Thank you.



",failure install docker make install problem upgrade pip install removed make install docker scratch problem might tried identify problem repository anyway thank,issue,negative,negative,negative,negative,negative,negative
296380982,"In sync training, the optimizer needs to maintain the global_step by itself, so I shouldn't use a callback to maintain it. That's a problem.",sync training need maintain use maintain problem,issue,negative,neutral,neutral,neutral,neutral,neutral
296279631,Have you tried to disable GPU? How did you install opencv? Opencv can cause segfault if installed from 3rd-party prebuilt or sometimes on GPU.,tried disable install cause sometimes,issue,negative,neutral,neutral,neutral,neutral,neutral
296154751,"Their tutorial describes a distributed **async** training. But this pull request deals with a **sync** training procedure, which I found more intuitively and less hacky. I have no experience which version is faster (async/sync) or better. But I guess this depends on the actual problem. ",tutorial distributed training pull request sync training procedure found intuitively le hacky experience version faster better guess actual problem,issue,negative,positive,positive,positive,positive,positive
296138103,"I'm sorry I gave you a little clue.
Same problem in other machines (I used same docker env.)

I thought that the segfault is always occurred on ModelSaver, but this is not true.
If I add meaningless code(e.g. add dummy argument), it stops immediately like belows.

[0421 09:09:53 @base.py:116] Setup callbacks graph ...
[0421 09:09:53 @base.py:187] Building predictor graph towerp0 on gpu=0 ...
Segmentation fault (core dumped)

One of the other phenomena is that segfault always occurs at the end, even if it is not interrupted.

I think I have to find more clues to explain you.
One of the suspects is that I didn't install TF & TP from scratch. 



",sorry gave little clue problem used docker thought always true add meaningless code add dummy argument immediately like setup graph building predictor graph segmentation fault core one phenomenon always end even interrupted think find explain one install scratch,issue,negative,negative,negative,negative,negative,negative
296105868,"Sorry but there's nothing I can do about it given the little information.
In most cases, any segfault is a problem of either tensorflow or the environment.

It's better if you could give more information, e.g., when does it segfault. Does it always segfault at the same place. At which line does it segfault. Have you tried other TF versions (e.g. TF1.1). Have you tried disable/enable GPU. Have you tried other machines, etc...",sorry nothing given little information problem either environment better could give information always place line tried tried tried,issue,negative,negative,neutral,neutral,negative,negative
295996378,"https://www.tensorflow.org/versions/master/deploy/distributed 
The tutorial I'm looking at doesn't use supervisor. Why do you think it's needed?",tutorial looking use supervisor think,issue,negative,neutral,neutral,neutral,neutral,neutral
295990214,Thanks very much for trying out! I'll play with your code soon.,thanks much trying play code soon,issue,positive,positive,positive,positive,positive,positive
295854357,"Nice! Currently, I cannot see where I wired my layers in the wrong way. Would be cool to check it against your implementation.",nice currently see wired wrong way would cool check implementation,issue,negative,positive,positive,positive,positive,positive
295853098,"I have updated the [gist](https://gist.github.com/PatWie/47ca4a676698952536d4e9bc07a4f49d) and downloaded the celebA dataset. There is still a bug somewhere and I will have a look at it tomorrow. So it is actually ""running"" but producing nonsense.",gist still bug somewhere look tomorrow actually running nonsense,issue,negative,neutral,neutral,neutral,neutral,neutral
295699847,"Isn't the novelty of their work just adding a new update-op
https://gist.github.com/PatWie/47ca4a676698952536d4e9bc07a4f49d#file-began-py-L43
(this is not working, I have not the CelebA dataset here and should download it at some point.)

",novelty work new working point,issue,negative,positive,positive,positive,positive,positive
295650949,"In the distributed settings, the init_op is passed to the Supervisor-class. I did not find a way to separate the creation of the Session and the call of the init_op (for global variables). I updated the pull-request and close this issue to have only one place to discuss.",distributed find way separate creation session call global close issue one place discus,issue,negative,neutral,neutral,neutral,neutral,neutral
295649070,"I messed up the last commit and will rebase again.
edit: commit is rebase on the current HEAD",last commit rebase edit commit rebase current head,issue,positive,neutral,neutral,neutral,neutral,neutral
295648550,"Now, 
a few updates and findings on this:

- the global_step tensor has to be placed on a [specific device](https://github.com/tensorflow/tensorflow/blob/17c47804b86e340203d451125a721310033710f1/tensorflow/python/training/supervisor.py#L879-L883)
- sometimes TF needs the `global_step` op in the optimizer call. It is required to prevent a stale optimizer.
- there is an complex interplay between the MonitoredSession and the `tf.train.Supervisor` and I am not willing to re-implement most parts of the `Supervisor` class.
- it requires a lot of additional changes in terms of *where* to log and *when* to log or when to write a summary. It seems that the `tf.train.Supervisor` handles this by [knowing whether](https://github.com/tensorflow/tensorflow/blob/17c47804b86e340203d451125a721310033710f1/tensorflow/python/training/supervisor.py#L657-L667) the worker is a chief-worker or not.

For the current version I set the correct ip addresses in:
```python
MACHINE_Q = 'xxx.x.xx.xxx'  # noqa
MACHINE_G = 'xxx.x.xx.xxx'  # noqa
```

and then fire up the example by
```
python mnist-distributed.py --task_id 0 --job ps --gpu 0 # on MACHINE_Q
python mnist-distributed.py --task_id 0 --job worker --gpu 0 # on MACHINE_Q
python mnist-distributed.py --task_id 1 --job worker --gpu 0 # on MACHINE_G
```

The graph is building fine on all machines.  Although, I currently get an issue with the [monitored session](https://github.com/tensorflow/tensorflow/blob/52dcb2590bb9274262656c958c105cb5e5cc1300/tensorflow/python/training/monitored_session.py#L428).
```
AttributeError: 'DistributedTrainer' object has no attribute '_monitored_sess'
2017-04-20 11:04:53.496895: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:647] failed to record completion event; therefore, failed to create inter-stream dependency
2017-04-20 11:04:53.496958: I tensorflow/stream_executor/stream.cc:4125] stream 0x5c544a0 did not memcpy device-to-host; source: 0x330a3c0800
2017-04-20 11:04:53.496972: E tensorflow/stream_executor/stream.cc:289] Error recording event in stream: error recording CUDA event on stream 0x5c54570: CUDA_ERROR_DEINITIALIZED; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2017-04-20 11:04:53.496989: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED
2017-04-20 11:04:53.497000: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1
Aborted
```

It seems that the `sessioncreate.py` in this repo already should distinguish between chief-workers and non-chief-workers to make a distributed version to work. The relevant reference is in https://github.com/tensorflow/tensorflow/blob/52dcb2590bb9274262656c958c105cb5e5cc1300/tensorflow/python/training/monitored_session.py#L343

I ",tensor specific device sometimes need call prevent stale complex interplay willing supervisor class lot additional log log write summary knowing whether worker current version set correct python fire example python job python job worker python job worker graph building fine although currently get issue session object attribute record completion event therefore create dependency stream source error recording event stream error recording event stream marking stream bad event object may fault monitor error polling event status query event unexpected event status aborted already distinguish make distributed version work relevant reference,issue,negative,negative,neutral,neutral,negative,negative
295544284,"But we don't have to use the saving/restoring routines of it right?
I haven't tried running with supervisor so I'm not sure what part of it is necessary.",use right tried running supervisor sure part necessary,issue,negative,positive,positive,positive,positive,positive
295478084,"Usually we don't take ""example request"", but I'm interested in this one as well.",usually take example request interested one well,issue,positive,neutral,neutral,neutral,neutral,neutral
294963846,I was using an older version of tensorpack (dated Feb '17). Let me try the latest version and see. Thanks.,older version let try latest version see thanks,issue,negative,positive,positive,positive,positive,positive
294940441,How do you install tensorpack? Please use the latest version of tensorpack. (see README,install please use latest version see,issue,negative,positive,positive,positive,positive,positive
294934357,pip install tornado can solve the issue,pip install tornado solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
294546371,"As far as I know, it is currently not possible to specify the GPUs for each worker. That's why I used a dict instead.

My current version (I haven't pushed yet) just creates a ClusterTrainer relying on clusterSpec and uses all GPU from CUDA_VISIBLE_DEVICES like the default behavior of TF. But the code is currently not complete. I will post any updates this week if I have time to code.",far know currently possible specify worker used instead current version yet like default behavior code currently complete post week time code,issue,negative,positive,neutral,neutral,positive,positive
294518684,"Could you consider providing the clusterspec as a parameter to allow different mechanisms to initialize it?

This is because depending on the distributed system the most sensible manner to initialize the clusterspec will vary, as mentioned in https://github.com/ppwwyyxx/tensorpack/issues/193",could consider providing parameter allow different initialize depending distributed system sensible manner initialize vary,issue,negative,neutral,neutral,neutral,neutral,neutral
294260437,"The existing BSDS dataset yields data of specific shape and size and value range (see its [documentation](http://tensorpack.readthedocs.io/en/latest/modules/dataflow.dataset.html#tensorpack.dataflow.dataset.BSDS500)). So if your dataflow is different, the preprocessing may have to be different.",data specific shape size value range see documentation different may different,issue,negative,neutral,neutral,neutral,neutral,neutral
294259939,"No file format restrictions. But you'll need to write such function in python to yield your images and labels and change the existing `get_data` function to use your data and (maybe) your own preprocessing.
See the [documentation](http://tensorpack.readthedocs.io/en/latest/tutorial/extend/dataflow.html)",file format need write function python yield change function use data maybe see documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
294243490,"I don't remember what number you should get in the end, but the shape looks good.",remember number get end shape good,issue,negative,positive,positive,positive,positive,positive
294242809,"I was able to complete the training. In order to make sure that my training process went OK, can you please verify if the following graphs look reasonable (graphs below were generated using tensorboard)?

![cost_summary](https://cloud.githubusercontent.com/assets/21690396/25056859/69dd787a-2120-11e7-9cef-f5c8d7f4e4d4.PNG)

![val_precision_recall](https://cloud.githubusercontent.com/assets/21690396/25056885/987cdb76-2120-11e7-8e64-e7e6adcc5192.PNG)

![trainerror](https://cloud.githubusercontent.com/assets/21690396/25056939/f2410baa-2120-11e7-84b9-44597818ba2c.PNG)
",able complete training order make sure training process went please verify following look reasonable,issue,positive,positive,positive,positive,positive,positive
294199068,"`ClassificationError` by default looks for this tensor in the graph. Otherwise it doesn't know ""what error"" to compute (e.g. you can have many ""errors"" in your model).

For example [here](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py#L207) it reports two errors, and the two corresponding tensors are defined [here](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py#L117).

See the [documents of ClassificationError](http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.ClassificationError)",default tensor graph otherwise know error compute many model example two two corresponding defined see,issue,negative,positive,positive,positive,positive,positive
294192145,"hi @flashgm , I just implemented continuous a3c in pytorch [here](https://github.com/andrewliao11/pytorch-a3c-mujoco).
Looking forward to your comments",hi continuous looking forward,issue,negative,neutral,neutral,neutral,neutral,neutral
294061230,"It looks like after I updated my numpy, opencv and msg-pack numpy packages, it worked. I used the versions you listed and for opencv I used version 3.1.0
Thanks for your help !  
I will let you know if I run into any other issues.",like worked used listed used version thanks help let know run,issue,positive,positive,positive,positive,positive,positive
293338137,"Thanks for the note. I've checked the Example proto, it supports variable size, but it doesn't contain the meta data inside (the size and dtype). So it's still not an ideal way to transfer ""list of tensors"".

Also we tend to move away from protobuf because they are slow, and causing build issues.",thanks note checked example proto variable size contain meta data inside size still ideal way transfer list also tend move away slow causing build,issue,positive,positive,positive,positive,positive,positive
293187769,"We do most of our decoding in python (for now) but intend to migrate to using `[tf.train.Example](https://www.tensorflow.org/api_docs/python/tf/train/Example)` which support variable size tensors and are natively supported by tensorflow. 

Let me know if you want to collaborate on this issue. Would be great to hear your requirements and use cases.",python intend migrate support variable size natively let know want collaborate issue would great hear use,issue,positive,positive,positive,positive,positive,positive
293123023,"Huh, that's odd. It's easily reproducible for me and happens right from the first epoch. I'm running a version of TF I compiled, but not that recently. I'll switch to the latest RC in the next few days and see if it's fixed for me.",huh odd easily reproducible right first epoch running version recently switch latest next day see fixed,issue,negative,positive,positive,positive,positive,positive
293075779,"It looks better from the interface. But it'll introduce two copies. One from zmq to a string tensor, and one from the whole string tensor to decoded list of tensors.

If another queue is added after the decode, the latency of copy can be hidden.",better interface introduce two one string tensor one whole string tensor list another queue added decode latency copy hidden,issue,negative,positive,positive,positive,positive,positive
293073633,"Maybe a better design would be to implement two ops:
- zmq_receive
- msg_decode",maybe better design would implement two,issue,negative,positive,positive,positive,positive,positive
293055555,"btw, I cannot reproduce this in DQN either (at least not in the first epoch).
I'm using some nightly prebuilt version of TF I downloaded last week.",reproduce either least first epoch nightly version last week,issue,negative,negative,neutral,neutral,negative,negative
293042486,"We use standard TF checkpoint, but Keras has its own format if I recall it correctly, so definitely would need extra effort.

Tensorpack doesn't assume you're doing classification, or assume you want to print the error rate. So you need to write them with symbolic functions. Using keras functions would run as well but be less informative.",use standard format recall correctly definitely would need extra effort assume classification assume want print error rate need write symbolic would run well le informative,issue,negative,neutral,neutral,neutral,neutral,neutral
293040435,"I took a look and it seems some additional work may be required to save out the weights in Keras format?

Also do you think there may be a problem if Keras supplies the logits portion as well?",took look additional work may save format also think may problem portion well,issue,negative,neutral,neutral,neutral,neutral,neutral
293040263,I looks like there was an annoucement at the devSummit. I rarely see machines that handle 8GPUs and each machine could locally read from the SSD. So the bottleneck would be at some other point.,like rarely see handle machine could locally read bottleneck would point,issue,negative,positive,positive,positive,positive,positive
293040103,"If no dataflow makes modifications by default, then no dataflow needs to make an explicit copy for potential modification.
Otherwise __every__ ProxyDataFlow would have to make an explicit copy.",default need make explicit copy potential modification otherwise would make explicit copy,issue,negative,neutral,neutral,neutral,neutral,neutral
293039382,"Cool do you have a link to where they mention this?

Also, one advantage of distributed training is you can amortize io to ease feeding the GPUs. In my case I have access to many 2 GPU machines and zero 8 GPU machines.",cool link mention also one advantage distributed training amortize io ease feeding case access many zero,issue,positive,positive,positive,positive,positive,positive
293033270,"But I like the `tf.decode` part, such that you just need to send raw data. So it can directly send msg packed data.",like part need send raw data directly send data,issue,negative,negative,neutral,neutral,negative,negative
293029682,The augmentor now would copy by default. So that issue was fixed,would copy default issue fixed,issue,negative,positive,neutral,neutral,positive,positive
293028352,"Seems a good reference!
It requires known shape of tensor? That's a big limitation",good reference known shape tensor big limitation,issue,negative,positive,positive,positive,positive,positive
292774177,"Sorry. For imagenet-resnet.py it works only for evaluation.
Because for evaluation it has:
`        session_init=get_model_loader(model_file),`, which supports both format.
However for training it is `        config.session_init = SaverRestore(args.load)`, which only accepts saver checkpoint.
You can change the code for your need. Note that `get_model_loader` is very simple (stupid):
```python
    if filename.endswith('.npy'):
        assert os.path.isfile(filename), filename
        return ParamRestore(np.load(filename, encoding='latin1').item())
    else:
        return SaverRestore(filename)
```",sorry work evaluation evaluation format however training saver change code need note simple stupid python assert return else return,issue,negative,negative,negative,negative,negative,negative
292774058,Yes. Just the `--load` option. It works for both npy and the checkpoint.,yes load option work,issue,negative,neutral,neutral,neutral,neutral,neutral
292766754,"The conversion was done in get_data already.
Should add it in build_graph if train from scratch. Just change the mean/std constant.
Also #176 ",conversion done already add train scratch change constant also,issue,negative,neutral,neutral,neutral,neutral,neutral
292762294,"Thanks ! Sure I will let you know which package helps.
",thanks sure let know package,issue,positive,positive,positive,positive,positive,positive
292762188,"The shape and type are
```python
        return [InputDesc(tf.float32, [None, None, None, 3], 'image'),
                InputDesc(tf.int32, [None, None, None], 'edgemap')]
```
Where the first dimension (batch size) is set to 8 in the data pre-processing stage.

If an upgrade works for you, could you identify which package is causing the problem? so I can update the requirements.",shape type python return none none none none none none first dimension batch size set data stage upgrade work could identify package causing problem update,issue,negative,positive,positive,positive,positive,positive
292761810," Also can you please let me know the dimensions and data type of input data fed into feed_dict ?

",also please let know data type input data fed,issue,negative,neutral,neutral,neutral,neutral,neutral
292761375,"Thanks Yuxin ! I did print it but the value dp[0] was a dict object with raw bytes as keys and values but should instead be a numpy array. Is it possible that the decoding of the input data has some issues ?
Can you please let me know the versions of the following packages you are using ? I installed all these packages using conda install .- numpy- msgpack-Python - msgpack-numpy

Thanks,Ajay
Sent from Yahoo Mail for iPhone
",thanks print value object raw instead array possible input data please let know following install thanks sent yahoo mail,issue,positive,positive,neutral,neutral,positive,positive
292760398,"The model is unrelated here. I'd guess it's better to try a different version of numpy or tensorflow?

Others have seen this as well: http://stackoverflow.com/questions/43071987/tensorflow-typeerror-float-argument-must-be-a-string-or-a-number-not-dict",model unrelated guess better try different version seen well,issue,positive,positive,positive,positive,positive,positive
292760235," Will it be possible to share your Vgg16.npy file? I am just wondering if there are issues with converting the caffe model.
Thanks !

Sent from Yahoo Mail for iPhone

",possible share file wondering converting model thanks sent yahoo mail,issue,positive,positive,neutral,neutral,positive,positive
292742844,"Thanks for the pointer! 
But I don't know anywhere a Session would be created earlier. From that thread it looks like the config issue is quite complicated.",thanks pointer know anywhere session would thread like issue quite complicated,issue,positive,negative,negative,negative,negative,negative
292742300,"Aha, it seems this is a TF issue: https://github.com/tensorflow/tensorflow/issues/8136

GPU settings appear to be set on the first call to tf.Session or tf.Server. Thus, simply making a new session before anything else runs and setting the correct GPU memory limit fixes my problem. I haven't yet figured out where the earlier call to tf.Session or tf.Server is though.",aha issue appear set first call thus simply making new session anything else setting correct memory limit problem yet figured call though,issue,negative,positive,positive,positive,positive,positive
292731686,"Which document is it? I thought the dynamic-loader design was just aimed to allow people to put ops anywhere.

There is still a small protobuf dependency (only used for data type and shape), and I don't like it either.
* I'll see if TF is willing to export the symbols otherwise it'll be very hard to build.
* Only dozens of bytes of protobuf is used, but it consumes about 1/4 of the runtime.

Since it's only data type and shape, I will definitely try a new format (probably a custom one to avoid more dependencies).",document thought design allow people put anywhere still small dependency used data type shape like either see willing export otherwise hard build used since data type shape definitely try new format probably custom one avoid,issue,positive,negative,neutral,neutral,negative,negative
292730755,"Thanks for looking at this!
Frankly I haven't looked at tensorflow distributed training in depth yet. Because I found now it's still hard to feed 8 GPUs on one machine, let alone multiple machines. That's why I'm working on data recently.

btw, TF say they'll publish a ""good"" example code of distributed training very soon, with the use of GPU StageArea. It'll include the best practice so I guess we can refer to that (rather than finding it out by ourselves) when it come out.",thanks looking frankly distributed training depth yet found still hard feed one machine let alone multiple working data recently say publish good example code distributed training soon use include best practice guess refer rather finding come,issue,positive,positive,positive,positive,positive,positive
292722121,"Ok, just tried on cifar and that seems to work while DQN (still) doesn't. Hopefully later today I can figure out what's different about the two that's causing this.",tried work still hopefully later today figure different two causing,issue,negative,neutral,neutral,neutral,neutral,neutral
292716719,"I strongly argue against sending Tensor-Protos over ZeroMQ. I tried to create a C++ example, but the documentation of TensorFlow clearly says, that they want you to put custom C++ code in the TF directory. I vote for using msgpack directly with a custom dataformat to further reduce the overall requirements when sending from C++. This would remove the TensorFlow-dependency.

edit:
https://github.com/cjweeks/tensorflow-cmake seems to be an alternative but need a compilation from source.",strongly argue sending tried create example documentation clearly want put custom code directory vote directly custom reduce overall sending would remove edit alternative need compilation source,issue,positive,positive,positive,positive,positive,positive
292710622,"It seems that this requires more changes than I expected. Currently, I am not sure how the incoming data can be handle in the ""tensorpack"" way.

But I managed to produce a minimal working example
https://gist.github.com/PatWie/89950d3f8491a9f0d84125dca0945afa

which I actually tested on the following settings:
- machineA: ps + worker, machineB: worker
- machineA: ps + worker + worker

I will try to add a ""distributed.py"" to the trainer section.",currently sure incoming data handle way produce minimal working example actually tested following worker worker worker worker try add trainer section,issue,negative,positive,neutral,neutral,positive,positive
292682142,"Does it work with DQN? I'm wondering if it's not getting passed all the way
through the trainers.

On Apr 7, 2017 20:29, ""Yuxin Wu"" <notifications@github.com> wrote:

> It works for me.
>
> diff --git i/examples/cifar-convnet.py w/examples/cifar-convnet.py
> index a8e18dc..8761bb2 100755
> --- i/examples/cifar-convnet.py
> +++ w/examples/cifar-convnet.py
> @@ -10,6 +10,7 @@ import os
>
>  import tensorpack.tfutils.symbolic_functions as symbf
>  from tensorpack.tfutils.summary import *
> +from tensorpack.tfutils.sesscreate import *
>  from tensorpack.utils.gpu import get_nr_gpu
>
>  """"""
> @@ -114,6 +115,13 @@ def get_config(cifar_classnum):
>      dataset_train = get_data('train', cifar_classnum)
>      dataset_test = get_data('test', cifar_classnum)
>
> +    tf_conf = tf.ConfigProto()
> +    tf_conf.gpu_options.per_process_gpu_memory_fraction = 0.05
> +    tf_conf.allow_soft_placement = True
> +    session_creator = NewSessionCreator(config=tf_conf)
> +
> +
>      def lr_func(lr):
>          if lr < 3e-5:
>              raise StopTraining()
> @@ -127,6 +135,7 @@ def get_config(cifar_classnum):
>              StatMonitorParamSetter('learning_rate', 'val_error', lr_func,
>                                     threshold=0.001, last_k=10),
>          ],
> +        session_creator=session_creator,
>          max_epoch=150,
>      )
>
> Then I can see it consumes only a little memory
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/220#issuecomment-292681287>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAyIwjiTYh5AKv8WS7rHBHcMcanyNvGLks5rttTTgaJpZM4M20HX>
> .
>
",work wondering getting way wrote work git index import o import import import import true raise see little memory thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
292681287,"It works for me. After applying your patch on a different example I can see the memory change.

<details>

```
diff --git i/examples/cifar-convnet.py w/examples/cifar-convnet.py
index a8e18dc..8761bb2 100755
--- i/examples/cifar-convnet.py
+++ w/examples/cifar-convnet.py
@@ -10,6 +10,7 @@ import os

 import tensorpack.tfutils.symbolic_functions as symbf
 from tensorpack.tfutils.summary import *
+from tensorpack.tfutils.sesscreate import *
 from tensorpack.utils.gpu import get_nr_gpu

 """"""
@@ -114,6 +115,13 @@ def get_config(cifar_classnum):
     dataset_train = get_data('train', cifar_classnum)
     dataset_test = get_data('test', cifar_classnum)

+    tf_conf = tf.ConfigProto()
+    tf_conf.gpu_options.per_process_gpu_memory_fraction = 0.05
+    tf_conf.allow_soft_placement = True
+    session_creator = NewSessionCreator(config=tf_conf)
+
+
     def lr_func(lr):
         if lr < 3e-5:
             raise StopTraining()
@@ -127,6 +135,7 @@ def get_config(cifar_classnum):
             StatMonitorParamSetter('learning_rate', 'val_error', lr_func,
                                    threshold=0.001, last_k=10),
         ],
+        session_creator=session_creator,
         max_epoch=150,
     )
```
</details>
",work patch different example see memory change git index import o import import import import true raise,issue,negative,positive,positive,positive,positive,positive
292680605,"The config should get passed to tensorflow successfully. This might be a TF issue or the config is not enough to make it control the memory usage.
The allow_growth option in the config may help. ",get successfully might issue enough make control memory usage option may help,issue,positive,positive,positive,positive,positive,positive
292679165,"Here's a tiny patch that shows what I'm trying to do. Running DQN.py with this patch I still see almost all of the memory on my GPU being reserved.

[code.txt](https://github.com/ppwwyyxx/tensorpack/files/907094/code.txt)
",tiny patch trying running patch still see almost memory reserved,issue,negative,neutral,neutral,neutral,neutral,neutral
292654997,"Just a reminder:

As already discussed this only works with the correct protobuf version. When installing TF from source it is easy to find the correct protobuf version in
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L309

Then
```bash
wget https://github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz
cd protobuf-2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a

replace_by_sed() {   local regex=""${1}"";   shift;   if echo ""${OSTYPE}"" | grep -q darwin; then     sed -i '' -e ""${regex}"" ""$@"";   else     sed -i -e ""${regex}"" ""$@"";   fi; }
replace_by_sed 's#https://googlemock.googlecode.com/files/gmock-1.7.0.zip#http://download.tensorflow.org/deps/gmock-1.7.0.zip#'   ""autogen.sh""

./autogen.sh
./configure --prefix=$HOME/protobuf

make -j 4
make install

export LD_LIBRARY_PATH=$HOME/protobuf/lib:$LD_LIBRARY_PATH
export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$HOME/protobuf/lib/pkgconfig
```",reminder already work correct version source easy find correct version bash local shift echo else fi make make install export export,issue,negative,positive,positive,positive,positive,positive
292629324,You definitely need Gitter or Slack. I will try that.,definitely need slack try,issue,negative,neutral,neutral,neutral,neutral,neutral
292629171,"Could you confirm `pkg-config --cflags --libs libzmq` produce your CXXFLAGS after setting PKG_CONFIG_PATH?

Also, the Makefile at first didn't include libzmq in `LIBS`. If that's the case you may need to pull.",could confirm produce setting also first include case may need pull,issue,negative,positive,positive,positive,positive,positive
292628676,"I have tested it and I have to add it. I currently try to create a `bazel` build file. [Here](https://github.com/PatWie/CppNumericalSolvers) I managed to let bazel download all dependencies automatically.

Creating an userop at a different place then inside the TF repo is probably not the common use-case. I tried this some time ago without success and today also add a question to [stackoverflow](http://stackoverflow.com/questions/43280922/build-custom-operation-for-tensorflow-with-bazel-in-a-separate-directory)

",tested add currently try create build file let automatically different place inside probably common tried time ago without success today also add question,issue,negative,neutral,neutral,neutral,neutral,neutral
292628326,"As long as you did `export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$HOME/zeromq/lib/pkgconfig/`, I suppose you don't need `CXXFLAGS += -I/home/patwie/zeromq/include/ -L/home/patwie/zeromq/lib/ -lzmq`.",long export suppose need,issue,negative,negative,neutral,neutral,negative,negative
292625939,"After the following commands

```
git clone https://github.com/zeromq/libzmq
cd libzmq
./configure --prefix=$HOME/zeromq
make
make install

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/zeromq/include/
export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$HOME/zeromq/lib/pkgconfig/
wget -O $HOME/zeromq/include/zmq.hpp https://raw.githubusercontent.com/zeromq/cppzmq/master/zmq.hpp

```

and explicitly adding
```
CXXFLAGS += -I/home/patwie/zeromq/include/ -L/home/patwie/zeromq/lib/ -lzmq
```

I get at least linking errors from tensorflow.",following git clone make make install export export explicitly get least linking,issue,negative,negative,negative,negative,negative,negative
292615913,"It looks like my package manager automatically includes zmq.hpp for me. (https://git.archlinux.org/svntogit/community.git/tree/trunk/PKGBUILD?h=packages/zeromq)
And none of the other packaging system seems to do this. Yay archlinux..",like package manager automatically none system,issue,negative,neutral,neutral,neutral,neutral,neutral
292575325,I'm using 4.2.1-1.  cppzmq only contains the cpp header but maybe the c header is incompatible.,header maybe header incompatible,issue,negative,neutral,neutral,neutral,neutral,neutral
292574764,Could you post the relevant code?,could post relevant code,issue,negative,positive,positive,positive,positive,positive
292255874,Agreed. Amazing how fast the GAN research has been in recent months. This repo has some of the best most flexible GAN implementations out there. Keep up the good work. 👍 ,agreed amazing fast gan research recent best flexible gan keep good work,issue,positive,positive,positive,positive,positive,positive
292082149,"Yes, I just wanted to let you know. 
It worked fine with larger batch sizes! Thanks.",yes let know worked fine batch size thanks,issue,positive,positive,positive,positive,positive,positive
292077376,"```
3c3
< # File: imagenet-resnet-fcneval.py
---
> # File: imagenet-resnet.py
31c31
<         return [InputDesc(tf.float32, [None, None, None, 3], 'input'),
---
>         return [InputDesc(tf.uint8, [None, INPUT_SHAPE, INPUT_SHAPE, 3], 'input'),
101c101
<                 argscope([Conv2D, MaxPooling, AvgPooling, BatchNorm], data_format=self.data_format):
---
>                 argscope([Conv2D, MaxPooling, GlobalAvgPooling, BatchNorm], data_format=self.data_format):
110,112c110,114
<                       .AvgPooling('gap', shape=7, stride=1)
<                       .Conv2D('linear', 1000, 1, nl=tf.identity, use_bias=True)())
<         logits = tf.reduce_mean(logits, [2, 3], name='logits')
---
>                       .GlobalAvgPooling('gap')
>                       .FullyConnected('linear', 1000, nl=tf.identity)())
> 
>         loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
>         loss = tf.reduce_mean(loss, name='xentropy-loss')
119a122,125
>         wd_cost = regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss')
>         add_moving_summary(loss, wd_cost)
>         self.cost = tf.add_n([loss, wd_cost], name='cost')
> 
125c131,133
< def get_data(train_or_test):
---
> def get_data(train_or_test, fake=False):
>     if fake:
>         return FakeData([[64, 224, 224, 3], [64]], 1000, random=False, dtype='uint8')
127c135
<     assert not isTrain
---
> 
130,143c138,192
<                           shuffle=True if isTrain else False, dir_structure='train')
< 
<     old_aug = imgaug.AugmentorList([
<         imgaug.ResizeShortestEdge(256),
<         imgaug.CenterCrop((224, 224))])
< 
<     class ResizeAug(imgaug.ImageAugmentor):
<         def _augment(self, img, _):
<             if min(img.shape[0], img.shape[1]) >= INPUT_SHAPE:
<                 return img
<             else:
<                 return old_aug.augment(img)
<     ds = AugmentImageComponent(ds, [ResizeAug()], copy=False)
<     ds = BatchData(ds, 1, remainder=not isTrain)
---
>                           shuffle=True if isTrain else False, dir_structure='original')
>     if isTrain:
>         class Resize(imgaug.ImageAugmentor):
>             """"""
>             crop 8%~100% of the original image
>             See `Going Deeper with Convolutions` by Google.
>             """"""
>             def _augment(self, img, _):
>                 h, w = img.shape[:2]
>                 area = h * w
>                 for _ in range(10):
>                     targetArea = self.rng.uniform(0.08, 1.0) * area
>                     aspectR = self.rng.uniform(0.75, 1.333)
>                     ww = int(np.sqrt(targetArea * aspectR))
>                     hh = int(np.sqrt(targetArea / aspectR))
>                     if self.rng.uniform() < 0.5:
>                         ww, hh = hh, ww
>                     if hh <= h and ww <= w:
>                         x1 = 0 if w == ww else self.rng.randint(0, w - ww)
>                         y1 = 0 if h == hh else self.rng.randint(0, h - hh)
>                         out = img[y1:y1 + hh, x1:x1 + ww]
>                         out = cv2.resize(out, (224, 224), interpolation=cv2.INTER_CUBIC)
>                         return out
>                 out = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)
>                 return out
> 
>         augmentors = [
>             Resize(),
>             imgaug.RandomOrderAug(
>                 [imgaug.Brightness(30, clip=False),
>                  imgaug.Contrast((0.8, 1.2), clip=False),
>                  imgaug.Saturation(0.4),
>                  # rgb-bgr conversion
>                  imgaug.Lighting(0.1,
>                                  eigval=[0.2175, 0.0188, 0.0045][::-1],
>                                  eigvec=np.array(
>                                      [[-0.5675, 0.7192, 0.4009],
>                                       [-0.5808, -0.0045, -0.8140],
>                                       [-0.5836, -0.6948, 0.4203]],
>                                      dtype='float32')[::-1, ::-1]
>                                  )]),
>             imgaug.Clip(),
>             imgaug.Flip(horiz=True),
>             imgaug.ToUint8()
>         ]
>     else:
>         augmentors = [
>             imgaug.ResizeShortestEdge(256),
>             imgaug.CenterCrop((224, 224)),
>             imgaug.ToUint8()
>         ]
>     ds = AugmentImageComponent(ds, augmentors, copy=False)
>     if isTrain:
>         ds = PrefetchDataZMQ(ds, min(20, multiprocessing.cpu_count()))
>     ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)
```
And of course the data needs to preprocessed differently.
Because in tensorflow, weights for convolution and fc happen to have the same layout, so the above code can evaluate the pre-trained ResNet model directly (and get a warning about reshape).",file file return none none none return none loss loss loss loss loss fake return assert else false class self min return else return else false class resize crop original image see going self area range area else else return return resize conversion else min course data need differently convolution happen layout code evaluate model directly get warning reshape,issue,negative,negative,negative,negative,negative,negative
292074113,You just need to modify the `Model` a little bit.,need modify model little bit,issue,negative,negative,negative,negative,negative,negative
291820588,"You're right. But it's fake anyway.. so it doesn't really matter to me.
This option works as a sanity check, but 256 could be just too large to run. You can change it for your need if you want something else for benchmark.",right fake anyway really matter option work sanity check could large run change need want something else,issue,negative,positive,neutral,neutral,positive,positive
291716418,"@PatWie : I am using the image2image translation for debugging the segmentation problem. 
However, I met the issue such that discriminator curve and generator curve are not going as expected. 
Once it is done, I will make a pull request :)

",translation segmentation problem however met issue discriminator curve generator curve going done make pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
291616500,"Ah, I should update my local copy. Sorry.",ah update local copy sorry,issue,negative,negative,negative,negative,negative,negative
291615735,"JoinData is already like that.
And this is also how data is read in the DiscoGAN example.",already like also data read example,issue,negative,neutral,neutral,neutral,neutral,neutral
291601226,"I vote for adding this to the library. 
@tmquan 
Maybe the best way is to add an example and define it directly there. ",vote library maybe best way add example define directly,issue,positive,positive,positive,positive,positive,positive
291504320,"ElasticDeform is quite common in biomedical image processing, especially in segmentation. 
If you look into the original U-Net paper, the authors also leveraged it intensively. 

It is good to know that the main library should be kept small and common :)
I will close this issue for other people would like to perform their own custom augmentations. 

Bests, ",quite common image especially segmentation look original paper also intensively good know main library kept small common close issue people would like perform custom,issue,positive,positive,neutral,neutral,positive,positive
291499306,"Thanks! The results look interesting. I should try it some day.
Please note that, I added models or augmentors only when they are very common (or sometimes for my personal convenient). I found everyone likes to do augmentation very differently, but then it's very hard for me to maintain all the pieces.
The nice thing is that you can just import and use your augmentor without adding it into tensorpack. I also had a bunch of private code with different ideas I tried, and there are also several augmentors in `examples/` but not in the library. Because I'd rather keep the main library small and only include the extensions as examples.",thanks look interesting try day please note added common sometimes personal convenient found everyone augmentation differently hard maintain nice thing import use without also bunch private code different tried also several library rather keep main library small include,issue,positive,positive,neutral,neutral,positive,positive
291411292,"Hi @ppwwyyxx ,

I managed to complete the implementation ElasticDeformation augmentation. 
It takes a random vector field prod(range(8, 16)) and truncates its boundary values. 
Deformation rate can be controlled via this range. 
Next, it performs warping the original image with this field. 

It would be great if you can include in the next release.

Bests

```Python
class ElasticDeform:
    def __init__(self):
        pass
    
    def reset_state(self):
        self.rng = get_rng(self)
        
    def _augment(self, img, param):
        assert img.ndim in [2, 3], img.ndim
        du, dv = param
        shape = img.shape
        
        DU = cv2.resize(du, (shape[-2], shape[-2])) 
        DV = cv2.resize(dv, (shape[-2], shape[-2])) 
        X, Y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))
        indices = np.reshape(Y+DV, (-1, 1)), np.reshape(X+DU, (-1, 1))
        
        flow = img.copy()
        from scipy.ndimage.interpolation 	import map_coordinates
        for k in range(3):
            tmp = map_coordinates(img[...,k], indices, order=1)
            flow[...,k] = tmp.reshape(shape[0:2])
        flow = flow.reshape(shape)
        return flow
    def _get_augment_params(self, d):
        """"""
        get the augmentor parameters
        """"""
        size = self.rng.choice(range(8,16)) #8
        ampl = self.rng.choice(range(8,16)) #8
        du = self.rng.uniform(-ampl, ampl, size=(size, size))
        dv = self.rng.uniform(-ampl, ampl, size=(size, size))    
        
        # Dont distort at the boundaries
        du[ 0,:] = 0; du[-1,:] = 0; du[:, 0] = 0; du[:,-1] = 0;
        dv[ 0,:] = 0; dv[-1,:] = 0; dv[:, 0] = 0; dv[:,-1] = 0;
        return du, dv
    def _augment_return_params(self, d):
        """"""
        Augment the image and return both image and params
        """"""
        prms = self._get_augment_params(d)
        return (self._augment(d, prms), prms)
```

```python
import glob
imgs = glob.glob(os.path.join('data/std/', 'lena.png'))
ds = ImageFromFile(imgs, channel=3, shuffle=False)
augmentors = [
    ElasticDeform(), 
    imgaug.ColorSpace(mode=cv2.COLOR_RGB2BGR),
]
ds = AugmentImageComponent(ds, augmentors)
ds = PrintData(ds, num=2) # only for debugging

gen = ds.get_data()
for dp in gen:
    newImg = np.array(dp)
    
    a = np.squeeze(cv2.imread(imgs[0]))
    b = np.squeeze(newImg)
    c = np.abs(a-b)
    concat = np.concatenate((a,b,c), axis=-2)
    concat = concat.astype(np.uint8)
    print concat.shape
    plt.figure(figsize=(20, 60))
    plt.imshow(concat, cmap=plt.cm.gray)
    plt.axis('off')
    plt.show()
```
![lena](https://cloud.githubusercontent.com/assets/4506781/24644469/ada2c922-194e-11e7-9aa0-ec7f7aa3c1cf.png)
",hi complete implementation augmentation random vector field prod range boundary deformation rate via range next warping original image field would great include next release python class self pas self self self param assert param shape shape shape shape shape shape shape index flow import range index flow shape flow shape return flow self get size range range size size size size dont distort return self augment image return image return python import gen gen print,issue,positive,positive,positive,positive,positive,positive
291184298,"```python
class MyAug:
   def _get_augment_params(self, img):
        return any params that have to be kept same for both input and output
        # any random number has to be generated with self.rng
    def _augment(self, img, params):
        return new_img using img and params

df = MyDataFlow() # produce [input, output]
df = AugmentImageComponents(df, [MyAug()], (0,1))
```",python class self return kept input output random number self return produce input output,issue,negative,negative,negative,negative,negative,negative
291183055,"Hi @ppwwyyxx , 

I would like to revisit this one and am working on the image segmentation. 
Specifically, I would like to implement a custom function on image augmentation which is so-called Elastic Deformation since the Gaussian Deformation is not robust to my problem and it requires the higher accuracy. 

In Elastic Deformation, both 3-channel input and output images need to used the *same* seed number to generate the same vector flow from numpy.
Is it straightforward to add such this customization? An example of interface could be helpful a lot. 

Thanks so much. 
",hi would like revisit one working image segmentation specifically would like implement custom function image augmentation elastic deformation since deformation robust problem higher accuracy elastic deformation input output need used seed number generate vector flow straightforward add example interface could helpful lot thanks much,issue,positive,positive,positive,positive,positive,positive
290968553,"@ppwwyyxx 
oh, the dataset I used is just a small one
[TIMIT_speech_database.zip](https://github.com/ppwwyyxx/tensorpack/files/888214/TIMIT_speech_database.zip)
",oh used small one,issue,negative,negative,negative,negative,negative,negative
290967818,"```
Found 111 entries in train.mdb
Found 11 entries in test.mdb
```
They shouldn't be only contain 111 and 11 elements, if that's what you're really using.
The files are very large I cannot upload them.",found found contain really large,issue,negative,positive,positive,positive,positive,positive
290967735,"@ppwwyyxx 
so it looks like my print contents were not correct, i think there may be somgthing wrong in my train.mdb and test.mdb files

would you mind uploading your .mdb files? i'll be very thankful for your help",like print content correct think may wrong would mind thankful help,issue,positive,negative,negative,negative,negative,negative
290967365,"It is named ""error"" https://github.com/ppwwyyxx/tensorpack/blob/6ba19a977509e911daaea2367cd0aaa62936b37e/examples/CTC-TIMIT/train-timit.py#L75

and get printed every epoch, on both training and validation set.",error get printed every epoch training validation set,issue,negative,neutral,neutral,neutral,neutral,neutral
290967298,"@ppwwyyxx 

https://github.com/ppwwyyxx/tensorpack/tree/4a30d18dd9512319ddbe2e7f1503abe45e1e80cb/examples/CTC-TIMIT
""
Results:

Get 0.28 LER (normalized edit distance) after about 40 epochs.
""
but i didn't find this ""normalized edit distance""",get edit distance find edit distance,issue,negative,neutral,neutral,neutral,neutral,neutral
290966094,"Thanks for finding it out. It looks like tensorflow changed their API at some moment.

What do you mean ""find the edit distance""",thanks finding like moment mean find edit distance,issue,positive,negative,neutral,neutral,negative,negative
290965502,"@ppwwyyxx thank you

and I also changed ""loss = tf.nn.ctc_loss(logits, label, seqlen, time_major=False)"" to ""loss = tf.nn.ctc_loss( label, logits, seqlen, time_major=False)"" in train_timit.py  ----  I found  that ""label"" should be the first arg ,because in ctc_ops.py, the function is defined as : 

def ctc_loss(labels, inputs, sequence_length,
             preprocess_collapse_repeated=False,
             ctc_merge_repeated=True, time_major=True):

-----------------------------------------------------------------------------------------

and now I ran ""./train-timit.py --train train.mdb --test test.mdb --stat stats.data"", it printed :

[0402 13:01:56 @logger.py:69] Argv: ./train-timit.py --train train.mdb --test test.mdb --stat stats.data
[0402 13:01:56 @format.py:82] Found 111 entries in train.mdb
[0402 13:01:57 @format.py:82] Found 11 entries in test.mdb
[0402 13:01:57 @param.py:190] Use train_log/train-timit/hyper.txt to control hyperparam learning_rate.
[0402 13:01:57 @common.py:94] fc input: [None, 128]
[0402 13:01:57 @common.py:102] fc output: [None, 62]

----------------------------------------------
is that right? where should i find the ""edit distance""?
",thank also loss label loss label found label first function defined ran train test printed train test found found use control input none output none right find edit distance,issue,negative,positive,positive,positive,positive,positive
290921858,"@ppwwyyxx 

when i ran ""./create-lmdb.py stat --db train.mdb"", it printed:

Traceback (most recent call last):
  File ""./create-lmdb.py"", line 136, in <module>
    compute_mean_std(args.db, args.output)
  File ""./create-lmdb.py"", line 105, in compute_mean_std
    ds = LMDBDataPoint(db, shuffle=False)
TypeError: __init__() got an unexpected keyword argument 'shuffle'

--------------------------------------------------------------



so i changed ""ds = LMDBDataPoint(db, shuffle=False)"" --> ""ds = LMDBDataPoint(db)"" (create-lmdb.py), then run ""./create-lmdb.py stat --db train.mdb"" again, but now it print:

Traceback (most recent call last):
  File ""./create-lmdb.py"", line 137, in <module>
    compute_mean_std(args.db, args.output)
  File ""./create-lmdb.py"", line 108, in compute_mean_std
    with get_tqdm(total=ds.size()) as bar:
  File ""/usr/local/lib/python2.7/dist-packages/tensorpack/dataflow/base.py"", line 73, in size
    return self.ds.size()
AttributeError: 'str' object has no attribute 'size'



=========================================

i dont know what's wrong with it



",ran printed recent call last file line module file line got unexpected argument run print recent call last file line module file line bar file line size return object attribute dont know wrong,issue,negative,negative,neutral,neutral,negative,negative
290916606,"Cannot reproduce the problem.
This is usually related to environment and setup. You may see more details through some local debugging and figure out which part is going wrong.",reproduce problem usually related environment setup may see local figure part going wrong,issue,negative,negative,negative,negative,negative,negative
290896069,"Get it.
So in the example case, I just need one `AugmentImageComponents` for geometry and one `AugmentImageComponent` for imgproc.",get example case need one geometry one,issue,negative,neutral,neutral,neutral,neutral,neutral
290849442,"Removed fake_ilsvrc12, doesn't seem necessary? 
Thanks very much!",removed seem necessary thanks much,issue,negative,positive,positive,positive,positive,positive
290648119,"After several edits, I was able to convince travis-ci to let this pull-request pass the tests. I changed some lines in the resnet example:
- add argument for data_format (for training)
- add argument for faking data

You probably want to squash and merge these changes.",several able convince let pas example add argument training add argument data probably want squash merge,issue,negative,positive,positive,positive,positive,positive
290452937,"TF0.8 was like a year ago (around last April). You can checkout to tensorpack a year ago to see how things work. But it's very likely that either tensorflow or tensorpack at that time don't have the utilities to train timit.
Compared to that it would be much easier to just use 1.0",like year ago around last year ago see work likely either time train would much easier use,issue,positive,positive,neutral,neutral,positive,positive
289871912,I think it's OK. There is already a line of FakeData because I use it for testing as well.,think already line use testing well,issue,negative,neutral,neutral,neutral,neutral,neutral
289802922,"I don't understand what exactly you want to do here. But you need to implement ""how to load the data"" yourself anyway, so you can do whatever to your data.",understand exactly want need implement load data anyway whatever data,issue,negative,positive,positive,positive,positive,positive
289759298,"I currently thinking about using FakeData by a flag in the ResNet example, which would simplify this testing.",currently thinking flag example would simplify testing,issue,negative,neutral,neutral,neutral,neutral,neutral
289617749,@ppwwyyxx Thanks! The debug hook works fine.,thanks hook work fine,issue,positive,positive,positive,positive,positive,positive
289578113,"Seems that a session.run call of a debug session cannot be run simultaneously in two threads. (which makes sense given what it did).
 But if using TF queues, then thread is necessary. It'll be hard to get around this limitation.

Apart from using SimpleTrainer, you can also just use the debug hook I mentioned [last time](https://github.com/ppwwyyxx/tensorpack/issues/191#issuecomment-287240204). It should be less error-prone than injecting a debug session and make every ops under inspection.",call session run simultaneously two sense given thread necessary hard get around limitation apart also use hook last time le session make every inspection,issue,negative,negative,neutral,neutral,negative,negative
289538713,"Oops. My mistake. Just realized that tf.sign outputs {-1, 0, 1} already.
Then applying the binary weight quantization on both weights & activations should make some kind of XNOR-Net. But still some effort are needed to make gradient correct.",mistake already binary weight quantization make kind still effort make gradient correct,issue,negative,positive,positive,positive,positive,positive
289532224,"It seems to be a limitation of tfdbg that it cannot work very well with queues.
Switching to SimpleTrainer might have tfdbg to work, since you won't care the speed anyway.
I'll do some more investigation later.",limitation work well switching might work since wo care speed anyway investigation later,issue,negative,neutral,neutral,neutral,neutral,neutral
289522782,"This is just a question of how to implement XNOR-Net in tensorflow. You'll have more supports from tensorflow communities.

Basically just first quantize things to 0 and 1 (with tf.sign), and transform it to -1 and 1 (with x * 2 - 1). And use tf.stop_gradient trick to make the backward correct.",question implement basically first quantize transform use trick make backward correct,issue,negative,positive,positive,positive,positive,positive
289521323,"Btw, resnet example now uses NCHW, which is not supported by TensorFlow on CPU. You might encounter this problem later..",example might encounter problem later,issue,negative,neutral,neutral,neutral,neutral,neutral
289520466,"The default dataset directory for `ILSVRCMeta` (containing train.txt, etc) is `ilsvrc_metadata`, not `ilsvrc`.
But you can pass `meta_dir` argument to `ILSVRC()` as well. It is the full path to the directory.",default directory pas argument well full path directory,issue,negative,positive,positive,positive,positive,positive
289483022,@PatWie Yes that's my first thought as well and the interface would be very simple. But since tf.Tensor doesn't really have such attributes this may not be a safe option. ,yes first thought well interface would simple since really may safe option,issue,positive,positive,positive,positive,positive,positive
289438587,"The ResNet problem was the missing protobuf-apt package.

@ppwwyyxx Can you have a look at some point. It seems that the ilsvrc-class reads from a different file the paths of the imagenet data. I am trying to fake a very small amount of image files on the fly and update the ""train.txt"" and ""val.txt"" files to read only these 4 images.

However, I get
https://travis-ci.org/ppwwyyxx/tensorpack/jobs/215508412#L356

```
 File ""/home/travis/virtualenv/python2.7.12/lib/python2.7/site-packages/tensorpack/dataflow/dataset/ilsvrc.py"", line 184, in get_data
              assert im is not None, fname
          AssertionError: ilsvrc/train/n02481823/n02481823_2939.JPEG
```

This is surprising, as there is not such a line 'ilsvrc/train/n02481823/n02481823_2939.JPEG' in train.txt",problem missing package look point different file data trying fake small amount image fly update read however get file line assert none surprising line,issue,negative,positive,neutral,neutral,positive,positive
289420563,In #203 I was thinking about making copy=True the default. Maybe I should really do that since users can easily meet this problem..,thinking making default maybe really since easily meet problem,issue,negative,positive,positive,positive,positive,positive
289416960,"You're right! My mistake. The copy option was not working.

@msbauer 
Augmentor may modify your datapoints for efficiency. In your case you yield references to the same datapoints, so the modification to dp will be visible. You can either yield `copy.deepcopy(dp)`, or use the `copy=True` option in the augmentor.",right mistake copy option working may modify efficiency case yield modification visible either yield use option,issue,negative,positive,positive,positive,positive,positive
289411091,"@ppwwyyxx I wonder why
`ds = AugmentImageComponents(ds, [imgaug.RandomCrop((4, 4))], index=[0], copy=True)`
from the latest version does not work here.

I think it should be:
````
copy_func = copy_mod.deepcopy if copy else lambda x: x
````

edit: You also want to copy the entire dp instead of the components:

see #208",wonder latest version work think copy else lambda edit also want copy entire instead see,issue,negative,positive,positive,positive,positive,positive
289409508,"What about:
````python
from tensorpack import *
import copy

class RepeatedDataPoint(ProxyDataFlow):

    """""" Take data points from another DataFlow and produce them a certain number of times
    dp1, ..., dp1, dp2, ..., dp2, ...
    """"""

    def __init__(self, ds, nr):
        """"""
        Args:
            ds (DataFlow): input DataFlow
            nr (int): number of times to repeat each datapoint.
        """"""
        self.nr = nr
        super(RepeatedDataPoint, self).__init__(ds)

    def size(self):
        """"""
        Raises:
            :class:`ValueError` when nr == -1.
        """"""
        return self.ds.size() * self.nr

    def get_data(self):
        for dp in self.ds.get_data():
            for _ in range(self.nr):
                yield copy.deepcopy(dp)


ds = FakeData([(10, 10, 3)], random=False)
ds = RepeatedDataPoint(ds, nr=5)
ds = AugmentImageComponents(ds, [imgaug.RandomCrop((4, 4))], index=[0])
ds = PrintData(ds, num=5)
````

The issue is that tensorpack overrides the input in AugmentImageComponent[s]. Therefore, after the first crop the next cropping is applied to an input which is already cropped. You can check this
by
```python
def get_data(self):
    for dp in self.ds.get_data():
        for _ in range(self.nr):
            print""input is"", dp[0].shape
            yield dp
```
which gives
```
input is (10, 10, 3)
input is (4, 4, 3)
input is (4, 4, 3)
input is (4, 4, 3)
input is (4, 4, 3)
[0327 12:12:31 @common.py:655] DataFlow Info:
datapoint 0<5 with 1 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0190, 0.9341]
datapoint 1<5 with 1 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0190, 0.9341]
datapoint 2<5 with 1 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0190, 0.9341]
datapoint 3<5 with 1 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0190, 0.9341]
datapoint 4<5 with 1 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0190, 0.9341]

```
vs
```python
def get_data(self):
    for dp in self.ds.get_data():
        for _ in range(self.nr):
            print""input is"", dp[0].shape
            yield copy.deepcopy(dp)
```
which gives
```
input is (10, 10, 3)
input is (10, 10, 3)
input is (10, 10, 3)
input is (10, 10, 3)
input is (10, 10, 3)
[0327 12:14:04 @common.py:655] DataFlow Info:
datapoint 0<5 with 1 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0074, 0.9956]
datapoint 1<5 with 1 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0010, 0.9808]
datapoint 2<5 with 1 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0074, 0.9956]
datapoint 3<5 with 1 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0094, 0.9222]
datapoint 4<5 with 1 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0276, 0.9933]

```",python import import copy class take data another produce certain number time self input number time repeat super self size self class return self range yield issue input therefore first crop next applied input already check python self range print input yield input input input input input shape range shape range shape range shape range shape range python self range print input yield input input input input input shape range shape range shape range shape range shape range,issue,positive,positive,positive,positive,positive,positive
289409059,"I am not using AugmentImageComponents but AugmentImageComponent (without s). That only applies it to the first component.

I meant more: why does my original RepeatedDataPoint not do what I expect it would do (which is: make 5 datapoints out of one datapoint which can be augmented independently)?",without first component meant original expect would make one augmented independently,issue,negative,positive,positive,positive,positive,positive
289407576,"The problem is that AugmentImageComponents is implemented to
> Apply image augmentors on several components, with shared augmentation parameters.",problem apply image several augmentation,issue,negative,neutral,neutral,neutral,neutral,neutral
289407371,"Thanks. I was also thinking about this workaround, however, then one has to chop up the datapoints again... can you explain to me what the problem with my original approach was?",thanks also thinking however one chop explain problem original approach,issue,negative,positive,positive,positive,positive,positive
289404280,"One solution is patching the AugmentImageComponents class or using
```python
from tensorpack import *


ds = FakeData([(10, 10, 3)], random=False)
ds = MapData(ds, lambda dp: [dp[0], dp[0], dp[0]])
ds = AugmentImageComponent(ds, [imgaug.RandomCrop((4, 4))], index=0)
ds = AugmentImageComponent(ds, [imgaug.RandomCrop((4, 4))], index=1)
ds = AugmentImageComponent(ds, [imgaug.RandomCrop((4, 4))], index=2)
ds = PrintData(ds, num=5)
```


gives
```
datapoint 0<5 with 3 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0204, 0.9997]
   dp 1: is ndarray of shape (4, 4, 3) with range [0.0042, 0.9929]
   dp 2: is ndarray of shape (4, 4, 3) with range [0.0204, 0.9997]
datapoint 1<5 with 3 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0204, 0.9997]
   dp 1: is ndarray of shape (4, 4, 3) with range [0.0042, 0.9929]
   dp 2: is ndarray of shape (4, 4, 3) with range [0.0042, 0.9929]
datapoint 2<5 with 3 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0286, 0.9997]
   dp 1: is ndarray of shape (4, 4, 3) with range [0.0340, 0.9862]
   dp 2: is ndarray of shape (4, 4, 3) with range [0.0286, 0.9408]
datapoint 3<5 with 3 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0003, 0.9862]
   dp 1: is ndarray of shape (4, 4, 3) with range [0.0286, 0.9300]
   dp 2: is ndarray of shape (4, 4, 3) with range [0.0286, 0.9794]
datapoint 4<5 with 3 components consists of
   dp 0: is ndarray of shape (4, 4, 3) with range [0.0207, 0.9929]
   dp 1: is ndarray of shape (4, 4, 3) with range [0.0003, 0.9929]
   dp 2: is ndarray of shape (4, 4, 3) with range [0.0286, 0.9300]
```",one solution class python import lambda shape range shape range shape range shape range shape range shape range shape range shape range shape range shape range shape range shape range shape range shape range shape range,issue,negative,neutral,neutral,neutral,neutral,neutral
289402550,@PatWie: Thanks for the quick reply. Your proposed solution does not seem to work. ,thanks quick reply solution seem work,issue,positive,positive,positive,positive,positive,positive
289377662,"What about patching the layers:
```python
@layer_register()
def Conv2D(x, out_channel, kernel_shape,
           padding='SAME', stride=1,
           W_init=None, b_init=None,
           nl=tf.identity, split=1, use_bias=True,
           data_format='NHWC'):
# ...
return_value = nl(tf.nn.bias_add(conv, b, data_format=data_format) if use_bias else conv, name='output')
return_value.W = W
return_value.b = b
return return_value
```

Then one can write

```python
l = Conv2D('conv1', image, out_channel=96, kernel_shape=11, stride=4, padding='VALID')
print sess.run(l.W)
```
",python else return one write python image print,issue,negative,neutral,neutral,neutral,neutral,neutral
289360539,"There isn't a very straightforward way because I don't know where it can be added. 
Currently the docs tell you the name of the variables, which are 'W', 'b': http://tensorpack.readthedocs.io/en/latest/modules/models.html#tensorpack.models.Conv2D

Then you can access the tensor by, e.g. `Graph = tf.get_default_graph(); W = Graph.get_tensor_by_name('conv0/W:0')`, assuming the name of the layer is 'conv0'.

You can also implement Conv2D yourself if you need more intermediate inputs.",straightforward way know added currently tell name access tensor graph assuming name layer also implement need intermediate,issue,negative,positive,positive,positive,positive,positive
289360145,"A bug was introduced lately. It always tries to evaluate the summary tensor in a separate sess.run call, which leads to error because the call may depend on feed_dict.",bug lately always evaluate summary tensor separate call error call may depend,issue,negative,negative,negative,negative,negative,negative
289337762,"Thanks for your reply. On the same note can you tell me how do i access the weights and biases from the previous layers define in linear warp.i need those as input for my next layers.
 I tried to find it in docs but didn't get any info on this.",thanks reply note tell access previous define linear need input next tried find get,issue,negative,positive,neutral,neutral,positive,positive
289321937,"LinearWrap is a syntax sugar useful when you don't need intermediate layer output.
If you need fc0 layer output, just stop LinearWrap at fc0.",syntax sugar useful need intermediate layer output need layer output stop,issue,negative,positive,positive,positive,positive,positive
289265424,"FYI, added a ""copy"" option to AugmentImageComponent but still defaults to false now.",added copy option still false,issue,negative,negative,negative,negative,negative,negative
289264478,"Just give it a second thought. Maybe it's not a very good idea to use a dangerous default, allowing the augmentors to modifiy the image. 
Maybe we should use a safe default (make a copy explicity) and allow turning it off when users know it's OK.",give second thought maybe good idea use dangerous default image maybe use safe default make copy allow turning know,issue,positive,positive,positive,positive,positive,positive
289264168,"It generates a continuous vector field of NxM so that it gives each pixel a small displacement but continuous in spatial domain. The field is generated by a mixture of gaussian centered at several spatial locations.

You can adjust the [parameters](http://tensorpack.readthedocs.io/en/latest/modules/dataflow.imgaug.html#tensorpack.dataflow.imgaug.GaussianDeform).

Augmentors doesn't gurantee they don't modify input image. They can do whatever to the input to speed up their processing. This specific augmentor doesn't modify the input image, though.
To avoid the modification goes to your original images (in case you keep them persistent in memory, for example), you can yield a deepcopy in your first dataflow.",continuous vector field small displacement continuous spatial domain field mixture centered several spatial adjust modify input image whatever input speed specific modify input image though avoid modification go original case keep persistent memory example yield first,issue,negative,positive,neutral,neutral,positive,positive
289253346,"""perspective"" may not be very important to use plus I don't have a clear idea what it is or how to parameterize it for people to use. Closing.",perspective may important use plus clear idea people use,issue,positive,positive,positive,positive,positive,positive
289252650,"Closing for now.
If you find anything you think is ""very common"" that should be saved with `ModelSaver` by default you're welcome to reopen.",find anything think common saved default welcome reopen,issue,positive,positive,positive,positive,positive,positive
289252082,"Are you using tensorpack models without tensorpack trainer?

When used separately, any tensorpack model needs to be used under a TowerContext. If you really want that you can wrap your model with 
```python
with TowerContext(tower_name='', is_training=True):
```
to let the model work properly.

It is now added in the [tutorial](http://tensorpack.readthedocs.io/en/latest/tutorial/model.html#use-models-outside-tensorpack)",without trainer used separately model need used really want wrap model python let model work properly added tutorial,issue,negative,positive,neutral,neutral,positive,positive
289034367,"There are interesting findings:
- matplotlib is not in requirements.txt. I am not sure if you want to add this.
- the ResNet script relies on ""import caffe"". This is only for reading the mean-file, right? Do you mind to use a pickle or numpy msgpack file instead? Another solution is to create a repo: github.com/tensorpack/data and place the `synsets.txt`, `synset_words.txt`, ... `imagenet_mean.bin` there. I can also add a test for this mean-file, later.


see log:
https://travis-ci.org/ppwwyyxx/tensorpack/jobs/214659882",interesting sure want add script import reading right mind use pickle file instead another solution create place also add test later see log,issue,positive,positive,positive,positive,positive,positive
288828442,"Thanks, works well indeed after downloading protoc 3.2.0",thanks work well indeed,issue,positive,positive,positive,positive,positive,positive
288826369,For newer versions you're expected to see something like this: https://github.com/google/protobuf/issues/7#issuecomment-64992592 in the generated caffe_pb2.py file. Otherwise it won't work with python 3.,see something like file otherwise wo work python,issue,negative,neutral,neutral,neutral,neutral,neutral
288824281,"You may need to upgrade your protoc.
```
$protoc --version
libprotoc 3.2.0
```",may need upgrade version,issue,negative,neutral,neutral,neutral,neutral,neutral
288823079,"If in the ResNet example directory I run:
 ./imagenet-resnet.py --data dataset_ILSVRC2012/   --gpu 0


I get the following:
```

  File ""/local/tensorflow350_gpu/lib/python3.5/site-packages/tensorpack/dataflow/dataset/ilsvrc.py"", line 157, in __init__
    meta = ILSVRCMeta(meta_dir)
  File ""/local/tensorflow350_gpu/lib/python3.5/site-packages/tensorpack/dataflow/dataset/ilsvrc.py"", line 33, in __init__
    self.caffepb = get_caffe_pb()
  File ""/local/tensorflow350_gpu/lib/python3.5/site-packages/tensorpack/utils/loadcaffe.py"", line 130, in get_caffe_pb
    return imp.load_source('caffepb', caffe_pb_file)
  File ""/local/python/3.5.0/lib/python3.5/imp.py"", line 172, in load_source
    module = _load(spec)
  File ""<frozen importlib._bootstrap>"", line 693, in _load
  File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 662, in exec_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
  File ""/local/tensorpack_data/caffe/caffe_pb2.py"", line 17, in <module>
    serialized_pb='\n\x0b\x63\x61\x66\x66\x65.proto\x12\x05\x63\x61\x66\x66\x65\""\x1c\n\tBlobShape\x12\x0f\n\x03\x64im\x18\x01 \x03(\x03\x42\x02\x10\x01\""\xcc\x01\n\tBlobProto\x12\x1f\n\x05shape\x18\x07 \x01(\x0b\x32\x10.caffe.BlobShape\x12\x10\n\x04\x64\x61ta\x18\x05 \x03(\x02\x42\x02\x10\x01\x12\x10\n\x04\x64iff\x18\x06 \x03(\x02\x42\x02\x10\x01\x12\x17\n\x0b\x64ouble_data\x18\x08 \x03(\x01\x42\x02\x10\x01\x12\x17\n\x0b\x64ouble_diff\x18\t \x03(\x01\x42\x02\x10\x01\x12\x0e\n\x03num\x18\x01 \x01(\x05:\x01\x30\x12\x13\n\x08\x63hannels\x18\x02 \x01(\x05:\x01\x30\x12\x11\n\x06height\x18\x03 \x01(\x05:\x01\x30\x12\x10\n\x05width\x18\x04 \x01(\x05:\x01\x30\""2\n\x0f\x42lobProtoVector\x12\x1f\n\x05\x62lobs\x18\x01 \x03(\x0b\x32\x10.caffe.BlobProto\""\x81\x01\n\x05\x44\x61tum\x12\x10\n\x08\x63hannels\x18\x01 \x01(\x05\x12\x0e\n\x06height\x18\x02 \x01(\x05\x12\r\n\x05width\x18\x03 \x01(\x05\x12\x0c\n\x04\x64\x61ta\x18\x04 \x01(\x0c\x12\r\n\x05label\x18\x05 \x01(\x05\x12\x12\n\nfloat_data\x18\x06 \x03(\x02\x12\x16\n\x07\x65ncoded\x18\x07 \x01(\x08:\x05\x66\x61lse\""\x8a\x02\n\x0f\x46illerParameter\x12\x16\n\x04type\x18\x01 \x01(\t:\x08\x63onstant\x12\x10\n\x05value\x18\x02 \x01(\x02:\x01\x30\x12\x0e\n\x03min\x18\x03 \x01(\x02:\x01\x30\x12\x0e\n\x03max\x18\x04 \x01(\x02:\x01\x31\x12\x0f\n\x04mean\x18\x05 \x01(\x02:\x01\x30\x12\x0e\n\x03std\x18\x06 \x01(\x02:\x01\x31\x12\x12\n\x06sparse\x18\x07 \x01(\x05:\x02-1\x12\x42\n\rvariance_norm\x18\x08 \x01(\x0e\x32#.caffe.FillerParameter.VarianceNorm:\x06\x46\x41N_IN\""4\n\x0cVarianceNorm\x12\n\n\x06\x46\x41N_IN\x10\x00\x12\x0b\n\x07\x46\x41N_OUT\x10\x01\x12\x0b\n\x07\x41VERAGE\x10\x02\""\x8e\x02\n\x0cNetParameter\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\r\n\x05input\x18\x03 \x03(\t\x12%\n\x0binput_shape\x18\x08 \x03(\x0b\x32\x10.caffe.BlobShape\x12\x11\n\tinput_dim\x18\x04 \x03(\x05\x12\x1d\n\x0e\x66orce_backward\x18\x05 \x01(\x08:\x05\x66\x61lse\x12\x1e\n\x05state\x18\x06 \x01(\x0b\x32\x0f.caffe.NetState\x12\x19\n\ndebug_info\x18\x07 \x01(\x08:\x05\x66\x61lse\x12$\n\x05layer\x18\x64 \x03(\x0b\x32\x15.caffe.LayerParameter\x12\'\n\x06layers\x18\x02 \x03(\x0b\x32\x17.caffe.V1LayerParameter\""\xc3\n\n\x0fSolverParameter\x12\x0b\n\x03net\x18\x18 \x01(\t\x12&\n\tnet_param\x18\x19 \x01(\x0b\x32\x13.caffe.NetParameter\x12\x11\n\ttrain_net\x18\x01 \x01(\t\x12\x10\n\x08test_net\x18\x02 \x03(\t\x12,\n\x0ftrain_net_param\x18\x15 \x01(\x0b\x32\x13.caffe.NetParameter\x12+\n\x0etest_net_param\x18\x16 \x03(\x0b\x32\x13.caffe.NetParameter\x12$\n\x0btrain_state\x18\x1a \x01(\x0b\x32\x0f.caffe.NetState\x12#\n\ntest_state\x18\x1b \x03(\x0b\x32\x0f.caffe.NetState\x12\x11\n\ttest_iter\x18\x03 \x03(\x05\x12\x18\n\rtest_interval\x18\x04 \x01(\x05:\x01\x30\x12 \n\x11test_compute_loss\x18\x13 \x01(\x08:\x05\x66\x61lse\x12!\n\x13test_initialization\x18  \x01(\x08:\x04true\x12\x0f\n\x07\x62\x61se_lr\x18\x05 \x01(\x02\x12\x0f\n\x07\x64isplay\x18\x06 \x01(\x05\x12\x17\n\x0c\x61verage_loss\x18! \x01(\x05:\x01\x31\x12\x10\n\x08max_iter\x18\x07 \x01(\x05\x12\x14\n\titer_size\x18$ \x01(\x05:\x01\x31\x12\x11\n\tlr_policy\x18\x08 \x01(\t\x12\r\n\x05gamma\x18\t \x01(\x02\x12\r\n\x05power\x18\n \x01(\x02\x12\x10\n\x08momentum\x18\x0b \x01(\x02\x12\x14\n\x0cweight_decay\x18\x0c \x01(\x02\x12\x1f\n\x13regularization_type\x18\x1d \x01(\t:\x02L2\x12\x10\n\x08stepsize\x18\r \x01(\x05\x12\x11\n\tstepvalue\x18\"" \x03(\x05\x12\x1a\n\x0e\x63lip_gradients\x18# \x01(\x02:\x02-1\x12\x13\n\x08snapshot\x18\x0e \x01(\x05:\x01\x30\x12\x17\n\x0fsnapshot_prefix\x18\x0f \x01(\t\x12\x1c\n\rsnapshot_diff\x18\x10 \x01(\x08:\x05\x66\x61lse\x12K\n\x0fsnapshot_format\x18% \x01(\x0e\x32%.caffe.SolverParameter.SnapshotFormat:\x0b\x42INARYPROTO\x12;\n\x0bsolver_mode\x18\x11 \x01(\x0e\x32!.caffe.SolverParameter.SolverMode:\x03GPU\x12\x14\n\tdevice_id\x18\x12 \x01(\x05:\x01\x30\x12\x17\n\x0brandom_seed\x18\x14 \x01(\x03:\x02-1\x12\x11\n\x04type\x18( \x01(\t:\x03SGD\x12\x14\n\x05\x64\x65lta\x18\x1f \x01(\x02:\x05\x31\x65-08\x12\x18\n\tmomentum2\x18\' \x01(\x02:\x05\x30.999\x12\x17\n\trms_decay\x18& \x01(\x02:\x04\x30.99\x12\x19\n\ndebug_info\x18\x17 \x01(\x08:\x05\x66\x61lse\x12\""\n\x14snapshot_after_train\x18\x1c \x01(\x08:\x04true\x12;\n\x0bsolver_type\x18\x1e \x01(\x0e\x32!.caffe.SolverParameter.SolverType:\x03SGD\x12\x1f\n\x11layer_wise_reduce\x18) \x01(\x08:\x04true\""+\n\x0eSnapshotFormat\x12\x08\n\x04HDF5\x10\x00\x12\x0f\n\x0b\x42INARYPROTO\x10\x01\""\x1e\n\nSolverMode\x12\x07\n\x03\x43PU\x10\x00\x12\x07\n\x03GPU\x10\x01\""U\n\nSolverType\x12\x07\n\x03SGD\x10\x00\x12\x0c\n\x08NESTEROV\x10\x01\x12\x0b\n\x07\x41\x44\x41GRAD\x10\x02\x12\x0b\n\x07RMSPROP\x10\x03\x12\x0c\n\x08\x41\x44\x41\x44\x45LTA\x10\x04\x12\x08\n\x04\x41\x44\x41M\x10\x05\""l\n\x0bSolverState\x12\x0c\n\x04iter\x18\x01 \x01(\x05\x12\x13\n\x0blearned_net\x18\x02 \x01(\t\x12!\n\x07history\x18\x03 \x03(\x0b\x32\x10.caffe.BlobProto\x12\x17\n\x0c\x63urrent_step\x18\x04 \x01(\x05:\x01\x30\""N\n\x08NetState\x12!\n\x05phase\x18\x01 \x01(\x0e\x32\x0c.caffe.Phase:\x04TEST\x12\x10\n\x05level\x18\x02 \x01(\x05:\x01\x30\x12\r\n\x05stage\x18\x03 \x03(\t\""s\n\x0cNetStateRule\x12\x1b\n\x05phase\x18\x01 \x01(\x0e\x32\x0c.caffe.Phase\x12\x11\n\tmin_level\x18\x02 \x01(\x05\x12\x11\n\tmax_level\x18\x03 \x01(\x05\x12\r\n\x05stage\x18\x04 \x03(\t\x12\x11\n\tnot_stage\x18\x05 \x03(\t\""\xa3\x01\n\tParamSpec\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x31\n\nshare_mode\x18\x02 \x01(\x0e\x32\x1d.caffe.ParamSpec.DimCheckMode\x12\x12\n\x07lr_mult\x18\x03 \x01(\x02:\x01\x31\x12\x15\n\ndecay_mult\x18\x04 \x01(\x02:\x01\x31\""*\n\x0c\x44imCheckMode\x12\n\n\x06STRICT\x10\x00\x12\x0e\n\nPERMISSIVE\x10\x01\""\x82\x14\n\x0eLayerParameter\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04type\x18\x02 \x01(\t\x12\x0e\n\x06\x62ottom\x18\x03 \x03(\t\x12\x0b\n\x03top\x18\x04 \x03(\t\x12\x1b\n\x05phase\x18\n \x01(\x0e\x32\x0c.caffe.Phase\x12\x13\n\x0bloss_weight\x18\x05 \x03(\x02\x12\x1f\n\x05param\x18\x06 \x03(\x0b\x32\x10.caffe.ParamSpec\x12\x1f\n\x05\x62lobs\x18\x07 \x03(\x0b\x32\x10.caffe.BlobProto\x12\x16\n\x0epropagate_down\x18\x0b \x03(\x08\x12$\n\x07include\x18\x08 \x03(\x0b\x32\x13.caffe.NetStateRule\x12$\n\x07\x65xclude\x18\t \x03(\x0b\x32\x13.caffe.NetStateRule\x12\x37\n\x0ftransform_param\x18\x64 \x01(\x0b\x32\x1e.caffe.TransformationParameter\x12(\n\nloss_param\x18\x65 \x01(\x0b\x32\x14.caffe.LossParameter\x12\x30\n\x0e\x61\x63\x63uracy_param\x18\x66 \x01(\x0b\x32\x18.caffe.AccuracyParameter\x12,\n\x0c\x61rgmax_param\x18g \x01(\x0b\x32\x16.caffe.ArgMaxParameter\x12\x34\n\x10\x62\x61tch_norm_param\x18\x8b\x01 \x01(\x0b\x32\x19.caffe.BatchNormParameter\x12)\n\nbias_param\x18\x8d\x01 \x01(\x0b\x32\x14.caffe.BiasParameter\x12,\n\x0c\x63oncat_param\x18h \x01(\x0b\x32\x16.caffe.ConcatParameter\x12?\n\x16\x63ontrastive_loss_param\x18i \x01(\x0b\x32\x1f.caffe.ContrastiveLossParameter\x12\x36\n\x11\x63onvolution_param\x18j \x01(\x0b\x32\x1b.caffe.ConvolutionParameter\x12)\n\ncrop_param\x18\x90\x01 \x01(\x0b\x32\x14.caffe.CropParameter\x12(\n\ndata_param\x18k \x01(\x0b\x32\x14.caffe.DataParameter\x12.\n\rdropout_param\x18l \x01(\x0b\x32\x17.caffe.DropoutParameter\x12\x33\n\x10\x64ummy_data_param\x18m \x01(\x0b\x32\x19.caffe.DummyDataParameter\x12.\n\reltwise_param\x18n \x01(\x0b\x32\x17.caffe.EltwiseParameter\x12\'\n\telu_param\x18\x8c\x01 \x01(\x0b\x32\x13.caffe.ELUParameter\x12+\n\x0b\x65mbed_param\x18\x89\x01 \x01(\x0b\x32\x15.caffe.EmbedParameter\x12&\n\texp_param\x18o \x01(\x0b\x32\x13.caffe.ExpParameter\x12/\n\rflatten_param\x18\x87\x01 \x01(\x0b\x32\x17.caffe.FlattenParameter\x12\x31\n\x0fhdf5_data_param\x18p \x01(\x0b\x32\x18.caffe.HDF5DataParameter\x12\x35\n\x11hdf5_output_param\x18q \x01(\x0b\x32\x1a.caffe.HDF5OutputParameter\x12\x33\n\x10hinge_loss_param\x18r \x01(\x0b\x32\x19.caffe.HingeLossParameter\x12\x33\n\x10image_data_param\x18s \x01(\x0b\x32\x19.caffe.ImageDataParameter\x12\x39\n\x13infogain_loss_param\x18t \x01(\x0b\x32\x1c.caffe.InfogainLossParameter\x12\x39\n\x13inner_product_param\x18u \x01(\x0b\x32\x1c.caffe.InnerProductParameter\x12+\n\x0binput_param\x18\x8f\x01 \x01(\x0b\x32\x15.caffe.InputParameter\x12\'\n\tlog_param\x18\x86\x01 \x01(\x0b\x32\x13.caffe.LogParameter\x12&\n\tlrn_param\x18v \x01(\x0b\x32\x13.caffe.LRNParameter\x12\x35\n\x11memory_data_param\x18w \x01(\x0b\x32\x1a.caffe.MemoryDataParameter\x12&\n\tmvn_param\x18x \x01(\x0b\x32\x13.caffe.MVNParameter\x12\x33\n\x0fparameter_param\x18\x91\x01 \x01(\x0b\x32\x19.caffe.ParameterParameter\x12.\n\rpooling_param\x18y \x01(\x0b\x32\x17.caffe.PoolingParameter\x12*\n\x0bpower_param\x18z \x01(\x0b\x32\x15.caffe.PowerParameter\x12+\n\x0bprelu_param\x18\x83\x01 \x01(\x0b\x32\x15.caffe.PReLUParameter\x12-\n\x0cpython_param\x18\x82\x01 \x01(\x0b\x32\x16.caffe.PythonParameter\x12\x33\n\x0frecurrent_param\x18\x92\x01 \x01(\x0b\x32\x19.caffe.RecurrentParameter\x12\x33\n\x0freduction_param\x18\x88\x01 \x01(\x0b\x32\x19.caffe.ReductionParameter\x12(\n\nrelu_param\x18{ \x01(\x0b\x32\x14.caffe.ReLUParameter\x12/\n\rreshape_param\x18\x85\x01 \x01(\x0b\x32\x17.caffe.ReshapeParameter\x12+\n\x0bscale_param\x18\x8e\x01 \x01(\x0b\x32\x15.caffe.ScaleParameter\x12.\n\rsigmoid_param\x18| \x01(\x0b\x32\x17.caffe.SigmoidParameter\x12.\n\rsoftmax_param\x18} \x01(\x0b\x32\x17.caffe.SoftmaxParameter\x12\'\n\tspp_param\x18\x84\x01 \x01(\x0b\x32\x13.caffe.SPPParameter\x12*\n\x0bslice_param\x18~ \x01(\x0b\x32\x15.caffe.SliceParameter\x12(\n\ntanh_param\x18\x7f \x01(\x0b\x32\x14.caffe.TanHParameter\x12\x33\n\x0fthreshold_param\x18\x80\x01 \x01(\x0b\x32\x19.caffe.ThresholdParameter\x12)\n\ntile_param\x18\x8a\x01 \x01(\x0b\x32\x14.caffe.TileParameter\x12\x36\n\x11window_data_param\x18\x81\x01 \x01(\x0b\x32\x1a.caffe.WindowDataParameter\""\xb6\x01\n\x17TransformationParameter\x12\x10\n\x05scale\x18\x01 \x01(\x02:\x01\x31\x12\x15\n\x06mirror\x18\x02 \x01(\x08:\x05\x66\x61lse\x12\x14\n\tcrop_size\x18\x03 \x01(\r:\x01\x30\x12\x11\n\tmean_file\x18\x04 \x01(\t\x12\x12\n\nmean_value\x18\x05 \x03(\x02\x12\x1a\n\x0b\x66orce_color\x18\x06 \x01(\x08:\x05\x66\x61lse\x12\x19\n\nforce_gray\x18\x07 \x01(\x08:\x05\x66\x61lse\""\xc2\x01\n\rLossParameter\x12\x14\n\x0cignore_label\x18\x01 \x01(\x05\x12\x44\n\rnormalization\x18\x03 \x01(\x0e\x32&.caffe.LossParameter.NormalizationMode:\x05VALID\x12\x11\n\tnormalize\x18\x02 \x01(\x08\""B\n\x11NormalizationMode\x12\x08\n\x04\x46ULL\x10\x00\x12\t\n\x05VALID\x10\x01\x12\x0e\n\nBATCH_SIZE\x10\x02\x12\x08\n\x04NONE\x10\x03\""L\n\x11\x41\x63\x63uracyParameter\x12\x10\n\x05top_k\x18\x01 \x01(\r:\x01\x31\x12\x0f\n\x04\x61xis\x18\x02 \x01(\x05:\x01\x31\x12\x14\n\x0cignore_label\x18\x03 \x01(\x05\""M\n\x0f\x41rgMaxParameter\x12\x1a\n\x0bout_max_val\x18\x01 \x01(\x08:\x05\x66\x61lse\x12\x10\n\x05top_k\x18\x02 \x01(\r:\x01\x31\x12\x0c\n\x04\x61xis\x18\x03 \x01(\x05\""9\n\x0f\x43oncatParameter\x12\x0f\n\x04\x61xis\x18\x02 \x01(\x05:\x01\x31\x12\x15\n\nconcat_dim\x18\x01 \x01(\r:\x01\x31\""j\n\x12\x42\x61tchNormParameter\x12\x18\n\x10use_global_stats\x18\x01 \x01(\x08\x12&\n\x17moving_average_fraction\x18\x02 \x01(\x02:\x05\x30.999\x12\x12\n\x03\x65ps\x18\x03 \x01(\x02:\x05\x31\x65-05\""]\n\rBiasParameter\x12\x0f\n\x04\x61xis\x18\x01 \x01(\x05:\x01\x31\x12\x13\n\x08num_axes\x18\x02 \x01(\x05:\x01\x31\x12&\n\x06\x66iller\x18\x03 \x01(\x0b\x32\x16.caffe.FillerParameter\""L\n\x18\x43ontrastiveLossParameter\x12\x11\n\x06margin\x18\x01 \x01(\x02:\x01\x31\x12\x1d\n\x0elegacy_version\x18\x02 \x01(\x08:\x05\x66\x61lse\""\xfc\x03\n\x14\x43onvolutionParameter\x12\x12\n\nnum_output\x18\x01 \x01(\r\x12\x17\n\tbias_term\x18\x02 \x01(\x08:\x04true\x12\x0b\n\x03pad\x18\x03 \x03(\r\x12\x13\n\x0bkernel_size\x18\x04 \x03(\r\x12\x0e\n\x06stride\x18\x06 \x03(\r\x12\x10\n\x08\x64ilation\x18\x12 \x03(\r\x12\x10\n\x05pad_h\x18\t \x01(\r:\x01\x30\x12\x10\n\x05pad_w\x18\n \x01(\r:\x01\x30\x12\x10\n\x08kernel_h\x18\x0b \x01(\r\x12\x10\n\x08kernel_w\x18\x0c \x01(\r\x12\x10\n\x08stride_h\x18\r \x01(\r\x12\x10\n\x08stride_w\x18\x0e \x01(\r\x12\x10\n\x05group\x18\x05 \x01(\r:\x01\x31\x12-\n\rweight_filler\x18\x07 \x01(\x0b\x32\x16.caffe.FillerParameter\x12+\n\x0b\x62ias_filler\x18\x08 \x01(\x0b\x32\x16.caffe.FillerParameter\x12;\n\x06\x65ngine\x18\x0f \x01(\x0e\x32\"".caffe.ConvolutionParameter.Engine:\x07\x44\x45\x46\x41ULT\x12\x0f\n\x04\x61xis\x18\x10 \x01(\x05:\x01\x31\x12\x1e\n\x0f\x66orce_nd_im2col\x18\x11 \x01(\x08:\x05\x66\x61lse\""+\n\x06\x45ngine\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\t\n\x05\x43\x41\x46\x46\x45\x10\x01\x12\t\n\x05\x43UDNN\x10\x02\""0\n\rCropParameter\x12\x0f\n\x04\x61xis\x18\x01 \x01(\x05:\x01\x32\x12\x0e\n\x06offset\x18\x02 \x03(\r\""\xa4\x02\n\rDataParameter\x12\x0e\n\x06source\x18\x01 \x01(\t\x12\x12\n\nbatch_size\x18\x04 \x01(\r\x12\x14\n\trand_skip\x18\x07 \x01(\r:\x01\x30\x12\x31\n\x07\x62\x61\x63kend\x18\x08 \x01(\x0e\x32\x17.caffe.DataParameter.DB:\x07LEVELDB\x12\x10\n\x05scale\x18\x02 \x01(\x02:\x01\x31\x12\x11\n\tmean_file\x18\x03 \x01(\t\x12\x14\n\tcrop_size\x18\x05 \x01(\r:\x01\x30\x12\x15\n\x06mirror\x18\x06 \x01(\x08:\x05\x66\x61lse\x12\""\n\x13\x66orce_encoded_color\x18\t \x01(\x08:\x05\x66\x61lse\x12\x13\n\x08prefetch\x18\n \x01(\r:\x01\x34\""\x1b\n\x02\x44\x42\x12\x0b\n\x07LEVELDB\x10\x00\x12\x08\n\x04LMDB\x10\x01\"".\n\x10\x44ropoutParameter\x12\x1a\n\rdropout_ratio\x18\x01 \x01(\x02:\x03\x30.5\""\xa0\x01\n\x12\x44ummyDataParameter\x12+\n\x0b\x64\x61ta_filler\x18\x01 \x03(\x0b\x32\x16.caffe.FillerParameter\x12\x1f\n\x05shape\x18\x06 \x03(\x0b\x32\x10.caffe.BlobShape\x12\x0b\n\x03num\x18\x02 \x03(\r\x12\x10\n\x08\x63hannels\x18\x03 \x03(\r\x12\x0e\n\x06height\x18\x04 \x03(\r\x12\r\n\x05width\x18\x05 \x03(\r\""\xa5\x01\n\x10\x45ltwiseParameter\x12\x39\n\toperation\x18\x01 \x01(\x0e\x32!.caffe.EltwiseParameter.EltwiseOp:\x03SUM\x12\r\n\x05\x63oeff\x18\x02 \x03(\x02\x12\x1e\n\x10stable_prod_grad\x18\x03 \x01(\x08:\x04true\""\'\n\tEltwiseOp\x12\x08\n\x04PROD\x10\x00\x12\x07\n\x03SUM\x10\x01\x12\x07\n\x03MAX\x10\x02\"" \n\x0c\x45LUParameter\x12\x10\n\x05\x61lpha\x18\x01 \x01(\x02:\x01\x31\""\xac\x01\n\x0e\x45mbedParameter\x12\x12\n\nnum_output\x18\x01 \x01(\r\x12\x11\n\tinput_dim\x18\x02 \x01(\r\x12\x17\n\tbias_term\x18\x03 \x01(\x08:\x04true\x12-\n\rweight_filler\x18\x04 \x01(\x0b\x32\x16.caffe.FillerParameter\x12+\n\x0b\x62ias_filler\x18\x05 \x01(\x0b\x32\x16.caffe.FillerParameter\""D\n\x0c\x45xpParameter\x12\x10\n\x04\x62\x61se\x18\x01 \x01(\x02:\x02-1\x12\x10\n\x05scale\x18\x02 \x01(\x02:\x01\x31\x12\x10\n\x05shift\x18\x03 \x01(\x02:\x01\x30\""9\n\x10\x46lattenParameter\x12\x0f\n\x04\x61xis\x18\x01 \x01(\x05:\x01\x31\x12\x14\n\x08\x65nd_axis\x18\x02 \x01(\x05:\x02-1\""O\n\x11HDF5DataParameter\x12\x0e\n\x06source\x18\x01 \x01(\t\x12\x12\n\nbatch_size\x18\x02 \x01(\r\x12\x16\n\x07shuffle\x18\x03 \x01(\x08:\x05\x66\x61lse\""(\n\x13HDF5OutputParameter\x12\x11\n\tfile_name\x18\x01 \x01(\t\""^\n\x12HingeLossParameter\x12\x30\n\x04norm\x18\x01 \x01(\x0e\x32\x1e.caffe.HingeLossParameter.Norm:\x02L1\""\x16\n\x04Norm\x12\x06\n\x02L1\x10\x01\x12\x06\n\x02L2\x10\x02\""\x97\x02\n\x12ImageDataParameter\x12\x0e\n\x06source\x18\x01 \x01(\t\x12\x15\n\nbatch_size\x18\x04 \x01(\r:\x01\x31\x12\x14\n\trand_skip\x18\x07 \x01(\r:\x01\x30\x12\x16\n\x07shuffle\x18\x08 \x01(\x08:\x05\x66\x61lse\x12\x15\n\nnew_height\x18\t \x01(\r:\x01\x30\x12\x14\n\tnew_width\x18\n \x01(\r:\x01\x30\x12\x16\n\x08is_color\x18\x0b \x01(\x08:\x04true\x12\x10\n\x05scale\x18\x02 \x01(\x02:\x01\x31\x12\x11\n\tmean_file\x18\x03 \x01(\t\x12\x14\n\tcrop_size\x18\x05 \x01(\r:\x01\x30\x12\x15\n\x06mirror\x18\x06 \x01(\x08:\x05\x66\x61lse\x12\x15\n\x0broot_folder\x18\x0c \x01(\t:\x00\""\'\n\x15InfogainLossParameter\x12\x0e\n\x06source\x18\x01 \x01(\t\""\xcb\x01\n\x15InnerProductParameter\x12\x12\n\nnum_output\x18\x01 \x01(\r\x12\x17\n\tbias_term\x18\x02 \x01(\x08:\x04true\x12-\n\rweight_filler\x18\x03 \x01(\x0b\x32\x16.caffe.FillerParameter\x12+\n\x0b\x62ias_filler\x18\x04 \x01(\x0b\x32\x16.caffe.FillerParameter\x12\x0f\n\x04\x61xis\x18\x05 \x01(\x05:\x01\x31\x12\x18\n\ttranspose\x18\x06 \x01(\x08:\x05\x66\x61lse\""1\n\x0eInputParameter\x12\x1f\n\x05shape\x18\x01 \x03(\x0b\x32\x10.caffe.BlobShape\""D\n\x0cLogParameter\x12\x10\n\x04\x62\x61se\x18\x01 \x01(\x02:\x02-1\x12\x10\n\x05scale\x18\x02 \x01(\x02:\x01\x31\x12\x10\n\x05shift\x18\x03 \x01(\x02:\x01\x30\""\xb8\x02\n\x0cLRNParameter\x12\x15\n\nlocal_size\x18\x01 \x01(\r:\x01\x35\x12\x10\n\x05\x61lpha\x18\x02 \x01(\x02:\x01\x31\x12\x12\n\x04\x62\x65ta\x18\x03 \x01(\x02:\x04\x30.75\x12\x44\n\x0bnorm_region\x18\x04 \x01(\x0e\x32\x1e.caffe.LRNParameter.NormRegion:\x0f\x41\x43ROSS_CHANNELS\x12\x0c\n\x01k\x18\x05 \x01(\x02:\x01\x31\x12\x33\n\x06\x65ngine\x18\x06 \x01(\x0e\x32\x1a.caffe.LRNParameter.Engine:\x07\x44\x45\x46\x41ULT\""5\n\nNormRegion\x12\x13\n\x0f\x41\x43ROSS_CHANNELS\x10\x00\x12\x12\n\x0eWITHIN_CHANNEL\x10\x01\""+\n\x06\x45ngine\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\t\n\x05\x43\x41\x46\x46\x45\x10\x01\x12\t\n\x05\x43UDNN\x10\x02\""Z\n\x13MemoryDataParameter\x12\x12\n\nbatch_size\x18\x01 \x01(\r\x12\x10\n\x08\x63hannels\x18\x02 \x01(\r\x12\x0e\n\x06height\x18\x03 \x01(\r\x12\r\n\x05width\x18\x04 \x01(\r\""d\n\x0cMVNParameter\x12 \n\x12normalize_variance\x18\x01 \x01(\x08:\x04true\x12\x1e\n\x0f\x61\x63ross_channels\x18\x02 \x01(\x08:\x05\x66\x61lse\x12\x12\n\x03\x65ps\x18\x03 \x01(\x02:\x05\x31\x65-09\""5\n\x12ParameterParameter\x12\x1f\n\x05shape\x18\x01 \x01(\x0b\x32\x10.caffe.BlobShape\""\xa2\x03\n\x10PoolingParameter\x12\x35\n\x04pool\x18\x01 \x01(\x0e\x32\"".caffe.PoolingParameter.PoolMethod:\x03MAX\x12\x0e\n\x03pad\x18\x04 \x01(\r:\x01\x30\x12\x10\n\x05pad_h\x18\t \x01(\r:\x01\x30\x12\x10\n\x05pad_w\x18\n \x01(\r:\x01\x30\x12\x13\n\x0bkernel_size\x18\x02 \x01(\r\x12\x10\n\x08kernel_h\x18\x05 \x01(\r\x12\x10\n\x08kernel_w\x18\x06 \x01(\r\x12\x11\n\x06stride\x18\x03 \x01(\r:\x01\x31\x12\x10\n\x08stride_h\x18\x07 \x01(\r\x12\x10\n\x08stride_w\x18\x08 \x01(\r\x12\x37\n\x06\x65ngine\x18\x0b \x01(\x0e\x32\x1e.caffe.PoolingParameter.Engine:\x07\x44\x45\x46\x41ULT\x12\x1d\n\x0eglobal_pooling\x18\x0c \x01(\x08:\x05\x66\x61lse\"".\n\nPoolMethod\x12\x07\n\x03MAX\x10\x00\x12\x07\n\x03\x41VE\x10\x01\x12\x0e\n\nSTOCHASTIC\x10\x02\""+\n\x06\x45ngine\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\t\n\x05\x43\x41\x46\x46\x45\x10\x01\x12\t\n\x05\x43UDNN\x10\x02\""F\n\x0ePowerParameter\x12\x10\n\x05power\x18\x01 \x01(\x02:\x01\x31\x12\x10\n\x05scale\x18\x02 \x01(\x02:\x01\x31\x12\x10\n\x05shift\x18\x03 \x01(\x02:\x01\x30\""g\n\x0fPythonParameter\x12\x0e\n\x06module\x18\x01 \x01(\t\x12\r\n\x05layer\x18\x02 \x01(\t\x12\x13\n\tparam_str\x18\x03 \x01(\t:\x00\x12 \n\x11share_in_parallel\x18\x04 \x01(\x08:\x05\x66\x61lse\""\xc0\x01\n\x12RecurrentParameter\x12\x15\n\nnum_output\x18\x01 \x01(\r:\x01\x30\x12-\n\rweight_filler\x18\x02 \x01(\x0b\x32\x16.caffe.FillerParameter\x12+\n\x0b\x62ias_filler\x18\x03 \x01(\x0b\x32\x16.caffe.FillerParameter\x12\x19\n\ndebug_info\x18\x04 \x01(\x08:\x05\x66\x61lse\x12\x1c\n\rexpose_hidden\x18\x05 \x01(\x08:\x05\x66\x61lse\""\xad\x01\n\x12ReductionParameter\x12=\n\toperation\x18\x01 \x01(\x0e\x32%.caffe.ReductionParameter.ReductionOp:\x03SUM\x12\x0f\n\x04\x61xis\x18\x02 \x01(\x05:\x01\x30\x12\x10\n\x05\x63oeff\x18\x03 \x01(\x02:\x01\x31\""5\n\x0bReductionOp\x12\x07\n\x03SUM\x10\x01\x12\x08\n\x04\x41SUM\x10\x02\x12\t\n\x05SUMSQ\x10\x03\x12\x08\n\x04MEAN\x10\x04\""\x8d\x01\n\rReLUParameter\x12\x19\n\x0enegative_slope\x18\x01 \x01(\x02:\x01\x30\x12\x34\n\x06\x65ngine\x18\x02 \x01(\x0e\x32\x1b.caffe.ReLUParameter.Engine:\x07\x44\x45\x46\x41ULT\""+\n\x06\x45ngine\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\t\n\x05\x43\x41\x46\x46\x45\x10\x01\x12\t\n\x05\x43UDNN\x10\x02\""Z\n\x10ReshapeParameter\x12\x1f\n\x05shape\x18\x01 \x01(\x0b\x32\x10.caffe.BlobShape\x12\x0f\n\x04\x61xis\x18\x02 \x01(\x05:\x01\x30\x12\x14\n\x08num_axes\x18\x03 \x01(\x05:\x02-1\""\xa5\x01\n\x0eScaleParameter\x12\x0f\n\x04\x61xis\x18\x01 \x01(\x05:\x01\x31\x12\x13\n\x08num_axes\x18\x02 \x01(\x05:\x01\x31\x12&\n\x06\x66iller\x18\x03 \x01(\x0b\x32\x16.caffe.FillerParameter\x12\x18\n\tbias_term\x18\x04 \x01(\x08:\x05\x66\x61lse\x12+\n\x0b\x62ias_filler\x18\x05 \x01(\x0b\x32\x16.caffe.FillerParameter\""x\n\x10SigmoidParameter\x12\x37\n\x06\x65ngine\x18\x01 \x01(\x0e\x32\x1e.caffe.SigmoidParameter.Engine:\x07\x44\x45\x46\x41ULT\""+\n\x06\x45ngine\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\t\n\x05\x43\x41\x46\x46\x45\x10\x01\x12\t\n\x05\x43UDNN\x10\x02\""L\n\x0eSliceParameter\x12\x0f\n\x04\x61xis\x18\x03 \x01(\x05:\x01\x31\x12\x13\n\x0bslice_point\x18\x02 \x03(\r\x12\x14\n\tslice_dim\x18\x01 \x01(\r:\x01\x31\""\x89\x01\n\x10SoftmaxParameter\x12\x37\n\x06\x65ngine\x18\x01 \x01(\x0e\x32\x1e.caffe.SoftmaxParameter.Engine:\x07\x44\x45\x46\x41ULT\x12\x0f\n\x04\x61xis\x18\x02 \x01(\x05:\x01\x31\""+\n\x06\x45ngine\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\t\n\x05\x43\x41\x46\x46\x45\x10\x01\x12\t\n\x05\x43UDNN\x10\x02\""r\n\rTanHParameter\x12\x34\n\x06\x65ngine\x18\x01 \x01(\x0e\x32\x1b.caffe.TanHParameter.Engine:\x07\x44\x45\x46\x41ULT\""+\n\x06\x45ngine\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\t\n\x05\x43\x41\x46\x46\x45\x10\x01\x12\t\n\x05\x43UDNN\x10\x02\""/\n\rTileParameter\x12\x0f\n\x04\x61xis\x18\x01 \x01(\x05:\x01\x31\x12\r\n\x05tiles\x18\x02 \x01(\x05\""*\n\x12ThresholdParameter\x12\x14\n\tthreshold\x18\x01 \x01(\x02:\x01\x30\""\xc1\x02\n\x13WindowDataParameter\x12\x0e\n\x06source\x18\x01 \x01(\t\x12\x10\n\x05scale\x18\x02 \x01(\x02:\x01\x31\x12\x11\n\tmean_file\x18\x03 \x01(\t\x12\x12\n\nbatch_size\x18\x04 \x01(\r\x12\x14\n\tcrop_size\x18\x05 \x01(\r:\x01\x30\x12\x15\n\x06mirror\x18\x06 \x01(\x08:\x05\x66\x61lse\x12\x19\n\x0c\x66g_threshold\x18\x07 \x01(\x02:\x03\x30.5\x12\x19\n\x0c\x62g_threshold\x18\x08 \x01(\x02:\x03\x30.5\x12\x19\n\x0b\x66g_fraction\x18\t \x01(\x02:\x04\x30.25\x12\x16\n\x0b\x63ontext_pad\x18\n \x01(\r:\x01\x30\x12\x17\n\tcrop_mode\x18\x0b \x01(\t:\x04warp\x12\x1b\n\x0c\x63\x61\x63he_images\x18\x0c \x01(\x08:\x05\x66\x61lse\x12\x15\n\x0broot_folder\x18\r \x01(\t:\x00\""\xeb\x01\n\x0cSPPParameter\x12\x16\n\x0epyramid_height\x18\x01 \x01(\r\x12\x31\n\x04pool\x18\x02 \x01(\x0e\x32\x1e.caffe.SPPParameter.PoolMethod:\x03MAX\x12\x33\n\x06\x65ngine\x18\x06 \x01(\x0e\x32\x1a.caffe.SPPParameter.Engine:\x07\x44\x45\x46\x41ULT\"".\n\nPoolMethod\x12\x07\n\x03MAX\x10\x00\x12\x07\n\x03\x41VE\x10\x01\x12\x0e\n\nSTOCHASTIC\x10\x02\""+\n\x06\x45ngine\x12\x0b\n\x07\x44\x45\x46\x41ULT\x10\x00\x12\t\n\x05\x43\x41\x46\x46\x45\x10\x01\x12\t\n\x05\x43UDNN\x10\x02\""\xe0\x13\n\x10V1LayerParameter\x12\x0e\n\x06\x62ottom\x18\x02 \x03(\t\x12\x0b\n\x03top\x18\x03 \x03(\t\x12\x0c\n\x04name\x18\x04 \x01(\t\x12$\n\x07include\x18  \x03(\x0b\x32\x13.caffe.NetStateRule\x12$\n\x07\x65xclude\x18! \x03(\x0b\x32\x13.caffe.NetStateRule\x12/\n\x04type\x18\x05 \x01(\x0e\x32!.caffe.V1LayerParameter.LayerType\x12\x1f\n\x05\x62lobs\x18\x06 \x03(\x0b\x32\x10.caffe.BlobProto\x12\x0e\n\x05param\x18\xe9\x07 \x03(\t\x12>\n\x0f\x62lob_share_mode\x18\xea\x07 \x03(\x0e\x32$.caffe.V1LayerParameter.DimCheckMode\x12\x10\n\x08\x62lobs_lr\x18\x07 \x03(\x02\x12\x14\n\x0cweight_decay\x18\x08 \x03(\x02\x12\x13\n\x0bloss_weight\x18# \x03(\x02\x12\x30\n\x0e\x61\x63\x63uracy_param\x18\x1b \x01(\x0b\x32\x18.caffe.AccuracyParameter\x12,\n\x0c\x61rgmax_param\x18\x17 \x01(\x0b\x32\x16.caffe.ArgMaxParameter\x12,\n\x0c\x63oncat_param\x18\t \x01(\x0b\x32\x16.caffe.ConcatParameter\x12?\n\x16\x63ontrastive_loss_param\x18( \x01(\x0b\x32\x1f.caffe.ContrastiveLossParameter\x12\x36\n\x11\x63onvolution_param\x18\n \x01(\x0b\x32\x1b.caffe.ConvolutionParameter\x12(\n\ndata_param\x18\x0b \x01(\x0b\x32\x14.caffe.DataParameter\x12.\n\rdropout_param\x18\x0c \x01(\x0b\x32\x17.caffe.DropoutParameter\x12\x33\n\x10\x64ummy_data_param\x18\x1a \x01(\x0b\x32\x19.caffe.DummyDataParameter\x12.\n\reltwise_param\x18\x18 \x01(\x0b\x32\x17.caffe.EltwiseParameter\x12&\n\texp_param\x18) \x01(\x0b\x32\x13.caffe.ExpParameter\x12\x31\n\x0fhdf5_data_param\x18\r \x01(\x0b\x32\x18.caffe.HDF5DataParameter\x12\x35\n\x11hdf5_output_param\x18\x0e \x01(\x0b\x32\x1a.caffe.HDF5OutputParameter\x12\x33\n\x10hinge_loss_param\x18\x1d \x01(\x0b\x32\x19.caffe.HingeLossParameter\x12\x33\n\x10image_data_param\x18\x0f \x01(\x0b\x32\x19.caffe.ImageDataParameter\x12\x39\n\x13infogain_loss_param\x18\x10 \x01(\x0b\x32\x1c.caffe.InfogainLossParameter\x12\x39\n\x13inner_product_param\x18\x11 \x01(\x0b\x32\x1c.caffe.InnerProductParameter\x12&\n\tlrn_param\x18\x12 \x01(\x0b\x32\x13.caffe.LRNParameter\x12\x35\n\x11memory_data_param\x18\x16 \x01(\x0b\x32\x1a.caffe.MemoryDataParameter\x12&\n\tmvn_param\x18\"" \x01(\x0b\x32\x13.caffe.MVNParameter\x12.\n\rpooling_param\x18\x13 \x01(\x0b\x32\x17.caffe.PoolingParameter\x12*\n\x0bpower_param\x18\x15 \x01(\x0b\x32\x15.caffe.PowerParameter\x12(\n\nrelu_param\x18\x1e \x01(\x0b\x32\x14.caffe.ReLUParameter\x12.\n\rsigmoid_param\x18& \x01(\x0b\x32\x17.caffe.SigmoidParameter\x12.\n\rsoftmax_param\x18\' \x01(\x0b\x32\x17.caffe.SoftmaxParameter\x12*\n\x0bslice_param\x18\x1f \x01(\x0b\x32\x15.caffe.SliceParameter\x12(\n\ntanh_param\x18% \x01(\x0b\x32\x14.caffe.TanHParameter\x12\x32\n\x0fthreshold_param\x18\x19 \x01(\x0b\x32\x19.caffe.ThresholdParameter\x12\x35\n\x11window_data_param\x18\x14 \x01(\x0b\x32\x1a.caffe.WindowDataParameter\x12\x37\n\x0ftransform_param\x18$ \x01(\x0b\x32\x1e.caffe.TransformationParameter\x12(\n\nloss_param\x18* \x01(\x0b\x32\x14.caffe.LossParameter\x12&\n\x05layer\x18\x01 \x01(\x0b\x32\x17.caffe.V0LayerParameter\""\xd8\x04\n\tLayerType\x12\x08\n\x04NONE\x10\x00\x12\n\n\x06\x41\x42SVAL\x10#\x12\x0c\n\x08\x41\x43\x43URACY\x10\x01\x12\n\n\x06\x41RGMAX\x10\x1e\x12\x08\n\x04\x42NLL\x10\x02\x12\n\n\x06\x43ONCAT\x10\x03\x12\x14\n\x10\x43ONTRASTIVE_LOSS\x10%\x12\x0f\n\x0b\x43ONVOLUTION\x10\x04\x12\x08\n\x04\x44\x41TA\x10\x05\x12\x11\n\rDECONVOLUTION\x10\'\x12\x0b\n\x07\x44ROPOUT\x10\x06\x12\x0e\n\nDUMMY_DATA\x10 \x12\x12\n\x0e\x45UCLIDEAN_LOSS\x10\x07\x12\x0b\n\x07\x45LTWISE\x10\x19\x12\x07\n\x03\x45XP\x10&\x12\x0b\n\x07\x46LATTEN\x10\x08\x12\r\n\tHDF5_DATA\x10\t\x12\x0f\n\x0bHDF5_OUTPUT\x10\n\x12\x0e\n\nHINGE_LOSS\x10\x1c\x12\n\n\x06IM2COL\x10\x0b\x12\x0e\n\nIMAGE_DATA\x10\x0c\x12\x11\n\rINFOGAIN_LOSS\x10\r\x12\x11\n\rINNER_PRODUCT\x10\x0e\x12\x07\n\x03LRN\x10\x0f\x12\x0f\n\x0bMEMORY_DATA\x10\x1d\x12\x1d\n\x19MULTINOMIAL_LOGISTIC_LOSS\x10\x10\x12\x07\n\x03MVN\x10\""\x12\x0b\n\x07POOLING\x10\x11\x12\t\n\x05POWER\x10\x1a\x12\x08\n\x04RELU\x10\x12\x12\x0b\n\x07SIGMOID\x10\x13\x12\x1e\n\x1aSIGMOID_CROSS_ENTROPY_LOSS\x10\x1b\x12\x0b\n\x07SILENCE\x10$\x12\x0b\n\x07SOFTMAX\x10\x14\x12\x10\n\x0cSOFTMAX_LOSS\x10\x15\x12\t\n\x05SPLIT\x10\x16\x12\t\n\x05SLICE\x10!\x12\x08\n\x04TANH\x10\x17\x12\x0f\n\x0bWINDOW_DATA\x10\x18\x12\r\n\tTHRESHOLD\x10\x1f\""*\n\x0c\x44imCheckMode\x12\n\n\x06STRICT\x10\x00\x12\x0e\n\nPERMISSIVE\x10\x01\""\xfd\x07\n\x10V0LayerParameter\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0c\n\x04type\x18\x02 \x01(\t\x12\x12\n\nnum_output\x18\x03 \x01(\r\x12\x16\n\x08\x62iasterm\x18\x04 \x01(\x08:\x04true\x12-\n\rweight_filler\x18\x05 \x01(\x0b\x32\x16.caffe.FillerParameter\x12+\n\x0b\x62ias_filler\x18\x06 \x01(\x0b\x32\x16.caffe.FillerParameter\x12\x0e\n\x03pad\x18\x07 \x01(\r:\x01\x30\x12\x12\n\nkernelsize\x18\x08 \x01(\r\x12\x10\n\x05group\x18\t \x01(\r:\x01\x31\x12\x11\n\x06stride\x18\n \x01(\r:\x01\x31\x12\x35\n\x04pool\x18\x0b \x01(\x0e\x32\"".caffe.V0LayerParameter.PoolMethod:\x03MAX\x12\x1a\n\rdropout_ratio\x18\x0c \x01(\x02:\x03\x30.5\x12\x15\n\nlocal_size\x18\r \x01(\r:\x01\x35\x12\x10\n\x05\x61lpha\x18\x0e \x01(\x02:\x01\x31\x12\x12\n\x04\x62\x65ta\x18\x0f \x01(\x02:\x04\x30.75\x12\x0c\n\x01k\x18\x16 \x01(\x02:\x01\x31\x12\x0e\n\x06source\x18\x10 \x01(\t\x12\x10\n\x05scale\x18\x11 \x01(\x02:\x01\x31\x12\x10\n\x08meanfile\x18\x12 \x01(\t\x12\x11\n\tbatchsize\x18\x13 \x01(\r\x12\x13\n\x08\x63ropsize\x18\x14 \x01(\r:\x01\x30\x12\x15\n\x06mirror\x18\x15 \x01(\x08:\x05\x66\x61lse\x12\x1f\n\x05\x62lobs\x18\x32 \x03(\x0b\x32\x10.caffe.BlobProto\x12\x10\n\x08\x62lobs_lr\x18\x33 \x03(\x02\x12\x14\n\x0cweight_decay\x18\x34 \x03(\x02\x12\x14\n\trand_skip\x18\x35 \x01(\r:\x01\x30\x12\x1d\n\x10\x64\x65t_fg_threshold\x18\x36 \x01(\x02:\x03\x30.5\x12\x1d\n\x10\x64\x65t_bg_threshold\x18\x37 \x01(\x02:\x03\x30.5\x12\x1d\n\x0f\x64\x65t_fg_fraction\x18\x38 \x01(\x02:\x04\x30.25\x12\x1a\n\x0f\x64\x65t_context_pad\x18: \x01(\r:\x01\x30\x12\x1b\n\rdet_crop_mode\x18; \x01(\t:\x04warp\x12\x12\n\x07new_num\x18< \x01(\x05:\x01\x30\x12\x17\n\x0cnew_channels\x18= \x01(\x05:\x01\x30\x12\x15\n\nnew_height\x18> \x01(\x05:\x01\x30\x12\x14\n\tnew_width\x18? \x01(\x05:\x01\x30\x12\x1d\n\x0eshuffle_images\x18@ \x01(\x08:\x05\x66\x61lse\x12\x15\n\nconcat_dim\x18\x41 \x01(\r:\x01\x31\x12\x36\n\x11hdf5_output_param\x18\xe9\x07 \x01(\x0b\x32\x1a.caffe.HDF5OutputParameter\"".\n\nPoolMethod\x12\x07\n\x03MAX\x10\x00\x12\x07\n\x03\x41VE\x10\x01\x12\x0e\n\nSTOCHASTIC\x10\x02\""W\n\x0ePReLUParameter\x12&\n\x06\x66iller\x18\x01 \x01(\x0b\x32\x16.caffe.FillerParameter\x12\x1d\n\x0e\x63hannel_shared\x18\x02 \x01(\x08:\x05\x66\x61lse*\x1c\n\x05Phase\x12\t\n\x05TRAIN\x10\x00\x12\x08\n\x04TEST\x10\x01'
  File ""/local/tensorflow350_gpu/lib/python3.5/site-packages/google/protobuf/descriptor.py"", line 824, in __new__
    return _message.default_pool.AddSerializedFile(serialized_pb)
TypeError: expected bytes, str found

```


",example directory run data get following file line meta file line file line return file line module spec file frozen line file frozen line file frozen line file frozen line file line module file line return found,issue,negative,neutral,neutral,neutral,neutral,neutral
288819837,Could you post your command and the full log?,could post command full log,issue,negative,positive,positive,positive,positive,positive
288095908,"Please note, that TF-record files do not support random-access as they contain only compressed information. Even for the number of entries, one need to iterate over the entire DB (similar to the LMDB without having a __keys__ entry).

",please note support contain compressed information even number one need iterate entire similar without entry,issue,positive,neutral,neutral,neutral,neutral,neutral
287988447,"This might be the last bug of DQN.
Callback will set learning rate in designate epoch like this.
```
ScheduledHyperParamSetter('learning_rate',
                          [(150, 4e-4), (250, 1e-4), (350, 5e-5)])
```
However when it attempt to update the value, the following error occurs
```
[0321 14:18:27 @param.py:144] learning_rate at epoch 151 will change to 0.00040000
W tensorflow/core/kernels/queue_base.cc:294] _0_input_queue: Skipping cancelled enqueue attempt with queue not closed
Traceback (most recent call last):
[0321 14:18:27 @input_data.py:124] EnqueueThread Exited.
  File ""./DQN.py"", line 250, in <module>
    DQN_Trainer(config).train()
  File ""/home/james847286/Work/RL/DeepQNetwork/tensorpack/train/dqn_trainer.py"", line 97, in train
    self.main_loop()
  File ""/home/james847286/Work/RL/DeepQNetwork/tensorpack/train/dqn_trainer.py"", line 171, in main_loop
    self._callbacks.trigger_epoch()
  File ""/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/base.py"", line 121, in trigger_epoch
    self._trigger_epoch()
  File ""/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/group.py"", line 104, in _trigger_epoch
    cb.trigger_epoch()
  File ""/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/base.py"", line 121, in trigger_epoch
    self._trigger_epoch()
  File ""/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/base.py"", line 181, in _trigger_epoch
    self.trigger()
  File ""/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/base.py"", line 168, in trigger
    self._trigger()
  File ""/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/param.py"", line 160, in _trigger
    self._set_param()
  File ""/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/param.py"", line 168, in _set_param
    self.param.set_value(v)
  File ""/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/param.py"", line 79, in set_value
    self.var.load(v)
AttributeError: 'Variable' object has no attribute 'load'
Segmentation fault (core dumped)
```",might last bug set learning rate designate epoch like however attempt update value following error epoch change skipping attempt queue closed recent call last file line module file line train file line file line file line file line file line file line trigger file line file line file line object attribute segmentation fault core,issue,negative,negative,neutral,neutral,negative,negative
287812545,"Thanks for replying.
Btw Tensorpack is the best tensorflow repo I’ve ever seen, great job!


> On 20 Mar 2017, at 11:37 PM, Yuxin Wu <notifications@github.com> wrote:
> 
> That's the term TF guys use for data-parallel training: https://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch <https://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch>
> Here I extend the concept to use in inference as well.
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub <https://github.com/ppwwyyxx/tensorpack/issues/194#issuecomment-287798369>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ANoVprhLuidXsFvgMvacjOBqyLswJ_UTks5rnp1JgaJpZM4MhU4i>.
> 

",thanks best ever seen great job mar wrote term use training extend concept use inference well state reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
287798369,"That's the term TF guys use for data-parallel training: https://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch

Here I extend the concept to use in inference as well.",term use training extend concept use inference well,issue,negative,neutral,neutral,neutral,neutral,neutral
287747496,"Hi, 

Thanks for solving the problem!
May I ask another personal question that what’s the purpose for constructing “tower” since it occurs in several examples?
Does it relate to multi-thread training or something else?

Best,
James

",hi thanks problem may ask another personal question purpose tower since several relate training something else best,issue,positive,positive,positive,positive,positive,positive
287695594,Right. I'll use a better name.,right use better name,issue,negative,positive,positive,positive,positive,positive
287669781,"The above commit added a keras example. Tested with latest Keras installed from pypi (since Keras is upgrading recently, versions may matter).
Now people can define keras model and utilize the more efficient tensorpack trainers, and even multi gpu training!

Since I'm apparently not a keras user, I only tested a very simple keras model. Not sure how it goes with more complex models. Feel free to report errors you may encounter.",commit added example tested latest since recently may matter people define model utilize efficient even training since apparently user tested simple model sure go complex feel free report may encounter,issue,positive,positive,positive,positive,positive,positive
287664969,MultiGPU on a single machine should run properly then. Are there unresolved questions?,single machine run properly unresolved,issue,negative,negative,neutral,neutral,negative,negative
287616781,"Just added a function `utils.fix_rng_seed` in 546af8b2b3b0dd82b0.
You'll also need to avoid any kind of parallelism:
1. set 1 inter/intra op parallelism thread in session config
2. don't use any kind of prefetch
3. set use_locking=True in the optimizer

And set seed at the beginning of your program by both `fix_rng_seed` and `tf.set_random_seed`.

And this still cannot guarantee deterministic results, they just make things more deterministic. After doing the above things I see very tiny differences when training mnist for a couple of epochs.
Because a lot of common ops (both on GPU and CPU) are non-deterministic. https://github.com/tensorflow/tensorflow/issues/3103

",added function also need avoid kind parallelism set parallelism thread session use kind set set seed beginning program still guarantee deterministic make deterministic see tiny training couple lot common,issue,positive,positive,positive,positive,positive,positive
287582123,"That private API still exists in latest TF.
But the above fix should solve the issue for earlier versions.

I'll also update the doc to not mention tcmalloc anymore, since jemalloc seems better.",private still latest fix solve issue also update doc mention since better,issue,positive,positive,positive,positive,positive,positive
287571148,"Oh I see when I was reading the code I misunderstood tower to mean a cluster with gpus on separate machines. Slurm is like Kubernetes, it is for running in a datacenter or supercomputer with many physical machines, each of which can have different numbers of gpus. Basically you add some specially formatted comments to a shell script requesting CPUs, GPUs, and memory from a cluster then it will run the shell script as requested with environment variables filled out detailing what specific machines you are running on.

Don't worry if multiple machines isn't something tensorpack supports. I wasn't sure, that's why I asked!",oh see reading code misunderstood tower mean cluster separate like running many physical different basically add specially shell script memory cluster run shell script environment filled specific running worry multiple something sure,issue,negative,positive,positive,positive,positive,positive
287564417,"I was using a private TF API which may be different between TF versions.
I will push a fix soon.",private may different push fix soon,issue,negative,neutral,neutral,neutral,neutral,neutral
287560160,"Thanks! Disable tcmalloc solve the problem above.
However I encountered another problem.

```
[0319 01:09:44 @base.py:187] Building predictor graph towerp0 on gpu=0 ...
[0319 01:09:44 @base.py:120] Finalize the graph, create the session ...
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
Traceback (most recent call last):
  File ""./DQN.py"", line 243, in <module>
    QueueInputTrainer(config).train()
  File ""/home/james847286/anaconda2/envs/tf/lib/python2.7/site-packages/tensorpack/train/base.py"", line 91, in train
    self.setup()
  File ""/home/james847286/anaconda2/envs/tf/lib/python2.7/site-packages/tensorpack/train/base.py"", line 123, in setup
    self.sess = self._monitored_sess._tf_sess()  # expose the underlying session also
AttributeError: 'MonitoredSession' object has no attribute '_tf_sess'
Segmentation fault (core dumped)
```
It seems that we do not define ```_tf_sess()``` for self._monitored_sess.",thanks disable solve problem however another problem building predictor graph finalize graph create session device device name bus id recent call last file line module file line train file line setup expose underlying session also object attribute segmentation fault core define,issue,negative,positive,neutral,neutral,positive,positive
287551412,"Could you try without tcmalloc?
TensorFlow now builds with jemalloc together, so maybe there is a problem mixing the two.",could try without together maybe problem two,issue,negative,neutral,neutral,neutral,neutral,neutral
287504001,Thanks a lot! Now it works fine!!,thanks lot work fine,issue,positive,positive,positive,positive,positive,positive
287479280,"Do you just want multiGPU? Distributed training are not supported at the moment.

If just multiGPU, is slurm anywhere different from a normal machine with multiple GPUs? I know nothing about slurm so I'm not sure what's the issue.

There are quite a number of examples with SyncMultiGPUTrainer. And the usage is just the same with Async version.

examples/HED is a segmentation example. Nothing really different from other tasks.",want distributed training moment anywhere different normal machine multiple know nothing sure issue quite number usage version segmentation example nothing really different,issue,negative,positive,positive,positive,positive,positive
287475602,"That tensorlayer example doesn't use any multi tower training or inference (it uses the same tower for inference).
It will also work here if you don't use multi tower training or inference.

The problem is to have multi-gpu training work with Keras.",example use tower training inference tower inference also work use tower training inference problem training work,issue,negative,neutral,neutral,neutral,neutral,neutral
287475057,"👍 got it, I had a virtualenv hanging around. I rebuilt the package and the layer registered fine. 

Ok I'll work through that, thanks a lot. ",got hanging around rebuilt package layer registered fine work thanks lot,issue,positive,positive,positive,positive,positive,positive
287473877,"There is a [TensorLayer example with Keras integration](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_keras.py), also created when I asked the same question there, perhaps that might be helpful?

",example integration also question perhaps might helpful,issue,negative,neutral,neutral,neutral,neutral,neutral
287449560,"1. Then it's very likely you `pip install` it, as I said above. Then any changes locally wouldn't appear unless you're in the top level directory.

2. What's input into the graph is determined [here]( https://github.com/ppwwyyxx/tensorpack/blob/master/examples/A3C-Gym/train-atari.py#L190), and then getting batched [here](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/A3C-Gym/train-atari.py#L212)
If you want extra inputs, just add more InputDesc and change what's put into the queue. ",likely pip install said locally would appear unless top level directory input graph determined getting want extra add change put queue,issue,positive,positive,positive,positive,positive,positive
287442084,"The reason is that:
1. The default session config being used doesn't allow soft placement (fixed above)
2. When tensor placement failed, TensorFlow doesn't give the correct error message. (https://github.com/tensorflow/tensorflow/issues/8507)",reason default session used allow soft placement fixed tensor placement give correct error message,issue,negative,positive,neutral,neutral,positive,positive
287417257,I'll take a look. The second method should work fine.,take look second method work fine,issue,negative,positive,positive,positive,positive,positive
287390252,"I use the imagenet-resnet.py to test tf_debug in tensorpack. I try the first method, which uses SessionCreatorAdapter to create a session_creator and pass it to TrainConfig. Following the instructinos in tensorflow document, https://www.tensorflow.org/programmers_guide/debugger, I get an error when input 'run' command:
```
Traceback (most recent call last):
  File ""imagenet-resnet.py"", line 261, in <module>
    SyncMultiGPUTrainer(config).train()
  File ""/home/user/tensorflow_workspace/tensorpack/tensorpack/train/base.py"", line 91, in train
    self.setup()
  File ""/home/user/tensorflow_workspace/tensorpack/tensorpack/train/base.py"", line 127, in setup
    self.sess.run(init_op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/framework.py"", line 448, in run
    run_end_resp = self.on_run_end(run_end_req)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py"", line 264, in on_run_end
    self._dump_root, partition_graphs=partition_graphs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py"", line 451, in __init__
    raise IOError(""Dump root directory %s does not exist"" % dump_root)
IOError: Dump root directory /tmp/tfdbg_ri8gw6 does not exist
```
I also tried with the tf_debug example given by tensorflow:
```
python -m tensorflow.python.debug.examples.debug_mnist --debug
```
and it works fine.

This issue https://github.com/tensorflow/tensorflow/issues/7615 mentions the similar problem, but with the example given by tensorflow and in Windows platform.",use test try first method create pas following document get error input command recent call last file line module file line train file line setup file line run file line file line raise dump root directory exist dump root directory exist also tried example given python work fine issue similar problem example given platform,issue,negative,positive,positive,positive,positive,positive
287258011,"The probIem is that new layers don't seem to import past the top level. I did a fresh clone and pasted your example into tensorpack/tensorpack/models. 

At the  top level I get : 
```
>>> from tensorpack import models
>>> models.Conv2D
<function tensorpack.models.conv2d.Conv2D>
>>> models.Lstm
<function tensorpack.models.lstm.Lstm>
```
But as soon as I drop into `examples` or `A3C-Gym` I get an error
```
>>> from tensorpack import models
>>> models.Conv2D
<function tensorpack.models.conv2d.Conv2D>
>>> models.Lstm
'module' object has no attribute 'Lstm'
```

Thanks for your help, I just have one more question. I noticed that in the LSTM example you linked to, the solution utilizes lots of inputs like this
```python
self._inputvar_agent_indexs = InputDesc(tf.int32, [None], 'agent_index')
self._inputvar_R = InputDesc(tf.float32, [None, None], 'R')
self._inputvar_td = InputDesc(tf.float32, [None, None], 'td')
self._inputvar_is_over = InputDesc(tf.int32, [None], 'is_over')
self._inputvar_sequence_length = InputDesc(tf.int32, [None], 'sequence_length')
```
And these are obviously done by feeding dicts with `sess.run(states, feed_dict = {...})`

Does this play nicely with the trainer/predictor thats already going in `MySimulatorMaster`? Or does it overwrite everything, and the workers will need to be rewritten. ",new seem import past top level fresh clone pasted example top level get import function function soon drop get error import function object attribute thanks help one question example linked solution lot like python none none none none none none none obviously done feeding play nicely thats already going overwrite everything need,issue,positive,positive,positive,positive,positive,positive
287252271,"A simple file like this placed in `models` is enough for `from tensorpack import *` to find `Lstm`:
```python
from .common import layer_register
__all__ = ['Lstm']
@layer_register()
def Lstm(x):
    pass
```

You can also manage the states by letting the predictor fetch the hidden state, and feeding the hidden state in training. This is supported under the framework as well. I mentioned this in that thread also, but I like the other solution.",simple file like enough import find python import pas also manage predictor fetch hidden state feeding hidden state training framework well thread also like solution,issue,positive,negative,neutral,neutral,negative,negative
287246367,"Right, the model thing isn't really important. But for what its worth, it doesn't seem to register when just placed as a file as tensorpack/tensorpack/models/lstm.py

I've seen that solution before, but it looked like it was ignoring the simulators entirely by just building a state matrix of each agent and keeping it outside of the graph. Is that going to be the best method? Or is there a better way to extract hidden states from the simulator, and control how they get fed back into the graph. 
",right model thing really important worth seem register file seen solution like entirely building state matrix agent keeping outside graph going best method better way extract hidden simulator control get fed back graph,issue,positive,positive,positive,positive,positive,positive
287240204,"Now you can pass an optional `session_creator` in `TrainConfig`. In theory you can choose what session to use for the entire training as long as you write a SessionCreator yourself.
To create a debug session, [`SessionCreatorAdapter`](http://tensorpack.readthedocs.io/en/latest/modules/tfutils.html#tensorpack.tfutils.sesscreate.SessionCreatorAdapter) may help you. You can use:
```python
SessionCreatorAdapter(NewSessionCreator(), 
    lambda sess: tf_debug.LocalCLIDebugWrapperSession(sess, other_options))
```

I also just found there is a debug hook in tensorflow, so an easier way might be just to convert the hook to callback:
```python
        callbacks=[
            HookToCallback(tf_debug.LocalCLIDebugHook()), ...
```

I never used tf_debug so there might be usage I'm not covering. Please leave a note if you found tensorpack makes it harder to do something you could have done easier with tensorflow.",pas optional theory choose session use entire training long write create session may help use python lambda sess sess also found hook easier way might convert hook python never used might usage covering please leave note found harder something could done easier,issue,positive,negative,neutral,neutral,negative,negative
287207991,"2. This A3C implementation doesn't use T_MAX in a batch as the original paper, but a large batch with randomly shuffled experience, so simply adding a LSTM layer shouldn't work. 
It may need a continuous sequence in a batch, plus some way to manipulate states of each game simulator. Others have given a good way to do this: https://github.com/NVlabs/GA3C/issues/3#issuecomment-281247793",implementation use batch original paper large batch randomly experience simply layer work may need continuous sequence batch plus way manipulate game simulator given good way,issue,positive,positive,neutral,neutral,positive,positive
287206048,"1. If you added the layer to `tensorpack/models` as the first snippet, it should be accessible, just as any other layers. 
Maybe you've installed tensorpack through pip, so you are not running the actual code you're changing?
And you can also just register the layer in your own code and import it.",added layer first snippet accessible maybe pip running actual code also register layer code import,issue,negative,positive,positive,positive,positive,positive
287184610,"lol I've been there as well deleting my models....
But directory is set at the very beginning before loading the models. I wonder how this can be detected and warned.",well directory set beginning loading wonder,issue,negative,neutral,neutral,neutral,neutral,neutral
287181640,"There is a small bug, when loading a check point from the current file and choosing ""delete"" the loaded checkpoint is delete. Good that I always set my best models to read-only ;-)

But you are right, setting the label should be done manually.",small bug loading check point current file choosing delete loaded delete good always set best right setting label done manually,issue,positive,positive,positive,positive,positive,positive
287177040,"Did you mean you have this error only in inference (not training)?
",mean error inference training,issue,negative,negative,negative,negative,negative,negative
287117765,"For the moment you can hack https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/train/base.py#L133.

Customized session creation is available in prediction time, but not in training right now. I'll try to fix it.",moment hack session creation available prediction time training right try fix,issue,negative,positive,positive,positive,positive,positive
287097464,That's very strange. I will make a change to adopt your fix.,strange make change adopt fix,issue,negative,negative,neutral,neutral,negative,negative
287080829,"yes.
It will end up [here](https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/tfutils/varmanip.py#L149) and use the latest checkpoint.",yes end use latest,issue,negative,positive,positive,positive,positive,positive
286952965,"Just getting back to this now, I'm actually looking at multi-gpu training with segmentation. I can also run on a slurm cluster where each node has multiple gpus.",getting back actually looking training segmentation also run cluster node multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
286816239,"```
""""""
This script loads the pre-trained ResNet-18 model with (W,A,G) = (1,4,32)
It has 59.2% top-1 and 81.5% top-5 validation error on ILSVRC12 validation set.
To run on images:
    ./resnet-dorefa.py --load pretrained.npy --run a.jpg b.jpg
To eval on ILSVRC validation set:
    ./resnet-dorefa.py --load pretrained.npy --eval --data /path/to/ILSVRC
""""""
```
These are the only two usages of this script. It is not a training script.",script model validation error validation set run load run validation set load data two script training script,issue,negative,neutral,neutral,neutral,neutral,neutral
286814987,"My question is after running this command `./resnet-dorefa.py --data dorefa/ --gpu 0` on a CPU just to check the code with a small dataset of imagenet ILSVRC12 (like 15mb). It just run and finish without any output , so i am confused whether it should print something or show me the training status on the console. 
Also , where does resent-dorefa.py it stores the npy files if it trains successfully? ",question running command data check code small like run finish without output confused whether print something show training status console also successfully,issue,positive,positive,neutral,neutral,positive,positive
286774660,"This would depend on how ""sample"" is written.
If it uses `SaverRestore(model_file)` in the end, then model_file can either be `dir/checkpoint`, or what you have right now will also work.",would depend sample written end either right also work,issue,negative,positive,positive,positive,positive,positive
286737969,"sorry if i was unclear. i wanted to get the model_file after training and pass to sample(). i ended up using:

`
model_file = tf.train.get_checkpoint_state(logger.LOG_DIR).model_checkpoint_path

sample(args.data, model_file)
`",sorry unclear get training pas sample ended sample,issue,negative,negative,negative,negative,negative,negative
286610696,``Look at the docstring in *-dorefa.py to see detailed usage and performance.``,look see detailed usage,issue,negative,positive,positive,positive,positive,positive
286599533,"Also, I am simply trying to train resnet-dorefa.py with this command `./resnet-dorefa.py --dorefa 1,4,6 --data dorefa/ --gpu 0` .

Inside dorefa folder i have two folders train and val. train folder has small chunk of imagenet dataset. it has just two folders n02115913  and n02116738.  and similarly dorefa has val folder which have folder n02115913  and n02116738 containing annotations of train folders.

The command just gets executed without any output or errors.",also simply trying train command data inside folder two train train folder small chunk two similarly folder folder train command executed without output,issue,negative,negative,neutral,neutral,negative,negative
286444226,"I'm not sure what you mean exactly.
But if you want to load the latest model in a separate script, ""LOGGER_DIR/checkpoint"" is the last saved checkpoint.",sure mean exactly want load latest model separate script last saved,issue,positive,positive,positive,positive,positive,positive
286280114,"The parameter is used to set available gpus and one gpu will be used.
This example doesn't include multiple gpu support.",parameter used set available one used example include multiple support,issue,negative,positive,positive,positive,positive,positive
286223032,"Now it saves the variables by `tf.train.Saver` and also save a `MetaGraphDef` protobuf file. These are exactly what a `SavedModel` is saving.
Apart from these, a `SavedModel` also saves assets, tag names for different metagraphs, signatures, etc. These are specific to applications and users, and I don't think I can make decision for users what to save and what not to save. 
If you need these extra features, you can write a new callback just like `ModelSaver` and save what you'd like to save.",also save file exactly saving apart also asset tag different specific think make decision save save need extra write new like save like save,issue,positive,positive,neutral,neutral,positive,positive
285999584,"You can convert the label either on the dataflow (you can use MapDataComponent) or inside the model (you can use tf.gather or tf.gather_nd).

The class order I'm using is the same as caffe. You can see it at `$TENSORPACK_DATASET/ilsvrc_metadata/synset*.txt`",convert label either use inside model use class order see,issue,negative,neutral,neutral,neutral,neutral,neutral
285965752,"As I said they may not use the same class id:
https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py#L119",said may use class id,issue,negative,neutral,neutral,neutral,neutral,neutral
285957526,"

I went with your advice of doing an eval on the model. The eval is giving me very bad results: Top1 Error: 0.98718, Top5 Error: 0.83158

I dont know what is going wrong and where. I added the BGR to RGB and follow preprocessing items that tf/slim does but its not helping.

Some related links: 
Preprocessing: https://github.com/tensorflow/models/blob/master/slim/preprocessing/inception_preprocessing.py#L237-L275

Other efforts trying to do eval on IRv2: https://github.com/kentsommer/keras-inception-resnetV2
This dicussion also mentions something is wrong somewhere: https://github.com/kentsommer/keras-inception-resnetV2/issues/1

Could you give this code a try when you get a chance. Thanks",went advice model giving bad top error top error dont know going wrong added follow helping related link trying also something wrong somewhere could give code try get chance thanks,issue,positive,negative,neutral,neutral,negative,negative
285643597,"`Dropout` by default does this automatically. (use `is_training=None`)
If you set `is_training=True`, then even in inference time it is still True.",dropout default automatically use set even inference time still true,issue,negative,positive,positive,positive,positive,positive
285641422,"For Dropout, if I set isTraining = True, then during inference of the validation set, is the dropout set to isTraining=False ? ",dropout set true inference validation set dropout set,issue,negative,positive,positive,positive,positive,positive
285546350,"Maybe you don't have enough disk space?
Image2Image models are quite large.",maybe enough disk space quite large,issue,negative,positive,positive,positive,positive,positive
285437057,Right. This fix has to be added as long as opencv is involved.,right fix added long involved,issue,negative,positive,positive,positive,positive,positive
285346551,"Hello, in Eq (2), it says that when xi, yi ∈ {-1, 1}, we can use xnor. 

> Σixiyi  = N - 2*bitcount(xnor(xi, yi)), xi, yi ∈ {-1, 1} ∀ i

Should it be **xor** instead of xnor?
For instance, x := -1 -1 -1 and y := 1 1 1, the dot product is -3 and N - 2*bitcount(xnor(x, y)) is 3 - 2\*0 = 3.
Thus, the equivalence is not hold. Is there any misunderstanding?
Thanks!",hello xi use xi xi instead instance dot product thus equivalence hold misunderstanding thanks,issue,negative,positive,positive,positive,positive,positive
284922984,The new one uses transpose and NCHW format. The old one uses NHWC format.,new one transpose format old one format,issue,negative,positive,positive,positive,positive,positive
284922567,"The new one does this tf.transpose(image, [0, 3, 1, 2]) transformation. The old version didnt and the model was produced without this transformation. 
In the new version, when reading the old model, is this transformation handled somewhere internally now?",new one image transformation old version didnt model produced without transformation new version reading old model transformation handled somewhere internally,issue,negative,positive,neutral,neutral,positive,positive
284913371,"The new version does equivalent computations, but faster. You don't need to do anything.",new version equivalent faster need anything,issue,negative,positive,positive,positive,positive,positive
284824470,That fixed it and is running properly now. Thanks so much for your quick help and for your impressive contributions to the community. ,fixed running properly thanks much quick help impressive community,issue,positive,positive,positive,positive,positive,positive
284823296,If you move it out you have to move the index file as well. That's tensorflow requirement.,move move index file well requirement,issue,negative,neutral,neutral,neutral,neutral,neutral
284822894,"The following error was returned:

shannon@frolfcuda:~/Desktop/tensorpack/examples/A3C-Gym$ ENV=Breakout-v0; ./run-atari.py --load model-24000.data-00000-of-00001 --env ""$ENV"" --episode 10 --output /home/shannon/Desktop/trashoutput
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
[0307 14:04:13 @run-atari.py:97] Environment Name: Breakout-v0
[2017-03-07 14:04:13,878] Making new env: Breakout-v0
[0307 14:04:13 @varmanip.py:162] WRN [SaverRestore] ./model-24000.data-00000-of-00001 is corrected to ./model-24000 when restoring the model.
Traceback (most recent call last):
  File ""./run-atari.py"", line 106, in <module>
    session_init=SaverRestore(args.load),
  File ""/home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages/tensorpack/tfutils/sessinit.py"", line 90, in __init__
    model_path = get_checkpoint_path(model_path)
  File ""/home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages/tensorpack/tfutils/varmanip.py"", line 164, in get_checkpoint_path
    assert os.path.isfile(model_path) or os.path.isfile(model_path + '.index'), model_path
AssertionError: ./model-24000
",following error returned load episode output successfully library locally successfully library locally successfully library locally successfully library locally successfully library locally environment name making new corrected model recent call last file line module file line file line assert,issue,positive,positive,positive,positive,positive,positive
284817850,"Just --load your `model-xxx.data-xxx` file.
They are not the same format, but `--load` option in the script takes care of both.",load file format load option script care,issue,negative,neutral,neutral,neutral,neutral,neutral
284798440,"I was using the proper pip that was associated with the virtual environment the whole time as evidenced from 'which pip' occasionally.

I have managed to get tensorpack working properly in the virtual environment after a number of uninstalls and reinstalls.  I believe it was resolved by omitting the -user as recommended above.

Thanks for your help!, and you may consider appending the main readme re: installation with noting conda users of virtual environment should omit the --user",proper pip associated virtual environment whole time pip occasionally get working properly virtual environment number believe resolved thanks help may consider main installation virtual environment omit user,issue,positive,positive,neutral,neutral,positive,positive
284673905,"The problem seems to be:
1. don't use `--user` in anaconda
2. setuptools in anaconda is too old. I upgrade it with `pip install -U setuptools`.",problem use user anaconda anaconda old upgrade pip install,issue,negative,positive,neutral,neutral,positive,positive
284644645,Check the stackoverflow link. Does `which pip` give you the correct pip location?,check link pip give correct pip location,issue,negative,neutral,neutral,neutral,neutral,neutral
284588284,"I have since tried uninstalling both from within the environment and outside of it, reinstalling within, etc.   It claims to install successfully, but when using conda list, the package does not come up. And cannot be imported.

But it can be installed, but when it is uninstalled, it shows that the directory is /.local/lib/python3.5/site-packages/tensorpack-0.1.6-py3.5.egg-info

Which is not the virtual environment directory where other packages become installed to and can be uninstalled from.",since tried within environment outside within install successfully list package come uninstalled directory virtual environment directory become uninstalled,issue,negative,positive,positive,positive,positive,positive
284585318,"Also I just created a new environment with conda, same error.

- created brand new environment using conda create -n packin python=3.5
-launched environment

- installed opencv via 'conda install opencv'

-installed tensorflow 1.0 via 'pip install tensorflow-gpu'

- installed tensorpack via 'pip install --user -U git+https://github.com/ppwwyyxx/tensorpack.git'  and received the following output:

(packin) shannon@frolfcuda:~/tensorpack/examples/A3C-Gym$ pip install --user -U git+https://github.com/ppwwyyxx/tensorpack.git
Collecting git+https://github.com/ppwwyyxx/tensorpack.git
  Cloning https://github.com/ppwwyyxx/tensorpack.git to /tmp/pip-qf6uz9ca-build
  Ignoring subprocess32: markers 'python_version < ""3.0""' don't match your environment
  Ignoring functools32: markers 'python_version < ""3.0""' don't match your environment
Collecting numpy (from tensorpack==0.1.6)
  Using cached numpy-1.12.0-cp35-cp35m-manylinux1_x86_64.whl
Requirement already up-to-date: six in /home/shannon/anaconda3/envs/packin/lib/python3.5/site-packages (from tensorpack==0.1.6)
Collecting termcolor (from tensorpack==0.1.6)
Collecting tqdm>4.11.1 (from tensorpack==0.1.6)
  Using cached tqdm-4.11.2-py2.py3-none-any.whl
Collecting msgpack-python (from tensorpack==0.1.6)
Collecting msgpack-numpy (from tensorpack==0.1.6)
  Using cached msgpack_numpy-0.3.9-py2.py3-none-any.whl
Collecting pyzmq (from tensorpack==0.1.6)
  Downloading pyzmq-16.0.2-cp35-cp35m-manylinux1_x86_64.whl (3.0MB)
    100% |████████████████████████████████| 3.0MB 495kB/s 
Installing collected packages: numpy, termcolor, tqdm, msgpack-python, msgpack-numpy, pyzmq, tensorpack
  Found existing installation: tensorpack 0.1.6
    Uninstalling tensorpack-0.1.6:
      Successfully uninstalled tensorpack-0.1.6
  Running setup.py install for tensorpack ... done
Successfully installed msgpack-numpy-0.3.9 msgpack-python-0.4.8 numpy-1.12.0 pyzmq-16.0.2 tensorpack-0.1.6 termcolor-1.1.0 tqdm-4.11.2

------

However when I run 'conda list' in this environment, tensorpack is not in the list now?

Also of course same general error of not importing with no module found.

",also new environment error brand new environment create environment via install via install via install user received following output pip install user match environment match environment requirement already six collected found installation successfully uninstalled running install done successfully however run list environment list also course general error module found,issue,negative,positive,positive,positive,positive,positive
284578200,"running your pip command again from within the virtual environment i wish to use ( universe-starter-agent ), yielded an uninstall and reinstall, then again failing to import (even being run from the tensorpack/examples/A3C-Gym directory)

-----

(universe-starter-agent) shannon@frolfcuda: ~/tensorpack/examples/A3C-Gym$  pip install --user -U git+https://github.com/ppwwyyxx/tensorpack.git
Collecting git+https://github.com/ppwwyyxx/tensorpack.git
  Cloning https://github.com/ppwwyyxx/tensorpack.git to /tmp/pip-vtqoc3px-build
  Ignoring subprocess32: markers 'python_version < ""3.0""' don't match your environment
  Ignoring functools32: markers 'python_version < ""3.0""' don't match your environment
Requirement already up-to-date: numpy in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)
Requirement already up-to-date: six in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)
Requirement already up-to-date: termcolor in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)
Requirement already up-to-date: tqdm>4.11.1 in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)
Requirement already up-to-date: msgpack-python in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)
Requirement already up-to-date: msgpack-numpy in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)
Requirement already up-to-date: pyzmq in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)
Installing collected packages: tensorpack
  Found existing installation: tensorpack 0.1.6
    Uninstalling tensorpack-0.1.6:
      Successfully uninstalled tensorpack-0.1.6
  Running setup.py install for tensorpack ... done
Successfully installed tensorpack-0.1.6
(universe-starter-agent) shannon@frolfcuda:~/tensorpack/examples/A3C-Gym$ python


Python 3.5.3 |Continuum Analytics, Inc.| (default, Feb 22 2017, 21:13:27) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorpack
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named 'tensorpack'
",running pip command within virtual environment wish use reinstall failing import even run directory pip install user match environment match environment requirement already requirement already six requirement already requirement already requirement already requirement already requirement already collected found installation successfully uninstalled running install done successfully python python analytics default red hat type help copyright license information import recent call last file line module module,issue,positive,positive,positive,positive,positive,positive
284577084,"Could you post the log of the pip command?
Also, anaconda probably itself does something tricky about packages and import path, though I never used it. I may try it later to see if I can reproduce the problem. 
Meanwhile could you make sure you're using both pip and python from anaconda but not from the system? http://stackoverflow.com/questions/21201003/anaconda-not-finding-my-packages-installed-with-pip",could post log pip command also anaconda probably something tricky import path though never used may try later see reproduce problem meanwhile could make sure pip python anaconda system,issue,negative,positive,positive,positive,positive,positive
284515005,"You are correct. 
I re-checked and its a mistake on my part. I was comparing accuracies across different resnet depth runs.",correct mistake part across different depth,issue,negative,neutral,neutral,neutral,neutral,neutral
284391829,"i fixed the shifts to be random. also to make sure 90 deg rotations are pixel exact for odd and even canvas sizes, i made two small changes:

1. use np.round before int in largest_rotated_rect()
2. subtract 0.5 from center param of getRotationMatrix2D
",fixed random also make sure deg exact odd even canvas size made two small use subtract center param,issue,negative,negative,neutral,neutral,negative,negative
284362345,"OK, I was thinking about how I would use it, but keeping the size seems useful as well. You can keep it this way.",thinking would use keeping size useful well keep way,issue,positive,positive,positive,positive,positive,positive
284361551,"i think that rotating (w,h) to (w,h) is indeed the expected behavior, since many algorithms assume a fixed input size. let me know your decision if you want a separate rot90, and if rot90 should have a different behavior. thanks!",think rotating indeed behavior since many assume fixed input size let know decision want separate rot rot different behavior thanks,issue,negative,positive,positive,positive,positive,positive
284307437,"It is supposed to be exactly the continuation of the last saved checkpoint. 
Could you provide more information (command, logs)?",supposed exactly continuation last saved could provide information command,issue,negative,positive,positive,positive,positive,positive
284205275,"You're right! Thanks!
However, changing the code may affect the claimed accuracy of my pre-trained models (and I don't want to train again), so I'll run some tests before fixing this.

UPDATE: code and models were corrected.",right thanks however code may affect accuracy want train run fixing update code corrected,issue,negative,positive,positive,positive,positive,positive
284195571,"You probably are bounded by data loading. It's very hard to feed 8 TitanX Pascal, it will be blocked either by hard disk or CPU preprocessing speed.
You can either reduce disk pressure or use less preprocessing depending on what's the bottleneck. Or you may use better hardware.
Some relevant information at http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html.

You can also use pure symbolic input as what the official examples are using. They should be more efficient as everything is written in C++.
 #202 is tracking more solutions to this problem.",probably bounded data loading hard feed blocked either hard disk speed either reduce disk pressure use le depending bottleneck may use better hardware relevant information also use pure symbolic input official efficient everything written problem,issue,negative,positive,positive,positive,positive,positive
283886337,"OK. I thought this problem was fixed by tensorflow already. Turns out it still happens on some systems.

I'll update some examples to avoid this.",thought problem fixed already turn still update avoid,issue,negative,positive,neutral,neutral,positive,positive
283885884,"After adding `import cv2` at the first line of `cifar10-resnet.py` and `cifar1-convnet.py`, the problem was solved!

Thank you very much!",import first line problem thank much,issue,negative,positive,positive,positive,positive,positive
283884585,"If the above code crashes, could you import cv2 at the first line and try again?",code could import first line try,issue,negative,positive,positive,positive,positive,positive
283884439,"Then `cifar-convnet.py` should be equivalent to the following:
```python
import tensorflow as tf
import argparse
import numpy as np
import os
from tensorpack import *
import tensorpack.tfutils.symbolic_functions as symbf
from tensorpack.tfutils.summary import *
from tensorpack.utils.gpu import get_nr_gpu
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
with tf.Graph().as_default():
    logger.auto_set_dir()
    ds = dataset.Cifar10('train')
```",equivalent following python import import import import o import import import import,issue,negative,neutral,neutral,neutral,neutral,neutral
283884169,"It happened when loading training data.

It made no effect with removing the prefetch and augmentors. ",loading training data made effect removing,issue,negative,neutral,neutral,neutral,neutral,neutral
283882249,"Did the crash happen when loading training data or validation/test data?
If the latter, could you remove the prefetch (or even the augmentors) in cifar-convnet.py to see if it segfaults?",crash happen loading training data data latter could remove even see,issue,negative,neutral,neutral,neutral,neutral,neutral
283879648,"It works by running:

```
python2 -c ""from tensorpack import dataset; c = dataset.Cifar10('train')"" 
```

Output:

```
...
9998
9999
/home/swoda/tensorpack_data/cifar10_data/cifar-10-batches-py/data_batch_5 50000
read_cifar done
```",work running python import output done,issue,negative,neutral,neutral,neutral,neutral,neutral
283879148,"If this is where it crashes I have no clue now. 
```
python -c ""from tensorpack import dataset; c = dataset.Cifar10('train')"" 
```
should also crash for you if it really breaks there.",clue python import also crash really,issue,negative,positive,positive,positive,positive,positive
283878377,"I added some `print` in the original code:

```
# tensorpack/dataflow/dataset/cifar.py

...
        for k in range(IMG_NUM):
            img = data[k].reshape(3, 32, 32)
            img = np.transpose(img, [1, 2, 0])
            ret.append([img, label[k]])
            #print k
        print fname, len(ret)
    print ""read_cifar done""
```

Output: 
```
...
/home/swoda/tensorpack_data/cifar10_data/cifar-10-batches-py/data_batch_1 10000
/home/swoda/tensorpack_data/cifar10_data/cifar-10-batches-py/data_batch_2 20000
/home/swoda/tensorpack_data/cifar10_data/cifar-10-batches-py/data_batch_3 30000
/home/swoda/tensorpack_data/cifar10_data/cifar-10-batches-py/data_batch_4 40000
Segmentation fault (core dumped)
```

Then, I printed the `k`:

```
...
5105
5106
5107
5108
5109
Segmentation fault (core dumped)
```

It seems that the the location of the error is different sometimes.",added print original code range data label print print ret print done output segmentation fault core printed segmentation fault core location error different sometimes,issue,negative,positive,positive,positive,positive,positive
283876847,"<del> Could you post the full log so I can see which stage it was in?</del>
Once it starts reading data, it starts using the TF session as well.

If you compile opencv yourself, it's likely you've enabled CUDA or OpenCL support in opencv which will cause segfault. Try running without GPU to see it happens.",could post full log see stage reading data session well compile likely support cause try running without see,issue,positive,positive,positive,positive,positive,positive
283876635,"In fact, I traced debug the problem, it's just a problem of data reading. (The program was still in the data loading stage before the training phase, when it occurred.)

----

I installed the OpenCV according to the guide: [OpenCV: Installation in Linux](http://docs.opencv.org/3.1.0/d7/d9f/tutorial_linux_install.html#gsc.tab=0).",fact problem problem data reading program still data loading stage training phase according guide installation,issue,negative,neutral,neutral,neutral,neutral,neutral
283876513,Was it? I don't remember I've saved the epoch number in checkpoint before.,remember saved epoch number,issue,negative,neutral,neutral,neutral,neutral,neutral
283876343,Wasn't there already an older version of tensorpack that saved the epoch number an global step into the checkpoint file?,already older version saved epoch number global step file,issue,negative,positive,neutral,neutral,positive,positive
283876022,"How do you install opencv?
And could you try running without GPU to see if the error occurs? (use CUDA_VISIBLE_DEVICES=)",install could try running without see error use,issue,negative,neutral,neutral,neutral,neutral,neutral
283875875,"Yes. An ""epoch"" can have different semantics to different people.

But anyway, I think it's good if I could add a feature to (optionally) automatically recover the epoch number from log. I'll see what can be done.",yes epoch different semantics different people anyway think good could add feature optionally automatically recover epoch number log see done,issue,positive,positive,positive,positive,positive,positive
283875009,"I tried the another example `cifar-convnet.py`, but the same problem occurred.  Before the `cifar`,  I have test the example `HED`, it runs well. 

I re-downloaded the file `cifar-10-python.tar.gz`  to exclude that the file is damaged.

---

I have installed OpenCV 3.1, CUDA 8.0, cuDNN 5.1( without using anaconda ).",tried another example problem test example well file exclude file without anaconda,issue,negative,neutral,neutral,neutral,neutral,neutral
283874745,"OK .  Tensorflow is a flexible tool,  but there are still not have a unified coding standard。 thanks for your contributions very well,",flexible tool still unified thanks well,issue,positive,positive,positive,positive,positive,positive
283874116,And what epoch number it prints usually doesn't have any effect to the training. The only difference it may make is when you use a epoch-number-based learning rate setter.,epoch number usually effect training difference may make use learning rate setter,issue,negative,negative,negative,negative,negative,negative
283873033,"I couldn't reproduce this problem. Note that there are multiple threads running so it's unlikely it is a problem with data reading, but more likely to be in training.

Could you try if the simple examples (mnist-convnet.py, cifar-convnet.py) can run?
And how do you install opencv? Are you using anaconda?",could reproduce problem note multiple running unlikely problem data reading likely training could try simple run install anaconda,issue,negative,negative,negative,negative,negative,negative
283872986,"Yes, I means that when I restore a model in epoch N, the program is start with N, but the print is start epoch 1, I think the print should be change to N. (sorry for my poor english)",yes restore model epoch program start print start epoch think print change sorry poor,issue,negative,negative,negative,negative,negative,negative
283872427,"Yes. If you want to start from a different epoch number, you can use the [starting_epoch](http://tensorpack.readthedocs.io/en/latest/modules/train.html#tensorpack.train.TrainConfig) option.",yes want start different epoch number use option,issue,positive,neutral,neutral,neutral,neutral,neutral
283872239,"thanks, when i use --load, the parameters are restored by a checkpoint at 70 epoch, but  the start epoch is reset to 1. ",thanks use load epoch start epoch reset,issue,negative,positive,positive,positive,positive,positive
283870662,"I see. What's the solution for now? Use TF 0.12?

> On Mar 2, 2017, at 9:05 PM, Yuxin Wu <notifications@github.com> wrote:
> 
> Sorry, looking at the comment in tensorflow/tensorflow#7038, it looks like 1.0 is still not new enough.
> 
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",see solution use mar wrote sorry looking comment like still new enough reply directly view mute thread,issue,positive,negative,neutral,neutral,negative,negative
283866568,"Sorry, looking at the comment in https://github.com/tensorflow/tensorflow/issues/7038, it looks like 1.0 is still not new enough. Maybe you have to use the nightly version or a custom build.",sorry looking comment like still new enough maybe use nightly version custom build,issue,negative,negative,negative,negative,negative,negative
283857001,"Most examples have a `--load` option which can restore from a saver.
Basically set the `session_init` option in your training config, to `SaverRestore(...)`
",load option restore saver basically set option training,issue,negative,neutral,neutral,neutral,neutral,neutral
283848582,I think its better to add a choice to logger.auto_set_dir() to restore the last checkpoint,think better add choice restore last,issue,positive,positive,positive,positive,positive,positive
283690582,"For the second one we want random 90 deg rotations to 0, 90, 180, 270

I will make a PR

On Mar 2, 2017 5:39 PM, ""Yuxin Wu"" <notifications@github.com> wrote:

> The second one is just imgaug.MapImage(lambda x: cv2.transpose(x)) ?
>
> The first one does look like something others may need, but this probably
> won't come to top of my todo list very soon. Contributions are welcome.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/170#issuecomment-283688107>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AEOzC4T4Lqp6aQ3KKDxvhZwvblcdGlHJks5rhuKtgaJpZM4MRGK_>
> .
>
",second one want random deg make mar wrote second one lambda first one look like something may need probably wo come top list soon welcome thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
283688107,"The second one is just `imgaug.MapImage(lambda x: cv2.transpose(x))` ?

The first one does look like something others may need, but this probably won't come to top of my todo list very soon. Contributions are welcome.",second one lambda first one look like something may need probably wo come top list soon welcome,issue,positive,positive,positive,positive,positive,positive
283662574,"The shape constant is not very helpful in the script, because the network definitely need some changes if working with a different shape.
Simple extension is to add one more layer and use 512x512 images.",shape constant helpful script network definitely need working different shape simple extension add one layer use,issue,positive,neutral,neutral,neutral,neutral,neutral
283662029,"fastest response ever!
better use your SHAPE parameter instead of hard-coding 256.
btw, do you think pix2pix could work well for larger images if we change say, 256->512 and 286->572. or would you also change something in the architecture?

",response ever better use shape parameter instead think could work well change say would also change something architecture,issue,positive,positive,positive,positive,positive,positive
283654199,Oh I see. Will add it soon.,oh see add soon,issue,negative,neutral,neutral,neutral,neutral,neutral
283604608,rc1 is old. You can use the 1.0 release version.,old use release version,issue,negative,positive,neutral,neutral,positive,positive
283598740,"This problem doesnt seem to be fixed. I am working with TF 1.0.0rc1 and HEAD version of this repo. 
Here is what I am getting after first epoch:
```
236 ^[[32m[0227 18:27:56 @base.py:160]^[[0m Start Epoch 1 ...
 237 ^[[32m[0227 18:48:56 @base.py:168]^[[0m Epoch 1 (global_step 5000) finished, time:1259.80 sec.
 238 ^[[32m[0227 18:48:57 @saver.py:65]^[[0m Model saved to resnet34BaselineTrain/model-5000.
 239 ^[[32m[0227 18:54:41 @group.py:43]^[[0m Callbacks took 345.025 sec in total. InferenceRunner: 343.555sec
 240 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m input_queue_size: 50
 241 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m l2_regularize_loss: 0.48794
 242 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m learning_rate: 0.1
 243 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m train-error-top1: 0.96621
 244 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m train-error-top5: 0.87891
 245 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m val-error-top1: 0.96124
 246 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m val-error-top5: 0.87464
 247 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m xentropy-loss: 5.6556
```
The final error is also off: 
```
1548 ^[[32m[0301 18:02:54 @base.py:160]^[[0m Start Epoch 110 ...
1549 ^[[32m[0301 18:23:36 @base.py:168]^[[0m Epoch 110 (global_step 550000) finished, time:1241.72 sec.
1550 ^[[32m[0301 18:23:37 @saver.py:65]^[[0m Model saved to resnet34BaselineTrain/model-550000.
1551 ^[[32m[0301 18:29:02 @group.py:43]^[[0m Callbacks took 326.282 sec in total. InferenceRunner: 325.909sec
1552 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m input_queue_size: 50
1553 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m l2_regularize_loss: 0.55047
1554 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m learning_rate: 1e-05
1555 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m train-error-top1: 0.33629
1556 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m train-error-top5: 0.1454
1557 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m val-error-top1: 0.3068
1558 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m val-error-top5: 0.11092
1559 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m xentropy-loss: 1.414
1560 ^[[32m[0301 18:29:02 @input_data.py:124]^[[0m EnqueueThread Exited.
```
When I had done this run a while back using TF 0.12 this is what I was getting after the first epoch:
```
254 ^[[32m[1227 12:08:31 @timer.py:46]^[[0m Epoch 1 (global_step 5000) finished, time:1432.42sec.
 255 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m cost: 4.6151
 256 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m input_queue_size: 48.462
 257 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m l2_regularize_loss: 0.62589
 258 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m learning_rate: 0.1
 259 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m train-error-top1: 0.80657
 260 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m train-error-top5: 0.55757
 261 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m val-error-top1: 0.76442
 262 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m val-error-top5: 0.5148
 263 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m xentropy-loss: 3.9892
```
",problem doesnt seem fixed working head version getting first epoch start epoch epoch finished time sec model saved took sec total final error also start epoch epoch finished time sec model saved took sec total done run back getting first epoch epoch finished time cost,issue,negative,positive,neutral,neutral,positive,positive
282929440,"Not currently. But you don't have to make something __into__ tensorpack to use it. 
It should be very easy to implement (just use `sklearn.metric.auc`).",currently make something use easy implement use,issue,negative,positive,positive,positive,positive,positive
282653002,"maybe it will cause small performance degradation. there are other differences compared to caffe as well.
duplicate of #54.",maybe cause small performance degradation well duplicate,issue,negative,negative,negative,negative,negative,negative
282544458,"So you are using monitors? They will be deprecated:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/contrib.learn.monitors.md

> DEPRECATED FUNCTION
> THIS FUNCTION IS DEPRECATED. It will be removed after 2016-12-05. Instructions for updating: > Monitors are deprecated. Please use tf.train.SessionRunHook.

https://www.tensorflow.org/versions/master/api_docs/python/contrib.learn.monitors/other_functions_and_classes

> Monitor is deprecated in favor of SessionRunHook. If you're using a monitor, you can wrap it with a hook using function. It is recommended to implement hook version of your monitor.",function function removed please use monitor favor monitor wrap hook function implement hook version monitor,issue,positive,neutral,neutral,neutral,neutral,neutral
282531549,"To not producing any train-logs, just remove ModelSaver from callbacks, and do not set logger dir. The rest should run as usual. (effective after 000b2ea328dd35e60c)

You can just add your action to the argument parser, and both `auto_set_dir` and `set_logger_dir` have the option `action`.",remove set logger rest run usual effective add action argument parser option action,issue,negative,positive,positive,positive,positive,positive
282495866,"Adding an extra name like this doesn't help: `dirname + label` can still be an existing directory.

Moreover, since you have a python variable already containing the actual name of the log directory, why still use `auto_set_dir` instead of calling `set_logger_dir` directly? `auto_set_dir` is just to automatically generate a name which ""probably is what you want"", saving just one line of code.",extra name like help label still directory moreover since python variable already actual name log directory still use instead calling directly automatically generate name probably want saving one line code,issue,positive,positive,neutral,neutral,positive,positive
282493946,"I think these are two different information:
- user-defined label
- timestamp from current run

Maybe something like `[scriptname]-[userlabel | default:""default""]-[timestamp]` as the subdir in train_log?",think two different information label current run maybe something like default default,issue,negative,neutral,neutral,neutral,neutral,neutral
282493816,"I do not really like the interactive prompt. I would do the following way #164. There should also be a way to not produce any train-logs (something like a debug-mode). Is there any elegant way to handle the entire thing ""keep/new/backup/delete"" via command line arguments?",really like interactive prompt would following way also way produce something like elegant way handle entire thing via command line,issue,positive,positive,positive,positive,positive,positive
282493317,"OK. I think we can ask for a new name when ""new"" is chosen.",think ask new name new chosen,issue,negative,positive,positive,positive,positive,positive
282448225,"Here, a minibatch was trained after obtaining 4 new transition tuples. This is the ""update frequency"" parameter in the original paper.
So then I guess you should expect 12500000 steps to be equivalent to 200M frames in the paper, assuming all parameters (batch size, etc) are consistent with the paper.",trained new transition update frequency parameter original paper guess expect equivalent paper assuming batch size consistent paper,issue,negative,positive,positive,positive,positive,positive
282426655,"Thanks for your reply. After reading the human level control papaer, I think that in ddqn case. The frame means adding a new transition tuple   (s,a,r,s'). Thus, each step is 4 frames (action repeat). after each step(observation), the system updates itself by a minibatch(32). 

Therefore, in our case, I think that we still need to run 50,000,000 steps (5000 epochs ) to get  the result. Just to confirm that in tensorpack, we train a minibatch after obtaining a new single observation, am I right?",thanks reply reading human level control think case frame new transition thus step action repeat step observation system therefore case think still need run get result confirm train new single observation right,issue,negative,positive,positive,positive,positive,positive
282193128,"Theoretically you can use any symbolic functions. This includes Keras layers as long as you can call it on a tensorflow tensor,  e.g.:
```python
from keras.layers import Dense
...
logits = Dense(10, activation=None)(logits)    # logits is a tf.Tensor
```

But there are some issues:
1. If you use any keras layers **with variables** in tensorpack, none of the inference functionality will work anymore (including validation on test set every epoch) , because keras doesn't respect `tf.variable_scope`. MultiGPU won't work either because of the same reason.
2. If you use any keras layers which has different training/testing behavior (e.g. dropout), keras require an extra ""is_training"" tensor to be fed into the graph. You'll need to add a callback:
```python
class KerasHook(Callback):
    def _before_run(self, ctx):
        return tf.train.SessionRunArgs(fetches=[],feed_dict={'keras_learning_phase:0':1})
```
This will feed an extra tensor for every training iterations.

UPDATE:
It looks like keras does weight sharing by calling the same keras layer multiple times: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html . Though it's not compatible with tensorflow variable scope, maybe inference & multiGPU training can still work if you save the layer object in the model, and reuse the layer in subsequent calls to `_build_graph`. I'll test whether this would work when I have time.",theoretically use symbolic long call tensor python import dense dense use none inference functionality work validation test set every epoch respect wo work either reason use different behavior dropout require extra tensor fed graph need add python class self return feed extra tensor every training update like weight calling layer multiple time though compatible variable scope maybe inference training still work save layer object model reuse layer subsequent test whether would work time,issue,positive,negative,neutral,neutral,negative,negative
281867081,Two possible methods were mentioned in the comment of this issue: https://github.com/NVlabs/GA3C/issues/3,two possible comment issue,issue,negative,neutral,neutral,neutral,neutral,neutral
281583773,"Sorry, I missed the requirement >= 1.0
I switch to the tf1.0, all things goes well. 
thanks.",sorry requirement switch go well thanks,issue,positive,negative,negative,negative,negative,negative
281273616,"It's either a hardware or a cuda/driver problem. That's not something I can solve.
Possible things you can try:
upgrade/reinstall cuda or driver
run with only 1 of the GPU (with CUDA_VISIBLE_DEVICES=0 or 1)
use a newer version of tensorflow (0.12 is not supported by tensorpack) 
reboot",either hardware problem something solve possible try driver run use version,issue,negative,neutral,neutral,neutral,neutral,neutral
281133284,Probably it is better to merge this now to prevent manually resolving merge conflicts.,probably better merge prevent manually merge,issue,negative,positive,positive,positive,positive,positive
281089034,"Actually ""datapoint"" was the term I planned to use, on anything yielded by a dataflow. It might not be a good name though..",actually term use anything might good name though,issue,negative,positive,positive,positive,positive,positive
281083359,"An update after email communication with the user:
on some systems `pip install --user -U pip setuptools` is needed to update pip&setuptools. Otherwise you may see UNKNOWN as the package name.",update communication user pip install user pip update pip otherwise may see unknown package name,issue,negative,negative,neutral,neutral,negative,negative
280999272,I think `setup_graph` and `before_train` are better names than `begin` and `after_create_session`. Won't do more renaming.,think better begin wo,issue,negative,positive,positive,positive,positive,positive
280969893,"It would be very ideal to use WGAN under multiGPU automatically, but I don't how I could do that. 

The boundary is still not clear in what an optimizer should do.
SyncMultiGPUTrainer produces one train_op in the end, should it use optimizer to implement the logic? AsyncMultiGPUTrainer uses several different train_ops, should it be in trainer?

There are ways to put the ratio into the graph with some loop operators, but it may look very complicated (not sure about how simple it can be made). And in general users shouldn't be forbidden writing python loops inside run_step.",would ideal use automatically could boundary still clear one end use implement logic several different trainer way put ratio graph loop may look complicated sure simple made general forbidden writing python inside,issue,positive,positive,positive,positive,positive,positive
280948134,"I was in the middle of nowhere for some days with bad internet connection and missed several commits of tensorpack.

I think the trainer is related to the infra-structure: multi-gpu, multi-device, sync, asyn, QueueInput, TensorInput and a good optimizer should not know which hardware is used or where the data is coming from. An optimizer should provide a `compute_gradients` or `train_op`.
In the ideal case any GANoptimizer, WGANoptimizer should work under all Trainers, which is currently not the case.

So clipping and run multiple steps can be part of the optimizer itself, like you [already did](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/GAN/WGAN-CelebA.py#L40-L51).

I would argue that the ratio to two optimization ops in the GAN should be part of the model. I will let you know if the is a discussable alternative, as the current implementation is understandable.",middle nowhere day bad connection several think trainer related sync good know hardware used data coming provide ideal case work currently case clipping run multiple part like already would argue ratio two optimization gan part model let know discussable alternative current implementation understandable,issue,positive,positive,positive,positive,positive,positive
280929649,"It seems good to just use `SessionRunHook` to implement callback. Probably will also rename `extra_fetches` and `trigger_step` to `before_run` and `after_run` with similar API.

1. Calling `before_run` every step doesn't have visible overhead.
2. The way `_HookedSession` is implemented is better: it injects the `run` call directly, so the hook is transparent. However with how callback is implemented, you need to use `get_extra_fetches()` and return the results in every `run_step` call.
3. I'll keep both `trainer.sess` and `trainer.monitored_sess` (or add an alias to `trainer.hooked_sess`), so that a custom trainer can choose whether to trigger the hooks in a specific run, just like how it is done in WGAN now. 

A tricky issue:
4. It's better to allow users to trigger the hooks multiple times in one `run_step` call. 
Then how should `global_step` be updated? This is tricky under the assumption that `global_step` variable should be consistent with `trainer.global_step` as well as `steps_per_epoch`.
When you implement a function `run_step`, you should expect what's inside is really one `step`, no matter how many `hooked_session.run()` get called inside. 
But then the problem becomes: some need to be triggered once for each `session.run` call, but some (e.g., global step increment, progress bar) need to be triggered once for each `run_step` call. How to make this happen from design?",good use implement probably also rename similar calling every step visible overhead way better run call directly hook transparent however need use return every call keep add alias custom trainer choose whether trigger specific run like done tricky issue better allow trigger multiple time one call tricky assumption variable consistent well implement function expect inside really one step matter many get inside problem becomes need triggered call global step increment progress bar need triggered call make happen design,issue,positive,positive,positive,positive,positive,positive
280923017,But this implementation uses different (larger) network architecture and different batch size. To reproduce the score (not the number of frames) on breakout game you'll only need to run it for about one day. Haven't tested a lot of games.,implementation different network architecture different batch size reproduce score number breakout game need run one day tested lot,issue,negative,negative,negative,negative,negative,negative
280854424,"Seeing 30% improvement on ResNet-ImageNet.
However Image2Image becomes slower if changed to NCHW. (maybe deconv is not fast? haven't tested a lot)",seeing improvement however becomes maybe fast tested lot,issue,negative,positive,positive,positive,positive,positive
280850850,"Currently only added for the most common layers.
Saw 30~40% speed up on cifar10 resnet. Most examples are still left unchanged now.",currently added common saw speed still left unchanged,issue,negative,negative,neutral,neutral,negative,negative
280836152,"The most likely reason this happens to you is that you installed different versions of some packages at different places and they got messed up with wrong environment variables or other improper settings. You'll need to google more.
A suggestion is to never run `sudo pip install` on anything. 
One way that might solve your current problem is to `sudo pip uninstall` everything under /usr/local/lib/python2.7/dist-packages/. Not sure if it will work.",likely reason different different got wrong environment improper need suggestion never run pip install anything one way might solve current problem pip everything sure work,issue,negative,neutral,neutral,neutral,neutral,neutral
280829944,"No. Have you make sured that the network takes the same input format (rgb/bgr, value range, etc) and output class id? It may not be the same as what's used by the code here.

You'd better eval the model before training on it.",make network input format value range output class id may used code better model training,issue,positive,positive,positive,positive,positive,positive
280828094,"@ppwwyyxx Did you continue the training process for this network?

I was re-training, starting from the checkpoint and (1) the process is very slow 0.70it/sec on 8 GPUs and (2) the error rate is quite bad - 0.98 top1 and 0.84 top5 after 2 epochs. 
The only change to the code I did from the ones I attached last was to change 1000 to 1001 . Learning rate is 1e-6.

Any idea what is going on?
",continue training process network starting process slow error rate quite bad top top change code attached last change learning rate idea going,issue,negative,positive,neutral,neutral,positive,positive
280717930,"Sure, I was only trying to provide a helpful reference. :-)",sure trying provide helpful reference,issue,positive,positive,positive,positive,positive,positive
280533184,"TensorFlow doesn't know what is a layer, what is forward/backward. So there is no way doing this.

It does have a lower-level profiling which I may integrate.",know layer way may integrate,issue,negative,neutral,neutral,neutral,neutral,neutral
280460271,"Closing this.

1. If you could look into why the checkpoint loading is slow that would be helpful. This is a performance aspect not functionality. I am able to get the experiment running.

2. I had to change the num_classes to 1001 (line 94/95) to get this running. I thought imagenet has 1000 classes. The train folder has 1000 folders. Where is the additional 1 class/folder?

",could look loading slow would helpful performance aspect functionality able get experiment running change line get running thought class train folder additional,issue,negative,positive,neutral,neutral,positive,positive
280318917,This issue is about re-implementing the mentioned distributed version within TensorPack and not adding more dependencies. Probably the most straight-forward way is something like a `multiple-devices-sync-trainer`.,issue distributed version within probably way something like,issue,negative,neutral,neutral,neutral,neutral,neutral
280268891,"Inception consumes a lot of memory. I can run it with batch 32 per GPU but not 64, Google also used this number in their recent papers.",inception lot memory run batch per also used number recent,issue,negative,neutral,neutral,neutral,neutral,neutral
280254029,"Thanks a lot for looking into this and helping out. 

> On Feb 15, 2017, at 11:15 PM, Yuxin Wu <notifications@github.com> wrote:
> 
> One thing that may cause some training slow-down: google's training code handles the batch norm updates manually, by only using UPDATE_OPS from the first GPU.
> 
> With tensorpack models this was done automatically.
> But with slim models, currently UPDATE_OPS from all GPUs will be executed. This was from PR #81. Now it looks like ""applying the UPDATE_OPS blindly"" is not always what a user would want. @PatWie for comments.
> 
> One possible solution is to use slim.arg_scope to change the updates_collections option of slim.batch_norm. But I'm not familiar with slim so I'm not sure does it work.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",thanks lot looking helping wrote one thing may cause training training code batch norm manually first done automatically slim currently executed like blindly always user would want one possible solution use change option familiar slim sure work thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
280250470,"After 6640f9bbaa71bf81ecd9c7042d4ce, it can start training with multi-GPU. You should only see the following variables not found (just some summaries):
```
[0216 14:52:07 @sessinit.py:110] WRN Variable learning_rate in the graph not found in checkpoint!
[0216 14:52:08 @sessinit.py:110] WRN Variable tower0/train-error-top1/EMA in the graph not found in checkpoint!
[0216 14:52:08 @sessinit.py:110] WRN Variable tower0/cost/EMA in the graph not found in checkpoint!
[0216 14:52:08 @sessinit.py:110] WRN Variable tower0/total_cost/EMA in the graph not found in checkpoint!
[0216 14:52:08 @sessinit.py:110] WRN Variable input_queue_size/EMA in the graph not found in checkpoint!
[0216 14:52:08 @sessinit.py:110] WRN Variable tower0/train-error-top5/EMA in the graph not found in checkpoint!
[0216 14:52:08 @sessinit.py:110] WRN Variable tower0/regularize_loss/EMA in the graph not found in checkpoint!
```

Loading checkpoints with type checks (and casting) seems to slow it down a lot, because now it loads data from checkpoint to python, then to TF, instead of going to TF directly. I'll do some other tests on the speed.",start training see following found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found loading type casting slow lot data python instead going directly speed,issue,negative,negative,neutral,neutral,negative,negative
280227312,"I took at look the checkpoint. It seems to be the very old checkpoint format (before TF 0.8 maybe?) where the variable names don't contain the `:0` in the end. So there is a mismatch.
I'll fix some code to be compatible with that format.",took look old format maybe variable contain end mismatch fix code compatible format,issue,negative,positive,neutral,neutral,positive,positive
280176541,"I am able to load the checkpoint. I see WRN messages like this for every variable in the graph model:

```
...
...
Variable InceptionResnetV2/Conv2d_1a_3x3/weights in the graph not found in checkpoint!
Variable InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/beta in the graph not found in checkpoint!
Variable InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/moving_mean in the graph not found in checkpoint!
Variable InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/moving_variance in the graph not found in checkpoint!
Variable InceptionResnetV2/Conv2d_2a_3x3/weights in the graph not found in checkpoint!
Variable InceptionResnetV2/Conv2d_2a_3x3/BatchNorm/beta in the graph not found in checkpoint!
Variable InceptionResnetV2/Conv2d_2a_3x3/BatchNorm/moving_mean in the graph not found in checkpoint!
Variable InceptionResnetV2/Conv2d_2a_3x3/BatchNorm/moving_variance in the graph not found in checkpoint!

...
...

Variable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean in checkpoint not found in the graph!
Variable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance in checkpoint not found in the graph!
Variable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0b_1x3/weights in checkpoint not found in the graph!
Variable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/BatchNorm/beta in checkpoint not found in the graph!
Variable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean in checkpoint not found in the graph!
Variable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance in checkpoint not found in the graph!
Variable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/weights in checkpoint not found in the graph!
Variable InceptionResnetV2/Repeat_2/block8_9/Conv2d_1x1/biases in checkpoint not found in the graph!
Variable InceptionResnetV2/Repeat_2/block8_9/Conv2d_1x1/weights in checkpoint not found in the graph!
Variable global_step in checkpoint not found in the graph!
...

```

Is this the right behavior? I dont know if the checkpoint variables are being loaded/restored properly. Could you comment?

Attached is the log file till the point the epochs start. 

[log.log.txt](https://github.com/ppwwyyxx/tensorpack/files/778637/log.log.txt)

",able load see like every variable graph model variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph right behavior dont know properly could comment attached log file till point start,issue,negative,positive,positive,positive,positive,positive
280111150,"It may also be a good idea to move the python-side logic to ""optimizer"" (a new kind of optimizer incompatible with tf.train.Optimizer), so that trainer only handles the graph. I mentioned this because currently in GAN `_setup()` both build the graph and define the optimization ops.

But it's still not clear what is the boundary. Is multi-GPU training a kind of optimizer (because all its logic is to play with gradient stuff and make the update)? 

I still found it very hard to separate the two. So maybe I'll still let trainer do everything.. what do you think?",may also good idea move logic new kind incompatible trainer graph currently gan build graph define optimization still clear boundary training kind logic play gradient stuff make update still found hard separate two maybe still let trainer everything think,issue,positive,positive,positive,positive,positive,positive
280102004,"The clipping is added to the optimizer to avoid having a call like ""optimizer.post_update"". It's more efficient if everything is done in one op. It's also why GAN example only have one train_op.
trigger_step() would be enough for clipping if you run another op. (in this case I suppose it'll still be efficient).
But there will always be some logic cannot be easily handled by a single op, like scheduling two minimization ops in the WGAN case. 

I don't have a clear boundary between the two. Writing WGAN without the clipping optimizer (just apply the clipping manually) is also fine to me. I added the clipping optimizer because it is common (so there should be a tool for it), not because it shouldn't be in trainer.
What I had in mind is that: optimizers are just simple extensions of `tf.train.Optimizer` and should be mostly compatible with it (so it should produce an op in the end). 
Trainer does everything necessary to define what `run_step` is. It can use a slightly better optimizer if that's helpful, but in general all the non-standard logic should appear here. (Or in other words, tensorpack has done enough for you already.. take care of the rest yourself!)",clipping added avoid call like efficient everything done one also gan example one would enough clipping run another case suppose still efficient always logic easily handled single like two minimization case clear boundary two writing without clipping apply clipping manually also fine added clipping common tool trainer mind simple mostly compatible produce end trainer everything necessary define use slightly better helpful general logic appear done enough already take care rest,issue,positive,positive,positive,positive,positive,positive
280096726,"That would be the way if I would have tried to implement the changes. So the trainer just handles sync-multi-gpu, or so and each of these trainers can be applied to GANOptimizer, WGANOptimizer. Otherwise, whta is the advantage of putting the optimizer into the model, when it is just RMSprop, Adam and co?

Would be nice to hear your definition of an ""optimizer"" vs ""trainer"".",would way would tried implement trainer applied otherwise advantage model would nice hear definition trainer,issue,positive,positive,positive,positive,positive,positive
280094131,"My understanding is, that the optimizer contains all code for an update-step (probably consisting of multiple ops). And the trainer feeds data into the model, calls the optimizer and manage gpus. Then the trainer should call `optimizer.post_update` or sth like this?

So, isn't this:
https://github.com/ppwwyyxx/tensorpack/blob/master/examples/GAN/WGAN-CelebA.py#L70-L87
part of the optimizer itself?",understanding code probably multiple trainer data model manage trainer call like part,issue,negative,neutral,neutral,neutral,neutral,neutral
279943874,"In the paper it was 200M frames if I recalled correctly.
Each iteration here is a batch of 64 frames. It would take about 2.5\~3 days to run 200M frames (assume a training speed of 12\~18 it/s).",paper correctly iteration batch would take day run assume training speed,issue,negative,neutral,neutral,neutral,neutral,neutral
279935139,You should be able to load the model with the current HEAD now (don't need to remove the variable).,able load model current head need remove variable,issue,negative,positive,positive,positive,positive,positive
279923529,"Thanks. 
How does one remove a variable from a checkpoint? Simply delete that line or its more involved?

> On Feb 14, 2017, at 8:40 PM, Yuxin Wu <notifications@github.com> wrote:
> 
> The second problem is because TF 1.0 changes API of tf.concat. You should swap the argument order of tf.concat in inception_resnet_v2.py.
> 
> The first is because the checkpoint contains an unused variable ""global_step:0"" which happens to conflict with a variable tensorpack defined. In general there is no good way to solve this and it's best to remove unused variables from a checkpoint. But I'll push a change later to make it behave less strict in this case (print a warning instead of crash).
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",thanks one remove variable simply delete line involved wrote second problem swap argument order first unused variable conflict variable defined general good way solve best remove unused push change later make behave le strict case print warning instead crash thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
279915362,"The second problem is because TF 1.0 changes API of tf.concat. You should swap the argument order of `tf.concat` in `inception_resnet_v2.py`.

The first is because the checkpoint contains an unused variable ""global_step:0"" which happens to conflict with a variable tensorpack defined. In general there is no good way to solve this and it's best to remove unused variables from a checkpoint. But I'll push a change later to make it behave less strict in this case (print a warning instead of crash).",second problem swap argument order first unused variable conflict variable defined general good way solve best remove unused push change later make behave le strict case print warning instead crash,issue,negative,positive,positive,positive,positive,positive
279876785,"I updated to TF 1.0.0.
Getting a different error this time.

```
Traceback (most recent call last):
  File ""tpInception_resnet_v2.py"", line 204, in <module>
    SyncMultiGPUTrainer(config).train()
  File ""/home/akm/TF/tpNew/tensorpack/tensorpack/train/base.py"", line 64, in train
    self.setup()
  File ""/home/akm/TF/tpNew/tensorpack/tensorpack/train/base.py"", line 130, in setup
    self._setup()   # subclass will setup the graph
  File ""/home/akm/TF/tpNew/tensorpack/tensorpack/train/multigpu.py"", line 133, in _setup
    self.config.tower, lambda: self._get_cost_and_grad()[1])
  File ""/home/akm/TF/tpNew/tensorpack/tensorpack/train/multigpu.py"", line 41, in _multi_tower_grads
    grad_list.append(get_tower_grad_func())
  File ""/home/akm/TF/tpNew/tensorpack/tensorpack/train/multigpu.py"", line 133, in <lambda>
    self.config.tower, lambda: self._get_cost_and_grad()[1])
  File ""/home/akm/TF/tpNew/tensorpack/tensorpack/train/feedfree.py"", line 54, in _get_cost_and_grad
    self.build_train_tower()
  File ""/home/akm/TF/tpNew/tensorpack/tensorpack/train/feedfree.py"", line 43, in build_train_tower
    f()
  File ""/home/akm/TF/tpNew/tensorpack/tensorpack/train/feedfree.py"", line 36, in f
    self.model.build_graph(inputs)
  File ""/home/akm/TF/tpNew/tensorpack/tensorpack/models/model_desc.py"", line 113, in build_graph
    self._build_graph(model_inputs)
  File ""tpInception_resnet_v2.py"", line 53, in _build_graph
    logits, end_points = inception_resnet_v2(image, is_training=is_training)
  File ""/home/akm/TF/tpNew/tensorpack/examples/IRv2/inception_resnet_v2.py"", line 169, in inception_resnet_v2
    net = tf.concat(3, [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1047, in concat
    dtype=dtypes.int32).get_shape(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 651, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 367, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got list containing Tensors of type '_Message' instead.
Prefetch process exited.
Prefetch process exited.
```

I guess something to do with the concat API. I did minor mod to the file. 

",getting different error time recent call last file line module file line train file line setup subclass setup graph file line lambda file line file line lambda lambda file line file line file line file line file line image file line net file line file line file line ret value file line return constant file line constant value file line file line mismatch type mismatch got list type instead process process guess something minor file,issue,negative,negative,neutral,neutral,negative,negative
279832949,"No, sorry. I didnt work on this. I was setting up for training rather than
resuming from a checkpoint.
I couldnt get the training process to work back then.

On Tue, Feb 14, 2017 at 12:48 PM, a-maci <notifications@github.com> wrote:

> Hi -- I am following the method you described here to restore the
> inception_resnet_v2 checkpoint but getting some errors. See issue #148
> <https://github.com/ppwwyyxx/tensorpack/issues/148>
>
> Did you face any such problems? Were you able to train the network
> end-to-end?
>
> Thanks
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/111#issuecomment-279831190>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AXspSaHmMg4b0_ZnSst5usZNkEHZIkyNks5rchMwgaJpZM4Lml0s>
> .
>
",sorry didnt work setting training rather get training process work back tue wrote hi following method restore getting see issue face able train network thanks state reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
279831190,"Hi -- I am following the method you described here to restore the inception_resnet_v2 checkpoint but getting some errors. See issue #148 

Did you face any such problems? Were you able to train the network end-to-end?

Thanks",hi following method restore getting see issue face able train network thanks,issue,positive,positive,positive,positive,positive,positive
279482912,"Yeah, there is no difference. I think I misinterpreted
> replaces the 0 and 1 targets for a classifier with smoothed values, like .9 or .1

I would go with the default formulation. This affects almost everything `ones_like, zeros_like, <, >` in
https://github.com/ppwwyyxx/tensorpack/blob/master/examples/GAN/GAN.py#L50-L64

Sorry. It should be
````python
with tf.name_scope(""GAN_loss""):
    score_real = tf.sigmoid(logits_real)
    score_fake = tf.sigmoid(logits_fake)
    tf.summary.histogram('score-real', score_real)
    tf.summary.histogram('score-fake', score_fake)

    with tf.name_scope(""discrim""):
        d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
            logits=logits_real, labels=tf.ones_like(logits_real)), name='loss_real')
        d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
            logits=logits_fake, labels=tf.zeros_like(logits_fake)), name='loss_fake')

        d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name='accuracy_real')
        d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name='accuracy_fake')

        self.d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')
        self.d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name='loss')

    with tf.name_scope(""gen""):
        self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
            logits=logits_fake, labels=tf.ones_like(logits_fake)), name='loss')
        self.g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')

    add_moving_summary(self.g_loss, self.d_loss, self.d_accuracy, self.g_accuracy)
````",yeah difference think classifier like would go default formulation almost everything sorry python gen,issue,positive,negative,negative,negative,negative,negative
279472308,"> I have implemented these changes in my version:
> 
>     mini-batch discrimination
>     switched the labels 0 <-> 1
>     label-smoothing
> 
@PatWie 

Second comment on #105 ",version discrimination switched second comment,issue,negative,neutral,neutral,neutral,neutral,neutral
279469879,"@PatWie  Is there a reference? It doesn't sound like it will work -- it sounds equivalent. You let the discriminator predicting P(sample is fake) instead of P(sample is real) = 1 - P(sample is fake), but sigmoid(x) = 1 - sigmoid(-x) and weights are symmetrically initialized, so predicting either of them is the same.",reference sound like work equivalent let discriminator sample fake instead sample real sample fake sigmoid sigmoid symmetrically either,issue,negative,negative,neutral,neutral,negative,negative
279444769,I somewhere read it. But I never notice any differences. It is probably better to flip these labels real(1) and fake(0).,somewhere read never notice probably better flip real fake,issue,negative,negative,negative,negative,negative,negative
279417456,"I just noticed this line:
```
        Note, we swap 0, 1 labels as suggested in ""Improving GANs"".
```
What does it mean? I only know one trick used here: min log (1-D) -> max(log(D)), and this is suggested by the original GAN paper.",line note swap improving mean know one trick used min log log original gan paper,issue,positive,positive,neutral,neutral,positive,positive
279405667,"Superresolution is more or less a hobby for me. So I do not really rely on this. And the result is really worse than bicubic (I tried to optimize MSE or PSNR directly).

I just though, this would be the most easy example for fully-convolutional.",le hobby really rely result really worse tried optimize directly though would easy example,issue,negative,positive,neutral,neutral,positive,positive
279399877,I also think it would be a nice example if the results can be better (now it seems worse than bicubic?) And I like the SuperResolution topic so I may try it when I have more time.,also think would nice example better worse like topic may try time,issue,positive,positive,positive,positive,positive,positive
279335824,Cannot reproduce results even when playing with hyperparameters. Still it is nice example to show how to use fully-convolutional networks during inference.,reproduce even still nice example show use inference,issue,negative,positive,positive,positive,positive,positive
279308281,"I. That's great.
II. Not necessary. I just found benchmarking on the fly any given parts of the pipeline useful.
III. I'll definitely use a modified version of it. thanks.",great necessary found fly given pipeline useful definitely use version thanks,issue,positive,positive,positive,positive,positive,positive
279216278,"Hi, 
I was also thinking about splitting the deprecation stuff into a function and decorator. The only prerequisite I would say is to allow something like 

```bash
grep -rn . -e ""deprecated"" --include=*.py
```

There is probably no elegant way to use a decorator as a function itself without messing up with duck typing in python.",hi also thinking splitting deprecation stuff function decorator prerequisite would say allow something like bash probably elegant way use decorator function without messing duck python,issue,positive,positive,positive,positive,positive,positive
279216055,"It's very weird how I should use the arguments for logging. I don't know if there is a natural way to use one function to work both as a logging function and as a decorator, so I've split the logging to a separate function. Let me know if you have a better solution.",weird use logging know natural way use one function work logging function decorator split logging separate function let know better solution,issue,positive,positive,neutral,neutral,positive,positive
279147449,"Now the following two lines are the same:
```python
deprecated(""step_per_epoch"")(""Use steps_per_epoch"", ""2017-11-8"")
deprecated(""step_per_epoch"", ""2017-11-8"")(""Use steps_per_epoch"")
```
What is `fix` and why do you need both `fix` and `eos`? Seems to me only having `eos` is enough?",following two python use use fix need fix enough,issue,negative,neutral,neutral,neutral,neutral,neutral
279097021,"Example:
```python
from tensorpack import *

@deprecated(""This is renamed to _get_inputs()"", ""2017-12-24"")
def _get_input_vars(a, b):
    print((a, b))

@deprecated(""Use get_reused_placehdrs()"")
def get_input_vars(a, b, c):
    print((a, b, c))

_get_input_vars(1, 2)
get_input_vars(3, 4, 5)

deprecated(""step_per_epoch"")(""Use steps_per_epoch"", ""2017-11-8"")
deprecated(""config.set_tower"")(""Set config.tower or config.nr_tower directly."")

class Dummy(object):
    @deprecated(""remove this call"")
    def test(self, a, b=7):
        pass

def nested():
    d = Dummy()
    d.test(6) 

nested()

deprecated(""_get_input_vars() is renamed to _get_inputs()."", ""2017-04-11"")("""")

```",example python import print use print use set directly class dummy object remove call test self pas dummy,issue,negative,positive,neutral,neutral,positive,positive
278695957,"There is a lot of ""hooks"" under `tf.train`. The design of hooks is similar to step callbacks (the before_run method is like the dynamic fetch list) , and it could be helpful to integrate them.
In particular, the SummarySaverHook may help with the use case mentioned above.",lot design similar step method like dynamic fetch list could helpful integrate particular may help use case,issue,positive,positive,neutral,neutral,positive,positive
278674152,"I also notice that the `QueueInput` is coupled to all trainers directly and StageInput as simple drop-in replacement (argument) is not possible without using `hasattr` since it lacks many features. 
So this might be only interesting for inference or when using small datasets (MNIST).",also notice coupled directly simple replacement argument possible without since many might interesting inference small,issue,negative,positive,positive,positive,positive,positive
278674076,"But currently it's very hard to use it during training. Now there is unfortunately only little code reuse between ""predict during training"" and ""predict after training"" (offline predict).",currently hard use training unfortunately little code reuse predict training predict training predict,issue,negative,negative,negative,negative,negative,negative
278672135,"Yes I found that as well. In lighting, you'd better clip the return value to [0,255] when old_dtype==np.uint8",yes found well lighting better clip return value,issue,positive,positive,positive,positive,positive,positive
278669973,"The result looks very promising!
While writing the tutorial I found the dataflow in imagenet-resnet can be written more efficiently -- should use uint8 image instead of float32, I'll benchmark the dataflow more carefully before looking at integrating staging operators.

StagingArea is on GPU, unlike the queue. So you cannot put too many tensors to it.",result promising writing tutorial found written efficiently use image instead float carefully looking staging unlike queue put many,issue,positive,positive,positive,positive,positive,positive
278664269,"tf.ConditionalAccumulator is related. But it's API seems to be designed for async training, not for increasing batch size.",related designed training increasing batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
278582246,"A quick&dirty test reveals a small speedup on `imagenet-resnet.py` for small batchsize. But for a proper benchmark the output of tqdm is to unstable.

standalone-test: https://gist.github.com/PatWie/7bd193c07ff72110997649cc83057699
gives
```
[staging] finished in 222 ms
[FIFO] finished in 848 ms
```

",quick dirty test small small proper output unstable staging finished fifo finished,issue,negative,negative,negative,negative,negative,negative
278389932,Aggree on #137 and will reopen when a more elegant way is available.,reopen elegant way available,issue,negative,positive,positive,positive,positive,positive
278369631,"I don't know you're still working on this! Unless WGAN becomes a very common model in the future, I don't think it's necessary to complicate the existing GAN.py. Maybe just a single file like [my gist](https://gist.github.com/ppwwyyxx/f2ee18e08ef140259fd060321680f540) is fine.

There are others factors holding me back otherwise I would have pushed my code earlier already. WGAN reveals some design problems which prevent me from implementing it in easier ways. 
One is that I cannot use anything similar to the existing [VariableClippingOptimizer](https://github.com/tensorflow/tensorflow/blob/33e88f6d7b69f796d4fad452394237448f62b976/tensorflow/contrib/opt/python/training/variable_clipping_optimizer.py) without writing the _setup method myself. I really wish users don't have to touch the trainer for something as simple as clipping the variables.
Another is, when there are >1 ops to run in iterations, running SummaryMovingAverage() together with them may let the graph compute something it shouldn't have (i.e. summary some tensors that were not used by the current op). This doesn't happen in the current implementation, but it could happen with the current design.
I'm thinking about solving the issues (especially the first one) recently, so I could write WGAN easier. Currently this PR and the gist above can be a reference implementation if people want to see it, but I'd like to push it when appropriate changes are made to simplify it.",know still working unless becomes common model future think necessary complicate maybe single file like gist fine holding back otherwise would code already design prevent easier way one use anything similar without writing method really wish touch trainer something simple clipping another run running together may let graph compute something summary used current happen current implementation could happen current design thinking especially first one recently could write easier currently gist reference implementation people want see like push appropriate made simplify,issue,positive,positive,neutral,neutral,positive,positive
277848113,"Ok, MNIST is at least working and gives reasonable results when running all iterations. 
This commit is rebase on last HEAD and contains some refactoring of GAN as well.
I think its a good idea to put it into examples as it demonstrates how to change the trainer class when necessary.  How to proceed?",least working reasonable running commit rebase last head gan well think good idea put change trainer class necessary proceed,issue,positive,positive,positive,positive,positive,positive
277752719,"Sounds good. One should not forget to add CaffeLMDB as an alias/wrapper, because it is very nice to load Caffe data within one line.

I can do another pull-request with this refactoring, after I spend some time on the testing script to add more test cases and handle/caching the TENSORPACK_DATASET content.",good one forget add nice load data within one line another spend time testing script add test content,issue,positive,positive,positive,positive,positive,positive
277630452,"Thanks, this looks good to me now.
One thing I'm thinking about changing (probably later) is: LMDBDataDecoder, CaffeLMDB, etc are  just a mapping on the base LMDBData. If they are implemented with MapData, a change to the base class (like this one) don't have to affect any of them.",thanks good one thing thinking probably later base change base class like one affect,issue,positive,negative,negative,negative,negative,negative
277575529,"I had a wgan implementation locally. Didn't see very surprising results so I didn't intend to push it.
Have you seen anything interesting from your results?",implementation locally see surprising intend push seen anything interesting,issue,positive,positive,positive,positive,positive,positive
277543281,"It is not my primary focus but I really like to see a working wGAN as well. So far the progress is described here #135 hopefully without ugly hacks. 
This thread should be only for ""Improving GAN"".",primary focus really like see working well far progress hopefully without ugly thread improving gan,issue,positive,positive,neutral,neutral,positive,positive
277537476,"The current master branch has setup.*, so this can be closed.",current master branch setup closed,issue,negative,negative,neutral,neutral,negative,negative
277537369,"Ok,
I tested the latest version with:
````python

from tensorpack import *

ds = CaffeLMDB('/data/imageNet/ILSVRC2012/lmdb/train', keys='{:0>8d}')
ds = PrefetchDataZMQ(ds, 4)
# ds = PrefetchData(ds, 4)  # or this one, or none of the prefetching
ds.reset_state()
for i in ds.get_data():
    img, label = i
    print(img.shape, label)

````
I also prefer to let each process open a filehandle during reset_state as I observed some errors when directly dealing with tar files (in another dataflow reader).

So it works as expected. I will add the converter script (in another pull-request) as soon as I have figured out to write the compressed JPEG data and how to totally omit Caffe dependencies (optional). I saw, that already provide `dump_data_to_lmdb`.",tested latest version python import one none label print label also prefer let process open directly dealing tar another reader work add converter script another soon figured write compressed data totally omit optional saw already provide,issue,negative,positive,positive,positive,positive,positive
277536308,"I trained this network on ILSVRC12 for several days with 1.) Adam as in the paper and 2.) a scheduled LR (which gave better results). However, these are still far away from the results in the paper. I think this should be just a demo for training on small images and deploying to arbitrary images. 

I wonder, why they do not use this technique in their CVPR submission?!

The PixShift layer has been checked manually. 

edit: The result and question is an observation, not an appraisal.",trained network several day paper gave better however still far away paper think training small arbitrary wonder use technique submission layer checked manually edit result question observation appraisal,issue,negative,positive,neutral,neutral,positive,positive
277496109,"The code in master now all works for TFv1.0.0rc0 as far as I know. 
Why do you want to change it to work for v0.12?",code master work far know want change work,issue,negative,positive,neutral,neutral,positive,positive
277312556,"Thanks a lot and I'll refer to the resources you provided!
I've added setup.* which seems to work with `python setup.py install` in a virtualenv. Will test more later.",thanks lot refer provided added setup work python install test later,issue,negative,positive,neutral,neutral,positive,positive
277308933,"Thanks for the details and we love to hear different use cases from different people.
From the paper it looks like this is only used for prediction, after a model has been trained? If so it's not about DataFlow or callbacks. 
You'll notice that in, e.g. the GAN examples the testing part is written completely independent of the training part. It prepares the input tensor, runs an `OfflinePredictor`, and uses the output tensor. These are just python code you can write freely and they have nothing to do with tensorpack except the call to the predictor.",thanks love hear different use different people paper like used prediction model trained notice gan testing part written completely independent training part input tensor output tensor python code write freely nothing except call predictor,issue,positive,positive,positive,positive,positive,positive
277308614,"Ah yes, that's what I was thinking. The issue is how to elegantly recombine them. Is there a post-callback I can use? I assume I'll need to collect the patches on the output and hold them in RAM so I was curious if there was a callback or runner that would help me do the patch recombination. (Hold N output values then recombine them perform ""some action"" on them). Bonus points if I could vary N as needed to compensate for different image sizes (not a requirement by any means, but it would be a nice addition).

TLDR: The issue I have now is how to recombine them elegantly in the Tensorpack framework.",ah yes thinking issue elegantly recombine use assume need collect output hold ram curious runner would help patch recombination hold output recombine perform action bonus could vary compensate different image size requirement would nice addition issue recombine elegantly framework,issue,positive,positive,positive,positive,positive,positive
277307284,"So aside from the reference [implementation](https://github.com/martinarjovsky/WassersteinGAN), I found an implementation that can use [Tensorflow](https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/WassersteinGAN) as backend. (It seems like it might be using a feed_dict or something because I get relatively low GPU utilization), and even one in [Chainer](http://www.itdadao.com/articles/c19a1115660p0.html).

The fact that this GAN is already implemented in so many frameworks less than a week after release bodes really well. Also I have tested at least one of the implementation and while it was unoptimized, I was impressed how quickly the loss dropped (even if the epochs did take a while).  So far it seems that the might have actually solved (most if not all) of the issues which would be really impressive!",aside reference implementation found implementation use like might something get relatively low utilization even one chainer fact gan already many le week release really well also tested least one implementation quickly loss even take far might actually would really impressive,issue,positive,positive,positive,positive,positive,positive
277306885,"Then the definitiv answer is that the data provider needs to [generate patches](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.extract_patches_2d.html). Do this as a python generator to just feed a certain amount of patches into the net.

Fetch the output and combine everything  by Hanning-Window averaging method or similar.",answer data provider need generate python generator feed certain amount net fetch output combine everything method similar,issue,negative,positive,neutral,neutral,positive,positive
277305552,"To clarify it doesn't need to use the combined images in the loss function. I just want an easy way to output and reassemble the patches. I am asking because I do not know if the dataflow guarantees ordering within the batches and that they get placed in the same order for the output. 

I was using the GANs as an example. To be exact this is the paper I wanted to implement: http://www.ingentaconnect.com/content/ist/jist/2016/00000060/00000001/art00003 . (See Figure 4).

![image](https://cloud.githubusercontent.com/assets/2053727/22600775/0b90a550-ea0a-11e6-9628-91b4c17d3a53.png)


To be exact here is the figure from the paper that shows the process I want to implement.

I apologize as I just recently got interested in deep learning so I am still learning all the appropriate terminology. I have tried using tf.extract_image_patches(), but it seems to run out of memory when I attempt on larger images in vanilla Tensorflow. I was hoping this repo could automate the process and make it easier to do the preprocessing on the fly. 

I also would like to complement you on your excellent support on this repo. You guys have had absolutely excellent response times and are willing to help people fully utilize the framework. I just wished to applaud you on that front.",clarify need use combined loss function want easy way output reassemble know within get order output example exact paper implement see figure image exact figure paper process want implement apologize recently got interested deep learning still learning appropriate terminology tried run memory attempt vanilla could process make easier fly also would like complement excellent support absolutely excellent response time willing help people fully utilize framework wished applaud front,issue,positive,positive,positive,positive,positive,positive
277303552,"Then feel free to close this PR when starting from scratch. I agree, creating a pip package is not the most easiest thing.",feel free close starting scratch agree pip package easiest thing,issue,positive,positive,positive,positive,positive,positive
277303413,"@Skylion007  I still don't understand what you mean especially the context. Is this for training as well or only for testing? Doing stuff after getting the output of a network is not a job of DataFlow, but it doesn't sound like anything you would do for training either.

This is not a GAN repo, and I'm not even sure what patch-based GAN means to you. If you are talking about the PatchGAN mentioned in the image to image paper, it is fully convolutional and you can run a trained model on super large images already, without having to split things into patches and building a batch.",still understand mean especially context training well testing stuff getting output network job sound like anything would training either gan even sure gan talking image image paper fully convolutional run trained model super large already without split building batch,issue,positive,positive,positive,positive,positive,positive
277302613,"You can encode this as a part of the graph. (I did it here https://arxiv.org/abs/1607.04433)

The dataflow provided the patches as a stack (tensor of 5 dimensions). It is just transposing and reshaping the data to handle a virtual batchsize of batchsize*patches. However, I am not aware of an easy solution to actually use the combined image (all patches stiched) again within the loss function. Usually this is trained on patches individually or one uses fully convolutional networks.

But if your 10000x10000px does not fit to the gpu, it won't fit as patches on the gpu.

Edit: If I understand you correctly, you should google about Hanning-Window. Maybe there is already public code for that. But again this is usually done only during inference.",encode part graph provided stack tensor data handle virtual however aware easy solution actually use combined image within loss function usually trained individually one fully convolutional fit wo fit edit understand correctly maybe already public code usually done inference,issue,positive,positive,positive,positive,positive,positive
277299440,"I just want a dataflow process to make it easy to scale up an algorithm to patch based input. For example, let's say you have a massive 10000x10000 px image (not uncommon for satellite imagery). 

Dataflow would break image into small 256x256 patches. -> (break in batches of batchsize?) -> run network -> reassemble image by concatenating all patches into their proper place. 

^ (Also if patches overlap, it would be great to easily merge them by averaging the overlapping portions.)

The mapdata sounds like it might do what I want it to so I may try that. I was surprised that the repo didn't have any functions to do this since it's incredibly useful for Patch Based GANS.",want process make easy scale algorithm patch based input example let say massive image uncommon satellite imagery would break image small break run network reassemble image proper place also overlap would great easily merge like might want may try since incredibly useful patch based,issue,positive,positive,positive,positive,positive,positive
277286155,"After a quick look at the whole complicated process I think I'll first trying to write a good setup.py and setup.cfg (with all the settings configured properly, such as requirements) and then look at how to publish on travis. 
",quick look whole complicated process think first trying write good properly look publish travis,issue,negative,positive,positive,positive,positive,positive
277285285,"This is what **I** understand in proper documentation, but probably nobody else.",understand proper documentation probably nobody else,issue,negative,neutral,neutral,neutral,neutral,neutral
277283545,"So they claim, they solved all issues with GAN :wink: ?

As far as I can tell the differences to implement are:
- good weight initialization  (trivial to implement)
- get all weights from discriminator (alias critic). I only know a hacky solution. Maybe @ppwwyyxx has an elegant way to implement this
- add clipping (trivial to implement)
- run several optimization steps of the discriminator (currently ugly hack when implementing)

I just came up with [a few lines](https://github.com/PatWie/tensorpack-fork/blob/wgan/examples/GAN/GAN.py), but I have to read the paper in detail.
",claim gan wink far tell implement good weight trivial implement get discriminator alias critic know hacky solution maybe elegant way implement add clipping trivial implement run several optimization discriminator currently ugly hack came read paper detail,issue,positive,positive,positive,positive,positive,positive
277280533,Oh I didn't notice that. Could you rename the argument to `keys` and document this as the main feature (and the less-general string formatting as a syntax sugar)?,oh notice could rename argument document main feature string syntax sugar,issue,negative,positive,positive,positive,positive,positive
277270639,"The current implementation [can do this](https://github.com/PatWie/tensorpack-fork/blob/lmdb_fix/tensorpack/dataflow/format.py#L106-L107), too.


I like the API-design of having `set_keys`. But the function `open_lmdb` is already called in the constructor. So this means one has to write

````python
ds = CaffeLMDB('/data/imageNet/ILSVRC2012/lmdb/train')
ds.set_keys(np.arange(ds.size())
ds.open_lmdb()
````

The current version supports

````python
ds = CaffeLMDB('/data/imageNet/ILSVRC2012/lmdb/train', key_format=a_list_of_keys)
````
",current implementation like function already constructor one write python current version python,issue,negative,neutral,neutral,neutral,neutral,neutral
277217013,The lmdb format caffe used is very inefficient. It stores a serialized raw array without image compression.,format used inefficient raw array without image compression,issue,negative,negative,negative,negative,negative,negative
277216267,"I used:
https://gist.github.com/PatWie/9537acc0e1f24687613856b60dba7eab

The important part is Line 140.",used important part line,issue,negative,positive,positive,positive,positive,positive
277215999,Single lmdb file with jpeg string may still be faster. But since you already have an SSD the benefit could be very small.,single file string may still faster since already benefit could small,issue,negative,negative,negative,negative,negative,negative
277215275,"I unpacked all JPEGS to SSD without cropping and now I can read 422 images per second. 
How wrong I was thinking a single file would be faster. Is this roughly the same you get?

````python
from tensorpack import *

ds = dataset.ILSVRC12('/scratch/imagenet', 'train')
ds = MapData(ds, lambda dp: [dp[0][:, :, ::-1]])
ds = TestDataSpeed(ds)
ds.start_test()
````",unpacked without read per second wrong thinking single file would faster roughly get python import lambda,issue,negative,negative,negative,negative,negative,negative
277212217,"I think you should store jpeg string instead of the raw array. The decompression is faster than reading.
I've converted to lmdb before and the whole db should be less than 150G.",think store string instead raw array decompression faster reading converted whole le,issue,negative,negative,neutral,neutral,negative,negative
277211652,"Thanks for solving this issue! There was an issue about loading keys originally, reset_state() shouldn't need to load all the keys again. This will also slow down the dataflow a lot if you use prefetch.

The original idea was that, lmdb is just a k-v database, and the keys can be any string -- doesn't have to contain numbers. That's why it iterates the db to find the keys.

I like the idea to provide the keys, but the format is assuming the keys are numeric. Maybe you can just add a method in the base lmdb class to set the keys (by giving a list)? Then you can:
```
ds = CaffeLMDB('/data/imageNet/ILSVRC2012/lmdb/train')
ds.set_keys(np.arange(ds.size())
```
This will also make the API easier to understand, otherwise you'll need to explain that the format will be used against the index.",thanks issue issue loading originally need load also slow lot use original idea string contain find like idea provide format assuming maybe add method base class set giving list also make easier understand otherwise need explain format used index,issue,positive,negative,neutral,neutral,negative,negative
277206505,"I converted the entire tar file into lmdb. This gives 788GB for the training (without cropping) and is slow as hell when reading. Even sequential reading from tar is very slow (reads 17 images per second from ssd). That's why I was asking.

Thank you both for providing some concrete numbers.",converted entire tar file training without slow hell reading even sequential reading tar slow per second thank providing concrete,issue,positive,negative,neutral,neutral,negative,negative
277204524,"I was using the original ResNet18 script, the original ILSVRC12 dataset.
Hardware: 4 TitanX, normal hard disk (cannot fill the queue).
Speed: 2.3~2.5 it/s.

It will be faster if you 1. use a single file such as lmdb and 2. don't globally shuffle all training data, but maintain a pool to locally shuffle the data.
I think I'll put a page in docs about this when I have time to test the numbers.",original script original hardware normal hard disk fill queue speed faster use single file globally shuffle training data maintain pool locally shuffle data think put page time test,issue,negative,positive,neutral,neutral,positive,positive
277183537,"So my images are resized to 256px, and stored as individual JPEGs on the disk (not in a tarfile). With 256 batch size over 4 GPUs, I was getting around 1 it/sec.",individual disk batch size getting around,issue,negative,neutral,neutral,neutral,neutral,neutral
277172887,"Sorry for polluting this thread. But I am curios: What's the speed (it/sec or sec/iter) when training ResNet on your machine on imagenet (and what's the batch size, num gpus)? I get a very slow pre-fetching on my hardware here.
@rohitgirdhar @ppwwyyxx 
And you are really reading from all these cluttered single imagefiles with their original size from the tar files? I just currently try to figure out the most efficient way.
",sorry polluting thread speed training machine batch size get slow hardware really reading single original size tar currently try figure efficient way,issue,positive,negative,neutral,neutral,negative,negative
277156328,"I think it's just a standard SSD, I don't think we have any RAID configured on it. Though I'm far from an expert on this. PM me if you need specific model details, and I can try to figure that out.",think standard think raid though far expert need specific model try figure,issue,negative,positive,neutral,neutral,positive,positive
277140794,Yes I also think it's helpful to have a separate branch for developing. Will look at this when I have more time.,yes also think helpful separate branch look time,issue,positive,neutral,neutral,neutral,neutral,neutral
277140402,"Almost every example prints something evaluated on the validation set. For example, in mnist-convnet.py, the following line:
```python
            InferenceRunner(    # run inference(for validation) after every epoch
                dataset_test,   # the DataFlow instance used for validation
                # Calculate both the cost and the error for this DataFlow
                [ScalarStats('cross_entropy_loss'), ClassificationError('incorrect')]),
```
Note that ""cross_entropy_loss"" must be an op you defined in the graph already.

The validation is run every epoch. You can use `PeriodicTrigger(InferenceRunner(.........), every_k_epochs=10)` to run it every 10 epoch.",almost every example something validation set example following line python run inference validation every epoch instance used validation calculate cost error note must defined graph already validation run every epoch use run every epoch,issue,negative,neutral,neutral,neutral,neutral,neutral
277139787,"I don't know what exactly you want, but we certainly don't have anything like this.
If your input->output is one image -> one image, you can write an augmentor, by subclass ImageAugmentor and implement the related methods.
If you want one image -> several images in one tensor, or one datapoint -> one datapoint, you can use MapData to specify a mapping function.
More generally you can write a DataFlow subclass to implement whatever you want to do with your data, as long as the output can be represented as a tensor in the end.

We have a very draft version of tutorials that may help: [DataFlow](http://tensorpack.readthedocs.io/en/latest/user/dataflow.html).",know exactly want certainly anything like output one image one image write subclass implement related want one image several one tensor one one use specify function generally write subclass implement whatever want data long output tensor end draft version may help,issue,positive,positive,neutral,neutral,positive,positive
277139150,"Also, just curious, @rohitgirdhar From the log you seem to have very good hard disk, to be able to train without data loading overhead. Is it a RAID or SSD or anything? If it is a RAID could you tell me the configuration?  I don't have resources to test which setting is enough for the current implementation. ",also curious log seem good hard disk able train without data loading overhead raid anything raid could tell configuration test setting enough current implementation,issue,positive,positive,positive,positive,positive,positive
277138742,"I've also started a job to train it from scratch. At epoch 60, resnet-18 trained with 4 GPUs is getting 35% top1 val error. This is similar to the number I had before.
Although it haven't finished I guess the issue was resolved.",also job train scratch epoch trained getting top error similar number although finished guess issue resolved,issue,negative,positive,positive,positive,positive,positive
277072163,"As a user of this repo, I really hope that you start having a release branch as I have noticed a lot of almost daily breaking changes when using some the example code. 👍 ",user really hope start release branch lot almost daily breaking example code,issue,negative,positive,neutral,neutral,positive,positive
277071470,"Like lets say I have some ground truth data. Every 10th epoch or so, I would like to print out the cross_entropy between the predicted output and the expected output. This would be data that is different from the training data and thus would not be part of the dataflow. I am little confused about how to do this in a high level sense restarting the entire python process and loading the data from the last saved model.  I would really appreciate if I could get this metric so I knew when my model is overfitting. Maybe that could even be used as a halting condition of training phase?

To clarify here is the scenario. You have a folder containing 256x512 ground truth images for the Image2Image GAN. How would you use that as a validation test set without killing the process and starting over every X epochs?",like say ground truth data every th epoch would like print output output would data different training data thus would part little confused high level sense entire python process loading data last saved model would really appreciate could get metric knew model maybe could even used halting condition training phase clarify scenario folder ground truth gan would use validation test set without killing process starting every,issue,positive,negative,neutral,neutral,negative,negative
277000299,"It seems to be fixed.
With the earlier version of TF, I was also getting 95% error after 1st epoch before:
```
[0202 02:56:58 @stats.py:113] input_queue_size: 50
[0202 02:56:58 @stats.py:113] l2_regularize_loss: 1.2286
[0202 02:56:58 @stats.py:113] learning_rate: 0.1
[0202 02:56:58 @stats.py:113] train-error-top1: 0.96814
[0202 02:56:58 @stats.py:113] train-error-top5: 0.8822
[0202 02:56:58 @stats.py:113] val-error-top1: 0.9553
[0202 02:56:58 @stats.py:113] val-error-top5: 0.86338
[0202 02:56:58 @stats.py:113] xentropy-loss: 5.7036
```

But with today's build, I get after 1st epoch:
```
[0202 11:03:23 @stats.py:113] input_queue_size: 48.341
[0202 11:03:23 @stats.py:113] l2_regularize_loss: 1.2029
[0202 11:03:23 @stats.py:113] learning_rate: 0.1
[0202 11:03:23 @stats.py:113] train-error-top1: 0.77943
[0202 11:03:23 @stats.py:113] train-error-top5: 0.55875
[0202 11:03:23 @stats.py:113] val-error-top1: 0.80062
[0202 11:03:23 @stats.py:113] val-error-top5: 0.58646
[0202 11:03:23 @stats.py:113] xentropy-loss: 3.9533
```

Thanks for figuring this, @ppwwyyxx !",fixed version also getting error st epoch today build get st epoch thanks,issue,negative,positive,positive,positive,positive,positive
276962982,"Thanks! I renamed the class name to CamelCase, and replace `keep_dim` by `keep_dims` used by tensorflow (although it's different from `keepdims` used by numpy)..

// UPDATE:
a search shows that TF is likely going to move to `keepdims` as well. https://github.com/tensorflow/tensorflow/issues/6815
",thanks class name replace used although different used update search likely going move well,issue,positive,positive,neutral,neutral,positive,positive
276898574,It's a TF bug that seems to be fixed now. But I haven't got time to test it.,bug fixed got time test,issue,negative,positive,neutral,neutral,positive,positive
276882949,"Is this still a problem or you have resolved this? If you do want to checkout an earlier version of resnet for TF 12.1 do you recollect which git commit number to checkout?
Thanks",still problem resolved want version git commit number thanks,issue,positive,positive,positive,positive,positive,positive
276879677,"Also another point, I am actually not working with grayscale augments, but actually working on a dataset that has grayscale images. Any opencv function call appears to remove the last dimension (not just converting it to grayscale). I am well aware of ways to get the last dimension back and am using them, but I agree with @ppwwyyxx . Unless the operation explicitly changes the shape of an image, it should have the same behavior for multichannel and grayscale images.",also another point actually working actually working function call remove last dimension converting well aware way get last dimension back agree unless operation explicitly shape image behavior,issue,positive,positive,neutral,neutral,positive,positive
276820964,"I was thinking something like that, but I don't know if there is a good way to parameterize this as an augmentor.",thinking something like know good way,issue,positive,positive,positive,positive,positive,positive
276818778,"Maybe you do need a rgb/bgr option?
Or, since Hue also depends on this, an augmentor which does the RGB/BGR flip would also work.",maybe need option since hue also flip would also work,issue,negative,neutral,neutral,neutral,neutral,neutral
276818495,"Though this is due to opencv, I think it's good to do a reshape inside GaussianBlur to make it less surprising to users (including me).",though due think good reshape inside make le surprising,issue,positive,positive,positive,positive,positive,positive
276812389,"While I also use grayscale mappings often, I am not sure if it is really a good idea to include this into tensorpack because it depends on the image format: rgb or bgr.
You can copy
````python
class Grayscale(ImageAugmentor):
    """"""
    Convert image to grayscale.
    """"""

    def __init__(self, keep_dim=False):
        """"""
        Args:
            keep_dim: return image of shape [H, W, 1] or [H, W]
        """"""
        self._init(locals())

    def _augment(self, img, _):
        grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        if self.keep_dim:
            grey = grey[..., None]
        return grey
````

into your script. And then

````python
from tensorpack import *

ds = ImageFromFile([""original.jpg""], channel=3)
ds = AugmentImageComponent(ds, [Grayscale(keep_dim=True)])
print next(ds.get_data())[0].shape # gives (H, W, 1)

````
",also use often sure really good idea include image format copy python class convert image self return image shape self grey grey grey none return grey script python import print next,issue,positive,positive,positive,positive,positive,positive
276800781,"This is not an issue of tensorpack or `imaug.GaussianBlur`. OpenCV already removes the last dimension

````python
import cv2
img = cv2.imread(""original.jpg"")
grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
print grey.shape # is (H, W) instead of (H,W, 1)
````


You should use something like
````python
ds = AugmentImageComponent(ds, [imgaug.MapImage(lambda x: x[..., None])])
````
To get the last axis back.",issue already last dimension python import grey print instead use something like python lambda none get last axis back,issue,negative,negative,neutral,neutral,negative,negative
276786125,"@ppwwyyxx very nice refactoring of the `step_trigger`! 
I added these `extra_fetches` to `Progressbar` directly using postfix. Example is in SimilarityLearning.

It would be really cool, if tqdm could display both: examples/sec and sec/example.
https://github.com/tqdm/tqdm/issues/341",nice added directly postfix example would really cool could display,issue,positive,positive,positive,positive,positive,positive
276767634,"My colleague just recommended a paper on GANs: https://github.com/martinarjovsky/WassersteinGAN

Supposedly this paper uses earth mover's distance to ""virtually guarantee"" that GANs will converge.  Would love to see this implemented in Tensorpack. @ppwwyyxx @PatWie ",colleague paper supposedly paper earth mover distance virtually guarantee converge would love see,issue,positive,positive,positive,positive,positive,positive
276752138,"What do you mean by perspective? Something like:

````python
import cv2
import numpy as np

img = cv2.imread(""original.jpg"")
imageplane_coords = np.float32([[-30, -60],[368,52],[28,387],[389,390]])
worldplane_coords = np.float32([[0,0],[300,0],[0,300],[300,300]])
M = cv2.getPerspectiveTransform(imageplane_coords,worldplane_coords)
dst = cv2.warpPerspective(img,np.random.rand(3,3),(300,300))
cv2.imwrite(""output.jpg"", dst)
````
?
Should this use a random transformation matrix?
Saturation is already in the code.",mean perspective something like python import import use random transformation matrix saturation already code,issue,negative,negative,negative,negative,negative,negative
276562301,"What kind of metrics do you want?
As long as you can compute it in the graph, you can use the InferenceRunner callback with the ScalarStats inferencer.",kind metric want long compute graph use,issue,positive,positive,positive,positive,positive,positive
276397969,"OK. Btw, it's normal if there are some exceptions thrown in the end of training (after the final epoch) about ZMQ or socket.",normal thrown end training final epoch socket,issue,negative,positive,neutral,neutral,positive,positive
276346170,"I cannot unfortunately, it was running on an AWS machine and I wrongly assumed the log files would capture all errors. It was not a python related exception, rather something like ""unable to create thread"" or similar. If I train again, I'll pipe all of stdout/stderr to another log file. Apologies.",unfortunately running machine wrongly assumed log would capture python related exception rather something like unable create thread similar train pipe another log file,issue,negative,negative,negative,negative,negative,negative
276178073,"Yes, that was mistake at my end. The dataset hadnt downloaded properly.",yes mistake end properly,issue,negative,neutral,neutral,neutral,neutral,neutral
276075725,"The PTB example works fine here. 
Looking from the log, you might not have the correct validation data.",example work fine looking log might correct validation data,issue,negative,positive,positive,positive,positive,positive
276008998,"Ah, I was cleaning unused branches and accidentally deleted this branch. Sorry for that.",ah cleaning unused accidentally branch sorry,issue,negative,negative,negative,negative,negative,negative
276008991,This won't happen if you do the submission in another script and run the submission after all the runs.,wo happen submission another script run submission,issue,negative,neutral,neutral,neutral,neutral,neutral
275989249,"BTW, the char-rnn example works fine for me with TF update to 1.0",example work fine update,issue,negative,positive,positive,positive,positive,positive
275989031,"I updated to TF 1.0. Still facing some difficulties with the PTB example.

Getting this error now 

```
/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
Traceback (most recent call last):
  File ""./PTB-LSTM.py"", line 158, in <module>
    SimpleFeedfreeTrainer(config).train()
  File ""/home/userID/TF/tensorpack/tensorpack/train/base.py"", line 65, in train
    self.main_loop()
  File ""/home/userID/TF/tensorpack/tensorpack/train/base.py"", line 188, in main_loop
    self.trigger_epoch()
  File ""/home/userID/TF/tensorpack/tensorpack/train/base.py"", line 87, in trigger_epoch
    self.config.callbacks.trigger_epoch()
  File ""/home/userID/TF/tensorpack/tensorpack/callbacks/base.py"", line 98, in trigger_epoch
    self._trigger_epoch()
  File ""/home/userID/TF/tensorpack/tensorpack/callbacks/group.py"", line 124, in _trigger_epoch
    cb.trigger_epoch()
  File ""/home/userID/TF/tensorpack/tensorpack/callbacks/base.py"", line 98, in trigger_epoch
    self._trigger_epoch()
  File ""/home/userID/TF/tensorpack/tensorpack/callbacks/base.py"", line 155, in _trigger_epoch
    self.trigger()
  File ""/home/userID/TF/tensorpack/tensorpack/callbacks/base.py"", line 147, in trigger
    self._trigger()
  File ""/home/userID/TF/tensorpack/tensorpack/callbacks/inference_runner.py"", line 245, in _trigger
    self._write_summary_after_inference()
  File ""/home/userID/TF/tensorpack/tensorpack/callbacks/inference_runner.py"", line 248, in _write_summary_after_inference
    summary_inferencer(self.trainer, self.infs)
  File ""/home/userID/TF/tensorpack/tensorpack/callbacks/inference_runner.py"", line 56, in summary_inferencer
    ret = inf.after_inference()
  File ""/home/userID/TF/tensorpack/tensorpack/callbacks/inference.py"", line 52, in after_inference
    return self._after_inference()
  File ""/home/userID/TF/tensorpack/tensorpack/callbacks/inference.py"", line 101, in _after_inference
    assert len(self.stats) == len(self.names)
TypeError: object of type 'numpy.float64' has no len()
```",still facing example getting error mean empty slice recent call last file line module file line train file line file line file line file line file line file line file line trigger file line file line file line ret file line return file line assert object type,issue,negative,negative,negative,negative,negative,negative
275983705,"Yuxin,
thank you for guiding me, I ran run-atari.py again and received the following result:
...
Traceback (most recent call last):
  File ""./run-atari.py"", line 111, in <module>
    do_submit('/home/igor/tensorpack/examples/A3C-Gym/output_dir')
  File ""./run-atari.py"", line 83, in do_submit
    gym.upload(output, api_key='sk_M2DFWRqlTFWSwmGSnjHGKw')
  File ""/home/igor/gym/gym/scoreboard/api.py"", line 83, in upload
    evaluation = _upload(training_dir, algorithm_id, writeup, benchmark_run_id, api_key, ignore_open_monitors)
  File ""/home/igor/gym/gym/scoreboard/api.py"", line 102, in _upload
    raise error.Error(""Still have an open monitor on {}. You must run 'env.close()' before uploading."".format(', '.join(envs)))
gym.error.Error: Still have an open monitor on Breakout-v0. You must run 'env.close()' before uploading.
[2017-01-29 21:32:43,780] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/igor/tensorpack/examples/A3C-Gym/output_dir')
",thank ran received following result recent call last file line module file line output file line evaluation file line raise still open monitor must run still open monitor must run finished writing scoreboard via,issue,negative,neutral,neutral,neutral,neutral,neutral
275961546,"OK it's still related to the latest change in gym monitor API. It's only submitting the first episode.
I made another change which should make the submission work.",still related latest change gym monitor first episode made another change make submission work,issue,negative,positive,positive,positive,positive,positive
275948321,"Yuxin,

I ran: 
igor@igorfedorov:~/tensorpack/examples/A3C-Gym$ ENV=Breakout-v0; ./run-atari.py --load train_log/train-atari/checkpoint --env ""$ENV"" --episode 500 --output output_dir

at the end it says (Could you tell why it is only 1 episode, please and how to make complete submission  >=100 episodes?):
...
[2017-01-29 10:20:53,565] [Breakout-v0] Uploading 1 episodes of training data
[2017-01-29 10:20:54,931] [Breakout-v0] Uploading videos of 1 training episodes (167686 bytes)
[2017-01-29 10:20:55,357] [Breakout-v0] Creating evaluation object from /home/igor/tensorpack/examples/A3C-Gym/output_dir with learning curve and training video
[2017-01-29 10:20:55,538] 
****************************************************
You successfully uploaded your evaluation on Breakout-v0 to
OpenAI Gym! You can find it at:

    https://gym.openai.com/evaluations/eval_XHvvWuSRWGK51wcRdrBw

****************************************************
",ran load episode output end could tell episode please make complete submission training data training evaluation object learning curve training video successfully evaluation gym find,issue,positive,positive,positive,positive,positive,positive
275901648,"The network model and parameters are the same as in the paper. However, there are some difference in the pre-processing:
- in their work they use YCbCr colorspace (should not make differences when using large batchsize, so I use RGB and observed no saturated pixels)
- they first blur and then sub-sample for low-res, here I simply use bilinear downsampling
- for their final results they trained in ImageNet, this implementation just uses BSDS500 (I should download ImageNet at some point)

**Warning:** I cannot replicate the quality which is mentioned in the paper. This probably requires longer training and a larger dataset.

edit:
- it further seems to be an idea to standardize the visualization part from all examples across all example by create a new function in `tfutils`",network model paper however difference work use make large use saturated first blur simply use bilinear final trained implementation point warning replicate quality paper probably longer training edit idea standardize visualization part across example create new function,issue,negative,positive,positive,positive,positive,positive
275901139,This issue is very strange because I'm seeing different speed on two similar machines. May not be a TF issue this time.,issue strange seeing different speed two similar may issue time,issue,negative,negative,neutral,neutral,negative,negative
275901104,I hope the TF-team get things fixed. I am a little bit afraid of doing `git pull origin master` inside the TensorFlow-repo to build an updated version for now.,hope get fixed little bit afraid git pull origin master inside build version,issue,negative,negative,negative,negative,negative,negative
275897246,It seems so. But I haven't touched the code for months (and there are changes in gym as well) so I'm not completely sure.,touched code gym well completely sure,issue,positive,positive,positive,positive,positive,positive
275896112,"thank you, is submission script basically adding of do_submit(output) at the end of run-atari.py?",thank submission script basically output end,issue,negative,neutral,neutral,neutral,neutral,neutral
275894627,"My bad. Could you update tensorpack again?
To submit, you'll need to write the submission script yourself.",bad could update submit need write submission script,issue,negative,negative,negative,negative,negative,negative
275894320,"I replaced tensorpack and ran ./run-atari.py but still have a problem with upload, it says at the end '...You can upload them to the scoreboard via gym.upload('/home/igor/tensorpack/examples/A3C-Gym/output_dir')...' but I am not sure where to add this line:

igor@igorfedorov:~/tensorpack/examples/A3C-Gym$ ENV=Breakout-v0; ./run-atari.py --load train_log/train-atari/checkpoint --env ""$ENV"" --episode 500 --output output_dir
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
[0128 20:50:22 @run-atari.py:96] Environment Name: Breakout-v0
[2017-01-28 20:50:22,773] Making new env: Breakout-v0
[2017-01-28 20:50:23,221] Making new env: Breakout-v0
[2017-01-28 20:50:23,317] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0
[2017-01-28 20:50:23,361] Starting new video recorder writing to /home/igor/tensorpack/examples/A3C-Gym/output_dir/openaigym.video.0.16841.video000000.mp4
[0128 20:50:23 @common.py:101] conv0 input: [None, 84, 84, 12]
[0128 20:50:23 @common.py:109] conv0 output: [None, 84, 84, 32]
[0128 20:50:23 @common.py:101] pool0 input: [None, 84, 84, 32]
[0128 20:50:23 @common.py:109] pool0 output: [None, 42, 42, 32]
[0128 20:50:23 @common.py:101] conv1 input: [None, 42, 42, 32]
[0128 20:50:23 @common.py:109] conv1 output: [None, 42, 42, 32]
[0128 20:50:23 @common.py:101] pool1 input: [None, 42, 42, 32]
[0128 20:50:23 @common.py:109] pool1 output: [None, 21, 21, 32]
[0128 20:50:23 @common.py:101] conv2 input: [None, 21, 21, 32]
[0128 20:50:23 @common.py:109] conv2 output: [None, 21, 21, 64]
[0128 20:50:23 @common.py:101] pool2 input: [None, 21, 21, 64]
[0128 20:50:23 @common.py:109] pool2 output: [None, 10, 10, 64]
[0128 20:50:23 @common.py:101] conv3 input: [None, 10, 10, 64]
[0128 20:50:23 @common.py:109] conv3 output: [None, 10, 10, 64]
[0128 20:50:23 @common.py:101] fc0 input: [None, 10, 10, 64]
[0128 20:50:23 @common.py:109] fc0 output: [None, 512]
[0128 20:50:23 @common.py:101] fc-pi input: [None, 512]
[0128 20:50:23 @common.py:109] fc-pi output: [None, 6]
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1060 6GB
major: 6 minor: 1 memoryClockRate (GHz) 1.7845
pciBusID 0000:04:00.0
Total memory: 5.92GiB
Free memory: 5.38GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:04:00.0)
[0128 20:50:24 @sessinit.py:75] Restoring checkpoint from train_log/train-atari/model-3000000 ...
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/W/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/W/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/W/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/b/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/b/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/b/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/W/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/W/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/W/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/b/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/b/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/b/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/W/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/W/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/W/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/b/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/b/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/b/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/W/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/W/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/W/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/b/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/b/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/b/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/W/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/W/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/W/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/b/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/b/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/b/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/W/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/W/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/W/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/b/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/b/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/b/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/W/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/W/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/W/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/b/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/b/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/b/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/prelu/alpha/rms/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/prelu/alpha/rms/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/prelu/alpha/rms/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable beta1_power:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable beta2_power:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable cost/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable cost/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable cost/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable entropy_beta:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable explore_factor:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable fc-v/W:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable fc-v/b:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable global_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable input_queue_size/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable input_queue_size/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable input_queue_size/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable learning_rate:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable policy_loss/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable policy_loss/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable policy_loss/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable predict_reward/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable predict_reward/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable predict_reward/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable rms_advantage/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable rms_advantage/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable rms_advantage/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable value_loss/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable value_loss/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable value_loss/EMA:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable xentropy_loss/EMA/biased:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable xentropy_loss/EMA/local_step:0 in checkpoint not found in the graph!
[0128 20:50:24 @sessinit.py:137] WRN Variable xentropy_loss/EMA:0 in checkpoint not found in the graph!
[0128 20:50:26 @run-atari.py:72] Start evaluation: 
Traceback (most recent call last):
  File ""./run-atari.py"", line 108, in <module>
    run_submission(cfg, args.output, args.episode)
  File ""./run-atari.py"", line 76, in run_submission
    score = play_one_episode(player, predfunc)
  File ""/home/igor/tensorpack/examples/A3C-Gym/common.py"", line 31, in play_one_episode
    return np.mean(player.play_one_episode(f))
  File ""/home/igor/tensorpack/tensorpack/RL/envbase.py"", line 71, in play_one_episode
    r, isOver = self.action(act)
  File ""/home/igor/tensorpack/tensorpack/RL/history.py"", line 42, in action
    r, isOver = self.player.action(act)
  File ""/home/igor/tensorpack/tensorpack/RL/envbase.py"", line 139, in action
    return self.player.action(act)
  File ""/home/igor/tensorpack/tensorpack/RL/gymenv.py"", line 65, in action
    self.finish_episode()
  File ""/home/igor/tensorpack/tensorpack/RL/gymenv.py"", line 52, in finish_episode
    self.gymenv.monitor.flush()
  File ""/home/igor/gym/gym/core.py"", line 92, in monitor
    raise error.Error(""env.monitor has been deprecated as of 12/23/2016. Remove your call to `env.monitor.start(directory)` and instead wrap your env with `env = gym.wrappers.Monitor(env, directory)` to record data."")
gym.error.Error: env.monitor has been deprecated as of 12/23/2016. Remove your call to `env.monitor.start(directory)` and instead wrap your env with `env = gym.wrappers.Monitor(env, directory)` to record data.
[2017-01-28 20:51:23,061] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/igor/tensorpack/examples/A3C-Gym/output_dir')
",ran still problem end scoreboard via sure add line load episode output successfully library locally successfully library locally successfully library locally successfully library locally successfully library locally environment name making new making new deprecation warning replace call change made included version starting new video recorder writing input none output none pool input none pool output none input none output none pool input none pool output none input none output none pool input none pool output none input none output none input none output none input none output none found device name major minor total memory free memory device device name bus id variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph start evaluation recent call last file line module file line score player file line return file line act file line action act file line action return act file line action file line file line monitor raise remove call directory instead wrap directory record data remove call directory instead wrap directory record data finished writing scoreboard via,issue,positive,positive,positive,positive,positive,positive
275892402,"OpenAI changes the gym API in 0.7.0. 
You can either downgrade gym to an older version, or update tensorpack to the latest commit.",gym either downgrade gym older version update latest commit,issue,negative,positive,positive,positive,positive,positive
275892142,"I am sorry, I tried:

 ENV=Breakout-v0; ./run-atari.py --load train_log/train-atari/checkpoint --env ""$ENV"" --episode 500 --output output_dir

Now, it says:
raise error.Error(""env.monitor has been deprecated as of 12/23/2016. Remove your call to `env.monitor.start(directory)` and instead wrap your env with `env = gym.wrappers.Monitor(env, directory)` to record data."")
gym.error.Error: env.monitor has been deprecated as of 12/23/2016. Remove your call to `env.monitor.start(directory)` and instead wrap your env with `env = gym.wrappers.Monitor(env, directory)` to record data.
",sorry tried load episode output raise remove call directory instead wrap directory record data remove call directory instead wrap directory record data,issue,negative,negative,negative,negative,negative,negative
275884260,"When I tried ""To run a pretrained Atari model for 100 episodes"":
ENV=Breakout-v0; ./run-atari.py --load ""$ENV"".tfmodel --env ""$ENV"" --episode 100 --output output_dir

I got the following response (could you help to change the corresponding code, please?):

igor@igorfedorov:~/tensorpack/examples/OpenAIGym$ ENV=Breakout-v0; ./run-atari.py --load ""$ENV"".tfmodel --env ""$ENV"" --episode 100 --output output_dir
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
[0128 16:10:06 @run-atari.py:97] Environment Name: Breakout-v0
[2017-01-28 16:10:06,053] Making new env: Breakout-v0
[2017-01-28 16:10:06,898] Making new env: Breakout-v0
Traceback (most recent call last):
  File ""./run-atari.py"", line 109, in <module>
    run_submission(cfg, args.output, args.episode)
  File ""./run-atari.py"", line 71, in run_submission
    player = get_player(dumpdir=output)
  File ""./run-atari.py"", line 30, in get_player
    pl = GymEnv(ENV_NAME, dumpdir=dumpdir, auto_restart=False)
  File ""/home/igor/tensorpack/tensorpack/RL/gymenv.py"", line 48, in __init__
    self.gymenv.monitor.start(dumpdir)
  File ""/home/igor/gym/gym/core.py"", line 92, in monitor
    raise error.Error(""env.monitor has been deprecated as of 12/23/2016. Remove your call to `env.monitor.start(directory)` and instead wrap your env with `env = gym.wrappers.Monitor(env, directory)` to record data."")
gym.error.Error: env.monitor has been deprecated as of 12/23/2016. Remove your call to `env.monitor.start(directory)` and instead wrap your env with `env = gym.wrappers.Monitor(env, directory)` to record data.
",tried run model load episode output got following response could help change corresponding code please load episode output successfully library locally successfully library locally successfully library locally successfully library locally successfully library locally environment name making new making new recent call last file line module file line player file line file line file line monitor raise remove call directory instead wrap directory record data remove call directory instead wrap directory record data,issue,positive,positive,positive,positive,positive,positive
275883205,"Yuxin,

I tried to run 500 epochs of trained model by ...$ ENV=train_log/train-atari/checkpoint; ./run-atari.py --load ""$ENV"".tfmodel --env ""$ENV"" --episode 500 --output output_dir

but I got the following error (I use my api_key from OpenAI Gym):

raise error.Error('Attempted to look up malformed environment ID: {}. (Currently all IDs must be of the form {}.)'.format(id.encode('utf-8'), env_id_re.pattern))
gym.error.Error: Attempted to look up malformed environment ID: train_log/train-atari/checkpoint. (Currently all IDs must be of the form ^(?:[\w:-]+\/)?([\w:.-]+)-v(\d+)$.",tried run trained model load episode output got following error use gym raise look malformed environment id currently must form look malformed environment id currently must form,issue,negative,neutral,neutral,neutral,neutral,neutral
275865423,"Tried training with one GPU of batch_size 64. Get 95% top1 error after the first epoch, with TF 12.1.
So it's probably the upstream dequeue issue which makes multiple GPUs behave the same as one.",tried training one get top error first epoch probably upstream issue multiple behave one,issue,negative,positive,positive,positive,positive,positive
275861578,"Note that by default a layer should be under some variable scope as the first argument, just like Conv2D. So you should call it with: `revReLU('relu', x)`. And in LinearWrap this becomes `.revReLU('relu')`. With the latest commit this would work.

You can register it with `@layer_register(use_scope=False)` to disable the scope, then your original code would work.

Also, in your case you don't have to register it as a layer to use LinearWrap. You can use `apply`.
```python
def revReLU(x):
    return -1 * tf.nn.relu(x)
#....
c = (LinearWrap(a).Conv2D('c', 8, 7, stride=2, nl=tf.identity).apply(revReLU))()
```",note default layer variable scope first argument like call becomes latest commit would work register disable scope original code would work also case register layer use use apply python return,issue,positive,positive,positive,positive,positive,positive
275861077,"Ok it works. I will push again, when the model is trained and I have some results. Can you please change the `psnr` with your next commit?",work push model trained please change next commit,issue,positive,neutral,neutral,neutral,neutral,neutral
275859002,"The rename sounds good. It's more straightforward that way.

The error is from tf.reshape so it seems to suggest something wrong with the symbolic code. ",rename good straightforward way error suggest something wrong symbolic code,issue,negative,positive,positive,positive,positive,positive
275856197,I didn't make linearwrap to work with custom layers. Will try to fix that.,make work custom try fix,issue,negative,neutral,neutral,neutral,neutral,neutral
275847667,I am not sure if the progress bar really has to be a progress.,sure progress bar really progress,issue,positive,positive,positive,positive,positive,positive
275833923,"@Neltherion  Now you can use `PeriodicTrigger(ModelSaver(), every_k_steps=100, every_k_epochs=1)` to trigger the saver every epoch, and also every 100 steps (the counter was reset every epoch).

Also, [ModelSaver](http://tensorpack.readthedocs.io/en/latest/modules/tensorpack.callbacks.html#tensorpack.callbacks.ModelSaver) has some other options to automatically delete old checkpoints to save disk space.",use trigger saver every epoch also every counter reset every epoch also automatically delete old save disk space,issue,negative,positive,neutral,neutral,positive,positive
275832956,"It means the model at that time was not good.
Training can diverge sometimes, especially on difficult games.",model time good training diverge sometimes especially difficult,issue,negative,negative,negative,negative,negative,negative
275831477,"Yuxin,

I noticed that for Epoch 478:
[0127 22:31:02 @stats.py:101] max_score: 0
[0127 22:31:02 @stats.py:101] mean_score: 0

What could it mean?

[0127 21:46:17 @timer.py:42] Start Epoch 478 (global_step 2868000) ...
100%|##############################################|6000/6000[44:41<00:00, 2.24it/s]
[0127 22:30:58 @timer.py:46] Epoch 478 (global_step 2868000) finished, time:2681.73sec.
[2017-01-27 22:31:00,517] Making new env: Breakout-v0
0it [00:00, ?it/s]
[0127 22:31:01 @common.py:76] Waiting for all the workers to finish the last run...
[2017-01-27 22:31:01,823] Making new env: Breakout-v0
[0127 22:31:02 @stats.py:101] SummaryGradient/conv0/W/rms: 0.001804
[0127 22:31:02 @stats.py:101] SummaryGradient/conv0/b/rms: 0.04172
[0127 22:31:02 @stats.py:101] SummaryGradient/conv1/W/rms: 0.00080977
[0127 22:31:02 @stats.py:101] SummaryGradient/conv1/b/rms: 0.014749
[0127 22:31:02 @stats.py:101] SummaryGradient/conv2/W/rms: 0.00074902
[0127 22:31:02 @stats.py:101] SummaryGradient/conv2/b/rms: 0.0053119
[0127 22:31:02 @stats.py:101] SummaryGradient/conv3/W/rms: 0.00090987
[0127 22:31:02 @stats.py:101] SummaryGradient/conv3/b/rms: 0.0082322
[0127 22:31:02 @stats.py:101] SummaryGradient/fc-pi/W/rms: 0.003632
[0127 22:31:02 @stats.py:101] SummaryGradient/fc-pi/b/rms: 0.0046092
[0127 22:31:02 @stats.py:101] SummaryGradient/fc-v/W/rms: 0.023866
[0127 22:31:02 @stats.py:101] SummaryGradient/fc-v/b/rms: 0.027606
[0127 22:31:02 @stats.py:101] SummaryGradient/fc0/W/rms: 0.00017017
[0127 22:31:02 @stats.py:101] SummaryGradient/fc0/b/rms: 0.0010221
[0127 22:31:02 @stats.py:101] SummaryGradient/prelu/alpha/rms: 0.083858
[0127 22:31:02 @stats.py:101] async_global_step: 2.868e+06
[0127 22:31:02 @stats.py:101] cost: 0.0018653
[0127 22:31:02 @stats.py:101] input_queue_size: 2.2872e-37
[0127 22:31:02 @stats.py:101] learning_rate: 0.0001
[0127 22:31:02 @stats.py:101] max_score: 0
[0127 22:31:02 @stats.py:101] mean_score: 0
[0127 22:31:02 @stats.py:101] policy_loss: -1.5109
[0127 22:31:02 @stats.py:101] predict_reward: 2.9229
[0127 22:31:02 @stats.py:101] rms_advantage: 0.19438
[0127 22:31:02 @stats.py:101] value_loss: 2.6367
[0127 22:31:02 @stats.py:101] xentropy_loss: -177.4
[0127 22:31:03 @group.py:42] Callbacks took 3.605 sec in total. Periodic-Evaluator: 1.646sec; StatPrinter: 1.423sec
",epoch could mean start epoch epoch finished time making new waiting finish last run making new cost took sec total,issue,negative,negative,neutral,neutral,negative,negative
275821964,"No limit is set except max_epoch=1000. If you want to stop it when it completes you have to first define what is ""complete"".",limit set except want stop first define complete,issue,negative,positive,positive,positive,positive,positive
275821835,I am sorry for asking but I looked through the train-atari.py and do not see any other convergence parameter/limit except max_epoch=1000. I would like to let it complete the training. How to estimate how much 'Epoch's left?,sorry see convergence except would like let complete training estimate much left,issue,negative,negative,negative,negative,negative,negative
275821471,"You also need to ""--load"" your own model instead of the model you downloaded. It should be like ""train_log/train-atari/checkpoint""",also need load model instead model like,issue,negative,neutral,neutral,neutral,neutral,neutral
275821389,"thank you, and in order to run the trained model (for 500 'Epoch's) and upload to the OpenAI Gym, I should replace 100 with 500 and use my api_key? Is it right?

ENV=Breakout-v0; ./run-atari.py --load ""$ENV"".tfmodel --env ""$ENV"" --episode 100 --output output_dir",thank order run trained model gym replace use right load episode output,issue,negative,positive,positive,positive,positive,positive
275819335,"Do you think [this tqdm feature](https://github.com/tqdm/tqdm#description-and-additional-stats) works for the case here?
If so we can make it an extra option in the ProgressBar callback.",think feature work case make extra option,issue,negative,neutral,neutral,neutral,neutral,neutral
275818350,"TF 1.0.0rc0 was released yesterday. It was making quite a lot of incompatible changes, including some involving RNN/LSTM.
I found one more incompatible change from 12.1 to 1.0, so I updated the code a bit, now the code works with version 1.0.
You can find the 1.0 packages [here](https://www.tensorflow.org/versions/r1.0/get_started/os_setup#pip_installation)",yesterday making quite lot incompatible found one incompatible change code bit code work version find,issue,negative,neutral,neutral,neutral,neutral,neutral
275817081,"Now that tqdm is a callback, this can probably be implemented as a callback as well. And then users get to choose what to use.",probably well get choose use,issue,negative,neutral,neutral,neutral,neutral,neutral
275816809,You can stop it by pressing Ctrl-C (maybe several times).,stop pressing maybe several time,issue,negative,neutral,neutral,neutral,neutral,neutral
275814909,"I also noticed errors when running examples that use RNN/LSTM apis.
For instance, for PTB-LSTM example I am getting 

  File ""/home/user/TF/tpNew/tensorpack/examples/PennTreebank/reader.py"", line 118, in ptb_producer
    [batch_size, (i + 1) * num_steps])
TypeError: strided_slice() takes at least 4 arguments (3 given)

I saw that the TF APIs related to RNNs/LSTMs changed recently. Are these errors related to these API changes?",also running use instance example getting file line least given saw related recently related,issue,negative,negative,neutral,neutral,negative,negative
275812275,"Error that I get is 

```
Traceback (most recent call last):
  File ""./char-rnn.py"", line 191, in <module>
    sample(args.load, args.start, args.num)
  File ""./char-rnn.py"", line 149, in sample
    state = model.initial.eval({input_vars[0]: dummy_input})
AttributeError: 'tuple' object has no attribute 'eval'
```",error get recent call last file line module sample file line sample state object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
275808766,"Apologies for churn. I was able to make the code run by making these changes:

```
cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=param.rnn_size)
cell = tf.nn.rnn_cell.MultiRNNCell([cell] * param.num_rnn_layer)
outputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')
```

I am using TF version 0.12

After training the model, how do I generate text? I am using this command line 
`python char-rnn.py --gpu 0,1 --load train_log/char-rnn/model-XXXX sample `
and getting errors.",churn able make code run making cell cell cell cell initial version training model generate text command line python load sample getting,issue,negative,positive,positive,positive,positive,positive
275797183,"I changed the BasicLSTMCell to LSTMBlockCell and it no longer complains. But I dont know what to replace for the MultiRNNCell.
It seems the TF API has changed.
 ",longer dont know replace,issue,negative,neutral,neutral,neutral,neutral,neutral
275774187,Any clever idea to adapt this feature as a step-trigger? Overriding just the default? Or replacing the tqdm callback?,clever idea adapt feature default,issue,negative,positive,positive,positive,positive,positive
275725858,"The training is still running. Can I stop it and start using or I should wait until it finish/converge?
",training still running stop start wait,issue,negative,neutral,neutral,neutral,neutral,neutral
275722340,"Yuxin,

How many 'Epoch' does it usually take to train the model? All 1000 (max val)? My last output:
100%|####################################################|2/2[04:34<00:00, 0.01it/s]
[0127 08:58:40 @common.py:76] Waiting for all the workers to finish the last run...
[0127 08:58:42 @stats.py:101] SummaryGradient/conv0/W/rms: 0.0025302
[0127 08:58:42 @stats.py:101] SummaryGradient/conv0/b/rms: 0.04538
[0127 08:58:42 @stats.py:101] SummaryGradient/conv1/W/rms: 0.0013358
[0127 08:58:42 @stats.py:101] SummaryGradient/conv1/b/rms: 0.023961
[0127 08:58:42 @stats.py:101] SummaryGradient/conv2/W/rms: 0.0011286
[0127 08:58:42 @stats.py:101] SummaryGradient/conv2/b/rms: 0.0077145
[0127 08:58:42 @stats.py:101] SummaryGradient/conv3/W/rms: 0.0012831
[0127 08:58:42 @stats.py:101] SummaryGradient/conv3/b/rms: 0.012985
[0127 08:58:42 @stats.py:101] SummaryGradient/fc-pi/W/rms: 0.0047387
[0127 08:58:42 @stats.py:101] SummaryGradient/fc-pi/b/rms: 0.0065827
[0127 08:58:42 @stats.py:101] SummaryGradient/fc-v/W/rms: 0.028517
[0127 08:58:42 @stats.py:101] SummaryGradient/fc-v/b/rms: 0.030862
[0127 08:58:42 @stats.py:101] SummaryGradient/fc0/W/rms: 0.00021981
[0127 08:58:42 @stats.py:101] SummaryGradient/fc0/b/rms: 0.0013471
[0127 08:58:42 @stats.py:101] SummaryGradient/prelu/alpha/rms: 0.094278
[0127 08:58:42 @stats.py:101] async_global_step: 2.772e+06
[0127 08:58:42 @stats.py:101] cost: 0.014949
[0127 08:58:42 @stats.py:101] input_queue_size: 2.3282e-37
[0127 08:58:42 @stats.py:101] learning_rate: 0.0001
[0127 08:58:42 @stats.py:101] max_score: 420
[0127 08:58:42 @stats.py:101] mean_score: 407
[0127 08:58:42 @stats.py:101] policy_loss: -2.8096
[0127 08:58:42 @stats.py:101] predict_reward: 3.2488
[0127 08:58:42 @stats.py:101] rms_advantage: 0.25385
[0127 08:58:42 @stats.py:101] value_loss: 5.6258
[0127 08:58:42 @stats.py:101] xentropy_loss: -180.54
[0127 08:58:43 @group.py:42] Callbacks took 279.949 sec in total. Periodic-Evaluator: 277.974sec
[0127 08:58:43 @timer.py:42] Start Epoch 463 (global_step 2778000) ...
",many usually take train model last output waiting finish last run cost took sec total start epoch,issue,negative,positive,neutral,neutral,positive,positive
275660205,Could you post the error message?,could post error message,issue,negative,neutral,neutral,neutral,neutral,neutral
275659446,"I did get the same crash. However, nothing useful appears in the logs (IMO). Also I'm using an old, modified version of tensorpack to do my own experiments. None-the-less, below is the output of the log file, in case it helps.

[log.txt](https://github.com/ppwwyyxx/tensorpack/files/735244/log.txt)",get crash however nothing useful also old version output log file case,issue,negative,positive,positive,positive,positive,positive
275294920,"Are you also expecting ""WR"" in the first log? 
If not, then these logs look like no problem to me. The variables printed in the second log are removed from npy.",also first log look like problem printed second log removed,issue,negative,positive,positive,positive,positive,positive
275207423,"> One thing to note is that, a lot of variables that're in the checkpoint won't be in the npy dict.

 -- This makes sense.

I can attach the entire log file for both the experiments if you want.

Here is what I see when I re-start from a model trained on my machine (ResNet/imagenet-resnet.py with d=34)

```
log-model.log
Argv: imagenet-resnet.py --gpu 0,1,2,3 --data /tank/imagenet-tensorpack-data --load /home/userID/TF/tensorpack/examples/ResNet/train_log/imagenet-resnet/model-550000 -d 34
<the input and output tensorshapes messages>
<applying regularizor messages>
<filter shapes messages>
<ModelSaver messages>
[32m[0123 17:15:18 @base.py:120][0m Initializing graph variables ...
[32m[0123 17:15:23 @sessinit.py:82][0m Restoring checkpoint from /home/userID/TF/tensorpack/examples/ResNet/train_log/imagenet-resnet/model-550000 ...
[32m[0123 17:15:25 @concurrency.py:24][0m Starting EnqueueThread
[32m[0123 17:15:25 @base.py:139][0m Start training with global_step=550000
[32m[0123 17:42:18 @timer.py:46][0m Epoch 1 (global_step 555000) finished, time:1612.92sec.
```

Slightly different run (d=18) but this is what I see when I start with a pre-trained model (npy file) from modelzoo.

```
log-npy.log
Argv: ttq-modified-imagenet-resnet.py --gpu 4,5,6,7 --data /tank/imagenet-tensorpack-data --load preTrainedModels/ImageNet-ResNet18.npy -d 18
<the input and output tensorshapes messages>
<applying regularizor messages>
<filter shapes messages>
<ModelSaver messages>
[32m[0124 10:44:12 @sessinit.py:169][0m Params to restore: group0/block1/conv1/bn/variance/EMA:0, group1/block1/preact/bn/mean/EMA:0, group1/block0/preact/bn/variance/EMA:0, group0/block1/conv1/bn/beta:0, group0/block1/preact/bn/variance/EMA:0, group3/block1/conv1/W:0, group0/block1/conv1/bn/gamma:0, group2/block0/conv1/bn/beta:0, group0/block0/conv1/bn/mean/EMA:0, bnlast/bn/mean/EMA:0, group1/block1/conv2/W:0, group3/block0/conv1/bn/gamma:0, group3/block0/convshortcut/W:0, group3/block1/conv2/W:0, group2/block1/preact/bn/beta:0, group0/block1/preact/bn/gamma:0, group0/block1/preact/bn/beta:0, linear/W:0, group1/block1/conv1/bn/variance/EMA:0, conv0/W:0, group1/block0/convshortcut/W:0, group3/block1/conv1/bn/variance/EMA:0, group2/block1/conv1/bn/variance/EMA:0, group0/block0/conv1/W:0, conv0/bn/gamma:0, group2/block1/preact/bn/variance/EMA:0, group1/block1/conv1/W:0, group2/block1/preact/bn/gamma:0, group0/block0/conv1/bn/variance/EMA:0, group3/block1/preact/bn/mean/EMA:0, group2/block0/convshortcut/W:0, group3/block0/conv1/W:0, bnlast/bn/variance/EMA:0, bnlast/bn/beta:0, group0/block1/conv1/W:0, group0/block1/conv2/W:0, group2/block0/conv1/bn/mean/EMA:0, group2/block1/conv1/W:0, group1/block0/conv1/bn/mean/EMA:0, group3/block0/conv2/W:0, group3/block0/conv1/bn/mean/EMA:0, group0/block0/conv2/W:0, group0/block1/conv1/bn/mean/EMA:0, group2/block0/conv1/bn/variance/EMA:0, group1/block1/preact/bn/variance/EMA:0, group1/block0/conv1/bn/gamma:0, group3/block0/preact/bn/variance/EMA:0, group2/block1/conv2/W:0, group3/block0/preact/bn/beta:0, group3/block0/preact/bn/gamma:0, conv0/bn/beta:0, group3/block1/preact/bn/variance/EMA:0, group2/block0/conv2/W:0, linear/b:0, conv0/bn/variance/EMA:0, group2/block0/preact/bn/variance/EMA:0, group1/block0/preact/bn/beta:0, bnlast/bn/gamma:0, group1/block0/preact/bn/gamma:0, group1/block1/preact/bn/gamma:0, group1/block0/conv1/bn/variance/EMA:0, group3/block0/conv1/bn/variance/EMA:0, group0/block1/preact/bn/mean/EMA:0, group3/block0/conv1/bn/beta:0, group1/block1/conv1/bn/gamma:0, group3/block1/conv1/bn/gamma:0, group2/block0/conv1/bn/gamma:0, group0/block0/conv1/bn/beta:0, group2/block1/conv1/bn/beta:0, group0/block0/conv1/bn/gamma:0, conv0/bn/mean/EMA:0, group1/block0/preact/bn/mean/EMA:0, group1/block1/preact/bn/beta:0, group1/block0/conv2/W:0, group2/block0/preact/bn/beta:0, group1/block1/conv1/bn/beta:0, group2/block1/conv1/bn/gamma:0, group2/block0/conv1/W:0, group2/block0/preact/bn/gamma:0, group3/block1/conv1/bn/mean/EMA:0, group2/block1/conv1/bn/mean/EMA:0, group1/block1/conv1/bn/mean/EMA:0, group3/block1/preact/bn/beta:0, group1/block0/conv1/W:0, group2/block1/preact/bn/mean/EMA:0, group2/block0/preact/bn/mean/EMA:0, group3/block1/conv1/bn/beta:0, group3/block0/preact/bn/mean/EMA:0, group3/block1/preact/bn/gamma:0, group1/block0/conv1/bn/beta:0
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable cost/EMA/biased:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable cost/EMA/local_step:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable cost/EMA:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable global_step:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable group0/block0/conv1/WR:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable group0/block0/conv2/WR:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable group0/block1/conv1/WR:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable group0/block1/conv2/WR:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable group1/block0/conv1/WR:0 in the graph not found in the dict!
...
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable input_queue_size/EMA/biased:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable input_queue_size/EMA/local_step:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable input_queue_size/EMA:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable l2_regularize_loss/EMA/biased:0 in the graph not found in the dict!
[32m[0124 10:44:12 @sessinit.py:172][0m [5m[31mWRN[0m Variable l2_regularize_loss/EMA/local_step:0 in the graph not found in the dict!
...
[32m[0124 10:44:12 @sessinit.py:179][0m Restoring from dict ...
[32m[0124 10:44:19 @concurrency.py:24][0m Starting EnqueueThread
[32m[0124 10:44:19 @base.py:139][0m Start training with global_step=0
[32m[0124 11:08:37 @timer.py:46][0m Epoch 1 (global_step 5000) finished, time:1458.37sec.
```

The variables WR are some constant multipliers I added to the model in each experiment. But as you see the run with .npy loading prints it out and the one with model restore doesnt. 

",one thing note lot wo sense attach entire log file want see model trained machine data load input output filter graph starting start training epoch finished time slightly different run see start model file data load input output filter restore variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found variable graph found starting start training epoch finished time constant added model experiment see run loading one model restore doesnt,issue,positive,neutral,neutral,neutral,neutral,neutral
275168832,"I. You can just use self.cost in the graph however you like (print it with tf.Print, skip training conditioned on it (with tf.cond and tf.stop_gradient). It __is__ the cost of the current datapoint before backprop happens.
Maybe I lost some context here, but you don't need trigger_step to do this.
If you want to write some Python logic based on the loss, now you can do it by the new Callback interface with trigger_step:
```python
def _extra_fetches(self):
    return ['name of the cost tensor:0']
def _trigger_step(self, cost):
    print(cost)
```

II. (i) The queue occupancy statistics was added for quite a while.
(ii) Do you mean you want the time of the minimization op only, but don't want others?
For efficiency, the other ops that you need in each step (like the callbacks I drafted above) are run together with the minimization op in one sess.run call, so the time alone cannot be measured.
The time of the sess.run call and the time you process the data (the trigger_step method) can be measured separately. Is that really necessary? I'm expecting trigger_step should never do anything heavy, and you can tell whether something is heavy or not from the training speed already.

III. Now you can use your own timer in trigger_step. I even implemented the [progress bar](https://github.com/ppwwyyxx/tensorpack/blob/a51e2de463d7b9bf62618e46c3c14529344f8dc3/tensorpack/callbacks/steps.py#L71) as a callback (which means you can use a different progress bar). But as I said, you'll only be able to measure the total time of running the graph + running all the trigger_step from all callbacks.",use graph however like print skip training conditioned cost current maybe lost context need want write python logic based loss new interface python self return cost tensor self cost print cost queue occupancy statistic added quite mean want time minimization want efficiency need step like run together minimization one call time alone measured time call time process data method measured separately really necessary never anything heavy tell whether something heavy training speed already use timer even progress bar use different progress bar said able measure total time running graph running,issue,positive,negative,neutral,neutral,negative,negative
275009394,"I think they should've produced similar warning messages, could you post what you saw?

One thing to note is that, a lot of variables that're in the checkpoint won't be in the npy dict. The npy dict I published was intended for inference, so a lot of summary variables and internal state of optimizer were removed already. For ResNet this could make the npy dict about 1/2 size of the checkpoint.",think produced similar warning could post saw one thing note lot wo intended inference lot summary internal state removed already could make size,issue,negative,neutral,neutral,neutral,neutral,neutral
275003757,"You can simply change the one line which reads the mnist dataset to two other lines which reads the CelebA dataset like the DCGAN-CelebA.py example.
But you'll certainly need to change your network architecture and it's impossible to do it automatically.
I would refuse to use command line to define complicated things like a network, because it only makes things less flexible. A .py file can expose everything configurable to you, but a short command line string only has a small portion of its ability. You can certainly add the parameters you want to customize to command line options, by just one line of `parser.add_argument`, but I wouldn't do that in the examples because different people want to customize different parts.

What's in #107 is only about InfoGAN but not improved GAN.",simply change one line two like example certainly need change network architecture impossible automatically would refuse use command line define complicated like network le flexible file expose everything short command line string small portion ability certainly add want command line one line would different people want different gan,issue,positive,negative,negative,negative,negative,negative
274951593,"Another issue with the wait infogan is written is that it's not easily configurable for nonmnist datasets. It would be nice to make it more flexible for other datasets. Other infogan [repos](https://github.com/JonathanRaiman/tensorflow-infogan) even let you set the discriminator and generator via the command line. Expanding infogan to run on different datasets would really also show if tensorpack is flexible. Just a thought. Using infogan on the celebA dataset for instance would be a good exercise. 

Any updates on the improved GAN? Was it included in #107 or was that just the groundwork? Looking forward to trying out the improvements.",another issue wait written easily would nice make flexible even let set discriminator generator via command line expanding run different would really also show flexible thought instance would good exercise gan included groundwork looking forward trying,issue,positive,positive,positive,positive,positive,positive
274912164,"I noticed one thing - ParamRestore and SaverRestore dont output similar warning messages. 

ParamRestore prints WRN messages for variables not present in the restored graph but the SaverRestore doesnt.

Would be helpful if these are both consistent.
",one thing dont output similar warning present graph doesnt would helpful consistent,issue,negative,positive,neutral,neutral,positive,positive
274692982,"On imagenet larger batch usually help improve the model. If you made it smaller to fit on your GPU it may get worse.
I'm just using the batch size 256 used by fb.resnet.torch. It probably will train better with a larger batch size.

There're some other discussions [here](https://github.com/tornadomeet/ResNet/issues/28) about batch size. They tried larger batch size.",batch usually help improve model made smaller fit may get worse batch size used probably train better batch size batch size tried batch size,issue,positive,positive,neutral,neutral,positive,positive
274689171,"I see. 
I had changed the batch size to fit on 2 GPUs. I am getting 0.65% higher error rate than you reported. 

> On Jan 23, 2017, at 6:53 PM, Yuxin Wu <notifications@github.com> wrote:
> 
> The total batch size is fixed so it shouldn't be very sensitive to the number of GPUs.
> Some of the models are trained with 2 GPUs and some with 4. I couldn't remember which is which.
> What's the number you get?
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",see batch size fit getting higher error rate wrote total batch size fixed sensitive number trained could remember number get state reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
274687777,"Currently the script only tries to initialize from a tensorflow checkpoint.
You can change it to load from a dictionary.
```
-        config.session_init = SaverRestore(args.load)
+        config.session_init = ParamRestore(np.load(args.load, encoding='latin1').item())
```",currently script initialize change load dictionary,issue,negative,neutral,neutral,neutral,neutral,neutral
274686987,"The total batch size is fixed so it shouldn't be very sensitive to the number of GPUs.
Some of the models are trained with 2 GPUs and some with 4. I couldn't remember which is which.
What's the number you get?",total batch size fixed sensitive number trained could remember number get,issue,negative,positive,neutral,neutral,positive,positive
274682401,"You can use 
```
.Conv2D(....)
.print_tensor()
.Conv2D(...)
```
Also, for named layers like Conv2D, Deconv2D, the output name is usually just ""prediction/output"", or ""deconv7/output"". The documentation would [mention](http://tensorpack.readthedocs.io/en/latest/modules/tensorpack.models.html#tensorpack.models.Conv2D) the output name.",use also like output name usually documentation would mention output name,issue,negative,negative,negative,negative,negative,negative
274666086,"How many GPU cores did you use for training each of these ResNet configurations? 
I am not able to get to the same accuracy level that you mention for ResNet-34 with 2 GPUs.

In general, I have found the accuracy (which depends on learning rate schedule) to be quite sensitive to the number of GPUs. Is this your observation as well? Any workaround/tips/tricks you use to get around this issue?

Thanks",many use training able get accuracy level mention general found accuracy learning rate schedule quite sensitive number observation well use get around issue thanks,issue,positive,positive,positive,positive,positive,positive
274561268,"Thanks so much... The problem is gone and the only thing that remains is that there's some sort of scope added to the prediction tensor's name based on the tower.

Here's what I'm calling:

`self.trainer.get_predict_func(input_names=['luminance'], output_names=['gen/prediction'])`

and here's the naming error:

`""The name 'towerp0/gen/prediction:0' refers to a Tensor which does not exist. The operation, 'towerp0/gen/prediction', does not exist in the graph.""`

I read your tutorial on how to get the name of tensors in Tensorpack but how should we do it when they are combined in one scope like the code below:

```
 with argscope(Deconv2D, nl=BNReLU, kernel_shape=4, stride=2):
                return (LinearWrap(e8)
                        .Deconv2D('deconv1', NF * 8)
                        .Dropout()
                        .ConcatWith(3, e7)
                        .Deconv2D('deconv2', NF * 8)
                        .Dropout()
                        .ConcatWith(3, e6)
                        .Deconv2D('deconv3', NF * 8)
                        .Dropout()
                        .ConcatWith(3, e5)
                        .Deconv2D('deconv4', NF * 8)
                        .ConcatWith(3, e4)
                        .Deconv2D('deconv5', NF * 4)
                        .ConcatWith(3, e3)
                        .Deconv2D('deconv6', NF * 2)
                        .ConcatWith(3, e2)
                        .Deconv2D('deconv7', NF * 1)
                        .ConcatWith(3, e1)
                        .Deconv2D('prediction', OUT_CH)())
```",thanks much problem gone thing remains sort scope added prediction tensor name based tower calling naming error name tensor exist operation exist graph read tutorial get name combined one scope like code return,issue,negative,positive,positive,positive,positive,positive
274557132,"I use 
````
self._predictor_factory = PredictorFactory(self.sess, self.model, [0])
````

**after** the `super.init`. You just need to switch the lines in the constructor.",use need switch constructor,issue,negative,neutral,neutral,neutral,neutral,neutral
274553024,"Sorry for dragging this but running `self._setup_predictor_factory()` results in calling `self._predictor_factory = PredictorFactory(self.sess, self.model, self.config.predict_tower)` but then I get this error : `'GANTrainer' object has no attribute 'sess'`
",sorry dragging running calling get error object attribute,issue,negative,negative,negative,negative,negative,negative
274551333,"Just for information: it has being training for ~16 days (500 'Epoch's) on the following hardware (let me edit the post when it's done):
GPU: GeForce GTX 1060 6Gb
CPU: AMD Phenom 9950 Agena Quad-Core 2.6 GHz
memory: 8Gb",information training day following hardware let edit post done memory,issue,negative,neutral,neutral,neutral,neutral,neutral
274549179,"Yes the GAN trainer doesn't implement the interface so it's using the base class.
One simple way to make GANTrainer support this kind of inference, is:
```
--- i/examples/GAN/GAN.py
+++ w/examples/GAN/GAN.py
@@ -7,7 +7,8 @@ import tensorflow as tf
 import numpy as np
 import time
 from tensorpack import (FeedfreeTrainerBase, TowerContext,
-                        get_global_step_var, QueueInput, ModelDesc)
+                        get_global_step_var, QueueInput, ModelDesc,
+                        MultiPredictorTowerTrainer)
 from tensorpack.tfutils.summary import add_moving_summary
 from tensorpack.tfutils.gradproc import apply_grad_processors, CheckGradient
 from tensorpack.dataflow import DataFlow
@@ -68,9 +69,10 @@ class GANModelDesc(ModelDesc):
         return [CheckGradient()]


-class GANTrainer(FeedfreeTrainerBase):
+class GANTrainer(FeedfreeTrainerBase, MultiPredictorTowerTrainer):
     def __init__(self, config):
         self._input_method = QueueInput(config.dataflow)
+        self._setup_predictor_factory()
         super(GANTrainer, self).__init__(config)

     def _setup(self):
```
Then you'll be able to use ""get_predict_func"".
This is not like an official solution -- it's quite weird actually. I'm doing some refactoring of the trainers to make things more straightforward.",yes gan trainer implement interface base class one simple way make support kind inference import import import time import import import import class return self super self self able use like official solution quite weird actually make straightforward,issue,positive,positive,neutral,neutral,positive,positive
274546968,Right now I'm using the Image2Image example for colorization and just want to add a callback for converting the LAB images and saving them. I've made modifications so that the code accepts LAB images but the last part is to modify it for the callback...right now I don't know how to modify Image2Image to accept the Callback...,right example colorization want add converting lab saving made code lab last part modify right know modify accept,issue,negative,positive,positive,positive,positive,positive
274545701,Thanks... I'll take a look and hopefully will close the issue soon...,thanks take look hopefully close issue soon,issue,positive,positive,positive,positive,positive,positive
274545323,"The documents only contain snippets but not a complete script, so there might be confusions.
What trainer are you using? It looks like you're using ""Trainer"" , but ""Trainer"" is just a base class. You should use ""SimpleTrainer"" or ""QueueInputTrainer"".
Since there isn't a complete script for it, you might meet other problems as well. Currently all the tutorial pages are just drafts, that's why there isn't a link to the tutorial. Before the tutorial is finished you can take a look at examples/ to find some actual scripts.",contain complete script might trainer like trainer trainer base class use since complete script might meet well currently tutorial link tutorial tutorial finished take look find actual,issue,positive,negative,negative,negative,negative,negative
274358672,"Yuxin,

may I ask you what kind of hardware you currently use?",may ask kind hardware currently use,issue,positive,positive,positive,positive,positive,positive
274315277,You accidentally used an old Readme during  merging ;-) But really nice edits!,accidentally used old really nice,issue,negative,positive,positive,positive,positive,positive
274314513,The bottleneck is probably the CPU computation. It needs to simulate dozens of games at the same time.,bottleneck probably computation need simulate time,issue,negative,neutral,neutral,neutral,neutral,neutral
274249810,"In this case, is the bottle neck the transfer of data between CPU and GPU or it is something else?",case bottle neck transfer data something else,issue,negative,neutral,neutral,neutral,neutral,neutral
274249781,This one is only quad-core. Normally a server CPU would have 12~40 cores.,one normally server would,issue,negative,positive,positive,positive,positive,positive
274249509,"Thank you! yes, the cpu is not that great : https://www.newegg.com/Product/Product.aspx?Item=N82E16819103291

How the cpu might affect training?",thank yes great might affect training,issue,positive,positive,positive,positive,positive,positive
274249422,"```
[0121 00:22:28 @stats.py:101] max_score: 864
[0121 00:22:28 @stats.py:101] mean_score: 543.19
```
So it already have very good score. It learns very well.

From the log I cannot tell why it trains very slow. Maybe it's because the CPU is not fast enough for the simulation.",already good score well log tell slow maybe fast enough simulation,issue,positive,positive,positive,positive,positive,positive
274083723,"Thanks @PatWie for the example code. I ended up going with @ppwwyyxx 's approach, which worked after I removed references to `EXTRA_SAVE_VARS_KEY` in `scripts/dump-model-params.py`. The log got overwritten somehow, but I'm running another (also duplicate) experiment - if I end up with the same crash I'll attach the appropriate log files.",thanks example code ended going approach worked removed log got somehow running another also duplicate experiment end crash attach appropriate log,issue,negative,positive,positive,positive,positive,positive
274000367,I see. I'll have to write it differently to avoid importing some external packages by mistake. Thanks for finding it out.,see write differently avoid external mistake thanks finding,issue,negative,positive,neutral,neutral,positive,positive
273999666,"The authors recently open sourced their implementation here: https://github.com/czhu95/ternarynet

It is based on tensorpack!

I have a version but if you want to replicate the numbers in the paper it's better to go with their official version. 

> On Jan 19, 2017, at 10:41 PM, Yuxin Wu <notifications@github.com> wrote:
> 
> @thadpasce16 has an implementation. I haven't got time to test it but I was told it had good results.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",recently open implementation based version want replicate paper better go official version wrote implementation got time test told good reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
273998942,"I see why.
Because I have installed the dataset package (a database interface package) before and there is a naming conflict.",see package interface package naming conflict,issue,negative,neutral,neutral,neutral,neutral,neutral
273994537,"The [CI](https://travis-ci.org/ppwwyyxx/tensorpack) can import tensorpack, so it's not likely to be a problem of the code.

Do you happen to have a file called freeze.pyc somewhere? (e.g. in tensorpack/dataflow/).",import likely problem code happen file somewhere,issue,negative,neutral,neutral,neutral,neutral,neutral
273994082,"$ python mnist-convnet.py
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
Traceback (most recent call last):
  File ""mnist-convnet.py"", line 19, in <module>
    from tensorpack import *
  File ""/home/yangchen/projects/work/binarized/tensorpack/tensorpack/__init__.py"", line 8, in <module>
    from tensorpack.train import *
  File ""/home/yangchen/projects/work/binarized/tensorpack/tensorpack/train/__init__.py"", line 29, in <module>
    global_import(module_name)
  File ""/home/yangchen/projects/work/binarized/tensorpack/tensorpack/train/__init__.py"", line 13, in global_import
    p = __import__(name, globals(), locals(), level=1)
  File ""/home/yangchen/projects/work/binarized/tensorpack/tensorpack/train/base.py"", line 13, in <module>
    from .config import TrainConfig
  File ""/home/yangchen/projects/work/binarized/tensorpack/tensorpack/train/config.py"", line 7, in <module>
    from ..callbacks.group import Callbacks
  File ""/home/yangchen/projects/work/binarized/tensorpack/tensorpack/callbacks/__init__.py"", line 28, in <module>
    _global_import(module_name)
  File ""/home/yangchen/projects/work/binarized/tensorpack/tensorpack/callbacks/__init__.py"", line 13, in _global_import
    p = __import__(name, globals(), locals(), level=1)
  File ""/home/yangchen/projects/work/binarized/tensorpack/tensorpack/callbacks/inference_runner.py"", line 11, in <module>
    from ..dataflow import DataFlow
  File ""/home/yangchen/projects/work/binarized/tensorpack/tensorpack/dataflow/__init__.py"", line 29, in <module>
    _global_import(module_name)
  File ""/home/yangchen/projects/work/binarized/tensorpack/tensorpack/dataflow/__init__.py"", line 16, in _global_import
    p = __import__(name, globals(), locals(), level=1)
ImportError: No module named freeze



I see there is no dependency for freeze.
And it gives the same error even I installed the freeze package.
",python successfully library locally successfully library locally successfully library locally successfully library locally successfully library locally recent call last file line module import file line module import file line module file line name file line module import file line module import file line module file line name file line module import file line module file line name module freeze see dependency freeze error even freeze package,issue,positive,positive,positive,positive,positive,positive
273989808,@thadpasce16 has an implementation in tensorpack. I haven't got time to test it but I was told it had good results.,implementation got time test told good,issue,negative,positive,positive,positive,positive,positive
273970553,"In ternary weights, using different scale weight for positive and negative could improve the accuracy, the paper is here https://arxiv.org/abs/1612.01064, does anybody have ideals of how to implement it in this tensorpack framework? 
",ternary different scale weight positive negative could improve accuracy paper anybody implement framework,issue,negative,negative,neutral,neutral,negative,negative
273697668,"Please see the above answer.  And there is a script ""scripts/dump-model-params.py"" which does what the above script is doing.
Also @nickfraser do you know why it crashed? If not could you post some logs so that I can check if it is my problem.",please see answer script script also know could post check problem,issue,negative,neutral,neutral,neutral,neutral,neutral
273646858,Yes even the figure in the paper has some 7 vs 9 confusion.,yes even figure paper confusion,issue,negative,neutral,neutral,neutral,neutral,neutral
273559317,"This gives

![infogan-mnist](https://cloud.githubusercontent.com/assets/6756603/22077369/39f5a40c-ddb4-11e6-8b32-6ed6e7acbb60.jpg)

The issue with the ""7"" and ""9"" columns can be probably solved by other hyper-parameters or even another run. Even the previous version had some trouble about ""1"" and ""7"". The paper says:

> Unless noted otherwise, learning rate is 2e-4 for D and 1e-3 for G; λ is set to 1.",issue probably even another run even previous version trouble paper unless noted otherwise learning rate set,issue,negative,negative,negative,negative,negative,negative
273557069,"Reading values into numpy arrays is pretty straight forward:


````python
# -*- coding: UTF-8 -*-

import tensorflow as tf
import os

# read Graph + Checkpoint
with tf.Session() as sess:
    graph_path = ""path/to/graph/foo.meta""
    model_path = os.path.dirname(graph_path)

    loader = tf.train.import_meta_graph(graph_path)
    loader.restore(sess, tf.train.latest_checkpoint(model_path))
    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
    for v in train_vars:
        name = v.name
        tshape = v.get_shape()
        # numpy from here
        value = v.eval()
        shape = value.shape
        print name, tshape, shape

````",reading pretty straight forward python import import o read graph sess loader sess name value shape print name shape,issue,positive,positive,positive,positive,positive,positive
273544503,"Thank you very much.

On Sat, Jan 14, 2017 at 8:03 PM, Yuxin Wu <notifications@github.com> wrote:

> I didn't know that would happen. I was ignoring this error all the time
> and things are fine.
> Maybe there are some changes in gym that make this error matter? I'll try
> it when I have time.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/97#issuecomment-272619956>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AWiZ4HEjyaaVdZDI6Ga0Ty0peZ3Ygsxgks5rSLmWgaJpZM4Lf02L>
> .
>
",thank much sat wrote know would happen error time fine maybe gym make error matter try time thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
273525342,"Well, I haven't looked at their code in detail and started from your code. I just wrote a LaTeX file with all ugly calculations. In the end the entire InfoGan is simply a weighted sum of mutual information (including the GAN objective). Maybe I missed something. It is a good idea to get a 1:1 mapping to their code as a sanity check. 

I did not get the last point and have to look it up. 

Edit:

- there is only one entropy term in the objective
- maybe i should improve the documentation here
- the `sample` also produces a batch
- consider the uniform-distribution class: it samples also have mean 0

Currently, I am surprised why I expected these nice results because it is really a product of independent distributions. So it has to look the way from the post above. ",well code detail code wrote latex file ugly end entire simply weighted sum mutual information gan objective maybe something good idea get code sanity check get last point look edit one entropy term objective maybe improve documentation sample also batch consider class also mean currently nice really product independent look way post,issue,positive,positive,neutral,neutral,positive,positive
273523895,"There are some problems I found:
* Naming confusions. Comparing to the original InfoGAN code, your ""sample"" is actually their ""sample_prior"", and they do have a ""sample"" method that does ""sampling from a batch of dist_info"" as one could expect from the name. Your ""entropy"" is actually ""entropy under the prior distribution"" but their ""entropy"" is different (and not used). ""activate_dist"" was OK (it performs an activation) but ""model_param"" is probably more confusing (?).
* You probably can check the static shape of input/output in the base class. For example, prior()  should return some thing with shape ""param_dim"". This would be very helpful especially when now it's confusing what these methods really mean.
* They have ""prior_dist_info"" (like your ""prior"" but returns a batch), and a ""sample"" method which takes a batch of ""dist_info"". Combining the two would give a ""sample_prior"" which is your ""sample"". This naturally solves the 1D/2D shape problem you're having as the TODO, as you can then require loglikelihood to take 2D input.
* From their code the ""prior"" of Uniform should be zero-mean, unit-stddev, and ""sample_prior"" is override to not sample from this prior. The prior in your code is different, and this will affect how ""entropy under prior"" is computed. Perhaps this explains your results.",found naming original code sample actually sample method sampling batch one could expect name entropy actually entropy prior distribution entropy different used activation probably probably check static shape base class example prior return thing shape would helpful especially really mean like prior batch sample method batch combining two would give sample naturally shape problem require take input code prior uniform override sample prior prior code different affect entropy prior perhaps,issue,positive,negative,neutral,neutral,negative,negative
273487153,"> The uniform distribution doesn't look as good as the papers (from figure and curve), is that what you mean?

Yeah, I do not get why my results are not correct.

> The current code looks good, thanks a lot for that! Could you put the changes related to improved GAN in a separate PR?

The only change mentioned in #105 is currently ""swap labels"". But anyway when merging this one needs to update image2image, ... a well. If you say these changes are ok, I would adjust the other examples to make them work with these changes this evening.",uniform distribution look good figure curve mean yeah get correct current code good thanks lot could put related gan separate change currently swap anyway one need update well say would adjust make work evening,issue,positive,positive,positive,positive,positive,positive
273485254,"The uniform distribution doesn't look as good as the papers (from figure and curve), is that what you mean?

The current code looks good, thanks a lot for that! Could you put the changes related to improved GAN in a separate PR? Then this PR would be only about InfoGAN and easier to manage.
The other PR might depend on GANModelDesc so I'll try to merge this soon.",uniform distribution look good figure curve mean current code good thanks lot could put related gan separate would easier manage might depend try merge soon,issue,positive,positive,positive,positive,positive,positive
273452967,"@ppwwyyxx Can you have a look before at the InfoGAN before I adjust the other examples.

![graph](https://cloud.githubusercontent.com/assets/6756603/22062248/13bb3864-dd79-11e6-962d-80f2feb53799.jpg)

I would say it definitely learns some reasonable mappings from a latent factor ~ uni(-1, 1)

![latent](https://cloud.githubusercontent.com/assets/6756603/22062485/4361260e-dd7a-11e6-8ff2-04c46e6b951b.jpg)

![loss](https://cloud.githubusercontent.com/assets/6756603/22062504/625bb362-dd7a-11e6-9bf9-53284a199d05.jpg)



But it is not what I expected.",look adjust graph would say definitely reasonable latent factor latent loss,issue,negative,positive,positive,positive,positive,positive
273412184,"The printing only happens if you use the layers defined in [tensorpack.models](http://tensorpack.readthedocs.io/en/latest/modules/tensorpack.models.html).

You can use slim but slim doesn't have the print feature.",printing use defined use slim slim print feature,issue,negative,neutral,neutral,neutral,neutral,neutral
273395845,"Another question, is there a reason why eps is different? I saw 1e-12, 1e-10 and 1e-6 in the file.
1e-6 seems much larger and you only use it on sqrt, so I wonder this.",another question reason different saw file much use wonder,issue,negative,positive,neutral,neutral,positive,positive
273382657,I think I found it. Its already in the log file.,think found already log file,issue,negative,neutral,neutral,neutral,neutral,neutral
273374918,"They are available in the [model zoo](https://drive.google.com/drive/u/3/folders/0B9IPQTvr2BBkM0dUYTdZdm1YenM).

Btw, I read that simpler architectures like VGG/resnet is better in terms of transfer learning than inception. ",available model zoo read simpler like better transfer learning inception,issue,positive,positive,positive,positive,positive,positive
273007439,"I'm not sure what's the best way. Dummy data would work for some of the examples, but for others like mnist, it has to download the whole dataset. 
But maybe we'll only test those that work with dummy data? Sounds OK.",sure best way dummy data would work like whole maybe test work dummy data,issue,positive,positive,positive,positive,positive,positive
272940635,"> Maybe it's better to write several dummy training scripts containing lots of features for testing. This may avoid downloading mnist or prepare dummy data for examples. What do you think?

Do you really want to maintain additional examples without any function? Maybe just putting some images (some of image2image produced to not violate copyrights) and a small textfile into a repo  for `wget` then within travis. The release to run the real examples on very small datasets.

This will require a more flexible testing with teardown to clean up dirs such as `train_logs`",maybe better write several dummy training lot testing may avoid prepare dummy data think really want maintain additional without function maybe produced violate small within travis release run real small require flexible testing teardown clean,issue,negative,positive,neutral,neutral,positive,positive
272891670,"About the naming in examples, I started to use ucfirst for directories simply because I found a lot of them starts with abbreviations.. like HED, GAN, A3C, DQN. So I used uppercase for all of them.
Other files are lower-case unless they need to be imported (then it has to be lower_case).",naming use simply found lot like gan used unless need,issue,negative,neutral,neutral,neutral,neutral,neutral
272768477,"About improved GAN, it interests me as well because it introduces a lot of new ways of training a model, so it would be good to implement them to test whether the design of tensorpack is really flexible.

About InfoGAN, @PatWie could you elaborate what do you mean by ""correct log-likelihood""? I used OpenAI code as a reference when I wrote the examples but maybe I missed something.
Also it would be nice to have distribution classes like the original code.",gan well lot new way training model would good implement test whether design really flexible could elaborate mean correct used code reference wrote maybe something also would nice distribution class like original code,issue,positive,positive,positive,positive,positive,positive
272766626,"The xxx.meta generated by tensorpack is a MetaGraphDef protobuf.
The scripts expects a GraphDef protobuf.

You can take GraphDef from a MetaGraphDef like this:
```python
from tensorflow.core.protobuf import meta_graph_pb2
G = meta_graph_pb2.MetaGraphDef()
data = open('input.meta.pb', 'rb').read()
G.ParseFromString(data)
G = G.graph_def
open('graph.pb', 'wb').write(G.SerializeToString())
```

I'm not sure if it will work for the script then. I don't know what that script is doing.",take like python import data open data open sure work script know script,issue,positive,positive,positive,positive,positive,positive
272766166,"You'll also need to fix tf.nn.rnn, replace it by tf.contrib.rnn.static_rnn.

Maybe it's better to write several dummy training scripts containing lots of features for testing. This may avoid downloading mnist or prepare dummy data for examples. What do you think? ",also need fix replace maybe better write several dummy training lot testing may avoid prepare dummy data think,issue,negative,positive,positive,positive,positive,positive
272721389,"@PatWie I've noticed some difference on my own dataset with improved GAN. It seemed to help with my dataset in terms of yielding qualitatively better results at earlier epochs on their implementation. I maybe mistaken but isn't one important part of the paper you are missing is feature matching in section 3.1? Or have you called it something else? It also sounds like it will fix the issue you are having currently where the GAN is unstable. That sounds like possibly the most important improvement in the paper, but I could be mistaken.",difference gan help yielding qualitatively better implementation maybe mistaken one important part paper missing feature matching section something else also like fix issue currently gan unstable like possibly important improvement paper could mistaken,issue,positive,positive,positive,positive,positive,positive
272716993,"I have implemented these changes in my version:
- mini-batch discrimination
- switched the labels 0 <-> 1
- label-smoothing

However, I did not observe any (dramatic) changes in the examples. The results always look different in seperated runs. Did you run some tests with these changes? I can made another pull-request. But I do not want to exhaust @ppwwyyxx time with multiple pull-requests if they have no big improvement.

But the OpenAI guys have the correct *log*-likehood term in InfoGAN :wink:  I don't know what @ppwwyyxx's plans are. But the next example I planned to move to tensorpack is a clean-up of InfoGAN (GANTrainer should be a derived class of SimpleTrainer btw) with a distribution-class for different codes.",version discrimination switched however observe dramatic always look different run made another want exhaust time multiple big improvement correct log term wink know next example move derived class different,issue,negative,negative,neutral,neutral,negative,negative
272713654,"1. `change_gpu` is really nice!
2. fixed. I added variables during playing around with these loss-functions
3. it's a bad habit of mine
4. I removed it. But it would be a nice and helpful script. Avconv should be the replacement for ffmpeg on Ubuntu 16.

Any naming-conventions? Upper-case seems strange. 
- directories: ucfirst
- scripts: lower-case
- images lower-case (jpgs)?",really nice fixed added around bad habit mine removed would nice helpful script replacement strange,issue,positive,positive,positive,positive,positive,positive
272712629,"I will apply these changes 1-3.
Unfortunately, there is no way using OpenCV for animated gifs. But I can remove the animated-gif script. I though the `scripts` folder is more or less a collection of scripts.",apply unfortunately way animated remove script though folder le collection,issue,negative,negative,negative,negative,negative,negative
272712462,"Thanks! I'll run it some time to figure out what's happening. Some general comments:
1. I use --gpu everywhere only because it's shorter than CUDA_VISIBLE_DEVICES, but I hope not to provide a set of predefined ""standard"" arguments, at least not now. That reminds me of caffe where I'll need to make some changes in code, some in config file, some in command line.
It will look better to me if the interface is different, such as one line of `enable_common_flags()`, to make it clearer this is optional. But one single line of `os.environ['CUDA_VISIBLE_DEVICES']` is probably not worth the complexity to introduce new utilities either.
2. There is no need to use variable_scope and the reuse option, because there are no variables defined in those functions. (If there are, I would put them in `models/` instead of `symbolic_functions`.) You probably want `tf.name_scope`.
3. the argscope from slim would make things complicated because I had an argscope for models/ already. And seem like you're not using them anyway.
I think argscope only helps when you need to call a function with __long argument list__ for __multiple times__. This is usually not true for what's in `symbolic_functions`.
4. animate.py introduces too many dependencies, but putText and VideoWriter in cv2 should be enough (may not support gif format specifically, though). But if I were to write it, I'll just write one function which produces animation from a iterator of images. I wouldn't assume people want to put text at (0,0).
Also, if you just use it to create animation from files, ffmpeg might be simpler.",thanks run time figure happening general use everywhere shorter hope provide set standard least need make code file command line look better interface different one line make clearer optional one single line probably worth complexity introduce new either need use reuse option defined would put instead probably want slim would make complicated already seem like anyway think need call function argument usually true many enough may support gif format specifically though write write one function animation would assume people want put text also use create animation might simpler,issue,positive,positive,neutral,neutral,positive,positive
272698404,"Don't worry that's a bug of mine, should be fixed already.",worry bug mine fixed already,issue,negative,positive,neutral,neutral,positive,positive
272688151,"@ppwwyyxx 
Do you know what is going on with the python3 version [here](https://travis-ci.org/ppwwyyxx/tensorpack/jobs/192083670). I only use python2.",know going python version use python,issue,negative,neutral,neutral,neutral,neutral,neutral
272639574,"I think I'll merge it after clean-up the README a bit. I planned to use fewer images and stack images together to leave space for other type of visualizations. For example I really like Fig.3 in https://arxiv.org/pdf/1412.6806v3.pdf.

I'm OK with the term saliency map, since the paper is using it. 
I moved the saliency function out because I was confused with the name myself. People could understand the function very easily by looking at the code alone but might get confused by looking at the name and document. Since it is well-defined in the literature we can move it back and explain it better.",think merge bit use stack together leave space type example really like fig term map since paper function confused name people could understand function easily looking code alone might get confused looking name document since literature move back explain better,issue,negative,positive,neutral,neutral,positive,positive
272636806,"> A better solution will be there after step-callbacks are ready.

Then I'll be waiting... Thanks!",better solution ready waiting thanks,issue,positive,positive,positive,positive,positive,positive
272636504,"@Neltherion You can use for example `PeriodicCallback(ModelSaver(), 10)` to let ModelSaver get triggered less frequently, if you feel that models are taking too much space.
A better solution will be there after step-callbacks are ready.",use example let get triggered le frequently feel taking much space better solution ready,issue,positive,positive,positive,positive,positive,positive
272636208,"How many images do you want to dump after each epoch?

I use something related in my private examples to generate an animation from image files. 
There is a small writeup how I export Images directly to JPEG without `tf.summary` in  [OnlineExporter](http://tensorpack.readthedocs.io/en/latest/casestudies/colorize.html#callbacks). There are two ways:
- preload some images
- use another dataflow instance",many want dump epoch use something related private generate animation image small export directly without two way use another instance,issue,negative,positive,neutral,neutral,positive,positive
272635871,"Great! Thanks...

by the way, I wanted to use steps_per_epoch so that I could see the image summaries in Tensorboard but this ends up pretty costly... for example after every 5 steps the logfile `events.out.tfevents.148441180....` gets updated with the summaries but we also end up with huge model files like `model-5.data-00000-of-00001` which is in my case a 670MB file... now this gets repeated and after 10 epochs we have 10x670MB less storage...

can we update the summaries just fine while just overwriting the model file so that in the end we end up with one model instead of one for every step of the way?",great thanks way use could see image pretty costly example every also end huge model like case file repeated le storage update fine model file end end one model instead one every step way,issue,positive,positive,positive,positive,positive,positive
272634595,"You can use

````python
step_per_epoch=500
````
in your train-config.
Then the callbacks will be triggered after 500 updates. This is also related to #39 and step-callbacks.",use python triggered also related,issue,negative,neutral,neutral,neutral,neutral,neutral
272632002,"Thank you for your time... it's working right now and is pretty much faster than what [yenchenlin](https://github.com/yenchenlin/pix2pix-tensorflow) has implemented... I only need time to get around the architecture...
But somehow, this project reminds me of Keras: abstracting some layers and providing easier to work interfaces... am I wrong?",thank time working right pretty much faster need time get around architecture somehow project providing easier work wrong,issue,positive,positive,neutral,neutral,positive,positive
272631241,Yes it's alright. The first one was fixed in 6d67faf93a7 today. The second one is because tensorflow changes their API recently on master: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard5/tf.nn.sigmoid_cross_entropy_with_logits.md,yes alright first one fixed today second one recently master,issue,positive,positive,neutral,neutral,positive,positive
272630846,"Should I further make some edits? How? Amend previous commits, make new commits? I originally planned to do this tomorrow.
The correct term is probably: *saliency maps*. The best references are:

[Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901v3.pdf)
[Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps
](https://arxiv.org/pdf/1312.6034v2.pdf)

The [guided backprob](https://arxiv.org/pdf/1412.6806v3.pdf) is based on these. Just being curious: Is there a reason to move the saliency_op from the symb-functions to this example.

I did a quick search on google and found a [related Theano version](https://github.com/Lasagne/Recipes/blob/master/examples/Saliency%20Maps%20and%20Guided%20Backpropagation.ipynb).

Btw: related work is https://arxiv.org/pdf/1604.00825v1.pdf",make amend previous make new originally tomorrow correct term probably best understanding convolutional deep inside convolutional image classification based curious reason move example quick search found related version related work,issue,positive,positive,positive,positive,positive,positive
272629648,"Ok... I commented this line `ds = PrefetchDataZMQ(ds, 1)` and had to change 2 parts of the code... 

the first part was to use `config.dataflow` instead of `config.dataset` in `tensorpack-master/examples/GAN/GAN.py:18` and the second part was to change `labels` to `targets` in `sigmoid_cross_entropy_with_logits`... I'm using Tensorflow 0.12.

If what I've done about the Prefetching is OK, then I'm closing this issue...",line change code first part use instead second part change done issue,issue,negative,positive,positive,positive,positive,positive
272627206,"I fixed some of the documentation issue, simplify some code, and rearranged the location of some functions. 

Another question I'm having in mind: could it be confusing to call it saliency ? This word never appears in the paper, and the author described the method as a visualization method to find out what the layers has learned. But ""saliency"" has its own field of research in CV.",fixed documentation issue simplify code location another question mind could call word never paper author method visualization method find learned field research,issue,negative,positive,neutral,neutral,positive,positive
272619956,"I didn't know that would happen. I was ignoring this error all the time and things are fine.
Maybe there are some changes in gym that make this error matter? I'll try it when I have time.",know would happen error time fine maybe gym make error matter try time,issue,negative,positive,positive,positive,positive,positive
272619664,"Since this error make the result imcomplete, the result won't be able to upload to gym. But it doesn't matter. thank you.",since error make result result wo able gym matter thank,issue,negative,positive,positive,positive,positive,positive
272616319,"inproc wouldn't work at all. Regarding the error, are you using any of the examples or have you modified the data? It shouldn't try to pickle the augmentor. Do you have an stack trace of the first error?

Btw, if you are running Image2Image.py, maybe just use `PrefetchData(ds, 100, 1)`, or just remove prefetch completely. Dataflow is not the bottleneck here. ",would work regarding error data try pickle stack trace first error running maybe use remove completely bottleneck,issue,negative,positive,positive,positive,positive,positive
272615600,"@ppwwyyxx
Thanks for the quick response... Changing the protocol to `TCP` (or `inproc`) avoids the first error but the second error (which I explained in the post above) still remains... I do believe its main problem resides here:

```
  File ""C:\Anaconda3\lib\multiprocessing\reduction.py"", line 59, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'AugmentImageComponents.__init__.<locals>.func'
```

which eventually results in this error:

```
  File ""PATH_TO_PACKAGE\Tensorpack\tensorpack\dataflow\prefetch.py"", line 193, in __del__
    x.terminate()
  File ""C:\Anaconda3\lib\multiprocessing\process.py"", line 113, in terminate
    self._popen.terminate()
AttributeError: 'NoneType' object has no attribute 'terminate'
```",thanks quick response protocol first error second error post still remains believe main problem file line dump file protocol ca pickle local object eventually error file line file line terminate object attribute,issue,negative,positive,positive,positive,positive,positive
272612124,"Windows doesn't support ipc communication: http://stackoverflow.com/questions/15386121/does-zeromq-support-ipc-as-a-transport-channel-on-windows.

To make it work you'll have to use tcp. The invalid argument error is probably because you didn't write the address correctly. It should be something like tcp://127.0.0.1:1234.
Using tcp, it probably will run as slow as the python multiprocessing module. So maybe you can just use PrefetchData instead of PrefetchDataZMQ.",support communication make work use invalid argument error probably write address correctly something like probably run slow python module maybe use instead,issue,negative,negative,negative,negative,negative,negative
272356502,"Thanks for the rich explanation! I'm OK with merging these now, but please know that I may rearrange/modify glance.md a lot in the future, probably split some of the content into separate pages.",thanks rich explanation please know may lot future probably split content separate,issue,positive,positive,positive,positive,positive,positive
272094766,"I change those thing and added
````bash
python -c ""import cv2; print('OpenCV '+ cv2.__version__)""
python -c ""import tensorflow as tf; print('TensorFlow '+ tf.__version__)""
````",change thing added bash python import print python import print,issue,negative,neutral,neutral,neutral,neutral,neutral
272066556,"Just an update, there is a draft api doc now at http://tensorpack.readthedocs.io/. The tutorial is still missing, and I know that's most important.",update draft doc tutorial still missing know important,issue,negative,positive,neutral,neutral,positive,positive
271579581,"Alright
````python
def _setup_graph(self):
        self.pred = self.trainer.get_predict_func(['noise', 'code'], ['generator/fake_samples'])
````
is perfect!",alright python self perfect,issue,positive,positive,positive,positive,positive,positive
271517634,Thanks a lot and especially for the detailed documentation!,thanks lot especially detailed documentation,issue,negative,positive,positive,positive,positive,positive
271464379,Please leave the PR as-is or close it for now. We'll revisit it and can adopt the live data monitor when trigger_step is ready.,please leave close revisit adopt live data monitor ready,issue,positive,positive,positive,positive,positive,positive
271327257,"Thanks for pointing out. It's working in HEAD now.

I still have a bunch of old code under my private examples/, so I haven't setup a pre-commit hook for it. Will make that later.",thanks pointing working head still bunch old code private setup hook make later,issue,negative,positive,neutral,neutral,positive,positive
271318953,"\_\_init\_\_ usually should do nothing interesting because it is executed even before you had a TrainConfig.

To access the session you can use `self.trainer.sess` inside Callback. This is available after (included when) setup_graph is called.
The session is the default session in before_train, trigger_epoch, after_train, but not in setup_graph (because when graph is not finalized there is not a valid session).

-----------
About your task, if you want to do prediction, probably it's easier to use `trainer.get_predict_func`. It will return an OnlinePredictor in a separate tower with is_training=False. But if you don't want that for some reason you can certainly create the OnlinePredictor yourself, in either setup_graph or before_train.",usually nothing interesting executed even access session use inside available included session default session graph valid session task want prediction probably easier use return separate tower want reason certainly create either,issue,positive,positive,positive,positive,positive,positive
271247415,"The linting failed because the current master has one issue:

> ./ConvolutionalPoseMachines/load-cpm.py:27:1: E302 expected 2 blank lines, found 1

This was introduced in 13dc646b408e88532744c626691727a68e025bd0, see
https://github.com/ppwwyyxx/tensorpack/commits/master",current master one issue blank found see,issue,negative,neutral,neutral,neutral,neutral,neutral
271146291,"I switched to CircleCI which saved a lot of time. 

#### Outputs:
with issue #92:
https://circleci.com/gh/PatWie/tensorpack-fork/5#config/containers/0
with current HEAD+this pull-request:
https://circleci.com/gh/PatWie/tensorpack-fork/7

This nice thing is that the entire run only takes bout 2minutes.

This also reveals that there might be another problem, although it is running fine here. This might be fixed when packing tensorpack as a pip package.",switched saved lot time issue current nice thing entire run bout also might another problem although running fine might fixed pip package,issue,positive,positive,positive,positive,positive,positive
271141495,"That's the point. There are several issues with Python under Travis:
- NumPy uses ucs2 and TensorFlow ucs4
- there is no OpenCV package for Python3
- there seem to be inconsistent Python versions of 3.4 within a single Travis-CI container
- compiling OpenCV from source takes quite some time (6min)

I will further try to debug the process here:
https://github.com/PatWie/travis_python_cv2_tf

which is a cleaned version of my 75 commits from yesterday:
https://github.com/PatWie/python_cv2_test

Unfortunately, running the Travis-containers locally gives not the same environment and same paths. Even worse, the ""Trusty"" containers are in beta. So if they change their stuff as frequently as TF does, it will be tedious to adapt these changes.

Maybe some of these might be worth to look at:
- https://circleci.com/features/
- http://codeship.com/

I would be happy if even a combination of Python2.7+TensorFlow0.12+OpenCV3 would work. I did not try to compile TensorFlow from source. But I think this is not desirable.",point several python travis package python seem inconsistent python within single container source quite time min try process version yesterday unfortunately running locally environment even worse trusty beta change stuff frequently tedious adapt maybe might worth look would happy even combination would work try compile source think desirable,issue,positive,negative,neutral,neutral,negative,negative
271127846,"I see. Great. 
Thanks. 

> On Jan 7, 2017, at 6:15 PM, Yuxin Wu <notifications@github.com> wrote:
> 
> The training was done not in tensorflow. There are some differences in the definition of Ops and also in architecture, e.g. we forgot to do the average in global average pooling (divide by 7x7) when we trained the model.
> The current model file is just an equivalent in tensorflow. Ideally a clean model should just remove those hacks.
> 
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",see great thanks wrote training done definition also architecture forgot average global average divide trained model current model file equivalent ideally clean model remove state reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
271124312,"The training was done not in tensorflow. There are some differences in the definition of Ops and also in architecture, e.g. we forgot to do the average in global average pooling (divide by 7x7) when we trained the model. 
The current model file is just an equivalent in tensorflow. We expect that these small differences won't make a big difference to the model, but ideally a clean model should just remove those hacks.",training done definition also architecture forgot average global average divide trained model current model file equivalent expect small wo make big difference model ideally clean model remove,issue,positive,negative,neutral,neutral,negative,negative
271123746,"Im going through the resnet-dorefa file. The model is similar but slightly
different compared to the model in imagenet-resnet file.

You mention some bugs in the architecture and workarounds in the code for
resnet-dorefa. What are these bugs and could you explain little bit on the
workaround you have done:

1. handling pool1
2. explicit padding
3. adding a tf.mul(49)

Thanks.

On Thu, Jan 5, 2017 at 12:42 AM, Yuxin Wu <notifications@github.com> wrote:

> We've released the 1,4,32-ResNet18 model on DoReFa-Net <http://dorefa.net>
> page.
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/40#issuecomment-270591890>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AWDdv1eTpfFF1ftKxtaGlflygwenavXfks5rPK0AgaJpZM4KluRh>
> .
>



-- 
Asit
",going file model similar slightly different model file mention architecture code could explain little bit done handling pool explicit padding thanks wrote model page state reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
271105574,"It seems to be nearly impossible to run TensorFlow(CPU) and OpenCV with Travis-CI. I will reopen this pull-request, when I found a solution. ",nearly impossible run reopen found solution,issue,negative,negative,negative,negative,negative,negative
271055135,I started to use __all__ for all module to work with sphinx api doc. This was causing some problems with import.,use module work sphinx doc causing import,issue,negative,neutral,neutral,neutral,neutral,neutral
270918005,"I can consider an option to disable tqdm, so that users can have more control over what to display within steps.",consider option disable control display within,issue,negative,neutral,neutral,neutral,neutral,neutral
270906070,I understand the current design. But within epochs you need to pass the data to tqdm or modifications of it to print it to the console. Another idea would be a TrainingStepCallback which contains the code of the most-inner loop from the training-main-loop. This would probably also help to change training-logic to GAN training. What do you think?,understand current design within need pas data print console another idea would code loop would probably also help change gan training think,issue,negative,neutral,neutral,neutral,neutral,neutral
270905214,That's a nice idea. I will update the code.,nice idea update code,issue,negative,positive,positive,positive,positive,positive
270904674,"Yes it would be good, but it's not general enough to make it a default inside trainer because there are cases it doesn't make sense.
You can implement it by adding a dataflow adapter, which prints the shape of the first datapoint. This way users can choose to use it, by e.g. (ds = PrintShape(ds)). This also avoid modifying code for every InputData type.",yes would good general enough make default inside trainer make sense implement adapter shape first way choose use also avoid code every type,issue,positive,positive,positive,positive,positive,positive
270903956,In fact I'd prefer no logic been implemented inside trainer if they can be done outside. The current epoch-wise logging is also done outside.,fact prefer logic inside trainer done outside current logging also done outside,issue,negative,neutral,neutral,neutral,neutral,neutral
270899367,That's why there is a try-catch-block. For most image-processing models it is good to double-check the dataflow-output on at least one sample. ,good least one sample,issue,negative,positive,positive,positive,positive,positive
270898971,"So you mean, for some basic statistics of scalars you prefer callbacks instead of simple tf.collections? That's the purpose of those: collecting tensors.",mean basic statistic prefer instead simple purpose,issue,negative,negative,negative,negative,negative,negative
270897685,"It will not need an extra sess.run. The trainer should know what all callbacks need, and fetch those tensors together with `train_op` in one sess.run.",need extra trainer know need fetch together one,issue,negative,neutral,neutral,neutral,neutral,neutral
270897504,"But the problem with any before/after step design is that this needs explicit addition `sess.run`. Here in this implementation it is simply extracted from the graph, when it is already computed.",problem step design need explicit addition implementation simply extracted graph already,issue,negative,neutral,neutral,neutral,neutral,neutral
270897253,"There is a plan to more generally support `trigger_step()` with user-specified data. Some discussion were in #39. The idea is that a Callback should define what it needs every step and what to do with it, not only to print them. And then the trainer would fetch those tensors every step and pass them to the callback.",plan generally support data discussion idea define need every step print trainer would fetch every step pas,issue,negative,positive,neutral,neutral,positive,positive
270891280,"It is not guranteed that dataflow produces datapoints of the same shape. In tasks like image segmentation, speech recognition, NLP, data can be any shape. So printing the shape of the first data doesn't make a good sense. ",shape like image segmentation speech recognition data shape printing shape first data make good sense,issue,positive,positive,positive,positive,positive,positive
270718838,"Thanks!

On Mon, Oct 31, 2016 at 7:44 PM, Yuxin Wu <notifications@github.com> wrote:

> a ResNet-18 model with (W,A,G)=(1,4,32) should get 60% accuracy. But the
> training was done in a private framework and was not converted to
> tensorflow.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/issues/40#issuecomment-257477329>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AWDdvzQ2WSGxoDUQLAm7chujfmGvM7Iaks5q5qeHgaJpZM4KluRh>
> .
>



-- 
Asit
",thanks mon wrote model get accuracy training done private framework converted thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
270685182,"Anyway, it should work already. I guess I can merge it first and look at the issues later.",anyway work already guess merge first look later,issue,negative,positive,positive,positive,positive,positive
270683567,"I'm still not following why do you need the scope.
1. In tensorpack, `get_cost` was never called under a variable scope other than the default scope (with name == ''). In multi-tower training it is called under different name scopes, though.
2. In multi-tower training, some of my tests show that the regularization losses won't get repeatedly added to the collection, so there is not a need for de-duplication. You already uses `ctx.is_main_training_tower`, so UPDATE_OPS is well-handled in multi-tower setting.

BTW, looking at the TF code it looks like `tf.get_collection` accepts scope of string type.
",still following need scope never variable scope default scope name training different name though training show regularization wo get repeatedly added collection need already setting looking code like scope string type,issue,negative,neutral,neutral,neutral,neutral,neutral
270676244,Yeah. Fixed. But the exception still happens iff the scope has no name.,yeah fixed exception still scope name,issue,negative,positive,neutral,neutral,positive,positive
270647568,"The last commit is tested with batch-norm and regularization:
https://github.com/PatWie/tensorpack-recipes/blob/master/mnist/run.py

and works.",last commit tested regularization work,issue,negative,neutral,neutral,neutral,neutral,neutral
270645430,"Please wait before merging, there are still some issues with the scope:

````
  File ""run.py"", line 96, in main
    SimpleTrainer(config).train()
  File ""/home/patwie/git/tensorpack/tensorpack/train/base.py"", line 60, in train
    self.setup()
  File ""/home/patwie/git/tensorpack/tensorpack/train/base.py"", line 108, in setup
    self._setup()
  File ""/home/patwie/git/tensorpack/tensorpack/train/trainer.py"", line 78, in _setup
    cost_var = model.get_cost()
  File ""/home/patwie/git/tensorpack/tensorpack/models/model_desc.py"", line 129, in get_cost
    regulization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES, scope=scope)
  File ""/home/patwie/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 4163, in get_collection
    return get_default_graph().get_collection(key, scope)
  File ""/home/patwie/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2761, in get_collection
    regex = re.compile(scope)
  File ""/usr/lib/python2.7/re.py"", line 194, in compile
    return _compile(pattern, flags)
  File ""/usr/lib/python2.7/re.py"", line 247, in _compile
    raise TypeError, ""first argument must be string or compiled pattern""
TypeError: first argument must be string or compiled pattern

````",please wait still scope file line main file line train file line setup file line file line file line return key scope file line scope file line compile return pattern file line raise first argument must string pattern first argument must string pattern,issue,negative,positive,positive,positive,positive,positive
270645298,"LGTM. I'll merge it soon. Thanks again! 
My idea is that the existing framework provides Trainer and ModelDesc for the most common use case (single cost optimization). For tasks more complicated than that I'll expect users to write some of their own code -- it'll just be too hard to write anything general.
For example, for GAN if you want to help users handle the collections, there is going to be a lot of issues about which cost should the collection be applied on -- ideally those appeared in generator should be on generator cost, and those appeared in discriminator should be on discriminator cost. But for extensions like mode-regularized GAN, there is an extra ""encoder"". For variants like Image2Image, BN statistics doesn't need update at all.  -- These are situations that cannot be simply addressed by an abstraction, so I'd rather just provide utility function and let users call them.

",merge soon thanks idea framework trainer common use case single cost optimization complicated expect write code hard write anything general example gan want help handle going lot cost collection applied ideally generator generator cost discriminator discriminator cost like gan extra like statistic need update simply abstraction rather provide utility function let call,issue,positive,negative,neutral,neutral,negative,negative
270636635,"I put that new stuff into ModelDescr. However, this currently does not affect possible GAN-models. For the examples, I will probably create another repo not to mess up your code. You can then decide whether you want them or not.

I think ModelDesc should have something like GanModelDesc such that the cost-function can be overridden.",put new stuff however currently affect possible probably create another mess code decide whether want think something like,issue,negative,negative,neutral,neutral,negative,negative
270546246,Yes that would be nice! Haven't got time to look into that.,yes would nice got time look,issue,positive,positive,positive,positive,positive,positive
270545291,"1. Loading a pre-trained model is just simply `--load model` handled in most examples. `--load` will try to load all parameters with the same name and print warning about the ummatched weights in the model, so to re-train some layers you should change their names before loading. [HED](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/HED) is such an example which loads pre-trained vgg..

2.  [FAQ](http://tensorpack.readthedocs.io/en/latest/tutorial/faq.html#how-to-freeze-some-variables-in-training) about freezing variables.",loading model simply load model handled load try load name print warning model change loading example freezing,issue,negative,neutral,neutral,neutral,neutral,neutral
270320359,"You can use `tf.select(mask, cost, tf.stop_gradient(cost))` to mask out some samples.",use mask cost cost mask,issue,negative,neutral,neutral,neutral,neutral,neutral
270216403,"@ppwwyyxx 
**On the first question**, I got it to work as you suggested, with the twist that I appended **gen/conv8/output** to output_names to get a 6x1x1x512 tensor, which does appear to be what I wanted. Much appreciated for your help!

One curious thing is that when I visualize the result of >1000 z vectors using t-SNE, I thought the diagram looks different (aside from coloring) from what I got with exactly the same training/testing datasets (which are frames from a celebrity interview video) using pix2pix/Torch.  Not sure what to make of it yet. Something interesting for me to look into.

Following is the tensorpack version of the t-SNE diagram, where the test samples seem fairly uniformly distributed in the z space (projected from 512D to 3D for display):
![tpack](https://cloud.githubusercontent.com/assets/13685803/21621474/2d2e0b9a-d1c7-11e6-888e-921a20cfff39.gif)
 
Following is the pix2pix/Torch version of the t-SNE diagram, which looks comparatively more lumpy:
![ex2](https://cloud.githubusercontent.com/assets/13685803/21621711/2e713fbc-d1c8-11e6-81ac-e65bc10c5aa7.gif)

",first question got work twist get tensor appear much help one curious thing visualize result thought diagram different aside coloring got exactly celebrity interview video sure make yet something interesting look following version diagram test seem fairly uniformly distributed space display following version diagram comparatively lumpy ex,issue,positive,positive,positive,positive,positive,positive
270135938,You should also not forget to add this to the settings of this github project such that each pull-request is tested.,also forget add project tested,issue,negative,neutral,neutral,neutral,neutral,neutral
270135491,Looks good! I will try to put my code in ModelDescr instead of trainer.,good try put code instead trainer,issue,negative,positive,positive,positive,positive,positive
270126400,"Thanks! I've fixed pep8 warnings across the project in several recent commits. And also added travis to run flake8: https://travis-ci.org/ppwwyyxx/tensorpack.

Note that I had a different `tox.ini` for examples/ which ignored warnings about `import *`.",thanks fixed pep across project several recent also added travis run flake note different import,issue,negative,positive,neutral,neutral,positive,positive
270097307,"Sorry I misunderstood your question just now. I thought you want to use a random z vector.
You can inject your own e8 vector, but the generator still will need e7 ~ e1 so you didn't bypass the conv. I expect changing e8 won't make much difference. 
The simplest way to try it out is just to replace e8 in the code by something like `e8 = tf.constant(xxx)`.",sorry misunderstood question thought want use random vector inject vector generator still need bypass expect wo make much difference way try replace code something like,issue,negative,negative,negative,negative,negative,negative
270079709,"I see. Now I'm accepting the idea of applying the two collections by default, because:
1. Nothing different when the two collections are not used
2. We can assume these two collections are only used for these two purposes

But still, I hope the trainer keeps as simple as possible. An alternative way is to apply these collections in `ModelDesc.get_cost()` by default. This way it goes into models, and users will also have a way to disable it (by override the function). What do you think of this?

EDIT: this also gives a nice boundary: all forward pass defined in Model, and trainer does the backward pass and update.",see idea two default nothing different two used assume two used two still hope trainer simple possible alternative way apply default way go also way disable override function think edit also nice boundary forward pas defined model trainer backward pas update,issue,positive,positive,positive,positive,positive,positive
270072997,"I understand your point. But in tfSlim the only purpose of the `UPDATE_OPS` collection is to manage batchnorm updates. In this sense the implementation from the pull request is totally equivalent to your BatchnormV2.
As there is no way of overriding the slim layers without patching the tf-repo, I have put it into the trainer.
Currently there is no overhead since it only updates the batch statistics in the main tower if the user asked for batchnorm.

Same for regularization: As long as there is no explizit regularization within in the ModelDesc, there are no aditional operations added to the graph.

In these two points: I would argue that since it is well documented behavior of tfSlim one can add these in the backend. I would further say it is the only way to add these as the user expect these additional operations to happen when using tfSlim.

If you still want, I can rebase again on your current HEAD of the repo.",understand point purpose collection manage sense implementation pull request totally equivalent way slim without put trainer currently overhead since batch statistic main tower user regularization long regularization within added graph two would argue since well behavior one add would say way add user expect additional happen still want rebase current head,issue,negative,positive,neutral,neutral,positive,positive
270053614,"1. In `sample()`, just append `conv8/output` to `output_names`. Then `o[0]` would still contain 6 RGB images, and `o[1]` would be 6 e8 tensors (of shape 6x512x1x1).

2. You can look at `DCGAN-CelebA.py` which uses an input z vector.
",sample append would still contain would shape look input vector,issue,negative,neutral,neutral,neutral,neutral,neutral
270038215,"I think it's important to keep things controllable by the users -- at least the entire forward pass, instead of hiding it in the trainer.
For example, BN updates is not necessarily applied before the cost tensor: in my implementation it is applied inside BN layer. If UPDATE_OPS collection is used for some other purposes, differences like this may be important.
You can provide very strong utilities function such as `apply_tflim_collections`, or a subclass such as `SlimModelDesc` which automatically applies these in `get_cost`, to simplify what an user has to do. But I'd still like the users to enable them explicitly.

This may make it a bit complicated for the users. But as long as there is a flexible enough backend, an easier wrapper can always be built upon it.",think important keep controllable least entire forward pas instead trainer example necessarily applied cost tensor implementation applied inside layer collection used like may important provide strong function subclass automatically simplify user still like enable explicitly may make bit complicated long flexible enough easier wrapper always built upon,issue,positive,positive,neutral,neutral,positive,positive
269989989,"If you put these files

tox.ini

````ini
[tox]
envlist = py26,py27

[flake8]
max-line-length = 120
exclude = .git,__init__.py
````
into the root-dir, then `flake8 .` should give the same output as in `travis.yml`


````YAML
language: ""python""
python:
   - ""2.7""
sudo: false
cache: pip

before_script:
  - pip install flake8
  - flake8 --version
script:
  - flake8 .

notifications:
  email: false
````",put tox flake exclude flake give output language python python false cache pip pip install flake flake version script flake false,issue,negative,negative,negative,negative,negative,negative
269988781,"About your points:

1. I put the batch_norm collection into the trainer because it has to be connected to the computation graph. I changed the place slightly in the last version of this pull-request.

2. You are right. This belongs to the model --> fixed. For the simple MNIST example, I think it is sufficient to put it there. However, in the long run, if you want to support tfSlim it might be good, to handle the regularization in the background. I mean, if `set(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))` is empty, then there is nothing added to the graph. So this can be transparently done in the background.

3. Now, BN-updates are only considered from a single tower which is sufficient for updates. 

4. I am not sure if the updated version - a mixture of your layers and tfSlim - is easier to understand :wink:",put collection trainer connected computation graph place slightly last version right model fixed simple example think sufficient put however long run want support might good handle regularization background mean set empty nothing added graph transparently done background considered single tower sufficient sure version mixture easier understand wink,issue,positive,positive,positive,positive,positive,positive
269977492,"Thanks! I'll run autopep8 and see what happens. My understanding is that it doesn't always do the right thing (?) so it might take some time to fix the style. It's modifying code everywhere so probably it's better I work on it.

I'll leave this open and we can look at the CI tools after existing style issues have been fixed.
",thanks run see understanding always right thing might take time fix style code everywhere probably better work leave open look style fixed,issue,positive,positive,positive,positive,positive,positive
269950563,This requires #81 to be merge beforehand. I will rebase this pull-request on the master branch when #81 was merged.,merge beforehand rebase master branch,issue,negative,neutral,neutral,neutral,neutral,neutral
269933430,"@ppwwyyxx 
That was indeed the problem. Much appreciated for the help.
",indeed problem much help,issue,negative,positive,positive,positive,positive,positive
269921207,"@ppwwyyxx   
That's very helpful info which got me over the problem I had earlier. There is only one hitch remains where the images I saved look off-color, like this:

![1-out-0](https://cloud.githubusercontent.com/assets/13685803/21583150/7043d68a-d041-11e6-92db-faee3044c0d4.jpg)

The above is expected to be in full color instead. My code looks like this:

    for i,img in enumerate(o):
        img = Image.fromarray(img, ""RGB"")
        path = os.path.join(""somePathHere-""+str(i)+"".jpg"")
        img.save(path)

I compared the pixel value in the array **img** with what got rendered in the file which appear to be the same, so I believe I am rendering exactly what got passed in **o**. Is there some pre-processing that I need to do with **img** here?  Thanks!
",helpful got problem one hitch remains saved look like full color instead code like enumerate path path value array got file appear believe rendering exactly got need thanks,issue,positive,positive,positive,positive,positive,positive
269896914,The gradient is already cut by the other `stop_gradient` call. Adding `stop_gradient` to the beginning changes nothing.,gradient already cut call beginning nothing,issue,negative,neutral,neutral,neutral,neutral,neutral
269896802,"(Newbie) question for clarification: why dont you have stop_gradient in the first line when calculating delta? As in 
`delta =  tf.stop_gradient(0.7 * tf.reduce_mean(tf.abs(x)))`

Thanks to both of you for sharing your code. Its quite helpful.",question clarification dont first line calculating delta delta thanks code quite helpful,issue,positive,positive,positive,positive,positive,positive
269896464,"Thanks @Junsong-Wang! Although not the same as the paper, this looks very smart.
I implemented the quantization following TWN paper today, inspired by @thadpasce16. 

```python
delta = 0.7 * tf.reduce_mean(tf.abs(x))
mask = tf.logical_or(x < -delta, x > delta)
maskf = tf.cast(mask, tf.float32)

x_side = x * maskf
Wl = tf.reduce_sum(tf.abs(x_side)) / tf.cast(tf.count_nonzero(x_side), tf.float32)

maskf = tf.stop_gradient(Wl * maskf)
with G.gradient_override_map({""Sign"":""Identity"", ""Mul"": ""Add""}):
    output = tf.sign(x) * maskf
return output

```",thanks although paper smart quantization following paper today inspired python delta mask delta mask sign identity add output return output,issue,positive,positive,positive,positive,positive,positive
269896048,"I didn't exactly follow the paper in the scale factor.
```
E = tf.stop_gradient(tf.reduce_mean(tf.abs(x))) 
with G.gradient_override_map({""Sign"": ""Identity""}): 
    return 0.5 * E * (tf.sign(x / E - 0.7) + tf.sign(x / E + 0.7)) 
```",exactly follow paper scale factor sign identity return,issue,negative,positive,positive,positive,positive,positive
269893560,"Of course. In line 187 of Image2Image.py:
```python
        o = o[0][:,:,:,::-1]
```
`o` would be a tensor of shape `(6, h, w * 3, 3)` containing 6 BGR images. Each image is like the visualization I put in tensorboard: it's (input, ground truth, output).

You can delete the line 188 `viz = next(build_patch_list(o, nr_row=3, nr_col=2, viz=True))` which opens a window for visualization, and add extra code to save the 6 images manually to somewhere. ",course line python would tensor shape image like visualization put input ground truth output delete line next window visualization add extra code save manually somewhere,issue,positive,neutral,neutral,neutral,neutral,neutral
269892883,"@ppwwyyxx 
Thanks for the quick reply. I have been using pix2pix/Torch for while, in fact all of the 400x400 training/testing datasets I used here were borrowed directly from my pix2pix/Torch. Now that I know that tensorpack requires 256x256 images I have adjusted my datasets accordingly. After adjusting for the model path as you mentioned above, I got the following error when running the test samples:

<pre>
[0101 00:14:08 @sessinit.py:71] Restoring checkpoint from ./train_log/Image2Image0101-000755/model-18 ...
[0101 00:14:08 @sessinit.py:141] WRN Variable pos_acc/EMA/local_step:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable input_queue_size/EMA:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable global_step:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable total_g_loss/EMA/biased:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_neg/EMA:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable g_CE_loss/EMA/biased:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable g_CE_loss/EMA:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable g_CE_loss/EMA/local_step:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable total_g_loss/EMA/local_step:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_pos/EMA/biased:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable neg_acc/EMA/local_step:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss/EMA:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable pos_acc/EMA:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable beta1_power:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable input_queue_size/EMA/biased:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable L1_loss/EMA:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_neg/EMA/local_step:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable L1_loss/EMA/biased:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable learning_rate:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable total_g_loss/EMA:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss/EMA/local_step:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable neg_acc/EMA/biased:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable beta2_power:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable neg_acc/EMA:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable L1_loss/EMA/local_step:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable pos_acc/EMA/biased:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable input_queue_size/EMA/local_step:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss/EMA/biased:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_pos/EMA:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_pos/EMA/local_step:0 in checkpoint not found in the graph!
[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_neg/EMA/biased:0 in checkpoint not found in the graph!
  0%|                                                                                                                      |0/1[00:00<?,?it/s]Failed to connect to Mir: Failed to connect to server socket: No such file or directory
Unable to init server: Could not connect: Connection refused

(random_window_name:20692): Gtk-WARNING **: cannot open display:
</pre>

The error seems to be about some GTK display setup which I could do without. Is there any way to just see the output images as files, or else see them in the Tensorboard?",thanks quick reply fact used directly know accordingly model path got following error running test variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph variable found graph connect mir connect server socket file directory unable server could connect connection open display error display setup could without way see output else see,issue,negative,positive,neutral,neutral,positive,positive
269891693,Of course you can. Currently the default directory is set by `logger.auto_set_dir`. You can delete that line and use `logger.set_logger_dir('some/directory')`.,course currently default directory set delete line use,issue,negative,neutral,neutral,neutral,neutral,neutral
269891653,"1. This script assumes images are all 256x256. In training, all training images are resized to 286 and cropped to 256 (`get_data()`) following the exact setup in the paper. Therefore to use the trained model, you'll need to resize the images, or retrain with 400x400 resolution (modify the `SHAPE` variable and the image preprocessing in get_data).

2. For Tensorflow SaverV2 format, you'll need to pass `/path/to/model-last` to the saver, instead of the `.index` or `.data` file. (See #79). 
I don't like this feature of Tensorflow either, so in one recent commit (f1fdb42e2f3d8), the loader in tensorpack now automatically replace `model-last.index` with `model-last`. If you update the code it should work fine.",script training training following exact setup paper therefore use trained model need resize retrain resolution modify shape variable image format need pas saver instead file see like feature either one recent commit loader automatically replace update code work fine,issue,positive,positive,positive,positive,positive,positive
269884535,"Thanks.

Is there a way to specify the logdir to store the results/log files/snapshots etc rather than have the script pick up the log dir name itself?

If the user doesnt specify a logdir then default to the behavior currently implemented. If user specifies a logdir then use that.",thanks way specify store rather script pick log name user doesnt specify default behavior currently user use,issue,negative,positive,neutral,neutral,positive,positive
269878198,"@Junsong-Wang Were you able to get this working?
Here is how I am implementing the TWN ternarization but I am not getting results.


            shape             = x.get_shape()
            weightAbs         = tf.stop_gradient(tf.abs(x))
    
            threshold         = tf.stop_gradient(tf.reduce_mean(weightAbs) * 0.7)

            absW_gt_threshold = tf.select( weightAbs > threshold, weightAbs, tf.zeros(shape))
            nnz                           = tf.cast(tf.count_nonzero(absW_gt_threshold), dtype=tf.float32)
            weightScale             = tf.stop_gradient(tf.reduce_sum(absW_gt_threshold)/(nnz))

            weights_p         = tf.select( x >  threshold, tf.ones(shape) *  weightScale, tf.zeros(shape))
            weights_n         = tf.select( x < -threshold, tf.ones(shape) * -weightScale, weights_p)

            mask_z            = tf.select( (x < threshold) & (x > -threshold), tf.zeros(shape), tf.ones(shape))
            with G.gradient_override_map({""Sign"":""Identity"", ""Mul"": ""Add""}):
                w = tf.sign(x) * tf.stop_gradient(mask_z)
            w = w * weights_n

            return w
",able get working getting shape threshold threshold shape threshold shape shape shape threshold shape shape sign identity add return,issue,negative,positive,positive,positive,positive,positive
269844082,"1. These are tensorflow Saver V2 format. You can load it by --load path/to/model-25000. See https://github.com/tensorflow/tensorflow/issues/6142 for more information (and some caveats).

2. 5000 is just a random number I chose. Regardless of this number, all images will be trained because:
+ ImageNet data get shuffled every time it is exhausted.
+ Even if it didn't do the shuffle, the data didn't get reset after every epoch, i.e. in the next epoch, the first batch will start from the 128000th image.",saver format load load see information random number chose regardless number trained data get every time exhausted even shuffle data get reset every epoch next epoch first batch start th image,issue,negative,negative,negative,negative,negative,negative
269748271,"This was actually intended, but yes it might be a bit misleading. Fixed in eb11e29c09fdec776682da9.",actually intended yes might bit misleading fixed,issue,negative,positive,neutral,neutral,positive,positive
269721366,"Oh I thought you used some summary features which are not included in TF 0.11?
Such as [this line](https://github.com/ppwwyyxx/tensorpack/blob/99362bfdbe2c52cf26abb5edec54e570b102d72d/tensorpack/tfutils/summary.py#L43), `tf.summary.histogram` can't be found in [TF0.11's summary API document page](https://www.tensorflow.org/versions/r0.11/api_docs/python/summary/).
",oh thought used summary included line ca found summary document page,issue,negative,neutral,neutral,neutral,neutral,neutral
269721009,TF0.11 and cv2.4 works fine for me. It's probably just a problem of anaconda packages.,work fine probably problem anaconda,issue,negative,positive,positive,positive,positive,positive
269718172,"Yes, I just reinstalled it and your code works! (maybe it's problem on my side)

BTW, may I send a PR to specify that code [here](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/OpenAIGym) needs

```
tensorflow 0.12
cv2 3.1
```

It's different with the prerequisites in tensorpack's README.",yes code work maybe problem side may send specify code need different,issue,negative,neutral,neutral,neutral,neutral,neutral
269712887,"So it is cv 2.4? Is it 2.4.11 or 12?
In the first message you said that your cv is 3.1.0

Anaconda packages are built by third-party and could have problems. 
This one is similar to #68. But here I assume you didn't modify any source code, so it's not a problem of exception.",first message said anaconda built could one similar assume modify source code problem exception,issue,negative,positive,positive,positive,positive,positive
269632436,The idea is that config2 is different from config1 (it's a different graph). And config2 contains the loader of the model saved by trainer1.,idea different different graph loader model saved trainer,issue,negative,neutral,neutral,neutral,neutral,neutral
269604933,"Thanks. 

Will I have any name-scope clashing if I do and would the weights be shared between these two configs:?

config1 = get_config()
config2 = get_config() # again
Trainer(config1).train() # max_epoch=15 Trainer(config2).train() #


The goal is to train a network with full precision for (say) 15 iterations and then switch to BWN after this. During this process I don't want to save/restore the model weights after the 15th iteration. 

Thanks. 


> On Dec 29, 2016, at 1:21 AM, Yuxin Wu <notifications@github.com> wrote:
> 
> It's already quite easy to do transfer learning, compared to other frameworks. It's just a --load.
> 
> There are automatic ways within the framework, e.g. you can define both models and use an input to choose which model to use. But this is not easier.
> 
> If you just don't want to stop in the middle, you can write them together, with something like:
> 
> Trainer(config1).train()  # max_epoch=15
> Trainer(config2).train()  # sess_init=....
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",thanks would two trainer trainer goal train network full precision say switch process want model th iteration thanks wrote already quite easy transfer learning load automatic way within framework define use input choose model use easier want stop middle write together something like trainer trainer thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
269603913,"An augmentor takes one item and produces one item only.

It can be made a simple utility function, or a dataset adapter. ",one item one item made simple utility function adapter,issue,negative,neutral,neutral,neutral,neutral,neutral
269602616,"It's already quite easy to do transfer learning, compared to other frameworks. It's just a `--load`.

There are automatic ways within the framework, e.g. you can define both models and use an input to choose which model to use. But this is not easier.

If you just don't want to stop in the middle, you can write them together, with something like:
```python
Trainer(config1).train()  # max_epoch=15
Trainer(config2).train()  # sess_init=....
```
",already quite easy transfer learning load automatic way within framework define use input choose model use easier want stop middle write together something like python trainer trainer,issue,positive,positive,positive,positive,positive,positive
269563498,"Normalize by a factor doesn't change the sign at all.

This normalization is only to make auto-differentiation be consistent with the paper.",normalize factor change sign normalization make consistent paper,issue,negative,positive,positive,positive,positive,positive
269330186,"My original tf version is 0.11, and cv2 version is 2.4 (according to the dependencies on README)",original version version according,issue,negative,positive,positive,positive,positive,positive
269264884,"Oops I make it work!
I think the prerequisite should change to tf 0.12?",make work think prerequisite change,issue,negative,neutral,neutral,neutral,neutral,neutral
269133031,"Issues are for potential problems with this library. Since you're training with your code, there are a lot of things that could go wrong in your code and we don't have time to look through it.

If you have 6k classes why do you think the error rate you got is not good? If you have a better model of some other architectures, I'd suggest making changes starting from that model.

From the information you provided, there are also issues in your architecture that are not consistent with how DoReFa-Net is designed. You quantized the activations before bn0 which doesn't have much run-time benefits but hurt performance. Activations before fct is also quantized but we prefer not to do this as discussed in the paper.",potential library since training code lot could go wrong code time look class think error rate got good better model suggest making starting model information provided also architecture consistent designed much hurt performance also prefer paper,issue,negative,positive,positive,positive,positive,positive
268963682,"the first experiment, which  **(W,A,G)=(32,32,32)**, after I change the learning rate to 0.01 and restore training and I got a much better convergence at `top_1_error=0.06`. but for the  **(W,A,G)=(1,2,32)** it's still the same. ",first experiment change learning rate restore training got much better convergence still,issue,positive,positive,positive,positive,positive,positive
268936744,"I run these two experiment at the same time on different GUPs, does this effect anything ?",run two experiment time different effect anything,issue,negative,neutral,neutral,neutral,neutral,neutral
268679498,"1. `regularize_cost` is designed only for regularizing variables, so it doesn't work for activations.
**To regularize activations, you can use `l1r = 0.001 * tf.nn.l2_loss(l1)`, and then add `l1r` to cost.**

2. `get_variable` with regularizer option would add the loss to a collection. Then you would still need to get that collection and add it to the total cost. 
Allowing this option would require most layers in tensorpack to have extra arguments (W_regularize, b_regularize, etc) and this doesn't make things much easier. I think regularize_cost with regular expression is OK.

3. There are a lot of things that cannot be done with LinearWrap. It's just a syntax sugar to use when your model is very simple. And regularizing activations is not a common things people do, so there is no functionality specifically designed for that.",designed work regularize use add cost regularizer option would add loss collection would still need get collection add total cost option would require extra make much easier think regular expression lot done syntax sugar use model simple common people functionality specifically designed,issue,negative,negative,neutral,neutral,negative,negative
268573332,"As a reference, the following code doesn't work in conda opencv2.4.12:
```python
import multiprocessing as mp
import numpy as np
import cv2

if __name__ == '__main__':
    def f():
        v = np.random.rand(299, 299, 3).astype('float32')
        v = cv2.resize(v, (64,64), interpolation=cv2.INTER_CUBIC)
    v = np.random.rand(299, 299, 3).astype('float32')
    v = cv2.resize(v, (64,64), interpolation=cv2.INTER_CUBIC)

    p = mp.Process(target=f)
    p.start()
```",reference following code work python import import import,issue,negative,neutral,neutral,neutral,neutral,neutral
268573023,"You must have other kind of exceptions if speed is 0it/s.
I have tests to confirm that opencv2.4.12 in conda doesn't work in multiple processes (and therefore doesn't throw exceptions in multiple processes), and switching to a different version fixes it. I think that would be the conclusion of this issue.",must kind speed confirm work multiple therefore throw multiple switching different version think would conclusion issue,issue,positive,positive,positive,positive,positive,positive
268481244,"if there is anything, tiny tests or experiments,  that i can help you find out the reason, just let me know...",anything tiny help find reason let know,issue,negative,neutral,neutral,neutral,neutral,neutral
268480754,"but before I do not change the shape of feeding tensor(using  `opencv2.4.12`), I also got the training speed `0 it/s` in return. which mean the problemis not cause by the tensor shape changing. that's my opinion.....>..<",change shape feeding tensor also got training speed return mean cause tensor shape opinion,issue,negative,negative,negative,negative,negative,negative
268479380,"After updating `opencv` to `3.1` and uncomment the line `arr = np.transpose(arr, [1,2,0]) `in `get_per_pixel_mean.` , keep `PrefetchDataZMQ` there, the code work properly.",line keep code work properly,issue,negative,neutral,neutral,neutral,neutral,neutral
268447138,"Yes. I found that using opencv3.1 in conda can make exception visible again. It's not a problem of conda itself.
",yes found make exception visible problem,issue,negative,neutral,neutral,neutral,neutral,neutral
268441727,"OpenCV was reading the images successfully. The problem is that you change the shape of images.

Are you saying that after updating opencv to 3.1 (and keep PrefetchDataZMQ still there), it can throw error now?",reading successfully problem change shape saying keep still throw error,issue,negative,positive,positive,positive,positive,positive
268378240,"I was following this tutorial and I missed one item (https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh), thats why my val datasets were organized in a folder!

",following tutorial one item thats organized folder,issue,negative,neutral,neutral,neutral,neutral,neutral
268371697,"The val images from ImageNet website are in one big folder originally. 
Other repo also confirms this, e.g. https://github.com/soumith/imagenet-multiGPU.torch.

I didn't use the test set.",one big folder originally also use test set,issue,negative,positive,positive,positive,positive,positive
268370567,"The folders downloaded from Imagenet website look something like this:

train/
    n01440764/ 
        n01440764_10026.JPEG
        n01440764_10027.JPEG
        ...
    n01443537/
        n01443537_10007.JPEG
        n01443537_10014.JPEG
        ...

test/
   ILSVRC2012_test_00000001.JPEG
   ILSVRC2012_test_00000002.JPEG
   ...

val/ 
   n01440764/
      ILSVRC2012_val_00000293.JPEG
      ILSVRC2012_val_00002138.JPEG
      ...
   n01443537/
      ILSVRC2012_val_00000236.JPEG
      ILSVRC2012_val_00000262.JPEG
      ...


In alexnet-dorefa and elsewhere why do you club all the val images into one big folder? Just for convenience or some other reason?

Also, do you use the **test** data sets anywhere in your experiments?

Thanks.

",look something like elsewhere club one big folder convenience reason also use test data anywhere thanks,issue,positive,positive,neutral,neutral,positive,positive
268293876,"1e-4 or 1e-3 is usually good for Adam optimizer.

batchsize/optimizer/learning rate are all in the example. You can just change them.",usually good rate example change,issue,negative,positive,positive,positive,positive,positive
268134243,"hi @ppwwyyxx 

finally, I have fixed the problem of datafolw by followling your advices. and now I can train my model with my data noramlly.

the problem lies in that the **Anaconda** `opencv=2.4.12`(compiled by np111py27_1) do not raise exception when it's fail to read image which lead to the dataflow can not feed data to TF operations and keep it waiting that leads to process bar dead.

follow you hints, my solution is simply update the version of `opencv ` from `2.4.12` to `3.1.0` (compiled by np111py27_1) using `conda install`, it turns out work for me.


thanks again for you help!",hi finally fixed problem train model data problem anaconda raise exception fail read image lead feed data keep waiting process bar dead follow solution simply update version install turn work thanks help,issue,negative,negative,neutral,neutral,negative,negative
268037907,"By the way the exception is saying that you feed a tensor  of shape [128,32,32,32] (32 channels) instead of [?, 32,32].

This is because you removed my original code ` # arr = np.transpose(arr, [1,2,0])` in get_per_pixel_mean.",way exception saying feed tensor shape instead removed original code,issue,negative,positive,positive,positive,positive,positive
268003569,"Are you saying that `tf.transpose` change the value of elements? You can print them to confirm this, and reprort bugs to tensorflow, with your detailed environment info. You can also try it without opencv to see if it still happens.

I cannot reproduce any of that. And I'm still skeptical about the conda environment..",saying change value print confirm detailed environment also try without see still reproduce still skeptical environment,issue,negative,negative,neutral,neutral,negative,negative
267956147,"I tracing the bug as follow:

I found that it failed to excute `self.dataflow.get_data()`

1. `input_dada.py` line 76  ` for dp in self.dataflow.get_data():`. 
2. `multigpu.py` line 120             `self._input_method = QueueInput(config.dataset, input_queue)`
3. `idcard-dorefa.py` line 224   `  data_train = get_data('train')`
4. `idcard.py` line 157 which is the implement of function ` def get_data(self):`

As it mentioned in above discussion, the result is quite weird after I `read` and` resize` **grayscale image**(in the example is digit '1') by cv2, then only applied `transpose operation` by `tf`, nothing more. 

the result in the following image `from left to right` are:
**tf output**     --|--    **resized result**   --|--   **orginal showed by plt**    --|--     **orignal showed linux image viewer**|

![qq 20161219164819](https://cloud.githubusercontent.com/assets/8264748/21306243/f9f12892-c60a-11e6-8ca1-12fe208bc2e9.png)

",tracing bug follow found line line line line implement function self discussion result quite weird read resize image example digit applied transpose operation nothing result following image left right output result orignal image viewer,issue,negative,negative,neutral,neutral,negative,negative
267936498,"After removing the `PrefetchDataZMQ `  I got exception of `feed failure` and queue `queue OutOfRangeError: `


here is the log info.

```
.........................above is normal output of initialization....................................
 [1219 18:46:39 @idcard-dorefa.py:93] Binarizing weight conv1/W
[1219 18:46:39 @idcard-dorefa.py:93] Binarizing weight conv2/W
[1219 18:46:39 @idcard-dorefa.py:93] Binarizing weight conv3/W
[1219 18:46:39 @idcard-dorefa.py:93] Binarizing weight fc0/W
[1219 18:46:42 @base.py:120] Initializing graph variables ...
[1219 18:46:43 @concurrency.py:24] Starting EnqueueThread
[1219 18:46:43 @base.py:142] Start training with global_step=0
  0%|                                                                                                                                                                 |0/10000[00:00<?,?it/s]2
[1219 18:46:43 @input_data.py:86] ERR Exception in EnqueueThread:
Traceback (most recent call last):
  File ""/home/nrp/program/tensorpack/tensorpack/train/input_data.py"", line 82, in run
    self.op.run(feed_dict=feed)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1449, in run
    _run_using_default_session(self, feed_dict, self.graph, session)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3668, in _run_using_default_session
    session.run(operation, feed_dict)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 943, in _run
    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (128, 32, 32, 32) for Tensor u'input:0', which has shape '(?, 32, 32)'
[1219 18:46:43 @input_data.py:93] Enqueue Thread Exited.

Traceback (most recent call last):
  File ""idcard-dorefa.py"", line 317, in <module>
    SyncMultiGPUTrainer(config).train()
  File ""/home/nrp/program/tensorpack/tensorpack/train/base.py"", line 59, in train
    self.main_loop()
  File ""/home/nrp/program/tensorpack/tensorpack/train/base.py"", line 154, in main_loop
    self.run_step() # implemented by subclass
  File ""/home/nrp/program/tensorpack/tensorpack/train/multigpu.py"", line 109, in run_step
    self.sess.run(self.train_op)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_input_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: tower0/input_deque = QueueDequeue[_class=[""loc:@input_queue""], component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_queue)]]
	 [[Node: tower0/InTopK_1/_15 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_557_tower0/InTopK_1"", tensor_type=DT_BOOL, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op u'tower0/input_deque', defined at:
  File ""idcard-dorefa.py"", line 317, in <module>
    SyncMultiGPUTrainer(config).train()
  File ""/home/nrp/program/tensorpack/tensorpack/train/base.py"", line 58, in train
    self.setup()
  File ""/home/nrp/program/tensorpack/tensorpack/train/base.py"", line 106, in setup
    self._setup()
  File ""/home/nrp/program/tensorpack/tensorpack/train/multigpu.py"", line 95, in _setup
    self.config.tower, lambda: self._get_cost_and_grad()[1])
  File ""/home/nrp/program/tensorpack/tensorpack/train/multigpu.py"", line 40, in _multi_tower_grads
    grad_list.append(get_tower_grad_func())
  File ""/home/nrp/program/tensorpack/tensorpack/train/multigpu.py"", line 95, in <lambda>
    self.config.tower, lambda: self._get_cost_and_grad()[1])
  File ""/home/nrp/program/tensorpack/tensorpack/train/feedfree.py"", line 39, in _get_cost_and_grad
    actual_inputs = self._get_input_tensors()
  File ""/home/nrp/program/tensorpack/tensorpack/train/feedfree.py"", line 30, in _get_input_tensors
    return self._input_method.get_input_tensors()
  File ""/home/nrp/program/tensorpack/tensorpack/train/input_data.py"", line 44, in get_input_tensors
    return self._get_input_tensors()
  File ""/home/nrp/program/tensorpack/tensorpack/train/input_data.py"", line 122, in _get_input_tensors
    ret = self.queue.dequeue(name='input_deque')
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 419, in dequeue
    self._queue_ref, self._dtypes, name=name)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 1057, in _queue_dequeue
    timeout_ms=timeout_ms, name=name)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

OutOfRangeError (see above for traceback): FIFOQueue '_0_input_queue' is closed and has insufficient elements (requested 1, current size 0)
	 [[Node: tower0/input_deque = QueueDequeue[_class=[""loc:@input_queue""], component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](input_queue)]]
	 [[Node: tower0/InTopK_1/_15 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_557_tower0/InTopK_1"", tensor_type=DT_BOOL, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

```",removing got exception feed failure queue queue log normal output weight weight weight weight graph starting start training err exception recent call last file line run file line run self session file line operation file line run file line feed value shape tensor shape thread recent call last file line module file line train file line subclass file line file line run file line file line file line raise type message closed insufficient current size node node defined file line module file line train file line setup file line lambda file line file line lambda lambda file line file line return file line return file line ret file line file line file line file line file line see closed insufficient current size node node,issue,negative,negative,neutral,neutral,negative,negative
267933958,"This gonna be very annoying.. You don't see many 2s so the feed operation in that loop probably failed , but silently.

Maybe removing the PrefetchDataZMQ line can make the exception visible.

Or you can try-except that loop and print the exception.",gon na annoying see many feed operation loop probably silently maybe removing line make exception visible loop print exception,issue,negative,negative,negative,negative,negative,negative
267927505,"i fixed code in `_build_graph` like that, but I sitll cant get **print many 2  in treminal** that you discribed

![image](https://cloud.githubusercontent.com/assets/8264748/21308704/12c89ad4-c616-11e6-8dd4-a12bb50c7e87.png)
",fixed code like cant get print many image,issue,negative,positive,positive,positive,positive,positive
267921047,"But after that change you'll need `image = tf.expand_dims(image, 3)` in `_build_graph` to reshape it back to [None, 32, 32,1] . Otherwise the rest of the model code wouldn't work.",change need image image reshape back none otherwise rest model code would work,issue,negative,neutral,neutral,neutral,neutral,neutral
267920777,"Normally you are supposed to see exceptions of such errors, but with anaconda we have to guess..",normally supposed see anaconda guess,issue,negative,positive,positive,positive,positive,positive
267920669,"Oh you probably need [None, 32, 32] instead of [None, 32, 32, 1], because imread return an image of shape [h, w].",oh probably need none instead none return image shape,issue,negative,neutral,neutral,neutral,neutral,neutral
267919825,"the input  after i fixed just like that.

![image](https://cloud.githubusercontent.com/assets/8264748/21307568/043625a4-c611-11e6-8e1d-af64cc8b87d8.png)
",input fixed like image,issue,negative,positive,neutral,neutral,positive,positive
267919552,"yes，I have already fixed the input.

and I did as you said add `print len(dp) before op.run at input_data.py` , but I **cant not** get the result you described..   ....
",already fixed input said add print cant get result,issue,negative,positive,neutral,neutral,positive,positive
267917952,"Anyway, you could add `print len(dp)` before `op.run` at input_data.py, line 81. It you could see it print __many__ 2 then the data part should be fine. ",anyway could add print line could see print data part fine,issue,negative,positive,positive,positive,positive,positive
267917271,"Have you fixed the channel of your inputvar already?

With a wrong input shape I could also reproduce your problem, with anaconda python, because it doesn't throw the exception.",fixed channel already wrong input shape could also reproduce problem anaconda python throw exception,issue,negative,negative,negative,negative,negative,negative
267914755,"I found a weird thing that anaconda python doesn't throw exceptions when it should, it just silently failed.

I can reproduce your problem if I use an empty mean and anaconda python. It did throw exception from the data loader with a normal python environment, but silently failed with anaconda python.

It's likely a bug of anaconda python. I'll figure that out later. Meanwhile, you can try the following:
add
```python
ds.reset_state()
for k in ds.get_data():
    print len(k)
```
before `ds = PrefetchDataZMQ(xxx`

In my tests, this would make anaconda throw the exception.",found weird thing anaconda python throw silently reproduce problem use empty mean anaconda python throw exception data loader normal python environment silently anaconda python likely bug anaconda python figure later meanwhile try following add python print would make anaconda throw exception,issue,negative,negative,neutral,neutral,negative,negative
267913306,"and here is the test code and result of using `cv2 to read and resize` image, then apply `transpose ` by `tf`.

```
import cv2
import tensorflow as tf
import matplotlib.pyplot as plt

filename = ""/home/nrp/data/idcard/val/fdd103d0-b7ec-44b5-a02b-3550b9ab085d/37.jpg""
im = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
plt.figure(1)
plt.imshow(im)
im = cv2.resize(im,(10,25))
plt.figure(2)
plt.imshow(im)

x = tf.Variable(im, name='x')
model = tf.initialize_all_variables()
with tf.Session() as session:
    x = tf.transpose(x)
    session.run(model)
    result = session.run(x)
plt.figure(3)
plt.imshow(result)
plt.show()

```

it seems that `tf.transpose` have change the orignal image.......... the result **from left to right**:

tf output     |     resized result      |   orginal showed by plt       |      orignal showed linux image viewer|

![qq 20161219164819](https://cloud.githubusercontent.com/assets/8264748/21306243/f9f12892-c60a-11e6-8ca1-12fe208bc2e9.png)


",test code result read resize image apply transpose import import import model session model result result change orignal image result left right output result orignal image,issue,negative,positive,positive,positive,positive,positive
267912172,"You seem to be still keeping the line ` pp_mean_224 = pp_mean[16:-16,16:-16]` which will slice the tensor to empty in your case. This is for imagenet only.",seem still keeping line slice tensor empty case,issue,negative,negative,neutral,neutral,negative,negative
267911409,I see. Also probably you cannot easily reproduce the stuck with several lines of code -- otherwise you wouldn't be able to run svhn examples....,see also probably easily reproduce stuck several code otherwise would able run,issue,negative,positive,positive,positive,positive,positive
267910562,"actually I use the official package of pre-built opencv provided by` conda-forge`



![qq 20161219164124](https://cloud.githubusercontent.com/assets/8264748/21306088/36081fa8-c60a-11e6-85a8-86603aa0fb08.png)



",actually use official package provided,issue,negative,neutral,neutral,neutral,neutral,neutral
267908635,`anaconda search -t conda opencv ` show me a list of (non-official) opencv 2.4.12 built. Could you be specific about which one you use?,anaconda search show list built could specific one use,issue,negative,neutral,neutral,neutral,neutral,neutral
267907734,Sorry. you should use absolute path. I see you have commented out my `os.path.join`.,sorry use absolute path see,issue,negative,negative,negative,negative,negative,negative
267904646,"You can confirm that first, by first import cv2 and resize an image, and then execute a tf operation.",confirm first first import resize image execute operation,issue,negative,positive,positive,positive,positive,positive
267903385,"how did  you fix it， finally ？

should I reinstall `opencv`  that build by myself ..? ",fix finally reinstall build,issue,negative,neutral,neutral,neutral,neutral,neutral
267903084,"It should be relative path if you read your own data code. But abs path could also work.
The problem is probably with opencv. It is found before that using a prebuilt opencv would cause tensorflow to get stuck.",relative path read data code path could also work problem probably found would cause get stuck,issue,negative,neutral,neutral,neutral,neutral,neutral
267899747,"I use `anancoda`, so I just simply use conda to install like this:

`conda install opencv=2.4.12`

and I have one thing need your confirmation that while reading images in you code you use relative path  or abs. path ?  in my understading this wont effect anything, I'm not sure about it. 

because in my implementation, I use the abs. path of the image to read it. here is the example of the name list of trainnig data:

![qq 20161219154459](https://cloud.githubusercontent.com/assets/8264748/21304730/2779d092-c602-11e6-814b-3415b8be0ea8.png)

 ",use simply use install like install one thing need confirmation reading code use relative path path wont effect anything sure implementation use path image read example name list data,issue,negative,positive,positive,positive,positive,positive
267897779,"hello there ?

after debuging the code by step, I found that the the code stuck in the place calling `sess.run()` by `tf` and I dont know how to trace it... could you help me out . thanks in advance. 

![qq 20161219151456](https://cloud.githubusercontent.com/assets/8264748/21304166/41c3cc54-c5fe-11e6-9c6f-6a6988d3c177.png)
",hello code step found code stuck place calling dont know trace could help thanks advance,issue,positive,positive,positive,positive,positive,positive
267875997," I have fix the problems you mentioned above.  the  original `batchsize=128` and I have more than 10000 items in both `train` set and `val` set.

but I still stuck in the same ouput....

![qq 20161219114657](https://cloud.githubusercontent.com/assets/8264748/21300662/de33712a-c5e0-11e6-98cd-75fc40cb201e.png)
",fix original train set set still stuck,issue,negative,positive,positive,positive,positive,positive
267875727,"> Oh wait.. do you mean that your data stop printing after your screenshot?... That's something you can debug on.

NO, it's still  printing I guess cause my data list is too long .....
",oh wait mean data stop printing something still printing guess cause data list long,issue,negative,negative,negative,negative,negative,negative
267875516,Oh wait.. do you mean that your data stop printing after your screenshot?... That's something you can debug on.,oh wait mean data stop printing something,issue,negative,negative,negative,negative,negative,negative
267873805,"Thanks. The dataset itself looks good. You also need to make sure it produces enough data (>4*batch size at least).
There are other problems as well, e.g. you kept the slicing for imagenet mean, which will make your mean empty. You read greyscale images but you didn't change the channel of InputVar.",thanks good also need make sure enough data batch size least well kept slicing mean make mean empty read change channel,issue,positive,positive,neutral,neutral,positive,positive
267873124,"Modefied the code as following: 
![2](https://cloud.githubusercontent.com/assets/8264748/21299983/847ef78a-c5db-11e6-9abe-2094625901ea.png)

I got the result in the terminal like that, and it seems that code sutck in some  loop.........

![qq 20161219111511](https://cloud.githubusercontent.com/assets/8264748/21300142/6e3cf21e-c5dc-11e6-943e-9921e85bc538.png)



",code following got result terminal like code loop,issue,negative,neutral,neutral,neutral,neutral,neutral
267869500,sorry. You need to add ds.reset_state() before the loop. Could you try again?,sorry need add loop could try,issue,negative,negative,negative,negative,negative,negative
267867501,"
![1](https://cloud.githubusercontent.com/assets/8264748/21299290/07d50f2c-c5d5-11e6-9137-8855037bfa32.png)

Run:
`python idcard-dorefa.py --data ../../tensorpack/dataflow/dataset/idcard --dorefa 1,2,6 --gpu 0,1,2,3
`

it reports an **ERROR**.  and  I have  checked  the  base class `RNGDataFlow` and `DataFlow`, seems that there is something  wrong with calling of `rng` method .

> nrp@scs4450-SYS-7048GR-TR:~/program/tensorpack/examples/DoReFa-Net$ python idcard-dorefa.py --data ../../tensorpack/dataflow/dataset/idcard --dorefa 1,2,6 --gpu 0,1,2,3
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
/home/nrp/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.
  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')
[1219 10:13:56 @idcard-dorefa.py:304] Batch per tower: 32
[1219 10:13:56 @logger.py:69] WRN Directory train_log/idcard-dorefa exists! Please either backup/delete it, or use a new directory.
[1219 10:13:56 @logger.py:71] WRN If you're resuming from a previous run you can choose to keep it.
[1219 10:13:56 @logger.py:72] Select Action: k (keep) / b (backup) / d (delete) / n (new):
d
[1219 10:13:57 @logger.py:57] Argv: idcard-dorefa.py --data ../../tensorpack/dataflow/dataset/idcard --dorefa 1,2,6 --gpu 0,1,2,3
Traceback (most recent call last):
  File ""idcard-dorefa.py"", line 306, in <module>
    config = get_config()
  File ""idcard-dorefa.py"", line 218, in get_config
    data_train = get_data('train')
  File ""idcard-dorefa.py"", line 158, in get_data
    for dsp in ds.get_data():
  File ""/home/nrp/program/tensorpack/tensorpack/dataflow/dataset/idcard.py"", line 163, in get_data
    self.rng(idxs)
AttributeError: 'IDCard12' object has no attribute 'rng'

",run python data error checked base class something wrong calling method python data successfully library locally successfully library locally successfully library locally successfully library locally successfully library locally building font cache may take moment building font cache may take moment batch per tower directory please either use new directory previous run choose keep select action keep backup delete new data recent call last file line module file line file line file line object attribute,issue,positive,positive,positive,positive,positive,positive
267866649,Thanks. You can close this PR. I will update when I have meaningful results/examples. ,thanks close update meaningful,issue,positive,positive,positive,positive,positive,positive
267837056,"The only layers get quantized in DoReFa-Net is Conv and FC.
In general fg is added directly after Conv (without any non-linearity) and fa is added directly before Conv. Then both forward and backward get quantized.",get general added directly without fa added directly forward backward get,issue,negative,positive,neutral,neutral,positive,positive
267807983,"Thanks.
I will work on (1) and (3).

Question: How should I include fg(.) and fa(.) in the implementation? E.g. is the below snippet correct (based on cifar10-resnet)?

Ex: 
c1 = Conv2D('conv1', b1, out_channel, stride=stride1, nl=BNReLU)
c1 = activate(c1) #quantizing activations of conv1
c1 = fg(c1)           #quantizing gradients for conv1 layer
c2 = Conv2D('conv2', c1, out_channel)
c2 = fg(c2)          #quantizing the gradients for conv2 layer, not quantizing the activations for this layer

For alexnet-dorefa I see this for example:
 .Conv2D('conv2', 384, 3)
                .apply(fg)
                .BatchNorm('bn2')
                .MaxPooling('pool2', 3, 2, padding='SAME')
                .apply(activate)

I dont get how the activations of conv2 alone (before bn2) are quantized. 
Does the above snippet quantize the output after maxpool? I.e.  CONV-->BN-->MXPOOL->QUANTIZE ?
 
I was thinking of quantizing on a per layer basis: CONV->QUANTIZE->BN->QUANTIZE->MXPOOL->QUANTIZE. Is my understanding incorrect?

Thanks!",thanks work question include fa implementation snippet correct based ex activate layer layer layer see example activate dont get alone snippet quantize output quantize thinking per layer basis quantize understanding incorrect thanks,issue,positive,positive,positive,positive,positive,positive
267807036,"Hi, thanks for your PR!

1. Your implementation, with BITA=32 and no fg, is not DoReFa-Net. It's just BWN (Binary Weight Network). It's still good if we can have a BWN example, though.

2. We don't include examples without a meaningful performance number (except for examples which illustrate a different type of problem), otherwise this would be just a combination of code snippets from other examples, which doesn't really teach others anything new.
By ""meaningful"", it should either be a state-of-the-art result, or a reproduced result of some paper. If you could reproduce the Cifar performance in BWN (binarize weights only), or BNN (binarize both weights and activations), this would make it a good example.

3. Same for ImageNet. We currently have a ResNet-18 model on ImageNet with 1 bit weights, 4 bit activations and 60% accuracy and it might get released soon. You can contribute if you have a similar performance with ResNet-18.

4. About the architecture, we chose the other ResNet variant which uses convolution to increase channels instead of pooling, so we don't have such problems.",hi thanks implementation binary weight network still good example though include without meaningful performance number except illustrate different type problem otherwise would combination code really teach anything new meaningful either result result paper could reproduce performance would make good example currently model bit bit accuracy might get soon contribute similar performance architecture chose variant convolution increase instead,issue,positive,positive,positive,positive,positive,positive
267692108,"Have you confirmed
```python
ds = IDCard12('xxx', 'train', shuffle=True)
for dp in d.get_data():
    print len(dp)
```
worked or not?
Also since you're using mini batch, the above code should print more lines than the batch size.",confirmed python print worked also since batch code print batch size,issue,negative,positive,positive,positive,positive,positive
267575548,"here is my scripts:

1. [`idcard.py` ](https://github.com/silverlining21/tensorpack/blob/master/tensorpack/dataflow/dataset/idcard.py)modifed from `ilvsrc.py`
2. [`idcard-dorefa.py` ](https://github.com/silverlining21/tensorpack/blob/master/examples/DoReFa-Net/idcard-dorefa.py)modifed from `alexnet-dorefa.py`

that's all the changes I had made.

and  i have just confirmed that dataset **SVHN** works well on my machine

",made confirmed work well machine,issue,negative,positive,positive,positive,positive,positive
267564801,"You should just write the DataFlow d such that:
```
for dp in d.get_data():
    print len(dp)
```
works at a reasonable speed. You can implement it any way you want.
I cannot debug your code without seeing any code.

Also, you'd better also run at least one a bit complicated examples, such as `svhn-digit-dorefa.py` , to make sure that it's not a problem on the library side. mnist might be too simple.",write print work reasonable speed implement way want code without seeing code also better also run least one bit complicated make sure problem library side might simple,issue,negative,positive,neutral,neutral,positive,positive
267550507,"It looks like you're using your own modified script but not any of my examples, and it sounds like my examples run correctly.

So it's likely that you didn't implement the DataFlow correctly to produce your data, or did some other modifications which break the program.",like script like run correctly likely implement correctly produce data break program,issue,positive,neutral,neutral,neutral,neutral,neutral
267547326,"谢谢了！
 
1、GPU应该是没问题的，我测试`mnist-convnet`样例可以正常运行
2、我跑的是单机多卡，没用跑分布式
3、下面这个是我机器上GPU占用情况，长时间保持不变都是这个占用情况。
![qq 20161216170917](https://cloud.githubusercontent.com/assets/8264748/21257316/70dc6f12-c3b2-11e6-891b-233248a65fe5.png)




下面这个是程序初始化日志，除了我标粗的部分，好像没有错误提示：

> nrp@scs4450-SYS-7048GR-TR:~/program/tensorpack/examples/DoReFa-Net$ python idcard-dorefa.py --dorefa 1,2,6  --data /home/nrp/data/idcard/ --gpu 0,1,2,3
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
/home/nrp/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.
  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')
[1216 16:48:03 @idcard-dorefa.py:307] Batch per tower: 32
[1216 16:48:03 @logger.py:69] WRN Directory train_log/idcard-dorefa exists! Please either backup/delete it, or use a new directory.
[1216 16:48:03 @logger.py:71] WRN If you're resuming from a previous run you can choose to keep it.
[1216 16:48:03 @logger.py:72] Select Action: k (keep) / b (backup) / d (delete) / n (new):
d
[1216 16:48:10 @logger.py:57] Argv: idcard-dorefa.py --dorefa 1,2,6 --data /home/nrp/data/idcard/ --gpu 0,1,2,3
[1216 16:48:10 @utils.py:60] **TENSORPACK_DATASET not set, using /home/nrp/program/tensorpack/tensorpack/dataflow/dataset for dataset.**
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:02:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x80c1500
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:03:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x80c4e80
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:82:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x80c8800
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:83:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:82:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:83:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:82:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:83:00.0)
[1216 16:48:17 @multigpu.py:29] Training a model of 4 tower
[1216 16:48:17 @multigpu.py:37] Building graph for training tower 0...
[1216 16:48:17 @_common.py:72] conv0 input: [None, 32, 32, 3]
[1216 16:48:17 @_common.py:80] conv0 output: [None, 6, 6, 96]
[1216 16:48:17 @_common.py:72] conv1 input: [None, 6, 6, 96]
[1216 16:48:17 @idcard-dorefa.py:90] Binarizing weight conv1/W
[1216 16:48:17 @_common.py:80] conv1 output: [None, 6, 6, 256]
[1216 16:48:17 @_common.py:72] pool1 input: [None, 6, 6, 256]
[1216 16:48:17 @_common.py:80] pool1 output: [None, 3, 3, 256]
[1216 16:48:17 @_common.py:72] conv2 input: [None, 3, 3, 256]
[1216 16:48:17 @idcard-dorefa.py:90] Binarizing weight conv2/W
[1216 16:48:17 @_common.py:80] conv2 output: [None, 3, 3, 384]
[1216 16:48:17 @_common.py:72] pool2 input: [None, 3, 3, 384]
[1216 16:48:17 @_common.py:80] pool2 output: [None, 2, 2, 384]
[1216 16:48:17 @_common.py:72] conv3 input: [None, 2, 2, 384]
[1216 16:48:17 @idcard-dorefa.py:90] Binarizing weight conv3/W
[1216 16:48:17 @_common.py:80] conv3 output: [None, 2, 2, 384]
[1216 16:48:17 @_common.py:72] conv4 input: [None, 2, 2, 384]
[1216 16:48:17 @idcard-dorefa.py:90] Binarizing weight conv4/W
[1216 16:48:17 @_common.py:80] conv4 output: [None, 2, 2, 256]
[1216 16:48:17 @_common.py:72] pool4 input: [None, 2, 2, 256]
[1216 16:48:17 @_common.py:80] pool4 output: [None, 1, 1, 256]
[1216 16:48:17 @_common.py:72] fc0 input: [None, 1, 1, 256]
[1216 16:48:17 @idcard-dorefa.py:90] Binarizing weight fc0/W
[1216 16:48:17 @_common.py:80] fc0 output: [None, 4096]
[1216 16:48:18 @_common.py:72] fc1 input: [None, 1, 1, 4096]
[1216 16:48:18 @idcard-dorefa.py:90] Binarizing weight fc1/W
[1216 16:48:18 @_common.py:80] fc1 output: [None, 4096]
[1216 16:48:18 @_common.py:72] fct input: [None, 1, 1, 4096]
[1216 16:48:18 @_common.py:80] fct output: [None, 6915]
[1216 16:48:18 @regularize.py:17] Apply regularizer for fc0/W:0
[1216 16:48:18 @regularize.py:17] Apply regularizer for fc1/W:0
[1216 16:48:18 @regularize.py:17] Apply regularizer for fct/W:0
[1216 16:48:19 @multigpu.py:37] Building graph for training tower 1...
[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight conv1/W
[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight conv2/W
[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight conv3/W
[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight conv4/W
[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight fc0/W
[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight fc1/W
[1216 16:48:20 @multigpu.py:37] Building graph for training tower 2...
[1216 16:48:20 @idcard-dorefa.py:90] Binarizing weight conv1/W
[1216 16:48:21 @idcard-dorefa.py:90] Binarizing weight conv2/W
[1216 16:48:21 @idcard-dorefa.py:90] Binarizing weight conv3/W
[1216 16:48:21 @idcard-dorefa.py:90] Binarizing weight conv4/W
[1216 16:48:21 @idcard-dorefa.py:90] Binarizing weight fc0/W
[1216 16:48:21 @idcard-dorefa.py:90] Binarizing weight fc1/W
[1216 16:48:22 @multigpu.py:37] Building graph for training tower 3...
[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight conv1/W
[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight conv2/W
[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight conv3/W
[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight conv4/W
[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight fc0/W
[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight fc1/W
[1216 16:48:24 @modelutils.py:26] Model Parameters: 
conv0/W:0: shape=[12, 12, 3, 96], dim=41472
conv1/W:0: shape=[5, 5, 48, 256], dim=307200
bn1/beta:0: shape=[256], dim=256
bn1/gamma:0: shape=[256], dim=256
conv2/W:0: shape=[3, 3, 256, 384], dim=884736
bn2/beta:0: shape=[384], dim=384
bn2/gamma:0: shape=[384], dim=384
conv3/W:0: shape=[3, 3, 192, 384], dim=663552
bn3/beta:0: shape=[384], dim=384
bn3/gamma:0: shape=[384], dim=384
conv4/W:0: shape=[3, 3, 192, 256], dim=442368
bn4/beta:0: shape=[256], dim=256
bn4/gamma:0: shape=[256], dim=256
fc0/W:0: shape=[256, 4096], dim=1048576
bnfc0/beta:0: shape=[4096], dim=4096
bnfc0/gamma:0: shape=[4096], dim=4096
fc1/W:0: shape=[4096, 4096], dim=16777216
bnfc1/beta:0: shape=[4096], dim=4096
bnfc1/gamma:0: shape=[4096], dim=4096
fct/W:0: shape=[4096, 6915], dim=28323840
fct/b:0: shape=[6915], dim=6915
Total param=48514819 (185.069347 MB assuming all float32)
[1216 16:48:24 @base.py:110] Setup callbacks ...
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top1/EMA:0 renamed to train-error-top1/EMA:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top5/EMA:0 renamed to train-error-top5/EMA:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cross_entropy_loss/EMA:0 renamed to cross_entropy_loss/EMA:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/AddN/EMA:0 renamed to AddN/EMA:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cost/EMA:0 renamed to cost/EMA:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top1/EMA/biased:0 renamed to train-error-top1/EMA/biased:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top1/EMA/local_step:0 renamed to train-error-top1/EMA/local_step:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top5/EMA/biased:0 renamed to train-error-top5/EMA/biased:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top5/EMA/local_step:0 renamed to train-error-top5/EMA/local_step:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cross_entropy_loss/EMA/biased:0 renamed to cross_entropy_loss/EMA/biased:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cross_entropy_loss/EMA/local_step:0 renamed to cross_entropy_loss/EMA/local_step:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/AddN/EMA/biased:0 renamed to AddN/EMA/biased:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/AddN/EMA/local_step:0 renamed to AddN/EMA/local_step:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cost/EMA/biased:0 renamed to cost/EMA/biased:0 when saving model.
[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cost/EMA/local_step:0 renamed to cost/EMA/local_step:0 when saving model.
[1216 16:48:25 @base.py:111] Building graph for predictor tower 0...
[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight conv1/W
[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight conv2/W
[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight conv3/W
[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight conv4/W
[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight fc0/W
[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight fc1/W
[1216 16:48:34 @base.py:120] Initializing graph variables ...
[1216 16:48:36 @concurrency.py:24] Starting EnqueueThread
[1216 16:48:36 @base.py:142] Start training with global_step=0
  0%|                                                                                                                                                                 |0/10000[00:00<?,?it/s]

",python data successfully library locally successfully library locally successfully library locally successfully library locally successfully library locally building font cache may take moment building font cache may take moment batch per tower directory please either use new directory previous run choose keep select action keep backup delete new data set found device name major minor total memory free memory context one currently active found device name major minor total memory free memory context one currently active found device name major minor total memory free memory context one currently active found device name major minor total memory free memory peer access device peer access device peer access device peer access device peer access device peer access device peer access device peer access device device device name bus id device device name bus id device device name bus id device device name bus id device device name bus id device device name bus id device device name bus id device device name bus id training model tower building graph training tower input none output none input none weight output none pool input none pool output none input none weight output none pool input none pool output none input none weight output none input none weight output none pool input none pool output none input none weight output none input none weight output none input none output none apply regularizer apply regularizer apply regularizer building graph training tower weight weight weight weight weight weight building graph training tower weight weight weight weight weight weight building graph training tower weight weight weight weight weight weight model total assuming float setup saving model saving model saving model saving model saving model saving model saving model saving model saving model saving model saving model saving model saving model saving model saving model building graph predictor tower weight weight weight weight weight weight graph starting start training,issue,positive,positive,positive,positive,positive,positive
267531114,"It could be a GPU problem
It could be that you're using distributed file system.
Or maybe your dataset directory is incorrect, then you'll see some errors during initialization.

You can try simpler examples (e.g. mnist, cifar) and see what works and what doesn't. You can monitor your gpu/cpu/memory usage. Otherwise there is no way to tell what's the reason.",could problem could distributed file system maybe directory incorrect see try simpler see work monitor usage otherwise way tell reason,issue,negative,neutral,neutral,neutral,neutral,neutral
267265546,"I pushed a fix just now which uses an older version of BN when it couldn't find fused_batch_norm.
But still I'd suggest updating TensorFlow is possible.",fix older version could find still suggest possible,issue,negative,positive,neutral,neutral,positive,positive
266985658,"Hi @Paseam, happy to hearing that you are implementing bit convolution kernel, I am also implementing it with same problem as your----too slow for training, could you give me some hints that how you did the implementation. Thanks in advance.

 ",hi happy hearing bit convolution kernel also problem slow training could give implementation thanks advance,issue,positive,positive,positive,positive,positive,positive
266348825,"Sorry but we're not going to release the implementation in very detail. But in general you wouldn't see bad performance as long as data is loaded in a cache-friendly way and all computation is performed in SIMD instructions.
This issue is not very related to tensorpack so I'm closing it for now. If you have questions about the paper please write to the authors.",sorry going release implementation detail general would see bad performance long data loaded way computation issue related paper please write,issue,negative,negative,negative,negative,negative,negative
266320678,"Sorry.  I must have pushed something wrong.  My apologies.



Seong-Joon Park, Managing Director
Move   *sf. *

On Sun, Dec 11, 2016 at 1:44 PM, Yuxin Wu <notifications@github.com> wrote:

> There are no changes in this PR. Is that a mistake?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/ppwwyyxx/tensorpack/pull/57#issuecomment-266263135>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABAN9cUpFYlSCWNnKirbJz84sL57L1Zdks5rG3-tgaJpZM4K8Njk>
> .
>
",sorry must something wrong park director move sun wrote mistake thread reply directly view mute thread,issue,negative,negative,negative,negative,negative,negative
266245316,Ah so with comparable units then tensorpack is at ~1152 steps/sec on a GPU compared to 419 steps/sec on [traai/async-deep-rl](https://github.com/traai/async-deep-rl)  with CPU. Yeah I figured there might be some key design/implementation differences too. ,ah comparable yeah figured might key,issue,negative,neutral,neutral,neutral,neutral,neutral
266244898,"It says 80M steps, and by steps I believe it means 1 action in the game. But here 1 iteration is a batch of 128 steps.
There are also differences in the model/input state, etc.",believe action game iteration batch also state,issue,negative,negative,negative,negative,negative,negative
266244770,"Cool, I had the config wrong! Reinstall of tensorflow with CUDA version 6.1 flag set fixed it, looks like I'm getting 9-10 it/s now. I really appreciate the info about GPU performance levels, I didn't know those details.

Now that I'm able to match your results I'm curious about another difference in iter/sec compared to a similar implementation. It looks like @traai has [traai/async-deep-rl](https://github.com/traai/async-deep-rl) can achieve 80 million iterations in 53 hours of breakout which is 419 iterations per second just on 8 cpu cores and no GPU. Is this perhaps an architectural difference of some sort?
",cool wrong reinstall version flag set fixed like getting really appreciate performance know able match curious another difference similar implementation like achieve million breakout per second perhaps architectural difference sort,issue,positive,positive,neutral,neutral,positive,positive
265894250,Another thing that's probably related. nvidia-smi shows that your GPU is in P8 state. Normally (if autoboost properly configured) the GPU would automatically change to high-performance state when some heavy jobs started. P8 is the low-power-low-performance state. You should expect it in P2 at least.,another thing probably related state normally properly would automatically change state heavy state expect least,issue,negative,negative,negative,negative,negative,negative
265893136,"You should look for logs from tensorflow about which device it's actually using, such as the following:
```
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:02:00.0
Total memory: 7.92GiB
Free memory: 7.81GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y

```
Also, if your cpu is heavily occupied it will also not run as fast.",look device actually following found device name major minor total memory free memory also heavily also run fast,issue,positive,positive,neutral,neutral,positive,positive
265889232,"At startup it actually does say it is going to run on the gpu as well:
```
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.8.0 locally
[2016-12-08 15:33:51,572] Making new env: Breakout-v0
[1208 15:33:52 @train-atari.py:248] [BA3C] Train on gpu 0 and infer on gpu 0
[1208 15:33:52 @logger.py:69] WRN Directory train_log/train-atari exists! Please either backup/delete it, or use a new directory.
[1208 15:33:52 @logger.py:71] WRN If you're resuming from a previous run you can choose to keep it.
[1208 15:33:52 @logger.py:72] Select Action: k (keep) / b (backup) / d (delete) / n (new):
d
[1208 15:34:04 @logger.py:57] Argv: ./train-atari.py --env Breakout-v0 --gpu 0
```",actually say going run well successfully library locally making new bac train infer directory please either use new directory previous run choose keep select action keep backup delete new,issue,positive,positive,positive,positive,positive,positive
265887124,"looks like python is there and I don't think I'm running any other python code so I think that's the tensorpack example:
```               │
+-----------------------------------------------------------------------------+                   │
| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |                   │
|-------------------------------+----------------------+----------------------+                   │
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |                   │
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |                   │
|===============================+======================+======================|                   │
|   0  GeForce GTX 1080    Off  | 0000:02:00.0     Off |                  N/A |                   │
| 27%   31C    P8    10W / 180W |    885MiB /  8112MiB |     13%      Default |                   │
+-------------------------------+----------------------+----------------------+                   │
                                                                                                  │
+-----------------------------------------------------------------------------+                   │
| Processes:                                                       GPU Memory |                   │
|  GPU       PID  Type  Process name                               Usage      |                   │
|=============================================================================|                   │
|    0      1758    G   /usr/lib/xorg/Xorg                             452MiB |                   │
|    0      2727    G   compiz                                         132MiB |                   │
|    0      3294    G   ...CTForProblematicRoots/disabled/ExtensionD    73MiB |                   │
|    0      5873    G   unity-control-center                            34MiB |                   │
|    0     11643    C   python                                         222MiB |                   │
+-----------------------------------------------------------------------------+
```

Though on the other hand my CPU usage is very high as well, about 60% of all 44 logical cores are in use. (server xenon cpu)",like python think running python code think example driver version name volatile fan temp compute mib mib default memory type process name usage mib mib mib mib python mib though hand usage high well logical use server xenon,issue,positive,positive,positive,positive,positive,positive
265875289,"I'm using 1080 and it could get 9it/s.
Could you double-check it is actually using GPU?
You can look at the log to see if it really detects the gpu, or run nvidia-smi while running, to see if gpu is being used.",could get could actually look log see really run running see used,issue,negative,positive,neutral,neutral,positive,positive
265848290,I forgot to run the pip install step. :-),forgot run pip install step,issue,negative,neutral,neutral,neutral,neutral,neutral
265837247,"@ppwwyyxx you're right, I ran an r0.12 install script but it doesn't seem to have worked correctly. Thanks! Will reopen if the problem remains after updating",right ran install script seem worked correctly thanks reopen problem remains,issue,negative,positive,positive,positive,positive,positive
265654328,`tf.summary.histogram` is added very recently to tensorflow. I think you're actually not using r0.12rc0. Could you double check your version?,added recently think actually could double check version,issue,negative,neutral,neutral,neutral,neutral,neutral
265624571,"quantize() with 2 bit would return values in {0,1/3,2/3,1}.",quantize bit would return,issue,negative,neutral,neutral,neutral,neutral,neutral
265618692,"Yes, I looked the code and understood when bitW=1 and bitW=32, but I am still wondering how the output looks like when bitW=2, i.e. quantize(x=0.002, k=2)? Is it still float32 number? or 2-bit integer number consist of 0 and 1? Thanks for your time!",yes code understood still wondering output like quantize still float number integer number consist thanks time,issue,positive,positive,positive,positive,positive,positive
265505868,"If you look at the code of `fw`, it uses binary-weight-network which includes a scaling factor on the binary values. Similar technique is also used in XNOR-Net.",look code scaling factor binary similar technique also used,issue,negative,neutral,neutral,neutral,neutral,neutral
265341063,Yes. This code was just meant to be a proof of effectiveness of our proposed quantization method.,yes code meant proof effectiveness quantization method,issue,negative,neutral,neutral,neutral,neutral,neutral
265340735,"So, the examples you provided still use original conv-operation, not bit-op?  ",provided still use original,issue,negative,positive,positive,positive,positive,positive
265339262,"As in the readme:
```
We're not planning to release those runtime bit-op libraries for now. In this repo, bit operations are run in float32.
```",release bit run float,issue,negative,neutral,neutral,neutral,neutral,neutral
265339041,"Hi, thanks for info and quick reply, I will try it later. I have another question regarding the implementation. According to what I understood from your paper, it needs to change the original convolution operation to bitwise operation in e.q (3) in your paper. It would be very interesting to know your implementation of this in forward and backward pass. Could you let me know where can I find the pieces of code in your implementation. Thanks again!",hi thanks quick reply try later another question regarding implementation according understood paper need change original convolution operation bitwise operation paper would interesting know implementation forward backward pas could let know find code implementation thanks,issue,positive,positive,positive,positive,positive,positive
265338101,"Thanks. This is a bug of tensorflow: https://github.com/tensorflow/tensorflow/issues/5888
Before the upstream fixes it, you can use an earlier version (0.11rc2 seems to work, but I'm not sure).",thanks bug upstream use version work sure,issue,positive,positive,positive,positive,positive,positive
264775140,"We haven't used it for object detection tasks.
We have applied it on [semantic segmentation](https://arxiv.org/abs/1612.00212) and [RNN](https://arxiv.org/abs/1611.10176).",used object detection applied semantic segmentation,issue,negative,neutral,neutral,neutral,neutral,neutral
264767206,"FYI, though it can run on CPU, I found it didn't produce reasonable score.
I guess it's because of the async update issues -- with everything running at slower speed the data I feed may become more outdated and affect training. If I add some random `time.sleep` in the code, even with GPU it learns nothing.",though run found produce reasonable score guess update everything running speed data feed may become outdated affect training add random code even nothing,issue,negative,negative,negative,negative,negative,negative
264686317,"It should be something like ""model-xxx"". There won't be `.tfmodel` suffix.
The model will be saved after each epoch.",something like wo suffix model saved epoch,issue,positive,neutral,neutral,neutral,neutral,neutral
264685891,"What is the name of the file? I dont see any ""*.tfmodel"" file. The directory contains the following two files:

events.out.tfevents.1480812949.[Name of machine].local
log.log

Do we have to pass the file path somewhere in the args?",name file dont see file directory following two name machine pas file path somewhere,issue,negative,neutral,neutral,neutral,neutral,neutral
264684091,"It should be in `train_log/train-atari`. It is saved every 6000 iterations. On a decent machine it should run 6~10 iterations/s.
(I've removed your original comment under the gist).",saved every decent machine run removed original comment gist,issue,positive,positive,positive,positive,positive,positive
263999067,"Aha, I see. 

Thank you for your help!",aha see thank help,issue,positive,neutral,neutral,neutral,neutral,neutral
263997698,"The model is not for `load-resnet.py`. It is the trained model by `imagenet-resnet.py`. They are different variants of ResNet.
`load-resnet.py` is ""A script to convert and run ImageNet-ResNet{50,101,152} Caffe models released by Kaiming."" You'll need to download the caffe model and use the converter to build a npy model, as mentioned in the readme.
",model trained model different script convert run need model use converter build model,issue,negative,neutral,neutral,neutral,neutral,neutral
263147298,Now you can do alternated multi-cost optimization by implementing the trainer logic in a separate trainer class. See examples of [Generative Adversarial Networks](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/GAN).,optimization trainer logic separate trainer class see generative,issue,negative,neutral,neutral,neutral,neutral,neutral
263052808,It shouldn't complete training without running for days.,complete training without running day,issue,negative,positive,neutral,neutral,positive,positive
263052767,Thank you so much. It completed training with some warnings. Started evaluation also with some warnings. ,thank much training evaluation also,issue,negative,positive,positive,positive,positive,positive
263051471,"You don't have permission to write in the `examples` directory.
That's likely because you're using virtualbox and put things under `/opt`.
You can use a different directory such as the user's home directory.",permission write directory likely put use different directory user home directory,issue,negative,neutral,neutral,neutral,neutral,neutral
263051237,"It moved further but now it is OSError: [Errno 13] Permission denied: 'train_log'

osboxes@osboxes:/opt/tensorpack/examples/Atari2600$ ls -l
total 148
-rw-r--r-- 1 root root   7255 Nov 26 07:41 atari.py
-rw-r--r-- 1 root root   5972 Nov 26 07:41 breakout.jpg
-rw-r--r-- 1 root root   3456 Nov 26 07:41 common.py
-rw-r--r-- 1 root root 111444 Nov 26 07:41 curve-breakout.png
-rwxr-xr-x 1 root root   8202 Nov 26 07:41 DQN.py
-rw-r--r-- 1 root root   1714 Nov 26 07:41 README.md

osboxes@osboxes:/opt/tensorpack/examples/OpenAIGym$ ls -l
total 26500
-rw-rw-r-- 1 osboxes osboxes 13561375 Nov 26 07:56 Asteroids-v0.tfmodel
-rw-rw-r-- 1 osboxes osboxes 13544955 Nov 26 07:56 Breakout-v0.tfmodel
-rw-r--r-- 1 root    root        3457 Nov 26 08:14 common.py
-rw-r--r-- 1 root    root        4659 Nov 26 07:41 README.md
-rwxr-xr-x 1 root    root        3278 Nov 26 07:41 run-atari.py
-rwxr-xr-x 1 root    root        9580 Nov 26 07:41 train-atari.py
osboxes@osboxes:/opt/tensorpack/examples/OpenAIGym$ ./train-atari.py --env Breakout-v0 --gpu 0
[1126 08:20:41 @bsds500.py:18] WRN Cannot import scipy. BSDS500 dataset won't be available!
[1126 08:20:41 @svhn.py:18] WRN Cannot import scipy. SVHNDigit dataset won't be available!
[1126 08:20:41 @format.py:16] WRN Error in 'import h5py'. HDF5Data won't be available.
[1126 08:20:41 @format.py:24] WRN Error in 'import lmdb'. LMDBData won't be available.
[1126 08:20:41 @format.py:31] WRN Error in 'import sklearn'. SVMLightData won't be available.
[1126 08:20:44 @concurrency.py:24] WRN Cannot import Future in tornado.concurrent. MultiThreadAsyncPredictor won't be available.
[2016-11-26 08:20:44,722] Making new env: Breakout-v0
[1126 08:20:45 @train-atari.py:249] [BA3C] Train on gpu 0 and infer on gpu 0
Traceback (most recent call last):
  File ""./train-atari.py"", line 255, in <module>
    config = get_config()
  File ""./train-atari.py"", line 177, in get_config
    logger.auto_set_dir()
  File ""/opt/tensorpack/tensorpack/utils/logger.py"", line 117, in auto_set_dir
    action=action)
  File ""/opt/tensorpack/tensorpack/utils/logger.py"", line 91, in set_logger_dir
    mkdir_p(dirname)
  File ""/opt/tensorpack/tensorpack/utils/fs.py"", line 22, in mkdir_p
    raise e
OSError: [Errno 13] Permission denied: 'train_log'",permission total root root root root root root root root root root root root total root root root root root root root root import wo available import wo available error wo available error wo available error wo available import future wo available making new bac train infer recent call last file line module file line file line file line file line raise permission,issue,negative,positive,positive,positive,positive,positive
263048863,It will affect. You need to copy the file like I said.,affect need copy file like said,issue,negative,neutral,neutral,neutral,neutral,neutral
263048804,I am using Ubuntu14.04 on VB which is installed on Windows10. But I use usb to transfer files from windows to ubuntu. May it affect it? Could you recommend something to resolve it?,use transfer may affect could recommend something resolve,issue,positive,neutral,neutral,neutral,neutral,neutral
263048176,"The project contains symbolic link. It looks like you're using Windows, or using a filesystem which doesn't support symbolic link.
In that case you need to copy the file `examples/Atari2600/common.py` to `examples/OpenAIGym`.",project symbolic link like support symbolic link case need copy file,issue,positive,neutral,neutral,neutral,neutral,neutral
263046923,"Thank you so much it helped. Then, I tried to reproduce the atari game on openAIGym it complained that gym is not installed. I installed but know I can not activate tensorflow environment wiht source ~/tensorflow/bin/activate . I tried to run without it and got the following : 

osboxes@osboxes:/opt/tensorpack/openAIgames-fedorov$ ./train-atari.py --env Breakout-v0 --gpu 0
[1126 06:06:17 @bsds500.py:18] WRN Cannot import scipy. BSDS500 dataset won't be available!
[1126 06:06:17 @svhn.py:18] WRN Cannot import scipy. SVHNDigit dataset won't be available!
[1126 06:06:17 @format.py:16] WRN Error in 'import h5py'. HDF5Data won't be available.
[1126 06:06:17 @format.py:24] WRN Error in 'import lmdb'. LMDBData won't be available.
[1126 06:06:17 @format.py:31] WRN Error in 'import sklearn'. SVMLightData won't be available.
[1126 06:06:18 @concurrency.py:24] WRN Cannot import Future in tornado.concurrent. MultiThreadAsyncPredictor won't be available.
Traceback (most recent call last):
  File ""./train-atari.py"", line 25, in <module>
    import common
  File ""/opt/tensorpack/openAIgames-fedorov/common.py"", line 1
    ../Atari2600/common.py
    ^
SyntaxError: invalid syntax
osboxes@osboxes:/opt/tensor",thank much tried reproduce game gym know activate environment source tried run without got following import wo available import wo available error wo available error wo available error wo available import future wo available recent call last file line module import common file line invalid syntax,issue,negative,positive,positive,positive,positive,positive
263042596,"Hi again,

I made new version of Ubuntu14.04 on VB and I still not able to run the code. Here, i would like to show my path to tensorpack and PYTHONPATH :

osboxes@osboxes:/opt/tensorpack/tensorpack$ ls -l
total 40
drwxr-xr-x 2 root root 4096 Nov 26 03:14 callbacks
drwxr-xr-x 4 root root 4096 Nov 26 03:14 dataflow
-rw-r--r-- 1 root root  578 Nov 26 03:14 __init__.py
drwxr-xr-x 2 root root 4096 Nov 26 03:14 models
drwxr-xr-x 2 root root 4096 Nov 26 03:14 predict
-rw-r--r-- 1 root root  453 Nov 26 03:14 README.md
drwxr-xr-x 2 root root 4096 Nov 26 03:14 RL
drwxr-xr-x 2 root root 4096 Nov 26 03:14 tfutils
drwxr-xr-x 2 root root 4096 Nov 26 03:14 train
drwxr-xr-x 2 root root 4096 Nov 26 03:14 utils
osboxes@osboxes:/opt/tensorpack/tensorpack$ echo $PYTHONPATH
:/opt/tensorpack/tensorpack
osboxes@osboxes:/opt/tensorpack/tensorpack$ ",hi made new version still able run code would like show path total root root root root root root root root root root predict root root root root root root root root train root root echo,issue,negative,positive,positive,positive,positive,positive
262891899,"it looks like i am on the right way, if I do echo $PYTHONPATH it gives me :/ABC/DEF/tensorpack . But something is still wrong.",like right way echo something still wrong,issue,negative,negative,negative,negative,negative,negative
262891008,"`export PYTHONPATH=$PYTHONPATH:/ABC/DEF/G` where ""/ABC/DEF/G"" is the absolute path to the whole directory you cloned, before running the program.",export absolute path whole directory running program,issue,negative,positive,positive,positive,positive,positive
262890108,"Yes, you are absolutely right. I am learning how to do it right. I see what are you saying. Could you show me how to do it, please?",yes absolutely right learning right see saying could show please,issue,positive,positive,positive,positive,positive,positive
262889262,"Roughly yes.... it sounds like you're not very familiar with command line and variables.
Basically you did `export PYTHONPATH=$PYTHONPATH:/ABC/DEF/G` where ""/ABC/DEF/G"" is the absolute path to the whole directory you cloned. `readlink` just helps you find it and you can ignore that part.",roughly yes like familiar command line basically export absolute path whole directory find ignore part,issue,negative,positive,positive,positive,positive,positive
262888261,"Thank you for helping!
I first did :  # git clone https://github.com/ppwwyyxx/tensorpack.git
then, export PYTHONPATH=$PYTHONPATH:`readlink -f path/to/tensorpack` , where I substituted path/to/tensorpack with/my/path/tensorpack

Is it right?",thank helping first git clone export substituted right,issue,positive,positive,positive,positive,positive,positive
262885428,"You should add the path to this repo to PYTHONPATH, as said at the end of [README.md](https://github.com/ppwwyyxx/tensorpack/blob/master/README.md).",add path said end,issue,negative,neutral,neutral,neutral,neutral,neutral
262347651,"If your image has 3 channels then the input `state` should have 3x4 channels (4 history).
But somehow you're feeding a `state` of 16 channels.",image input state history somehow feeding state,issue,negative,neutral,neutral,neutral,neutral,neutral
261038156,"Identity takes one input and min/max takes two, so they are not consistent and you cannot use the gradient of Identity directly. You'll need to write another function which implements the gradient of identity and register it.
",identity one input two consistent use gradient identity directly need write another function gradient identity register,issue,negative,positive,positive,positive,positive,positive
260896603,"I found if I didn't override the clip_by_value, the covergence speed is very slow, I think the cause is clip_by_value make most place zero gradient, only has gradient 1 in [-E, E]. So I think override the clip_by_value with identity could make the covergence speed much faster... so could you help to see the above problem.
",found override speed slow think cause make place zero gradient gradient think override identity could make speed much faster could help see problem,issue,negative,negative,neutral,neutral,negative,negative
260875363,"If I try to override the clip_by _value to identity, 

```
with G.gradient_override_map({""Maximum"":""Identity"", ""Minimum"":""Identity""}):
            clip_x = tf.clip_by_value(x/E, -2.0, 2.0) / 2.0
```

it occurs...

```
ValueError: Num gradients 1 generated for op name: ""tower0/conv4/clip_by_value""
op: ""Maximum""
input: ""tower0/conv4/clip_by_value/Minimum""
input: ""tower0/conv4/clip_by_value/y""
device: ""/device:GPU:0""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""_gradient_op_type""
  value {
    s: ""Identity""
  }
}
```

what is the problem...
",try override identity maximum identity minimum identity name maximum input input device key value type key value identity problem,issue,negative,neutral,neutral,neutral,neutral,neutral
260862826,"The paper said that the 0.7E is the approximate optimal threshold. yes, I also think the clip_by_value has no correct gradient. By the way, how I can get the gradient name of the tf function, so I can overwrite, such as tf.round is gradient name is Floor.
",paper said approximate optimal threshold yes also think correct gradient way get gradient name function overwrite gradient name floor,issue,positive,negative,negative,negative,negative,negative
260861505,"The E used here is derived for binary quantization and it seems like there are different equations for ternary cases in the paper.

You can write a small snippet to compute the output/gradient and see if it is expected. The gradient looks wrong to me. You probably need to override the gradient of clip_by_value and the division to identity.
",used derived binary quantization like different ternary paper write small snippet compute see gradient wrong probably need override gradient division identity,issue,negative,negative,negative,negative,negative,negative
260860369,"It seems not converged...I tested this equation dependently, the output is what I expected...But I don't know if the gradient is also correct for training. I can't find a good tensorflow fucntion to implement this. The only thing I want to do is if x > E, return 1, elif x < -E return -1, else, return 0, do you have any ideal, thanks.
",tested equation dependently output know gradient also correct training ca find good implement thing want return return else return ideal thanks,issue,positive,positive,positive,positive,positive,positive
260857003,"What do you mean by not working fine, and why do you choose such quantization equation? The equation looks rather arbitrary and seems like a lot of the entries would end up being zero.
",mean working fine choose quantization equation equation rather arbitrary like lot would end zero,issue,positive,positive,neutral,neutral,positive,positive
260243604,"Yes. a3c didn't support running on CPU. 

The latest commit 6087698d4c8d2e859cbe now added the support . 
i7 skylake is about 50x slower than TitanX.
",yes support running latest commit added support,issue,positive,positive,positive,positive,positive,positive
260242886,"Right, I had been using the A3C example in OpenAIGym and hadn't used the `--gpu` argument, but got the following:
`Traceback (most recent call last):
  File ""train-atari.py"", line 245, in <module>
    nr_gpu = get_nr_gpu()
  File ""/home/jalfred/Documents/tensorpack/tensorpack/utils/gpu.py"", line 19, in get_nr_gpu
    assert env is not None  # TODO
AssertionError
`
",right example used argument got following recent call last file line module file line assert none,issue,negative,positive,neutral,neutral,positive,positive
260242530,"This library doesn't require GPUs. 
If you're talking about any specific examples in the project, usually the `--gpu` argument is optional.
",library require talking specific project usually argument optional,issue,negative,negative,negative,negative,negative,negative
260216266,"btw, the problem could happen if your PWD is on a non-local filesystem (NFS, glusterFS, etc). That's the one thing I can think of now.
",problem could happen one thing think,issue,negative,neutral,neutral,neutral,neutral,neutral
260207524,"If I restore from pretrained model, by this 
`ENV=Breakout-v0; ./run-atari.py --load ""$ENV"".tfmodel --env ""$ENV"" --episode 100 --output output_dir1`
 it works well. The output is as follows. I didn't figure out why training from scratch doesn't work yet, it may be my problem...

```
[1113 14:31:57 @sessinit.py:137] WRN Variable fc-v/W in checkpoint not found in the graph!
[1113 14:31:57 @sessinit.py:137] WRN Variable fc-v/b in checkpoint not found in the graph!
[1113 14:31:57 @run-atari.py:67] Start evaluation: 
('Total:', 20.0)
[2016-11-13 14:32:14,458] Starting new video recorder writing to /output_dir1/openaigym.video.0.22993.video000001.mp4
('Total:', 20.0)
('Total:', 21.0)
('Total:', 21.0)
('Total:', 20.0)
('Total:', 20.0)
('Total:', 20.0)
('Total:', 21.0)
[2016-11-13 14:33:46,057] Starting new video recorder writing to /output_dir1/openaigym.video.0.22993.video000008.mp4
```
",restore model load episode output work well output figure training scratch work yet may problem variable found graph variable found graph start evaluation starting new video recorder writing starting new video recorder writing,issue,negative,positive,neutral,neutral,positive,positive
260167392,"You should see the progress bar moving immediately after seeing this. It's working for me with the current head.

It's hard to tell what's the reason. You could look at your output to see if there is any error message.
",see progress bar moving immediately seeing working current head hard tell reason could look output see error message,issue,negative,negative,negative,negative,negative,negative
260086554,"With floating point a+b+c != a+c+b, especially for large numbers. When there are millions of numbers it might be the case. But I also think the difference here is quite large. 
I'm closing this and you can track the relevant issue in tensorflow: https://github.com/tensorflow/tensorflow/issues/5527
",floating point especially large million might case also think difference quite large track relevant issue,issue,negative,positive,positive,positive,positive,positive
260086105,"tensorpack assumes you're only using flloat32, because higher precision doesn't improve models.

If you really want to do so you can monkey-patch `tf.get_variable` and add `dtype=tf.float64` whenever dtype is None.
",higher precision improve really want add whenever none,issue,positive,positive,positive,positive,positive,positive
259624126,"It seems the difference has relation to the size of the matrix, the difference increases with the matrix size becomes large. Since the fc0 has tens millions of  weights, the difference becomes a little large... It may relate to the detail of the implementation. Do you know some detail between the tf.reduce_mean and np.mean, thanks.
",difference relation size matrix difference matrix size becomes large since million difference becomes little large may relate detail implementation know detail thanks,issue,negative,positive,positive,positive,positive,positive
259622897,"I believe np.float is np.float64. And when you use different floating point precision this behavior is not a big surprise.
",believe use different floating point precision behavior big surprise,issue,negative,neutral,neutral,neutral,neutral,neutral
259619683,"I changed the tf.float32 to tf.float64, the output seems as close as I expected....However, if I change the numpy from np.float to np.float64, it seems no change... So it seems tf.float32 has much lower precision
",output close however change change much lower precision,issue,negative,positive,positive,positive,positive,positive
259615234,"Thanks for your reply, I have confirmed that this tiny different was induced by the tf.reduce_mean, I write a simple to verify this...

```
import numpy as np
import tensorflow as tf

weight_file = 'alexnet-126.npy'
param_dict = np.load(weight_file).item()
data = param_dict['fc0/W']

print 'Numpy results:{}'.format(np.mean(np.abs(data)))

vec = tf.placeholder(tf.float32, data.shape)
avg = tf.reduce_mean(tf.abs(vec))

avgs = []
with tf.Session() as sess:
    tf_mean = sess.run(avg, feed_dict={vec: data})
    print 'Tensorflow results:{}'.format(tf_mean)
```

If run this scripts. the output is
Numpy results:0.0377448014915
Tensorflow results:0.0374231785536
It seems that the difference is much larger than I expected...such as 1e-7, what is your option?
",thanks reply confirmed tiny different induced write simple verify import import data print data sess data print run output difference much option,issue,negative,positive,positive,positive,positive,positive
259610218,"If you are using `output_var_names` to specify which output you want to use, you can simply `print E` to find out the name of the tensor `E` so you can run inference on this tensor.
",specify output want use simply print find name tensor run inference tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
259605566,"Thanks for your comments....current I only check the FC layer, so there is no issue of padding.... I only dump the last pooling layer in the tensorflow and feed it to first FC layer in my numpy implementation. The most interesting thing is all the output keep a constant ratio.... So I doubt if it is induced by the factor E (tf.reduce_mean(tf.abs(x))), do you know how can I get this E value, so that I can double check, thanks. 
",thanks current check layer issue padding dump last layer feed first layer implementation interesting thing output keep constant ratio doubt induced factor know get value double check thanks,issue,positive,positive,positive,positive,positive,positive
259600367,"Possible reasons:
- Models are not mathematically the same. For example, with certain shape tensorflow may have a different padding mechanism from other frameworks such as caffe.
- floating point arithmetic. e.g.
  - (a+b)/c != a/c + b/c
  - a \* b + c with FMA instruction can have different outputs compared to simply doing multiplication and add.

If there is no problem with (1), then I'm not surprised to see a absolute difference of  ~1e-7 at first several layers and ~1e-5 at some last layers. If the error is very large you can check it layer by layer to see if some layer is mathematically different.
",possible mathematically example certain shape may different padding mechanism floating point arithmetic instruction different simply multiplication add problem see absolute difference first several last error large check layer layer see layer mathematically different,issue,negative,positive,neutral,neutral,positive,positive
259524516,"Yes, it now runs thanks. By the way efbf256 didn't work either but efc74f2 did.

Thanks again.
",yes thanks way work either thanks,issue,positive,positive,positive,positive,positive,positive
259521794,"It looks like the latest 775f5c9ad96846e20 is working. Please let me know if you encounter further error.
",like latest working please let know encounter error,issue,negative,positive,positive,positive,positive,positive
259520415,"Sorry I was doing some significant changes in the design and it looks like some part was broken.
I'm preparing a fix and meanwhile you can switch to some earlier commit e.g. efbf256e85eb9 to play with it.
",sorry significant design like part broken fix meanwhile switch commit play,issue,positive,negative,negative,negative,negative,negative
259434920,"I can train resnet101 with total batch size of 256 on 4 TitanX.
I haven't trained 152 myself yet.
",train total batch size trained yet,issue,negative,neutral,neutral,neutral,neutral,neutral
259213454,"The code is here: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/OpenAIGym/train-atari.py

And someone just wrote a paper to explain this design: http://openreview.net/pdf?id=r1VGvBcxl

Also see #20.

Final performance is roughly the same.
",code someone wrote paper explain design also see final performance roughly,issue,negative,negative,neutral,neutral,negative,negative
258998648,"Awesome, thanks for the quick reply and the good news!
",awesome thanks quick reply good news,issue,positive,positive,positive,positive,positive,positive
258744656,"the statistics has a different name for this config.
you might need to use something like `jq '.[] | .train_error_top1,.""val-top1-error""'` instead
",statistic different name might need use something like instead,issue,negative,neutral,neutral,neutral,neutral,neutral
258744509,"Cool, thanks.

One other item:
When I want to print the training error curves I get val error.

cat ../examples/DoReFa-Net/train_log/alexnet-dorefa/stat.json | jq '.[] | .train_error, .validation_error' | past
e - - | python plot-point.py --legend 'train,val' --xlabel 'epoch' --ylabel 'error'
Traceback (most recent call last):
  File ""plot-point.py"", line 313, in <module>
    main()
  File ""plot-point.py"", line 284, in main
    val = float(val)
ValueError: could not convert string to float: null
",cool thanks one item want print training error get error cat past python legend recent call last file line module main file line main float could convert string float null,issue,negative,positive,neutral,neutral,positive,positive
258743528,"Probably that's top-5 instead of top-1.  I just found a typo in the code which only prints the top-5 error rather than the top-1 error. Will fix soon.
Top-5 error of 30% at epoch 48 is normal. 

Fixed in 95b6437a.
",probably instead found typo code error rather error fix soon error epoch normal fixed ba,issue,negative,positive,positive,positive,positive,positive
258585758,"It should support python 3 already, except for some of the examples maybe.
 Please report if you find it incompatible with python 3.
",support python already except maybe please report find incompatible python,issue,positive,neutral,neutral,neutral,neutral,neutral
258512690,"Broken proto file. Closed.
",broken proto file closed,issue,negative,negative,negative,negative,negative,negative
258505811,"It works for others because caffe.proto doesn't have BOM.
Will you check your caffe.proto is downloaded correctly? Maybe it's not BOM but other unexpected characters.
I also tried 16.04.
",work bom check correctly maybe bom unexpected also tried,issue,negative,positive,neutral,neutral,positive,positive
258505064,"No, the link ends with ""Will update our parser to ignore BOM"". I dont know if protoc 2.6.1 incorporates this. I downloaded protoc 3.1.0 and I still get the above error. I wonder how it works for other people using protoc 2.6.1.
",link update parser ignore bom dont know still get error wonder work people,issue,negative,neutral,neutral,neutral,neutral,neutral
258362329,"protoc is in my path. The error is in compiling the caffe.proto file which I hadnt noticed before.

I get this error message upon compiling:
protoc caffe.proto --python_out=.
**caffe.proto:1:1: Expected top-level statement (e.g. ""message"").**

Any chance they updated the caffe.proto file (the file was last updated 4 months ago). Any chance you could upload your caffe.proto file in this repo?

Need help.
",path error file get error message upon statement message chance file file last ago chance could file need help,issue,negative,neutral,neutral,neutral,neutral,neutral
258361203,"I. Great.
II. I get what you mean. These statistics (relative time of occupancy or something like that) would be great. Although, the time of backpropagation alone (not matter if it's waiting for data or not) may still give you an Idea of how long other Callbacks take (sometimes it's as long as the backpropagation step alone).

Regarding the design I'm not saying it's particularly bad but now to use my own timer (it sends statistics somewhere) I have to override the context manager - not sure if that's what's desirable.
",great get mean statistic relative time occupancy something like would great although time alone matter waiting data may still give idea long take sometimes long step alone regarding design saying particularly bad use timer statistic somewhere override context manager sure desirable,issue,positive,positive,positive,positive,positive,positive
258352451,"2.6.1 is OK.
The function is straightforward:

``` python
def get_caffe_pb():
    dir = get_dataset_path('caffe')
    caffe_pb_file = os.path.join(dir, 'caffe_pb2.py')
    if not os.path.isfile(caffe_pb_file):
        proto_path = download(CAFFE_PROTO_URL, dir)
        ret = os.system('cd {} && protoc caffe.proto --python_out .'.format(dir))
        assert ret == 0, \
                ""caffe proto compilation failed! Did you install protoc?""
```

Maybe it failed to download the file successfully. Or maybe protoc isn't in your path. You can also run `protoc caffe.proto --python_out .` yourself to fix this.
",function straightforward python ret assert ret proto compilation install maybe file successfully maybe path also run fix,issue,negative,positive,positive,positive,positive,positive
258280046,"I. It is definitely already computed in that way. But currently the only way to access this information is through the graph (or summary). I was planning to allow train_op to support some output tensors (and accessible through `trigger_step()`).

II. For queue-based trainer, the timing **is** done of the body loop. It's just in a multi-queue pipeline, there is no way to accurately benchmark each component while the whole pipeline is running, because every component waits for others. I can add some statistics of, for example, the occupancy of the queues which reveals which component is the bottleneck. This is also used in some of the tensorflow official examples. 
",definitely already way currently way access information graph summary allow support output accessible trainer timing done body loop pipeline way accurately component whole pipeline running every component add statistic example occupancy component bottleneck also used official,issue,positive,positive,positive,positive,positive,positive
257993621,"If you want different tasks for different samples, then your `cost` should certainly be a vector of shape `(?,)` as well. `tf.select` only works for tensors of the same shape.
",want different different cost certainly vector shape well work shape,issue,positive,positive,neutral,neutral,positive,positive
257954369,"Yes I see what you are saying, `tf.select` is more appropriate for an application using different samples in a batch (though one could restructure the problem using both methods).  What I was wondering was more about the correct syntax involved in using `indicator` of shape `(?,)` with an unknown dimension in `tf.select` or `tf.cond`, when the cost vectors are of shape `()`.  For example, the following would get around this error, but I believe is only using the indicator associated with the 0th data for each batch:

```
self.cost = tf.select(indicator[0], cost_1, cost_2)
```
",yes see saying appropriate application different batch though one could problem wondering correct syntax involved indicator shape unknown dimension cost shape example following would get around error believe indicator associated th data batch indicator,issue,negative,positive,neutral,neutral,positive,positive
257792999,"I. Given a datapoint I want to compute the loss of my model on this datapoint and make sure it is computed before the model was trained on this datapoint (in this epoch at least). Then, since the datapoint is already in RAM, I want to use it for training.

II. I like this way of benchmarking dataloading and graph. I find one major problem tho:
It is not real time while training (might be important for many reasons for a cluster with multi-GPU setup where resources are shared - it's a real case scenario and I need to know if my 7 days working task is lagging for some reason). What is more when you experiment and change something in the graph, or add some augmentation should I run the tests again? Therefore at least some timing of body loop is a must. Ideally you should have also timer only for sess.run([train_op]) and timers should be normalized: time/batch_size (no matter what batch_size you get the same number).
Again - I may define shorter epoch (full epoch may last up to 12 hours in some experiments), but it becomes quite painful and not a very convenient workaround.
",given want compute loss model make sure model trained epoch least since already ram want use training like way graph find one major problem tho real time training might important many cluster setup real case scenario need know day working task lagging reason experiment change something graph add augmentation run therefore least timing body loop must ideally also timer matter get number may define shorter epoch full epoch may last becomes quite painful convenient,issue,negative,positive,positive,positive,positive,positive
257762088,"I. I'm still not sure.. do you mean ""if the loss is small enough, then don't train on this data point""?
If that is the case, you can probably just implement this in the graph, with tf.cond and tf.stop_gradient. But  maybe doing something on dataflow to not feed such datapoint would be nicer.

II. So you are using SimpleTrainer. And then you can get a reliable benchmark of train_op and data from the loop body. But this kind of benchmark won't work for any other queue-based trainers where data and train_op happen in separate threads.
Also, dataflow or tensorflow might need some time to warm up, so I'd just use `TestDataSpeed` to test the data and `FakeData` to test the graph, for a couple of hundred iterations.
",still sure mean loss small enough train data point case probably implement graph maybe something feed would get reliable data loop body kind wo work data happen separate also might need time warm use test data test graph couple hundred,issue,positive,positive,positive,positive,positive,positive
257737622,"You want different tasks for different samples in a batch, not different tasks for different batches. This is completely different.
To do what you want you can use tf.select on the two cost vectors. And it'll always compute both costs, because you cannot only compute part of a tensor, not in any existing libraries. Alternatively you can split your batch to two based on the indicator,  compute two costs and return a sum.
",want different different batch different different completely different want use two cost always compute compute part tensor alternatively split batch two based indicator compute two return sum,issue,negative,neutral,neutral,neutral,neutral,neutral
257675148,"Thanks for the helpful advice.  This method seems to work, but I am having one issue with regards to `tf.cond`.  In my model (similar to DoReFa), I have the indicator read in as an input for each data point:

```
def _get_input_vars(self):
    return [InputVar(tf.float32, [None, 224, 224, 3], 'input'),
            InputVar(tf.int32, [None], 'label'),
            InputVar(tf.bool, [None], 'indicator') ]
```

This indicator is used, as discussed, to condition the loss function:

```
def _build_graph(self, input_vars):
        image, label, indicator = input_vars
        ...
        self.cost = tf.cond(indicator, cost_1, cost_2)
```

Meaning that the predicate for `tf.cond` is a tensor of shape (?,) whereas it expects a boolean:

```
ValueError: Shapes (?,) and () are not compatible
```

Is there a proper way to format the predicate to get around this error?  Also, is there a nice way to confirm that `tf.cond` is using the correct loss function for each iteration?
",thanks helpful advice method work one issue model similar indicator read input data point self return none none none indicator used condition loss function self image label indicator indicator meaning predicate tensor shape whereas compatible proper way format predicate get around error also nice way confirm correct loss function iteration,issue,negative,positive,positive,positive,positive,positive
257626715,"I've tried the original architecture from DeepMind (no pooling), not much difference.
",tried original architecture much difference,issue,negative,positive,positive,positive,positive,positive
257477329,"A ResNet-18 model with (W,A,G)=(1,4,32) should get 60% accuracy. But the training was done in a private framework and was not converted to tensorflow.
",model get accuracy training done private framework converted,issue,negative,neutral,neutral,neutral,neutral,neutral
257421359,"1. That frequently? Just um... (it's complicated) sending some data to the server. Sending it every step is not the best solution.

I. Ok, I may completely miss the point. Could you help me on the following problem:
What is a best way to compute loss on a given datapoint, then run train_op on this datapoint? (otherwise it's a waste!)

II. The way I ""normally"" do is I benchmark the whole loop body including the data_generator.next() call and all the other calls, and separately the train step only to get reliable (?) benchmark whether GPU train_op or some other calls/loading data/saving etc. is the bottleneck. (Last time inefficiently written Augmentation (?) caused 1s whole loop vs 450ms train_op, the problem may be somewhere else tho)

by run_other_ops (that's what should have been) I meant computing predictions on current datapoint, or some sample of train set or anything else you want to compute or do (like saving) while training.
",frequently um complicated sending data server sending every step best solution may completely miss point could help following problem best way compute loss given run otherwise waste way normally whole loop body call separately train step get reliable whether bottleneck last time inefficiently written augmentation whole loop problem may somewhere else tho meant current sample train set anything else want compute like saving training,issue,positive,positive,positive,positive,positive,positive
257395808,"Sure it won't be hard to make the changes. But meanwhile I'm also curious about what your use case is and see if there's a better solution for that.
1. To run things more frequently you can change the epoch size, or use `PeriodicCallback` to adjust the frequency. Unless you really want something to run very frequently (every several iterations), and I wonder what that could be.
2. What do you mean by tensor statistics? `before the train_op` seems no different from `after the last train_op`, unless you're using the less efficient SimpleTrainer which loads data between two calls to the op.
3. As said above, you cannot accurately test the time of data loading by this two callbacks, because it is supposed to run in parallel to train_op. 
   To test the speed of your dataflow, you can either write a small function which simply loops over the dataflow, or do `TestDataSpeed(dataflow, size=1000).start_test()` which does the same thing.
   The iteration speed given in training should be the speed of `train_op`, unless the data is slower, in which case you can use `FakeData` instead to obtain a speed measurement of `train_op`. And what do you mean by `other_run_ops`?
",sure wo hard make meanwhile also curious use case see better solution run frequently change epoch size use adjust frequency unless really want something run frequently every several wonder could mean tensor statistic different last unless le efficient data two said accurately test time data loading two supposed run parallel test speed either write small function simply thing iteration speed given training speed unless data case use instead obtain speed measurement mean,issue,positive,positive,neutral,neutral,positive,positive
257191667,"This looks OK to me. Nevertheless you can easily compute some numerical gradients to check if the final formula is correct.
",nevertheless easily compute numerical check final formula correct,issue,negative,positive,positive,positive,positive,positive
257175582,"The code earlier has some bug on performance. The updated code now should produce the numbers I mentioned above (or better).
",code bug performance code produce better,issue,negative,positive,positive,positive,positive,positive
257102615,"Thanks a lot. I will try out your code.
Closing this issue for now. 
",thanks lot try code issue,issue,negative,positive,positive,positive,positive,positive
255573702,"Done! All requested modifications are added and also tested : )
",done added also tested,issue,negative,neutral,neutral,neutral,neutral,neutral
255551660,"That's Ok. It did compute for a while and create 5 mp4s of it learning the game, but it did stop the function after this error and go back to the $ prompt in Terminal.
The error seems to get cropped in places when I copy and paste it so I'm attaching
<img width=""588"" alt=""screen shot 2016-10-22 at 2 23 18 pm"" src=""https://cloud.githubusercontent.com/assets/14045554/19622272/42c6b436-9863-11e6-960c-211e47c54155.png"">
 a screenshot.
I'll put the entire log below and close the issue cause I'm not too invested in what it's about either...

$
$ python run-atari.py --load MsPacman-v0.tfmodel --env MsPacman-v0
[1021 20:56:46 @run-atari.py:87] Environment Name: MsPacman-v0
[2016-10-21 20:56:46,646] Making new env: MsPacman-v0
[2016-10-21 20:56:46,761] Making new env: MsPacman-v0
[2016-10-21 20:56:46,842] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.2209.video000000.mp4
[1021 20:56:46 @_common.py:61] conv0 input: [None, 84, 84, 12]
[1021 20:56:46 @_common.py:69] conv0 output: [None, 84, 84, 32]
[1021 20:56:46 @_common.py:61] pool0 input: [None, 84, 84, 32]
[1021 20:56:46 @_common.py:69] pool0 output: [None, 42, 42, 32]
[1021 20:56:46 @_common.py:61] conv1 input: [None, 42, 42, 32]
[1021 20:56:46 @_common.py:69] conv1 output: [None, 42, 42, 32]
[1021 20:56:46 @_common.py:61] pool1 input: [None, 42, 42, 32]
[1021 20:56:47 @_common.py:69] pool1 output: [None, 21, 21, 32]
[1021 20:56:47 @_common.py:61] conv2 input: [None, 21, 21, 32]
[1021 20:56:47 @_common.py:69] conv2 output: [None, 21, 21, 64]
[1021 20:56:47 @_common.py:61] pool2 input: [None, 21, 21, 64]
[1021 20:56:47 @_common.py:69] pool2 output: [None, 10, 10, 64]
[1021 20:56:47 @_common.py:61] conv3 input: [None, 10, 10, 64]
[1021 20:56:47 @_common.py:69] conv3 output: [None, 10, 10, 64]
[1021 20:56:47 @_common.py:61] fc0 input: [None, 10, 10, 64]
[1021 20:56:47 @_common.py:69] fc0 output: [None, 512]
[1021 20:56:47 @_common.py:61] fc-pi input: [None, 512]
[1021 20:56:47 @_common.py:69] fc-pi output: [None, 9]
[1021 20:56:47 @sessinit.py:70] Restoring checkpoint from MsPacman-v0.tfmodel.
[1021 20:56:47 @sessinit.py:132] WRN Variable fc-v/W:0 in checkpoint not found in the graph!
[1021 20:56:47 @sessinit.py:132] WRN Variable fc-v/b:0 in checkpoint not found in the graph!
('Total:', 5780.0)
[2016-10-21 20:57:39,242] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.2209.video000001.mp4
('Total:', 2120.0)
('Total:', 6300.0)
('Total:', 5600.0)
('Total:', 4330.0)
('Total:', 7520.0)
('Total:', 5870.0)
('Total:', 6690.0)
[2016-10-21 21:03:55,389] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.2209.video000008.mp4
('Total:', 5180.0)
('Total:', 4990.0)
('Total:', 6690.0)
('Total:', 4450.0)
('Total:', 4240.0)
('Total:', 4990.0)
('Total:', 5400.0)
('Total:', 6770.0)
('Total:', 6040.0)
('Total:', 4290.0)
('Total:', 5010.0)
('Total:', 4990.0)
('Total:', 6410.0)
('Total:', 5230.0)
('Total:', 5290.0)
('Total:', 4810.0)
('Total:', 6770.0)
('Total:', 5590.0)
('Total:', 7230.0)
[2016-10-21 21:38:11,856] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.2209.video000027.mp4
('Total:', 5710.0)
('Total:', 5180.0)
('Total:', 7430.0)
('Total:', 4970.0)
('Total:', 7210.0)
('Total:', 1520.0)
('Total:', 5190.0)
('Total:', 7900.0)
('Total:', 4120.0)
('Total:', 5610.0)
('Total:', 8670.0)
('Total:', 4720.0)
('Total:', 6010.0)
('Total:', 4550.0)
('Total:', 3260.0)
('Total:', 8050.0)
('Total:', 4870.0)
('Total:', 7160.0)
('Total:', 3480.0)
('Total:', 7120.0)
('Total:', 7800.0)
('Total:', 6190.0)
('Total:', 6810.0)
('Total:', 8520.0)
('Total:', 6180.0)
('Total:', 1690.0)
('Total:', 6620.0)
('Total:', 7160.0)
('Total:', 1530.0)
('Total:', 4610.0)
('Total:', 5680.0)
('Total:', 7790.0)
('Total:', 7350.0)
('Total:', 6910.0)
('Total:', 6500.0)
('Total:', 4990.0)
('Total:', 6870.0)
[2016-10-21 22:15:13,811] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.2209.video000064.mp4
('Total:', 7110.0)
('Total:', 6730.0)
('Total:', 8120.0)
('Total:', 4290.0)
('Total:', 7180.0)
('Total:', 5790.0)
('Total:', 6330.0)
('Total:', 5880.0)
('Total:', 4790.0)
('Total:', 6680.0)
('Total:', 8100.0)
('Total:', 6710.0)
('Total:', 4940.0)
('Total:', 5760.0)
('Total:', 6980.0)
('Total:', 4790.0)
('Total:', 7120.0)
('Total:', 4980.0)
('Total:', 6210.0)
('Total:', 4410.0)
('Total:', 6780.0)
('Total:', 4880.0)
('Total:', 8680.0)
('Total:', 4690.0)
('Total:', 7220.0)
('Total:', 9290.0)
('Total:', 4760.0)
('Total:', 6580.0)
('Total:', 7520.0)
('Total:', 7430.0)
('Total:', 5960.0)
('Total:', 4930.0)
('Total:', 7500.0)
('Total:', 6770.0)
('Total:', 6970.0)
('Total:', 6110.0)
Exception gym.error.Error: Error('env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)',) in <bound method AtariEnv.__del__ of <gym.envs.atari.atari_env.AtariEnv object at 0x11c966450>> ignored
Exception gym.error.Error: Error('env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)',) in <bound method Monitor.__del__ of <gym.monitoring.monitor.Monitor object at 0x11c966410>> ignored
$
$
$
",compute create learning game stop function error go back prompt terminal error get copy paste screen shot put entire log close issue cause either python load environment name making new making new starting new video recorder writing input none output none pool input none pool output none input none output none pool input none pool output none input none output none pool input none pool output none input none output none input none output none input none output none variable found graph variable found graph starting new video recorder writing starting new video recorder writing starting new video recorder writing starting new video recorder writing exception error garbage collected keep monitor must keep around reference object hint try variable code bound method object exception error garbage collected keep monitor must keep around reference object hint try variable code bound method object,issue,negative,positive,neutral,neutral,positive,positive
255551245,"Yes I saw that as well. Haven't got time to investigate that but it seems to me this complaint is not harmful. You should still get the output.
",yes saw well got time investigate complaint harmful still get output,issue,negative,negative,negative,negative,negative,negative
255474595,"Yes, one major assumption in this framework (right now) is that you have **one** cost function to optimize. So joint training of multi-task is easy, but alternative training and GAN style training is, although possible, a bit hacky. I'll think about what a proper abstraction is, to allow a model with multiple costs, but here is what can be done now:

Two DataFlow is equivalent to one DataFlow with an indicator, e.g., you have a dataflow which produces
[data1, label1], another produces [data2, label2]. Writing a DataFlow which produces [data1, label1, data2, label2, True/False] is sufficient for the model to know which cost to use and how to compute it. Then you can setup a model with 5 `InputVar` (the last is a `tf.bool`) and use `cost = tf.cond(indicator, get_loss1, get_loss2)`. 

This should work fine in terms of computation cost, because `tf.cond` has lazy evaluation of its arguments `fn1` and `fn2`, so you won't waste any computation, if you write something like:

``` python
data1, label1, data2, label2, indicator = inputs
# no code should be outside of the loss functions
def loss1():
    return whatever_model_with_cost(data1, label1)
def loss1():
    return whatever_model_with_cost(data2, label2)
self.cost = tf.cond(indicator, loss1, loss2)
```

However, the hacky part is that the mixed DataFlow has to produce some garbage data for the unused inputs, and pass this garbage data to the graph. Because with `tf.cond`, the graph would still assume the cost depends on both loss1 and loss2 and will require the user to feed some inputs for both losses. 

So the only inefficient part is generating and feeding garbage data for the unused inputs. If the inputs for your different tasks happen to have the same shape (or just same rank) you can use the same `InputVar` for both tasks, then there will be nothing inefficient. Otherwise you can define your `InputVar` with partial shape and only feed the minimum possible tensor to reduce the data feeding cost (although the cost won't be significant), but this is just not very elegant.
",yes one major assumption framework right one cost function optimize joint training easy alternative training gan style training although possible bit hacky think proper abstraction allow model multiple done two equivalent one indicator data label another data label writing data label data label sufficient model know cost use compute setup model last use cost indicator work fine computation cost lazy evaluation wo waste computation write something like python data label data label indicator code outside loss loss return data label loss return data label indicator loss loss however hacky part mixed produce garbage data unused pas garbage data graph graph would still assume cost loss loss require user feed inefficient part generating feeding garbage data unused different happen shape rank use nothing inefficient otherwise define partial shape feed minimum possible tensor reduce data feeding cost although cost wo significant elegant,issue,negative,positive,neutral,neutral,positive,positive
255274319,"It's running. Thank you so much for your help.
Do need to add
'--display true'
to the end of... 
$'python run-atari.py --load Breakout-v0.tfmodel --env Breakout-v0'
to see it play?
I just have the following in Terminal...
......[1020 19:43:34 @_common.py:69] conv3 output: [None, 10, 10, 64]
[1020 19:43:34 @_common.py:61] fc0 input: [None, 10, 10, 64]
[1020 19:43:34 @_common.py:69] fc0 output: [None, 512]
[1020 19:43:34 @_common.py:61] fc-pi input: [None, 512]
[1020 19:43:34 @_common.py:69] fc-pi output: [None, 6]
[1020 19:43:34 @sessinit.py:70] Restoring checkpoint from Breakout-v0.tfmodel.
[1020 19:43:34 @sessinit.py:132] WRN Variable fc-v/W in checkpoint not found in the graph!
[1020 19:43:34 @sessinit.py:132] WRN Variable fc-v/b in checkpoint not found in the graph!
('Total:', 450.0)
[2016-10-20 19:44:49,820] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.69274.video000001.mp4
('Total:', 850.0)
('Total:', 473.0)
('Total:', 860.0)
('Total:', 436.0)
('Total:', 461.0)
",running thank much help need add display end load see play following terminal output none input none output none input none output none variable found graph variable found graph starting new video recorder writing,issue,positive,positive,neutral,neutral,positive,positive
255272380,"The error is saying that the output file `gym-submit/*` already exists. If you delete the directory then it will run.
",error saying output file already delete directory run,issue,negative,neutral,neutral,neutral,neutral,neutral
255272041,"I got past the parts discussed earlier, and thank you again.
I though I was set up to go but I don't know what I'm doing with this. I'm taking a break from this for a while

$ python run-atari.py --load Breakout-v0.tfmodel --env Breakout-v0
[1020 12:37:33 @run-atari.py:87] Environment Name: Breakout-v0
[2016-10-20 12:37:33,622] Making new env: Breakout-v0
[2016-10-20 12:37:33,672] Making new env: Breakout-v0
Traceback (most recent call last):
  File ""run-atari.py"", line 98, in <module>
    run_submission(cfg)
  File ""run-atari.py"", line 66, in run_submission
    player = get_player(dumpdir=dirname)
  File ""run-atari.py"", line 27, in get_player
    pl = GymEnv(ENV_NAME, dumpdir=dumpdir, auto_restart=False)
  File ""/Users/me/PE11/gym/tensorpack/RL/gymenv.py"", line 33, in **init**
    self.gymenv.monitor.start(dumpdir)
  File ""/Users/me/PE11/gym/gym/monitoring/monitor.py"", line 138, in start
    You should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files.'''.format(directory, ', '.join(training_manifests[:5])))
gym.error.Error: Trying to write to monitor directory gym-submit with existing monitor files: gym-submit/openaigym.manifest.0.67134.manifest.json.

 You should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files.
$
$
",got past thank though set go know taking break python load environment name making new making new recent call last file line module file line player file line file line file line start use unique directory training run use automatically clear previous monitor directory trying write monitor directory monitor use unique directory training run use automatically clear previous monitor,issue,positive,positive,neutral,neutral,positive,positive
255188958,"If you had the file but it's not pointing correctly, then either your filesystem doesn't support such link, or you have copied the project to other filesystem (which doesn't support link) before.
In this case you can just copy the file from Atari2600/ to here.
This has nothing to do with python.
",file pointing correctly either support link copied project support link case copy file nothing python,issue,positive,neutral,neutral,neutral,neutral,neutral
255187868,"Thank for all the responses
The export PYTHONPATH=$PYTHONPATH thing seems to have worked
I think I'm down to one last thing...
That common.py file is pointing to another common.py file in the /Atari2600 folder if I'm not mistaken.
I know I should know how to do this before bothering a software wizard but I don't know how to get it to point to the Atari2600 common file
The original way it's written I get
    ../Atari2600/common.py
    ^
SyntaxError: invalid syntax

I dug online and couldn't figure out how to do it for python (OSX)
I tried saving the path as this instead (in the common.py file)...
~/Users/AnthonyCelio/PE11/gym/tensorpack/examples/Atari2600/common.py
     ^
SyntaxError: invalid syntax

and tried

```
~/Atari2600/common.py
 ^
```

SyntaxError: invalid syntax
",thank export thing worked think one last thing file pointing another file folder mistaken know know wizard know get point common file original way written get invalid syntax dug could figure python tried saving path instead file invalid syntax tried invalid syntax,issue,negative,positive,neutral,neutral,positive,positive
255169387,"1. There is a symbolic-link `common.py` in the [OpenAIGym directory](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/OpenAIGym), it looks like you lost that file somehow.
2. the command `readlink -f` may not work on MacOSX. You just need to use
   `export PYTHONPATH=$PYTHONPATH:/absolute/path/to/PE11/gym/tensorpack`
",directory like lost file somehow command may work need use export,issue,negative,neutral,neutral,neutral,neutral,neutral
255165825,"That is all I want to do. I'm just beyond my depth. When I type...
$ python run-atari.py --load Breakout-v0.tfmodel --env Breakout-v0
Traceback (most recent call last):
  File ""run-atari.py"", line 24, in <module>
    from common import play_one_episode
ImportError: cannot import name play_one_episode

I think the...
 Enable import tensorpack:
export PYTHONPATH=$PYTHONPATH:`readlink -f path/to/tensorpack`
... part from your instructions is my problem. I don't know.
It's to make Python able to find the tensorpack file?
I enter this and get an error...
$ export PYTHONPATH=$PYTHONPATH: readlink -f /Users/me/PE11/gym/tensorpack
-bash: export: `-f': not a valid identifier
-bash: export:`/Users/me/PE11/gym/tensorpack': not a valid identifier
",want beyond depth type python load recent call last file line module common import import name think enable import export part problem know make python able find file enter get error export export valid identifier export valid identifier,issue,negative,positive,neutral,neutral,positive,positive
255015193,"The formulation doesn't necessarily mean how we implement it.
F = 2 quantize(stuff) - 1. Therefore for whatever linear computation F is involved, we can use quantize(stuff) instead to compute the result, and then do a scaling/addition later accordingly.
This is mentioned in Sec 2.6, 

```
By construction, there is always an affine mapping between these low bitwidth
numbers and fixed-point integers. As a result, all the expensive operations can be accelerated signif-
icantly by the fixed-point integer dot product kernel
```
",formulation necessarily mean implement quantize stuff therefore whatever linear computation involved use quantize stuff instead compute result later accordingly sec construction always affine low result expensive accelerated integer dot product kernel,issue,negative,negative,negative,negative,negative,negative
255014373,"I don't understand what you're asking. What exactly do you want to do?

If you just want to see it play Breakout, use `run-atari.py` as the instructions in the readme and it'll produce a directory with videos.

```
./run-atari.py --load Breakout-v0.tfmodel --env Breakout-v0
```
",understand exactly want want see play breakout use produce directory load,issue,negative,positive,positive,positive,positive,positive
254919972,"Okay, here is an attempt at an explicit proof for the derivative of the weight quantization function used in DoReFa v2 paper. It is remarkable that we trust TensorFlow to auto-differentiate this correctly. Please let me know if this proof looks correct, thank you!

When implementing DoReFa outside of TensorFlow, it is necessary to have an explicit derivative for computing Step 18 of Algorithm 1 from DoReFa v2 paper: gW<sub>k</sub> = g<sub>W<sub>k</sub><sup>b</sup></sub> \* ∂W<sub>k</sub><sup>b</sup> / ∂W<sub>k</sub>.
- Note that dr<sub>i<sub>ℓ</sub></sub> refers to the _derivative_ of some function with respect to r<sub>i<sub>ℓ</sub></sub>, while d(r<sub>i<sub>ℓ</sub></sub>) is itself a function.
- r<sub>i<sub>ℓ</sub></sub> refers to some real-valued weight _i_ in layer _ℓ_. 

Let F<sub>w</sub><sup>k</sup> := 2 \* quantize<sub>k</sub>[ tanh(r<sub>i<sub>ℓ</sub></sub>) / ( 2*max(|tanh(r<sub>i</sub>)|) ) + 1/2 ] - 1

Given quantize<sub>k</sub>(r<sub>i<sub>ℓ</sub></sub>) := (1 / (2<sup>k</sup>-1)) \* round((2<sup>k</sup> - 1)r<sub>i<sub>ℓ</sub></sub>).

Find dF<sub>w</sub><sup>k</sup>/dr<sub>i<sub>ℓ</sub></sub>.

**First, express F<sub>w</sub><sup>k</sup> as the composition of multiple functions, so that _chain rule_ may be used.**
(1) F<sub>w</sub><sup>k</sup> = 2 \* quantize<sub>k</sub>[ tanh(r<sub>i<sub>ℓ</sub></sub>) \* ( 2*argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|) )<sup>-1</sup> + 1/2 ] - 1

(2) Define component functions as follows:
- h(r<sub>i<sub>ℓ</sub></sub>) = 2 \* argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|)
- e(r<sub>i<sub>ℓ</sub></sub>) = (r<sub>i<sub>ℓ</sub></sub>)<sup>-1</sup>
- d(r<sub>i<sub>ℓ</sub></sub>) = tanh(r<sub>i<sub>ℓ</sub></sub>) \* e(h(r<sub>i<sub>ℓ</sub></sub>)) + 1/2
- c(r<sub>i<sub>ℓ</sub></sub>) = quantize<sub>k</sub>(r<sub>i<sub>ℓ</sub></sub>) = (1 / (2<sup>k</sup>-1)) \* round((2<sup>k</sup>-1)r<sub>i<sub>ℓ</sub></sub>)
- b(r<sub>i<sub>ℓ</sub></sub>) = 2 \* r<sub>i<sub>ℓ</sub></sub> - 1

Note – to avoid confusion with F<sub>w</sub><sup>k</sup>(r<sub>i<sub>ℓ</sub></sub>), there is **no** f(r<sub>i<sub>ℓ</sub></sub>). There is also **no** g(r<sub>i<sub>ℓ</sub></sub>).

(3) ∴ F<sub>w</sub><sup>k</sup> = b(c(d(r<sub>i<sub>ℓ</sub></sub>)))
(4) (_via chain rule_) F’<sub>w</sub><sup>k</sup> = b’(c(d(r<sub>i<sub>ℓ</sub></sub>))) \* c’(d(r<sub>i<sub>ℓ</sub></sub>)) \* d’(r<sub>i<sub>ℓ</sub></sub>)
**Second, begin finding the three derivatives in (4), so that F' may be calculated.**
**Start with d’(r<sub>i<sub>ℓ</sub></sub>)**
(5) d(r<sub>i<sub>ℓ</sub></sub>) = tanh(r<sub>i<sub>ℓ</sub></sub>) \* e(h(r<sub>i<sub>ℓ</sub></sub>)) + 1/2
(6) Let a(r<sub>i<sub>ℓ</sub></sub>) = e(h(r<sub>i<sub>ℓ</sub></sub>))
(7) (_via product rule_) d’(r<sub>i<sub>ℓ</sub></sub>) = dtanh(r<sub>i<sub>ℓ</sub></sub>)/dr<sub>i<sub>ℓ</sub></sub> \* a(r<sub>i<sub>ℓ</sub></sub>) + tanh(r<sub>i<sub>ℓ</sub></sub>) \* a’(r<sub>i<sub>ℓ</sub></sub>)
(8) (_via Identity One_) d’(r<sub>i<sub>ℓ</sub></sub>) = (1-tanh<sup>2</sup>(r<sub>i<sub>ℓ</sub></sub>)) \* a(r<sub>i<sub>ℓ</sub></sub>) + tanh(r<sub>i<sub>ℓ</sub></sub>) \* a’(r<sub>i<sub>ℓ</sub></sub>)
**Now find a’(r<sub>i<sub>ℓ</sub></sub>)**
(9) (_via chain rule_) a’(r<sub>i<sub>ℓ</sub></sub>) = e’(h(r<sub>i<sub>ℓ</sub></sub>)) \* h’(r<sub>i<sub>ℓ</sub></sub>)
**Begin with differentiating h(r<sub>i<sub>ℓ</sub></sub>) = 2 \* argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|)**
(10) (_via Lemma One_) h’(r<sub>i<sub>ℓ</sub></sub>) = 0 if r<sub>i<sub>ℓ</sub></sub> ∉ ℓ<sub>max</sub>, and 2 \* (1/|ℓ<sub>max</sub>|) if if r<sub>i<sub>ℓ</sub></sub> ∈ ℓ<sub>max</sub>, where |ℓ<sub>max</sub>| denotes the cardinality of the set ℓ<sub>max</sub>, where ℓ<sub>max</sub> is the set of all r<sub>i<sub>ℓ</sub></sub> that satisfy argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|).
(11) (_via (9)_) ∴ a’(r<sub>i<sub>ℓ</sub></sub>) = 0 if r<sub>i<sub>ℓ</sub></sub> ∉ ℓ<sub>max</sub>
**Now find e’(h(r<sub>i<sub>ℓ</sub></sub>))**
(12) (_via power rule_) e’(h(r<sub>i<sub>ℓ</sub></sub>)) = -1 \* [2 \* argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|)]<sup>-2</sup>
(13) ∴ a’(r<sub>i<sub>ℓ</sub></sub>) = [ -1 \* (2 \* argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|))<sup>-2</sup> ] \* h’(r<sub>i<sub>ℓ</sub></sub>) = 
Case 1: 0 if r<sub>i<sub>ℓ</sub></sub> ∉ ℓ<sub>max</sub>
Case 2: [ -1 \* (2 \* argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|))<sup>-2</sup> ] \* [ 2 \* (1/|ℓ<sub>max</sub>|) ] if r<sub>i<sub>ℓ</sub></sub> ∈ ℓ<sub>max</sub>.
(14) ∴ d’(r<sub>i<sub>ℓ</sub></sub>) = (1-tanh<sup>2</sup>(r<sub>i<sub>ℓ</sub></sub>)) \* a(r<sub>i<sub>ℓ</sub></sub>) + tanh(r<sub>i<sub>ℓ</sub></sub>) \* a’(r<sub>i<sub>ℓ</sub></sub>) = 
Case 1: (1-tanh<sup>2</sup>(r<sub>i<sub>ℓ</sub></sub>)) \* a(r<sub>i<sub>ℓ</sub></sub>) if r<sub>i<sub>ℓ</sub></sub> ∉ ℓ<sub>max</sub>
Case 2: (1-tanh<sup>2</sup>(r<sub>i<sub>ℓ</sub></sub>)) \* a(r<sub>i<sub>ℓ</sub></sub>) + tanh(r<sub>i<sub>ℓ</sub></sub>) \* a’(r<sub>i<sub>ℓ</sub></sub>) if r<sub>i<sub>ℓ</sub></sub> ∈ ℓ<sub>max</sub>.
**Now find c’(d(r<sub>i<sub>ℓ</sub></sub>))**
(15) (_via Lemma Two_) c’(d(r<sub>i<sub>ℓ</sub></sub>)) = 1
**Now find b’(c(d(r<sub>i<sub>ℓ</sub></sub>))**
(16) b’(c(d(r<sub>i<sub>ℓ</sub></sub>)) = 2 
**Now find F’<sub>w</sub><sup>k</sup>**
(17) F’<sub>w</sub><sup>k</sup> = b’(c(d(r<sub>i<sub>ℓ</sub></sub>))) \* c’(d(r<sub>i<sub>ℓ</sub></sub>)) \* d’(r<sub>i<sub>ℓ</sub></sub>) 
= 2 \* 1 \* d’(r<sub>i<sub>ℓ</sub></sub>)
Case 1: 2 \* [ (1-tanh<sup>2</sup>(r<sub>i<sub>ℓ</sub></sub>)) \* a(r<sub>i<sub>ℓ</sub></sub>) ] if r<sub>i<sub>ℓ</sub></sub> ∉ ℓ<sub>max</sub>
Expanding Case 1:
= 2 \* [ 1-tanh<sup>2</sup>(r<sub>i<sub>ℓ</sub></sub>)) \* (2 \* argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|))<sup>-1</sup> ]
Case 2: 2 \* [ (1-tanh<sup>2</sup>(r<sub>i<sub>ℓ</sub></sub>)) \* a(r<sub>i<sub>ℓ</sub></sub>) + tanh(r<sub>i<sub>ℓ</sub></sub>) \* a’(r<sub>i<sub>ℓ</sub></sub>) ] if r<sub>i<sub>ℓ</sub></sub> ∈ ℓ<sub>max</sub>.
Expanding Case 2: 
= 2 \* [ 1-tanh<sup>2</sup>(r<sub>i<sub>ℓ</sub></sub>)) \* (2 \* argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|))<sup>-1</sup> + tanh(r<sub>i<sub>ℓ</sub></sub>) \* (-1 \* (2 \* argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|))<sup>-2</sup> ] \* [ 2 \* (1/|ℓ<sub>max</sub>|) ) ] if r<sub>i<sub>ℓ</sub></sub> ∈ ℓ<sub>max</sub>.

We now have an explicit formula for differentiating F<sub>w</sub><sup>k</sup> with respect to r<sub>i<sub>ℓ</sub></sub> over two cases, Case 1 in which r<sub>i<sub>ℓ</sub></sub> ∉ ℓ<sub>max</sub> and Case 2 in which r<sub>i<sub>ℓ</sub></sub> ∈ ℓ<sub>max</sub>.

**Identities**
1. Identity One: d/dx tanh(x) = 1-tanh<sup>2</sup>(x)

**Lemmas**
**[1] Lemma One: subderivative of the argmax function.**
Quoting Wikipedia, “the subderivative, subgradient, and subdifferential generalize the derivative to functions which are not differentiable. The subdifferential of a function is set-valued.” (https://en.wikipedia.org/wiki/Subderivative).

Intuitively, the subderivative is a way to differentiate continuous functions at non-differentiable points. 

Without proof we hold that the subderivative of a function f(x) with domain ∈ ℝ and range ∈ ℝ at a non-differentiable point x<sub>0</sub> is the closed set [_a_, _b_] where _a_ = lim<sub>x→x<sub>0-</sub></sub>[(f(x)-f(x<sub>0</sub>)) / (x-x<sub>0</sub>)] and _b_ = lim<sub>x→x<sub>0+</sub></sub>[(f(x)-f(x<sub>0</sub>)) / (x-x<sub>0</sub>)].

That is, the subderivative at point x<sub>0</sub> is a set whose elements are the closed interval [_a_, _b_] where _a_ and _b_ are the left-hand and right-hand derivatives of f(x) as x approaches x<sub>0</sub>, respectively.

(N.B.: You can think of the traditional derivative as a special case where the set comprising the solutions to the subderivative has only one element.)

**Accordingly, we define the subderivative of the argmax function by cases.**
(1) Let y(r<sub>i<sub>ℓ</sub></sub>) = argmax(|tanh(r<sub>i<sub>ℓ</sub></sub>)|), where y(r<sub>i<sub>ℓ</sub></sub>) evaluates to _true_ when r<sub>i<sub>ℓ</sub></sub> produces the maximum value of |tanh(r<sub>i<sub>ℓ</sub></sub>)| ∀ r<sub>i<sub>ℓ</sub></sub> ∈ ℓ, and _false_ otherwise.

That is, y(r<sub>i<sub>ℓ</sub></sub>) is evaluated ∀ r<sub>i<sub>ℓ</sub></sub> ∈ ℓ, and is true when its input is the weight in layer ℓ that produces the maximum value for the |tanh(r<sub>i<sub>ℓ</sub></sub>)| function.

(2) Let ℓ<sub>max</sub> := { r<sub>i<sub>ℓ</sub></sub> | y(r<sub>i<sub>ℓ</sub></sub>) } denote the set of all weights in layer ℓ that produces a maximum value as defined in (1).
**(3) Case one: argmax’(|tanh(r<sub>i<sub>ℓ</sub></sub>)|) = 0 if r<sub>i<sub>ℓ</sub></sub> ∉ ℓ<sub>max</sub>.**
**(4) Case two: argmax’(|tanh(r<sub>i<sub>ℓ</sub></sub>)|) = 1 / |ℓ<sub>max</sub>| if r<sub>i<sub>ℓ</sub></sub> ∈ ℓ<sub>max</sub>** where |ℓ<sub>max</sub>| denotes the cardinality (number of elements) of the set ℓ<sub>max</sub>. 

Note that if r<sub>i<sub>ℓ</sub></sub> is the only element in ℓ<sub>max</sub> then its subderivative is 1.

Note that if r<sub>i<sub>ℓ</sub></sub> is _not_ the only element in ℓ<sub>max</sub> then it splits the subderivative of 1 equally with all other r<sub>i<sub>ℓ</sub></sub> ∈ ℓ<sub>max</sub>.    

**[2] Lemma Two: differentiating quantize<sub>k</sub>(r<sub>i<sub>ℓ</sub></sub>) yields 1 under condition of Expectation of a Straight-Through Estimator.**  
(1) Given quantize<sub>k</sub>(r<sub>i<sub>ℓ</sub></sub>) = (1 / (2<sup>k</sup>-1)) \* round((2<sup>k</sup> - 1)r<sub>i<sub>ℓ</sub></sub>).
(2) Consider ~quantize<sub>k</sub>(r<sub>i<sub>ℓ</sub></sub>) ≔ (1 / (2<sup>k</sup>-1)) \* ((2<sup>k</sup> - 1)r<sub>i<sub>ℓ</sub></sub>) = r<sub>i<sub>ℓ</sub></sub>.
(3) ~quantize’<sub>k</sub>(r<sub>i<sub>ℓ</sub></sub>) = 1.
(4) Observation: in the absence of a _round()_ function the two scale factors cancel and the derivative is 1. Can we make the _round()_ function “disappear” in some justified fashion?
(5) Note that the _round(r<sub>i<sub>ℓ</sub></sub>)_ function adds some number _n_ ∈ [-0.5, 0.5] to r<sub>i<sub>ℓ</sub></sub> to round r<sub>i<sub>ℓ</sub></sub> to the nearest integer. 
(6) If _n_ = 0, then quantize’<sub>k</sub>(r<sub>i<sub>ℓ</sub></sub>) = ~quantize’<sub>k</sub>(r<sub>i<sub>ℓ</sub></sub>) = 1.
(7) if _n_ ≠ 0, then our outer scale factor of 1/(2<sup>k</sup>-1) is technically multiplying the result of (2<sup>k</sup>-1)_(r<sub>i<sub>ℓ</sub></sub> + _n_), and thus the scale factors do not cancel.
(8) Note that the probability of _n_ == 0 is low, and so any individual call to quantize<sub>k</sub>() is unlikely to result in a case where we can ignore the _round()_ function.
(8) However, under assumption that _n_ is drawn uniformly and at random from [-0.5, 0.5], the expectation 𝐄(_n_) = 0.
(9) *_∴ under expectation 𝐄(_n_) = 0, quantize’<sub>k</sub>(r<sub>i<sub>ℓ</sub></sub>) = ~quantize’<sub>k</sub>(r<sub>i<sub>ℓ</sub></sub>) = 1.**

Consider that the quantize<sub>k</sub> function decomposes into two scaling factors and a round operator. The two scaling factors are the multiplicands 1/(2<sup>k</sup>-1) and (2<sup>k</sup>-1), on the outside and inside of the round operator respectively. 

If we evaluated this function in the absence of a round operator, the two scalars would cancel, and the function would change in a manner directly proportional to the input r<sub>i</sub>, leaving a derivative of 1. Since a(r<sub>i</sub>) is 2 \* quantize<sub>k</sub>(r<sub>i</sub>) - 1, this derivative would be 2 \* 1 = 2.

The round operator is simply an additive shift of [-0.5, 0.5], as it either rounds a number down or up to the nearest integer value. Thus our outer scale factor of 1/(2<sup>k</sup>-1) is technically multiplying the result of (2<sup>k</sup>-1)*(r<sub>i</sub>)+ δ, where δ ∈ [-0.5, 0.5].

In any given evaluation of a(r<sub>i</sub>) it is unlikely that the two scale factors are exact inverses, due to the low probability of δ = 0. However, under assumption that δ is drawn uniformly and at random from [-0.5, 0.5], the expectation is 𝐄(δ) = 0. 
**∴ under expectation 𝐄(δ) = 0, dquantize<sub>k</sub>(r<sub>i</sub>)/dr<sub>i</sub> = 1**
",attempt explicit proof derivative weight quantization function used paper remarkable trust correctly please let know proof correct thank outside necessary explicit derivative step algorithm paper sub sub sub sup sub sup sub note sub sub function respect sub sub sub sub function sub sub weight layer let sub sup quantize sub tanh sub sub sub given quantize sub sub sub sup round sup sub sub find sub sup sub sub first express sub sup composition multiple may used sub sup quantize sub tanh sub sub sub sub sup define component sub sub sub sub sub sub sub sub sup sub sub tanh sub sub sub sub sub sub quantize sub sub sub sup round sup sub sub sub sub sub sub note avoid confusion sub sup sub sub sub sub also sub sub sub sup sub sub chain sub sup sub sub sub sub sub sub second begin finding three may calculated start sub sub sub sub tanh sub sub sub sub let sub sub sub sub product sub sub sub sub sub sub sub sub tanh sub sub sub sub identity sub sub sup sub sub sub sub tanh sub sub sub sub find sub sub chain sub sub sub sub sub sub begin sub sub sub sub lemma sub sub sub sub sub sub sub sub sub sub set sub sub set sub sub satisfy sub sub sub sub sub sub sub find sub sub power sub sub sub sub sup sub sub sub sub sup sub sub case sub sub sub case sub sub sup sub sub sub sub sub sub sup sub sub sub sub tanh sub sub sub sub case sup sub sub sub sub sub sub sub case sup sub sub sub sub tanh sub sub sub sub sub sub sub find sub sub lemma sub sub find sub sub sub sub find sub sup sub sup sub sub sub sub sub sub sub sub case sup sub sub sub sub sub sub sub expanding case sup sub sub sub sub sup case sup sub sub sub sub tanh sub sub sub sub sub sub sub expanding case sup sub sub sub sub sup tanh sub sub sub sub sup sub sub sub sub explicit formula sub sup respect sub sub two case sub sub sub case sub sub sub identity one tanh sup lemma one subderivative function subderivative generalize derivative differentiable function intuitively subderivative way differentiate continuous without proof hold subderivative function domain range point sub closed set lim sub sub sub sub lim sub sub sub sub subderivative point sub set whose closed interval sub respectively think traditional derivative special case set subderivative one element accordingly define subderivative function let sub sub sub sub sub sub sub sub maximum value sub sub sub sub otherwise sub sub sub sub true input weight layer maximum value sub sub function let sub sub sub sub sub denote set layer maximum value defined case one sub sub sub sub sub case two sub sub sub sub sub sub sub number set sub note sub sub element sub subderivative note sub sub element sub subderivative equally sub sub sub lemma two quantize sub sub sub condition expectation estimator given quantize sub sub sub sup round sup sub sub consider sub sub sub sup sup sub sub sub sub sub sub sub observation absence function two scale cancel derivative make function disappear fashion note sub sub function number sub sub round sub sub nearest integer quantize sub sub sub sub sub sub outer scale factor sup technically multiplying result sup sub sub thus scale cancel note probability low individual call quantize sub unlikely result case ignore function however assumption drawn uniformly random expectation expectation quantize sub sub sub sub sub sub consider quantize sub function two scaling round operator two scaling sup sup outside inside round operator respectively function absence round operator two would cancel function would change manner directly proportional input sub leaving derivative since sub quantize sub sub derivative would round operator simply additive shift either number nearest integer value thus outer scale factor sup technically multiplying result sup sub given evaluation sub unlikely two scale exact due low probability however assumption drawn uniformly random expectation expectation sub sub sub,issue,positive,negative,neutral,neutral,negative,negative
254367388,"`backwad_input` computes the gradient of C w.r.t.  a_{L-1} (or L-2, L-3,...), i.e. the output of previous layer. So the next loop iteration can use it.
`backward_weight` computes the gradient w.r.t. W_l (or L-1, L-2,...).

Yes in step 11 we just assume this is a trivial element-wise operation so we didn't introduce new notations to distinguish what's before the activation and what's after. (or ""gradients"" and ""errors"" in your words). So g_{a_k} is a variable that holds the gradient w.r.t. the output of layer k either before or after activation.
",gradient output previous layer next loop iteration use gradient yes step assume trivial operation introduce new distinguish activation variable gradient output layer either activation,issue,negative,negative,neutral,neutral,negative,negative
254348712,"I think I misunderstood `backward_input` and `backward_weight`. Can you please explain what steps 13 and 14 do? It is unclear how they operate within the loop started in step 10 (Algorithm 1, DoReFa v2 paper).

When backpropagating the following things need to happen:
1) Compute the output layer's gradient: this is dC/da<sub>L</sub>, the derivative of the cost function with respect to the final layer (layer L)'s output activations. 

This looks like step 9 in Algorithm 1: ""Compute g<sub>a<sub>L</sub></sub> = ∂C/∂a<sub>L</sub> knowing a<sub>L</sub> and a*.""

2) Feed this gradient through the derivative of the activation function, to get the error δ<sub>L</sub> of the output layer: δ<sub>L</sub> = g<sub>a<sub>L</sub></sub>⊙σ’(z<sub>L</sub>), where σ’() is the derivative of the activation function, z<sub>L</sub> are the weighted inputs to the last layer L, and ⊙ is the Hadamard product (elementwise product).

This looks like step 11 in Algorithm 1: ""Back-propagate _g<sub>a<sub>k</sub></sub>_ through activation function _h_.""

3) Backpropagate the error δ<sub>L</sub> ∀ ℓ < L. δ<sub>ℓ</sub> = ((w<sub>ℓ+1</sub>)<sup>T</sup> · δ<sub>ℓ+1</sub>) ⊙σ’(z<sub>ℓ</sub>).

This might be step 13? Except the output of step 13 is _non-quantized_ (note there is no <sup>b</sup>, the output is simply _g<sub>a<sub>k-1</sub></sub>_).

4) Apply the update to the weights: w<sub>ℓ</sub> → w<sub>ℓ</sub> - (η/m) \* (a<sub>ℓ-1</sub> · δ<sub>ℓ</sub>), where a<sub>ℓ-1</sub> is the activation vector from layer ℓ-1,  w<sub>ℓ</sub> is the weight matrix at layer ℓ, and _m_ is the mini-batch size.

This looks like steps 18 and 19 in Algorithm 1: 
_gW<sub>k</sub>_ = _gW<sub>k</sub><sup>b</sup>_ \* (∂W<sub>k</sub><sup>b</sup> / ∂W<sub>k</sub>). 
W<sub>k</sub><sup>t+1</sup> ← Update(W<sub>k</sub>, g<sub>W<sub>k</sub></sub>, η).

Questions:
Q1) In the four steps outlined above for backprop, when do we apply the quantization routine in Step 12 from Algorithm 1?
Q2) What is the meaning of `backward_input()` - is it one of the four steps above or something else?
Q3) What is the meaning of `backward_weight()` - is it one of the four steps above or something else?
Q4) Why is the output of step 13's `backward_input()` non-quantized?

EDIT: Upon re-reading, it looks like you're actually quantizing _errors_ not gradients? And step 13 uses the quantized error g<sub>a<sub>k</sub></sub><sup>b</sup> and the quantized weight matrix W<sub>k</sub><sup>b</sup> to get a non-quantized quantity for the previous layer, g<sub>a<sub>k-1</sub></sub>, which you must then use in Step 11 of the _next iteration of the for loop_ to get the true g<sub>a<sub>k-1</sub></sub>, at which point you quantize it to g<sub>a<sub>k-1</sub></sub><sup>b</sup> with Step 12.

Is this correct?

In this case the mistake would come from overloading the notation g<sub>a<sub>k-1</sub></sub> to refer to the error δ<sub>ℓ</sub> at layer ℓ as well as the quantity A in the following equation: δ<sub>ℓ</sub> = A ⊙σ’(z<sub>ℓ</sub>), where A == ((w<sub>ℓ+1</sub>)<sup>T</sup> · δ<sub>ℓ+1</sub>).
",think misunderstood please explain unclear operate within loop step algorithm paper following need happen compute output layer gradient sub derivative cost function respect final layer layer output like step algorithm compute sub sub sub knowing sub feed gradient derivative activation function get error sub output layer sub sub sub sub derivative activation function sub weighted last layer product product like step algorithm sub sub activation function error sub sub sub sup sub sub might step except output step note sup output simply sub sub apply update sub sub sub sub sub activation vector layer sub weight matrix layer size like algorithm sub sub sup sub sup sub sub sup update sub sub sub four outlined apply quantization routine step algorithm meaning one four something else meaning one four something else output step edit upon like actually step error sub sub sup weight matrix sub sup get quantity previous layer sub sub must use step iteration get true sub sub point quantize sub sub sup step correct case mistake would come notation sub sub refer error sub layer well quantity following equation sub sub sub sup sub,issue,negative,positive,neutral,neutral,positive,positive
254310832,"> The sign-vs-unsign problem is more relevant in FPGA. But as we are only doing summation, unsign numbers should be fine.

How do you implement this bitwise dot product kernel (equation 3 in section 2.1, DoReFa v2 paper) for negative weights? 

The quantize<sub>k</sub> function defined in Section 2.2 as Equation 5 outputs a number _r<sub>o</sub>_ ∈ [0, 1]. The affine transform on F<sub>w</sub><sup>k</sup>(r<sub>i</sub>) in Equation 9 takes the output of a quantize<sub>k</sub> function and multiplies by 2 and subtracts 1: F<sub>w</sub><sup>k</sup>(r<sub>i</sub>) = 2 \* quantize<sub>k</sub>(stuff) - 1.

Thus F<sub>w</sub><sup>k</sup>(r<sub>i</sub>) ∈ [-1, 1].

However, the procedure you define in Equation 3 only works for unsigned values. If some values _x<sub>i</sub>_ in the sequence _x_ or _y<sub>i</sub>_ in the sequence _y_ are negative, then their contribution to the dot product is a subtraction, not an addition, so the simple bitcount(and()) operation no longer suffices.

How did you change the bitwise dot product procedure to account for negative weights?

One possibility:
1. Add an additional sign bit to all M-bit fixed point integers _x<sub>i</sub>_ ∈ _x_ and all K-bit fixed point integers _y<sub>i</sub>_ ∈ _y_.
2. This bit is 1 if the number _x<sub>i</sub>_ is negative, and 0 if _x<sub>i</sub>_ is positive (likewise for the _y<sub>i</sub>_), but does not count as a place-value bit for multiplication.
3. Let bitwise_and<sub>(m, k)</sub> = and(c<sub>m</sub>(x), c<sub>k</sub>(y)), ∀(m, k), ignoring the sign bits.
4. Let bitwise_sign = xor(x<sub>i<sub>signed bit</sub></sub>, y<sub>i<sub>signed bit</sub></sub>). This gives us the sign of the product of x<sub>i</sub> and y<sub>i</sub>
5. ∀ bitwise_and<sub>(m, k)</sub> ∀(m, k), note that bitwise_sign<sub>(x<sub>i</sub>, y<sub>i</sub>)</sub> is a vector giving the sign for each element in bitwise_and<sub>(m, k)</sub>.
6. For each pair of vectors ( bitwise_and<sub>(m, k)</sub>, bitwise_sign<sub>(x<sub>i</sub>, y<sub>i</sub>)</sub> ) ∀(m,k), drop all members of bitwise_and<sub>(m, k)</sub> and their corresponding signs in bitwise_sign<sub>(x<sub>i</sub>, y<sub>i</sub>)</sub> where bitwise_and<sub>(m, k)</sub> == 0. This leaves us with the cases where the bitwise multiplication produced a 1, along with their signs.
7. For each pair of vectors ( bitwise_and<sub>(m, k)</sub>, bitwise_sign<sub>(x<sub>i</sub>, y<sub>i</sub>)</sub> ) ∀(m,k), compute bitcount[ bitwise_sign<sub>(x<sub>i</sub>, y<sub>i</sub>)</sub> ] to get the total number of negatives for the (m*k) place-value. The total number of positives is given by _len_(bitwise_sign<sub>(x<sub>i</sub>, y<sub>i</sub>)</sub>) - bitcount[ bitwise_sign<sub>(x<sub>i</sub>, y<sub>i</sub>)</sub> ].
8. Use the negative and positive accumulations in 7 to get the signed contribution to the dot product.
",problem relevant summation fine implement bitwise dot product kernel equation section paper negative quantize sub function defined section equation number sub affine transform sub sup sub equation output quantize sub function sub sup sub quantize sub stuff thus sub sup sub however procedure define equation work unsigned sub sequence sub sequence negative contribution dot product subtraction addition simple operation longer change bitwise dot product procedure account negative one possibility add additional sign bit fixed point sub fixed point sub bit number sub negative sub positive likewise sub count bit multiplication let sub sub sub sign let sub sub bit sub sub bit u sign product sub sub sub note sub sub sub vector giving sign element sub pair sub sub sub sub drop sub corresponding sub sub sub sub leaf u bitwise multiplication produced along pair sub sub sub sub compute sub sub sub get total number total number given sub sub sub sub sub sub use negative positive get contribution dot product,issue,negative,negative,neutral,neutral,negative,negative
253874131,"Thanks my mistake - steps 12 and 13 are part of a loop...
",thanks mistake part loop,issue,negative,positive,positive,positive,positive,positive
253686025,"Quantization is done in step 12 already. Not step 13.

`backward_input` doesn't involve anything with the activation function because it is handled in step 11.

You can use whatever bitwidth large enough to store these intermediate values because they only get involved in cheap operations.
",quantization done step already step involve anything activation function handled step use whatever large enough store intermediate get involved cheap,issue,negative,positive,positive,positive,positive,positive
253674826,"Another question - when we implement `backward_input()`, we end up doing the following:
dC/da<sub>L</sub> ⊙ σ’(z⃗<sub>L</sub>) = δ⃗<sub>L</sub>. 
(where dC/da<sub>L</sub> is a vector = g<sub>a<sub>L</sub></sub> from step 9 of DoReFa v2 Algorithm 1, and σ’(z⃗<sub>L</sub>) is the derivative of the activation function σ() evaluated over the weighted inputs z⃗<sub>L</sub>)

This gives us the error vector δ⃗<sub>L</sub> from which we can use `backward_weight()` to get the gradients with respect to the weight matrix.

The questions: 
1) What bit-width do you use for the components of the vector σ’(z⃗<sub>L</sub>)?
2) What bit-width do you use for the vector z⃗<sub>L</sub> (this may already be answered [here](https://github.com/ppwwyyxx/tensorpack/issues/27#issuecomment-250816137) as a bitwidth of (M+K) where M and K are the bit-widths of the multiplicands used to generate the components of z⃗<sub>L</sub>)

Thank you
",another question implement end following sub sub sub sub vector sub sub step algorithm sub derivative activation function weighted sub u error vector sub use get respect weight matrix use vector sub use vector sub may already used generate sub thank,issue,negative,neutral,neutral,neutral,neutral,neutral
253672662,"Confirming one more detail. On step 13 of Algorithm 1 in DoReFa v2 paper, you say:

> 13: g<sub>a<sub>k-1</sub></sub>←`backward_input`(g<sup>b</sup><sub>a<sub>k</sub></sub>, W<sup>b</sup><sub>k</sub>)

Does this mean that we have to re-quantize the gradients g<sub>a<sub>k-1</sub></sub> using _f_<sup>G</sup><sub>γ</sub>() before backpropagating back another layer?
",confirming one detail step algorithm paper say sub sub sup sub sub sup sub mean sub sub sup sub back another layer,issue,negative,negative,negative,negative,negative,negative
253652335,"1. the input to the max function is full precision, so yes.
2. yes.
",input function full precision yes yes,issue,positive,positive,positive,positive,positive,positive
253651568,"Thanks ppwwyyxx, two more questions:

1) When I'm differentiating 2 \* quantize<sub>k</sub>[ tanh(r<sub>ℓ<sub>i </sub></sub>) / (2*max(|tanh(r<sub>ℓ<sub>i </sub></sub>)|)) + 1/2] -1 with respect to dr<sub>ℓ<sub>i </sub></sub>, for that _max()_ function in the denominator, you write ""where the maximum is taken over all weights in that layer"" (page 4, DoReFa v2 paper).

Are you talking about finding the maxima of the full-precision, _non-quantized_ weights?

2) Does ""full-precision"" for DoReFa v2 paper mean ""float32"" ?
",thanks two quantize sub tanh sub sub sub sub respect sub sub function denominator write maximum taken layer page paper talking finding maximum paper mean float,issue,positive,negative,neutral,neutral,negative,negative
253355344,"Ohhhhhh I get it, so if there are multiple r<sub>ℓ<sub>i</sub></sub> that satisfy r<sub>ℓ<sub>max</sub></sub>, you just split the unit gradient of 1 evenly among them. 

So if r<sub>ℓ<sub>i</sub></sub> = r<sub>ℓ<sub>j</sub></sub> = r<sub>ℓ<sub>k</sub></sub> = r<sub>ℓ<sub>max</sub></sub>, then dr<sub>ℓ<sub>max</sub></sub>/dr<sub>ℓ<sub>i</sub></sub> = dr<sub>ℓ<sub>max</sub></sub>/dr<sub>ℓ<sub>j</sub></sub> = dr<sub>ℓ<sub>max</sub></sub>/dr<sub>ℓ<sub>k</sub></sub> = (1/3).

Okay wow, that's subtle. Please confirm that this is what you mean! 

Second question - if I can no longer treat d[2max(|tanh(r<sub>i</sub>)|)]<sup>-1</sup>/dr<sub>i</sub> as a constant, then I must use chain rule with it and expand my differentiation of 2 \* quantize<sub>k</sub>[ _tanh_(r<sub>i</sub>) / (2*_max_(|_tanh_(r<sub>i</sub>)|)) + 1/2] -1 further. Is this correct?
",get multiple sub sub satisfy sub sub split unit gradient evenly among sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub wow subtle please confirm mean second question longer treat sub sup sub constant must use chain rule expand differentiation quantize sub sub sub correct,issue,positive,negative,neutral,neutral,negative,negative
253353831,"With subgradient, the result can actually be any number in [0,1]. 
If you modify my code example above to try you'll see if input is [1,2,2,2], output will be [0,0.33,0.33,0.33]. But that's just a choice by tensorflow. Other library may have different choices. It's just people usually don't use zero because it's always better to have some gradient..
",result actually number modify code example try see input output choice library may different people usually use zero always better gradient,issue,negative,positive,neutral,neutral,positive,positive
253352698,"Yes, this is definitely a subgradient. However, it seems unstable. What do you recommend in the case where r<sub>ℓ<sub>i</sub></sub> = r<sub>ℓ<sub>j</sub></sub> =  r<sub>ℓ<sub>max</sub></sub>? In other words, what if there are two components of the r⃗<sub>ℓ</sub> weight vector that are both the same maximum?

I am not sure I understand fully, thank you.
",yes definitely however unstable recommend case sub sub sub sub sub sub two sub weight vector maximum sure understand fully thank,issue,positive,positive,positive,positive,positive,positive
253351914,"Yes. It works for optimization purpose and this is what is implemented in most frameworks. Technically this is not a derivative anymore, but a subgradient.
",yes work optimization purpose technically derivative,issue,positive,neutral,neutral,neutral,neutral,neutral
253347721,"Okay that's interesting. Let me confirm that I understand what you are saying.

Is this what you mean?
1. Let r⃗<sub>ℓ</sub> := a vector with weights from layer _ℓ_
2. Let r<sub>ℓ<sub>i</sub></sub> := the i<sup>th</sup> component of the weight vector r⃗<sub>ℓ</sub>
3. Let r<sub>ℓ<sub>max</sub></sub> := the maximum component of the weight vector r⃗<sub>ℓ</sub>
4. ∀ r<sub>ℓ<sub>i</sub></sub> where r<sub>ℓ<sub>i</sub></sub> ≠ r<sub>ℓ<sub>max</sub></sub>, dr<sub>ℓ<sub>max</sub></sub>/dr<sub>ℓ<sub>i</sub></sub> = 0.
5. When r<sub>ℓ<sub>i</sub></sub> = r<sub>ℓ<sub>max</sub></sub>, dr<sub>ℓ<sub>max</sub></sub>/dr<sub>ℓ<sub>i</sub></sub> = 1.
",interesting let confirm understand saying mean let sub vector layer let sub sub sup th component weight vector sub let sub sub maximum component weight vector sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub sub,issue,negative,positive,neutral,neutral,positive,positive
253281980,"max(x) is an amount that depends on x, more specifically it depends on the maximum element in x.
That's why the gradients would be 0,0,0,1. At least that's what's implemented in tensorflow.
Since this gradient is just a subgradient, treating max(x) as a constant and use 0,0,0,0 may also work in training. But I cannot guarantee that.
",amount specifically maximum element would least since gradient treating constant use may also work training guarantee,issue,negative,negative,negative,negative,negative,negative
253280628,"Regarding STE - okay, I think we are doing the same thing, just different justifications. My argument provides a reason for defining the gradient of quantize<sub>k</sub> as 1. (It is a way to convert the step-wise shape of the round() graph into a linear approximation, whose slope is 1.)

Regarding your `tf.gradients()` code - I don't understand. If we are differentiating 2 \* quantize<sub>k</sub>[ _tanh_(r<sub>i</sub>) / (2*_max_(|_tanh_(r<sub>i</sub>)|)) + 1/2] -1 with respect to r<sub>i</sub>, then the max(r<sub>i</sub>) for a given layer is just a per-layer constant. I think I am misunderstanding you. 
",regarding think thing different argument reason gradient quantize sub way convert shape round graph linear approximation whose slope regarding code understand quantize sub sub sub respect sub sub given layer constant think misunderstanding,issue,negative,negative,neutral,neutral,negative,negative
253278241,"I wrote the code to show what the gradient is.

STE can be thought of as a function whose derivative is defined differently, so that gradient can propagate better.

I don't understand your argument on expectation. But the function quantize_k, mathematically, has zero gradient almost everywhere: http://www.wolframalpha.com/input/?i=round(x+*+7)+%2F+7
We define the gradient to be 1 so that it can be trained.
",wrote code show gradient thought function whose derivative defined differently gradient propagate better understand argument expectation function mathematically zero gradient almost everywhere define gradient trained,issue,negative,positive,positive,positive,positive,positive
253275543,"I'll take a look at that, thank you for the code. Where does that code come
from?

Also, does my interpretation of the STE and its value under expectation
match yours?

On Oct 11, 2016 9:15 PM, ""Yuxin Wu"" notifications@github.com wrote:

> d quantizek(ri)/dri = 1. This is correct. That's exactly how we define
> the function quantize_k in (5) and (6).
> 
> max() actually does have a gradient. It varies with respect to the maximum
> value. You might need to take that into account.
> 
> a = tf.placeholder(dtype=tf.float32, name='a', shape=[4])
> b = tf.reduce_max(a)
> c = tf.gradients([b], [a])[0]with tf.Session() as sess:
>     v = np.asarray([1, 2, 3, 4], dtype='float32')
>     print sess.run(c, feed_dict={a:v})  # 0, 0, 0, 1
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> https://github.com/ppwwyyxx/tensorpack/issues/31#issuecomment-253114839,
> or mute the thread
> https://github.com/notifications/unsubscribe-auth/AEIejzEcxXrZN6ZOj3ZttIT3x2KoQtT4ks5qzF7HgaJpZM4KRgfU
> .
",take look thank code code come also interpretation value expectation match wrote correct exactly define function actually gradient respect maximum value might need take account sess print thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
253114839,"d quantize<sub>k</sub>(r<sub>i</sub>)/dr<sub>i</sub> = 1. This is correct. That's exactly how we define the function quantize_k in (5) and (6).

max() actually does have a gradient. It varies with respect to the maximum value. You might need to take that into account.

``` python
a = tf.placeholder(dtype=tf.float32, name='a', shape=[4])
b = tf.reduce_max(a)
c = tf.gradients([b], [a])[0]
with tf.Session() as sess:
    v = np.asarray([1, 2, 3, 4], dtype='float32')
    print sess.run(c, feed_dict={a:v})  # 0, 0, 0, 1
```
",quantize sub sub sub correct exactly define function actually gradient respect maximum value might need take account python sess print,issue,positive,positive,positive,positive,positive,positive
253071724,"> Wkb = f_w(Wk) defined in equation (9) so your second strategy is correct.

Okay, let's do this. Does the following result look correct to you? I am particularly anxious about my understanding of the Straight-Through-Estimator, as I could not find the Hinton video lecture you cite (which the Bengio paper you cite also references).

**dF<sub>w</sub><sup>k</sup>/dr<sub>i</sub> = 2 \* ⟦ (1-tanh<sup>2</sup>(r<sub>i</sub>)) \* [2 \* max(|tanh(r<sub>i</sub>)|)]<sup>-1</sup> ⟧**

# 

Let F<sub>w</sub><sup>k</sup> := 2 \* quantize<sub>k</sub>[ tanh(r<sub>i</sub>) / ( 2*max(|tanh(r<sub>i</sub>)|) ) + 1/2 ] - 1

Given quantize<sub>k</sub>(r<sub>i</sub>) := (1 / (2<sup>k</sup>-1)) \* round((2<sup>k</sup> - 1)r<sub>i</sub>).

Find dF<sub>w</sub><sup>k</sup>/dr<sub>i</sub>.

**First, express F<sub>w</sub><sup>k</sup> as the composition of multiple functions, so that _chain rule_ may be used.**
(1) F<sub>w</sub><sup>k</sup> = 2 \* quantize<sub>k</sub>[ tanh(r<sub>i</sub>) \* ( 2*max(|tanh(r<sub>i</sub>)|) )<sup>-1</sup> + 1/2 ] - 1

(2) Define component functions as follows:
- a(r<sub>i</sub>) = 2 \* [1/(2<sup>k</sup>-1) \* round((2<sup>k</sup>-1) \* r<sub>i</sub>)] -1
- b(r<sub>i</sub>) = tanh(r<sub>i</sub>) \* [ 2*max(|tanh(r<sub>i</sub>)|) ]<sup>-1</sup> + 1/2

∴  F<sub>w</sub><sup>k</sup> = a(b(r<sub>i</sub>))

(3) (_via chain rule_) dF<sub>w</sub><sup>k</sup>/dr<sub>i</sub> = a'(b(r<sub>i</sub>)) \* b'(r<sub>i</sub>)

**Start by expanding b'(r<sub>i</sub>)**
(4) The _max()_ function in the square brackets is a constant when derivating with respect to r<sub>i</sub>, as the max value is taken over all the weights r<sub>i</sub> in a given layer, and thus does not vary with respect to the particular r<sub>i</sub> with which we are derivating.
(5) ∴ (_via Identity One_) b'(r<sub>i</sub>) = (1-tanh<sup>2</sup>(r<sub>i</sub>)) \* [2 \* max(|tanh(r<sub>i</sub>)|)]<sup>-1</sup>

**Now expand a'(b(r<sub>i</sub>))**
First consider a'(r<sub>i</sub>)
(6) (_under condition of Expectation of a Straight-Through-Estimator_) a'(r<sub>i</sub>) = 2.
Consider that the quantize<sub>k</sub> function decomposes into two scaling factors and a round operator. The two scaling factors are the multiplicands 1/(2<sup>k</sup>-1) and (2<sup>k</sup>-1), on the outside and inside of the round operator respectively. 

If we evaluated this function in the absence of a round operator, the two scalars would cancel, and the function would change in a manner directly proportional to the input r<sub>i</sub>, leaving a derivative of 1. Since a(r<sub>i</sub>) is 2 \* quantize<sub>k</sub>(r<sub>i</sub>) - 1, this derivative would be 2 \* 1 = 2.

The round operator is simply an additive shift of [-0.5, 0.5], as it either rounds a number down or up to the nearest integer value. Thus our outer scale factor of 1/(2<sup>k</sup>-1) is technically multiplying the result of (2<sup>k</sup>-1)*(r<sub>i</sub>)+ δ, where δ ∈ [-0.5, 0.5].

In any given evaluation of a(r<sub>i</sub>) it is unlikely that the two scale factors are exact inverses, due to the low probability of δ = 0. However, under assumption that δ is drawn uniformly and at random from [-0.5, 0.5], the expectation is 𝐄(δ) = 0. 
**∴ under expectation 𝐄(δ) = 0, dquantize<sub>k</sub>(r<sub>i</sub>)/dr<sub>i</sub> = 1**
(7) Given (6), a'(b(r<sub>i</sub>)) = 2, as the derivative a' is a constant, and thus does not vary with respect to the function's input.

**Now compute dF<sub>w</sub><sup>k</sup>/dr<sub>i</sub> = a'(b(r<sub>i</sub>)) \* b'(r<sub>i</sub>)**
(8) dF<sub>w</sub><sup>k</sup>/dr<sub>i</sub> = 2 \* ⟦ (1-tanh<sup>2</sup>(r<sub>i</sub>)) \* [2 \* max(|tanh(r<sub>i</sub>)|)]<sup>-1</sup> ⟧ ∎

**Identities**
1. Identity One: d/dx tanh(x) = 1-tanh<sup>2</sup>(x)
Source: https://en.wikipedia.org/wiki/Hyperbolic_function#Derivatives
",defined equation second strategy correct let following result look correct particularly anxious understanding could find video lecture cite paper cite also sub sup sub sup sub sub sup let sub sup quantize sub tanh sub sub given quantize sub sub sup round sup sub find sub sup sub first express sub sup composition multiple may used sub sup quantize sub tanh sub sub sup define component sub sup round sup sub sub tanh sub sub sup sub sup sub chain sub sup sub sub sub start expanding sub function square constant respect sub value taken sub given layer thus vary respect particular sub identity sub sup sub sub sup expand sub first consider sub condition expectation sub consider quantize sub function two scaling round operator two scaling sup sup outside inside round operator respectively function absence round operator two would cancel function would change manner directly proportional input sub leaving derivative since sub quantize sub sub derivative would round operator simply additive shift either number nearest integer value thus outer scale factor sup technically multiplying result sup sub given evaluation sub unlikely two scale exact due low probability however assumption drawn uniformly random expectation expectation sub sub sub given sub derivative constant thus vary respect function input compute sub sup sub sub sub sub sup sub sup sub sub sup identity one tanh sup source,issue,positive,negative,neutral,neutral,negative,negative
252805147,"The derivative of max function is a zero vector but the position of the maximum is 1.
",derivative function zero vector position maximum,issue,negative,neutral,neutral,neutral,neutral,neutral
252781962,"I'm working on this but am getting stuck - how do you derivate the _max()_ function in the divisor?
",working getting stuck derivate function divisor,issue,negative,neutral,neutral,neutral,neutral,neutral
252765750,"∂Wkb/∂Wk == 1 is not right, because Wkb != quantize(Wk).
Wkb = f_w(Wk) defined in equation (9) so your second strategy is correct.
",right quantize defined equation second strategy correct,issue,negative,positive,positive,positive,positive,positive
252765110,"(6) says ∂c/∂r<sub>i</sub> = ∂c/∂r<sub>o</sub>, where r<sub>i</sub> = a single real-valued weight, and r<sub>o</sub> = a single _k_-bit quantized, real-valued weight.

We want to find a component expression for ∂W<sub>k</sub><sup>b</sup>/∂W<sub>k</sub>, where W<sub>k</sub><sup>b</sup> is a matrix containing quantized weights, and W<sub>k</sub> is the full-precision version of that matrix.

In component terms, we are looking for ∂r<sub>o</sub>/∂r<sub>i</sub>, as r<sub>o</sub> are the components of W<sub>k</sub><sup>b</sup>, and r<sub>i</sub> are the components of W<sub>k</sub>.

Given (6), that ∂c/∂r<sub>i</sub> = ∂c/∂r<sub>o</sub>, are you saying that ∂r<sub>i</sub> = ∂r<sub>o</sub>, which means ∂r<sub>o</sub>/∂r<sub>i</sub> = 1? 

In that case, line 18 of the algorithm on page six reduces from:
gW<sub>k</sub> = gW<sub>k</sub><sup>b</sup>(∂W<sub>k</sub><sup>b</sup>/∂W<sub>k</sub>)
gW<sub>k</sub> = gW<sub>k</sub><sup>b</sup>(1)
gW<sub>k</sub> = gW<sub>k</sub><sup>b</sup>

Is this what you intended? I think I am mistaken somehow but am not sure…

# 

My other strategy is to attempt computing the derivative with respect to r<sub>i</sub> for the following equation:
2 \* quantize<sub>k</sub>[ _tanh_(r<sub>i</sub>) / (2*_max_(|_tanh_(r<sub>i</sub>)|)) + 1/2] -1.

Please advise, thank you!
",sub sub sub single weight sub single weight want find component expression sub sup sub sub sup matrix sub version matrix component looking sub sub sub sub sup sub sub given sub sub saying sub sub sub sub case line algorithm page six sub sub sup sub sup sub sub sub sup sub sub sup intended think mistaken somehow strategy attempt derivative respect sub following equation quantize sub sub sub please advise thank,issue,positive,negative,neutral,neutral,negative,negative
252760811,"You cannot find it, because standard chain rule comes with tensorflow when you compute `tf.gradient`.

(5) and (6) defines the forward and backward (i.e. the derivative) of `quantize()` function. This is also in the code in the definition of `quantize()` function.
",find standard chain rule come compute forward backward derivative quantize function also code definition quantize function,issue,negative,neutral,neutral,neutral,neutral,neutral
252753364,"Thanks, 1 and 2 make sense to me.

For 3 - where can I find this calculation being done in your DoReFa code?

EDIT: Also, when you say ""where the derivative of `quantize()` function is already defined"" - are you referring to something specific in your paper?
",thanks make sense find calculation done code edit also say derivative quantize function already defined something specific paper,issue,negative,positive,neutral,neutral,positive,positive
252382677,"1. The underlying weights W<sub>k</sub>  are always full-precision, including the initialization stage. It is quantized to W<sub>k</sub><sup>b</sup> to compute Wx+b or conv(W,x)+b.
2. It is full precision. After training the network, we only need to keep the quantized value but a full precision update is needed in training.
3. It is standard chain rule. There is a explicit formula to compute W<sub>k</sub><sup>b</sup> from W<sub>k</sub>, where the derivative of `quantize()` function is already defined.
",underlying sub always stage sub sup compute full precision training network need keep value full precision update training standard chain rule explicit formula compute sub sup sub derivative quantize function already defined,issue,negative,positive,positive,positive,positive,positive
252368011,"Bias are not of a specific interest because they don't involve expensive computation, and presumably similar quantization methods would work for them as well. In our work we use batch normalization so the model doesn't contain bias term for those quantized layers.
",bias specific interest involve expensive computation presumably similar quantization would work well work use batch normalization model contain bias term,issue,positive,negative,negative,negative,negative,negative
252366923,"I don't remember you mentioning biases in the paper - do you have any method for quantizing biases? Or do you simply imagine a single bias unit connected to all neurons in all layers, and train the weight for those connections?
",remember paper method simply imagine single bias unit connected train weight,issue,negative,negative,neutral,neutral,negative,negative
252348885,"Yeah I had that impression as well but I'm not sure it has to be the case.. 
Anyway, if cost function is an average then averaging the gradient is necessary by chain-rule.
",yeah impression well sure case anyway cost function average gradient necessary,issue,positive,positive,positive,positive,positive,positive
252347967,"Sorry - I should have specified, where mini-batch size m > 1 I was implying that stochastic gradient descent would be used, where an estimate of the true gradient ∇C = 1/n \* Σ<sub>x</sub> (C<sub>x</sub>) ∀x ∈ X is given instead by ∇C ≈ 1/m \* Σ<sub>t=1 to m</sub>( ∇C<sub>x<sub>t</sub></sub> )

where _X_ is the set of all training samples and _C_ is the cost function, and _m_ is the size of a single mini-batch.

> For example if your cost function is the average cost of all samples then the gradient w.r.t some weight would also be averaged across all samples.

I was under the impression that [all cost functions](http://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications) must be able to be written as an average over cost functions _C<sub>x</sub>_ for individual training samples _x_ ∈ X. Is this incorrect?

EDIT: Regardless, I understand that the quantization method as described in Equation (12) of DoReFa-net paper v2 permits the implementer to apply a mini-batch specific scaling factor to weight the contribution of each mini-batch to the gradient. I believe this is what you mean by ""the maximum is taken over all axis of the gradient tensor _dr_ except for the mini-batch axis (therefore each instance in a mini-batch will have its own scaling factor)"". Please let me know if I am mistaken, thank you.
",sorry size stochastic gradient descent would used estimate true gradient sub sub given instead sub sub sub set training cost function size single example cost function average cost gradient weight would also across impression cost must able written average cost sub individual training incorrect edit regardless understand quantization method equation paper apply specific scaling factor weight contribution gradient believe mean maximum taken axis gradient tensor except axis therefore instance scaling factor please let know mistaken thank,issue,positive,negative,neutral,neutral,negative,negative
252345572,"Apart from the quantization it's just standard back-propagation. For example if your cost function is the average cost of all samples then the gradient w.r.t some weight would also be averaged across all samples.  
",apart quantization standard example cost function average cost gradient weight would also across,issue,negative,negative,neutral,neutral,negative,negative
252344138,"Thanks, if I did use a mini-batch with size > 1, say m = 4, then it seems like all that is necessary is to sum and average the quantized gradients. So in other words, if m=4 then we have 4 training inputs, x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, x<sub>4</sub> per mini-batch, and the gradient is: 1/4 \* Σ<sub>t=1 to m</sub> [ G<sub>t</sub><sup>b</sup> ], where G<sub>t</sub> is the output gradient vector from training input x<sub>t</sub>, and G<sub>t</sub><sup>b</sup> is the quantized gradient vector from training input x<sub>t</sub>. 

Is that correct?
",thanks use size say like necessary sum average training sub sub sub sub per gradient sub sub sup sub output gradient vector training input sub sub sup gradient vector training input sub correct,issue,positive,positive,neutral,neutral,positive,positive
252342333,"Thanks for your help. I'd like to confirm that I understand your gradient quantization method.

Let's say I was implementing DoReFa algorithm on a simple feedforward network. The output layer just outputs a single vector: a list of scalar values: O = [o<sub>1</sub>, o<sub>2</sub>, o<sub>3</sub>, o<sub>4</sub>, ..., o<sub>n</sub>].

I compute the gradients at the output layer: G = [g<sub>1</sub>, g<sub>2</sub>, g<sub>3</sub>, g<sub>4</sub>, ..., g<sub>n</sub>]

To produce the quantized gradients g<sup>b</sup><sub>j</sub> I then do the following (as in Equation (12)):
∀g<sub>j</sub> ∈ G, g<sup>b</sup><sub>j</sub> = 2*max(|G|) \* ( quantize<sub>k</sub> [g<sub>j</sub> / (2 \* max(|G|)) + ½ + N(_k_)] – ½ ).

Where `max(|G|)` is the element in G with the maximum absolute value.  So if G := [0, 4, -2, 1, -5], `max(|G|)` returns `5`.
",thanks help like confirm understand gradient quantization method let say algorithm simple network output layer single vector list scalar sub sub sub sub sub compute output layer sub sub sub sub sub produce sup sub following equation sub sup sub quantize sub sub element maximum absolute value,issue,positive,positive,neutral,neutral,positive,positive
252336866,"Oh I see, if I understand correctly B is just the size of the mini-batch, so ""the mini-batch dimension"" just means the number of training samples in the mini-batch. Thank you.

I'm not very familiar with convolutional nets. I'm currently trying to reproduce your team's results on a simple feedforward network. Is the C in [B,H,W,C] a vector with values for the channels red, green, blue?  (With H and W being height and width?)
",oh see understand correctly size dimension number training thank familiar convolutional currently trying reproduce team simple network vector red green blue height width,issue,negative,positive,neutral,neutral,positive,positive
252335846,"Usually when a mini-batch of B images of shape [H,W,C] is fed into the network for training, they are stacked to form one big tensor of shape [B,H,W,C] and get processed together. This is the case for most frameworks.
It's the same case for all the intermediate layer inputs/outputs. The first dimension is usually the batch dimension.
",usually shape fed network training form one big tensor shape get together case case intermediate layer first dimension usually batch dimension,issue,negative,negative,neutral,neutral,negative,negative
252335150,"Thanks - I do not know what you mean when you say ""the first dimension is the mini-batch dimension."" I'm familiar with mini-batch but not with TensorFlow. 
",thanks know mean say first dimension dimension familiar,issue,negative,positive,positive,positive,positive,positive
252334929,"Don't know which line are you talking about, but in general the code should work for any n-dimensional tensor, where the first dimension is the mini-batch dimension.
",know line talking general code work tensor first dimension dimension,issue,negative,positive,positive,positive,positive,positive
252332868,"Thanks - I've been experimenting with `maxx = tf.reduce_max(tf.abs(x), list(range(1,rank)), keep_dims=True)` to understand what it does. What do you mean by the ""batch"" axis?

I understand why you clipped for safety - that makes sense, thank you.

EDIT: In this code (https://github.com/ppwwyyxx/tensorpack/blob/master/examples/DoReFa-Net/dorefa.py) is _x_ a vector or a matrix?
",thanks list range rank understand mean batch axis understand clipped safety sense thank edit code vector matrix,issue,positive,negative,negative,negative,negative,negative
252331309,"Hi,
1) 3) **the maximum is taken over all axis of the gradient tensor _dr_ except for the mini-batch axis.** This is  the definition of `max_0`, and it is the line:
`maxx = tf.reduce_max(tf.abs(x), list(range(1,rank)), keep_dims=True)`
which takes the maximum on all axes except the first (batch) axis.

2) In the formulation of the paper we need the stochastic noise parameter in the open interval (-0.5/(2^k-1), 0.5/(2^k-1)), but in tensorflow, `tf.random_uniform` return a random number in the left-close-right-open interval.
When the left endpoint of the interval is reached, we'll need to quantize `-0.5/(2^k-1)`, and this may lead to a negative output, depending on whether round(-0.5) is implemented to be 0 or -1. Although this scenario is unlikely to happen, I clipped the value here only for safety.
",hi maximum taken axis gradient tensor except axis definition line list range rank maximum ax except first batch formulation paper need stochastic noise parameter open interval return random number interval left interval need quantize may lead negative output depending whether round although scenario unlikely happen clipped value safety,issue,negative,negative,negative,negative,negative,negative
251743935,"I had an example [script](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/HED/hed.py) to reproduce an edge detection paper. The same architecture (FCN) is supposed to work for image segmentation as well. 

UPDATE: Mask-RCNN was [added](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/FasterRCNN).
",example script reproduce edge detection paper architecture supposed work image segmentation well update added,issue,negative,neutral,neutral,neutral,neutral,neutral
251700285,"You can simply use indexing `W[row]` or `W[:,column]` to select the weights, where `row` or `column` is an integer scalar tf.Tensor. The gradient would work well. `tf.gather` would also work. Only the weights for that task would change.
",simply use indexing row select row column integer scalar gradient would work well would also work task would change,issue,negative,neutral,neutral,neutral,neutral,neutral
251694288,"Thank you for your fast reply.

Lets assume I have n tasks (n linear classifiers at the end of the network, the labels of all tasks have the same ""type/dimension"" but different meaning). At the moment I treat a task ID simply as an input value which flows through the net. I stored my n linear classifier weights in a 2-D Tensor (first dimension is task, second dimension is weights of the tasks).

The task is now simply used in tf.gather to pick the corresponding weights for the task. As I am using a momentum optimizer I do not know if some weights of an ""idle"" task could change as well. Mulitple sources I found on the web suggest that multiple optimizers should be used.

Could I run into issues with my setup?
",thank fast reply assume linear end network different meaning moment treat task id simply input value net linear classifier tensor first dimension task second dimension task simply used pick corresponding task momentum know idle task could change well found web suggest multiple used could run setup,issue,positive,positive,neutral,neutral,positive,positive
251688372,"If it is a cost function of several different cost, you just simply add them.
If each time the data is different, you can implement such DataFlow that generates different data for different tasks, and generate an indicator as well so that the model knows how to deal with them.
",cost function several different cost simply add time data different implement different data different generate indicator well model deal,issue,negative,neutral,neutral,neutral,neutral,neutral
250893656,"The output of multiplication will be fed into nonlinear activation functions, which will quantize these values to low bit-width numbers, before these values are passed to the next convolution.
",output multiplication fed nonlinear activation quantize low next convolution,issue,negative,neutral,neutral,neutral,neutral,neutral
250816137,"Thanks, i'm struggling to understand your point (2). Let's say we have the numbers:
A: `1 0 0` (4)
and
B: `1 0` (2)

Let's write out the `bitcount` and `and` operations:
1. We `and` the `0` from B's least significant digit with each of the digits in A. This will always produce 0.
2. We `and` the `1` from B's most significant digit with each of the digits in A. This will produce 0 except for the case where we `and` with A's most significant digit, which is also 1. In this case, `bitcount(and(1, 1))` gives `1`, and this value must be bitshifted by `2**(m+k)`, where `m=2` and `k=1`. 
3. Thus, we bitshift by `2**3` to get our final product of A \* B. This value is `1000`, which overflows the bitwidth of the two multiplicands.

As you point out, the `and(c_m(x), c_k(y))` will never overflow, and the `bitcount` total may be stored in a high-bitwidth number (say, uint_32) so that we can use the multiply routine over very long sequences of numbers.

My question is what to do about the output of that multiply routine - the returned product can be greater than the bitwidth of the specified parameters (Weights, Activations, Gradients), and it is unclear what bit-width to use for these intermediate values.

EDIT: Okay, I see that using a bitwidth of (M+K) for these intermediate values guarantees we have enough bits to represent them. I'm not sure if this is a lower bound on the necessary bitwidth, though I suspect it is. Unsure of how to prove the lower bound beyond empirically.

Proof:
Let M be the bit-width of multiplicand A, let K be the bit-width of multiplicand B. The maximum value A can take is 2<sup>M</sup>-1. The maximum value B can take is 2<sup>K</sup>-1. To calculate the maximum bit-width necessary to represent the product of A and B we can just multiply these maximum values together.

(2<sup>M</sup>-1) \* (2<sup>K</sup>-1) = 2<sup>(M+K)</sup>-2<sup>M</sup>-2<sup>K</sup>+1.

The maximum number representable with a bitwidth of (M+K) is, by definition, 2<sup>(M+K)</sup>-1. 
Given that M, K > 0, (-2<sup>M</sup>-2<sup>K</sup>) <= -4, and thus 2<sup>(M+K)</sup> - 1 > 2<sup>(M+K)</sup>-2<sup>M</sup>-2<sup>K</sup>+1.

Thus, we know a bitwidth of (M+K) will be sufficient.
",thanks struggling understand point let say let write least significant digit always produce significant digit produce except case significant digit also case value must thus get final product value two point never overflow total may number say use multiply routine long question output multiply routine returned product greater unclear use intermediate edit see intermediate enough represent sure lower bound necessary though suspect unsure prove lower bound beyond proof let multiplicand let multiplicand maximum value take sup maximum value take sup calculate maximum necessary represent product multiply maximum together sup sup sup sup sup maximum number representable definition sup given sup sup thus sup sup sup sup thus know sufficient,issue,negative,positive,positive,positive,positive,positive
250637959,"1) For accumulating intermediate values before the activation, one may optimize the hardware implementation by using lower bit-width addition at the start and use higher bit-width addition towards the higher stages of addition tree.

2) We only do multi-bit multiplication bit-by-bit. There will be no overflow in computing ""and(c_m(x), c_k(y))"". Now as explained by 1), bitcount[and(c_m(x), c_k(y)] will be kept as high bit-width number. Then we can compute the scaled sum of these high bit-width numbers (there will be only M K such numbers) by sufficient bit-width to ensure no overflow at a trivially small cost.
We already have a DoReFa-net running on FPGA that produces results agreeing with the outputs of GPU, hence we are quite sure there will be no overflows.
",intermediate activation one may optimize hardware implementation lower addition start use higher addition towards higher addition multiplication overflow kept high number compute scaled sum high sufficient ensure overflow trivially small cost already running agreeing hence quite sure,issue,positive,positive,positive,positive,positive,positive
250618082,"Two follow up questions:
1) How do you determine the bit-width for intermediate values? For example, when feeding forward we need to calculate weights \* previous activation + biases to get the input to the current layer's activation function. Let's call this value z (the ""weighted input""). What bit-width should z take?

2) How do you handle overflow with these fixed-bit-width integers? For example, assume you have weights restricted to 3-bits and activations restricted to 2-bits. Let's say you have a weight of 100 (4) and an activation of 10 (2) Their dot product is 1000 (8), and requires 4 bits to be represented. Do we just run the activation function on this 4-bit number, and then quantize down to the 2-bit activation value? Are there any other cases when overflow might happen?

Thank you very much
",two follow determine intermediate example feeding forward need calculate previous activation get input current layer activation function let call value weighted input take handle overflow example assume restricted restricted let say weight activation dot product run activation function number quantize activation value overflow might happen thank much,issue,negative,positive,neutral,neutral,positive,positive
250047130,"Just noticed these.
When M != K, it's also possible to not pad the shorter-bit-width numbers, if we implement specialized multiplications like 3bit-by-5bit in FPGA.

The sign-vs-unsign problem is more relevant in FPGA. But as we are only doing summation, unsign numbers should be fine.
",also possible pad implement specialized like problem relevant summation fine,issue,negative,positive,positive,positive,positive,positive
250027675,"Thanks, are my statements regarding 

> (1) what to do when _M_ != _K_   

and  

> (2) the status of all _x<sub>i</sub>_ and _y<sub>i</sub>_ as unsigned fixed point integers due to the later use of affine transformations

in agreement with your method?
",thanks regarding status sub sub unsigned fixed point due later use affine agreement method,issue,positive,positive,neutral,neutral,positive,positive
250026362,"Yes, it matches our method. Thanks for the visualization.
",yes method thanks visualization,issue,positive,positive,positive,positive,positive,positive
249977898,"It took a while for me to understand, but I think I have it. Can you tell me if my understanding matches yours? I found the need to substantially rewrite section 2.1 to make explicit what the sequences are, how they are defined, and how they relate to leveraging reduced bitwidth computation in the context of neural nets. Please let me know if the following is correct.

![dorefa 2 1 part 0](https://cloud.githubusercontent.com/assets/4333199/18889171/90611dc4-84b0-11e6-80cd-9994b6be3cfd.PNG)
![dorefa 2 1 part 1](https://cloud.githubusercontent.com/assets/4333199/18889082/3170695a-84b0-11e6-9747-bc3351ec0df9.PNG)
![dorefa 2 1 part 2](https://cloud.githubusercontent.com/assets/4333199/18889084/3177a86e-84b0-11e6-90c3-b351f347a4fb.PNG)
![dorefa 2 1 part 3](https://cloud.githubusercontent.com/assets/4333199/18889085/317964c4-84b0-11e6-92de-67945234be31.PNG)
![dorefa 2 1 part 4](https://cloud.githubusercontent.com/assets/4333199/18889083/31754da8-84b0-11e6-9481-f4f248916c8a.PNG)
",took understand think tell understanding found need substantially rewrite section make explicit defined relate reduced computation context neural please let know following correct part part part part part,issue,negative,neutral,neutral,neutral,neutral,neutral
249340212,"for any m, c_m(x) is a bit vector, not a bit. similar for c_k(y). The `and` is element-wise and operation of two bit vectors (of the same length p == q).
if say M=K=2, p=q=2. x = [3, 1] = [11, 01] in base 2, , y = [2, 0] = [10, 00].
Then c_0(x) = [1,1], c_1(x) = [1, 0], c_0(y) = [0, 0], c_1(y) = [1,0]
x \dot y = 3 * 2 + 1 * 0 = 6 = 1 \* 0 + 2 \* 1 + 2 \* 0 + 4 \* 1
",bit vector bit similar operation two bit length say base,issue,negative,negative,negative,negative,negative,negative
249285671,"In the latest version of paper released in July: http://arxiv.org/abs/1606.06160, we have corrected the equation to `bitcount(and(xi, yi))`.
",latest version paper corrected equation xi,issue,negative,positive,positive,positive,positive,positive
249206967,"Upgrading to Tensforflow 0.10 fixed the problem for me as well - thanks for the advice!
",fixed problem well thanks advice,issue,negative,positive,positive,positive,positive,positive
248986977,"Hi,
I faced the same issue with Tensorflow 0.9, but it worked when I upgraded to 0.10
",hi faced issue worked,issue,negative,neutral,neutral,neutral,neutral,neutral
248971227,"Using tensorflow 0.9 with CUDA 7.5, Python 2.7, cuDNN 4.0.  Here is a more detailed error when I run on the CPU:

```
$ CUDA_VISIBLE_DEVICES=. ./alexnet-dorefa.py --load npy/alexnet-126.npy --run /tmp/ilsvrc12/test/ILSVRC2012_test_00100000.JPEG --dorefa 1,2,6
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
[0922 13:16:05 @format.py:25] WRN Error in 'import lmdb'. LMDBData won't be available.
[0922 13:16:06 @concurrency.py:27] WRN Cannot import Future in tornado.concurrent. MultiThreadAsyncPredictor won't be available.
[0922 13:16:06 @_common.py:61] conv0 input: [None, 224, 224, 3]
[0922 13:16:06 @_common.py:69] conv0 output: [None, 54, 54, 96]
[0922 13:16:06 @_common.py:61] conv1 input: [None, 54, 54, 96]
[0922 13:16:06 @alexnet-dorefa.py:88] Binarizing weight conv1/W
[0922 13:16:06 @_common.py:69] conv1 output: [None, 54, 54, 256]
[0922 13:16:06 @_common.py:61] pool1 input: [None, 54, 54, 256]
[0922 13:16:06 @_common.py:69] pool1 output: [None, 27, 27, 256]
[0922 13:16:06 @_common.py:61] conv2 input: [None, 27, 27, 256]
[0922 13:16:06 @alexnet-dorefa.py:88] Binarizing weight conv2/W
[0922 13:16:06 @_common.py:69] conv2 output: [None, 27, 27, 384]
[0922 13:16:06 @_common.py:61] pool2 input: [None, 27, 27, 384]
[0922 13:16:06 @_common.py:69] pool2 output: [None, 14, 14, 384]
[0922 13:16:06 @_common.py:61] conv3 input: [None, 14, 14, 384]
[0922 13:16:06 @alexnet-dorefa.py:88] Binarizing weight conv3/W
[0922 13:16:07 @_common.py:69] conv3 output: [None, 14, 14, 384]
[0922 13:16:07 @_common.py:61] conv4 input: [None, 14, 14, 384]
[0922 13:16:07 @alexnet-dorefa.py:88] Binarizing weight conv4/W
[0922 13:16:07 @_common.py:69] conv4 output: [None, 14, 14, 256]
[0922 13:16:07 @_common.py:61] pool4 input: [None, 14, 14, 256]
[0922 13:16:07 @_common.py:69] pool4 output: [None, 6, 6, 256]
[0922 13:16:07 @_common.py:61] fc0 input: [None, 6, 6, 256]
[0922 13:16:07 @alexnet-dorefa.py:88] Binarizing weight fc0/W
[0922 13:16:07 @_common.py:69] fc0 output: [None, 4096]
[0922 13:16:07 @_common.py:61] fc1 input: [None, 4096]
[0922 13:16:07 @alexnet-dorefa.py:88] Binarizing weight fc1/W
[0922 13:16:07 @_common.py:69] fc1 output: [None, 4096]
[0922 13:16:07 @_common.py:61] fct input: [None, 4096]
[0922 13:16:07 @_common.py:69] fct output: [None, 1000]
[0922 13:16:07 @regularize.py:17] Apply regularizer for fc0/W:0
[0922 13:16:07 @regularize.py:17] Apply regularizer for fc1/W:0
[0922 13:16:07 @regularize.py:17] Apply regularizer for fct/W:0
E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: cop3
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: cop3
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:347] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.93  Tue Apr  5 18:18:24 PDT 2016

GCC version:  gcc version 4.4.7 20120313 (Red Hat 4.4.7-17) (GCC) 
""""""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.93.0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.
[0922 13:16:07 @sessinit.py:154] Params to restore: bn3/gamma:0, bnfc0/variance/EMA:0, bn1/beta:0, fc0/W:0, conv2/W:0, bnfc1/gamma:0, conv4/W:0, bnfc0/gamma:0, bn3/beta:0, bn1/gamma:0, bn4/gamma:0, bn4/beta:0, bnfc0/mean/EMA:0, bn4/mean/EMA:0, bnfc0/beta:0, bn1/variance/EMA:0, fct/b:0, conv3/W:0, bn3/variance/EMA:0, bnfc1/variance/EMA:0, bnfc1/mean/EMA:0, bn2/gamma:0, bn1/mean/EMA:0, fct/W:0, conv0/W:0, bn3/mean/EMA:0, fc1/W:0, conv1/W:0, bn2/beta:0, bn4/variance/EMA:0, bn2/variance/EMA:0, bn2/mean/EMA:0, bnfc1/beta:0
[0922 13:16:07 @sessinit.py:164] Restoring from dict ...
Traceback (most recent call last):
  File ""./alexnet-dorefa.py"", line 304, in <module>
    run_image(Model(), ParamRestore(np.load(args.load).item()), args.run)
  File ""./alexnet-dorefa.py"", line 254, in run_image
    predict_func = get_predict_func(pred_config)
  File ""/home/laceyg/tensorpack/tensorpack/predict/common.py"", line 74, in get_predict_func
    return OfflinePredictor(config)
  File ""/home/laceyg/tensorpack/tensorpack/predict/base.py"", line 99, in __init__
    config.session_init.init(sess)
  File ""/home/laceyg/tensorpack/tensorpack/tfutils/sessinit.py"", line 31, in init
    self._init(sess)
  File ""/home/laceyg/tensorpack/tensorpack/tfutils/sessinit.py"", line 165, in _init
    upd.update({name: value for name, value in six.iteritems(self.prms) if name in intersect})
  File ""/home/laceyg/tensorpack/tensorpack/tfutils/varmanip.py"", line 71, in update
    self.sess.run(op, feed_dict={p: value})
  File ""/home/laceyg/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 372, in run
    run_metadata_ptr)
  File ""/home/laceyg/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 640, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/laceyg/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 708, in _do_run
    target_list, options, run_metadata)
  File ""/home/laceyg/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 728, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: AttrValue must not have reference type value of float_ref
     for attr 'dtype'
    ; NodeDef: Placeholder = Placeholder[dtype=DT_FLOAT_REF, shape=[], _device=""/device:CPU:0""](); Op<name=Placeholder; signature= -> output:dtype; attr=dtype:type; attr=shape:shape,default=[]>
```

I may try to upgrade tensorflow to 0.10 to see if it resolves the issue.
",python detailed error run load run successfully library locally successfully library locally successfully library locally successfully library locally successfully library locally error wo available import future wo available input none output none input none weight output none pool input none pool output none input none weight output none pool input none pool output none input none weight output none input none weight output none pool input none pool output none input none weight output none input none weight output none input none output none apply regularizer apply regularizer apply regularizer call diagnostic information host cop cop version found unable find loaded program driver version file content version kernel module tue version version red hat kernel version available machine restore recent call last file line module model file line file line return file line sess file line sess file line name value name value name intersect file line update value file line run file line file line file line raise type message must reference type value output type shape may try upgrade see issue,issue,positive,positive,positive,positive,positive,positive
248966044,"It still works for me. Are you using the latest tensorflow? If yes, could you paste more detailed error?
",still work latest yes could paste detailed error,issue,negative,positive,positive,positive,positive,positive
248539481,"@ppwwyyxx many thanks your code is really great: out-of-the box learning Pong-v0 on moderately recent hardware (6cores + GPU) in ~11h to a mean score of ~20. Most efficient publicly available A3C (variant) implementation I have come across so far...
",many thanks code really great box learning moderately recent hardware mean score efficient publicly available variant implementation come across far,issue,positive,positive,positive,positive,positive,positive
248365145,"I've released the code for [Atari in Gym](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/OpenAIGym/train-atari.py) with A3C.
Don't have much document.. but maybe it'll help. 
",code gym much document maybe help,issue,negative,positive,positive,positive,positive,positive
248361652,"Oh! I didn't know you had a PR already. Thanks again!
",oh know already thanks,issue,negative,positive,positive,positive,positive,positive
248361416,"Thanks! Fixed in latest commit.
",thanks fixed latest commit,issue,positive,positive,positive,positive,positive,positive
248047951,"Confirm. The issue was resolved by upgrading CUDA to 7.5 and cudnn to v5.

On Sep 19, 2016, at 12:07 PM, Yuxin Wu <notifications@github.com<mailto:notifications@github.com>> wrote:

For the record, it's confirmed to be cudnn v4 problem after working with Yixuan.

—
You are receiving this because you commented.
Reply to this email directly, view it on GitHubhttps://github.com/ppwwyyxx/tensorpack/issues/5#issuecomment-248037511, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AGhqh9uxUX8Kr95W9Y0JFmpohxor6g52ks5qrrMngaJpZM4IPktX.
",confirm issue resolved wrote record confirmed problem working reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
248037511,"For the record, it's confirmed to be cudnn v4 problem after working with Yixuan.
",record confirmed problem working,issue,negative,positive,positive,positive,positive,positive
247852109,"Test this for svhn classification: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/svhn-digit-convnet.py, and it yields pretty similar performance as yours (44iters/s).
",test classification pretty similar performance,issue,negative,positive,positive,positive,positive,positive
247848462,"Thanks Yuxin. I’ve configured my environment and made sure tensorflow is all set up appropriately and running in gpu mode. What puzzled me most is the inference still runs very fast whereas the training is drastically slower. Could there be any caveats during training that is causing this? Such as not enough memory allocated, or multi-threading being slow?

100%|#############################################################################################################################################|390/390[35:36<00:00, 0.18it/s]
100%|###############################################################################################################################################|79/79[00:09<00:00, 8.13it/s]
",thanks environment made sure set appropriately running mode puzzled inference still fast whereas training drastically could training causing enough memory slow,issue,positive,positive,positive,positive,positive,positive
247818861,"Yes, so it shows it's using cudnn. Although cudnn v5 might be better, from their [documents](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#requirements).
",yes although might better,issue,positive,positive,positive,positive,positive,positive
247818691,"I do see these at the beginning:

I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
",see beginning successfully library locally successfully library locally successfully library locally successfully library locally successfully library locally,issue,positive,positive,positive,positive,positive,positive
247818598,"Yep the speed has been frustrating me - taking almost 2 days to train cifar-10. I can see my processes on two gpus, each of which uses 4GB memory. I am 97% sure it it running in GPU mode…Does the cuda version matter? I am using cuda4.0 as recommended by tensorflow site. Does the code detect error automatically if cudnn is not being used?
",yep speed taking almost day train see two memory sure running version matter site code detect error automatically used,issue,negative,positive,positive,positive,positive,positive
247818249,"That's very strange.
If I ran it with desktop CPU I had 5.54s/it, which is very close to your speed.
Are you sure GPU is used? Can you see your process in `nvidia-smi`? If not then some error must have been printed.
",strange ran close speed sure used see process error must printed,issue,negative,positive,positive,positive,positive,positive
247817537,"Yeah I wondered that too. Actually even with a single gpu it still remains 0.18iters/s. Any thing else could go wrong? I am following your original setup without customizing anything.

if args.gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu

```
config = get_config()
if args.load:
    config.session_init = SaverRestore(args.load)
if args.gpu:
    config.nr_tower = len(args.gpu.split(','))
SyncMultiGPUTrainer(config).train()
```

On Sep 17, 2016, at 8:32 PM, Yuxin Wu <notifications@github.com<mailto:notifications@github.com>> wrote:

I doubt this alone would make such a huge difference.
What's your speed with one gpu? Ideally it should also be around 2 iter /s.

—
You are receiving this because you commented.
Reply to this email directly, view it on GitHubhttps://github.com/ppwwyyxx/tensorpack/issues/5#issuecomment-247816373, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AGhqh0VaSIMmHgncZxwRV0EE73akPTURks5qrIargaJpZM4IPktX.
",yeah actually even single still remains thing else could go wrong following original setup without anything wrote doubt alone would make huge difference speed one ideally also around iter reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
247816373,"I doubt this alone would make such a huge difference.
What's your speed with one gpu? Ideally it should also be around 2 iter /s.
",doubt alone would make huge difference speed one ideally also around iter,issue,negative,positive,positive,positive,positive,positive
247816075,"Thanks for the quick response! That's super helpful. I am using the latest version of tensorflow and cuda7.5. I will look into tcmalloc. Would that make a huge difference (0.18iters/s -> 2iters/s) in terms of performance, or there could be something else going on I should also look into?
",thanks quick response super helpful latest version look would make huge difference performance could something else going also look,issue,positive,positive,positive,positive,positive,positive
247811232,"That's probably a problem on the environment. Something you can try:
Use a different tensorflow binary. Some previous versions are known to have performance problems.
Don't use cuda 8, there are performance problems reported.
Always use tcmalloc.
",probably problem environment something try use different binary previous known performance use performance always use,issue,negative,negative,neutral,neutral,negative,negative
247809255,"Hi Yuxin,

I am trying to train a ResNet with two towers, and found the training is very slow (0.18iters/s) on 2 TITAN X. It's taking days to train a CIFAR-10 model. The inference is very fast though (8iters/s). I have cudnn installed and setup appropriately. Do you have any suggestion on how to fix that? 

Thanks!
",hi trying train two found training slow taking day train model inference fast though setup appropriately suggestion fix thanks,issue,negative,positive,positive,positive,positive,positive
246906249,"I try to search urllib issue.
But current state are not good.....
(I understand, i can access not secure url, but can not access secure url,,,)

Thank you for your advises.
I continue to try.....
",try search issue current state good understand access secure access secure thank continue try,issue,positive,positive,positive,positive,positive,positive
246903572,"That's a python issue, and I don't think it's related to proxy.
urllib in python definitely supports http_proxy environment variables. But it looks like there is something wrong with SSL. Googling the error would give you something you can try.
",python issue think related proxy python definitely environment like something wrong error would give something try,issue,negative,negative,negative,negative,negative,negative
246546912,"Thank you.
Line 305 trace back are not occurred.

YES,YES.
I have another problem not tensorpack problem.
My network environment are under proxy network.

So, I set environment variables for proxy ,
http_proxy=http://pxoxyurl:8080/
https_proxy=https://pxoxyurl:8080/
HTTPS_PROXY=https://pxoxyurl:8080/
HTTP_PROXY=http://pxoxyurl:8080/
But, python or tensorflow can not understand these variables.....

---

$ ./alexnet-dorefa.py --load alexnet-126.npy --run a.jpg --dorefa 1,2,6
.
.
.
[0913 10:27:26 @fs.py:40] ERR Failed to download https://github.com/BVLC/caffe/raw/master/src/caffe/proto/caffe.proto
Traceback (most recent call last):
  File ""./alexnet-dorefa.py"", line 303, in <module>
    run_image(Model(), ParamRestore(np.load(args.load, encoding='latin1').item()), args.run)
  File ""./alexnet-dorefa.py"", line 255, in run_image
    meta = dataset.ILSVRCMeta()
  File ""/home/sounansu/tensorpack/tensorpack/dataflow/dataset/ilsvrc.py"", line 34, in **init**
    self.caffepb = get_caffe_pb()
  File ""/home/sounansu/tensorpack/tensorpack/utils/loadcaffe.py"", line 83, in get_caffe_pb
    proto_path = download(CAFFE_PROTO_URL, dir)
  File ""/home/sounansu/tensorpack/tensorpack/utils/fs.py"", line 36, in download
    fpath, _ = urllib.request.urlretrieve(url, fpath, reporthook=_progress)
  File ""/usr/lib/python2.7/urllib.py"", line 94, in urlretrieve
    return _urlopener.retrieve(url, filename, reporthook, data)
  File ""/usr/lib/python2.7/urllib.py"", line 240, in retrieve
    fp = self.open(url, data)
  File ""/usr/lib/python2.7/urllib.py"", line 208, in open
    return getattr(self, name)(url)
  File ""/usr/lib/python2.7/urllib.py"", line 437, in open_https
    h.endheaders(data)
  File ""/usr/lib/python2.7/httplib.py"", line 975, in endheaders
    self._send_output(message_body)
  File ""/usr/lib/python2.7/httplib.py"", line 835, in _send_output
    self.send(msg)
  File ""/usr/lib/python2.7/httplib.py"", line 797, in send
    self.connect()
  File ""/usr/lib/python2.7/httplib.py"", line 1182, in connect
    self.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file)
  File ""/usr/lib/python2.7/ssl.py"", line 487, in wrap_socket
    ciphers=ciphers)
  File ""/usr/lib/python2.7/ssl.py"", line 243, in __init__
    self.do_handshake()
  File ""/usr/lib/python2.7/ssl.py"", line 405, in do_handshake
    self._sslobj.do_handshake()

## IOError: [Errno socket error] [Errno 8] _ssl.c:510: EOF occurred in violation of protocol
",thank line trace back yes yes another problem problem network environment proxy network set environment proxy python understand load run err recent call last file line module model file line meta file line file line file line file line return data file line retrieve data file line open return self name file line data file line file line file line send file line connect sock file line file line file line socket error violation protocol,issue,negative,neutral,neutral,neutral,neutral,neutral
245922390,"Thanks.The source code is this.
This source code works well.

``` python
#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# File: cifar-convnet.py
# Author: Yuxin Wu <ppwwyyxx@gmail.com>
import tensorflow as tf
import argparse
import numpy as np
import os

from tensorpack import *
import tensorpack.tfutils.symbolic_functions as symbf
from tensorpack.tfutils.summary import *
from dorefa import get_dorefa

from tensorpack.tfutils.symbolic_functions import *

""""""
A small convnet model for Cifar10 or Cifar100 dataset.

Cifar10:
    90% validation accuracy after 40k step.
    91% accuracy after 80k step.
    19.3 step/s on Tesla M40

Not a good model for Cifar100, just for demonstration.
""""""
BITW = 1
BITA = 2
BITG = 6
BATCH_SIZE = 32
class Model(ModelDesc):
    def __init__(self, cifar_classnum):
        super(Model, self).__init__()
        self.cifar_classnum = cifar_classnum

    def _get_input_vars(self):
        return [InputVar(tf.float32, [None, 30, 30, 3], 'input'),
                InputVar(tf.int32, [None], 'label')]

    def _build_graph(self, input_vars, is_training):

        image, label = input_vars
        image = image / 4.0     # just to make range smaller

        fw, fa, fg = get_dorefa(BITW, BITA, BITG)
        # monkey-patch tf.get_variable to apply fw
        old_get_variable = tf.get_variable
        def new_get_variable(name, shape=None, **kwargs):
            v = old_get_variable(name, shape, **kwargs)
            # don't binarize first and last layer
            if name != 'W' or 'conv0' in v.op.name or 'fct' in v.op.name:
                return v
            else:
                logger.info(""Binarizing weight {}"".format(v.op.name))
                return fw(v)
        tf.get_variable = new_get_variable

        def nonlin(x):
            if BITA == 32:
                return tf.nn.relu(x)    # still use relu for 32bit cases
            return tf.clip_by_value(x, 0.0, 1.0)

        def activate(x):
            return fa(nonlin(x))
        def cabs(x):
            return tf.minimum(1.0, tf.abs(x), name='cabs')

        keep_prob = tf.constant(0.5 if is_training else 1.0)

        if is_training:
            tf.image_summary(""train_image"", image, 10)

        print ""aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa""
        print is_training
        with  argscope(FullyConnected, use_bias=False, nl=tf.identity), \
              argscope(Conv2D, nl=BNReLU(is_training), use_bias=False, kernel_shape=3):
            logits = LinearWrap(image) \
                    .Conv2D('conv1.1', out_channel=64)\
                    .apply(activate)\
                    .Conv2D('conv1.2', out_channel=64) \
                    .apply(fg)\
                    .BatchNorm('bn0',use_local_stat=is_training)\
                    .MaxPooling('pool1', 3, stride=2, padding='SAME') \
                    .apply(activate)\
                    .Conv2D('conv2.1', out_channel=128)\
                    .apply(fg)\
                    .BatchNorm('bn2',use_local_stat=is_training)\
                    .apply(activate)\
                    .Conv2D('conv2.2', out_channel=128)\
                    .apply(fg)\
                    .BatchNorm('bn3',use_local_stat=is_training)\
                    .MaxPooling('pool2', 3, stride=2, padding='SAME') \
                    .apply(activate)\
                    .Conv2D('conv3.1', out_channel=128, padding='VALID') \
                    .apply(fg)\
                    .BatchNorm('bn4',use_local_stat=is_training)\
                    .apply(activate)\
                    .Conv2D('conv3.2', out_channel=128, padding='VALID') \
                    .apply(fg)\
                    .BatchNorm('bn5',use_local_stat=is_training)\
                    .apply(activate)\
                    .FullyConnected('fc0', 1024 + 512,
                           b_init=tf.constant_initializer(0.1)) \
                    .tf.nn.dropout(keep_prob) \
                    .apply(fg)\
                    .BatchNorm('bn6',use_local_stat=is_training)\
                    .FullyConnected('fc1', 512,
                           b_init=tf.constant_initializer(0.1)) \
                    .apply(fg)\
                    .BatchNorm('bn7',use_local_stat=is_training)\
                    .apply(nonlin)\
                    .FullyConnected('linear', out_dim=self.cifar_classnum, nl=tf.identity)()
        tf.get_variable = old_get_variable

        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)
        cost = tf.reduce_mean(cost, name='cross_entropy_loss')

        prob = tf.nn.softmax(logits, name='output')

        # compute the number of failed samples, for ClassificationError to use at test time
        wrong = symbf.prediction_incorrect(logits, label)
        nr_wrong = tf.reduce_sum(wrong, name='wrong')
        # monitor training error
        add_moving_summary(tf.reduce_mean(wrong, name='train_error'))

        # weight decay on all W of fc layers
        wd_cost = tf.mul(0.004,
                         regularize_cost('fc.*/W', tf.nn.l2_loss),
                         name='regularize_loss')
        add_moving_summary(cost, wd_cost)

        add_param_summary([('.*/W', ['histogram'])])   # monitor W
        self.cost = tf.add_n([cost, wd_cost], name='cost')

def get_data(train_or_test, cifar_classnum):
    isTrain = train_or_test == 'train'
    if cifar_classnum == 10:
        ds = dataset.Cifar10(train_or_test)
    else:
        ds = dataset.Cifar100(train_or_test)
    if isTrain:
        augmentors = [
            imgaug.RandomCrop((30, 30)),
            imgaug.Flip(horiz=True),
            imgaug.Brightness(63),
            imgaug.Contrast((0.2,1.8)),
            imgaug.GaussianDeform(
                [(0.2, 0.2), (0.2, 0.8), (0.8,0.8), (0.8,0.2)],
                (30,30), 0.2, 3),
            imgaug.MeanVarianceNormalize(all_channel=True)
        ]
    else:
        augmentors = [
            imgaug.CenterCrop((30, 30)),
            imgaug.MeanVarianceNormalize(all_channel=True)
        ]
    ds = AugmentImageComponent(ds, augmentors)
    ds = BatchData(ds, 128, remainder=not isTrain)
    if isTrain:
        ds = PrefetchData(ds, 3, 2)
    return ds
def get_config(cifar_classnum):
    logger.auto_set_dir()

    # prepare dataset
    dataset_train = get_data('train', cifar_classnum)
    step_per_epoch = dataset_train.size()
    dataset_test = get_data('test', cifar_classnum)

    sess_config = get_default_sess_config(0.5)

    nr_gpu = get_nr_gpu()#1e-5でeの−５乗0.00001
    lr = tf.train.exponential_decay(
        learning_rate=1e-4,
        global_step=get_global_step_var(),
        decay_steps=step_per_epoch * (30 if nr_gpu == 1 else 20),
        decay_rate=0.5, staircase=True, name='learning_rate')
    tf.scalar_summary('learning_rate', lr)

    return TrainConfig(
        dataset=dataset_train,
        optimizer=tf.train.AdamOptimizer(lr, epsilon=1e-4),
        callbacks=Callbacks([
            StatPrinter(),
            ModelSaver(),
            InferenceRunner(dataset_train, ClassificationError())#dataset_testに書き換える
        ]),
        session_config=sess_config,
        model=Model(cifar_classnum),
        step_per_epoch=step_per_epoch,
        max_epoch=250,
    )

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.') # nargs='*' in multi mode
    parser.add_argument('--load', help='load model')
    parser.add_argument('--classnum', help='10 for cifar10 or 100 for cifar100',
                        type=int, default=10)
    args = parser.parse_args()

    if args.gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    else:
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'

    with tf.Graph().as_default():
        config = get_config(args.classnum)
        if args.load:
            config.session_init = SaverRestore(args.load)
        if args.gpu:
            config.nr_tower = len(args.gpu.split(','))
        #QueueInputTrainer(config).train()
        SimpleTrainer(config).train()
```
",source code source code work well python python file author import import import import o import import import import import small model validation accuracy step accuracy step good model class model self super model self self return none none self image label image image make range smaller fa apply name name shape first last layer name return else weight return return still use bit return activate return fa return else image print print image activate activate activate activate activate activate cost label cost cost prob compute number use test time wrong label wrong monitor training error wrong weight decay cost monitor cost else else return prepare else return parser list use mode load model else,issue,negative,negative,neutral,neutral,negative,negative
245839787,"Fixed a missing import line in 2e238998a
Also, the error indicates that you cannot download from `https://github.com/BVLC/caffe/raw/master/src/caffe/proto/caffe.proto`, so you may need to check your network as well.
",fixed missing import line ea also error may need check network well,issue,negative,negative,neutral,neutral,negative,negative
245312191,"From the log it looks like your repo was not up-to-date. It actually runs well with latest commit.
",log like actually well latest commit,issue,positive,positive,positive,positive,positive,positive
245293537,"Finally,the issue is resolved.
I will paste the final source code later.
Thanks.
",finally issue resolved paste final source code later thanks,issue,positive,positive,neutral,neutral,positive,positive
245045158,"I don't have available segmentation code for now. Will have some later.
",available segmentation code later,issue,negative,positive,positive,positive,positive,positive
244527230,"> Could you try again? I can access the link even in chrome incognito mode.

Tried that again. Looks like it's on my side. Sorry for this. Thanks for your respond.
",could try access link even chrome incognito mode tried like side sorry thanks respond,issue,positive,negative,negative,negative,negative,negative
244391403,"In our FPGA implementation, we have avoided all floating point computations.

Conceptually, the output of conv is ""k X + b"", where X contains only low bitwidth integer numbers and ""k"" and ""b"" are floating point numbers,  and then a quantizing NL is applied. W.l.o.g., we next assume activations are 2-bit, then there will be only 4 possible values. We can write a program to find a few integer thresholds so that we can use integer comparisons X > th1, th1 > X > th2, th2 > X > th3, th3 > X to construct the 2-bit activations. More details of the above method will appear in an upcoming arXiv paper.
",implementation floating point conceptually output low integer floating point applied next assume possible write program find integer use integer th th th th th th construct method appear upcoming paper,issue,negative,neutral,neutral,neutral,neutral,neutral
244379101,"Could you try again? I can access the link even in chrome incognito mode. 
",could try access link even chrome incognito mode,issue,negative,neutral,neutral,neutral,neutral,neutral
244287062,"I see, do you think the following scheme is also okay to quantize x
1. clip it to range [-1, (2^(k-1) - 1)/2^(k-1)], since k bits can only represent -2^(k-1) to 2^(k-1) -1 
2. quantize by round(x*2^(k-1)), and this is the integer could be used in FPGA.
one point is that the positive part has less resolution.
",see think following scheme also quantize clip range since represent quantize round integer could used one point positive part le resolution,issue,positive,positive,neutral,neutral,positive,positive
244285923,"In our current settings, only the expensive operations (convolution & gemm) are performed in low-bitwidth. The elementwise scaling and nonlinearity are still in floating point.
",current expensive convolution scaling still floating point,issue,negative,negative,negative,negative,negative,negative
244285442,"Thanks for your reply,  since we implement it in FPGA with pipeline feature. The output of the current feature will directly feed to the next layer.  For example, we use 2 bits to represent the weights, according what you mentioned, the weights will be scaled by 3 in FPGA,. After the operation, we usually need to do the bit trunc operation (divided by 3), however 3 is not easy to trunc.
",thanks reply since implement pipeline feature output current feature directly feed next layer example use represent according scaled operation usually need bit operation divided however easy,issue,positive,positive,neutral,neutral,positive,positive
244279876,"A linear operation (convolution or gemm) on [0,0.33,0.67,1] can be computed, by first applying the operation on 3 \* [0,0.33,0.67,1] = [0,1,2,3] and then a linear transform. Since [0,1,2,3] are all fixed-point integers of k bit, the operands in the first part can be efficiently  represented and computed on FPGA.
",linear operation convolution first operation linear transform since bit first part efficiently,issue,negative,positive,positive,positive,positive,positive
243952476,"Finally I understand the method to use it.
Thank you for your kindness.
",finally understand method use thank kindness,issue,positive,neutral,neutral,neutral,neutral,neutral
243887599,"Sorry but I found that the previous `.print()` feature only works in Python3 as `print` is a reserved keyword in Python2. I changed it do `.print_tensor()` instead.
",sorry found previous feature work python print reserved python instead,issue,negative,negative,negative,negative,negative,negative
243773700,"You can see an example in the docstring in `alexnet-dorefa.py`. Something like `--load path/to/model --run a.jpg b.jpg c.jpg`.
Also see `alexnet-dorefa.py --help`.
",see example something like load run also see help,issue,positive,neutral,neutral,neutral,neutral,neutral
243757328,"Thank you for your replying.
Your explanation is very clear so that I got confidence to implement it.

I have one more question.
I want to use the function of run_image() in DoReFa  net.
But, I couldn't understand how to specify the argument to run it.
I put the  the command like this.

## python alexnet-dorefa.py --run

But it did't work well because the training was started.

Thanks a lot!
",thank explanation clear got confidence implement one question want use function net could understand specify argument run put command like python run work well training thanks lot,issue,positive,positive,positive,positive,positive,positive
243749363,"Thanks. https://github.com/ppwwyyxx/tensorpack/blob/master/examples/load-alexnet.py#L60 has a function to load an existing model (through the `session_init` option) and run the model on new data.
In the example it's loading from a dict. You could use `session_init=SaverRestore('train_log/xxx/model-xxx"")` if you're loading from a TF-format checkpoint.

DoReFa net examples also contain such functions at [here](https://github.com/ppwwyyxx/tensorpack/blob/master/examples/DoReFa-Net/alexnet-dorefa.py#L246). 
Feel free to ask if it's not clear enough.
",thanks function load model option run model new data example loading could use loading net also contain feel free ask clear enough,issue,positive,positive,positive,positive,positive,positive
243663369,"When model is written in this syntax-sugar, it'll be harder to find out names of those tensors.

What you can do is to split the expression and print the tensors to find out the names:

``` python
            logits = (LinearWrap(image)
                .Conv2D('conv0', 96, 12, stride=4, padding='VALID')
                .apply(activate)())
            print(logits)
            logits = (LinearWrap(logits)
           #.......
```

I also just added a convenient `print` method for the linear model builder in the latest commit. After a pull you can use the following:

``` python
            logits = (LinearWrap(image)
                .Conv2D('conv0', 96, 12, stride=4, padding='VALID')
                .print()  # conv0/output
                .apply(activate)
                #.......
```

to find out the name of the tensors.
",model written harder find split expression print find python image activate print also added convenient print method linear model builder latest commit pull use following python image activate find name,issue,negative,positive,positive,positive,positive,positive
243517895,"Thanks for your reply.
I would like to check the output of each layer in the DoReFa-Net example.
It uses this format to define the graph:
""logits = (LinearWrap(image)
                .Conv2D('conv0', 96, 12, stride=4, padding='VALID')
                .apply(activate)...""
How can I get the variable name of the output of each layer?
I have tried to use tf.all_variables(), but it returned an empty list.
",thanks reply would like check output layer example format define graph image activate get variable name output layer tried use returned empty list,issue,positive,positive,neutral,neutral,positive,positive
243328593,"Do you want to do this in inference time? Just add the names of those tensors to output names.
",want inference time add output,issue,negative,neutral,neutral,neutral,neutral,neutral
237457461,"haven't had a chance to look. is there still an issue?
",chance look still issue,issue,negative,neutral,neutral,neutral,neutral,neutral
234328495,"Yes, you can specify different learning rate for each parameter, by scaling its gradient. You can overwrite the `get_gradient_processor()` method in your Model with something like:

``` python
    def get_gradient_processor(self):
        return [ScaleGradient([('conv.*/W', 0.5), ('fc.*', 2)])]
```

ScaleGradient takes a list of `(regex, multiplier)` tuple. Parameters that are scaled will get printed out so you could check.

// UPDATE: this usage was later deprecated. See comment below.",yes specify different learning rate parameter scaling gradient overwrite method model something like python self return list multiplier scaled get printed could check update usage later see comment,issue,positive,neutral,neutral,neutral,neutral,neutral
230961082,"Hi,
We're not planning to release the bit-op runtime library in the near future. 
The model isn't going to be released.
",hi release library near future model going,issue,negative,positive,neutral,neutral,positive,positive
228295126,"The issue seems to be resolved by updating drivers and cuda versions. Thank you for your help!
",issue resolved thank help,issue,positive,neutral,neutral,neutral,neutral,neutral
227801988,"Maybe my code uses some part of tensorflow that's broken on 1080. I may get some access to test on 1080 in a week but not now.

Also, I think only cuda 8 supports Pascal Architecture. How can you use 1080 with older cuda versions?
",maybe code part broken may get access test week also think architecture use older,issue,negative,negative,negative,negative,negative,negative
227798556,"I can reproduce your results on the CPU of the tensorpack example. I can also reproduce the official mnist tensorflow tutorial results with GPU.

I am using the older cuda and cudnn versions which are compatible with the published tensorflow binaries. Maybe compiling from source with the latest version can help?
",reproduce example also reproduce official tutorial older compatible maybe source latest version help,issue,negative,positive,positive,positive,positive,positive
227786718,"Hi,
I'm not sure what's going wrong. But I tested on 1080 before and did see some weird things on that.
Now I don't have a 1080 at hand for testing, but maybe you could run on CPU to see if things go well. For mnist it should get 98~99% accuracy after first epoch. 

I'm settings `CUDA_VISIBLE_DEVICES` in `mnist-convnet.py` to 0 by default, so maybe you'll need to delete that line to run on CPU.
",hi sure going wrong tested see weird hand testing maybe could run see go well get accuracy first epoch default maybe need delete line run,issue,negative,negative,neutral,neutral,negative,negative
221191226,"If data becomes a bottleneck, PrefetchDataZMQ should be used instead of PrefetchData. Usually several times faster.
",data becomes bottleneck used instead usually several time faster,issue,negative,neutral,neutral,neutral,neutral,neutral
221175254,"Thanks! Now  `CifarBase` is the baseclass of `Cifar10` and `Cifar100`. And the duplicate code in dataflow is also removed.
",thanks duplicate code also removed,issue,negative,positive,positive,positive,positive,positive
221104659,"Thanks a lot for sending the first pull request! 
However, examples are for illustration purpose of the usage of this library. Since the cifar100 example is almost identical to cifar10, it's not necessary to add an extra one specifically for cifar100.

Same for the dataflow code. We would really prefer minimal duplicated code and more modularity, because that's all this project is intended for. 
Something you may try:
Have `Cifar100` being a subclass of `Cifar10`, or more generally, use a  `CifarBase` and have a new `Cifar10` and `Cifar100` being subclasses of `CifarBase`. `Cifar10` and `Cifar100` would only be different in how they load the data, and that could be done by calling the corresponding function by different arguments (different URLs, different keys for the label, etc.) This way most existing code can be reused.
",thanks lot sending first pull request however illustration purpose usage library since example almost identical necessary add extra one specifically code would really prefer minimal code project intended something may try subclass generally use new would different load data could done calling corresponding function different different different label way code,issue,negative,positive,neutral,neutral,positive,positive
215168156,"Confirm the performance issue is due to slow socket communication of multiprocessing.
Will work on a better version.
",confirm performance issue due slow socket communication work better version,issue,negative,positive,neutral,neutral,positive,positive
214824229,"Confirm that pickle is really slow for ILSVRC-size images:

``` python
import numpy as np
#import cPickle as pickle
import pickle
import time
d = np.random.rand(32, 299, 299, 3).astype('float32')
print(time.time())
dump = pickle.dumps(d)
print(time.time())
dd = pickle.loads(dump)
print(time.time())
```

cPickle doesn't help. But python3 is much better.
Dill helps as well in python2.
",confirm pickle really slow python import import pickle import pickle import time print dump print dump print help python much better dill well python,issue,negative,positive,neutral,neutral,positive,positive
214796413,"Found that ILSVRC12 data flow only has a speed of 6it/s while the queue is full. Probably due to serializing and message passing among processes.
",found data flow speed queue full probably due message passing among,issue,negative,positive,positive,positive,positive,positive
214667138,"Thanks for replying me. What you said is true and reasonable. I will concentrate on the self-contained parts now and get my hands dirty. 
",thanks said true reasonable concentrate get dirty,issue,negative,positive,neutral,neutral,positive,positive
214655155,"Thanks a lot for your interest. Currently the project is usable but still in a very immature stage: there are a lot of design choices to be made, and some performance issues to be solved. APIs might change, and distributed training are not even on the calendar (probably will require more refactoring). These are why it's probably not a good time to write documentations.

Despite that, the `dataflow` module and the `model` module are mostly self-contained and reusable components that probably won't change a lot in the future. I may start working on some documentation on these parts.
",thanks lot interest currently project usable still immature stage lot design made performance might change distributed training even calendar probably require probably good time write despite module model module mostly probably wo change lot future may start working documentation,issue,positive,positive,positive,positive,positive,positive
214578079,"Sorry I introduced a bug yesterday. I'll work on a fix soon. For the moment you can checkout to fd21c3b1afa4b870bbd5f12c1f68e264aafe4c02 or before.
",sorry bug yesterday work fix soon moment,issue,negative,negative,negative,negative,negative,negative
